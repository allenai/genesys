{
    "variantname": "HierarchicalCompressiveFastTTTLinear",
    "review": "The HierarchicalFastTTTLinear proposal presents a novel approach to enhancing FastTTTLinear by integrating hierarchical multi-scale processing with temporal latent bottlenecks and adaptive state compression. After comprehensive analysis of the proposal and comparison with existing research, here is a detailed review:\n\nCLARITY (4.5/5):\nThe proposal is exceptionally well-structured and clearly articulated, with:\n+ Well-defined objectives and motivation\n+ Clear mathematical formulations\n+ Detailed implementation guidelines\n+ Comprehensive theoretical analysis\nMinor weakness: Some implementation details regarding test-time training preservation could be more specific.\n\nINNOVATION (4.3/5):\nThe proposal demonstrates significant innovation through:\n+ Novel integration of hierarchical processing with temporal latent bottlenecks\n+ Unique approach to adaptive state compression\n+ Hardware-aware implementation considerations\n+ Preservation of test-time training capabilities\nHowever:\n- Some concepts build on existing work (e.g., Dynamic Memory Compression, H-Transformer-1D)\n- Memory optimization techniques share similarities with existing approaches\n\nFEASIBILITY (4.0/5):\nImplementation appears feasible within the framework:\n+ Clear mathematical formulations\n+ Detailed implementation guidelines\n+ Consideration of practical constraints\n+ Compatibility with existing optimizations\nConcerns:\n- Complex integration of multiple components\n- Test-time training preservation challenges\n- State management complexity\n\nSCALABILITY (4.4/5):\nThe design shows strong potential for scaling:\n+ Linear complexity through hierarchical processing\n+ Efficient state management through compression\n+ Memory-optimized implementation\n+ Hardware-aware considerations\nMinor concerns:\n- Memory bandwidth requirements for very long sequences\n- Communication overhead in distributed settings\n\nACCURACY AND ROBUSTNESS (4.2/5):\nExpected improvements in performance:\n+ Enhanced modeling of long-range dependencies\n+ Improved feature extraction through multi-scale processing\n+ Better handling of varied sequence lengths\n+ Robust state management\nConcerns:\n- Balance between compression and information preservation\n- Impact on model quality with aggressive compression\n- Stability of hierarchical processing\n\nEFFICIENCY (4.5/5):\nStrong focus on computational efficiency:\n+ Linear complexity maintained\n+ Memory-efficient state management\n+ Hardware-aware implementation\n+ Efficient parallel processing\nMinor concerns:\n- Additional computation from hierarchical processing\n- Memory access patterns optimization needed",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient state compression neural networks",
            "detail": "Find papers discussing techniques for efficient state compression and management in neural networks, particularly those focusing on:\n1. Adaptive compression methods\n2. Content-based state reduction\n3. Memory-efficient state updates\n4. Integration with attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing techniques for efficient state compression and management in neural networks, particularly those focusing on:\n1. Adaptive compression methods\n2. Content-based state reduction\n3. Memory-efficient state updates\n4. Integration with attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.92)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.92)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.91)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.91)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.82)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.82)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n#### 4. Latent Attention for Linear Time Transformers (Avg. Score: 0.74)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.74)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. Glorot, X., Bordes, A., and Bengio, Y. Deep Sparse Rectifier Neural Networks. In JMLR Workshop and Conference Proceedings, pp. 315-323, 2011. Gokaslan, A. and Cohen, V. OpenWebText Corpus, 2019. URL http://Skylion007.github.io/ OpenWebTextCorpus. Gu, A., Goel, K., and R\u00e9, C. Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396, 2021. Hutter, M. The Human Knowledge Compression Prize, 2002. URL https:// www.kurzweilai.net/hutter-prizefor-lossless-compression-of-humanknowledge. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General Perception with Iterative Attention. In International Conference on Machine Learning, pp. 4651-4664. PMLR, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Khan, S., Naseer, M., Hayat, M., Zamir, S.\n```\n\n#### 5. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.66)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 10/16 (Score: 0.66)*\n\n```\nURL http: / / arxiv.org/abs/2310. 05736. arXiv:2310.05736 [cs]. Lanchantin, J., Toshniwal, S., Weston, J., Szlam, A., and Sukhbaatar, S. Learning to Reason and Memorize with Self-Notes, October 2023. URL http: / / arxiv org/abs/2305.00833. arXiv:2305.00833 [cs]. Mu, J., Li, X. L., and Goodman, N. Learning to Compress Prompts with Gist Tokens, July 2023. URL http: / / arxiv.org/abs/2304.08467. arXiv:2304.08467 [cs]. O'Shea, K. and Nash, R. An Introduction to Convolutional Neural Networks, December 2015. URL http: / / arxiv.org/abs/1511.08458. arXiv:1511.08458 [cs]. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. ISSN 1533-7928. URL http: / jmlr.org/papers/v21/20-074.html. Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., and Orr, D. SparQ Attention: Bandwidth-Efficient LLM Inference, December 2023.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient state compression neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks\n\n*From Search Query: efficient state compression neural networks*\n\n*Zhiwei Deng, Olga Russakovsky*\n\n**Abstract:** We propose an algorithm that compresses the critical information of a large dataset into compact addressable memories. These memories can then be recalled to quickly re-train a neural network and recover the performance (instead of storing and re-training on the full original dataset). Building upon the dataset distillation framework, we make a key observation that a shared common representation allows for more efficient and effective distillation. Concretely, we learn a set of bases (aka ``memories'') which are shared between classes and combined through learned flexible addressing functions to generate a diverse set of training examples. This leads to several benefits: 1) the size of compressed data does not necessarily grow linearly with the number of classes; 2) an overall higher compression rate with more effective distillation is achieved; and 3) more generalized queries are allowed beyond recalling the original classes. We demonstrate state-of-the-art results on the dataset distillation task across six benchmarks, including up to 16.5% and 9.7% in retained accuracy improvement when distilling CIFAR10 and CIFAR100 respectively. We then leverage our framework to perform continual learning, achieving state-of-the-art results on four benchmarks, with 23.2% accuracy improvement on MANY. The code is released on our project webpage https://github.com/princetonvisualai/RememberThePast-DatasetDistillation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 79  (*Influential: 17*)\n\n#### 2. LegoNet: Efficient Convolutional Neural Networks with Lego Filters\n\n*From Search Query: efficient state compression neural networks*\n\n*Zhaohui Yang, Yunhe Wang, Chuanjian Liu, Hanting Chen, Chunjing Xu, Boxin Shi, Chao Xu, Chang Xu*\n\n**TL;DR:** A split-transform-merge strategy for an ef\ufb01cient convolution by exploiting intermediate Lego feature maps is developed and Inspired by network engineering, it is suggested that an ordinary \ufb01lter in the neural network can be upgraded to a sophisticated module as well.\n\n**Abstract:** This paper aims to build ef\ufb01cient convolutional neural networks using a set of Lego \ufb01lters. Many successful building blocks, e.g. inception and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recognition tasks. Beyond these high-level modules, we suggest that an ordinary \ufb01lter in the neural network can be upgraded to a sophisticated module as well. Filter modules are established by assem-bling a shared set of Lego \ufb01lters that are often of much lower dimensions. Weights in Lego \ufb01lters and binary masks to stack Lego \ufb01lters for these \ufb01lter modules can be simultaneously optimized in an end-to-end manner as usual. Inspired by network engineering, we develop a split-transform-merge strategy for an ef\ufb01cient convolution by exploiting intermediate Lego feature maps. The compression and acceleration achieved by Lego Networks using the proposed Lego \ufb01lters have been theoretically discussed. Experimental results on benchmark datasets and deep models demonstrate the advantages of the proposed Lego \ufb01lters and their potential real-world applications on mobile devices.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 5*)\n\n#### 3. WoodFisher: Efficient Second-Order Approximation for Neural Network Compression\n\n*From Search Query: efficient state compression neural networks*\n\n*Sidak Pal Singh, Dan Alistarh*\n\n**TL;DR:** It is demonstrated that WoodFisher significantly outperforms popular state-of-the-art methods for one-shot pruning and can be extended to take into account first-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime.\n\n**Abstract:** Second-order information, in the form of Hessian- or Inverse-Hessian-vector products, is a fundamental tool for solving optimization problems. Recently, there has been significant interest in utilizing this information in the context of deep neural networks; however, relatively little is known about the quality of existing approximations in this context. Our work examines this question, identifies issues with existing approaches, and proposes a method called WoodFisher to compute a faithful and efficient estimate of the inverse Hessian. \nOur main application is to neural network compression, where we build on the classic Optimal Brain Damage/Surgeon framework. We demonstrate that WoodFisher significantly outperforms popular state-of-the-art methods for one-shot pruning. Further, even when iterative, gradual pruning is considered, our method results in a gain in test accuracy over the state-of-the-art approaches, for pruning popular neural networks (like ResNet-50, MobileNetV1) trained on standard image classification datasets such as ImageNet ILSVRC. We examine how our method can be extended to take into account first-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 138  (*Influential: 14*)\n\n#### 4. ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training\n\n*From Search Query: efficient state compression neural networks*\n\n*Chia-Yu Chen, Jiamin Ni, Songtao Lu, Xiaodong Cui, Pin-Yu Chen, Xiao Sun, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Wei Zhang, K. Gopalakrishnan*\n\n**TL;DR:** This work proposes a new compression technique, Scalable Sparsified Gradient Compression (ScaleCom), that leverages similarity in the gradient distribution amongst learners to provide significantly improved scalability and provides favorable convergence guarantees and is compatible with gradient all-reduce techniques.\n\n**Abstract:** Large-scale distributed training of Deep Neural Networks (DNNs) on state-of-the-art platforms is expected to be severely communication constrained. To overcome this limitation, numerous gradient compression techniques have been proposed and have demonstrated high compression ratios. However, most existing methods do not scale well to large scale distributed systems (due to gradient build-up) and/or fail to evaluate model fidelity (test accuracy) on large datasets. To mitigate these issues, we propose a new compression technique, Scalable Sparsified Gradient Compression (ScaleCom), that leverages similarity in the gradient distribution amongst learners to provide significantly improved scalability. Using theoretical analysis, we show that ScaleCom provides favorable convergence guarantees and is compatible with gradient all-reduce techniques. Furthermore, we experimentally demonstrate that ScaleCom has small overheads, directly reduces gradient traffic and provides high compression rates (65-400X) and excellent scalability (up to 64 learners and 8-12X larger batch sizes over standard training) across a wide range of applications (image, language, and speech) without significant accuracy loss.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 61  (*Influential: 3*)\n\n#### 5. Entroformer: A Transformer-based Entropy Model for Learned Image Compression\n\n*From Search Query: efficient state compression neural networks*\n\n*Yichen Qian, Ming Lin, Xiuyu Sun, Zhiyu Tan, Rong Jin*\n\n**TL;DR:** This work proposes a novel transformer-based entropy model, termed Entroformer, to capture long-range dependencies in probability distribution estimation effectively and efficiently and achieves state-of-the-art performance on image compression while being time-efficient.\n\n**Abstract:** One critical component in lossy deep image compression is the entropy model, which predicts the probability distribution of the quantized latent representation in the encoding and decoding modules. Previous works build entropy models upon convolutional neural networks which are inefficient in capturing global dependencies. In this work, we propose a novel transformer-based entropy model, termed Entroformer, to capture long-range dependencies in probability distribution estimation effectively and efficiently. Different from vision transformers in image classification, the Entroformer is highly optimized for image compression, including a top-k self-attention and a diamond relative position encoding. Meanwhile, we further expand this architecture with a parallel bidirectional context model to speed up the decoding process. The experiments show that the Entroformer achieves state-of-the-art performance on image compression while being time-efficient.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 108  (*Influential: 26*)\n\n### 3 related papers from Papers with Code\n\n#### 1. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: efficient state compression neural networks*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 2. HPTQ: Hardware-Friendly Post Training Quantization\n\n*From Search Query: efficient state compression neural networks*\n\n*Arnon Netzer, Roy H. Jennings, Idit Diamant, Oranit Dror, Lior Dikstein, Elad Cohen, Reuven Peretz, Hai Victor Habi*\n\n**Abstract:** Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.\n\n**Published:** 2021-09-19\n\n\n\n#### 3. Dataset Distillation with Infinitely Wide Convolutional Networks\n\n*From Search Query: efficient state compression neural networks*\n\n*Jaehoon Lee, Lechao Xiao, Roman Novak, Timothy Nguyen*\n\n**Abstract:** The effectiveness of machine learning algorithms arises from being able to extract useful features from large amounts of data. As model and dataset sizes increase, dataset distillation methods that compress large datasets into significantly smaller yet highly performant ones will become valuable in terms of training efficiency and useful feature extraction. To that end, we apply a novel distributed kernel based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. For instance, using only 10 datapoints (0.02% of original dataset), we obtain over 65% test accuracy on CIFAR-10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. Our state-of-the-art results extend across many other settings for MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-27\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient state compression and management, here are some key techniques and papers that align with your requirements:\n\n## Adaptive Compression Methods\n- The paper on **NeuZip** introduces an algorithm for compressing neural networks while maintaining their full capabilities. This method dynamically compresses the neural network, which could be adapted for efficient state compression in autoregressive models.\n- **Efficient Neural Network Pruning Based on Relative Importance** discusses a pruning algorithm that incorporates a novel regularization term to enhance the effectiveness of pruning. This approach could be adapted for adaptive compression by dynamically adjusting the pruning ratio based on the relative importance of weights and neurons.\n\n## Content-Based State Reduction\n- The concept of **selective state propagation** from the Mamba paper can be leveraged to reduce state dimensions based on the content. By letting the state space model parameters be functions of the input, the model can selectively propagate or forget information, which is crucial for efficient state management [Analysis Note].\n- **Loki's low-rank approximation** suggests that key vectors in attention mechanisms lie in a significantly lower-dimensional space. This finding can be used to implement content-based dimension reduction, reducing the memory footprint without significant performance loss [Analysis Note].\n\n## Memory-Efficient State Updates\n- The **NeuZip** algorithm also focuses on memory-efficient training and inference by compressing floating-point numbers, which can be applied to state updates in autoregressive models to reduce memory usage.\n- **Efficient Neural Network Pruning** includes an optimized matrix multiplication algorithm to compute the contributions of entire columns of weights simultaneously, which can be applied to improve the efficiency of state updates.\n\n## Integration with Attention Mechanisms\n- **CoLT5's conditional computation** insights can inform the integration of state compression with attention mechanisms. By selectively applying heavy computation based on token importance, the model can balance efficiency and capacity. This approach can be combined with attention mechanisms to optimize state management [Analysis Note].\n- **Loki's low-rank keys for efficient sparse attention** demonstrate how to reduce the dimensionality of key vectors in attention mechanisms. This technique can be integrated with state space models to achieve more efficient and scalable autoregressive language models [Analysis Note].\n\n### Additional Considerations\n- **Model Distillation** techniques can also be used to compress deep neural networks, including autoregressive language models. By transferring knowledge from a larger model to a smaller one, model distillation can help in achieving efficient state management and reducing the overall model size.\n\nBy incorporating these techniques, researchers can develop more efficient autoregressive language models that optimize state compression, reduce memory footprint, and integrate effectively with attention mechanisms.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis\n\nThe initial analysis reveals several key areas for potential improvement in LM block design:\n\na) Memory and State Management:\n- Current designs like FastTTTLinear show limitations in memory efficiency for long sequences\n- Opportunity exists for better state compression and management\n- Need for more efficient handling of intermediate representations\n\nb) Computational Efficiency:\n- FastTTTLinear's test-time training capability is valuable but computationally intensive\n- Room for optimization in attention computation and state updates\n- Potential for improved parallelization and hardware utilization\n\nc) Architectural Innovation:\n- Mamba's selective state space models show promise for efficient sequence modeling\n- CoLT5's conditional computation offers insights into resource allocation\n- Loki's low-rank approximation suggests opportunities for dimension reduction\n\n2. Useful References with Excerpts\n\nFrom Mamba paper:\n\"First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\nThis suggests potential for selective information propagation in state management.\n\nFrom CoLT5:\n\"Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token.\"\n\nHighlights need for efficient token processing.\n\nFrom Loki:\n\"Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models.\"\n\nSuggests opportunity for dimension reduction in key representations.\n\n3. Key Insights and Analysis\n\na) State Space Integration:\n- Mamba's selective state propagation could be adapted for more efficient memory management\n- Potential to combine with FastTTTLinear's test-time training capability\n- Need to maintain stability while improving efficiency\n\nb) Conditional Processing:\n- CoLT5's insights on token importance could inform state compression\n- Opportunity to selectively apply heavy computation\n- Balance needed between efficiency and model capacity\n\nc) Dimension Optimization:\n- Loki's findings on key vector dimensionality suggest room for optimization\n- Could reduce memory footprint without significant performance loss\n- Potential for adaptive dimension reduction based on content\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in efficient state compression techniques\n2. Methods for adaptive dimension reduction in neural networks\n3. Techniques for combining state space models with attention mechanisms\n\n5. References\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[2] Ainslie, J., et al. (2023). CoLT5: Faster Long-Range Transformers with Conditional Computation. Conference on Empirical Methods in Natural Language Processing.\n\n[3] Singhania, P., et al. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint.\n\n[4] Chen, T., Zhang, R., & Hinton, G.E. (2022). Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "efficient state space models compression",
            "detail": "Find papers discussing state space models and their compression techniques, particularly focusing on:\n1. Memory-efficient implementations\n2. Adaptive state reduction methods\n3. Integration with attention mechanisms\n4. Hardware-aware optimizations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing state space models and their compression techniques, particularly focusing on:\n1. Memory-efficient implementations\n2. Adaptive state reduction methods\n3. Integration with attention mechanisms\n4. Hardware-aware optimizations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.98)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.98)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.97)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.97)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.96)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.96)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.82)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.82)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 5. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.75)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.75)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient state space models compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression\n\n*From Search Query: efficient state space models compression*\n\n*Yufeng Zhang, Hang Yu, Jianguo Li, Weiyao Lin*\n\n**TL;DR:** A novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression that incorporates an efficient autoregressive Markov model based entropy coder and a Straight-Through Hardmax Quantization scheme to enhance the optimization of discrete latent space.\n\n**Abstract:** A BSTRACT Learned lossless data compression has garnered significant attention recently due to its superior compression ratios compared to traditional compressors. However, the computational efficiency of these models jeopardizes their practicality. This paper proposes a novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression. Our approach incorporates two essential innovations. First, we propose the Finite-State AutoRe-gressive (FSAR) entropy coder, an efficient autoregressive Markov model based entropy coder that utilizes a lookup table to expedite autoregressive entropy coding. Next, we present a Straight-Through Hardmax Quantization (STHQ) scheme to enhance the optimization of discrete latent space. Our experiments show that the proposed lossless compression method could improve the compression ratio by up to 6% compared to the baseline, with negligible extra computational time. Our work provides valuable insights into enhancing the computational efficiency of learned lossless data compression, which can have practical applications in various fields. Code is available at https://github.com/alipay/Finite_ State_Autoregressive_Entropy_Coding .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models\n\n*From Search Query: efficient state space models compression*\n\n*Qingqing Cao, Sewon Min, Yizhong Wang, Hannaneh Hajishirzi*\n\n**TL;DR:** BTR is introduced, which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference, and accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance.\n\n**Abstract:** Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 3. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\n\n*From Search Query: efficient state space models compression*\n\n*Keshav Santhanam, O. Khattab, Jon Saad-Falcon, Christopher Potts, M. Zaharia*\n\n**TL;DR:** Maize is introduced, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction and establishes state-of-the-art quality within and outside the training domain.\n\n**Abstract:** Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce Maize, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate Maize across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6\u201310x.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 313  (*Influential: 67*)\n\n#### 4. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: efficient state space models compression*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: efficient state space models compression*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n### 3 related papers from Papers with Code\n\n#### 1. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: efficient state space models compression*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 2. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\n\n*From Search Query: efficient state space models compression*\n\n*Karsten Kreis, Sanja Fidler, Seung Wook Kim, Tim Dockhorn, Huan Ling, Robin Rombach, Andreas Blattmann*\n\n**Abstract:** Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://research.nvidia.com/labs/toronto-ai/VideoLDM/\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-04-18\n\n\n\n#### 3. FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU\n\n*From Search Query: efficient state space models compression*\n\n*Ce Zhang, Ion Stoica, Christopher R\u00e9, Percy Liang, Joseph E. Gonzalez, Clark Barrett, Beidi Chen, Zhiqiang Xie, Daniel Y. Fu, Max Ryabinin, Zhuohan Li, Binhang Yuan, Lianmin Zheng, Ying Sheng*\n\n**Abstract:** The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen\n\n**Published:** 2023-03-13\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using efficient state space models and their compression techniques, here are some key points and references that align with your requirements:\n\n## Memory-Efficient Implementations\n- State-space models (SSMs) have shown promise in reducing computational complexity while maintaining performance. For instance, the Longhorn model uses SSMs to achieve linear decoding efficiency and parallelism during training, which is more memory-efficient compared to traditional Transformers.\n- The MambaIR model, although focused on image restoration, introduces the Residual State Space Block, which employs convolution and channel attention. This approach can be adapted for sequence modeling, offering a memory-efficient way to handle long-range dependencies with linear complexity.\n\n## Adaptive State Reduction Methods\n- The Mamba model's selective state propagation mechanism is particularly relevant. It allows the model to selectively propagate or forget information along the sequence length dimension based on the current token, which is a form of adaptive state reduction.\n- The SPikE-SSM model integrates a refined Leaky Integrate-and-Fire (LIF) neuron with reset-refractory dynamics, enabling adaptive compression and maintaining high sparsity while achieving high accuracy. This hierarchical integration of the neuron model into the SSM block can be adapted for language models to reduce state dimensions dynamically.\n\n## Integration with Attention Mechanisms\n- While the papers do not directly discuss integrating SSMs with attention mechanisms, the concept of using SSMs as \"meta-modules\" that compress history into a state can be combined with attention. For example, the compressed state from an SSM can be used as input to an attention mechanism, allowing for efficient scaling to larger context windows.\n- The idea of latent representations, as hinted in the analysis note, can be explored further. Latent attention mechanisms can be used in conjunction with SSMs to scale to larger context windows efficiently[Analysis Note].\n\n## Hardware-Aware Optimizations\n- The Longhorn model's design avoids the inefficiencies of traditional backpropagation through time by using a recurrent form during inference, which is more hardware-friendly. This approach can be optimized further with tiling and memory hierarchy optimization strategies, similar to those used in FlashAttention.\n- The SPikE-SSM model's use of a parallel max-min boundary compression strategy (PMBC) to accelerate inference is another example of hardware-aware optimization. This strategy enables parallel processing for long sequence modeling, which can be beneficial for language models.\n\n## Additional Insights\n- Model compression techniques such as low-rank approximations, pruning, and quantization can be applied to SSMs to further reduce their memory footprint. These techniques are crucial for maintaining model quality while reducing computational resources.\n- The integration of SSMs with other efficient models, such as those using matrix product states (MPS), could offer additional benefits in terms of scalability and efficiency. MPS can represent quantum states in a compact form, which might be useful for highly dimensional data in language models.\n\nBy combining these insights, researchers can develop more efficient, scalable, and adaptive autoregressive language models that leverage the strengths of state space models and advanced compression techniques.\n",
            "analysis": "Analysis Note: State Compression and Memory Management in LM Blocks\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing FastTTTLinear through efficient state compression and memory management:\n\na) Hardware-Aware Memory Management:\n- FlashAttention's IO-aware approach shows potential for optimizing memory access patterns\n- Importance of tiling and memory hierarchy optimization for efficient state management\n- Need for hardware-specific acceleration strategies\n\nb) Selective State Processing:\n- Mamba's selective state propagation mechanism offers insights for adaptive compression\n- Content-based state reduction can improve efficiency without sacrificing model quality\n- Potential for hierarchical state management\n\nc) Efficient Compression Techniques:\n- Low-rank approximations show promise for reducing memory footprint\n- Opportunity for adaptive compression based on content importance\n- Need for lossless compression methods for critical state information\n\n2. Useful References with Excerpts\n\nFrom FlashAttention:\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.\"\n\nThis suggests potential for memory-efficient state management through careful IO optimization.\n\nFrom Mamba:\n\"First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\nThis indicates the value of selective state processing for efficiency.\n\nFrom Latent Attention:\n\"The method is readily usable as a drop-in replacement for the standard attention mechanism [...] allowing scaling to context windows much larger than practical in standard attention.\"\n\nShows potential for efficient scaling through latent representations.\n\n3. Key Insights and Analysis\n\na) Memory Hierarchy Optimization:\n- Tiling-based approaches can significantly reduce memory bandwidth requirements\n- Hardware-aware implementation crucial for real-world performance\n- Need to balance compression ratio with computational overhead\n\nb) Selective Processing:\n- Content-dependent state compression can improve efficiency\n- Importance of maintaining critical information while reducing redundancy\n- Potential for multi-scale state representation\n\nc) Integration Strategies:\n- Need for efficient fusion of compressed states with attention mechanism\n- Importance of maintaining test-time training capability\n- Opportunity for adaptive compression based on sequence characteristics\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in efficient state space models and their compression techniques\n2. Methods for adaptive dimension reduction in neural networks\n3. Techniques for combining compressed states with attention mechanisms\n\n5. References\n\n[1] Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\". Neural Information Processing Systems.\n\n[2] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". arXiv preprint.\n\n[3] Dolga, R., et al. (2024). \"Latent Attention for Linear Time Transformers\". arXiv preprint.\n\n[4] Wang, Y., & Xiao, Z. (2024). \"LoMA: Lossless Compressed Memory Attention\". arXiv preprint."
        },
        {
            "ready": false,
            "query": "efficient attention ssm integration",
            "detail": "Find papers discussing the integration of state space models with attention mechanisms, focusing on:\n1. Hybrid architectures combining SSMs and attention\n2. Efficient attention mechanisms for long sequences\n3. Memory-efficient attention implementations\n4. Performance comparisons between different approaches",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing the integration of state space models with attention mechanisms, focusing on:\n1. Hybrid architectures combining SSMs and attention\n2. Efficient attention mechanisms for long sequences\n3. Memory-efficient attention implementations\n4. Performance comparisons between different approaches\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 1.00)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 1.00)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 1.00)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 4. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 1.00)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 1.00)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient attention ssm integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer\n\n*From Search Query: efficient attention ssm integration*\n\n*Fabian Galetzka, J. Rose, David Schlangen, Jens Lehmann*\n\n**TL;DR:** This work proposes a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights, and shows that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency.\n\n**Abstract:** To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 21  (*Influential: 3*)\n\n#### 2. Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification\n\n*From Search Query: efficient attention ssm integration*\n\n*Jungmin Yun, Mihyeon Kim, Youngbin Kim*\n\n**TL;DR:** This work proposes integrating two strategies: token pruning and token combining, which not only improve the model's performance but also reduce its computational demands.\n\n**Abstract:** Transformer-based models have achieved dominant performance in numerous NLP tasks. Despite their remarkable successes, pre-trained transformers such as BERT suffer from a computationally expensive self-attention mechanism that interacts with all tokens, including the ones unfavorable to classification performance. To overcome these challenges, we propose integrating two strategies: token pruning and token combining. Token pruning eliminates less important tokens in the attention mechanism's key and value as they pass through the layers. Additionally, we adopt fuzzy logic to handle uncertainty and alleviate potential mispruning risks arising from an imbalanced distribution of each token's importance. Token combining, on the other hand, condenses input sequences into smaller sizes in order to further compress the model. By integrating these two approaches, we not only improve the model's performance but also reduce its computational demands. Experiments with various datasets demonstrate superior performance compared to baseline models, especially with the best improvement over the existing BERT model, achieving +5%p in accuracy and +5.6%p in F1 score. Additionally, memory cost is reduced to 0.61x, and a speedup of 1.64x is achieved.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2024\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 3. Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models\n\n*From Search Query: efficient attention ssm integration*\n\n*Qiuhao Lu, D. Dou, Thien Huu Nguyen*\n\n**TL;DR:** This work introduces an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-ef\ufb01cient way and proposes to encode domain knowledge via adapters, which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs.\n\n**Abstract:** Domain-speci\ufb01c pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-speci\ufb01c PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-speci\ufb01c knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive ar-eas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-ef\ufb01cient way. More speci\ufb01cally, we propose to encode domain knowledge via adapters , which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-speci\ufb01c adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the bene\ufb01ts of the proposed architecture and the knowledge-speci\ufb01c adapters across multiple PLMs.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 3*)\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: efficient attention ssm integration*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models\n\n*From Search Query: efficient attention ssm integration*\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: efficient attention ssm integration*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Mamba-FETrack: Frame-Event Tracking via State Space Model\n\n*From Search Query: efficient attention ssm integration*\n\n*Bo Jiang, Xiao Wang, Zhe Wu, Shuai Wang, Shiao Wang, Ju Huang*\n\n**Abstract:** RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream). Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets. However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism. This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking. Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams. Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network. The fused features will be fed into the tracking head for target object localization. Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker. Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking. The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}.\n\n**Published:** 2024-04-28\n\n\n\n#### 3. MedMamba: Vision Mamba for Medical Image Classification\n\n*From Search Query: efficient attention ssm integration*\n\n*Zhenzhang Li, Yubiao Yue*\n\n**Abstract:** Since the era of deep learning, convolutional neural networks (CNNs) and vision transformers (ViTs) have been extensively studied and widely used in medical image classification tasks. Unfortunately, CNN's limitations in modeling long-range dependencies result in poor classification performances. In contrast, ViTs are hampered by the quadratic computational complexity of their self-attention mechanism, making them difficult to deploy in real-world settings with limited computational resources. Recent studies have shown that state space models (SSMs) represented by Mamba can effectively model long-range dependencies while maintaining linear computational complexity. Inspired by it, we proposed MedMamba, the first Vision Mamba for generalized medical image classification. Concretely, we introduced a novel hybrid basic block named SS-Conv-SSM, which purely integrates the convolutional layers for extracting local features with the abilities of SSM to capture long-range dependencies, aiming to model medical images from different image modalities efficiently. By employing the grouped convolution strategy and channel-shuffle operation, MedMamba successfully provides fewer model parameters and a lower computational burden for efficient applications without sacrificing accuracy. We thoroughly evaluated MedMamba using 16 datasets containing ten imaging modalities and 411,007 images. Experimental results show that MedMamba demonstrates competitive performance on most tasks compared with the state-of-the-art methods. This work aims to explore the potential of Vision Mamba and establish a new baseline for medical image classification, thereby providing valuable insights for developing more powerful Mamba-based artificial intelligence algorithms and applications in medicine. The source codes and all pre-trained weights of MedMamba are available at https://github.com/YubiaoYue/MedMamba.\n\n**Published:** 2024-03-06\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by integrating state space models (SSMs) with attention mechanisms, here are some key findings and approaches from the provided sources:\n\n## Hybrid Architectures Combining SSMs and Attention\n\n- The paper on **E-Tamba** introduces a hybrid model that combines the strengths of Transformers and SSMs, such as Mamba. E-Tamba involves fine-tuning pre-trained Transformer and SSM models to create a hybrid architecture that leverages the memory efficiency of SSMs and the long-context capabilities of Transformers. This approach shows significant memory savings while maintaining performance in long-context tasks.\n\n- **Mamba** itself is designed to optimize efficiency and effectiveness by using selective state representations. While Mamba is primarily autoregressive and lacks bidirectional contextual relationships, integrating it with attention mechanisms, such as in the **BiMamba** module, can capture bidirectional relationships. This involves processing the input sequence in both forward and backward directions using weight-shared unidirectional Mamba modules.\n\n## Efficient Attention Mechanisms for Long Sequences\n\n- **E-Tamba** also addresses the issue of long-context performance by incorporating a global shared self-attention layer, which helps in optimizing memory efficiency. This hybrid approach allows for better handling of long sequences compared to pure SSMs, which struggle with tasks like long-context copying and retrieval.\n\n- The **SPikE-SSM** model, which integrates spiking neural networks (SNNs) with SSMs, proposes a novel approach for efficient long-sequence modeling. While not directly focused on attention, it demonstrates how dynamic and sparse models can be efficient for long sequences, which could be adapted to include attention mechanisms for improved performance.\n\n## Memory-Efficient Attention Implementations\n\n- **Mamba** and its variants, such as **E-Tamba**, offer significant memory efficiency. Mamba's use of dynamic matrices (B and C) that are dependent on the input allows for selective state propagation, reducing memory usage. E-Tamba achieves nearly 3X memory savings compared to pure Transformer models during long-context inference.\n\n- The **convolutional representation** of SSMs, as discussed in the Mamba architecture, allows for parallelized training, which can be more memory-efficient compared to traditional recurrent methods. This approach can be combined with attention mechanisms to maintain efficiency during both training and inference.\n\n## Performance Comparisons Between Different Approaches\n\n- **E-Tamba** evaluates its performance on tasks such as long-context copying and phone book retrieval, showing that it leverages the strengths of both Transformers and SSMs. It achieves better memory efficiency and comparable performance to state-of-the-art models like Llama-2 but with fewer training tokens.\n\n- The **SPikE-SSM** model is compared with existing spiking SSMs and demonstrates its effectiveness and robustness on long-range arena benchmarks and large language datasets like WikiText-103. This shows the potential of integrating SNNs with SSMs for efficient and accurate long-sequence modeling, which could be extended to include attention mechanisms.\n\nIn summary, integrating SSMs with attention mechanisms can be achieved through hybrid architectures like E-Tamba, which combines the strengths of both models. Efficient attention mechanisms can be designed using dynamic and adaptive state representations as in Mamba, and memory efficiency can be optimized through convolutional representations and selective state propagation. These approaches offer promising directions for improving autoregressive language model design.\n",
            "analysis": "Analysis Note: State Space Models and Memory Optimization\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing FastTTTLinear through state space models (SSMs) and efficient memory management:\n\na) State Space Model Integration:\n- Mamba's selective state propagation offers efficient sequence modeling\n- LOCOST demonstrates SSMs' effectiveness for long sequences\n- DenseSSM shows potential for enhanced information flow between layers\n\nb) Memory Efficiency:\n- Need for efficient compression of intermediate states\n- Importance of hardware-aware implementation\n- Potential for adaptive dimension reduction\n\nc) Architectural Innovation:\n- Opportunity to combine SSMs with test-time training\n- Need for efficient fusion of compressed states\n- Potential for hierarchical state management\n\n2. Useful References with Excerpts\n\nFrom Mamba paper:\n\"First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\nThis suggests potential for content-dependent state management.\n\nFrom LOCOST:\n\"With a computational complexity of O(L log L), this architecture can handle significantly longer sequences than state-of-the-art models [...] while saving up to 50% memory during training and up to 87% during inference.\"\n\nShows significant efficiency gains possible with SSMs.\n\nFrom DenseSSM:\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output.\"\n\nIndicates potential for improved information flow.\n\n3. Key Insights and Analysis\n\na) State Management:\n- SSMs offer linear-time sequence modeling with selective state updates\n- Content-dependent state propagation can improve efficiency\n- Need for careful balance between compression and information preservation\n\nb) Memory Optimization:\n- Hardware-aware implementation crucial for real-world performance\n- Potential for adaptive compression based on content importance\n- Opportunity for hierarchical state representation\n\nc) Integration Strategy:\n- Need to maintain test-time training capability while improving efficiency\n- Potential for combining SSMs with FastTTTLinear's existing mechanisms\n- Importance of stable training dynamics\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Techniques for efficient integration of SSMs with attention mechanisms\n2. Methods for adaptive compression in neural networks\n3. Approaches for maintaining test-time training with compressed states\n\n5. References\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[2] Le Bronnec, F., et al. (2024). LOCOST: State-Space Models for Long Document Abstractive Summarization. Conference of the European Chapter of the Association for Computational Linguistics.\n\n[3] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[4] Sieber, J., et al. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint."
        },
        {
            "ready": false,
            "query": "adaptive compression neural networks hierarchical",
            "detail": "Find papers discussing adaptive compression and hierarchical processing in neural networks, particularly focusing on:\n1. Dynamic compression techniques\n2. Hierarchical information flow\n3. Memory-efficient architectures\n4. Hardware-aware implementations",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing adaptive compression and hierarchical processing in neural networks, particularly focusing on:\n1. Dynamic compression techniques\n2. Hierarchical information flow\n3. Memory-efficient architectures\n4. Hardware-aware implementations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.98)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.98)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.24)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.24)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 3. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.22)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.22)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n#### 4. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.15)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.17)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.13)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive compression neural networks hierarchical\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. An Adaptive Empirical Bayesian Method for Sparse Deep Learning\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*Wei Deng, Xiao Zhang, F. Liang, Guang Lin*\n\n**TL;DR:** A novel adaptive empirical Bayesian method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors, which leads to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks and the state of theart compression performance on CIFAR10 with Residual Networks.\n\n**Abstract:** We propose a novel adaptive empirical Bayesian (AEB) method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors. The proposed method works by alternatively sampling from an adaptive hierarchical posterior distribution using stochastic gradient Markov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic approximation (SA). We further prove the convergence of the proposed method to the asymptotically correct distribution under mild conditions. Empirical applications of the proposed method lead to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks (CNN) and the state-of-the-art compression performance on CIFAR10 with Residual Networks. The proposed method also improves resistance to adversarial attacks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 40  (*Influential: 2*)\n\n#### 2. Adaptive Estimators Show Information Compression in Deep Neural Networks\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*Ivan Chelombiev, Conor J. Houghton, Cian O\u2019Donnell*\n\n**TL;DR:** More robust mutual information estimation techniques are developed, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions, which explore compression in networks with a range of different activation functions.\n\n**Abstract:** To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 7*)\n\n#### 3. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 23  (*Influential: 2*)\n\n#### 4. RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*Edouard Yvinec, Arnaud Dapogny, M. Cord, K\u00e9vin Bailly*\n\n**TL;DR:** Red is presented, a data-free structured, unified approach to tackle structured pruning ofDeep Neural Networks by proposing a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors.\n\n**Abstract:** Deep Neural Networks (DNNs) are ubiquitous in today's computer vision land-scape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to re-train the model. In this paper, we present RED, a data-free structured, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 0*)\n\n#### 5. GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang*\n\n**TL;DR:** This work introduces a spatio-temporal pre- training framework that seamlessly integrates with downstream baselines and enhances their performance, and introduces an adaptive mask strategy as part of the pre-training mechanism.\n\n**Abstract:** In recent years, there has been a rapid development of spatio-temporal prediction techniques in response to the increasing demands of traffic management and travel planning. While advanced end-to-end models have achieved notable success in improving predictive performance, their integration and expansion pose significant challenges. This work aims to address these challenges by introducing a spatio-temporal pre-training framework that seamlessly integrates with downstream baselines and enhances their performance. The framework is built upon two key designs: (i) We propose a spatio-temporal mask autoencoder as a pre-training model for learning spatio-temporal dependencies. The model incorporates customized parameter learners and hierarchical spatial pattern encoding networks. These modules are specifically designed to capture spatio-temporal customized representations and intra- and inter-cluster region semantic relationships, which have often been neglected in existing approaches. (ii) We introduce an adaptive mask strategy as part of the pre-training mechanism. This strategy guides the mask autoencoder in learning robust spatio-temporal representations and facilitates the modeling of different relationships, ranging from intra-cluster to inter-cluster, in an easy-to-hard training manner. Extensive experiments conducted on representative benchmarks demonstrate the effectiveness of our proposed method. We have made our model implementation publicly available at https://github.com/HKUDS/GPT-ST.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. LVAC: Learned Volumetric Attribute Compression for Point Clouds using Coordinate Based Networks\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*George Toderici, Nick Johnston, Sung Jin Hwang, Philip A. Chou, Berivan Isik*\n\n**Abstract:** We consider the attributes of a point cloud as samples of a vector-valued volumetric function at discrete positions. To compress the attributes given the positions, we compress the parameters of the volumetric function. We model the volumetric function by tiling space into blocks, and representing the function over each block by shifts of a coordinate-based, or implicit, neural network. Inputs to the network include both spatial coordinates and a latent vector per block. We represent the latent vectors using coefficients of the region-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based point cloud codec G-PCC. The coefficients, which are highly compressible, are rate-distortion optimized by back-propagation through a rate-distortion Lagrangian loss in an auto-decoder configuration. The result outperforms RAHT by 2--4 dB. This is the first work to compress volumetric functions represented by local coordinate-based neural networks. As such, we expect it to be applicable beyond point clouds, for example to compression of high-resolution neural radiance fields.\n\n**Published:** 2021-11-17\n\n\n\n#### 2. Residual Dense Network for Image Restoration\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*Yu Kong, Yulun Zhang, Yapeng Tian, Bineng Zhong, Yun Fu*\n\n**Abstract:** Convolutional neural network has recently achieved great success for image restoration (IR) and also offered hierarchical features. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in IR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.\n\n**Published:** 2018-12-25\n\n\n\n#### 3. QARV: Quantization-Aware ResNet VAE for Lossy Image Compression\n\n*From Search Query: adaptive compression neural networks hierarchical*\n\n*Zhan Ma, Yuning Huang, Fengqing Zhu, Jack Ma, Ming Lu, Zhihao Duan*\n\n**Abstract:** This paper addresses the problem of lossy image compression, a fundamental problem in image processing and information theory that is involved in many real-world applications. We start by reviewing the framework of variational autoencoders (VAEs), a powerful class of generative probabilistic models that has a deep connection to lossy compression. Based on VAEs, we develop a novel scheme for lossy image compression, which we name quantization-aware ResNet VAE (QARV). Our method incorporates a hierarchical VAE architecture integrated with test-time quantization and quantization-aware training, without which efficient entropy coding would not be possible. In addition, we design the neural network architecture of QARV specifically for fast decoding and propose an adaptive normalization operation for variable-rate compression. Extensive experiments are conducted, and results show that QARV achieves variable-rate compression, high-speed decoding, and a better rate-distortion performance than existing baseline methods. The code of our method is publicly accessible at https://github.com/duanzhiihao/lossy-vae\n\n**Published:** 2023-02-16\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design with a focus on adaptive compression, hierarchical processing, memory-efficient architectures, and hardware-aware implementations, here are some key insights and relevant information from the provided sources and additional context:\n\n## Dynamic Compression Techniques\n- The paper on \"Developmental Plasticity-inspired Adaptive Pruning for Deep Neural Networks and Spiking Neural Networks\" introduces a dynamic pruning method inspired by brain developmental plasticity. This method, known as DPAP, dynamically prunes inactive synapses and neurons based on their activity levels, which can be adapted to reduce the computational and memory requirements of neural networks. This approach is particularly relevant for achieving adaptive compression and preventing overfitting and underfitting.\n\n## Hierarchical Information Flow\n- The concept of hierarchical processing is highlighted in the analysis note through the example of CHELA, which integrates short-long convolutions with linear attention in a divide-and-conquer manner. This approach leverages tiling and hierarchy to improve memory access patterns and efficiency. Similar hierarchical structures can be applied to autoregressive models to enhance their processing efficiency[Analysis Note].\n\n## Memory-Efficient Architectures\n- The LOCOST architecture is mentioned for its ability to handle long sequences efficiently while reducing memory usage significantly. It achieves this through a computational complexity of \\(O(L \\log L)\\), which is beneficial for memory-efficient designs. This approach can be adapted to autoregressive models to optimize memory usage during training and inference[Analysis Note].\n\n## Hardware-Aware Implementations\n- Hardware-aware implementations are crucial for real-world performance gains. The analysis note emphasizes the importance of tiling-based approaches to reduce memory bandwidth requirements. For autoregressive models, integrating hardware-aware optimization strategies, such as those used in CHELA and LOCOST, can significantly improve performance and efficiency[Analysis Note].\n\n## Additional Insights\n\n### Adaptive Compression\n- Adaptive compression techniques can be integrated into autoregressive models by selectively compressing less important states or weights. The DPAP method, for instance, can be adapted to prune less active components of the model, thereby reducing the overall size and computational cost without significant loss in accuracy.\n\n### Hierarchical Processing\n- Hierarchical processing can be achieved through architectures like DenseMamba, which uses dense hidden connections to improve information flow between layers. This can be particularly beneficial for autoregressive models that need to process long sequences efficiently[Analysis Note].\n\n### State Management\n- Selective state propagation, as seen in Mamba and Mamba-2, can help in managing the state of autoregressive models efficiently. This approach ensures that only relevant states are propagated, reducing the memory and computational overhead[Analysis Note].\n\nBy integrating these concepts\u2014dynamic compression techniques, hierarchical information flow, memory-efficient architectures, and hardware-aware implementations\u2014researchers can develop more efficient and scalable autoregressive language models that outperform existing state-of-the-art models.\n",
            "analysis": "Analysis Note: Integration of SSMs, Attention and Memory Optimization\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing FastTTTLinear through the integration of state space models (SSMs), efficient attention mechanisms, and memory optimization techniques:\n\na) Hybrid Architectures:\n- CHELA demonstrates successful integration of linear attention with short-long convolutions\n- Transformers and SSMs can be unified through structured state space duality (SSD)\n- Mamba shows effectiveness of selective state propagation\n\nb) Memory Efficiency:\n- Hardware-aware implementations crucial for real performance gains\n- Tiling and hierarchy can improve memory access patterns\n- Adaptive compression techniques show promise\n\nc) Architectural Innovation:\n- DenseSSM's approach to information flow between layers\n- LOCOST's efficient handling of long sequences\n- Mamba-2's refined selective SSM design\n\n2. Useful References with Excerpts\n\nFrom CHELA paper:\n\"We leverage the thought of tiling and hierarchy to propose CHELA [...] which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.\"\n\nThis suggests potential for hierarchical processing with efficient memory access.\n\nFrom Transformers are SSMs:\n\"Our state space duality (SSD) framework allows us to design a new architecture whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster.\"\n\nShows theoretical connection between attention and SSMs.\n\nFrom LOCOST:\n\"With a computational complexity of O(L log L), this architecture can handle significantly longer sequences than state-of-the-art models [...] while saving up to 50% memory during training and up to 87% during inference.\"\n\nDemonstrates significant efficiency gains possible.\n\n3. Key Insights and Analysis\n\na) Memory Hierarchy Optimization:\n- Tiling-based approaches can significantly reduce memory bandwidth requirements\n- Hardware-aware implementation crucial for real-world performance\n- Need to balance compression ratio with computational overhead\n\nb) Hybrid Architecture Design:\n- SSMs and attention mechanisms can be theoretically unified\n- Short-long convolutions offer stable alternative to pure SSMs\n- Hierarchical processing can improve efficiency\n\nc) State Management:\n- Selective state propagation shows promise for efficiency\n- Dense connections can improve information flow\n- Need for adaptive compression based on content importance\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in adaptive compression techniques for neural networks\n2. Methods for efficient integration of hierarchical processing\n3. Hardware-aware optimization strategies for memory access\n\n5. References\n\n[1] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\". International Conference on Machine Learning.\n\n[2] Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\". arXiv preprint.\n\n[3] Le Bronnec, F., et al. (2024). \"LOCOST: State-Space Models for Long Document Abstractive Summarization\". Conference of the European Chapter of the Association for Computational Linguistics.\n\n[4] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\". arXiv preprint.\n\n[5] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"test-time training hierarchical implementation\", \"hardware-aware memory optimization\", \"compressed state management\"",
            "detail": "Search for papers and implementations that:\n\n1. Detail specific implementations of test-time training in hierarchical models\n2. Present hardware-aware memory optimization techniques\n3. Describe state management strategies in compressed models\n4. Analyze the impact of compression on model quality\n5. Discuss parallel processing in hierarchical architectures\n\nFocus on:\n- Implementation details of TTT preservation\n- Memory access optimization techniques\n- State compression strategies\n- Hardware-specific optimizations\n- Performance impact measurements",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations that:\n\n1. Detail specific implementations of test-time training in hierarchical models\n2. Present hardware-aware memory optimization techniques\n3. Describe state management strategies in compressed models\n4. Analyze the impact of compression on model quality\n5. Discuss parallel processing in hierarchical architectures\n\nFocus on:\n- Implementation details of TTT preservation\n- Memory access optimization techniques\n- State compression strategies\n- Hardware-specific optimizations\n- Performance impact measurements\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.06)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.06)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.04)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 22/51 (Score: 0.06)*\n\n```\nThis trend is highlighted in Figure 19 (in Appendix). We leave further investigation of this trend to future work. The left panel of Figure 2 is a zoomed-in view of Figure 13. For clarity, we leave TF pretrain out of Figure 2 and only show TF finetune (labeled as Transformer) since it is the stronger baseline. Figure 14 reproduces the right panel of Figure 2, now with TTT-MLP and additional discussion. ![](https://cdn.mathpix.com/cropped/2024_09_17_1d28964b3a79d5da6317g-16.jpg?height=478&width=747&top_left_y=1886&top_left_x=342)\n\nFigure 14. Reproduction of the right panel of Figure 2, now with TTT-MLP. Comparing to TTT-Linear, TTT-MLP performs slightly worse at short context but better at long context. This observation matches our expectation that the MLP as hidden state is more expressive than the linear model. Again, all methods have matched training FLOPs as Mamba 1.4B. For TTT-Linear and TTT-MLP, this protocol implies matched inference FLOPs. Transformer (TF finetune) has $2.8 \\times$ the inference FLOPs, giving it an advantage as our baseline. ![](https://cdn.mathpix.com/cropped/2024_09_17_1d28964b3a79d5da6317g-17.jpg?height=408&width=1409&top_left_y=292&top_left_x=357)\n\nFigure 15. Benchmark on an NVIDIA A100 GPU with 80G HBM and PCIe connections. Left: Same as Figure 3, but now with TTT-MLP. Note that our Transformer baseline is significantly faster then that in the Mamba paper, because we use vLLM [46], a state-of-the-art serving system, instead of the HuggingFace Transformer [77]. Right: For generate (decode), TTT-Linear and Mamba have almost the same latency, which is significantly smaller than that of Transformer and TTT-MLP. ### 3.3 Wall-clock time\n\nLLM training and inference can be decomposed into forward, backward, and generate. Prompt processing during inference, also known as prefill, is the same operation as forward during training, except that the intermediate activations do not need to be stored for backward. Since both forward (during training and inference) and backward can be parallelized, we use the dual form. Generating new tokens, also known as decode, is inherently sequential, so we use the primal form. Due to resource constraints, our experiments are written in JAX and run on TPUs. On a v5e-256 TPU pod, the Transformer baseline takes 0.30 s per iteration of training at context 2 k , while TTT-Linear takes 0.27 s per iteration, already $10 \\%$ faster without any systems optimization. However, Mamba (implemented in PyTorch, Triton, and CUDA) can only run on GPUs, so for fair comparison, we also rewrite our method with preliminary systems optimization to run on GPUs. Specifically, we write a GPU kernel for forward in ThunderKittens [66]. Historically, RNNs have been inefficient during forward and backward due to poor use of parallelism and matmuls. Our goal with the forward kernel is to demonstrate the effectiveness of mini-batch TTT and the dual form for these problems. A kernel for backward should have the same properties in efficiency as forward, but requires more complex logic for manual differentiation, therefore is left for future work. The left panel of Figure 15 shows the latency for batch size 16 of our forward kernel. All models are 1.3B (1.4B for Mamba). Time per token grows linearly for Transformer as context length increases, but stays roughly constant for the other methods. ${ }^{14}$ Note that our Transformer baseline is significantly faster that in the Mamba paper, because we use vLLM [46], a state-of-the-art serving system, instead of the HuggingFace Transformer [77]. In addition, we write another GPU kernel for generate in Triton [72], and benchmark its speed in the right panel of Figure 15 for batch size 512. Another popular metric for wall-clock time is throughput, which takes into account the potential benefit of being able to use a larger batch size. ${ }^{15}$ For completeness, we report the throughput for forward and generate in Figure 20 (in Appendix). All the observations and ordering between methods above still hold for throughput. [^10]\n## 4 Related Work\n\n### 4.1 Modern RNNs\n\nMamba is one of the many Structured State-Space Models [27, 21, 57, 18]. The hidden state in these models is a vector, similar to in LSTMs. For TTT-Linear or TTT-MLP, the hidden state is a matrix or two matrices, therefore larger. In Figure 14, we find that TTT layers can take advantage of their larger hidden states to compress more information in long context, where TTT-MLP outperforms TTT-Linear, which in turn outperforms Mamba. Similar to TTT-Linear, RWKV [55, 56], xLSTM [5], and Gated Linear Attention (GLA) [79] also have matrix hidden states, which are inherited from linear attention [41]. Modern RNNs such as GLA use chunk-wise parallelism to improve hardware efficiency, so tokens inside a chunk can be processed with matmuls instead of a cumsum. However, chunk-wise parallelism does not change the expressiveness of the model, since all temporal dependencies are still equivalent to a cumsum. In contrast, mini-batch TTT allows more complex temporal dependencies across mini-batches. Each hidden state $W_{t}$ depends on previous $W s$ within its mini-batch still through a cumsum, but depends on Ws in previous mini-batches also through the gradient operator. As illustrated Figure 8, mini-batch TTT enables a trade-off between expressiveness and hardware efficiency, since a smaller batch size $b$ leads to better perplexity at the cost of higher latency. This trade-off is a unique and important feature of TTT. As shown in Table 1 , the intermediate batch size $b=16$ significantly outperforms $b=T$ which is fully cumsum. ### 4.2 Learning at Test Time\n\nThe idea of learning at test time has a long history in machine learning. One of the earliest versions of this idea is called local learning (Bottou and Vapnik [10]): For each test input, train on its neighbors before making a prediction. This procedure has been effectively applied to models ranging from SVMs [81] to modern LLMs [29]. Another early version of learning at test time is called transductive learning [22]. The principle of transduction, as stated by Vladimir Vapnik [74], is to \"... get the answer that you really need, but not a more general one.\" Practical implementations of transductive learning use test data to add constraints to the margin of SVMs [39,17]. However, transductive learning usually needs multiple test instances to be empirically effective, unlike many instantiations of test-time training, which only need a test single instance (image, video, or natural language sequence) at a time. In computer vision, the idea of learning at test time has been applied for decades to applications such as face detection [38], object detection [53], image super-resolution [65], and 3D reconstruction [50]. More recently, the same idea has also been applied to natural language processing, where it is called dynamic evaluation [44, 45]. The basic approach is to directly finetune a language model on the test sequence, which often comes in the form of a prompt. Next, we discuss two relevant lines of work in detail: test-time training and fast weights. ### 4.2.1 Test-Time Training\n\nThe core idea of Test-Time Training (TTT) is that each test instance defines its own learning problem, where this test instance alone is the target of generalization [69]. Concretely, for each test instance $x$, the conventional practice is to predict $f(x)$, using a predictor $f$ that is optimized for all training instances on average. TTT first formulates a learning problem defined by $x$, then trains a model $f_{x}$ on $x$ (often with $f$ as initialization), and predicts $f_{x}(x)$. Since the test instance comes without its label, the learning problem can only be formulated with a self-supervised task. Prior work has shown that TTT with reconstruction significantly improves performance especially on outliers [23]. Improvements become even more pronounced when testing on video frames that arrive in a stream and TTT is autoregressive [76], as $f_{t}$ is trained on past frames $x_{1}, \\ldots, x_{t}$. The autoregressive connection makes [76] most relevant to our paper. Conceptually, the biggest difference between our paper and prior work is that our reconstruction task is learned in an outer loop, instead of handcrafted with human priors. Follow-up work to TTT has explored applications such as robot manipulation [28] and locomotion [68], among others, that often require different designs for the self-supervised task. ### 4.2.2 Fast Weights\n\nThe general idea of fast weights is to update the parameters of a \"fast\" model on only the most relevant data, as opposed to the conventional practice of updating a \"slow\" model on all data [71]. This idea has existed since the 1980s [32]. The most relevant data can be the test instance itself, therefore TTT can be viewed as a special case of fast weights. Prior work in fast weights usually avoids forming an explicit learning problem that optimizes some objective on data. For example, the update rule of Hebbian learning and Hopfield networks [35] simply adds $x x^{T}$ (or some variant thereof) [4] to the fast weights given each input $x$. In contrast, TTT embraces the idea of formulating an explicit learning problem, where the test instance is the target of generalization. Our update rule is also an explicit step of optimization. The idea of fast weight programmers (FWPs) is to update the fast weights with a \"slow\" model [62]. Our inner-loop weights $W$ can be viewed as \"fast\" and outer-loop weights $\\theta$ as \"slow\". Therefore, networks containing TTT layers can be viewed as a special case of FWPs [43], similar to how TTT can be viewed as a special case of fast weights. The FWP with the Hebbian update rule above is equivalent to linear attention [60], therefore also to naive TTT-Linear with batch gradient descent. The definition of FWPs is very broad. In fact, all networks with some gating mechanism, such as Transformers with SwiGLU blocks [63], can also be viewed as a special case of FWPs ${ }^{16}$. Recent work has been experimenting with FWPs for language modeling: Irie et al. [37] design \"fast\" networks with weights produced as output of a \"slow\" networks. Clark et al. [16] give a Transformer a final layer of fast weights, whose initialization is trained as slow weights. Our contribution relative to existing work on FWPs, again, is formulating an explicit learning problem for the update, which enables us to borrow tools from learning such as mini-batch and LN. ### 4.3 Learning to Learn\n\nFor decades, researchers have been arguing that learning to learn, also known as meta-learning or bi-level optimization, should be a critical component of intelligence [61, 6, 70, 47]. In prior work such as [2], [20] and [52], the inner loop learns from an entire dataset at a time instead of a sequence, so the outer loop needs a collection of datasets or tasks. In short, the outer loop is \"one level above\" regular training. Since it is hard to collect millions of datasets, this outer loop is hard to scale. In contrast, for TTT, each sequence itself is a dataset and defines its own generalization problem. The inner loop is \"one level below\" regular training, so our outer loop is only another solution to the canonical problem of supervised learning, instead of a new problem setting like generalization across datasets. As illustrated in Table 2, our outer loop is \"at the same level\" as regular training. This makes our outer loop easier to scale. [^11]|  | Inner loop | Outer loop | Subsection |\n| :--- | :--- | :--- | :--- |\n| Piece of data | Token $x_{t}$ | Sequence $x_{1}, \\ldots, x_{T}$ |  |\n| Training set | Sequence $x_{1}, \\ldots, x_{T}$ | Dataset of sequences, e.g., Books | 2.2 .2 .2 |\n| Objective | Reconstruction (loss $\\ell$ ) | Next-token prediction |  |\n| Parameters | $W$ (weights of $f$ ) | $\\theta_{\\text {rest }}($ rest of the network) |  |\n|  |  | $\\theta_{K}, \\theta_{Q}, \\theta_{V}$ (reconstruction views) | 2.3 |\n|  |  | $\\theta_{\\text {init }}$ and $\\theta_{\\mathrm{lr}}$ | 2.7 |\n\nTable 2. In summary, our paper reformulates supervised learning as learning to learn, with two nested loops.\n```\n\n##### *Relevant Chunk: No. 33/51 (Score: 0.02)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. [22] A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In In Uncertainty in Artificial Intelligence, pages 148-155. Morgan Kaufmann, 1998. [23] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 2022. [24] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling, 2020. [25] Xinyang Geng. EasyLM: A Simple And Scalable Training Framework for Large Language Models. https://github.com/young-geng/EasyLM, mar 2023. https://github.com/ young-geng/EasyLM. [26] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\u00e0, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. arXiv preprint arXiv:2007.04309, 2020. [29] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models.\n```\n\n#### 3. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.02)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.02)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n#### 4. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.02)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.02)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"test-time training hierarchical implementation\", \"hardware-aware memory optimization\", \"compressed state management\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification\n\n*From Search Query: \"test-time training hierarchical implementation\"*\n\n*Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi*\n\n**TL;DR:** A novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse- grained predictions to significantly reduce mistake severity.\n\n**Abstract:** We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces avg. mistake severity while improving top-1 accuracy on the iNaturalist-19 and tieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in top-1 accuracy while significantly decreasing the severity of mistakes as training data decreases for the fine-grained classes. The simplicity and post-hoc nature of HiE renders it practical to be used with any off-the-shelf trained model to improve its predictions further.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction\n\n*From Search Query: \"test-time training hierarchical implementation\"*\n\n*Zeshuai Deng, Zhuokun Chen, Shuaicheng Niu, Thomas H. Li, Bohan Zhuang, Mingkui Tan*\n\n**TL;DR:** An efficient test-time adaptation framework for SR is presented, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types, and designs a second-order degradation scheme to construct paired data based on the degradation type of the test image, predicted by a pre-trained degradation classifier.\n\n**Abstract:** Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, e.g., Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (e.g., blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 3. Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality\n\n*From Search Query: \"test-time training hierarchical implementation\"*\n\n*Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu*\n\n**TL;DR:** Hierarchical Decomposition (HiDe-)Prompt is proposed, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy.\n\n**Abstract:** Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at \\url{https://github.com/thu-ml/HiDe-Prompt}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 9*)\n\n#### 4. On-Device Training Under 256KB Memory\n\n*From Search Query: \"hardware-aware memory optimization\"*\n\n*Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, Song Han*\n\n**TL;DR:** This framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW.\n\n**Abstract:** On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource does not allow full back-propagation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/0pUFZYdoMY8.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 141  (*Influential: 17*)\n\n#### 5. Hardware-Aware Compression with Random Operation Access Specific Tile (ROAST) Hashing\n\n*From Search Query: \"hardware-aware memory optimization\"*\n\n*Aditya Desai, K. Zhou, Anshumali Shrivastava*\n\n**TL;DR:** With ROAST, the authors can efficiently train and deploy the model using a much smaller memory footprint in text and image classification tasks, and introduce global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest.\n\n**Abstract:** Advancements in deep learning are often associated with increasing model sizes. Training and deploying large models require sophisticated hardware and incur significantly higher costs. Thus, model compression is a widely explored approach to solving the problem. However, SOTA techniques fall short in one or more desirable aspects of compression - for instance, pruning does not reduce memory for training, quantization can only provide up to 32 \u00d7 compression, Hashed-Net is cache-inefficient, etc. This paper proposes a model-agnostic, cache-friendly, and hardware-aware model compression approach: Random Operation Access Specific Tile (ROAST) hashing. ROAST collapses the parameters by club-bing them through a lightweight mapping. While clubbing these parameters, ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern. ROAST is up to \u223c 25 \u00d7 faster to train and \u223c 50 \u00d7 faster to infer than the popular parameter sharing method HashedNet. Additionally, ROAST introduces global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest. With ROAST, we can efficiently train and deploy the model using a much smaller memory footprint ( \u223c 10 \u2212 100 \u00d7 lesser) in text and image classification tasks. ROAST-MM kernel implementation is open-source 1\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 6. Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\n\n*From Search Query: \"compressed state management\"*\n\n*Jeonghoon Kim, J. H. Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, S. Kwon, Dongsoo Lee*\n\n**TL;DR:** Parameter-Efficient and Quantization-aware Adaptation (PEQA) is presented - a simple yet effective method that combines the advantages of PEFT with quantized LLMs and significantly reduces the memory overhead associated with the optimizer state.\n\n**Abstract:** Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 66  (*Influential: 2*)\n\n#### 7. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\n\n*From Search Query: \"compressed state management\"*\n\n*Mohammad Zalbagi Darestani, Jiayu Liu, Reinhard Heckel*\n\n**TL;DR:** This work proposes a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference that essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Abstract:** Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution $P$ and training on another distribution $Q$, and evaluating both models on $Q$. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 8. Whittle Index with Multiple Actions and State Constraint for Inventory Management\n\n*From Search Query: \"compressed state management\"*\n\n*Chuheng Zhang, Xiangsen Wang, Wei Jiang, Xianliang Yang, Siwei Wang, Lei Song, Jiang Bian*\n\n**Abstract:** ,\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Natural Language-conditioned Reinforcement Learning with Inside-out Task\n  Language Development and Translation\n\n*From Search Query: \"test-time training hierarchical implementation\"*\n\n*Jing-Cheng Pang, Xin-Yu Yang, Si-Hang Yang, Yang Yu*\n\n**Abstract:** Natural Language-conditioned reinforcement learning (RL) enables the agents\nto follow human instructions. Previous approaches generally implemented\nlanguage-conditioned RL by providing human instructions in natural language\n(NL) and training a following policy. In this outside-in approach, the policy\nneeds to comprehend the NL and manage the task simultaneously. However, the\nunbounded NL examples often bring much extra complexity for solving concrete RL\ntasks, which can distract policy learning from completing the task. To ease the\nlearning burden of the policy, we investigate an inside-out scheme for natural\nlanguage-conditioned RL by developing a task language (TL) that is task-related\nand unique. The TL is used in RL to achieve highly efficient and effective\npolicy training. Besides, a translator is trained to translate NL into TL. We\nimplement this scheme as TALAR (TAsk Language with predicAte Representation)\nthat learns multiple predicates to model object relationships as the TL.\nExperiments indicate that TALAR not only better comprehends NL instructions but\nalso leads to a better instruction-following policy that improves 13.4% success\nrate and adapts to unseen expressions of NL instruction. The TL can also be an\neffective task abstraction, naturally compatible with hierarchical RL.\n\n**Published:** 2023-02-18T15:49:09Z  (*Updated: 2023-02-18T15:49:09Z*)\n\n\n\n#### 2. Tree Transformer: Integrating Tree Structures into Self-Attention\n\n*From Search Query: \"test-time training hierarchical implementation\"*\n\n*Yau-Shian Wang, Hung-Yi Lee, Yun-Nung Chen*\n\n**Abstract:** Pre-training Transformer from large-scale raw texts and fine-tuning on the\ndesired task have achieved state-of-the-art results on diverse NLP tasks.\nHowever, it is unclear what the learned attention captures. The attention\ncomputed by attention heads seems not to match human intuitions about\nhierarchical structures. This paper proposes Tree Transformer, which adds an\nextra constraint to attention heads of the bidirectional Transformer encoder in\norder to encourage the attention heads to follow tree structures. The tree\nstructures can be automatically induced from raw texts by our proposed\n\"Constituent Attention\" module, which is simply implemented by self-attention\nbetween two adjacent words. With the same training procedure identical to BERT,\nthe experiments demonstrate the effectiveness of Tree Transformer in terms of\ninducing tree structures, better language modeling, and further learning more\nexplainable attention scores.\n\n**Published:** 2019-09-14T17:49:37Z  (*Updated: 2019-11-01T20:38:07Z*)\n\n\n\n#### 3. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"hardware-aware memory optimization\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 4. ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training\n  Quantization Framework for W8A8 Transformers\n\n*From Search Query: \"hardware-aware memory optimization\"*\n\n*Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He*\n\n**Abstract:** Quantization techniques are pivotal in reducing the memory and computational\ndemands of deep neural network inference. Existing solutions, such as\nZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook\ncrucial memory-bounded operators and the complexities of per-token\nquantization. Addressing these gaps, we present a novel, fully\nhardware-enhanced robust optimized post-training W8A8 quantization framework,\nZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and\ncompute-intensive operators, aiming for optimal hardware performance.\nAdditionally, it offers flexibility by allowing specific INT8 modules to switch\nto FP16/BF16 mode, enhancing accuracy.\n\n**Published:** 2023-10-26T18:34:41Z  (*Updated: 2023-10-26T18:34:41Z*)\n\n\n\n#### 5. Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes\n\n*From Search Query: \"compressed state management\"*\n\n*Lucio Dery, Steven Kolawole, Jean-Fran\u00e7ois Kagy, Virginia Smith, Graham Neubig, Ameet Talwalkar*\n\n**Abstract:** Given the generational gap in available hardware between lay practitioners\nand the most endowed institutions, LLMs are becoming increasingly inaccessible\nas they grow in size. Whilst many approaches have been proposed to compress\nLLMs to make their resource consumption manageable, these methods themselves\ntend to be resource intensive, putting them out of the reach of the very user\ngroups they target. In this work, we explore the problem of structured pruning\nof LLMs using only forward passes. We seek to empower practitioners to prune\nmodels so large that their available hardware has just enough memory to run\ninference. We develop Bonsai, a gradient-free, perturbative pruning method\ncapable of delivering small, fast, and accurate pruned models.\n  We observe that Bonsai outputs pruned models that (i) outperform those\ngenerated by more expensive gradient-based structured pruning methods, and (ii)\nare twice as fast (with comparable accuracy) as those generated by\nsemi-structured pruning methods requiring comparable resources as Bonsai. We\nalso leverage Bonsai to produce a new sub-2B model using a single A6000 that\nyields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM\nleaderboard.\n\n**Published:** 2024-02-08T04:48:26Z  (*Updated: 2024-02-09T19:53:56Z*)\n\n\n\n#### 6. Compressing Cross-Lingual Multi-Task Models at Qualtrics\n\n*From Search Query: \"compressed state management\"*\n\n*Daniel Campos, Daniel Perry, Samir Joshi, Yashmeet Gambhir, Wei Du, Zhengzheng Xing, Aaron Colak*\n\n**Abstract:** Experience management is an emerging business area where organizations focus\non understanding the feedback of customers and employees in order to improve\ntheir end-to-end experiences. This results in a unique set of machine learning\nproblems to help understand how people feel, discover issues they care about,\nand find which actions need to be taken on data that are different in content\nand distribution from traditional NLP domains. In this paper, we present a case\nstudy of building text analysis applications that perform multiple\nclassification tasks efficiently in 12 languages in the nascent business area\nof experience management. In order to scale up modern ML methods on experience\ndata, we leverage cross lingual and multi-task modeling techniques to\nconsolidate our models into a single deployment to avoid overhead. We also make\nuse of model compression and model distillation to reduce overall inference\nlatency and hardware cost to the level acceptable for business needs while\nmaintaining model prediction quality. Our findings show that multi-task\nmodeling improves task performance for a subset of experience management tasks\nin both XLM-R and mBert architectures. Among the compressed architectures we\nexplored, we found that MiniLM achieved the best compression/performance\ntradeoff. Our case study demonstrates a speedup of up to 15.61x with 2.60%\naverage task degradation (or 3.29x speedup with 1.71% degradation) and\nestimated savings of 44% over using the original full-size model. These results\ndemonstrate a successful scaling up of text classification for the challenging\nnew area of ML for experience management.\n\n**Published:** 2022-11-29T04:46:27Z  (*Updated: 2022-11-29T04:46:27Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Focal Modulation Networks\n\n*From Search Query: \"test-time training hierarchical implementation\"*\n\n*Jianfeng Gao, Lu Yuan, Xiyang Dai, Chunyuan Li, Jianwei Yang*\n\n**Abstract:** We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3. Code and checkpoints are available at https://github.com/microsoft/FocalNet.\n\n**Published:** 2022-03-22\n\n\n\n#### 2. SaLinA: Sequential Learning of Agents\n\n*From Search Query: \"test-time training hierarchical implementation\"*\n\n*Daniel H. Thompson, Pierre-Alexandre Kamienny, Jean-Baptiste Gaya, Song Duong, Alfredo De la Fuente, Ludovic Denoyer*\n\n**Abstract:** SaLinA is a simple library that makes implementing complex sequential learning models easy, including reinforcement learning algorithms. It is built as an extension of PyTorch: algorithms coded with \\SALINA{} can be understood in few minutes by PyTorch users and modified easily. Moreover, SaLinA naturally works with multiple CPUs and GPUs at train and test time, thus being a good fit for the large-scale training use cases. In comparison to existing RL libraries, SaLinA has a very low adoption cost and capture a large variety of settings (model-based RL, batch RL, hierarchical RL, multi-agent RL, etc.). But SaLinA does not only target RL practitioners, it aims at providing sequential learning capabilities to any deep learning programmer.\n\n**Published:** 2021-10-15\n\n\n\n#### 3. Pareto-Optimal Quantized ResNet Is Mostly 4-bit\n\n*From Search Query: \"hardware-aware memory optimization\"*\n\n*Lukasz Lew, Chas Leichner, Oleg Rybakov, Jonathan Malmaud, Shivani Agrawal, Lisa Wang, Amirali Abdolrashidi*\n\n**Abstract:** Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher accuracy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.\n\n**Published:** 2021-05-07\n\n\n\n#### 4. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\n\n*From Search Query: \"hardware-aware memory optimization\"*\n\n*Song Han, Xingyu Dang, Guangxuan Xiao, Wei-Chen Wang, Wei-Ming Chen, Chuang Gan, Shang Yang, Haotian Tang, Jiaming Tang, Ji Lin*\n\n**Abstract:** Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.\n\n**Published:** 2023-06-01\n\n\n\n#### 5. Chimp: Efficient Lossless Floating Point Compression for Time Series Databases\n\n*From Search Query: \"compressed state management\"*\n\n*Yannis Kotidis, Katia Papakonstantinopoulou, Panagiotis Liakos*\n\n**Abstract:** Applications in diverse domains such as astronomy, economics\r\nand industrial monitoring, increasingly press the need for analyz\u0002ing massive collections of time series data. The sheer size of the\r\nlatter hinders our ability to efficiently store them and also yields\r\nsignificant storage costs. Applying general purpose compression al\u0002gorithms would effectively reduce the size of the data, at the expense\r\nof introducing significant computational overhead. Time Series\r\nManagement Systems that have emerged to address the challenge\r\nof handling this overwhelming amount of information, cannot suf\u0002fer the ingestion rate restrictions that such compression algorithms\r\nwould cause. Data points are usually encoded using faster, stream\u0002ing compression approaches. However, the techniques that contem\u0002porary systems use do not fully utilize the compression potential\r\nof time series data, with implications in both storage requirements\r\nand access times. In this work, we propose a novel streaming com\u0002pression algorithm, suitable for floating point time series data. We\r\nempirically establish properties exhibited by a diverse set of time\r\nseries and harness these features in our proposed encodings. Our\r\nexperimental evaluation demonstrates that our approach readily\r\noutperforms competing techniques, attaining compression ratios\r\nthat are competitive with slower general purpose algorithms, and\r\non average around 50% of the space required by state-of-the-art\r\nstreaming approaches. Moreover, our algorithm outperforms all\r\nearlier techniques with regards to both compression and access time,\r\noffering a significantly improved trade-off between space and speed.\r\nThe aforementioned benefits of our approach \u2013in terms of all space\r\nrequirements, compression time and read access\u2013 significantly im\u0002prove the efficiency in which we can store and analyze time series\r\ndata.\n\n**Proceeding:** proceedings-of-the-vldb-endowment-2022-7\n\n**Published:** 2022-07-01\n\n\n\n#### 6. VidToMe: Video Token Merging for Zero-Shot Video Editing\n\n*From Search Query: \"compressed state management\"*\n\n*Ming-Hsuan Yang, Xiaokang Yang, Chao Ma, Xirui Li*\n\n**Abstract:** Diffusion models have made significant advances in generating high-quality images, but their application to video generation has remained challenging due to the complexity of temporal motion. Zero-shot video editing offers a solution by utilizing pre-trained image diffusion models to translate source videos into new ones. Nevertheless, existing methods struggle to maintain strict temporal consistency and efficient memory consumption. In this work, we propose a novel approach to enhance temporal consistency in generated videos by merging self-attention tokens across frames. By aligning and compressing temporally redundant tokens across frames, our method improves temporal coherence and reduces memory consumption in self-attention computations. The merging strategy matches and aligns tokens according to the temporal correspondence between frames, facilitating natural temporal consistency in generated video frames. To manage the complexity of video processing, we divide videos into chunks and develop intra-chunk local token merging and inter-chunk global token merging, ensuring both short-term video continuity and long-term content consistency. Our video editing approach seamlessly extends the advancements in image editing to video editing, rendering favorable results in temporal consistency over state-of-the-art methods.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2023-12-17\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on test-time training, hardware-aware memory optimization, and compressed state management, here are some key findings and references that align with your requirements:\n\n## Test-Time Training in Hierarchical Models\n\n- The concept of **Test-Time Training (TTT)** layers, as discussed in the YouTube video and associated paper, introduces a novel approach to sequence modeling where the hidden state of the model is updated through self-supervised learning during the test time. This method includes two instantiations: TTT linear and TTT MLP. These layers are designed to reduce perplexity by conditioning on more tokens, which is crucial for hierarchical models.\n\n## Hardware-Aware Memory Optimization Techniques\n\n- **Dynamic Memory Compression (DMC)**: This technique, as mentioned in the analysis summary, achieves significant memory optimization without degrading performance. DMC Transformers can increase throughput by up to 7x during auto-regressive inference while preserving the original downstream performance with up to 4x cache compression. This is a hardware-aware optimization that can be integrated into hierarchical models[Analysis Summary].\n\n- **FlashDecoding++**: This method introduces hardware-aware optimization techniques such as asynchronized softmax with a unified max value, which avoids synchronization overhead and optimizes memory access patterns. This is particularly important for efficient inference on GPUs[Analysis Summary].\n\n## State Management Strategies in Compressed Models\n\n- **Temporal Latent Bottleneck**: This approach divides computation into two streams: a fast stream and a slow, recurrent stream that learns a specialized and compressed representation. This multi-scale processing framework is essential for managing states in compressed models and can be integrated with hierarchical processing[Analysis Summary].\n\n- **H-Transformer-1D**: This method provides an efficient hierarchical approach to compute attention with linear run time and memory complexity. It demonstrates how hierarchical attention can be implemented efficiently, which is crucial for state management in compressed models[Analysis Summary].\n\n## Impact of Compression on Model Quality\n\n- The **Dynamic Memory Compression** study shows that significant memory compression (up to 4x) can be achieved without degrading the original downstream performance. This indicates that careful compression strategies can maintain model quality while optimizing memory usage[Analysis Summary].\n\n## Parallel Processing in Hierarchical Architectures\n\n- **TTT Layers**: The paper on TTT layers discusses practical innovations to enhance the efficiency of these layers, including the use of mini-batches and a duo-form for operations. These innovations significantly improve training speed on modern hardware, making TTT linear a practical component for large language models. This approach also highlights the importance of parallel processing in hierarchical architectures.\n\n## Implementation Details and Hardware-Specific Optimizations\n\n- **FlashAttention**: This method, while not specifically mentioned in the provided sources, is relevant from the references. It focuses on IO-awareness and optimizes memory access patterns, which is crucial for hardware-specific optimizations. Similar techniques can be applied to hierarchical models to ensure efficient memory access and parallel processing[Reference 1 in Analysis Summary].\n\nIn summary, the key insights and references point to the following:\n\n- **Test-Time Training**: TTT layers can be integrated into hierarchical models to update the hidden state during test time, improving performance and reducing perplexity.\n- **Memory Optimization**: Techniques like Dynamic Memory Compression and FlashDecoding++ provide hardware-aware optimizations that can significantly improve memory efficiency without compromising model quality[Analysis Summary].\n- **State Management**: Approaches such as the Temporal Latent Bottleneck and H-Transformer-1D offer efficient state management strategies in compressed models[Analysis Summary].\n- **Parallel Processing**: Innovations in TTT layers and methods like FlashDecoding++ highlight the importance of parallel processing and efficient memory access in hierarchical architectures[Analysis Summary].\n\nThese findings should help researchers in designing and optimizing autoregressive language models with a focus on test-time training, hardware-aware memory optimization, and efficient state management.\n",
            "analysis": "### Analysis Summary\n\nAfter three rounds of searches, we have gathered significant insights into hierarchical processing, temporal bottlenecks, and hardware optimization for language models. The search results reveal several important aspects that inform the evaluation of the HierarchicalFastTTTLinear proposal:\n\n### Key References and Excerpts\n\n1. **Dynamic Memory Compression (Nawrot et al., 2024)**\n- \"DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference [...] preserves the original downstream performance with up to 4x cache compression\"\n- Demonstrates feasibility of significant memory optimization without performance degradation\n\n2. **FlashDecoding++ (Hong et al., 2024)**\n- \"Asynchronized softmax with unified max value [...] introduces a unified max value technique for different partial softmax computations to avoid synchronization\"\n- Shows importance of hardware-aware optimization for memory access patterns\n\n3. **Temporal Latent Bottleneck (Didolkar et al., 2022)**\n- \"Divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation\"\n- Provides framework for multi-scale processing and temporal compression\n\n4. **H-Transformer-1D (Zhu & Soricut, 2021)**\n- \"Efficient hierarchical method to compute attention [...] has linear run time and memory complexity\"\n- Demonstrates successful implementation of hierarchical attention\n\n### Key Insights\n\n1. **Memory Optimization Approaches**:\n- Dynamic compression shows significant throughput improvements (up to 7x)\n- Adaptive cache management crucial for efficiency\n- Need to balance compression with performance preservation\n\n2. **Hierarchical Processing**:\n- Linear complexity achievable with hierarchical attention\n- Multi-stream processing (fast/slow) shows promise\n- Integration with temporal bottlenecks requires careful design\n\n3. **Hardware Considerations**:\n- Asynchronous computation important for efficiency\n- Memory access patterns critical for performance\n- Unified approaches to reduce synchronization overhead\n\n4. **Implementation Challenges**:\n- Need to maintain test-time training capabilities\n- Balance between compression and model quality\n- Hardware-specific optimizations required\n\n### Areas Needing Further Investigation\n\n1. **Test-Time Training Integration**:\n- How to preserve TTT capabilities with hierarchical processing\n- Impact of compression on TTT effectiveness\n- Efficient state management during TTT\n\n2. **Hardware-Specific Optimizations**:\n- Detailed implementation strategies for different hardware\n- Memory access pattern optimization\n- Parallel processing capabilities\n\n### Future Search Plan\n\nNeed additional search focusing on:\n1. Specific implementations of test-time training in hierarchical models\n2. Hardware-specific optimization techniques for memory access\n3. State management strategies in compressed models\n\n### References\n\n1. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems, 35, 16344-16359.\n\n2. Didolkar, A., Gupta, K., Goyal, A., Lamb, A., Ke, N. R., & Bengio, Y. (2022). Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning. Neural Information Processing Systems.\n\n3. Hong, K., Dai, G., Xu, J., Mao, Q., Li, X., Liu, J., ... & Wang, Y. (2024). FlashDecoding++: Faster Large Language Model Inference on GPUs. arXiv preprint.\n\n4. Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n5. Zhu, Z., & Soricut, R. (2021). H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. Annual Meeting of the Association for Computational Linguistics."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"hierarchical temporal bottleneck language models\", \"adaptive memory compression transformers\", \"multi-scale sequence processing\"",
            "detail": "Search for papers and implementations that:\n\n1. Combine hierarchical processing with temporal latent bottlenecks in language models\n2. Implement adaptive state compression in transformer architectures\n3. Use multi-scale processing for improved efficiency\n4. Modify FastTTTLinear or similar architectures\n5. Address memory optimization in long-sequence processing\n6. Discuss hardware-aware implementation of hierarchical models\n\nFocus on:\n- Implementation details of hierarchical processing\n- Memory compression techniques\n- Integration with existing architectures\n- Performance impact on long sequences\n- Hardware optimization strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations that:\n\n1. Combine hierarchical processing with temporal latent bottlenecks in language models\n2. Implement adaptive state compression in transformer architectures\n3. Use multi-scale processing for improved efficiency\n4. Modify FastTTTLinear or similar architectures\n5. Address memory optimization in long-sequence processing\n6. Discuss hardware-aware implementation of hierarchical models\n\nFocus on:\n- Implementation details of hierarchical processing\n- Memory compression techniques\n- Integration with existing architectures\n- Performance impact on long sequences\n- Hardware optimization strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.89)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.89)*\n\n```\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, pages 2978-2988. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186. Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Nitesh Bharadwaj Gundavarapu, Alex M Lamb, Nan Rosemary Ke, and Yoshua Bengio. 2022. Temporal latent bottleneck: Synthesis of fast and slow processing mechanisms in sequence learning. Advances in Neural Information Processing Systems, 35:10505-10520. Ming Ding, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Cogltx: Applying BERT to long texts. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pretraining for natural language understanding and generation. In Annual Conference on Neural Information Processing Systems 2019, pages 13042-13054. Edouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving neural language models with a continuous cache. In 5th International Conference on Learning Representations, ICLR 2017.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.77)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.77)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.76)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.76)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.56)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.56)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 5. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.55)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.55)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hierarchical temporal bottleneck language models\", \"adaptive memory compression transformers\", \"multi-scale sequence processing\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models\n\n*From Search Query: \"hierarchical temporal bottleneck language models\"*\n\n*Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin*\n\n**TL;DR:** This work proposes TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena, and conducts extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.\n\n**Abstract:** Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world. Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark. To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena. TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models. We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings. Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning. Besides, LLMs exhibit capability discrepancies across different reasoning categories. Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges. We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning. Resources are available at: https://github.com/zchuz/TimeBench\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 2. LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages\n\n*From Search Query: \"hierarchical temporal bottleneck language models\"*\n\n*M. Agarwal, Md Mahfuz Ibn Alam, Antonios Anastasopoulos*\n\n**TL;DR:** This work takes a step towards tackling the data bottleneck by compiling a corpus of over 50K parallel children's stories in 350+ languages and dialects, and the computation bottleneck by building lightweight hierarchical models for language identification.\n\n**Abstract:** Knowing the language of an input text/audio is a necessary first step for using almost every natural language processing (NLP) tool such as taggers, parsers, or translation systems. Language identification is a well-studied problem, sometimes even considered solved; in reality, most of the world's 7000 languages are not supported by current systems. This lack of representation affects large-scale data mining efforts and further exacerbates data shortage for low-resource languages. We take a step towards tackling the data bottleneck by compiling a corpus of over 50K parallel children's stories in 350+ languages and dialects, and the computation bottleneck by building lightweight hierarchical models for language identification. Our data can serve as benchmark data for language identification of short texts and for understudied translation directions such as those between Indian or African languages. Our proposed method, Hierarchical LIMIT, uses limited computation to expand coverage into excluded languages while maintaining prediction quality.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Compositional Foundation Models for Hierarchical Planning\n\n*From Search Query: \"hierarchical temporal bottleneck language models\"*\n\n*Anurag Ajay, Seung-Jun Han, Yilun Du, Shaung Li, Abhishek Gupta, T. Jaakkola, Josh Tenenbaum, L. Kaelbling, Akash Srivastava, Pulkit Agrawal*\n\n**TL;DR:** A foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks and enforce consistency between the models via iterative refinement is proposed.\n\n**Abstract:** To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 40  (*Influential: 8*)\n\n#### 4. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n\n*From Search Query: \"adaptive memory compression transformers\"*\n\n*Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao*\n\n**TL;DR:** Adaptive KV cache compression is introduced, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs) and demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss.\n\n**Abstract:** In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 105  (*Influential: 13*)\n\n#### 5. Context Compression for Auto-regressive Transformers with Sentinel Tokens\n\n*From Search Query: \"adaptive memory compression transformers\"*\n\n*Siyu Ren, Qi Jia, Kenny Q. Zhu*\n\n**TL;DR:** This work proposes a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context.\n\n**Abstract:** The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 6. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: \"adaptive memory compression transformers\"*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 7. Multi-Scale Representation Learning on Proteins\n\n*From Search Query: \"multi-scale sequence processing\"*\n\n*Vignesh Ram Somnath, Charlotte Bunne, Andreas Krause*\n\n**TL;DR:** A multi-scale graph construction of a protein -- HoloProt -- connecting surface to structure and sequence is introduced, and the model performs consistently and reliably across different dataset splits, outperforming all baselines on most splits.\n\n**Abstract:** Proteins are fundamental biological entities mediating key roles in cellular function and disease. This paper introduces a multi-scale graph construction of a protein -- HoloProt -- connecting surface to structure and sequence. The surface captures coarser details of the protein, while sequence as primary component and structure -- comprising secondary and tertiary components -- capture finer details. Our graph encoder then learns a multi-scale representation by allowing each level to integrate the encoding from level(s) below with the graph at that level. We test the learned representation on different tasks, (i.) ligand binding affinity (regression), and (ii.) protein function prediction (classification). On the regression task, contrary to previous methods, our model performs consistently and reliably across different dataset splits, outperforming all baselines on most splits. On the classification task, it achieves a performance close to the top-performing model while using 10x fewer parameters. To improve the memory efficiency of our construction, we segment the multiplex protein surface manifold into molecular superpixels and substitute the surface with these superpixels at little to no performance loss.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 82  (*Influential: 8*)\n\n#### 8. Multilingual, multi-scale and multi-layer visualization of sequence-based intermediate representations\n\n*From Search Query: \"multi-scale sequence processing\"*\n\n*Carlos Escolano Peinado, M. Costa-juss\u00e0, Elora Lacroux, P. Alcocer*\n\n**TL;DR:** This work is supported by a Google Faculty Research Award and the Spanish Ministerio de Economia y Competitividad.\n\n**Abstract:** This work is supported by a Google Faculty Research Award. This workis also supported by the Spanish Ministerio de Economia y Competitividad, the European Regional Development Fund and the Agencia Estatal de Investigacion, through the post-doctoral senior grant Ramon y Cajal, contracts TEC2015-69266-P and TIN2017-88515-C2-1-R(GEN3DLIVE) (MINECO/FEDER,EU), and contract PCIN-2017-079 (AEI/MINECO).\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 9. Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment\n\n*From Search Query: \"multi-scale sequence processing\"*\n\n*Tianhe Wu, Shu Shi, Haoming Cai, Ming Cao, Jing Xiao, Yinqiang Zheng, Yujiu Yang*\n\n**TL;DR:** A novel multi-sequence network for BOIQA called Assessor360 is proposed, derived from the realistic multi-assessor ODI quality assessment procedure, which outperforms state-of-the-art methods on multiple OIQA datasets.\n\n**Abstract:** Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively assess the human perceptual quality of omnidirectional images (ODIs) without relying on pristine-quality image information. It is becoming more significant with the increasing advancement of virtual reality (VR) technology. However, the quality assessment of ODIs is severely hampered by the fact that the existing BOIQA pipeline lacks the modeling of the observer's browsing process. To tackle this issue, we propose a novel multi-sequence network for BOIQA called Assessor360, which is derived from the realistic multi-assessor ODI quality assessment procedure. Specifically, we propose a generalized Recursive Probability Sampling (RPS) method for the BOIQA task, combining content and details information to generate multiple pseudo-viewport sequences from a given starting point. Additionally, we design a Multi-scale Feature Aggregation (MFA) module with a Distortion-aware Block (DAB) to fuse distorted and semantic features of each viewport. We also devise Temporal Modeling Module (TMM) to learn the viewport transition in the temporal domain. Extensive experimental results demonstrate that Assessor360 outperforms state-of-the-art methods on multiple OIQA datasets. The code and models are available at https://github.com/TianheWu/Assessor360.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: \"hierarchical temporal bottleneck language models\"*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 2. RevUp: Revise and Update Information Bottleneck for Event Representation\n\n*From Search Query: \"hierarchical temporal bottleneck language models\"*\n\n*Mehdi Rezaee, Francis Ferraro*\n\n**Abstract:** The existence of external (``side'') semantic knowledge has been shown to\nresult in more expressive computational event models. To enable the use of side\ninformation that may be noisy or missing, we propose a semi-supervised\ninformation bottleneck-based discrete latent variable model. We reparameterize\nthe model's discrete variables with auxiliary continuous latent variables and a\nlight-weight hierarchical structure. Our model is learned to minimize the\nmutual information between the observed data and optional side knowledge that\nis not already captured by the new, auxiliary variables. We theoretically show\nthat our approach generalizes past approaches, and perform an empirical case\nstudy of our approach on event modeling. We corroborate our theoretical results\nwith strong empirical experiments, showing that the proposed method outperforms\nprevious proposed approaches on multiple datasets.\n\n**Published:** 2022-05-24T17:54:59Z  (*Updated: 2023-02-14T18:58:14Z*)\n\n\n\n#### 3. Extreme Compression of Large Language Models via Additive Quantization\n\n*From Search Query: \"adaptive memory compression transformers\"*\n\n*Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan Alistarh*\n\n**Abstract:** The emergence of accurate open large language models (LLMs) has led to a race\ntowards performant quantization techniques which can enable their execution on\nend-user devices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression-defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter-from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our algorithm, called AQLM, generalizes the classic\nAdditive Quantization (AQ) approach for information retrieval to advance the\nstate-of-the-art in LLM compression, via two innovations: 1) learned additive\nquantization of weight matrices in input-adaptive fashion, and 2) joint\noptimization of codebook parameters across each transformer blocks. Broadly,\nAQLM is the first scheme that is Pareto optimal in terms of\naccuracy-vs-model-size when compressing to less than 3 bits per parameter, and\nsignificantly improves upon all known schemes in the extreme compression (2bit)\nregime. In addition, AQLM is practical: we provide fast GPU and CPU\nimplementations of AQLM for token generation, which enable us to match or\noutperform optimized FP16 implementations for speed, while executing in a much\nsmaller memory footprint.\n\n**Published:** 2024-01-11T18:54:44Z  (*Updated: 2024-09-11T07:48:26Z*)\n\n\n\n#### 4. LoMA: Lossless Compressed Memory Attention\n\n*From Search Query: \"adaptive memory compression transformers\"*\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.\n\n**Published:** 2024-01-16T09:18:46Z  (*Updated: 2024-02-04T03:14:08Z*)\n\n\n\n#### 5. Modeling sequential annotations for sequence labeling with crowds\n\n*From Search Query: \"multi-scale sequence processing\"*\n\n*Xiaolei Lu, Tommy W. S. Chow*\n\n**Abstract:** Crowd sequential annotations can be an efficient and cost-effective way to\nbuild large datasets for sequence labeling. Different from tagging independent\ninstances, for crowd sequential annotations the quality of label sequence\nrelies on the expertise level of annotators in capturing internal dependencies\nfor each token in the sequence. In this paper, we propose Modeling sequential\nannotation for sequence labeling with crowds (SA-SLC). First, a conditional\nprobabilistic model is developed to jointly model sequential data and\nannotators' expertise, in which categorical distribution is introduced to\nestimate the reliability of each annotator in capturing local and non-local\nlabel dependency for sequential annotation. To accelerate the marginalization\nof the proposed model, a valid label sequence inference (VLSE) method is\nproposed to derive the valid ground-truth label sequences from crowd sequential\nannotations. VLSE derives possible ground-truth labels from the token-wise\nlevel and further prunes sub-paths in the forward inference for label sequence\ndecoding. VLSE reduces the number of candidate label sequences and improves the\nquality of possible ground-truth label sequences. The experimental results on\nseveral sequence labeling tasks of Natural Language Processing show the\neffectiveness of the proposed model.\n\n**Published:** 2022-09-20T02:51:23Z  (*Updated: 2022-09-20T02:51:23Z*)\n\n\n\n#### 6. Why self-attention is Natural for Sequence-to-Sequence Problems? A\n  Perspective from Symmetries\n\n*From Search Query: \"multi-scale sequence processing\"*\n\n*Chao Ma, Lexing Ying*\n\n**Abstract:** In this paper, we show that structures similar to self-attention are natural\nto learn many sequence-to-sequence problems from the perspective of symmetry.\nInspired by language processing applications, we study the orthogonal\nequivariance of seq2seq functions with knowledge, which are functions taking\ntwo inputs -- an input sequence and a ``knowledge'' -- and outputting another\nsequence. The knowledge consists of a set of vectors in the same embedding\nspace as the input sequence, containing the information of the language used to\nprocess the input sequence. We show that orthogonal equivariance in the\nembedding space is natural for seq2seq functions with knowledge, and under such\nequivariance the function must take the form close to the self-attention. This\nshows that network structures similar to self-attention are the right\nstructures to represent the target function of many seq2seq problems. The\nrepresentation can be further refined if a ``finite information principle'' is\nconsidered, or a permutation equivariance holds for the elements of the input\nsequence.\n\n**Published:** 2022-10-13T05:10:48Z  (*Updated: 2022-10-13T05:10:48Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome\n\n*From Search Query: \"adaptive memory compression transformers\"*\n\n*Han Liu, Ramana Davuluri, Pratik Dutta, Weijian Li, Yanrong Ji, Zhihan Zhou*\n\n**Abstract:** Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $36$ distinct datasets across $9$ tasks, with input lengths ranging from $70$ to $10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with $21 \\times$ fewer parameters and approximately $92 \\times$ less GPU time in pre-training.\n\n**Published:** 2023-06-26\n\n\n\n#### 2. Multi-scale Poisson process approaches for differential expression analysis of high-throughput sequencing data\n\n*From Search Query: \"multi-scale sequence processing\"*\n\n*Anonymous*\n\n**Abstract:** Estimating and testing for differences in molecular phenotypes (e.g. gene expression, chromatin accessibility, transcription factor binding) across conditions is an important part of understanding the molecular basis of gene regulation. These phenotypes are commonly measured using high-throughput sequencing assays (e.g., RNA-seq, ATAC-seq, ChIP-seq), which provide high-resolution count data that reflect how the phenotypes vary along the genome. Multiple methods have been proposed to help exploit these high-resolution measurements for differential expression analysis. However, they ignore the count nature of the data, instead using normal approximations that work well only for data with large sample sizes or high counts. Here we develop count-based methods to address this problem. We model the data for each sample using an inhomogeneous Poisson process with spatially structured underlying intensity function, and then, building on multi-scale models for the Poisson process, estimate and test for differences in the underlying intensity function across samples (or groups of samples). Using both simulation and real ATAC-seq data we show that our method outperforms previous normal-based methods, especially in situations with small sample sizes or low counts.\n\n**Published:** 2021-06-25\n\n\n\n#### 3. WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing\n\n*From Search Query: \"multi-scale sequence processing\"*\n\n*Jian Wu, Xiangzhan Yu, Michael Zeng, Furu Wei, Yao Qian, Yanmin Qian, Shuo Ren, Long Zhou, Xiong Xiao, Takuya Yoshioka, Naoyuki Kanda, Jinyu Li, Zhuo Chen, Shujie Liu, Yu Wu, Zhengyang Chen, Chengyi Wang, Sanyuan Chen*\n\n**Abstract:** Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.\n\n**Published:** 2021-10-26\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models using the concepts of hierarchical temporal bottleneck language models, adaptive memory compression transformers, and multi-scale sequence processing, here are some relevant findings and suggestions based on the provided sources and additional insights:\n\n## Hierarchical Processing with Temporal Latent Bottlenecks\n\n- The concept of using temporal latent bottlenecks is somewhat analogous to the idea of enforcing interpretability in time series Transformers through Concept Bottleneck Models. This approach involves modifying the training objective to align the model's representations with predefined interpretable concepts, such as time features and simple autoregressive models. While this is not directly applied to language models, it provides a foundation for understanding how hierarchical and temporal bottlenecks can be integrated.\n\n## Adaptive State Compression in Transformer Architectures\n\n- The Mamba model, discussed in the context of state-space sequence models, offers insights into efficient processing of sequential data. Mamba models are formulated to have near-linear scaling in sequence length and possess mechanisms for modeling long-range dependencies, which could be adapted for state compression in transformer architectures. The EMMA model extends Mamba to include structural and hierarchical alignment, which can be seen as a form of adaptive state compression.\n\n## Multi-Scale Processing for Improved Efficiency\n\n- Multi-scale processing is a key aspect of the EMMA model, which uses a Multi-scale Feature Fusion (MFF) module to hierarchically preserve fine-grained visual features. This approach can be adapted to language models to process sequences at multiple scales, improving efficiency and reducing computational overhead. The hierarchical processing in EMMA aligns with the need for multi-scale sequence processing in language models.\n\n## Modification of FastTTTLinear or Similar Architectures\n\n- The FastTTTLinear architecture, known for its efficient attention mechanisms, can be modified to incorporate hierarchical processing and adaptive state compression. The proposal to modify FastTTTLinear GAU (Generalized Attention Unit) suggests integrating hierarchical multi-scale processing and adaptive state compression mechanisms. This would involve studying compatibility issues and the impact on test-time training capabilities, similar to how EMMA integrates hierarchical alignment with Mamba models.\n\n## Memory Optimization in Long-Sequence Processing\n\n- Memory optimization is crucial for long-sequence processing. The Mamba model and its extensions, such as EMMA, offer significant improvements in throughput and efficiency compared to traditional transformer models. These models achieve this through near-linear scaling in sequence length and principled mechanisms for modeling long-range dependencies, which can be applied to optimize memory usage in language models.\n\n## Hardware-Aware Implementation of Hierarchical Models\n\n- Hardware-aware implementations are essential for optimizing the performance of hierarchical models. The EMMA model, for instance, addresses the computational overhead of current MLLMs by proposing architectural changes that enable more efficient processing of data. This includes pixel-wise alignment and multi-scale feature fusion, which can be adapted to language models to ensure hardware-efficient implementations.\n\n### Additional Insights and Recommendations\n\n- **Hierarchical Transformers**: Research on hierarchical transformers has shown benefits in multi-level processing, reducing computational complexity. This framework can be adapted to integrate temporal latent bottlenecks and multi-scale processing, aligning with the proposal's key aspects[Initial Analysis].\n  \n- **Memory-Efficient Attention**: Papers on memory-efficient attention mechanisms provide insights into achieving O(1) memory complexity, which is crucial for long-sequence processing. These strategies can be integrated into the proposed hierarchical and adaptive state compression framework[Initial Analysis].\n\n- **Novelty Assessment and Technical Feasibility**: To assess the novelty of the proposed integration, it is essential to search for similar hierarchical and temporal approaches in the literature. Investigating unique aspects and comparing them with existing modifications to FastTTTLinear or other architectures will help in understanding the technical feasibility and potential implementation challenges[Initial Analysis].\n\nBy combining these insights, the researchers can develop a novel autoregressive language model that leverages hierarchical processing, adaptive state compression, and multi-scale sequence processing, while optimizing for memory efficiency and hardware-aware implementations. This approach has the potential to improve performance on long sequences, enhance scalability, and provide better overall efficiency.\n",
            "analysis": "Initial Analysis of HierarchicalFastTTTLinear Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) Hierarchical Multi-scale Processing:\n- The proposal introduces hierarchical processing with temporal latent bottlenecks\n- Need to investigate existing hierarchical architectures in language models\n- Research temporal latent bottlenecks and their applications\n- Examine multi-scale processing approaches\n\nb) Memory Optimization:\n- Adaptive state compression mechanism proposed\n- Need to study existing memory compression techniques\n- Research efficient state management approaches\n- Investigate hardware-aware memory optimizations\n\nc) Integration with FastTTTLinear:\n- Proposal modifies FastTTTLinear GAU\n- Need to understand compatibility issues\n- Research similar modifications to FastTTTLinear\n- Study impact on test-time training capabilities\n\n2. Investigation Plan:\n\nPhase 1: Hierarchical Processing\n- Research hierarchical architectures in LMs\n- Study temporal latent bottlenecks\n- Examine multi-scale processing techniques\n\nPhase 2: Memory Management\n- Investigate state compression methods\n- Study adaptive memory techniques\n- Research hardware-efficient implementations\n\nPhase 3: Integration Analysis\n- Analyze FastTTTLinear modifications\n- Study test-time training preservation\n- Research similar hybrid approaches\n\n3. Initial Comparison with Previous Designs:\n\nThe proposal appears to have some overlap with:\n- AdaptiveSparseTTTLinear: Both use hierarchical processing but with different approaches\n- GPT-2-AHLA: Similar memory optimization goals but different mechanisms\n- However, the specific combination of hierarchical processing with temporal latent bottlenecks appears novel\n\n4. Key References Found:\n\na) Temporal Latent Bottleneck paper:\n- Provides foundation for temporal bottleneck concept\n- Shows benefits in sequence learning\n- Demonstrates improved sample efficiency\n\nb) Memory-Efficient Attention paper:\n- Offers insights into efficient memory management\n- Shows possibility of O(1) memory complexity\n- Provides implementation strategies\n\nc) Hierarchical Transformers paper:\n- Demonstrates multi-level processing benefits\n- Shows reduced computational complexity\n- Provides framework for hierarchical modeling\n\n5. Areas Needing Further Investigation:\n\na) Novelty Assessment:\n- Need to search for similar hierarchical+temporal approaches\n- Investigate unique aspects of proposed integration\n- Compare with existing FastTTTLinear modifications\n\nb) Technical Feasibility:\n- Research implementation challenges\n- Study memory-efficiency trade-offs\n- Investigate hardware optimization potential\n\nc) Performance Impact:\n- Study potential accuracy improvements\n- Research scalability implications\n- Investigate robustness enhancements\n\nInitial assessment suggests the proposal has merit but requires thorough investigation of existing approaches and potential implementation challenges."
        },
        {
            "ready": false,
            "query": "\"hierarchical transformer architecture implementation\", \"temporal bottleneck language model optimization\"",
            "detail": "Search for papers and implementations that:\n\n1. Describe specific implementations of hierarchical processing in transformer architectures\n2. Detail methods for integrating temporal bottlenecks in language models\n3. Discuss optimization techniques for hierarchical models\n4. Present empirical results on memory-efficient transformer variants\n5. Analyze trade-offs between compression and model performance\n\nFocus on:\n- Implementation details and architecture specifications\n- Performance metrics and benchmarks\n- Memory efficiency measurements\n- Integration challenges and solutions\n- Hardware optimization strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations that:\n\n1. Describe specific implementations of hierarchical processing in transformer architectures\n2. Detail methods for integrating temporal bottlenecks in language models\n3. Discuss optimization techniques for hierarchical models\n4. Present empirical results on memory-efficient transformer variants\n5. Analyze trade-offs between compression and model performance\n\nFocus on:\n- Implementation details and architecture specifications\n- Performance metrics and benchmarks\n- Memory efficiency measurements\n- Integration challenges and solutions\n- Hardware optimization strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.24)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.24)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.18)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.18)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.15)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.15)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n#### 4. Soaring from 4K to 400K: Extending LLM\u2019s Context with Activation Beacon (Avg. Score: 0.10)\n\n*Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 30  (*Influential: 1*)\n\n**TL;DR:** Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts and works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference.\n\n**Abstract:** The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small training cost. Our experiment verifies Activation Beacon's effectiveness of context extension: it can remarkably accomplish high-quality extension of Llama-2-7B's context by $\\times100$ times (from 4K to 400K); meanwhile, it can also achieve superior performances across a variety of long-context language modeling and understanding tasks. The source code and model checkpoint are available at \\url{https://github.com/FlagOpen/FlagEmbedding}.\n\n##### *Relevant Chunk: No. 22/27 (Score: 0.10)*\n\n```\narXiv preprint arXiv:2307.09288, 2023. [31] Tunstall, L., Von Werra, L., and Wolf, T. Natural language processing with transformers, 2022. [32] Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. [33] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.04768\n[34] Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. Augmenting language models with long-term memory. CoRR, abs/2306.07174, 2023. doi: 10.48550/ARXIV.2306. 07174. URL https://doi.org/10.48550/arXiv.2306.07174. [35] Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=TrjbxzRcnf-. [36] Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [37] Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. CoRR, abs/2310.03025, 2023. doi: 10.48550/ARXIV.2310.03025. URL https://doi.org/10 48550/arXiv. 2310.03025\n[38] Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences.\n```\n\n#### 5. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.09)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 22/34 (Score: 0.09)*\n\n```\nInternational Conference on Computer AidedDesign, pages 448-455. Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. ArXiv, abs/1805.04623. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. ArXiv, abs/2001.04451. Yang Liu and Mirella Lapata. 2019. Hierarchical transformers for multi-document summarization. In $A C L$. Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. ArXiv, abs/1508.04025. Chris Manning and Hinrich Sch\u00fctze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA. Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. Document-level neural machine translation with hierarchical attention networks. In EMNLP. K. Nabors, T. Korsmeyer, and J.\n```\n\n\n\n---\n## Found 13 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: \"hierarchical transformer architecture implementation\", \"temporal bottleneck language model optimization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation\n\n*From Search Query: \"hierarchical transformer architecture implementation\"*\n\n*Wenqing Zheng, S. Sharan, Ajay Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, Zhangyang Wang*\n\n**TL;DR:** This work proposes ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes, demonstrating that the progressive generation eases the reasoning procedure and guides the language model to generate higher-quality solutions.\n\n**Abstract:** For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of\"outline-then-detail\". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to jointly encode the natural language descriptions and syntactically aligned I/O data samples. Extensive evaluations show that ChainCoder outperforms state-of-the-arts, demonstrating that our progressive generation eases the reasoning procedure and guides the language model to generate higher-quality solutions. Our codes are available at: https://github.com/VITA-Group/ChainCoder.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 2. HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer\n\n*From Search Query: \"hierarchical transformer architecture implementation\"*\n\n*Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, Qi Tian*\n\n**TL;DR:** A new architecture named HiViT (short for hierarchical ViT, which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning), after pre-trained using masked autoencoder on ImageNet-1K.\n\n**Abstract:** There has been a debate on the choice of plain vs. hierarchical vision transformers, where researchers often believe that the former (e.g., ViT) has a simpler design but the latter (e.g., Swin) enjoys higher recognition accuracy. Recently, the emerge of masked image modeling (MIM), a self-supervised pre-training method, raised a new challenge to vision transformers in terms of flexibility, i.e., part of image patches or tokens are to be discarded, which seems to claim the advantages of plain vision transformers. In this paper, we delve deep into the comparison between ViT and Swin, revealing that (i) the performance gain of Swin is mainly brought by a deepened backbone and relative positional encoding, (ii) the hierarchical design of Swin can be simplified into hierarchical patch embedding (proposed in this work), and (iii) other designs such as shifted-window attentions can be removed. By removing the unnecessary operations, we come up with a new architecture named HiViT (short for hierarchical ViT), which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning. In particular, after pre-trained using masked autoencoder (MAE) on ImageNet-1K, HiViT-B reports a 84.6% accuracy on ImageNet-1K classification, a 53.3% box AP on COCO detection, and a 52.8% mIoU on ADE20K segmentation, significantly surpassing the baseline. Code is available at https://github.com/zhangxiaosong18/hivit.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 1*)\n\n#### 3. Hierarchical Graph Transformer with Adaptive Node Sampling\n\n*From Search Query: \"hierarchical transformer architecture implementation\"*\n\n*Zaixin Zhang, Qi Liu, Qingyong Hu, Cheekong Lee*\n\n**TL;DR:** This paper identifies the main deficiencies of current graph transformers and formulate the optimization strategies of node sampling in Graph Transformer as an adversary bandit problem, where the rewards are related to the attention weights and can vary in the training procedure.\n\n**Abstract:** The Transformer architecture has achieved remarkable success in a number of domains including natural language processing and computer vision. However, when it comes to graph-structured data, transformers have not achieved competitive performance, especially on large graphs. In this paper, we identify the main deficiencies of current graph transformers:(1) Existing node sampling strategies in Graph Transformers are agnostic to the graph characteristics and the training process. (2) Most sampling strategies only focus on local neighbors and neglect the long-range dependencies in the graph. We conduct experimental investigations on synthetic datasets to show that existing sampling strategies are sub-optimal. To tackle the aforementioned problems, we formulate the optimization strategies of node sampling in Graph Transformer as an adversary bandit problem, where the rewards are related to the attention weights and can vary in the training procedure. Meanwhile, we propose a hierarchical attention scheme with graph coarsening to capture the long-range interactions while reducing computational complexity. Finally, we conduct extensive experiments on real-world datasets to demonstrate the superiority of our method over existing graph transformers and popular GNNs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 63  (*Influential: 10*)\n\n#### 4. Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model\n\n*From Search Query: \"temporal bottleneck language model optimization\"*\n\n*Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, D. Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, V. Chaudhary, Shuai Xu, Xia Hu*\n\n**TL;DR:** This work proposes a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient in a stochastic manner.\n\n**Abstract:** With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, neural networks are usually trained using stochastic gradient descent. We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance. Following this motivation, we propose a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones. By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7$\\times$ peak memory reduction with almost no accuracy drop and enables up to $6.4\\times$ larger batch size. Under the same hardware, WTA-CRS enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 5. TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding\n\n*From Search Query: \"temporal bottleneck language model optimization\"*\n\n*Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou*\n\n**TL;DR:** This work introduces a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block, and proposes an efficient method called TEmporal-Spatial Token Aggregation (TESTA), which condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame.\n\n**Abstract:** Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. To tackle this issue, we propose an efficient method called TEmporal-Spatial Token Aggregation (TESTA). TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame. TESTA can reduce the number of visual tokens by 75% and thus accelerate video encoding. Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block. We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks. Experimental results show that TESTA improves computing efficiency by 1.7 times, and achieves significant performance gains from its scalability in processing longer input frames, e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 2*)\n\n#### 6. COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models\n\n*From Search Query: \"temporal bottleneck language model optimization\"*\n\n*Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, Weiping Wang*\n\n**TL;DR:** This work proposes a collaborative optimization for PLMs that integrates static model compression and dynamic inference acceleration, and proposes a joint training approach that calibrates slenderization and preserves contributive structures to each exit instead of only the final layer.\n\n**Abstract:** Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity. For resource-constrained devices, there is an urgent need for a spatially and temporally efficient model which retains the major capacity of PLMs. However, existing statically compressed models are unaware of the diverse complexities between input instances, potentially resulting in redundancy and inadequacy for simple and complex inputs. Also, miniature models with early exiting encounter challenges in the trade-off between making predictions and serving the deeper layers. Motivated by such considerations, we propose a collaborative optimization for PLMs that integrates static model compression and dynamic inference acceleration. Specifically, the PLM is slenderized in width while the depth remains intact, complementing layer-wise early exiting to speed up inference dynamically. To address the trade-off of early exiting, we propose a joint training approach that calibrates slenderization and preserves contributive structures to each exit instead of only the final layer. Experiments are conducted on GLUE benchmark and the results verify the Pareto optimality of our approach at high compression and acceleration rate with 1/8 parameters and 1/19 FLOPs of BERT.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 4  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. Cascaded Head-colliding Attention\n\n*From Search Query: \"hierarchical transformer architecture implementation\"*\n\n*Lin Zheng, Zhiyong Wu, Lingpeng Kong*\n\n**Abstract:** Transformers have advanced the field of natural language processing (NLP) on\na variety of important tasks. At the cornerstone of the Transformer\narchitecture is the multi-head attention (MHA) mechanism which models pairwise\ninteractions between the elements of the sequence. Despite its massive success,\nthe current framework ignores interactions among different heads, leading to\nthe problem that many of the heads are redundant in practice, which greatly\nwastes the capacity of the model. To improve parameter efficiency, we\nre-formulate the MHA as a latent variable model from a probabilistic\nperspective. We present cascaded head-colliding attention (CODA) which\nexplicitly models the interactions between attention heads through a\nhierarchical variational distribution. We conduct extensive experiments and\ndemonstrate that CODA outperforms the transformer baseline, by $0.6$ perplexity\non \\texttt{Wikitext-103} in language modeling, and by $0.6$ BLEU on\n\\texttt{WMT14 EN-DE} in machine translation, due to its improvements on the\nparameter efficiency.\\footnote{Our implementation is publicly available at\n\\url{https://github.com/LZhengisme/CODA}.}\n\n**Published:** 2021-05-31T10:06:42Z  (*Updated: 2021-05-31T10:06:42Z*)\n\n\n\n#### 2. How transformers learn structured data: insights from hierarchical\n  filtering\n\n*From Search Query: \"hierarchical transformer architecture implementation\"*\n\n*Jerome Garnier-Brun, Marc M\u00e9zard, Emanuele Moscato, Luca Saglietti*\n\n**Abstract:** We introduce a hierarchical filtering procedure for generative models of\nsequences on trees, enabling control over the range of positional correlations\nin the data. Leveraging this controlled setting, we provide evidence that\nvanilla encoder-only transformer architectures can implement the optimal Belief\nPropagation algorithm on both root classification and masked language modeling\ntasks. Correlations at larger distances corresponding to increasing layers of\nthe hierarchy are sequentially included as the network is trained. We analyze\nhow the transformer layers succeed by focusing on attention maps from models\ntrained with varying degrees of filtering. These attention maps show clear\nevidence for iterative hierarchical reconstruction of correlations, and we can\nrelate these observations to a plausible implementation of the exact inference\nalgorithm for the network sizes considered.\n\n**Published:** 2024-08-27T15:23:09Z  (*Updated: 2024-08-27T15:23:09Z*)\n\n\n\n#### 3. Which transformer architecture fits my data? A vocabulary bottleneck in\n  self-attention\n\n*From Search Query: \"temporal bottleneck language model optimization\"*\n\n*Noam Wies, Yoav Levine, Daniel Jannai, Amnon Shashua*\n\n**Abstract:** After their successful debut in natural language processing, Transformer\narchitectures are now becoming the de-facto standard in many domains. An\nobstacle for their deployment over new modalities is the architectural\nconfiguration: the optimal depth-to-width ratio has been shown to dramatically\nvary across data types (e.g., $10$x larger over images than over language). We\ntheoretically predict the existence of an embedding rank bottleneck that limits\nthe contribution of self-attention width to the Transformer expressivity. We\nthus directly tie the input vocabulary size and rank to the optimal\ndepth-to-width ratio, since a small vocabulary size or rank dictates an added\nadvantage of depth over width. We empirically demonstrate the existence of this\nbottleneck and its implications on the depth-to-width interplay of Transformer\narchitectures, linking the architecture variability across domains to the often\nglossed-over usage of different vocabulary sizes or embedding ranks in\ndifferent domains. As an additional benefit, our rank bottlenecking framework\nallows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models\nsuch as ALBERT and T5.\n\n**Published:** 2021-05-09T13:08:26Z  (*Updated: 2021-06-09T17:18:03Z*)\n\n\n\n#### 4. Scaling Language Model Size in Cross-Device Federated Learning\n\n*From Search Query: \"temporal bottleneck language model optimization\"*\n\n*Jae Hun Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Theertha Suresh, Shankar Kumar, Rajiv Mathews*\n\n**Abstract:** Most studies in cross-device federated learning focus on small models, due to\nthe server-client communication and on-device computation bottlenecks. In this\nwork, we leverage various techniques for mitigating these bottlenecks to train\nlarger language models in cross-device federated learning. With systematic\napplications of partial model training, quantization, efficient transfer\nlearning, and communication-efficient optimizers, we are able to train a $21$M\nparameter Transformer and $20.2$M parameter Conformer that achieve the same or\nbetter perplexity as that of a similarly sized LSTM with $\\sim10\\times$ smaller\nclient-to-server communication cost and $11\\%$ lower perplexity than smaller\nLSTMs commonly studied in literature.\n\n**Published:** 2022-03-31T15:51:53Z  (*Updated: 2022-06-24T19:20:47Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. Automatic Piano Transcription with Hierarchical Frequency-Time Transformer\n\n*From Search Query: \"hierarchical transformer architecture implementation\"*\n\n*Yuki Mitsufuji, Wei-Hsiang Liao, Yuhta Takida, Yukara Ikemiya, Taketo Akama, Keisuke Toyama*\n\n**Abstract:** Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.\n\n**Published:** 2023-07-10\n\n\n\n#### 2. Tutel: Adaptive Mixture-of-Experts at Scale\n\n*From Search Query: \"hierarchical transformer architecture implementation\"*\n\n*Yongqiang Xiong, Mao Yang, Fan Yang, Peng Cheng, Joe Chau, Prabhat Ram, Jithin Jose, Rafael Salas, Zilong Wang, Han Hu, Ze Liu, Ziyue Yang, Yifan Xiong, Wei Cui, Changho Hwang*\n\n**Abstract:** Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on this key design, Flex also implements various MoE acceleration techniques. Aggregating all techniques, Flex finally delivers huge speedup at any scale -- 4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs, respectively, over the previous state-of-the-art. Our evaluation shows that Flex efficiently and effectively runs a real-world MoE-based model named SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x and 2.11x speedup in training and inference over Fairseq, respectively. On effectiveness, the SwinV2-MoE model achieves superior accuracy in both pre-training and down-stream computer vision tasks such as COCO object detection than the counterpart dense model, indicating the readiness of Flex for end-to-end real-world model training and inference.\n\n**Published:** 2022-06-07\n\n\n\n#### 3. Free Video-LLM: Prompt-guided Visual Perception for Efficient Training-free Video LLMs\n\n*From Search Query: \"temporal bottleneck language model optimization\"*\n\n*Yunhe Wang, Enhua Wu, wei he, Yehui Tang, Jianyuan Guo, Kai Han*\n\n**Abstract:** Vision-language large models have achieved remarkable success in various multi-modal tasks, yet applying them to video understanding remains challenging due to the inherent complexity and computational demands of video data. While training-based video-LLMs deliver high performance, they often require substantial resources for training and inference. Conversely, training-free approaches offer a more efficient alternative by adapting pre-trained image-LLMs models for video tasks without additional training, but they face inference efficiency bottlenecks due to the large number of visual tokens generated from video frames. In this work, we present a novel prompt-guided visual perception framework (abbreviated as Free Video-LLM) for efficient inference of training-free video LLMs. The proposed framework decouples spatial-temporal dimension and performs temporal frame sampling and spatial RoI cropping respectively based on task-specific prompts. Our method effectively reduces the number of visual tokens while maintaining high performance across multiple video question-answering benchmarks. Extensive experiments demonstrate that our approach achieves competitive results with significantly fewer tokens, offering an optimal trade-off between accuracy and computational efficiency compared to state-of-the-art video LLMs. The code will be available at https://github.com/contrastive/FreeVideoLLM.\n\n**Published:** 2024-10-14\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical transformer architectures, temporal bottlenecks, and optimization techniques, here are some key findings and references that align with your requirements:\n\n## Hierarchical Transformer Architecture Implementation\n\n- **Hierarchical Processing**: Models like the Pyramid Vision Transformer v2 (PVTv2) and Swin Transformer, although primarily designed for vision tasks, demonstrate hierarchical processing. These models can be adapted for language tasks by incorporating similar hierarchical structures. For example, PVTv2 uses a mix of transformer layers and convolution operations to process data at multiple scales, which could be a valuable approach for language models to capture multi-scale contextual information.\n\n## Temporal Bottleneck Integration in Language Models\n\n- **Temporal Latent Bottlenecks**: While the concept of temporal latent bottlenecks is more commonly discussed in the context of temporal reasoning (e.g., TimeBench by Chu et al.), integrating such bottlenecks into language models involves designing mechanisms to compress and manage temporal contexts efficiently. The \"Model Tells You What to Discard\" approach by Ge et al. and \"Context Compression\" by Ren et al. provide insights into adaptive cache management and context compression, which can be adapted to include temporal bottlenecks.\n\n## Optimization Techniques for Hierarchical Models\n\n- **Dynamic Memory Compression**: The work by Nawrot et al. on Dynamic Memory Compression (DMC) Transformers is highly relevant. DMC achieves significant throughput improvements during auto-regressive inference while preserving downstream performance. This approach involves compressing the cache and managing memory efficiently, which is crucial for hierarchical models.\n- **Adaptive KV Cache**: Ge et al.'s work on adaptive KV cache compression emphasizes evicting long-range contexts and discarding non-special tokens, which can be integrated into hierarchical models to optimize memory usage.\n\n## Empirical Results on Memory-Efficient Transformer Variants\n\n- **Performance Metrics**: The Dynamic Memory Compression approach has shown up to a 7x throughput increase during auto-regressive inference and up to 4x cache compression without sacrificing performance. This indicates that such techniques can significantly improve the efficiency of hierarchical transformer models.\n- **EfficientFormer**: Although focused on vision tasks, the EfficientFormer model's approach to revisiting network architecture, dimension-consistent design, and latency-driven slimming can provide valuable insights for optimizing language models. For instance, EfficientFormer achieves high accuracy with low latency, which could be a model for optimizing hierarchical language models.\n\n## Trade-Offs Between Compression and Model Performance\n\n- **Balancing Compression and Performance**: The key challenge is to balance memory compression with performance preservation. Studies like \"Model Tells You What to Discard\" and \"Context Compression\" highlight the importance of adaptive management and incremental compression to maintain model quality while reducing memory and computational costs.\n\n## Hardware Optimization Strategies\n\n- **Hardware-Aware Design**: The parallel nature of transformers makes them highly efficient on modern hardware like GPUs, which are designed for large-scale matrix operations. Optimizing models for hardware, such as using techniques like FlashAttention that are IO-aware, can significantly improve performance and efficiency.\n\n### Implementation Considerations and Integration Challenges\n\n- **Integration with Existing Architectures**: Integrating hierarchical processing and temporal bottlenecks into existing transformer architectures requires careful design to ensure that the model can still capture both local and global relationships within sequences. This involves balancing the complexity of the hierarchical structure with the need for efficient memory management and hardware optimization.\n\nBy focusing on these aspects, researchers can develop more efficient, scalable, and performant autoregressive language models that leverage hierarchical processing, temporal bottlenecks, and advanced optimization techniques.\n",
            "analysis": "### Analysis Summary\n\nThe initial search has revealed several important aspects of the HierarchicalFastTTTLinear proposal that require further investigation:\n\n1. **Temporal Latent Bottlenecks**\n- The concept appears in recent work like TimeBench (Chu et al., 2023)\n- Need more specific information about implementation in language models\n- Current research focuses mainly on temporal reasoning rather than model architecture\n\n2. **Memory Compression Techniques**\n- Recent work like \"Model Tells You What to Discard\" (Ge et al., 2023) shows promising approaches\n- Dynamic Memory Compression (Nawrot et al., 2024) offers relevant insights\n- Need more details on integration with hierarchical processing\n\n3. **Multi-scale Processing**\n- Limited direct application in language models found\n- Most research focuses on other domains (proteins, images)\n- Need to investigate adaptation to language model context\n\n### Useful References with Key Excerpts\n\n1. **Dynamic Memory Compression (Nawrot et al., 2024)**\n\"DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference [...] preserves the original downstream performance with up to 4x cache compression\"\n\n2. **Adaptive KV Cache (Ge et al., 2023)**\n\"evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens\"\n\n3. **Context Compression (Ren et al., 2023)**\n\"incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost\"\n\n### Key Insights\n\n1. **Memory Optimization Approaches**:\n- Dynamic compression shows significant throughput improvements\n- Adaptive cache management is crucial for efficiency\n- Need to balance compression with performance preservation\n\n2. **Hierarchical Processing**:\n- Limited research on combining with memory compression\n- Potential synergy with temporal bottlenecks\n- Need to investigate impact on model quality\n\n3. **Implementation Considerations**:\n- Hardware-aware design is crucial\n- Trade-offs between compression and model performance\n- Integration challenges with existing architectures\n\n### Future Search Plan\n\n1. **Next Search Focus**:\n- Specific implementations of hierarchical processing in language models\n- Integration methods for temporal bottlenecks\n- Hardware optimization techniques for compressed models\n\n2. **Keywords for Next Search**:\n- \"hierarchical transformer architecture implementation\"\n- \"temporal bottleneck language model optimization\"\n- \"efficient memory management transformers\"\n\n### References\n\n1. Chu, Z., Chen, J., Chen, Q., Yu, W., Wang, H., Liu, M., & Qin, B. (2023). TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models. Annual Meeting of the Association for Computational Linguistics.\n\n2. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., & Gao, J. (2023). Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. International Conference on Learning Representations.\n\n3. Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n4. Ren, S., Jia, Q., & Zhu, K. Q. (2023). Context Compression for Auto-regressive Transformers with Sentinel Tokens. Conference on Empirical Methods in Natural Language Processing.\n\n5. Dao, T., Fu, D., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "\"test-time training hierarchical transformers\", \"temporal bottleneck optimization hardware\"",
            "detail": "Search for papers and implementations that:\n\n1. Discuss preservation of test-time training capabilities in hierarchical models\n2. Detail the interaction between temporal bottlenecks and hierarchical processing\n3. Present hardware optimization strategies for hierarchical transformers\n4. Analyze efficiency-accuracy trade-offs in hierarchical models\n5. Describe implementation details of temporal bottlenecks in transformer architectures\n\nFocus on:\n- Test-time training mechanisms\n- Integration challenges and solutions\n- Hardware efficiency considerations\n- Performance impact measurements\n- Implementation specifications",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations that:\n\n1. Discuss preservation of test-time training capabilities in hierarchical models\n2. Detail the interaction between temporal bottlenecks and hierarchical processing\n3. Present hardware optimization strategies for hierarchical transformers\n4. Analyze efficiency-accuracy trade-offs in hierarchical models\n5. Describe implementation details of temporal bottlenecks in transformer architectures\n\nFocus on:\n- Test-time training mechanisms\n- Integration challenges and solutions\n- Hardware efficiency considerations\n- Performance impact measurements\n- Implementation specifications\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Linear Attention Sequence Parallelism (Avg. Score: 0.07)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 13/24 (Score: 0.07)*\n\n```\nA., Tanaka, M., Zhang, C., Zhang, M., Song, S. L., Rajbhandari, S., and He, Y. Deepspeed Ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. Reducing activation recomputation in large transformer models, 2022.\n```\n\n#### 2. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.05)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 42/46 (Score: 0.05)*\n\n```\n[N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n## Appendix\n\n## 6 Related Work\n\nHierarchical or Multiscale Recurrent neural networks. This work takes inspiration from a wide array of work on introducing multiple scales of processing into recurrent neural networks (Chung et al. 2016; Hihi \\& Bengio, 1995; Mozer, 1991, Schmidhuber, 1991, Koutn\u00edk et al., 2014). These works divide the processing into multiple streams each operating at a different temporal granularity. While these works mainly focus on recurrent neural networks and their application is mainly on natural language tasks, we focus on introducing multiple streams of processing and a hierarchical structure into Transformers while also focusing on a broader range of domains beyond natural language. Transformers. Some of the components we describe in the proposed model have been used previously in various Transformer models. Transformer XL (Dai et al., 2019) also divides the input into segments. Each segment considers the tokens from the current segment and the previous segment for attention without passing gradients into the previous segments. A number of previous works (Zhang et al., 2021; Liu et al., 2021b, Wu et al., 2021, Yuan et al., 2021, Wang et al., 2021; Yang et al., 2021) have worked on introducing a hierarchical structure in Transformers mainly in the domain of vision. The main goal of these works has been to introduce convolution-like hierarchies into Vision Transformers (Dosovitskiy et al. 2020). While these works progressively reduce the spatial resolution of the inputs in order to introduce hierarchies, we introduce hierarchies by adding another slow stream of information processing and without reducing the spatial resolution of the inputs. We also provision for the higher level of the hierarchy (i.e. the slow stream) to provide information to the lower levels as top-down conditioning which is not possible in any of the previous works. Top-Down Conditioning. Top-down information is information propagated from higher to lower levels of the network. It represents the models beliefs of the world and provides context for interpreting perceptual information.\n```\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.02)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.02)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Soaring from 4K to 400K: Extending LLM\u2019s Context with Activation Beacon (Avg. Score: 0.01)\n\n*Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 30  (*Influential: 1*)\n\n**TL;DR:** Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts and works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference.\n\n**Abstract:** The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small training cost. Our experiment verifies Activation Beacon's effectiveness of context extension: it can remarkably accomplish high-quality extension of Llama-2-7B's context by $\\times100$ times (from 4K to 400K); meanwhile, it can also achieve superior performances across a variety of long-context language modeling and understanding tasks. The source code and model checkpoint are available at \\url{https://github.com/FlagOpen/FlagEmbedding}.\n\n##### *Relevant Chunk: No. 22/27 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2307.09288, 2023. [31] Tunstall, L., Von Werra, L., and Wolf, T. Natural language processing with transformers, 2022. [32] Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. [33] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.04768\n[34] Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. Augmenting language models with long-term memory. CoRR, abs/2306.07174, 2023. doi: 10.48550/ARXIV.2306. 07174. URL https://doi.org/10.48550/arXiv.2306.07174. [35] Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=TrjbxzRcnf-. [36] Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [37] Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. CoRR, abs/2310.03025, 2023. doi: 10.48550/ARXIV.2310.03025. URL https://doi.org/10 48550/arXiv. 2310.03025\n[38] Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences.\n```\n\n#### 5. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.01)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.01)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 13 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: \"test-time training hierarchical transformers\", \"temporal bottleneck optimization hardware\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words\n\n*From Search Query: \"test-time training hierarchical transformers\"*\n\n*Yu Bao, Srinivasan Sivanandan, Theofanis Karaletsos*\n\n**TL;DR:** ChannelViT, a modification to the ViT architecture that enhances reasoning across the input channels and introduces Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time, is proposed.\n\n**Abstract:** Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate the performance of ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat (satellite imaging). Our results show that ChannelViT outperforms ViT on classification tasks and generalizes well, even when a subset of input channels is used during testing. Across our experiments, HCS proves to be a powerful regularizer, independent of the architecture employed, suggesting itself as a straightforward technique for robust ViT training. Lastly, we find that ChannelViT generalizes effectively even when there is limited access to all channels during training, highlighting its potential for multi-channel imaging under real-world conditions with sparse sensors. Our code is available at https://github.com/insitro/ChannelViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 2. Test-Time Training on Nearest Neighbors for Large Language Models\n\n*From Search Query: \"test-time training hierarchical transformers\"*\n\n*Moritz Hardt, Yu Sun*\n\n**TL;DR:** This work establishes a first baseline of test-time training for language modeling by building a large-scale distributed index based on text embeddings of the Pile dataset and fine-tunes the model on retrieved data at test time.\n\n**Abstract:** Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 3. Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification\n\n*From Search Query: \"test-time training hierarchical transformers\"*\n\n*Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi*\n\n**TL;DR:** A novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse- grained predictions to significantly reduce mistake severity.\n\n**Abstract:** We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces avg. mistake severity while improving top-1 accuracy on the iNaturalist-19 and tieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in top-1 accuracy while significantly decreasing the severity of mistakes as training data decreases for the fine-grained classes. The simplicity and post-hoc nature of HiE renders it practical to be used with any off-the-shelf trained model to improve its predictions further.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Low-Fidelity Video Encoder Optimization for Temporal Action Localization\n\n*From Search Query: \"temporal bottleneck optimization hardware\"*\n\n*Mengmeng Xu*\n\n**TL;DR:** This paper proposes to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that end-to-end optimization for the video encoder becomes operable under the memory conditions of a mid-range hardware budget, favourably solving the task discrepancy problem and providing more effective feature representations.\n\n**Abstract:** Temporal action localization (TAL) is a fundamental yet challenging task in video understanding. Existing TAL methods rely on pre-training a video encoder through action classification supervision. This results in a task discrepancy problem for the video encoder -- trained for action classification, but used for TAL. Intuitively, end-to-end model optimization is a good solution. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity end-to-end (LoFi) video encoder pre-training method. Instead of always using the full training configurations for TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that end-to-end optimization for the video encoder becomes operable under the memory conditions of a mid-range hardware budget. Crucially, this enables the gradient to flow backward through the video encoder from a TAL loss supervision, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi pre-training approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream ResNet50 based alternatives with expensive optical flow, often by a good margin.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 24  (*Influential: 2*)\n\n#### 5. Temporal Effective Batch Normalization in Spiking Neural Networks\n\n*From Search Query: \"temporal bottleneck optimization hardware\"*\n\n*Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, Tiejun Huang*\n\n**TL;DR:** Experimental results show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Abstract:** Spiking Neural Networks (SNNs) are promising in neuromorphic hardware owing to utilizing spatio-temporal information and sparse event-driven signal processing. However, it is challenging to train SNNs due to the non-differentiable nature of the binary firing function. The surrogate gradients alleviate the training problem and make SNNs obtain comparable performance as Artificial Neural Networks (ANNs) with the same structure. Unfortunately, batch normalization, contributing to the success of ANNs, does not play a prominent role in SNNs because of the additional temporal dimension. To this end, we propose an effective normalization method called temporal effective batch normalization (TEBN). By rescaling the presynaptic inputs with different weights at every time-step, temporal distributions become smoother and uniform. Theoretical analysis shows that TEBN can be viewed as a smoother of SNN\u2019s optimization landscape and could help stabilize the gradient norm. Experimental results on both static and neuromorphic datasets show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 63  (*Influential: 7*)\n\n#### 6. Faster proximal algorithms for matrix optimization using Jacobi-based eigenvalue methods\n\n*From Search Query: \"temporal bottleneck optimization hardware\"*\n\n*Hamza Fawzi, H. Goulbourne*\n\n**TL;DR:** This paper proposes to use an old and surprisingly simple method due to Jacobi to compute eigenvalue and singular value decompositions, and demonstrates that it can lead to substantial gains in terms of computation time compared to standard approaches.\n\n**Abstract:** We consider proximal splitting algorithms for convex optimization problems over matrices. A signi\ufb01cant computational bottleneck in many of these algorithms is the need to compute a full eigenvalue or singular value decomposition at each iteration for the evaluation of a proximal operator. In this paper we propose to use an old and surprisingly simple method due to Jacobi to compute these eigenvalue and singular value decompositions, and we demonstrate that it can lead to substantial gains in terms of computation time compared to standard approaches. We rely on three essential properties of this method: (a) its ability to exploit an approximate decomposition as an initial point, which in the case of iterative optimization algorithms can be obtained from the previous iterate; (b) its parallel nature which makes it a great \ufb01t for hardware accelerators such as GPUs, now common in machine learning, and (c) its simple termination criterion which allows us to trade-off accuracy with computation time. We demonstrate the ef\ufb01cacy of this approach on a variety of algorithms and problems, and show that, on a GPU, we can obtain 5 to 10x speed-ups in the evaluation of proximal operators compared to standard CPU or GPU linear algebra routines. Our \ufb01ndings are supported by new theoretical results providing guarantees on the approximation quality of proximal operators obtained using approximate eigenvalue or singular value decompositions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 4  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. Learning Syntax Without Planting Trees: Understanding When and Why\n  Transformers Generalize Hierarchically\n\n*From Search Query: \"test-time training hierarchical transformers\"*\n\n*Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A. Smith, Navin Goyal, Yulia Tsvetkov*\n\n**Abstract:** Transformers trained on natural language data have been shown to learn its\nhierarchical structure and generalize to sentences with unseen syntactic\nstructures without explicitly encoding any structural bias. In this work, we\ninvestigate sources of inductive bias in transformer models and their training\nthat could cause such generalization behavior to emerge. We extensively\nexperiment with transformer models trained on multiple synthetic datasets and\nwith different training objectives and show that while other objectives e.g.\nsequence-to-sequence modeling, prefix language modeling, often failed to lead\nto hierarchical generalization, models trained with the language modeling\nobjective consistently learned to generalize hierarchically. We then conduct\npruning experiments to study how transformers trained with the language\nmodeling objective encode hierarchical structure. When pruned, we find joint\nexistence of subnetworks within the model with different generalization\nbehaviors (subnetworks corresponding to hierarchical structure and linear\norder). Finally, we take a Bayesian perspective to further uncover\ntransformers' preference for hierarchical generalization: We establish a\ncorrelation between whether transformers generalize hierarchically on a dataset\nand whether the simplest explanation of that dataset is provided by a\nhierarchical grammar compared to regular grammars exhibiting linear\ngeneralization.\n\n**Published:** 2024-04-25T07:10:29Z  (*Updated: 2024-05-31T23:47:15Z*)\n\n\n\n#### 2. A Hierarchical Transformer for Unsupervised Parsing\n\n*From Search Query: \"test-time training hierarchical transformers\"*\n\n*Ashok Thillaisundaram*\n\n**Abstract:** The underlying structure of natural language is hierarchical; words combine\ninto phrases, which in turn form clauses. An awareness of this hierarchical\nstructure can aid machine learning models in performing many linguistic tasks.\nHowever, most such models just process text sequentially and there is no bias\ntowards learning hierarchical structure encoded into their architecture. In\nthis paper, we extend the recent transformer model (Vaswani et al., 2017) by\nenabling it to learn hierarchical representations. To achieve this, we adapt\nthe ordering mechanism introduced in Shen et al., 2018, to the self-attention\nmodule of the transformer architecture. We train our new model on language\nmodelling and then apply it to the task of unsupervised parsing. We achieve\nreasonable results on the freely available subset of the WSJ10 dataset with an\nF1-score of about 50%.\n\n**Published:** 2020-03-30T22:07:22Z  (*Updated: 2020-03-30T22:07:22Z*)\n\n\n\n#### 3. FlashDecoding++: Faster Large Language Model Inference on GPUs\n\n*From Search Query: \"temporal bottleneck optimization hardware\"*\n\n*Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, Yu Wang*\n\n**Abstract:** As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n\n**Published:** 2023-11-02T14:57:03Z  (*Updated: 2024-01-05T12:41:13Z*)\n\n\n\n#### 4. FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems\n\n*From Search Query: \"temporal bottleneck optimization hardware\"*\n\n*Thomas Randall, Tyler Allen, Rong Ge*\n\n**Abstract:** Word2Vec remains one of the highly-impactful innovations in the field of\nNatural Language Processing (NLP) that represents latent grammatical and\nsyntactical information in human text with dense vectors in a low dimension.\nWord2Vec has high computational cost due to the algorithm's inherent\nsequentiality, intensive memory accesses, and the large vocabularies it\nrepresents. While prior studies have investigated technologies to explore\nparallelism and improve memory system performance, they struggle to effectively\ngain throughput on powerful GPUs.\n  We identify memory data access and latency as the primary bottleneck in prior\nworks on GPUs, which prevents highly optimized kernels from attaining the\narchitecture's peak performance. We present a novel algorithm, FULL-W2V, which\nmaximally exploits the opportunities for data reuse in the W2V algorithm and\nleverages GPU architecture and resources to reduce access to low memory levels\nand improve temporal locality. FULL-W2V is capable of reducing accesses to GPU\nglobal memory significantly, e.g., by more than 89\\%, compared to prior\nstate-of-the-art GPU implementations, resulting in significant performance\nimprovement that scales across successive hardware generations. Our prototype\nimplementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to\nVolta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards\nwith the same embedding quality. In-depth analysis indicates that the reduction\nof memory accesses through register and shared memory caching and\nhigh-throughput shared memory reduction leads to a significantly improved\narithmetic intensity. FULL-W2V can potentially benefit many applications in NLP\nand other domains.\n\n**Published:** 2023-12-12T21:22:07Z  (*Updated: 2023-12-12T21:22:07Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. Focal Modulation Networks\n\n*From Search Query: \"test-time training hierarchical transformers\"*\n\n*Jianfeng Gao, Lu Yuan, Xiyang Dai, Chunyuan Li, Jianwei Yang*\n\n**Abstract:** We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3. Code and checkpoints are available at https://github.com/microsoft/FocalNet.\n\n**Published:** 2022-03-22\n\n\n\n#### 2. Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline\n\n*From Search Query: \"test-time training hierarchical transformers\"*\n\n*Jin Tang, Yonghong Tian, Bo Jiang, Lin Zhu, Chuanming Tang, Shiao Wang, Xiao Wang*\n\n**Abstract:** Tracking using bio-inspired event cameras has drawn more and more attention in recent years. Existing works either utilize aligned RGB and event data for accurate tracking or directly learn an event-based tracker. The first category needs more cost for inference and the second one may be easily influenced by noisy events or sparse spatial resolution. In this paper, we propose a novel hierarchical knowledge distillation framework that can fully utilize multi-modal / multi-view information during training to facilitate knowledge transfer, enabling us to achieve high-speed and low-latency visual tracking during testing by using only event signals. Specifically, a teacher Transformer-based multi-modal tracking framework is first trained by feeding the RGB frame and event stream simultaneously. Then, we design a new hierarchical knowledge distillation strategy which includes pairwise similarity, feature representation, and response maps-based knowledge distillation to guide the learning of the student Transformer network. Moreover, since existing event-based tracking datasets are all low-resolution ($346 \\times 260$), we propose the first large-scale high-resolution ($1280 \\times 720$) dataset named EventVOT. It contains 1141 videos and covers a wide range of categories such as pedestrians, vehicles, UAVs, ping pongs, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, COESOT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. The dataset, evaluation toolkit, and source code are available on \\url{https://github.com/Event-AHU/EventVOT_Benchmark}\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2023-09-26\n\n\n\n#### 3. Spin Summations: A High-Performance Perspective\n\n*From Search Query: \"temporal bottleneck optimization hardware\"*\n\n*Anonymous*\n\n**Abstract:** Besides tensor contractions, one of the most pronounced computational\nbottlenecks in the non-orthogonally spin-adapted forms of the quantum chemistry\nmethods CCSDT and CCSDTQ, and their approximate forms---including CCSD(T) and\nCCSDT(Q)---are spin summations. At a first sight, spin summations are\noperations similar to tensor transpositions; a closer look instead reveals\nadditional challenges to high-performance calculations, including temporal\nlocality as well as scattered memory accesses. This publication explores a\nsequence of algorithmic solutions for spin summations, each exploiting\nindividual properties of either the underlying hardware (e.g. caches,\nvectorization), or the problem itself (e.g. factorizability). The final\nalgorithm combines the advantages of all the solutions, while avoiding their\ndrawbacks; this algorithm, achieves high-performance through parallelization,\nvectorization, and by exploiting the temporal locality inherent to spin\nsummations. Combined, these optimizations result in speedups between 2.4x and\n5.5x over the NCC quantum chemistry software package. In addition to such a\nperformance boost, our algorithm can perform the spin summations in-place, thus\nreducing the memory footprint by 2x over an out-of-place variant.\n\n**Published:** 2017-05-18\n\n\n\n\n\n---\n## Web search results\n\n To address the researcher's goals and the detailed analysis provided, here are some key findings and suggestions based on the given keywords and the need for specific information on test-time training, hierarchical transformers, and hardware optimization.\n\n## Test-Time Training in Hierarchical Models\n\n### Test-Time Training (TTT) Layers\nThe concept of Test-Time Training (TTT) layers, as discussed in the YouTube video and its associated paper, is crucial. TTT layers update the hidden state of the model through self-supervised learning during the test phase. This approach is particularly relevant for hierarchical models as it allows for dynamic adaptation without requiring additional task-specific annotations. The TTT layers, including TTT linear and TTT MLP, demonstrate the ability to reduce perplexity by conditioning on more tokens, which is a key aspect for autoregressive language models.\n\n## Interaction Between Temporal Bottlenecks and Hierarchical Processing\n\n### Hierarchical Attention and Temporal Context\nThe H-Transformer-1D model, which computes attention in a hierarchical manner, shows linear runtime and memory complexity. This hierarchical approach can be integrated with temporal context handling, potentially mitigating temporal bottlenecks. By breaking down the sequence into smaller blocks and processing them hierarchically, models can handle longer sequences more efficiently, which is essential for autoregressive models dealing with temporal data.\n\n### Blockwise Parallel Transformer\nThe Blockwise Parallel Transformer enables training sequences much longer than vanilla Transformers by processing blocks in parallel. This method can help in managing temporal bottlenecks by dividing the sequence into manageable blocks, allowing for more efficient processing and reducing the impact of temporal dependencies.\n\n## Hardware Optimization Strategies\n\n### FlashAttention and IO-Awareness\nFlashAttention, an IO-aware exact attention algorithm, uses tiling to reduce memory reads/writes, which is critical for hardware-efficient implementation. This approach can be integrated into hierarchical transformers to optimize memory usage and improve performance during both training and inference.\n\n### Dynamic Memory Compression\nDynamic Memory Compression (DMC) Transformers achieve significant memory optimization without compromising performance. This method can be applied to hierarchical transformers to enhance their efficiency, especially during auto-regressive inference where memory usage can be a bottleneck.\n\n## Efficiency-Accuracy Trade-offs and Implementation Details\n\n### TTT Layers Efficiency\nTTT layers, particularly TTT linear, demonstrate better efficiency in terms of computational cost (fewer FLOPS) while maintaining or improving performance compared to baselines like the Transformer and Mamba. This efficiency is crucial for large language models and can be optimized further with mini-batches and specific hardware configurations.\n\n### Hierarchical Spiking Transformers\nAlthough not directly focused on language models, the QKFormer model, which integrates Spiking Neural Networks (SNNs) with Transformer architectures, shows promising results in terms of energy efficiency and performance. The hierarchical structure and novel spike-form Q-K attention module could provide insights into designing more efficient hierarchical transformers for language tasks.\n\n## Implementation Specifications and Performance Impact\n\n### Practical Innovations in TTT Layers\nThe paper on TTT layers discusses practical innovations such as using mini-batches and a duo-form for operations, which significantly improve training speed on modern hardware. These innovations make TTT linear a practical component for large language models, highlighting the importance of implementation details in achieving both efficiency and performance.\n\n### Performance Measurements\nThe performance of these models is often measured in terms of perplexity, floating-point operations (FLOPS), and memory usage. For instance, TTT linear outperforms Mamba in perplexity while using fewer FLOPS, indicating a better efficiency-accuracy trade-off. Similar measurements should be applied to hierarchical models to ensure they meet the performance and efficiency criteria.\n\nIn summary, to improve autoregressive language model design, researchers should focus on:\n\n- Integrating Test-Time Training (TTT) layers to enable dynamic adaptation during the test phase.\n- Utilizing hierarchical attention mechanisms like H-Transformer-1D to manage temporal context efficiently.\n- Implementing blockwise parallel processing to handle longer sequences.\n- Applying hardware optimization strategies such as FlashAttention and Dynamic Memory Compression to enhance efficiency.\n- Analyzing the efficiency-accuracy trade-offs through detailed performance measurements.\n\nThese approaches can help in designing hierarchical transformers that are efficient, scalable, and perform well in both training and inference phases.\n",
            "analysis": "### Analysis Summary\n\nThe search results have provided valuable insights into hierarchical processing, temporal bottlenecks, and memory optimization in transformer architectures. However, more specific information is needed about certain aspects of the proposal.\n\n### Key References and Excerpts\n\n1. **Dynamic Memory Compression (Nawrot et al., 2024)**\n- \"DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference [...] preserves the original downstream performance with up to 4x cache compression\"\n- Demonstrates feasibility of significant memory optimization without performance loss\n\n2. **FlashAttention (Dao et al., 2022)**\n- \"IO-aware exact attention algorithm that uses tiling to reduce memory reads/writes\"\n- Provides framework for hardware-efficient implementation\n\n3. **Blockwise Parallel Transformer (Liu & Abbeel, 2023)**\n- \"enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods\"\n- Shows potential of block-based processing for efficiency\n\n4. **H-Transformer-1D (Zhu & Soricut, 2021)**\n- \"efficient hierarchical method to compute attention [...] has linear run time and memory complexity\"\n- Demonstrates successful hierarchical attention implementation\n\n### Key Insights\n\n1. **Memory Optimization Approaches**\n- Dynamic compression shows promising results (7x throughput increase)\n- Block-based processing enables longer sequence handling\n- Hardware-aware implementation crucial for efficiency\n\n2. **Hierarchical Processing**\n- Linear complexity achievable with hierarchical attention\n- Block-wise computation provides scalability benefits\n- Integration with temporal aspects needs more investigation\n\n3. **Areas Needing Further Research**\n- Specific interaction between hierarchical processing and temporal bottlenecks\n- Impact on test-time training capabilities\n- Hardware optimization strategies for combined approach\n\n### Future Search Plan\n\nNeed additional search focusing on:\n\n1. Test-time training preservation in hierarchical models\n2. Interaction between temporal bottlenecks and hierarchical processing\n3. Hardware optimization for combined architectures\n\n### References\n\n1. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems, 35, 16344-16359.\n\n2. Liu, H., & Abbeel, P. (2023). Blockwise Parallel Transformer for Large Context Models. arXiv preprint.\n\n3. Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n4. Zhu, Z., & Soricut, R. (2021). H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. Annual Meeting of the Association for Computational Linguistics.\n\n5. Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., Dou, Z. (2024). Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon. arXiv preprint.\n\nNeed more research before providing final review."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.927099,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.065603,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "\n# HierarchicalCompressiveFastTTTLinear: Enhancing FastTTTLinear with Hierarchical Processing and Adaptive State Compression\n\n## 1. Motivation\n\nCurrent language models face significant challenges in efficiently processing long sequences while maintaining expressiveness and adaptability. While FastTTTLinear addresses some efficiency concerns through test-time training capabilities, there remain opportunities for improvement through:\n- More efficient memory management for long sequences\n- Better handling of hierarchical patterns\n- Reduced computational overhead\n- Improved hardware utilization\n\n## 2. Problem Analysis\n\n### Current Limitations\n\n1. **Memory Management**:\n   - High memory requirements for storing states\n   - Inefficient compression of intermediate representations\n   - Limited ability to adapt to varying sequence lengths\n\n2. **Computational Efficiency**:\n   - Fixed computation patterns regardless of content importance\n   - Suboptimal hardware utilization\n   - Memory bandwidth bottlenecks\n\n3. **Information Flow**:\n   - Limited ability to capture hierarchical relationships\n   - Inefficient propagation of long-range dependencies\n   - Fixed state representation regardless of content\n\n## 3. Core Idea and Philosophy\n\nHierarchicalCompressiveFastTTTLinear introduces three key innovations:\n\n1. **Hierarchical State Processing**:\n   - Multi-level state representation\n   - Content-dependent processing depth\n   - Efficient information flow between levels\n\n2. **Adaptive State Compression**:\n   - Dynamic compression ratios based on content importance\n   - Hardware-aware tiling for efficient memory access\n   - Selective state propagation\n\n3. **Memory-Efficient Implementation**:\n   - IO-aware computation patterns\n   - Optimized memory hierarchy utilization\n   - Efficient parallel processing\n\n## 4. Design Plan\n\n### Architecture Overview\n\n```python\nclass HierarchicalCompressiveFastTTTLinear(nn.Module):\n    def __init__(self, embed_dim, num_levels=3, compression_ratio=0.5):\n        super().__init__()\n        self.hierarchical_processor = HierarchicalProcessor(\n            embed_dim, num_levels)\n        self.state_compressor = AdaptiveStateCompressor(\n            embed_dim, compression_ratio)\n        self.memory_manager = IOAwareMemoryManager(embed_dim)\n        \n    def forward(self, x, state=None):\n        # Process input through hierarchical levels\n        hierarchical_states = self.hierarchical_processor(x)\n        \n        # Compress states adaptively\n        compressed_states = self.state_compressor(\n            hierarchical_states)\n        \n        # Manage memory efficiently\n        output = self.memory_manager(compressed_states)\n        \n        return output\n```\n\n### Key Components\n\n1. **Hierarchical Processor**:\n```python\nclass HierarchicalProcessor(nn.Module):\n    def __init__(self, dim, num_levels):\n        super().__init__()\n        self.levels = nn.ModuleList([\n            LevelProcessor(dim, level_idx)\n            for level_idx in range(num_levels)\n        ])\n        \n    def forward(self, x):\n        states = []\n        current = x\n        \n        # Process through levels\n        for level in self.levels:\n            current = level(current)\n            states.append(current)\n            \n        return self.combine_states(states)\n```\n\n2. **Adaptive State Compressor**:\n```python\nclass AdaptiveStateCompressor(nn.Module):\n    def __init__(self, dim, base_ratio):\n        super().__init__()\n        self.importance_estimator = nn.Linear(dim, 1)\n        self.compression_net = nn.Linear(dim, dim // 2)\n        \n    def forward(self, states):\n        # Compute importance scores\n        scores = self.importance_estimator(states)\n        \n        # Apply adaptive compression\n        compression_mask = self.compute_mask(scores)\n        compressed = self.compress_states(\n            states, compression_mask)\n            \n        return compressed\n```\n\n3. **IO-Aware Memory Manager**:\n```python\nclass IOAwareMemoryManager(nn.Module):\n    def __init__(self, dim, block_size=256):\n        super().__init__()\n        self.block_size = block_size\n        self.processor = nn.Linear(dim, dim)\n        \n    def forward(self, states):\n        # Process in memory-efficient blocks\n        blocks = self.split_into_blocks(states)\n        processed = []\n        \n        for block in blocks:\n            result = self.process_block(block)\n            processed.append(result)\n            \n        return self.combine_blocks(processed)\n```\n\n## 5. Implementation Guidelines\n\n1. **Initialization**:\n```python\ndef init_hierarchical_layers(module):\n    if isinstance(module, HierarchicalProcessor):\n        for level in module.levels:\n            nn.init.orthogonal_(level.weight)\n```\n\n2. **Training Configuration**:\n```python\nconfig = {\n    'num_levels': 3,\n    'base_compression_ratio': 0.5,\n    'block_size': 256,\n    'learning_rate': 1e-4\n}\n```\n\n3. **Memory Management**:\n```python\ndef optimize_memory_access(states, block_size):\n    if states.shape[1] > block_size:\n        return process_in_blocks(states, block_size)\n    return states\n```\n\n## 6. Theoretical Analysis\n\n1. **Computational Complexity**:\n- Hierarchical processing: O(n log n) where n is sequence length\n- State compression: O(n) with adaptive ratio\n- Overall: Linear in sequence length\n\n2. **Memory Usage**:\n- Compressed states: O(n * r) where r is compression ratio\n- Hierarchical states: O(n * log n)\n- Total: Sublinear in sequence length\n\n3. **Information Flow**:\n- Efficient gradient propagation through skip connections\n- Stable updates through normalized processing\n- Effective long-range dependency modeling\n\n## 7. Research Summary\n\nKey findings from research:\n1. Hierarchical processing improves efficiency\n2. Adaptive compression reduces memory usage\n3. IO-aware implementation crucial for performance\n4. Hardware-aware design essential for real-world applications\n\n## 8. Evolution of Design\n\nThe design evolved through several iterations:\n1. Initial focus on hierarchical processing\n2. Addition of adaptive compression\n3. Integration of IO-aware computation\n4. Optimization for hardware efficiency\n\n## 9. References\n\n[1] Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\". Neural Information Processing Systems.\n\n[2] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". arXiv preprint.\n\n[3] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\". International Conference on Machine Learning.\n\n[4] Le Bronnec, F., et al. (2024). \"LOCOST: State-Space Models for Long Document Abstractive Summarization\". Conference of the European Chapter of the Association for Computational Linguistics.\n\n## 10. Conclusion\n\nHierarchicalCompressiveFastTTTLinear represents a significant advancement in language model architecture by efficiently combining hierarchical processing with adaptive state compression. The design addresses key limitations of existing approaches while maintaining computational efficiency and improving model expressiveness. Through careful integration of hardware-aware optimizations and efficient parallel processing, the architecture provides a foundation for building more efficient and powerful language models.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "HierarchicalCompressiveFastTTTLinear enhances FastTTTLinear by introducing hierarchical processing and adaptive state compression mechanisms. The design incorporates a multi-level state representation with dynamic compression ratios, hardware-aware tiling strategies for efficient memory access, and selective state propagation. This combination enables more efficient processing of long sequences while maintaining model quality and test-time training capabilities.",
    "ideation": null,
    "modelname": "hierarchicalcompressivettt",
    "suggestions": "1. Detailed Implementation Strategy:\n- Provide more specific details about preserving test-time training capabilities\n- Include concrete examples of hardware-specific optimizations\n- Elaborate on the interaction between hierarchical processing and temporal bottlenecks\n\n2. Memory Management:\n- Add specific compression ratio guidelines\n- Include adaptive thresholds for state compression\n- Detail recovery mechanisms for compressed states\n\n3. Performance Validation:\n- Define specific metrics for evaluating compression impact\n- Include ablation study guidelines\n- Propose benchmark suite for evaluation\n\n4. Hardware Optimization:\n- Add specific memory access pattern optimizations\n- Include parallel processing strategies\n- Detail hardware-specific implementation considerations\n\n5. Stability Considerations:\n- Add gradient flow analysis through hierarchical layers\n- Include stability analysis for compressed states\n- Propose mechanisms for handling edge cases\n\n6. Integration Guidelines:\n- Provide step-by-step integration process\n- Include compatibility checks with existing systems\n- Detail migration strategies for existing models",
    "user_input": ""
}