{
    "implementation": {
        "review": null,
        "root": "TTT",
        "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
        "proposal_traces": [],
        "rating": null,
        "declares": {
            "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
            "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
        },
        "units": {
            "TTT": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "FastTTTLinear",
                    "SwiGluMLP",
                    "RMSNorm",
                    "Conv"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "FastTTTLinear": {
                "review": "# Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.5```\n\n## 1. Strengths of the Implementation\n\n1. **Efficient Architecture Design**:\n   - Clean separation of components with well-defined interfaces\n   - Effective use of linear attention mechanisms\n   - Smart integration of local convolution for enhanced context modeling\n\n2. **Optimization Features**:\n   - Vectorized operations throughout\n   - Efficient memory management with caching system\n   - Careful handling of numerical stability\n\n3. **Code Quality**:\n   - Comprehensive documentation\n   - Clear variable naming\n   - Robust input validation\n   - Well-structured methods with single responsibilities\n\n4. **Technical Innovations**:\n   - Integration of gated linear attention with test-time training\n   - Adaptive computation through gating mechanisms\n   - Efficient state management system\n\n## 2. Areas for Improvement\n\n1. **Performance Optimization**:\n```python\ndef _compute_attention_efficient(self, Q, K, V, chunk_size=1024):\n    \"\"\"Compute attention with memory-efficient chunking.\"\"\"\n    B, H, L, D = Q.shape\n    outputs = []\n    \n    for i in range(0, L, chunk_size):\n        chunk_end = min(i + chunk_size, L)\n        Q_chunk = Q[:, :, i:chunk_end, :]\n        K_chunk = K[:, :, :chunk_end, :]\n        V_chunk = V[:, :, :chunk_end, :]\n        \n        # Compute attention for current chunk\n        QK = torch.matmul(Q_chunk, K_chunk.transpose(-2, -1))\n        QK = QK / math.sqrt(D)\n        A = F.softmax(QK, dim=-1)\n        chunk_output = torch.matmul(A, V_chunk)\n        outputs.append(chunk_output)\n    \n    return torch.cat(outputs, dim=2)\n```\n\n2. **Enhanced Caching System**:\n```python\nclass CacheManager:\n    \"\"\"Manages KV cache with automatic pruning.\"\"\"\n    def __init__(self, max_len=50000):\n        self.max_len = max_len\n        self.cache_k = None\n        self.cache_v = None\n        \n    def update(self, key, value):\n        if self.cache_k is None:\n            self.cache_k = key\n            self.cache_v = value\n            return\n            \n        self.cache_k = torch.cat([self.cache_k, key], dim=2)\n        self.cache_v = torch.cat([self.cache_v, value], dim=2)\n        \n        if self.cache_k.size(2) > self.max_len:\n            self.cache_k = self.cache_k[:, :, -self.max_len:]\n            self.cache_v = self.cache_v[:, :, -self.max_len:]\n```\n\n3. **Gradient Flow Optimization**:\n```python\ndef _gated_activation(self, x):\n    \"\"\"Enhanced activation with better gradient properties.\"\"\"\n    gate = torch.sigmoid(self.gate_proj(x))\n    value = F.gelu(self.value_proj(x))\n    return gate * value\n```\n\n## 3. Innovation and Impact Assessment\n\n### Innovative Aspects:\n1. **Adaptive Computation**:\n   - Dynamic feature selection through gating\n   - Content-dependent attention patterns\n   - Efficient test-time adaptation\n\n2. **Memory Efficiency**:\n   - Linear complexity attention\n   - Smart caching mechanisms\n   - Gradient checkpointing support\n\n3. **Architecture Design**:\n   - Integration of multiple attention mechanisms\n   - Flexible state management\n   - Robust normalization strategy\n\n### Potential Impact:\n1. **Scalability**:\n   - Linear complexity enables processing of very long sequences\n   - Efficient memory usage allows larger batch sizes\n   - Adaptive computation provides better resource utilization\n\n2. **Performance**:\n   - Improved efficiency through vectorized operations\n   - Better feature selection through gating\n   - Enhanced context modeling with local convolution\n\n3. **Adaptability**:\n   - Test-time training capabilities\n   - Dynamic feature selection\n   - Flexible attention patterns\n\n## 4. Integration and Scalability Considerations\n\n### Strengths:\n1. **Clean Interface**:\n   - Well-defined input/output specifications\n   - Proper handling of intermediate variables\n   - Clear separation of concerns\n\n2. **Resource Management**:\n   - Efficient memory usage\n   - Scalable attention computation\n   - Smart caching system\n\n### Potential Challenges:\n1. **Memory Management**:\n   - Need careful handling of cache growth\n   - Potential memory fragmentation\n   - Cache invalidation strategies\n\n2. **Computational Overhead**:\n   - Multiple normalization layers\n   - Gating computations\n   - State management overhead\n\n## 5. Recommendations for the Coder\n\n1. **Add Performance Monitoring**:\n```python\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = defaultdict(list)\n        \n    def log_metric(self, name, value):\n        self.metrics[name].append(value)\n        \n    def get_summary(self):\n        return {k: np.mean(v) for k, v in self.metrics.items()}\n```\n\n2. **Implement Adaptive Pruning**:\n```python\ndef _prune_attention(self, attention_weights, threshold=0.01):\n    \"\"\"Prune attention weights below threshold.\"\"\"\n    mask = attention_weights < threshold\n    return attention_weights.masked_fill(mask, 0.0)\n```\n\n3. **Add Diagnostic Tools**:\n```python\ndef diagnose_attention_patterns(self, save_dir=None):\n    \"\"\"Analyze and visualize attention patterns.\"\"\"\n    stats = {\n        'sparsity': [],\n        'max_attention': [],\n        'entropy': []\n    }\n    # Implement attention pattern analysis\n    return stats\n```\n\n4. **Enhance Documentation**:\n   - Add more usage examples\n   - Document performance characteristics\n   - Provide benchmarking results\n\n5. **Consider Extensions**:\n   - Implement quantization support\n   - Add sparse computation paths\n   - Support for mixed precision training\n\nThe implementation shows excellent promise in advancing efficient attention mechanisms for language models. The combination of linear attention, gating, and test-time training capabilities provides a strong foundation for scalable and adaptive language modeling. With the suggested improvements, particularly in performance optimization and monitoring, this implementation could offer significant advantages in both efficiency and effectiveness.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "unit_test_fasttttlinear": "@gau_test\ndef test_FastTTTLinear_unit_test_fasttttlinear(device=None, dtype=None) ->None:\n    \"\"\"\n    Unit test for FastTTTLinear GAU.\n    \n    Args:\n        device: The device to run the test on.\n        dtype: The data type of the tensors.\n    \n    Raises:\n        AssertionError: If the output shape is incorrect or numerical properties fail.\n    \"\"\"\n    embed_dim = 512\n    block_loc = 0, 0\n    num_heads = 8\n    head_dim = embed_dim // num_heads\n    fast_ttt_linear = FastTTTLinear(embed_dim=embed_dim, block_loc=\n        block_loc, kwarg_all={}, device=device, dtype=dtype,\n        num_attention_heads=num_heads, mini_batch_size=16)\n    fast_ttt_linear.eval()\n    batch_size = 2\n    seq_len = 1024\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    inv_freq = 1.0 / 10000 ** (torch.arange(0, head_dim, 2, dtype=torch.\n        float32, device=device) / head_dim)\n    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n    freqs = torch.einsum('i,j->ij', t, inv_freq)\n    emb = torch.cat([freqs, freqs], dim=-1)\n    cos = emb.cos().unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads,\n        -1, -1)\n    sin = emb.sin().unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads,\n        -1, -1)\n    Z = {'cos': cos, 'sin': sin}\n    with torch.no_grad():\n        Y, updated_Z = fast_ttt_linear(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(updated_Z, dict\n        ), 'Output intermediate variables Z must be a dict'\n    assert 'cos' in updated_Z and 'sin' in updated_Z, 'Rotary embeddings must be present in Z'\n    print('FastTTTLinear unit test passed.')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n    \n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n    \n    **Key Features:**\n    \n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n    - **Gradient Checkpointing**: Supports gradient checkpointing to reduce memory usage during training.\n    - **Mixed Precision Support**: Compatible with automatic mixed precision for improved performance.\n    - **State Management**: Provides methods to get and update state dictionaries for test-time training.\n    - **Caching Mechanism**: Implements key-value caching for efficient inference on long sequences.\n    - **Validation Methods**: Includes input validation to ensure correctness of inputs and configurations.\n    \n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n        mini_batch_size (int, optional): Mini-batch size for test-time training. Default: 16.\n        rope_theta (float, optional): Scaling factor for rotary embeddings. Default: 10000.0.\n        ttt_base_lr (float, optional): Base learning rate for test-time training. Default: 1.0.\n        gradient_checkpointing (bool, optional): Enable gradient checkpointing to save memory. Default: False.\n        use_amp (bool, optional): Use Automatic Mixed Precision for faster computation. Default: False.\n    \n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **cos**: Cosine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\n        - **sin**: Sine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\n    \n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Updated intermediate variables dictionary.\n    \n    **Example:**\n    \n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> cos = torch.randn(2, 8, 1024, 64)\n        >>> sin = torch.randn(2, 8, 1024, 64)\n        >>> Y, Z = fast_ttt_linear(X, cos=cos, sin=sin)\n    \n    **References:**\n    \n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads: int=4,\n        mini_batch_size: int=16, rope_theta: float=10000.0, ttt_base_lr:\n        float=1.0, gradient_checkpointing: bool=False, use_amp: bool=False,\n        **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.embed_dim = embed_dim\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.post_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n        self.mini_batch_size = mini_batch_size\n        self.rope_theta = rope_theta\n        self.ttt_base_lr = ttt_base_lr\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_amp = use_amp\n        self.register_buffer('cache_k', torch.zeros(0))\n        self.register_buffer('cache_v', torch.zeros(0))\n\n    def rotate_half(self, x):\n        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n        x1, x2 = x.chunk(2, dim=-1)\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Applies rotary positional embeddings to query and key tensors.\n        \n        Args:\n            q: Query tensor of shape (batch, num_heads, seq_len, head_dim)\n            k: Key tensor of shape (batch, num_heads, seq_len, head_dim)\n            cos: Cosine of position embeddings tensor\n            sin: Sine of position embeddings tensor\n            \n        Returns:\n            Tuple of rotated query and key tensors\n        \"\"\"\n        if cos is not None and sin is not None:\n            q_rot = q * cos + self.rotate_half(q) * sin\n            k_rot = k * cos + self.rotate_half(k) * sin\n            return q_rot, k_rot\n        return q, k\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of FastTTTLinear.\n        \n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (e.g., 'cos', 'sin').\n        \n        Returns:\n            Y (torch.Tensor): Output tensor of shape (batch_size, seq_len, embed_dim).\n            Z' (dict): Updated intermediate variables.\n        \"\"\"\n        self.validate_inputs(X, **Z)\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_conv = self.local_conv(X.transpose(1, 2))\n        X_conv = X_conv.transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        cos = Z.get('cos', None)\n        sin = Z.get('sin', None)\n        Q, K = self.apply_rotary_pos_emb(Q, K, cos, sin)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        QV = Q_prime * V\n        K_cumsum = K_prime.cumsum(dim=2)\n        QV_cumsum = (K_prime * V).cumsum(dim=2)\n        denominator = (Q_prime * K_cumsum).sum(dim=-1, keepdim=True) + 1e-06\n        numerator = Q_prime * QV_cumsum\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.post_norm(output, **Z)\n        return output, Z\n\n    def get_state_dict_custom(self):\n        \"\"\"Returns stateful parameters for test-time training.\"\"\"\n        return {'gate_Q': self.gate_Q.state_dict(), 'gate_K': self.gate_K.\n            state_dict(), 'post_norm': self.post_norm.state_dict()}\n\n    def update_state_custom(self, state_dict):\n        \"\"\"Updates stateful parameters during test-time training.\"\"\"\n        self.gate_Q.load_state_dict(state_dict['gate_Q'])\n        self.gate_K.load_state_dict(state_dict['gate_K'])\n        self.post_norm.load_state_dict(state_dict['post_norm'])\n\n    def cache_kv(self, key: torch.Tensor, value: torch.Tensor):\n        \"\"\"Caches key-value pairs for efficient inference.\"\"\"\n        if self.cache_k.nelement() == 0 or self.cache_k.size(2) < key.size(2):\n            self.cache_k = key.detach()\n            self.cache_v = value.detach()\n        else:\n            self.cache_k = torch.cat([self.cache_k, key.detach()], dim=2)\n            self.cache_v = torch.cat([self.cache_v, value.detach()], dim=2)\n\n    def get_cached_kv(self):\n        \"\"\"Retrieves cached key-value pairs.\"\"\"\n        return self.cache_k, self.cache_v\n\n    def validate_inputs(self, X: torch.Tensor, **Z):\n        \"\"\"Validates input tensors and configurations.\"\"\"\n        assert X.dim(\n            ) == 3, f'Expected 3D input (batch, seq_len, dim), got {X.dim()}D'\n        assert X.size(-1\n            ) == self.embed_dim, f'Expected embed_dim {self.embed_dim}, got {X.size(-1)}'\n        if 'cos' in Z or 'sin' in Z:\n            assert Z['cos'].shape == Z['sin'\n                ].shape, \"Mismatched shapes for 'cos' and 'sin'\"\n            assert Z['cos'].dim(\n                ) == 4, f\"'cos' and 'sin' should be 4D tensors, got {Z['cos'].dim()}D\"\n\n    def analyze_attention_patterns(self, save_path: str=None):\n        \"\"\"Analyzes and optionally saves attention pattern statistics.\"\"\"\n        pass\n",
                "rating": 4.5,
                "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n- **Gradient Checkpointing**: Supports gradient checkpointing to reduce memory usage during training.\\n- **Mixed Precision Support**: Compatible with automatic mixed precision for improved performance.\\n- **State Management**: Provides methods to get and update state dictionaries for test-time training.\\n- **Caching Mechanism**: Implements key-value caching for efficient inference on long sequences.\\n- **Validation Methods**: Includes input validation to ensure correctness of inputs and configurations.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n    mini_batch_size (int, optional): Mini-batch size for test-time training. Default: 16.\\n    rope_theta (float, optional): Scaling factor for rotary embeddings. Default: 10000.0.\\n    ttt_base_lr (float, optional): Base learning rate for test-time training. Default: 1.0.\\n    gradient_checkpointing (bool, optional): Enable gradient checkpointing to save memory. Default: False.\\n    use_amp (bool, optional): Use Automatic Mixed Precision for faster computation. Default: False.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **cos**: Cosine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\\n    - **sin**: Sine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z'**: Updated intermediate variables dictionary.\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> cos = torch.randn(2, 8, 1024, 64)\\n    >>> sin = torch.randn(2, 8, 1024, 64)\\n    >>> Y, Z = fast_ttt_linear(X, cos=cos, sin=sin)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "rope_theta": 10000.0,
                    "mini_batch_size": 16,
                    "num_attention_heads": 4,
                    "gradient_checkpointing": false,
                    "ttt_base_lr": 1.0,
                    "use_amp": false
                },
                "design_traces": null
            },
            "Conv": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "conv_kernel": 4,
                    "rms_norm_eps": 1e-06
                },
                "design_traces": null
            },
            "RotaryEmbedding": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rotaryembedding": "@gau_test\ndef test_RotaryEmbedding_test_rotaryembedding(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rotaryembedding = RotaryEmbedding(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rotaryembedding(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass RotaryEmbedding(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, dim=None, max_position_embeddings=16, base\n        =10000, scaling_factor=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scaling_factor = scaling_factor\n        self.dim = dim if dim is not None else embed_dim // 4\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n            torch.int64).float().to(device) / self.dim)\n        self.register_buffer('inv_freq', inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def _forward(self, X, input, position_ids, **Z):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = input.device.type\n        device_type = device_type if isinstance(device_type, str\n            ) and device_type != 'mps' else 'cpu'\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n                ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        Z['cos'] = cos.to(**self.factory_kwargs)\n        Z['sin'] = sin.to(**self.factory_kwargs)\n        return X, Z\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RotaryEmbedding\",\"document\":\"\\nRotaryEmbedding\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "scaling_factor": 1.0,
                    "dim": null,
                    "base": 10000,
                    "max_position_embeddings": 16
                },
                "design_traces": null
            },
            "RMSNorm": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "eps": 1e-05
                },
                "design_traces": null
            },
            "TTTLinear": {
                "review": "```rating 4.2```\n\n### **Strengths of the Implementation**\n\n1. **Alignment with Proposal**:\n   - **Integration of Gated Linear Attention (GLA)**: The `FastTTTLinear` GAU successfully incorporates GLA, utilizing data-dependent gates (`gate_Q` and `gate_K`) to modulate queries and keys. This aligns perfectly with the proposal's goal to enhance TTTLinear with GLA for improved efficiency and scalability.\n   - **Incorporation of RWKV Concepts**: The implementation draws inspiration from the RWKV architecture, maintaining stateful representations and enabling efficient recursive updates, which is crucial for test-time training capabilities.\n\n2. **Efficiency and Vectorization**:\n   - **Vectorized Operations**: The GAU eliminates Python-level for-loops by leveraging efficient tensor operations, ensuring that attention computations are optimized for performance.\n   - **Causal Attention Mechanism**: By using cumulative sum operations for causal masking, the implementation maintains linear complexity, which is essential for handling long sequences efficiently.\n\n3. **Numerical Stability and Normalization**:\n   - **Layer Normalization**: Applying `LayerNorm` to queries and keys stabilizes computations and helps maintain gradient flow during training.\n   - **Residual Connections**: The use of residual connections ensures that gradients can flow seamlessly through the network, aiding in stable and efficient training.\n\n4. **Comprehensive Documentation**:\n   - **Detailed Docstrings**: Each class and method is accompanied by thorough docstrings that elucidate functionality, arguments, inputs, outputs, and references. This enhances code readability and maintainability.\n\n5. **Successful Functionality Checks**:\n   - **Format and Functionality Compliance**: The implementation has passed both format and functionality checks, indicating adherence to the required structure and correct integration within the larger language model framework.\n\n### **Areas for Improvement and Specific Suggestions**\n\n1. **Optimization of Attention Computations**:\n   - **Replace `torch.einsum` with More Efficient Operations**: While `torch.einsum` provides flexibility, it can be computationally intensive. Consider using `torch.matmul` or other optimized tensor operations to enhance performance, especially for large batch sizes or sequence lengths.\n   \n2. **Enhancing Numerical Stability**:\n   - **Guard Against Division by Zero**: Although `epsilon` is added to the denominator in the attention computation, ensure that all potential sources of numerical instability are addressed, especially when dealing with very small variances or large sequence lengths.\n   - **Activation Function Alternatives**: Explore alternative activation functions beyond ELU that might offer better stability or performance in specific scenarios.\n\n3. **Comprehensive Testing**:\n   - **Expand Unit Tests**: Develop more extensive unit tests that cover a wider range of scenarios, including edge cases like extremely long sequences, varying batch sizes, and different embedding dimensions. This will ensure robustness and identify potential issues early.\n   - **Integration Testing**: Beyond isolated unit tests, perform integration tests to validate how `FastTTTLinear` interacts with other GAUs and the overall model, ensuring seamless functionality during both forward and backward passes.\n\n4. **Memory Optimization**:\n   - **Efficient Handling of Rotary Embeddings**: Rotary embeddings can be memory-intensive. Investigate ways to optimize their implementation, such as caching repeated computations or utilizing more memory-efficient data structures.\n   - **Batch Processing Enhancements**: Optimize memory usage during batch processing, especially when dealing with mini-batches, to prevent potential bottlenecks in training or inference.\n\n5. **Documentation Enhancements**:\n   - **Mathematical Formulations**: Incorporate mathematical equations and formulations within the docstrings to provide a clearer understanding of the attention mechanisms and transformations being applied.\n   - **Usage Examples**: Provide more comprehensive usage examples, including common pitfalls and best practices, to aid other developers in effectively utilizing the GAU.\n\n6. **Parameter Initialization and Training Stabilization**:\n   - **Advanced Initialization Strategies**: Beyond Xavier uniform initialization, explore other initialization strategies that might offer better convergence properties for specific layers.\n   - **Gradient Clipping**: Implement gradient clipping to prevent exploding gradients, especially during the test-time training updates, enhancing training stability.\n\n### **Comments on Innovation and Potential Impact**\n\n- **Innovative Integration**:\n  - **GLA and RWKV Synergy**: The combination of Gated Linear Attention and RWKV-inspired stateful representations represents a significant innovation, potentially offering the best of both worlds\u2014expressiveness and efficiency.\n  \n- **Scalability**:\n  - **Linear Complexity**: By reducing attention complexity from quadratic to linear with respect to sequence length, `FastTTTLinear` is well-positioned to handle extremely long-context scenarios, a critical requirement for state-of-the-art language models.\n  \n- **Performance Gains**:\n  - **Efficient Long-Context Processing**: The GAU is expected to provide substantial improvements in processing long sequences, both in terms of speed and memory consumption, thereby enabling more extensive and nuanced language understanding.\n  \n- **Potential Risks and Concerns**:\n  - **Integration Complexity**: Introducing a highly specialized GAU like `FastTTTLinear` may introduce complexities in integration, especially if downstream or upstream GAUs have differing assumptions or requirements.\n  - **Training Dynamics**: The test-time training aspect adds an additional layer of complexity to the training dynamics, which might require careful tuning to prevent issues like overfitting or unstable updates.\n\n### **Recommendations for the Coder**\n\n1. **Focus on Optimization**:\n   - Prioritize optimizing tensor operations to ensure that the GAU not only aligns with theoretical efficiency gains but also realizes them in practical implementations.\n\n2. **Enhance Testing Protocols**:\n   - Develop and incorporate a comprehensive suite of unit and integration tests to validate the GAU's functionality across diverse scenarios, ensuring robustness and reliability.\n\n3. **Improve Documentation**:\n   - Expand the existing documentation to include mathematical underpinnings and detailed usage guidelines, facilitating easier adoption and understanding by other team members.\n\n4. **Monitor Training and Inference Performance**:\n   - Continuously monitor performance metrics during training and inference to identify and address any emerging issues related to speed, memory usage, or stability.\n\n5. **Collaborate on Integration**:\n   - Work closely with other team members responsible for different GAUs to ensure that `FastTTTLinear` integrates seamlessly within the broader model architecture, maintaining consistency and compatibility.\n\n6. **Explore Further Innovations**:\n   - Investigate additional enhancements, such as exploring different gating mechanisms or integrating alternative positional encoding techniques, to further boost the GAU's performance and versatility.\n\nBy addressing these areas, the implementation of `FastTTTLinear` can be refined to fully realize its potential, contributing significantly to the advancement of the language model's capabilities in efficiency, scalability, and performance.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "test_ttt_linear": "@gau_test\ndef test_TTTLinear_test_ttt_linear(device=None, dtype=None) ->None:\n    \"\"\"Unit test for TTTLinear\"\"\"\n    embed_dim = 64\n    block_loc = 0, 0\n    kwarg_all = {}\n    ttt_linear = TTTLinear(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all=kwarg_all, device=device, dtype=dtype)\n    B, L, D = 2, 16, embed_dim\n    X = torch.randn(B, L, D, device=device, dtype=dtype)\n    Y, Z = ttt_linear(X)\n    assert Y.shape == X.shape, f'Expected output shape {X.shape}, but got {Y.shape}'\n    print('TTTLinear unit test passed.')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\nfrom typing import Any, Dict, Optional, Tuple, Union\n\n\nclass TTTLinear(GAUBase):\n    \"\"\"\n    TTTLinear: A Test-Time Training Linear GAU.\n\n    This unit implements a linear attention mechanism with test-time training (TTT),\n    updating its hidden states during inference via self-supervised learning steps.\n\n    **Key Features:**\n\n    - Utilizes a linear model as the hidden state, updated via gradient steps.\n    - Employs a mini-batch TTT approach for hardware efficiency.\n    - Incorporates rotary embeddings for positional information.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **position_ids**: Positional IDs tensor of shape (batch_size, seq_len).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example:**\n\n        ```python\n        ttt_linear = TTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        X = torch.randn(2, 1024, 512)\n        Y, Z = ttt_linear(X)\n        ```\n\n    **References:**\n\n    - Sun, Y., et al. (2023). *Learning to (Learn at Test Time): RNNs with Expressive Hidden States*.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, mini_batch_size=16, rope_theta=10000.0,\n        ttt_base_lr=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.head_dim = self.hidden_size // self.num_heads\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.learnable_ttt_lr_weight = nn.Parameter(torch.zeros(self.\n            num_heads, self.head_dim, **self.factory_kwargs))\n        self.learnable_ttt_lr_bias = nn.Parameter(torch.zeros(self.\n            num_heads, 1, **self.factory_kwargs))\n        self.ttt_norm_weight = nn.Parameter(torch.ones(self.num_heads, self\n            .head_dim, **self.factory_kwargs))\n        self.ttt_norm_bias = nn.Parameter(torch.zeros(self.num_heads, self.\n            head_dim, **self.factory_kwargs))\n        self.W1 = nn.Parameter(torch.zeros(self.num_heads, self.head_dim,\n            self.head_dim, **self.factory_kwargs))\n        self.b1 = nn.Parameter(torch.zeros(self.num_heads, 1, self.head_dim,\n            **self.factory_kwargs))\n        token_idx = 1.0 / torch.arange(1, self.mini_batch_size + 1, **self.\n            factory_kwargs)\n        self.register_buffer('token_idx', token_idx)\n        self.learnable_token_idx = nn.Parameter(torch.zeros(self.\n            mini_batch_size, **self.factory_kwargs))\n        self.rotary_emb = RotaryEmbedding(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.post_norm = nn.LayerNorm(self.hidden_size, eps=1e-06, **self.\n            factory_kwargs)\n        self.num_heads = num_attention_heads\n        self.mini_batch_size = mini_batch_size\n        self.rope_theta = rope_theta\n        self.ttt_base_lr = ttt_base_lr\n\n    def rotate_half(self, x):\n        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n        x1, x2 = x.chunk(2, dim=-1)\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Applies rotary positional embeddings to query and key.\"\"\"\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n        return q_rot, k_rot\n\n    def _forward(self, X, position_ids=None, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        mini_batch_size = self.mini_batch_size\n        if position_ids is None:\n            position_ids = torch.arange(L, dtype=torch.long, device=X.device\n                ).unsqueeze(0)\n        Z['position_ids'] = position_ids % self.mini_batch_size\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        _, Z = self.rotary_emb(X, **Z)\n        cos = Z['cos'].unsqueeze(1)\n        sin = Z['sin'].unsqueeze(1)\n        Q, K = self.apply_rotary_pos_emb(Q, K, cos, sin)\n        scores = torch.einsum('bhld,bhmd->bhlm', Q, K) / D_H ** 0.5\n        attention = torch.softmax(scores, dim=-1)\n        context = torch.einsum('bhlm,bhmd->bhld', attention, V)\n        context = context.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.o_proj(context)\n        output = self.post_norm(output)\n        return output, Z\n",
                "rating": 4.2,
                "spec": "{\"unitname\":\"TTTLinear\",\"document\":\"TTTLinear: A Test-Time Training Linear GAU.\\n\\nThis unit implements a linear attention mechanism with test-time training (TTT),\\nupdating its hidden states during inference via self-supervised learning steps.\\n\\n**Key Features:**\\n\\n- Utilizes a linear model as the hidden state, updated via gradient steps.\\n- Employs a mini-batch TTT approach for hardware efficiency.\\n- Incorporates rotary embeddings for positional information.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **position_ids**: Positional IDs tensor of shape (batch_size, seq_len).\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n\\n**Example:**\\n\\n    ```python\\n    ttt_linear = TTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    X = torch.randn(2, 1024, 512)\\n    Y, Z = ttt_linear(X)\\n    ```\\n\\n**References:**\\n\\n- Sun, Y., et al. (2023). *Learning to (Learn at Test Time): RNNs with Expressive Hidden States*.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                "children": [
                    "RotaryEmbedding"
                ],
                "suggestions": null,
                "args": {
                    "rope_theta": 10000.0,
                    "mini_batch_size": 16,
                    "ttt_base_lr": 1.0
                },
                "design_traces": null
            },
            "SwiGluMLP": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "intermediate_size": null
                },
                "design_traces": null
            }
        },
        "suggestions": null,
        "name": "adaptivesparsettt"
    },
    "status": "implemented",
    "history": [
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "# Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.2```\n\n## 1. Strengths of the Implementation\n\n1. **Efficient Attention Mechanism**:\n   - Successfully implements gated linear attention with O(n) complexity\n   - Vectorized operations eliminate Python-level loops\n   - Smart use of cumulative sums for efficient attention computation\n\n2. **Robust Architecture Design**:\n   - Clean separation of concerns between attention, gating, and normalization\n   - Well-structured initialization with proper weight initialization\n   - Effective use of both LayerNorm and RMSNorm for different components\n\n3. **Numerical Stability**:\n   - Careful handling of numerical stability with epsilon terms\n   - Proper normalization of attention scores\n   - Safe handling of edge cases in rotary embeddings\n\n4. **Memory Efficiency**:\n   - Efficient tensor operations minimize memory overhead\n   - In-place operations where possible\n   - Smart reuse of intermediate tensors\n\n5. **Code Quality**:\n   - Comprehensive docstrings and comments\n   - Clear variable naming conventions\n   - Proper error handling and input validation\n\n## 2. Areas for Improvement\n\n1. **Performance Optimization**:\n```python\n# Add gradient checkpointing support\ndef forward(self, x, use_checkpoint=False):\n    if use_checkpoint and self.training:\n        return torch.utils.checkpoint.checkpoint(self._forward, x)\n    return self._forward(x)\n\n# Add mixed precision support\n@torch.cuda.amp.autocast()\ndef _forward(self, X, **Z):\n    # existing implementation\n```\n\n2. **State Management**:\n```python\n# Add state management methods\ndef get_state_dict(self):\n    \"\"\"Returns stateful parameters for test-time training\"\"\"\n    return {\n        'gate_Q': self.gate_Q.state_dict(),\n        'gate_K': self.gate_K.state_dict(),\n        'norm': self.norm.state_dict()\n    }\n\ndef update_state(self, state_dict):\n    \"\"\"Updates stateful parameters during test-time training\"\"\"\n    self.gate_Q.load_state_dict(state_dict['gate_Q'])\n    self.gate_K.load_state_dict(state_dict['gate_K'])\n    self.norm.load_state_dict(state_dict['norm'])\n```\n\n3. **Caching Mechanism**:\n```python\n# Add key-value caching for inference\ndef cache_kv(self, key, value, layer_idx):\n    \"\"\"Caches key-value pairs for efficient inference\"\"\"\n    if not hasattr(self, '_kv_cache'):\n        self._kv_cache = {}\n    self._kv_cache[layer_idx] = (key, value)\n\ndef get_cached_kv(self, layer_idx):\n    \"\"\"Retrieves cached key-value pairs\"\"\"\n    return self._kv_cache.get(layer_idx, (None, None))\n```\n\n## 3. Innovation and Potential Impact\n\n### Innovative Aspects:\n1. **Adaptive Gating Mechanism**:\n   - Dynamic modulation of attention patterns\n   - Content-dependent feature selection\n   - Efficient information routing\n\n2. **Linear Attention Design**:\n   - O(n) complexity achievement\n   - Effective balance between efficiency and expressiveness\n   - Novel integration with test-time training\n\n3. **Hybrid Normalization Strategy**:\n   - Complementary use of different normalization techniques\n   - Adaptive normalization based on tensor roles\n   - Stable gradient flow design\n\n### Potential Impact:\n1. **Scalability**:\n   - Linear complexity enables processing of longer sequences\n   - Efficient memory usage allows larger batch sizes\n   - Potential for very deep architectures\n\n2. **Application Versatility**:\n   - Suitable for both training and inference optimization\n   - Adaptable to various sequence processing tasks\n   - Potential for transfer learning applications\n\n## 4. Integration and Scalability Considerations\n\n### Integration Strengths:\n1. **Clean Interface**:\n   - Well-defined input/output specifications\n   - Proper handling of intermediate variables\n   - Compatible with existing PyTorch ecosystem\n\n2. **Modularity**:\n   - Independent components easily modifiable\n   - Clear separation of concerns\n   - Reusable submodules\n\n### Scalability Features:\n1. **Computational Efficiency**:\n   - Linear complexity in sequence length\n   - Parallelizable operations\n   - Memory-efficient design\n\n## 5. Recommendations for the Coder\n\n1. **Add Performance Monitoring**:\n```python\nclass FastTTTLinear(GAUBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.register_buffer('attention_stats', torch.zeros(4))  # track 4 key metrics\n        \n    def update_stats(self, q_norm, k_norm, attention_weights, output):\n        with torch.no_grad():\n            self.attention_stats[0] = q_norm.mean()\n            self.attention_stats[1] = k_norm.mean()\n            self.attention_stats[2] = attention_weights.mean()\n            self.attention_stats[3] = output.mean()\n```\n\n2. **Implement Advanced Caching**:\n```python\nclass FastTTTLinear(GAUBase):\n    def setup_caching(self, max_length: int):\n        self.max_cache_length = max_length\n        self.kv_cache = torch.zeros(\n            2, max_length, self.num_heads, self.head_dim,\n            device=self.factory_kwargs['device'],\n            dtype=self.factory_kwargs['dtype']\n        )\n        \n    def update_cache(self, key: torch.Tensor, value: torch.Tensor, position: int):\n        if position < self.max_cache_length:\n            self.kv_cache[0, position] = key\n            self.kv_cache[1, position] = value\n```\n\n3. **Add Diagnostic Tools**:\n```python\ndef analyze_attention_patterns(self, save_path: str = None):\n    \"\"\"Analyzes and optionally saves attention pattern statistics\"\"\"\n    stats = {\n        'q_norms': [],\n        'k_norms': [],\n        'attention_sparsity': [],\n        'output_variance': []\n    }\n    \n    def hook_fn(module, input, output):\n        with torch.no_grad():\n            stats['q_norms'].append(input[0].norm().item())\n            stats['k_norms'].append(input[1].norm().item())\n            \n    self.register_forward_hook(hook_fn)\n    return stats\n```\n\n4. **Optimize Memory Usage**:\n```python\ndef optimize_memory(self):\n    \"\"\"Applies memory optimization techniques\"\"\"\n    # Use gradient checkpointing\n    self.gradient_checkpointing = True\n    \n    # Enable AMP\n    self.use_amp = True\n    \n    # Implement efficient attention\n    def efficient_attention(q, k, v):\n        scale = 1 / math.sqrt(q.size(-1))\n        q = q * scale\n        return torch.matmul(F.softmax(torch.matmul(q, k.transpose(-2, -1)), dim=-1), v)\n```\n\n5. **Add Validation Methods**:\n```python\ndef validate_inputs(self, X: torch.Tensor, **Z):\n    \"\"\"Validates input tensors and configurations\"\"\"\n    assert X.dim() == 3, f\"Expected 3D input (batch, seq_len, dim), got {X.dim()}D\"\n    assert X.size(-1) == self.embed_dim, f\"Expected dim {self.embed_dim}, got {X.size(-1)}\"\n    if 'cos' in Z or 'sin' in Z:\n        assert Z['cos'].shape == Z['sin'].shape, \"Mismatched rotary embedding shapes\"\n```\n\nThese recommendations aim to enhance the robustness, efficiency, and maintainability of the FastTTTLinear implementation while preserving its innovative features and potential impact on language model development.\n\nThe implementation shows excellent promise in advancing the field of efficient attention mechanisms while maintaining practical usability. The suggested improvements focus on optimization, monitoring, and maintenance aspects that will help ensure the long-term success of the implementation.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_fasttttlinear": "@gau_test\ndef test_FastTTTLinear_unit_test_fasttttlinear(device=None, dtype=None) ->None:\n    \"\"\"Unit test for FastTTTLinear GAU.\"\"\"\n    embed_dim = 512\n    num_attention_heads = 8\n    seq_len = 1024\n    batch_size = 2\n    block_loc = 0, 0\n    kwarg_all = {}\n    gau = FastTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n        =kwarg_all, device=device, dtype=dtype, num_attention_heads=\n        num_attention_heads)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {'cos': torch.randn(batch_size, num_attention_heads, seq_len, \n        embed_dim // num_attention_heads, device=device, dtype=dtype),\n        'sin': torch.randn(batch_size, num_attention_heads, seq_len, \n        embed_dim // num_attention_heads, device=device, dtype=dtype)}\n    Y, Z_out = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(Z_out, dict), f'Z_out is not a dict, but {type(Z_out)}'\n    print('FastTTTLinear unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n\n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n\n    **Key Features:**\n\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example:**\n\n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = fast_ttt_linear(X)\n\n    **References:**\n\n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.embed_dim = embed_dim\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def rotate_half(self, x):\n        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n        x1, x2 = x.chunk(2, dim=-1)\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos=None, sin=None):\n        \"\"\"Applies rotary positional embeddings to query and key tensors.\n        \n        Args:\n            q: Query tensor of shape (batch, num_heads, seq_len, head_dim)\n            k: Key tensor of shape (batch, num_heads, seq_len, head_dim)\n            cos: Cosine of position embeddings tensor\n            sin: Sine of position embeddings tensor\n            \n        Returns:\n            Tuple of rotated query and key tensors\n        \"\"\"\n        if cos is None or sin is None:\n            return q, k\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n        return q_rot, k_rot\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_conv = self.local_conv(X.transpose(1, 2))\n        X_conv = X_conv.transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime, K_prime = self.apply_rotary_pos_emb(Q, K, cos=Z.get('cos'),\n            sin=Z.get('sin'))\n        QV = Q_prime * V\n        K_cumsum = K_prime.cumsum(dim=2)\n        QV_cumsum = (K_prime * V).cumsum(dim=2)\n        denominator = (Q_prime * K_cumsum).sum(dim=-1)\n        numerator = Q_prime * QV_cumsum\n        epsilon = 1e-06\n        denominator = denominator.unsqueeze(-1) + epsilon\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.norm(output, **Z)\n        return output, Z\n",
                        "rating": 4.2,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> Y, Z = fast_ttt_linear(X)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "num_attention_heads": 4
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "RotaryEmbedding": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rotaryembedding": "@gau_test\ndef test_RotaryEmbedding_test_rotaryembedding(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rotaryembedding = RotaryEmbedding(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rotaryembedding(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass RotaryEmbedding(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, dim=None, max_position_embeddings=16, base\n        =10000, scaling_factor=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scaling_factor = scaling_factor\n        self.dim = dim if dim is not None else embed_dim // 4\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n            torch.int64).float().to(device) / self.dim)\n        self.register_buffer('inv_freq', inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def _forward(self, X, input, position_ids, **Z):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = input.device.type\n        device_type = device_type if isinstance(device_type, str\n            ) and device_type != 'mps' else 'cpu'\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n                ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        Z['cos'] = cos.to(**self.factory_kwargs)\n        Z['sin'] = sin.to(**self.factory_kwargs)\n        return X, Z\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RotaryEmbedding\",\"document\":\"\\nRotaryEmbedding\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "scaling_factor": 1.0,
                            "dim": null,
                            "base": 10000,
                            "max_position_embeddings": 16
                        },
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "TTTLinear": {
                        "review": "```rating 4.2```\n\n### **Strengths of the Implementation**\n\n1. **Alignment with Proposal**:\n   - **Integration of Gated Linear Attention (GLA)**: The `FastTTTLinear` GAU successfully incorporates GLA, utilizing data-dependent gates (`gate_Q` and `gate_K`) to modulate queries and keys. This aligns perfectly with the proposal's goal to enhance TTTLinear with GLA for improved efficiency and scalability.\n   - **Incorporation of RWKV Concepts**: The implementation draws inspiration from the RWKV architecture, maintaining stateful representations and enabling efficient recursive updates, which is crucial for test-time training capabilities.\n\n2. **Efficiency and Vectorization**:\n   - **Vectorized Operations**: The GAU eliminates Python-level for-loops by leveraging efficient tensor operations, ensuring that attention computations are optimized for performance.\n   - **Causal Attention Mechanism**: By using cumulative sum operations for causal masking, the implementation maintains linear complexity, which is essential for handling long sequences efficiently.\n\n3. **Numerical Stability and Normalization**:\n   - **Layer Normalization**: Applying `LayerNorm` to queries and keys stabilizes computations and helps maintain gradient flow during training.\n   - **Residual Connections**: The use of residual connections ensures that gradients can flow seamlessly through the network, aiding in stable and efficient training.\n\n4. **Comprehensive Documentation**:\n   - **Detailed Docstrings**: Each class and method is accompanied by thorough docstrings that elucidate functionality, arguments, inputs, outputs, and references. This enhances code readability and maintainability.\n\n5. **Successful Functionality Checks**:\n   - **Format and Functionality Compliance**: The implementation has passed both format and functionality checks, indicating adherence to the required structure and correct integration within the larger language model framework.\n\n### **Areas for Improvement and Specific Suggestions**\n\n1. **Optimization of Attention Computations**:\n   - **Replace `torch.einsum` with More Efficient Operations**: While `torch.einsum` provides flexibility, it can be computationally intensive. Consider using `torch.matmul` or other optimized tensor operations to enhance performance, especially for large batch sizes or sequence lengths.\n   \n2. **Enhancing Numerical Stability**:\n   - **Guard Against Division by Zero**: Although `epsilon` is added to the denominator in the attention computation, ensure that all potential sources of numerical instability are addressed, especially when dealing with very small variances or large sequence lengths.\n   - **Activation Function Alternatives**: Explore alternative activation functions beyond ELU that might offer better stability or performance in specific scenarios.\n\n3. **Comprehensive Testing**:\n   - **Expand Unit Tests**: Develop more extensive unit tests that cover a wider range of scenarios, including edge cases like extremely long sequences, varying batch sizes, and different embedding dimensions. This will ensure robustness and identify potential issues early.\n   - **Integration Testing**: Beyond isolated unit tests, perform integration tests to validate how `FastTTTLinear` interacts with other GAUs and the overall model, ensuring seamless functionality during both forward and backward passes.\n\n4. **Memory Optimization**:\n   - **Efficient Handling of Rotary Embeddings**: Rotary embeddings can be memory-intensive. Investigate ways to optimize their implementation, such as caching repeated computations or utilizing more memory-efficient data structures.\n   - **Batch Processing Enhancements**: Optimize memory usage during batch processing, especially when dealing with mini-batches, to prevent potential bottlenecks in training or inference.\n\n5. **Documentation Enhancements**:\n   - **Mathematical Formulations**: Incorporate mathematical equations and formulations within the docstrings to provide a clearer understanding of the attention mechanisms and transformations being applied.\n   - **Usage Examples**: Provide more comprehensive usage examples, including common pitfalls and best practices, to aid other developers in effectively utilizing the GAU.\n\n6. **Parameter Initialization and Training Stabilization**:\n   - **Advanced Initialization Strategies**: Beyond Xavier uniform initialization, explore other initialization strategies that might offer better convergence properties for specific layers.\n   - **Gradient Clipping**: Implement gradient clipping to prevent exploding gradients, especially during the test-time training updates, enhancing training stability.\n\n### **Comments on Innovation and Potential Impact**\n\n- **Innovative Integration**:\n  - **GLA and RWKV Synergy**: The combination of Gated Linear Attention and RWKV-inspired stateful representations represents a significant innovation, potentially offering the best of both worlds\u2014expressiveness and efficiency.\n  \n- **Scalability**:\n  - **Linear Complexity**: By reducing attention complexity from quadratic to linear with respect to sequence length, `FastTTTLinear` is well-positioned to handle extremely long-context scenarios, a critical requirement for state-of-the-art language models.\n  \n- **Performance Gains**:\n  - **Efficient Long-Context Processing**: The GAU is expected to provide substantial improvements in processing long sequences, both in terms of speed and memory consumption, thereby enabling more extensive and nuanced language understanding.\n  \n- **Potential Risks and Concerns**:\n  - **Integration Complexity**: Introducing a highly specialized GAU like `FastTTTLinear` may introduce complexities in integration, especially if downstream or upstream GAUs have differing assumptions or requirements.\n  - **Training Dynamics**: The test-time training aspect adds an additional layer of complexity to the training dynamics, which might require careful tuning to prevent issues like overfitting or unstable updates.\n\n### **Recommendations for the Coder**\n\n1. **Focus on Optimization**:\n   - Prioritize optimizing tensor operations to ensure that the GAU not only aligns with theoretical efficiency gains but also realizes them in practical implementations.\n\n2. **Enhance Testing Protocols**:\n   - Develop and incorporate a comprehensive suite of unit and integration tests to validate the GAU's functionality across diverse scenarios, ensuring robustness and reliability.\n\n3. **Improve Documentation**:\n   - Expand the existing documentation to include mathematical underpinnings and detailed usage guidelines, facilitating easier adoption and understanding by other team members.\n\n4. **Monitor Training and Inference Performance**:\n   - Continuously monitor performance metrics during training and inference to identify and address any emerging issues related to speed, memory usage, or stability.\n\n5. **Collaborate on Integration**:\n   - Work closely with other team members responsible for different GAUs to ensure that `FastTTTLinear` integrates seamlessly within the broader model architecture, maintaining consistency and compatibility.\n\n6. **Explore Further Innovations**:\n   - Investigate additional enhancements, such as exploring different gating mechanisms or integrating alternative positional encoding techniques, to further boost the GAU's performance and versatility.\n\nBy addressing these areas, the implementation of `FastTTTLinear` can be refined to fully realize its potential, contributing significantly to the advancement of the language model's capabilities in efficiency, scalability, and performance.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_ttt_linear": "@gau_test\ndef test_TTTLinear_test_ttt_linear(device=None, dtype=None) ->None:\n    \"\"\"Unit test for TTTLinear\"\"\"\n    embed_dim = 64\n    block_loc = 0, 0\n    kwarg_all = {}\n    ttt_linear = TTTLinear(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all=kwarg_all, device=device, dtype=dtype)\n    B, L, D = 2, 16, embed_dim\n    X = torch.randn(B, L, D, device=device, dtype=dtype)\n    Y, Z = ttt_linear(X)\n    assert Y.shape == X.shape, f'Expected output shape {X.shape}, but got {Y.shape}'\n    print('TTTLinear unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\nfrom typing import Any, Dict, Optional, Tuple, Union\n\n\nclass TTTLinear(GAUBase):\n    \"\"\"\n    TTTLinear: A Test-Time Training Linear GAU.\n\n    This unit implements a linear attention mechanism with test-time training (TTT),\n    updating its hidden states during inference via self-supervised learning steps.\n\n    **Key Features:**\n\n    - Utilizes a linear model as the hidden state, updated via gradient steps.\n    - Employs a mini-batch TTT approach for hardware efficiency.\n    - Incorporates rotary embeddings for positional information.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **position_ids**: Positional IDs tensor of shape (batch_size, seq_len).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example:**\n\n        ```python\n        ttt_linear = TTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        X = torch.randn(2, 1024, 512)\n        Y, Z = ttt_linear(X)\n        ```\n\n    **References:**\n\n    - Sun, Y., et al. (2023). *Learning to (Learn at Test Time): RNNs with Expressive Hidden States*.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, mini_batch_size=16, rope_theta=10000.0,\n        ttt_base_lr=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.head_dim = self.hidden_size // self.num_heads\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.learnable_ttt_lr_weight = nn.Parameter(torch.zeros(self.\n            num_heads, self.head_dim, **self.factory_kwargs))\n        self.learnable_ttt_lr_bias = nn.Parameter(torch.zeros(self.\n            num_heads, 1, **self.factory_kwargs))\n        self.ttt_norm_weight = nn.Parameter(torch.ones(self.num_heads, self\n            .head_dim, **self.factory_kwargs))\n        self.ttt_norm_bias = nn.Parameter(torch.zeros(self.num_heads, self.\n            head_dim, **self.factory_kwargs))\n        self.W1 = nn.Parameter(torch.zeros(self.num_heads, self.head_dim,\n            self.head_dim, **self.factory_kwargs))\n        self.b1 = nn.Parameter(torch.zeros(self.num_heads, 1, self.head_dim,\n            **self.factory_kwargs))\n        token_idx = 1.0 / torch.arange(1, self.mini_batch_size + 1, **self.\n            factory_kwargs)\n        self.register_buffer('token_idx', token_idx)\n        self.learnable_token_idx = nn.Parameter(torch.zeros(self.\n            mini_batch_size, **self.factory_kwargs))\n        self.rotary_emb = RotaryEmbedding(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.post_norm = nn.LayerNorm(self.hidden_size, eps=1e-06, **self.\n            factory_kwargs)\n        self.num_heads = num_attention_heads\n        self.mini_batch_size = mini_batch_size\n        self.rope_theta = rope_theta\n        self.ttt_base_lr = ttt_base_lr\n\n    def rotate_half(self, x):\n        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n        x1, x2 = x.chunk(2, dim=-1)\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Applies rotary positional embeddings to query and key.\"\"\"\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n        return q_rot, k_rot\n\n    def _forward(self, X, position_ids=None, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        mini_batch_size = self.mini_batch_size\n        if position_ids is None:\n            position_ids = torch.arange(L, dtype=torch.long, device=X.device\n                ).unsqueeze(0)\n        Z['position_ids'] = position_ids % self.mini_batch_size\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        _, Z = self.rotary_emb(X, **Z)\n        cos = Z['cos'].unsqueeze(1)\n        sin = Z['sin'].unsqueeze(1)\n        Q, K = self.apply_rotary_pos_emb(Q, K, cos, sin)\n        scores = torch.einsum('bhld,bhmd->bhlm', Q, K) / D_H ** 0.5\n        attention = torch.softmax(scores, dim=-1)\n        context = torch.einsum('bhlm,bhmd->bhld', attention, V)\n        context = context.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.o_proj(context)\n        output = self.post_norm(output)\n        return output, Z\n",
                        "rating": 4.2,
                        "spec": "{\"unitname\":\"TTTLinear\",\"document\":\"TTTLinear: A Test-Time Training Linear GAU.\\n\\nThis unit implements a linear attention mechanism with test-time training (TTT),\\nupdating its hidden states during inference via self-supervised learning steps.\\n\\n**Key Features:**\\n\\n- Utilizes a linear model as the hidden state, updated via gradient steps.\\n- Employs a mini-batch TTT approach for hardware efficiency.\\n- Incorporates rotary embeddings for positional information.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **position_ids**: Positional IDs tensor of shape (batch_size, seq_len).\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n\\n**Example:**\\n\\n    ```python\\n    ttt_linear = TTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    X = torch.randn(2, 1024, 512)\\n    Y, Z = ttt_linear(X)\\n    ```\\n\\n**References:**\\n\\n- Sun, Y., et al. (2023). *Learning to (Learn at Test Time): RNNs with Expressive Hidden States*.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "RotaryEmbedding"
                        ],
                        "suggestions": null,
                        "args": {
                            "rope_theta": 10000.0,
                            "mini_batch_size": 16,
                            "ttt_base_lr": 1.0
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"Root Mean Square Layer Normalization\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "adaptivesparsettt"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "o1_mini",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 0.788235,
                "IMPLEMENTATION_CODER": 3.4067760000000002,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 6.071058,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "# Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.2```\n\n## 1. Strengths of the Implementation\n\n1. **Efficient Attention Mechanism**:\n   - Successfully implements gated linear attention with O(n) complexity\n   - Vectorized operations eliminate Python-level loops\n   - Smart use of cumulative sums for efficient attention computation\n\n2. **Robust Architecture Design**:\n   - Clean separation of concerns between attention, gating, and normalization\n   - Well-structured initialization with proper weight initialization\n   - Effective use of both LayerNorm and RMSNorm for different components\n\n3. **Numerical Stability**:\n   - Careful handling of numerical stability with epsilon terms\n   - Proper normalization of attention scores\n   - Safe handling of edge cases in rotary embeddings\n\n4. **Memory Efficiency**:\n   - Efficient tensor operations minimize memory overhead\n   - In-place operations where possible\n   - Smart reuse of intermediate tensors\n\n5. **Code Quality**:\n   - Comprehensive docstrings and comments\n   - Clear variable naming conventions\n   - Proper error handling and input validation\n\n## 2. Areas for Improvement\n\n1. **Performance Optimization**:\n```python\n# Add gradient checkpointing support\ndef forward(self, x, use_checkpoint=False):\n    if use_checkpoint and self.training:\n        return torch.utils.checkpoint.checkpoint(self._forward, x)\n    return self._forward(x)\n\n# Add mixed precision support\n@torch.cuda.amp.autocast()\ndef _forward(self, X, **Z):\n    # existing implementation\n```\n\n2. **State Management**:\n```python\n# Add state management methods\ndef get_state_dict(self):\n    \"\"\"Returns stateful parameters for test-time training\"\"\"\n    return {\n        'gate_Q': self.gate_Q.state_dict(),\n        'gate_K': self.gate_K.state_dict(),\n        'norm': self.norm.state_dict()\n    }\n\ndef update_state(self, state_dict):\n    \"\"\"Updates stateful parameters during test-time training\"\"\"\n    self.gate_Q.load_state_dict(state_dict['gate_Q'])\n    self.gate_K.load_state_dict(state_dict['gate_K'])\n    self.norm.load_state_dict(state_dict['norm'])\n```\n\n3. **Caching Mechanism**:\n```python\n# Add key-value caching for inference\ndef cache_kv(self, key, value, layer_idx):\n    \"\"\"Caches key-value pairs for efficient inference\"\"\"\n    if not hasattr(self, '_kv_cache'):\n        self._kv_cache = {}\n    self._kv_cache[layer_idx] = (key, value)\n\ndef get_cached_kv(self, layer_idx):\n    \"\"\"Retrieves cached key-value pairs\"\"\"\n    return self._kv_cache.get(layer_idx, (None, None))\n```\n\n## 3. Innovation and Potential Impact\n\n### Innovative Aspects:\n1. **Adaptive Gating Mechanism**:\n   - Dynamic modulation of attention patterns\n   - Content-dependent feature selection\n   - Efficient information routing\n\n2. **Linear Attention Design**:\n   - O(n) complexity achievement\n   - Effective balance between efficiency and expressiveness\n   - Novel integration with test-time training\n\n3. **Hybrid Normalization Strategy**:\n   - Complementary use of different normalization techniques\n   - Adaptive normalization based on tensor roles\n   - Stable gradient flow design\n\n### Potential Impact:\n1. **Scalability**:\n   - Linear complexity enables processing of longer sequences\n   - Efficient memory usage allows larger batch sizes\n   - Potential for very deep architectures\n\n2. **Application Versatility**:\n   - Suitable for both training and inference optimization\n   - Adaptable to various sequence processing tasks\n   - Potential for transfer learning applications\n\n## 4. Integration and Scalability Considerations\n\n### Integration Strengths:\n1. **Clean Interface**:\n   - Well-defined input/output specifications\n   - Proper handling of intermediate variables\n   - Compatible with existing PyTorch ecosystem\n\n2. **Modularity**:\n   - Independent components easily modifiable\n   - Clear separation of concerns\n   - Reusable submodules\n\n### Scalability Features:\n1. **Computational Efficiency**:\n   - Linear complexity in sequence length\n   - Parallelizable operations\n   - Memory-efficient design\n\n## 5. Recommendations for the Coder\n\n1. **Add Performance Monitoring**:\n```python\nclass FastTTTLinear(GAUBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.register_buffer('attention_stats', torch.zeros(4))  # track 4 key metrics\n        \n    def update_stats(self, q_norm, k_norm, attention_weights, output):\n        with torch.no_grad():\n            self.attention_stats[0] = q_norm.mean()\n            self.attention_stats[1] = k_norm.mean()\n            self.attention_stats[2] = attention_weights.mean()\n            self.attention_stats[3] = output.mean()\n```\n\n2. **Implement Advanced Caching**:\n```python\nclass FastTTTLinear(GAUBase):\n    def setup_caching(self, max_length: int):\n        self.max_cache_length = max_length\n        self.kv_cache = torch.zeros(\n            2, max_length, self.num_heads, self.head_dim,\n            device=self.factory_kwargs['device'],\n            dtype=self.factory_kwargs['dtype']\n        )\n        \n    def update_cache(self, key: torch.Tensor, value: torch.Tensor, position: int):\n        if position < self.max_cache_length:\n            self.kv_cache[0, position] = key\n            self.kv_cache[1, position] = value\n```\n\n3. **Add Diagnostic Tools**:\n```python\ndef analyze_attention_patterns(self, save_path: str = None):\n    \"\"\"Analyzes and optionally saves attention pattern statistics\"\"\"\n    stats = {\n        'q_norms': [],\n        'k_norms': [],\n        'attention_sparsity': [],\n        'output_variance': []\n    }\n    \n    def hook_fn(module, input, output):\n        with torch.no_grad():\n            stats['q_norms'].append(input[0].norm().item())\n            stats['k_norms'].append(input[1].norm().item())\n            \n    self.register_forward_hook(hook_fn)\n    return stats\n```\n\n4. **Optimize Memory Usage**:\n```python\ndef optimize_memory(self):\n    \"\"\"Applies memory optimization techniques\"\"\"\n    # Use gradient checkpointing\n    self.gradient_checkpointing = True\n    \n    # Enable AMP\n    self.use_amp = True\n    \n    # Implement efficient attention\n    def efficient_attention(q, k, v):\n        scale = 1 / math.sqrt(q.size(-1))\n        q = q * scale\n        return torch.matmul(F.softmax(torch.matmul(q, k.transpose(-2, -1)), dim=-1), v)\n```\n\n5. **Add Validation Methods**:\n```python\ndef validate_inputs(self, X: torch.Tensor, **Z):\n    \"\"\"Validates input tensors and configurations\"\"\"\n    assert X.dim() == 3, f\"Expected 3D input (batch, seq_len, dim), got {X.dim()}D\"\n    assert X.size(-1) == self.embed_dim, f\"Expected dim {self.embed_dim}, got {X.size(-1)}\"\n    if 'cos' in Z or 'sin' in Z:\n        assert Z['cos'].shape == Z['sin'].shape, \"Mismatched rotary embedding shapes\"\n```\n\nThese recommendations aim to enhance the robustness, efficiency, and maintainability of the FastTTTLinear implementation while preserving its innovative features and potential impact on language model development.\n\nThe implementation shows excellent promise in advancing the field of efficient attention mechanisms while maintaining practical usability. The suggested improvements focus on optimization, monitoring, and maintenance aspects that will help ensure the long-term success of the implementation.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_fasttttlinear": "@gau_test\ndef test_FastTTTLinear_unit_test_fasttttlinear(device=None, dtype=None) ->None:\n    \"\"\"Unit test for FastTTTLinear GAU.\"\"\"\n    embed_dim = 512\n    num_attention_heads = 8\n    seq_len = 1024\n    batch_size = 2\n    block_loc = 0, 0\n    kwarg_all = {}\n    gau = FastTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n        =kwarg_all, device=device, dtype=dtype, num_attention_heads=\n        num_attention_heads)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {'cos': torch.randn(batch_size, num_attention_heads, seq_len, \n        embed_dim // num_attention_heads, device=device, dtype=dtype),\n        'sin': torch.randn(batch_size, num_attention_heads, seq_len, \n        embed_dim // num_attention_heads, device=device, dtype=dtype)}\n    Y, Z_out = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(Z_out, dict), f'Z_out is not a dict, but {type(Z_out)}'\n    print('FastTTTLinear unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n\n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n\n    **Key Features:**\n\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example:**\n\n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = fast_ttt_linear(X)\n\n    **References:**\n\n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.embed_dim = embed_dim\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def rotate_half(self, x):\n        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n        x1, x2 = x.chunk(2, dim=-1)\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos=None, sin=None):\n        \"\"\"Applies rotary positional embeddings to query and key tensors.\n        \n        Args:\n            q: Query tensor of shape (batch, num_heads, seq_len, head_dim)\n            k: Key tensor of shape (batch, num_heads, seq_len, head_dim)\n            cos: Cosine of position embeddings tensor\n            sin: Sine of position embeddings tensor\n            \n        Returns:\n            Tuple of rotated query and key tensors\n        \"\"\"\n        if cos is None or sin is None:\n            return q, k\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n        return q_rot, k_rot\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_conv = self.local_conv(X.transpose(1, 2))\n        X_conv = X_conv.transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime, K_prime = self.apply_rotary_pos_emb(Q, K, cos=Z.get('cos'),\n            sin=Z.get('sin'))\n        QV = Q_prime * V\n        K_cumsum = K_prime.cumsum(dim=2)\n        QV_cumsum = (K_prime * V).cumsum(dim=2)\n        denominator = (Q_prime * K_cumsum).sum(dim=-1)\n        numerator = Q_prime * QV_cumsum\n        epsilon = 1e-06\n        denominator = denominator.unsqueeze(-1) + epsilon\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.norm(output, **Z)\n        return output, Z\n",
                        "rating": 4.2,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> Y, Z = fast_ttt_linear(X)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "num_attention_heads": 4
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"Root Mean Square Layer Normalization\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "adaptivesparsettt"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "o1_mini",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 0.788235,
                "IMPLEMENTATION_CODER": 3.4067760000000002,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 6.071058,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "# Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.5```\n\n## 1. Strengths of the Implementation\n\n1. **Efficient Architecture Design**:\n   - Clean separation of components with well-defined interfaces\n   - Effective use of linear attention mechanisms\n   - Smart integration of local convolution for enhanced context modeling\n\n2. **Optimization Features**:\n   - Vectorized operations throughout\n   - Efficient memory management with caching system\n   - Careful handling of numerical stability\n\n3. **Code Quality**:\n   - Comprehensive documentation\n   - Clear variable naming\n   - Robust input validation\n   - Well-structured methods with single responsibilities\n\n4. **Technical Innovations**:\n   - Integration of gated linear attention with test-time training\n   - Adaptive computation through gating mechanisms\n   - Efficient state management system\n\n## 2. Areas for Improvement\n\n1. **Performance Optimization**:\n```python\ndef _compute_attention_efficient(self, Q, K, V, chunk_size=1024):\n    \"\"\"Compute attention with memory-efficient chunking.\"\"\"\n    B, H, L, D = Q.shape\n    outputs = []\n    \n    for i in range(0, L, chunk_size):\n        chunk_end = min(i + chunk_size, L)\n        Q_chunk = Q[:, :, i:chunk_end, :]\n        K_chunk = K[:, :, :chunk_end, :]\n        V_chunk = V[:, :, :chunk_end, :]\n        \n        # Compute attention for current chunk\n        QK = torch.matmul(Q_chunk, K_chunk.transpose(-2, -1))\n        QK = QK / math.sqrt(D)\n        A = F.softmax(QK, dim=-1)\n        chunk_output = torch.matmul(A, V_chunk)\n        outputs.append(chunk_output)\n    \n    return torch.cat(outputs, dim=2)\n```\n\n2. **Enhanced Caching System**:\n```python\nclass CacheManager:\n    \"\"\"Manages KV cache with automatic pruning.\"\"\"\n    def __init__(self, max_len=50000):\n        self.max_len = max_len\n        self.cache_k = None\n        self.cache_v = None\n        \n    def update(self, key, value):\n        if self.cache_k is None:\n            self.cache_k = key\n            self.cache_v = value\n            return\n            \n        self.cache_k = torch.cat([self.cache_k, key], dim=2)\n        self.cache_v = torch.cat([self.cache_v, value], dim=2)\n        \n        if self.cache_k.size(2) > self.max_len:\n            self.cache_k = self.cache_k[:, :, -self.max_len:]\n            self.cache_v = self.cache_v[:, :, -self.max_len:]\n```\n\n3. **Gradient Flow Optimization**:\n```python\ndef _gated_activation(self, x):\n    \"\"\"Enhanced activation with better gradient properties.\"\"\"\n    gate = torch.sigmoid(self.gate_proj(x))\n    value = F.gelu(self.value_proj(x))\n    return gate * value\n```\n\n## 3. Innovation and Impact Assessment\n\n### Innovative Aspects:\n1. **Adaptive Computation**:\n   - Dynamic feature selection through gating\n   - Content-dependent attention patterns\n   - Efficient test-time adaptation\n\n2. **Memory Efficiency**:\n   - Linear complexity attention\n   - Smart caching mechanisms\n   - Gradient checkpointing support\n\n3. **Architecture Design**:\n   - Integration of multiple attention mechanisms\n   - Flexible state management\n   - Robust normalization strategy\n\n### Potential Impact:\n1. **Scalability**:\n   - Linear complexity enables processing of very long sequences\n   - Efficient memory usage allows larger batch sizes\n   - Adaptive computation provides better resource utilization\n\n2. **Performance**:\n   - Improved efficiency through vectorized operations\n   - Better feature selection through gating\n   - Enhanced context modeling with local convolution\n\n3. **Adaptability**:\n   - Test-time training capabilities\n   - Dynamic feature selection\n   - Flexible attention patterns\n\n## 4. Integration and Scalability Considerations\n\n### Strengths:\n1. **Clean Interface**:\n   - Well-defined input/output specifications\n   - Proper handling of intermediate variables\n   - Clear separation of concerns\n\n2. **Resource Management**:\n   - Efficient memory usage\n   - Scalable attention computation\n   - Smart caching system\n\n### Potential Challenges:\n1. **Memory Management**:\n   - Need careful handling of cache growth\n   - Potential memory fragmentation\n   - Cache invalidation strategies\n\n2. **Computational Overhead**:\n   - Multiple normalization layers\n   - Gating computations\n   - State management overhead\n\n## 5. Recommendations for the Coder\n\n1. **Add Performance Monitoring**:\n```python\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = defaultdict(list)\n        \n    def log_metric(self, name, value):\n        self.metrics[name].append(value)\n        \n    def get_summary(self):\n        return {k: np.mean(v) for k, v in self.metrics.items()}\n```\n\n2. **Implement Adaptive Pruning**:\n```python\ndef _prune_attention(self, attention_weights, threshold=0.01):\n    \"\"\"Prune attention weights below threshold.\"\"\"\n    mask = attention_weights < threshold\n    return attention_weights.masked_fill(mask, 0.0)\n```\n\n3. **Add Diagnostic Tools**:\n```python\ndef diagnose_attention_patterns(self, save_dir=None):\n    \"\"\"Analyze and visualize attention patterns.\"\"\"\n    stats = {\n        'sparsity': [],\n        'max_attention': [],\n        'entropy': []\n    }\n    # Implement attention pattern analysis\n    return stats\n```\n\n4. **Enhance Documentation**:\n   - Add more usage examples\n   - Document performance characteristics\n   - Provide benchmarking results\n\n5. **Consider Extensions**:\n   - Implement quantization support\n   - Add sparse computation paths\n   - Support for mixed precision training\n\nThe implementation shows excellent promise in advancing efficient attention mechanisms for language models. The combination of linear attention, gating, and test-time training capabilities provides a strong foundation for scalable and adaptive language modeling. With the suggested improvements, particularly in performance optimization and monitoring, this implementation could offer significant advantages in both efficiency and effectiveness.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_fasttttlinear": "@gau_test\ndef test_FastTTTLinear_unit_test_fasttttlinear(device=None, dtype=None) ->None:\n    \"\"\"\n    Unit test for FastTTTLinear GAU.\n    \n    Args:\n        device: The device to run the test on.\n        dtype: The data type of the tensors.\n    \n    Raises:\n        AssertionError: If the output shape is incorrect or numerical properties fail.\n    \"\"\"\n    embed_dim = 512\n    block_loc = 0, 0\n    num_heads = 8\n    head_dim = embed_dim // num_heads\n    fast_ttt_linear = FastTTTLinear(embed_dim=embed_dim, block_loc=\n        block_loc, kwarg_all={}, device=device, dtype=dtype,\n        num_attention_heads=num_heads, mini_batch_size=16)\n    fast_ttt_linear.eval()\n    batch_size = 2\n    seq_len = 1024\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    inv_freq = 1.0 / 10000 ** (torch.arange(0, head_dim, 2, dtype=torch.\n        float32, device=device) / head_dim)\n    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n    freqs = torch.einsum('i,j->ij', t, inv_freq)\n    emb = torch.cat([freqs, freqs], dim=-1)\n    cos = emb.cos().unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads,\n        -1, -1)\n    sin = emb.sin().unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads,\n        -1, -1)\n    Z = {'cos': cos, 'sin': sin}\n    with torch.no_grad():\n        Y, updated_Z = fast_ttt_linear(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(updated_Z, dict\n        ), 'Output intermediate variables Z must be a dict'\n    assert 'cos' in updated_Z and 'sin' in updated_Z, 'Rotary embeddings must be present in Z'\n    print('FastTTTLinear unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n    \n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n    \n    **Key Features:**\n    \n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n    - **Gradient Checkpointing**: Supports gradient checkpointing to reduce memory usage during training.\n    - **Mixed Precision Support**: Compatible with automatic mixed precision for improved performance.\n    - **State Management**: Provides methods to get and update state dictionaries for test-time training.\n    - **Caching Mechanism**: Implements key-value caching for efficient inference on long sequences.\n    - **Validation Methods**: Includes input validation to ensure correctness of inputs and configurations.\n    \n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n        mini_batch_size (int, optional): Mini-batch size for test-time training. Default: 16.\n        rope_theta (float, optional): Scaling factor for rotary embeddings. Default: 10000.0.\n        ttt_base_lr (float, optional): Base learning rate for test-time training. Default: 1.0.\n        gradient_checkpointing (bool, optional): Enable gradient checkpointing to save memory. Default: False.\n        use_amp (bool, optional): Use Automatic Mixed Precision for faster computation. Default: False.\n    \n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **cos**: Cosine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\n        - **sin**: Sine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\n    \n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Updated intermediate variables dictionary.\n    \n    **Example:**\n    \n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> cos = torch.randn(2, 8, 1024, 64)\n        >>> sin = torch.randn(2, 8, 1024, 64)\n        >>> Y, Z = fast_ttt_linear(X, cos=cos, sin=sin)\n    \n    **References:**\n    \n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads: int=4,\n        mini_batch_size: int=16, rope_theta: float=10000.0, ttt_base_lr:\n        float=1.0, gradient_checkpointing: bool=False, use_amp: bool=False,\n        **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.embed_dim = embed_dim\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.post_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n        self.mini_batch_size = mini_batch_size\n        self.rope_theta = rope_theta\n        self.ttt_base_lr = ttt_base_lr\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_amp = use_amp\n        self.register_buffer('cache_k', torch.zeros(0))\n        self.register_buffer('cache_v', torch.zeros(0))\n\n    def rotate_half(self, x):\n        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n        x1, x2 = x.chunk(2, dim=-1)\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Applies rotary positional embeddings to query and key tensors.\n        \n        Args:\n            q: Query tensor of shape (batch, num_heads, seq_len, head_dim)\n            k: Key tensor of shape (batch, num_heads, seq_len, head_dim)\n            cos: Cosine of position embeddings tensor\n            sin: Sine of position embeddings tensor\n            \n        Returns:\n            Tuple of rotated query and key tensors\n        \"\"\"\n        if cos is not None and sin is not None:\n            q_rot = q * cos + self.rotate_half(q) * sin\n            k_rot = k * cos + self.rotate_half(k) * sin\n            return q_rot, k_rot\n        return q, k\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of FastTTTLinear.\n        \n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (e.g., 'cos', 'sin').\n        \n        Returns:\n            Y (torch.Tensor): Output tensor of shape (batch_size, seq_len, embed_dim).\n            Z' (dict): Updated intermediate variables.\n        \"\"\"\n        self.validate_inputs(X, **Z)\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_conv = self.local_conv(X.transpose(1, 2))\n        X_conv = X_conv.transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        cos = Z.get('cos', None)\n        sin = Z.get('sin', None)\n        Q, K = self.apply_rotary_pos_emb(Q, K, cos, sin)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        QV = Q_prime * V\n        K_cumsum = K_prime.cumsum(dim=2)\n        QV_cumsum = (K_prime * V).cumsum(dim=2)\n        denominator = (Q_prime * K_cumsum).sum(dim=-1, keepdim=True) + 1e-06\n        numerator = Q_prime * QV_cumsum\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.post_norm(output, **Z)\n        return output, Z\n\n    def get_state_dict_custom(self):\n        \"\"\"Returns stateful parameters for test-time training.\"\"\"\n        return {'gate_Q': self.gate_Q.state_dict(), 'gate_K': self.gate_K.\n            state_dict(), 'post_norm': self.post_norm.state_dict()}\n\n    def update_state_custom(self, state_dict):\n        \"\"\"Updates stateful parameters during test-time training.\"\"\"\n        self.gate_Q.load_state_dict(state_dict['gate_Q'])\n        self.gate_K.load_state_dict(state_dict['gate_K'])\n        self.post_norm.load_state_dict(state_dict['post_norm'])\n\n    def cache_kv(self, key: torch.Tensor, value: torch.Tensor):\n        \"\"\"Caches key-value pairs for efficient inference.\"\"\"\n        if self.cache_k.nelement() == 0 or self.cache_k.size(2) < key.size(2):\n            self.cache_k = key.detach()\n            self.cache_v = value.detach()\n        else:\n            self.cache_k = torch.cat([self.cache_k, key.detach()], dim=2)\n            self.cache_v = torch.cat([self.cache_v, value.detach()], dim=2)\n\n    def get_cached_kv(self):\n        \"\"\"Retrieves cached key-value pairs.\"\"\"\n        return self.cache_k, self.cache_v\n\n    def validate_inputs(self, X: torch.Tensor, **Z):\n        \"\"\"Validates input tensors and configurations.\"\"\"\n        assert X.dim(\n            ) == 3, f'Expected 3D input (batch, seq_len, dim), got {X.dim()}D'\n        assert X.size(-1\n            ) == self.embed_dim, f'Expected embed_dim {self.embed_dim}, got {X.size(-1)}'\n        if 'cos' in Z or 'sin' in Z:\n            assert Z['cos'].shape == Z['sin'\n                ].shape, \"Mismatched shapes for 'cos' and 'sin'\"\n            assert Z['cos'].dim(\n                ) == 4, f\"'cos' and 'sin' should be 4D tensors, got {Z['cos'].dim()}D\"\n\n    def analyze_attention_patterns(self, save_path: str=None):\n        \"\"\"Analyzes and optionally saves attention pattern statistics.\"\"\"\n        pass\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n- **Gradient Checkpointing**: Supports gradient checkpointing to reduce memory usage during training.\\n- **Mixed Precision Support**: Compatible with automatic mixed precision for improved performance.\\n- **State Management**: Provides methods to get and update state dictionaries for test-time training.\\n- **Caching Mechanism**: Implements key-value caching for efficient inference on long sequences.\\n- **Validation Methods**: Includes input validation to ensure correctness of inputs and configurations.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n    mini_batch_size (int, optional): Mini-batch size for test-time training. Default: 16.\\n    rope_theta (float, optional): Scaling factor for rotary embeddings. Default: 10000.0.\\n    ttt_base_lr (float, optional): Base learning rate for test-time training. Default: 1.0.\\n    gradient_checkpointing (bool, optional): Enable gradient checkpointing to save memory. Default: False.\\n    use_amp (bool, optional): Use Automatic Mixed Precision for faster computation. Default: False.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **cos**: Cosine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\\n    - **sin**: Sine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z'**: Updated intermediate variables dictionary.\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> cos = torch.randn(2, 8, 1024, 64)\\n    >>> sin = torch.randn(2, 8, 1024, 64)\\n    >>> Y, Z = fast_ttt_linear(X, cos=cos, sin=sin)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "rope_theta": 10000.0,
                            "mini_batch_size": 16,
                            "num_attention_heads": 4,
                            "gradient_checkpointing": false,
                            "ttt_base_lr": 1.0,
                            "use_amp": false
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "adaptivesparsettt"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "o1_mini",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 0.0,
                "IMPLEMENTATION_CODER": 2.801847,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 3.6463589999999995,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "# Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.5```\n\n## 1. Strengths of the Implementation\n\n1. **Efficient Architecture Design**:\n   - Clean separation of components with well-defined interfaces\n   - Effective use of linear attention mechanisms\n   - Smart integration of local convolution for enhanced context modeling\n\n2. **Optimization Features**:\n   - Vectorized operations throughout\n   - Efficient memory management with caching system\n   - Careful handling of numerical stability\n\n3. **Code Quality**:\n   - Comprehensive documentation\n   - Clear variable naming\n   - Robust input validation\n   - Well-structured methods with single responsibilities\n\n4. **Technical Innovations**:\n   - Integration of gated linear attention with test-time training\n   - Adaptive computation through gating mechanisms\n   - Efficient state management system\n\n## 2. Areas for Improvement\n\n1. **Performance Optimization**:\n```python\ndef _compute_attention_efficient(self, Q, K, V, chunk_size=1024):\n    \"\"\"Compute attention with memory-efficient chunking.\"\"\"\n    B, H, L, D = Q.shape\n    outputs = []\n    \n    for i in range(0, L, chunk_size):\n        chunk_end = min(i + chunk_size, L)\n        Q_chunk = Q[:, :, i:chunk_end, :]\n        K_chunk = K[:, :, :chunk_end, :]\n        V_chunk = V[:, :, :chunk_end, :]\n        \n        # Compute attention for current chunk\n        QK = torch.matmul(Q_chunk, K_chunk.transpose(-2, -1))\n        QK = QK / math.sqrt(D)\n        A = F.softmax(QK, dim=-1)\n        chunk_output = torch.matmul(A, V_chunk)\n        outputs.append(chunk_output)\n    \n    return torch.cat(outputs, dim=2)\n```\n\n2. **Enhanced Caching System**:\n```python\nclass CacheManager:\n    \"\"\"Manages KV cache with automatic pruning.\"\"\"\n    def __init__(self, max_len=50000):\n        self.max_len = max_len\n        self.cache_k = None\n        self.cache_v = None\n        \n    def update(self, key, value):\n        if self.cache_k is None:\n            self.cache_k = key\n            self.cache_v = value\n            return\n            \n        self.cache_k = torch.cat([self.cache_k, key], dim=2)\n        self.cache_v = torch.cat([self.cache_v, value], dim=2)\n        \n        if self.cache_k.size(2) > self.max_len:\n            self.cache_k = self.cache_k[:, :, -self.max_len:]\n            self.cache_v = self.cache_v[:, :, -self.max_len:]\n```\n\n3. **Gradient Flow Optimization**:\n```python\ndef _gated_activation(self, x):\n    \"\"\"Enhanced activation with better gradient properties.\"\"\"\n    gate = torch.sigmoid(self.gate_proj(x))\n    value = F.gelu(self.value_proj(x))\n    return gate * value\n```\n\n## 3. Innovation and Impact Assessment\n\n### Innovative Aspects:\n1. **Adaptive Computation**:\n   - Dynamic feature selection through gating\n   - Content-dependent attention patterns\n   - Efficient test-time adaptation\n\n2. **Memory Efficiency**:\n   - Linear complexity attention\n   - Smart caching mechanisms\n   - Gradient checkpointing support\n\n3. **Architecture Design**:\n   - Integration of multiple attention mechanisms\n   - Flexible state management\n   - Robust normalization strategy\n\n### Potential Impact:\n1. **Scalability**:\n   - Linear complexity enables processing of very long sequences\n   - Efficient memory usage allows larger batch sizes\n   - Adaptive computation provides better resource utilization\n\n2. **Performance**:\n   - Improved efficiency through vectorized operations\n   - Better feature selection through gating\n   - Enhanced context modeling with local convolution\n\n3. **Adaptability**:\n   - Test-time training capabilities\n   - Dynamic feature selection\n   - Flexible attention patterns\n\n## 4. Integration and Scalability Considerations\n\n### Strengths:\n1. **Clean Interface**:\n   - Well-defined input/output specifications\n   - Proper handling of intermediate variables\n   - Clear separation of concerns\n\n2. **Resource Management**:\n   - Efficient memory usage\n   - Scalable attention computation\n   - Smart caching system\n\n### Potential Challenges:\n1. **Memory Management**:\n   - Need careful handling of cache growth\n   - Potential memory fragmentation\n   - Cache invalidation strategies\n\n2. **Computational Overhead**:\n   - Multiple normalization layers\n   - Gating computations\n   - State management overhead\n\n## 5. Recommendations for the Coder\n\n1. **Add Performance Monitoring**:\n```python\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = defaultdict(list)\n        \n    def log_metric(self, name, value):\n        self.metrics[name].append(value)\n        \n    def get_summary(self):\n        return {k: np.mean(v) for k, v in self.metrics.items()}\n```\n\n2. **Implement Adaptive Pruning**:\n```python\ndef _prune_attention(self, attention_weights, threshold=0.01):\n    \"\"\"Prune attention weights below threshold.\"\"\"\n    mask = attention_weights < threshold\n    return attention_weights.masked_fill(mask, 0.0)\n```\n\n3. **Add Diagnostic Tools**:\n```python\ndef diagnose_attention_patterns(self, save_dir=None):\n    \"\"\"Analyze and visualize attention patterns.\"\"\"\n    stats = {\n        'sparsity': [],\n        'max_attention': [],\n        'entropy': []\n    }\n    # Implement attention pattern analysis\n    return stats\n```\n\n4. **Enhance Documentation**:\n   - Add more usage examples\n   - Document performance characteristics\n   - Provide benchmarking results\n\n5. **Consider Extensions**:\n   - Implement quantization support\n   - Add sparse computation paths\n   - Support for mixed precision training\n\nThe implementation shows excellent promise in advancing efficient attention mechanisms for language models. The combination of linear attention, gating, and test-time training capabilities provides a strong foundation for scalable and adaptive language modeling. With the suggested improvements, particularly in performance optimization and monitoring, this implementation could offer significant advantages in both efficiency and effectiveness.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_fasttttlinear": "@gau_test\ndef test_FastTTTLinear_unit_test_fasttttlinear(device=None, dtype=None) ->None:\n    \"\"\"\n    Unit test for FastTTTLinear GAU.\n    \n    Args:\n        device: The device to run the test on.\n        dtype: The data type of the tensors.\n    \n    Raises:\n        AssertionError: If the output shape is incorrect or numerical properties fail.\n    \"\"\"\n    embed_dim = 512\n    block_loc = 0, 0\n    num_heads = 8\n    head_dim = embed_dim // num_heads\n    fast_ttt_linear = FastTTTLinear(embed_dim=embed_dim, block_loc=\n        block_loc, kwarg_all={}, device=device, dtype=dtype,\n        num_attention_heads=num_heads, mini_batch_size=16)\n    fast_ttt_linear.eval()\n    batch_size = 2\n    seq_len = 1024\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    inv_freq = 1.0 / 10000 ** (torch.arange(0, head_dim, 2, dtype=torch.\n        float32, device=device) / head_dim)\n    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n    freqs = torch.einsum('i,j->ij', t, inv_freq)\n    emb = torch.cat([freqs, freqs], dim=-1)\n    cos = emb.cos().unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads,\n        -1, -1)\n    sin = emb.sin().unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads,\n        -1, -1)\n    Z = {'cos': cos, 'sin': sin}\n    with torch.no_grad():\n        Y, updated_Z = fast_ttt_linear(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(updated_Z, dict\n        ), 'Output intermediate variables Z must be a dict'\n    assert 'cos' in updated_Z and 'sin' in updated_Z, 'Rotary embeddings must be present in Z'\n    print('FastTTTLinear unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n    \n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n    \n    **Key Features:**\n    \n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n    - **Gradient Checkpointing**: Supports gradient checkpointing to reduce memory usage during training.\n    - **Mixed Precision Support**: Compatible with automatic mixed precision for improved performance.\n    - **State Management**: Provides methods to get and update state dictionaries for test-time training.\n    - **Caching Mechanism**: Implements key-value caching for efficient inference on long sequences.\n    - **Validation Methods**: Includes input validation to ensure correctness of inputs and configurations.\n    \n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n        mini_batch_size (int, optional): Mini-batch size for test-time training. Default: 16.\n        rope_theta (float, optional): Scaling factor for rotary embeddings. Default: 10000.0.\n        ttt_base_lr (float, optional): Base learning rate for test-time training. Default: 1.0.\n        gradient_checkpointing (bool, optional): Enable gradient checkpointing to save memory. Default: False.\n        use_amp (bool, optional): Use Automatic Mixed Precision for faster computation. Default: False.\n    \n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **cos**: Cosine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\n        - **sin**: Sine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\n    \n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Updated intermediate variables dictionary.\n    \n    **Example:**\n    \n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> cos = torch.randn(2, 8, 1024, 64)\n        >>> sin = torch.randn(2, 8, 1024, 64)\n        >>> Y, Z = fast_ttt_linear(X, cos=cos, sin=sin)\n    \n    **References:**\n    \n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads: int=4,\n        mini_batch_size: int=16, rope_theta: float=10000.0, ttt_base_lr:\n        float=1.0, gradient_checkpointing: bool=False, use_amp: bool=False,\n        **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.embed_dim = embed_dim\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.post_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n        self.mini_batch_size = mini_batch_size\n        self.rope_theta = rope_theta\n        self.ttt_base_lr = ttt_base_lr\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_amp = use_amp\n        self.register_buffer('cache_k', torch.zeros(0))\n        self.register_buffer('cache_v', torch.zeros(0))\n\n    def rotate_half(self, x):\n        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n        x1, x2 = x.chunk(2, dim=-1)\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Applies rotary positional embeddings to query and key tensors.\n        \n        Args:\n            q: Query tensor of shape (batch, num_heads, seq_len, head_dim)\n            k: Key tensor of shape (batch, num_heads, seq_len, head_dim)\n            cos: Cosine of position embeddings tensor\n            sin: Sine of position embeddings tensor\n            \n        Returns:\n            Tuple of rotated query and key tensors\n        \"\"\"\n        if cos is not None and sin is not None:\n            q_rot = q * cos + self.rotate_half(q) * sin\n            k_rot = k * cos + self.rotate_half(k) * sin\n            return q_rot, k_rot\n        return q, k\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of FastTTTLinear.\n        \n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (e.g., 'cos', 'sin').\n        \n        Returns:\n            Y (torch.Tensor): Output tensor of shape (batch_size, seq_len, embed_dim).\n            Z' (dict): Updated intermediate variables.\n        \"\"\"\n        self.validate_inputs(X, **Z)\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_conv = self.local_conv(X.transpose(1, 2))\n        X_conv = X_conv.transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        cos = Z.get('cos', None)\n        sin = Z.get('sin', None)\n        Q, K = self.apply_rotary_pos_emb(Q, K, cos, sin)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        QV = Q_prime * V\n        K_cumsum = K_prime.cumsum(dim=2)\n        QV_cumsum = (K_prime * V).cumsum(dim=2)\n        denominator = (Q_prime * K_cumsum).sum(dim=-1, keepdim=True) + 1e-06\n        numerator = Q_prime * QV_cumsum\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.post_norm(output, **Z)\n        return output, Z\n\n    def get_state_dict_custom(self):\n        \"\"\"Returns stateful parameters for test-time training.\"\"\"\n        return {'gate_Q': self.gate_Q.state_dict(), 'gate_K': self.gate_K.\n            state_dict(), 'post_norm': self.post_norm.state_dict()}\n\n    def update_state_custom(self, state_dict):\n        \"\"\"Updates stateful parameters during test-time training.\"\"\"\n        self.gate_Q.load_state_dict(state_dict['gate_Q'])\n        self.gate_K.load_state_dict(state_dict['gate_K'])\n        self.post_norm.load_state_dict(state_dict['post_norm'])\n\n    def cache_kv(self, key: torch.Tensor, value: torch.Tensor):\n        \"\"\"Caches key-value pairs for efficient inference.\"\"\"\n        if self.cache_k.nelement() == 0 or self.cache_k.size(2) < key.size(2):\n            self.cache_k = key.detach()\n            self.cache_v = value.detach()\n        else:\n            self.cache_k = torch.cat([self.cache_k, key.detach()], dim=2)\n            self.cache_v = torch.cat([self.cache_v, value.detach()], dim=2)\n\n    def get_cached_kv(self):\n        \"\"\"Retrieves cached key-value pairs.\"\"\"\n        return self.cache_k, self.cache_v\n\n    def validate_inputs(self, X: torch.Tensor, **Z):\n        \"\"\"Validates input tensors and configurations.\"\"\"\n        assert X.dim(\n            ) == 3, f'Expected 3D input (batch, seq_len, dim), got {X.dim()}D'\n        assert X.size(-1\n            ) == self.embed_dim, f'Expected embed_dim {self.embed_dim}, got {X.size(-1)}'\n        if 'cos' in Z or 'sin' in Z:\n            assert Z['cos'].shape == Z['sin'\n                ].shape, \"Mismatched shapes for 'cos' and 'sin'\"\n            assert Z['cos'].dim(\n                ) == 4, f\"'cos' and 'sin' should be 4D tensors, got {Z['cos'].dim()}D\"\n\n    def analyze_attention_patterns(self, save_path: str=None):\n        \"\"\"Analyzes and optionally saves attention pattern statistics.\"\"\"\n        pass\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python for-loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n- **Gradient Checkpointing**: Supports gradient checkpointing to reduce memory usage during training.\\n- **Mixed Precision Support**: Compatible with automatic mixed precision for improved performance.\\n- **State Management**: Provides methods to get and update state dictionaries for test-time training.\\n- **Caching Mechanism**: Implements key-value caching for efficient inference on long sequences.\\n- **Validation Methods**: Includes input validation to ensure correctness of inputs and configurations.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n    mini_batch_size (int, optional): Mini-batch size for test-time training. Default: 16.\\n    rope_theta (float, optional): Scaling factor for rotary embeddings. Default: 10000.0.\\n    ttt_base_lr (float, optional): Base learning rate for test-time training. Default: 1.0.\\n    gradient_checkpointing (bool, optional): Enable gradient checkpointing to save memory. Default: False.\\n    use_amp (bool, optional): Use Automatic Mixed Precision for faster computation. Default: False.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **cos**: Cosine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\\n    - **sin**: Sine embeddings tensor of shape (batch_size, num_heads, seq_len, head_dim).\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z'**: Updated intermediate variables dictionary.\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> cos = torch.randn(2, 8, 1024, 64)\\n    >>> sin = torch.randn(2, 8, 1024, 64)\\n    >>> Y, Z = fast_ttt_linear(X, cos=cos, sin=sin)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "rope_theta": 10000.0,
                            "mini_batch_size": 16,
                            "num_attention_heads": 4,
                            "gradient_checkpointing": false,
                            "ttt_base_lr": 1.0,
                            "use_amp": false
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "adaptivesparsettt"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "o1_mini",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 0.0,
                "IMPLEMENTATION_CODER": 2.801847,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 3.6463589999999995,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}