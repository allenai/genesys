{
    "variantname": "HierarchicalTTTLinear",
    "review": "### Comprehensive Review of the Proposal: FastTTTLinear\n\n#### 1. Clarity\nThe proposal for **FastTTTLinear** is clearly articulated with well-defined objectives. It aims to enhance the TTTLinear GAU by integrating Gated Linear Attention (GLA) and concepts from the RWKV architecture to improve computational efficiency and scalability for long sequences. The motivation, problem analysis, and theoretical justification sections provide a coherent narrative that aligns with the proposed modifications. The inclusion of detailed pseudo-code further elucidates the proposed modifications, making it accessible for implementation and review.\n\n#### 2. Innovation\nThe proposal introduces innovative improvements by leveraging GLA and RWKV, which are not commonly integrated into existing autoregressive models. The use of data-dependent gates in GLA to maintain linear complexity while enhancing expressiveness is a novel approach. Compared to existing research, this proposal stands out by focusing on test-time training capabilities alongside efficiency improvements. However, similar concepts have been explored in other architectures like Mamba and RWKV, so the novelty lies in the specific integration within the TTTLinear framework.\n\n#### 3. Feasibility\nThe design appears feasible within the given framework, leveraging existing components like linear projections and convolutional augmentations. The pseudo-code provided demonstrates a clear path to implementation, suggesting that the proposed modifications can be integrated into the current model architecture. However, integration complexity is a noteworthy concern. Merging GLA with RWKV-inspired mechanisms requires careful synchronization to ensure seamless interaction between adaptive gating and state management.\n\n#### 4. Scalability\nThe design is likely to scale efficiently with larger models or more data due to its linear complexity in both time and space. The integration of linear attention mechanisms should allow the model to handle longer sequences without a significant increase in computational resources. Leveraging hardware-efficient implementations and parallelizable computations aligns with modern infrastructure capabilities, promoting scalability as model sizes and dataset volumes increase.\n\n#### 5. Accuracy and Robustness\nThe proposal anticipates improvements in model performance, particularly in handling long sequences. The use of GLA should enhance the model's ability to capture complex patterns, potentially improving accuracy on diverse inputs. However, the impact on robustness, especially under noisy conditions, needs further validation. Ensuring that the integration does not inadvertently introduce overfitting or instability is essential for maintaining robustness.\n\n#### 6. Efficiency\nThe design offers potential improvements in computational efficiency and memory usage by reducing complexity from quadratic to linear. This is particularly beneficial for processing long sequences, aligning with the proposal's objectives. The emphasis on hardware-aware optimizations and efficient tensor operations further bolsters its efficiency credentials.\n\n### Strengths and Concerns\n\n**Strengths:**\n- The integration of GLA and RWKV is innovative and well-justified.\n- The proposal addresses key challenges in efficiency and scalability for long sequences.\n- The design maintains test-time training capabilities, enhancing adaptability.\n\n**Concerns:**\n- The impact on robustness under noisy inputs needs further exploration.\n- The proposal lacks empirical validation, which is crucial for assessing real-world performance.\n- The complexity of integrating GLA and RWKV may introduce unforeseen challenges during implementation.\n\n### Suggestions for Improvement\n- Conduct empirical studies to validate the theoretical claims, focusing on robustness and adaptability.\n- Provide more detailed implementation guidelines, particularly for integrating GLA and RWKV.\n- Explore potential challenges in the integration process and propose solutions or mitigations.\n\n### Final Rating\nBased on the proposal's clarity, innovation, feasibility, scalability, and potential impact on efficiency and accuracy, I assign a rating of **4.2**. The proposal is promising and well-conceived, but it requires empirical validation and further exploration of robustness to achieve a higher rating.",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient attention mechanism, linear complexity, long context",
            "detail": "Extract information about efficient attention mechanisms and linear complexity approaches in language models, focusing on techniques that can handle long sequences while maintaining computational efficiency.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nExtract information about efficient attention mechanisms and linear complexity approaches in language models, focusing on techniques that can handle long sequences while maintaining computational efficiency.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 1.00)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 1.00)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n##### *Relevant Chunk: No. 15/32 (Score: 1.00)*\n\n```\nIts effectiveness has been extensively validated. The approach involves using tiling strategies to minimize the amount of memory reads/writes between the high bandwidth memory (HBM) and on-chip SRAM. ## 8. Conclusion and Limitations\n\nWe presented CHELA for robust and efficient modeling of long sequences. CHELA is an SSM-attention hybrid architecture that computes both structured and data-dependent patterns in linear time with respect to sequence length. Its superior performance is enabled by considering the global view of the Short-Long Convolutions and real linear\n\nHardware-Efficient Linear Attention with gating mechanisms. Compared to other hybrid linear models, we are currently the first to achieve performance improvement while maintaining linear complexity. Our large-scale and diverse experiments demonstrate that CHELA is an efficient and flexible long sequence model with excellent performance on image, text, logical reasoning, and speech data. However, CHELA also has some limitations. The optimal combinations of short convolutions are not explored in this paper, which should be a future research direction of designing dynamic short-convolutional components according to the input data. Moreover, the time-varying SSM is a different idea to achieve this goal. By embracing CHELA as a starting point in the integration of hardware-efficient implementation into hybrid models, we are taking the initial step towards achieving greater efficiency gains in the future. ## Acknowledgements\n\nThis work was supported by Ministry of Science and Technology of the People's Republic of China (No. 2021YFA1301603), National Natural Science Foundation of China Project (No. U21A20427), Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University and Project (No. WU2023C019) from the Westlake University Industries of the Future Research Funding. This work was done when Li Wang and Zedong Wang interned at Westlake University. We thank the AI Station of Westlake University for the support of GPUs. ## Impact Statement\n\nThe goal of this paper is to advance research in longsequence modeling by introducing an efficient model design CHELA. We have considered broader ethical impacts and do not foresee CHELA directly leading to negative societal consequences. All datasets and models used are existing public resources that do not contain private or sensitive information. Through discussing the hybrid design of linear attention and SSM models, we aim to make sequence mixers much more efficient by fully leveraging hardware and different model features. Besides, as the community proposes new methods, we encourage discussing any potential negative impacts early in the research process. Overall, we believe hardware-efficient style and the combination of structured and data-dependent patterns are the dominant trends of the future for efficient sequence modeling. ## References\n\nAinslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268-284, Online, 2020. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / 2020$.emnlp-main. 19. URL https://aclanthology.org/2020. emnlp-main. 19. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. ArXiv preprint, abs/1607.06450, 2016. URL https: //arxiv.org/abs/1607.06450. Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. In 7 th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum? $i d=$ ByxZX20qFQ. Bai, S., Kolter, J. Z., and Koltun, V. Trellis networks for sequence modeling. arXiv preprint arXiv:1810.06682, 2018. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 1.00)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 3/25 (Score: 1.00)*\n\n```\nMultiple methods have been proposed to replace the softmax operation. For instance, Katharopoulos et al. (2020a) employ the $1+$ elu activation function, Qin et al. (2022b) utilize the cosine function to approximate softmax properties, and Ke et al. (2021); Zheng et al. (2022; 2023) leverage sampling strategies to directly mimic softmax operation. Despite having a theoretical complexity of $O\\left(n d^{2}\\right)$, the practical computational efficiency of linear attention diminishes notably in causal attention scenarios, primarily due to the necessity for cumsum operations (Hua et al., 2022). ### 2.2. IO-aware Attention\n\nThe FlashAttention series (Dao et al., 2022; Dao, 2023) focuses on system-level optimizations for the efficient implementation of the standard attention operator on GPU platforms. Extensive validation has demonstrated its effectiveness. The approach employs tiling strategies to minimize the volume of memory reads/writes between the GPU's high bandwidth memory (HBM) and on-chip SRAM. To address the issue of slow computation for Linear Attention in the causal setting, Lightning Attention 1 (Qin et al., 2023b) employs the approach of FlashAttention-1/2, which involves segmenting the inputs $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ into blocks, transferring them from slow HBM to fast SRAM, and then computing the attention output with respect to these blocks. Subsequently, the final results are accumulated. Although this method is much more efficient than the PyTorch implementation, it does not take advantage of the computational characteristics inherent to Linear Attention, and the theoretical complexity remains $O\\left(n^{2} d\\right)$. ### 2.3. Long Sequence Handling in LLM\n\nA widely adopted strategy to tackle challenges related to length extrapolation involves the integration of Relative Positional Encoding (RPE) techniques (Su et al., 2021; Qin et al., 2023c), strategically directing attention towards neighboring tokens. ALiBi (Press et al., 2022) utilizes linear decay biases in attention mechanisms to mitigate the impact of distant tokens. Roformer (Su et al., 2021) introduces a novel Rotary Position Embedding (RoPE) method, widely embraced in the community, effectively leveraging positional information for transformer-based language model learning. Kerple (Chi et al., 2022) explores shift-invariant conditionally positive definite kernels within RPEs, introducing a suite of kernels aimed at enhancing length extrapolation properties, with ALiBi recognized as one of its instances. Furthermore, Sandwich (Chi et al., 2023) postulates a hypothesis elucidating the mechanism behind ALiBi , empirically validating it by incorporating the hypothesis into sinusoidal positional embeddings. (Qin et al., 2024) explored the sufficient conditions for additive relative position encoding to have extrapolation capabilities. Instead of investigating the length extrapolation capability of transformers, some works also attempt to directly increase the context window sizes. Chen et al. (2023) introduces Position Interpolation (PI), extending context window sizes of RoPE-based pretrained Large Language Models (LLMs) such as LLaMA models to up to 32768 with minimal finetuning (within 1000 steps). StreamingLLM (Xiao et al., 2023) proposes leveraging the attention sink phenomenon, maintaining the Key and Value information of initial tokens to substantially recover the performance of window attention.\n```\n\n#### 3. Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention (Avg. Score: 1.00)\n\n*Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Chuanfu Xiao, Xingcheng Zhang, Dahua Lin, Chao Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper proposes SampleAttention, an adaptive structured and near-lossless sparse attention, which can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times compared with FlashAttention.\n\n**Abstract:** Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.\n\n##### *Relevant Chunk: No. 8/25 (Score: 1.00)*\n\n```\narXiv preprint arXiv:2312.11805, 2023. [15] Anthropic. Claude. https://www.anthropic.com/claude 2023. [16] Moonshot. Kimi chat. https://kimi.moonshot.cn/, 2023. [17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. [18] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC: Encoding long and structured inputs in transformers. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268-284, Online, November 2020. Association for Computational Linguistics. [19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020. [21] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. [22] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to $1,000,000,000$ tokens. arXiv preprint arXiv:2307.02486, 2023. [23] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Fran\u00e7ois Fleuret. Faster causal attention over large sequences through sparse flash attention. arXiv preprint arXiv:2306.01160, 2023. [25] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. [26] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. In The Twelfth International Conference on Learning Representations, 2023. [27] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [28] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020. [29] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 1.00)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 19/35 (Score: 1.00)*\n\n```\nAlbert Gu, Karan Goel, and Christopher R\u00e9. 2021. Efficiently modeling long sequences with structured state spaces. ArXiv preprint, abs/2111.00396. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724-736, Seattle, United States. Association for Computational Linguistics. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419-1436, Online. Association for Computational Linguistics. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 1318 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51565165.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient attention mechanism, linear complexity, long context\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Efficient Attention via Control Variates\n\n*From Search Query: efficient attention mechanism*\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 2. Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism\n\n*From Search Query: efficient attention mechanism*\n\n*Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Kai Han, Yunhe Wang*\n\n**TL;DR:** This study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations, and implements MAE-style pretraining in the YOLO-series for the first time, allowing Y OLOseries models could be to benefit from unsupervised pretraining.\n\n**Abstract:** In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 72  (*Influential: 6*)\n\n#### 3. Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification\n\n*From Search Query: efficient attention mechanism*\n\n*Jungmin Yun, Mihyeon Kim, Youngbin Kim*\n\n**TL;DR:** This work proposes integrating two strategies: token pruning and token combining, which not only improve the model's performance but also reduce its computational demands.\n\n**Abstract:** Transformer-based models have achieved dominant performance in numerous NLP tasks. Despite their remarkable successes, pre-trained transformers such as BERT suffer from a computationally expensive self-attention mechanism that interacts with all tokens, including the ones unfavorable to classification performance. To overcome these challenges, we propose integrating two strategies: token pruning and token combining. Token pruning eliminates less important tokens in the attention mechanism's key and value as they pass through the layers. Additionally, we adopt fuzzy logic to handle uncertainty and alleviate potential mispruning risks arising from an imbalanced distribution of each token's importance. Token combining, on the other hand, condenses input sequences into smaller sizes in order to further compress the model. By integrating these two approaches, we not only improve the model's performance but also reduce its computational demands. Experiments with various datasets demonstrate superior performance compared to baseline models, especially with the best improvement over the existing BERT model, achieving +5%p in accuracy and +5.6%p in F1 score. Additionally, memory cost is reduced to 0.61x, and a speedup of 1.64x is achieved.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2024\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 4. Linear Complexity Randomized Self-attention Mechanism\n\n*From Search Query: linear complexity*\n\n*Lin Zheng, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** A novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers is proposed and sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA).\n\n**Abstract:** Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 28  (*Influential: 1*)\n\n#### 5. SOFT: Softmax-free Transformer with Linear Complexity\n\n*From Search Query: linear complexity*\n\n*Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, T. Xiang, Li Zhang*\n\n**TL;DR:** For the first time, a softmax-free transformer or SOFT is proposed, which significantly improves the computational efficiency of existing ViT variants and allows much longer token sequences to be permitted in SOFT, resulting in superior trade-off between accuracy and complexity.\n\n**Abstract:** Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 132  (*Influential: 9*)\n\n#### 6. EcoFormer: Energy-Saving Attention with Linear Complexity\n\n*From Search Query: linear complexity*\n\n*Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, Bohan Zhuang*\n\n**TL;DR:** This work proposes a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space and enables most of the expensive multiply-accumulate operations in attention with simple accumulations to save considerable on-chip energy footprint on edge devices.\n\n**Abstract:** Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on the equivalence between the inner product of binary codes and the Hamming distance as well as the associative property of matrix multiplication, we can approximate the attention in linear complexity by expressing it as a dot-product of binary codes. Moreover, the compact binary representations of queries and keys enable us to replace most of the expensive multiply-accumulate operations in attention with simple accumulations to save considerable on-chip energy footprint on edge devices. Extensive experiments on both vision and language tasks show that EcoFormer consistently achieves comparable performance with standard attentions while consuming much fewer resources. For example, based on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy footprint reduction with only a 0.33% performance drop compared to the standard attention. Code is available at https://github.com/ziplab/EcoFormer.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 7. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\n\n*From Search Query: long context*\n\n*Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li*\n\n**TL;DR:** This paper introduces LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understandings of large language models.\n\n**Abstract:** Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 265  (*Influential: 77*)\n\n#### 8. Effective Long-Context Scaling of Foundation Models\n\n*From Search Query: long context*\n\n*Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas O\u011fuz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma*\n\n**TL;DR:** An effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens is presented and ablation results suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance.\n\n**Abstract:** We present an effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens. Our models are built through continual pretraining from Llama 2 checkpoints with longer text sequences and on a dataset where long texts are upsampled. We perform extensive evaluation using language modeling, synthetic context probing tasks, and a wide range of downstream benchmarks. Across all evaluations, our models achieve consistent improvements on most regular-context tasks and significant improvements on long-context tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure that is free of expensive annotation, the presented models can already surpass \\texttt{gpt-3.5-turbo-16k}\u2018s overall performance on long-context benchmarks. Alongside these results, we provide an in-depth analysis on each individual component of our method. We delve into Llama\u2019s position encodings and discuss its key limitation in modeling long data. We examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths \u2013 ablation results suggest that having abundant long texts in the pretrain dataset is \\textit{not} the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 139  (*Influential: 12*)\n\n#### 9. LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression\n\n*From Search Query: long context*\n\n*Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu*\n\n**TL;DR:** This work proposes LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges of higher computational cost, performance reduction, and position bias.\n\n**Abstract:** In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 108  (*Influential: 22*)\n\n### 6 related papers from ArXiv\n\n#### 1. LSTM-based Deep Learning Models for Non-factoid Answer Selection\n\n*From Search Query: efficient attention mechanism*\n\n*Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou*\n\n**Abstract:** In this paper, we apply a general deep learning (DL) framework for the answer\nselection task, which does not depend on manually defined features or\nlinguistic tools. The basic framework is to build the embeddings of questions\nand answers based on bidirectional long short-term memory (biLSTM) models, and\nmeasure their closeness by cosine similarity. We further extend this basic\nmodel in two directions. One direction is to define a more composite\nrepresentation for questions and answers by combining convolutional neural\nnetwork with the basic framework. The other direction is to utilize a simple\nbut efficient attention mechanism in order to generate the answer\nrepresentation according to the question context. Several variations of models\nare provided. The models are examined by two datasets, including TREC-QA and\nInsuranceQA. Experimental results demonstrate that the proposed models\nsubstantially outperform several strong baselines.\n\n**Published:** 2015-11-12T22:01:54Z  (*Updated: 2016-03-28T04:12:45Z*)\n\n\n\n#### 2. Efficient Transformer Knowledge Distillation: A Performance Review\n\n*From Search Query: efficient attention mechanism*\n\n*Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence*\n\n**Abstract:** As pretrained transformer language models continue to achieve\nstate-of-the-art performance, the Natural Language Processing community has\npushed for advances in model compression and efficient attention mechanisms to\naddress high computational requirements and limited input sequence length.\nDespite these separate efforts, no investigation has been done into the\nintersection of these two fields. In this work, we provide an evaluation of\nmodel compression via knowledge distillation on efficient attention\ntransformers. We provide cost-performance trade-offs for the compression of\nstate-of-the-art efficient attention architectures and the gains made in\nperformance in comparison to their full attention counterparts. Furthermore, we\nintroduce a new long-context Named Entity Recognition dataset, GONERD, to train\nand test the performance of NER models on long sequences. We find that\ndistilled efficient attention transformers can preserve a significant amount of\noriginal model performance, preserving up to 98.6% across short-context tasks\n(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context\nQuestion-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on\nlong-context Named Entity Recognition (GONERD), while decreasing inference\ntimes by up to 57.8%. We find that, for most models on most tasks, performing\nknowledge distillation is an effective method to yield high-performing\nefficient attention models with low costs.\n\n**Published:** 2023-11-22T19:19:37Z  (*Updated: 2023-11-22T19:19:37Z*)\n\n\n\n#### 3. Retentive Network: A Successor to Transformer for Large Language Models\n\n*From Search Query: linear complexity*\n\n*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation\narchitecture for large language models, simultaneously achieving training\nparallelism, low-cost inference, and good performance. We theoretically derive\nthe connection between recurrence and attention. Then we propose the retention\nmechanism for sequence modeling, which supports three computation paradigms,\ni.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel\nrepresentation allows for training parallelism. The recurrent representation\nenables low-cost $O(1)$ inference, which improves decoding throughput, latency,\nand GPU memory without sacrificing performance. The chunkwise recurrent\nrepresentation facilitates efficient long-sequence modeling with linear\ncomplexity, where each chunk is encoded parallelly while recurrently\nsummarizing the chunks. Experimental results on language modeling show that\nRetNet achieves favorable scaling results, parallel training, low-cost\ndeployment, and efficient inference. The intriguing properties make RetNet a\nstrong successor to Transformer for large language models. Code will be\navailable at https://aka.ms/retnet.\n\n**Published:** 2023-07-17T16:40:01Z  (*Updated: 2023-08-09T08:53:08Z*)\n\n\n\n#### 4. Superiority of Softmax: Unveiling the Performance Edge Over Linear\n  Attention\n\n*From Search Query: linear complexity*\n\n*Yichuan Deng, Zhao Song, Tianyi Zhou*\n\n**Abstract:** Large transformer models have achieved state-of-the-art results in numerous\nnatural language processing tasks. Among the pivotal components of the\ntransformer architecture, the attention mechanism plays a crucial role in\ncapturing token interactions within sequences through the utilization of\nsoftmax function.\n  Conversely, linear attention presents a more computationally efficient\nalternative by approximating the softmax operation with linear complexity.\nHowever, it exhibits substantial performance degradation when compared to the\ntraditional softmax attention mechanism.\n  In this paper, we bridge the gap in our theoretical understanding of the\nreasons behind the practical performance gap between softmax and linear\nattention. By conducting a comprehensive comparative analysis of these two\nattention mechanisms, we shed light on the underlying reasons for why softmax\nattention outperforms linear attention in most scenarios.\n\n**Published:** 2023-10-18T03:17:57Z  (*Updated: 2023-10-18T03:17:57Z*)\n\n\n\n#### 5. LongReward: Improving Long-context Large Language Models with AI\n  Feedback\n\n*From Search Query: long context*\n\n*Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin Niu, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li*\n\n**Abstract:** Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance.\n\n**Published:** 2024-10-28T17:50:42Z  (*Updated: 2024-10-28T17:50:42Z*)\n\n\n\n#### 6. How to Train Long-Context Language Models (Effectively)\n\n*From Search Query: long context*\n\n*Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen*\n\n**Abstract:** We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- Instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context tasks, and we evaluate models after SFT with instruction data\nas this better reveals long-context abilities. Supported by our robust\nevaluations, we run thorough experiments to decide the data mix for continued\npre-training, the instruction tuning dataset, and many other design choices. We\nfind that (1) code repositories and books are excellent sources of long data,\nbut it is crucial to combine them with high-quality short data; (2) training\nwith a sequence length beyond the evaluation length boosts long-context\nperformance; (3) for SFT, using only short instruction datasets yields strong\nperformance on long-context tasks. Our final model, ProLong-8B, which is\ninitialized from Llama-3 and trained on 40B tokens, demonstrates\nstate-of-the-art long-context performance among similarly sized models at a\nlength of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of\nlong-context tasks despite having seen only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs.\n\n**Published:** 2024-10-03T16:46:52Z  (*Updated: 2024-10-03T16:46:52Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation\n\n*From Search Query: efficient attention mechanism*\n\n*Jianlin Su, Rui Li, Shunyi Zheng, Chenxi Duan*\n\n**Abstract:** In this paper, to remedy this deficiency, we propose a Linear Attention Mechanism which is approximate to dot-product attention with much less memory and computational costs. The efficient design makes the incorporation between attention mechanisms and neural networks more flexible and versatile. Experiments conducted on semantic segmentation demonstrated the effectiveness of linear attention mechanism. Code is available at https://github.com/lironui/Linear-Attention-Mechanism.\n\n**Published:** 2020-07-29\n\n\n\n#### 2. BAM: A Balanced Attention Mechanism for Single Image Super Resolution\n\n*From Search Query: efficient attention mechanism*\n\n*Cheng Shen, Haotian Hu, Fanyi Wang*\n\n**Abstract:** Recovering texture information from the aliasing regions has always been a major challenge for Single Image Super Resolution (SISR) task. These regions are often submerged in noise so that we have to restore texture details while suppressing noise. To address this issue, we propose a Balanced Attention Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to suppress extreme noise in the large scale feature maps while MSAM preserves high-frequency texture details. Thanks to the parallel structure, these two modules not only conduct self-optimization, but also mutual optimization to obtain the balance of noise reduction and high-frequency texture restoration during the back propagation process, and the parallel structure makes the inference faster. To verify the effectiveness and robustness of BAM, we applied it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently improve the networks performance, and for those originally with attention mechanism, the substitution with BAM further reduces the amount of parameters and increases the inference speed. Moreover, we present a dataset with rich texture aliasing regions in real scenes, named realSR7. Experiments prove that BAM achieves better super-resolution results on the aliasing area.\n\n**Published:** 2021-04-15\n\n\n\n#### 3. Linformer: Self-Attention with Linear Complexity\n\n*From Search Query: linear complexity*\n\n*Madian Khabsa, Hao Ma, Sinong Wang, Belinda Z. Li, Han Fang*\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n**Published:** 2020-06-08\n\n\n\n#### 4. Efficient Attention: Attention with Linear Complexities\n\n*From Search Query: linear complexity*\n\n*Shuai Yi, Haiyu Zhao, Hongsheng Li, Zhuoran Shen, Mingyuan Zhang*\n\n**Abstract:** Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.\n\n**Published:** 2018-12-04\n\n\n\n#### 5. Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection\n\n*From Search Query: long context*\n\n*Guanhang Wu, Ronny Votel, Vivek Rathod, Sara Beery, Jonathan Huang*\n\n**Abstract:** In static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame. We apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP.\n\n**Conference:** context-r-cnn-long-term-temporal-context-for\n\n**Published:** 2019-12-07\n\n\n\n#### 6. Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer\n\n*From Search Query: long context*\n\n*Dhabaleswar K. Panda, Hari Subramoni, Aamir Shafi, Olatunji Ruwase, Masahiro Tanaka, Sam Ade Jacobs, Jinghan Yao*\n\n**Abstract:** Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.\n\n**Published:** 2024-08-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient attention mechanisms, linear complexity approaches, and the ability to handle long sequences, here are some key insights and techniques from the provided sources and additional relevant information:\n\n## Efficient Attention Mechanisms\n\n### Multi-Head Density Adaptive Attention Mechanism (DAAM)\nThe DAAM proposed in the paper \"Robust Parameter-Efficient Fine-Tuning Across Multiple Modalities\" introduces a novel probabilistic attention framework. DAAM integrates learnable mean and variance into its attention mechanism, allowing it to dynamically adjust attention based on input context. This approach enhances the model's adaptability to non-stationary data and improves performance, especially in handling long-range dependencies and non-Gaussian data traits.\n\n### Chunk-Based Processing and Selective Attention\nThe LongHeads approach suggests allowing each attention head to process in-distribution length by selecting and attending to important context chunks. This method helps in generalizing to longer sequences without the out-of-distribution issues that standard attention mechanisms face. This chunk-based processing can be more efficient and effective for handling long sequences [Analysis Note].\n\n### Position-Aware Attention Mechanisms\nThe PoSE (Positional Skip-wisE) training method involves dividing the original context window into several chunks and designing distinct skipping bias terms to manipulate the position indices of each chunk. This approach simulates long inputs using a fixed context window, which can improve long-range dependency modeling and reduce computational complexity [Analysis Note].\n\n## Linear Complexity Approaches\n\n### Linear Attention Mechanisms\nTraditional multi-head attention mechanisms in Transformers have quadratic complexity due to the dot-product attention formula. However, recent work like LongHeads and PoSE suggests using linear attention mechanisms or approximations that reduce this complexity. For example, linear attention mechanisms such as those using kernel methods or low-rank approximations can achieve linear complexity while maintaining performance [Analysis Note].\n\n### StreamingLLM with Attention Sinks\nThe StreamingLLM framework introduces an efficient way to handle long sequences by utilizing attention sinks. This method allows large language models to generalize to infinite sequence lengths without fine-tuning, by caching the key and value states of initial tokens and using a placeholder token as a dedicated attention sink. This approach significantly reduces memory usage and computational overhead, making it suitable for streaming applications.\n\n## Handling Long Sequences\n\n### Window Attention and Attention Sinks\nThe StreamingLLM framework demonstrates that window attention, combined with the concept of attention sinks, can efficiently handle long sequences. By keeping the key and value states of initial tokens, the model can recover performance even when the text length surpasses the cache size, ensuring stable and efficient language modeling for up to 4 million tokens.\n\n### Chunk-Based Processing\nBoth LongHeads and PoSE emphasize chunk-based processing as a way to extend context windows efficiently. By dividing the input into chunks and selectively attending to important parts, these methods reduce the computational and memory requirements associated with handling long sequences [Analysis Note].\n\n## Memory Efficiency and Test-Time Adaptation\n\n### Efficient State Representation\nEfficient state representation is crucial for handling long sequences. Approaches like state compression (e.g., adaptivesparselm_1) and selective processing can reduce memory requirements. Innovative architectures such as those using chunk-based processing offer a good balance between efficiency and effectiveness[Analysis Note].\n\n### Test-Time Training Mechanisms\nCurrent approaches mainly focus on linear attention or state compression for test-time training. However, there is an opportunity to develop more efficient test-time training mechanisms, potentially leveraging position-aware adaptation to improve performance. This could involve integrating DAAM or similar adaptive attention mechanisms into test-time training frameworks [Analysis Note].\n\nBy incorporating these techniques, researchers can develop more efficient and scalable autoregressive language models that handle long sequences effectively while maintaining computational efficiency and robustness.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nFrom examining the seed design FastTTTLinear and related references, several key observations emerge:\n\na) Current Landscape:\n- FastTTTLinear combines Gated Linear Attention with test-time training capabilities\n- Existing siblings focus on state compression (adaptivesparselm_1) and RWKV integration (gatedrwkvlinear)\n- Recent work like LongHeads and PoSE shows promise in extending context windows efficiently\n\nb) Key Challenges:\n- Balancing computational efficiency with model expressiveness\n- Managing memory usage for long sequences\n- Maintaining test-time training capabilities while improving efficiency\n- Ensuring stability and robustness of attention mechanisms\n\nc) Opportunities:\n- Exploring novel attention mechanisms beyond standard linear attention\n- Improving position encoding for better long-range dependencies\n- Enhancing test-time adaptation capabilities\n- Optimizing memory usage through innovative architectures\n\n2. Useful References with Excerpts:\n\nFrom LongHeads paper:\n\"Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks.\"\n\nThis suggests potential for chunk-based processing with selective attention.\n\nFrom PoSE paper:\n\"Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window [...] achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk.\"\n\nThis indicates opportunities for innovative positional encoding schemes.\n\n3. Key Insights and Analysis:\n\na) Attention Mechanism:\n- Current linear attention approaches may sacrifice expressiveness for efficiency\n- Chunk-based processing with selective attention shows promise\n- Position-aware attention could improve long-range dependency modeling\n\nb) Memory Management:\n- Efficient state representation is crucial for long sequences\n- Selective processing can reduce memory requirements\n- Chunk-based approaches offer good balance of efficiency and effectiveness\n\nc) Test-Time Adaptation:\n- Current approaches focus mainly on linear attention or state compression\n- Opportunity exists for more efficient test-time training mechanisms\n- Position-aware adaptation could improve performance\n\n4. Future Search Plan:\nNeed to investigate:\n1. Recent advances in position-aware attention mechanisms\n2. Efficient implementations of chunk-based processing\n3. Memory-efficient approaches for test-time training\n\n5. References:\n\n1. Lu, Y., Zhou, X., He, W., Zhao, J., Ji, T., Gui, T., Zhang, Q., & Huang, X. (2024). LongHeads: Multi-Head Attention is Secretly a Long Context Processor. arXiv preprint.\n\n2. Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., & Li, S. (2023). PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. arXiv preprint.\n\n3. Ganesh, A. N. (2024). Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hardware efficient implementation, hierarchical attention, adaptive compression",
            "detail": "Extract information about hardware-efficient implementations of hierarchical attention mechanisms and adaptive compression techniques, focusing on practical implementation details and performance optimizations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nExtract information about hardware-efficient implementations of hierarchical attention mechanisms and adaptive compression techniques, focusing on practical implementation details and performance optimizations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.45)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.45)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n#### 2. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.12)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.12)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 3. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.06)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 3/38 (Score: 0.06)*\n\n```\nThis ensures an accurate profiling of the attention influences to facilitate better compression results. Our contributions are summarized as follows. - Heterogeneous Elastic Rules. We propose heterogeneous elastic rules for masks of each attention head. We formulate MoA compression search space to include a diverse range of elastic rules that tailor the local attention span relative to the input length for each attention head. The heterogeneous elastic rules improve the fact retrieval accuracy of MoA from 25\\% to $98 \\%$ compared with masks with uniform span and scaling function for each head. - Calibration Dataset Construction We emphasize the importance of data engineering in LLM compression. Our findings demonstrate that using datasets with long-range dependencies and referencing the original LLM's responses are crucial for accurately profiling the influences of compression. - Automatic Optimization. We propose an automatic pipeline to find the optimal compression plan encompassing heterogeneous elastic rules for various attention heads. This pipeline can efficiently find the optimal plan within several hours, for example, two hours for compressing Vicuna-13B. Experiments show that MoA achieves $5.5 \\times$ to $6.7 \\times$ throughput improvements on 7 B and 13 B dense LLMs at a $50 \\%$ density (the average of KV-Cache length / input length), with only $1 \\%$ average relative degradation in retrieval accuracy. Additionally, MoA achieves over $90 \\%$ retrieval accuracy with just $25 \\%$ average density, far surpassing sparse attention baselines that need a density of $75 \\%$ to $100 \\%$ for similar performance. On long-context understanding benchmarks, MoA performs comparably to dense models, with a maximum relative performance drop of less than $5 \\%$, which is about one-sixth of that observed with the uniform sparse attention baseline. Our code is available at https://github.com/thu-nics/MoA\n\n## 2 Preliminary and Related work\n\n### 2.1 Attention mechanism\n\nThe Multi-Head Self Attention (MHA) mechanism [62] is crucial to the functionality of LLMs.\n```\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.01)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.01)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 5. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.01)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 35/41 (Score: 0.01)*\n\n```\nIn NAACL, 2018. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and Efficient Posttraining Quantization for Large Language Models. In ICML, 2023. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A Nystr\u00f6m-based Algorithm for Approximating Self-attention. In AAAI, 2021. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated Linear Attention Transformers with Hardware-efficient Training. arXiv preprint arXiv:2312.06635, 2023. You, H., Sun, Z., Shi, H., Yu, Z., Zhao, Y., Zhang, Y., Li, C., Li, B., and Lin, Y. ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 273-286. IEEE, 2023a. You, H., Xiong, Y., Dai, X., Wu, B., Zhang, P., Fan, H., Vajda, P., and Lin, Y. C. Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference. In CVPR, 2023b. You, H., Shi, H., Guo, Y., and Lin, Y. ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. Advances in Neural Information Processing Systems, 36, 2024. Zeng, Z., Xiong, Y., Ravi, S., Acharya, S., Fung, G. M., and Singh, V. You Only Sample (almost) Once: Linear Cost Self-attention via Bernoulli Sampling. In ICML, 2021. Zhang, X., Zhao, J., and LeCun, Y. Character-level Convolutional Networks for Text Classification.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hardware efficient implementation, hierarchical attention, adaptive compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient implementation*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 2. Simple Hardware-Efficient PCFGs with Independent Left and Right Productions\n\n*From Search Query: hardware efficient implementation*\n\n*Wei Liu, Songlin Yang, Yoon Kim, Kewei Tu*\n\n**TL;DR:** This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions, and finds that this formalism scales more effectively both as a language model and as an unsupervised parser.\n\n**Abstract:** Scaling dense PCFGs to thousands of nonterminals via a low-rank parameterization of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions. Despite imposing a stronger independence assumption than the low-rank approach, we find that this formalism scales more effectively both as a language model and as an unsupervised parser. As an unsupervised parser, our simple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language model, it obtains a perplexity of 119.0, outperforming similarly-sized low-rank PCFGs. We further introduce \\emph{FlashInside}, a hardware IO-aware implementation of the inside algorithm for efficiently scaling simple PCFGs.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware efficient implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4300  (*Influential: 554*)\n\n#### 5. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n#### 6. HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level\n\n*From Search Query: hierarchical attention*\n\n*Haoran Luo, E. Haihong, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin*\n\n**TL;DR:** A novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention, that addresses the issue of HKG multi-position prediction for the first time and achieves state-of-the-art performance in link prediction tasks on HKG standard datasets.\n\n**Abstract:** Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs\u2019 representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 7. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n\n*From Search Query: adaptive compression*\n\n*Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao*\n\n**TL;DR:** Adaptive KV cache compression is introduced, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs) and demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss.\n\n**Abstract:** In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 104  (*Influential: 13*)\n\n#### 8. Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering\n\n*From Search Query: adaptive compression*\n\n*Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong*\n\n**TL;DR:** This paper advocates a new principle for ICL: self-adaptive in-context learning, and proposes a general select-then-rank framework and instantiate it with new selection and ranking algorithms.\n\n**Abstract:** Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example organization (i.e., selection and permutation) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code will be released to facilitate future research.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 86  (*Influential: 9*)\n\n#### 9. Adaptive Compression of Word Embeddings\n\n*From Search Query: adaptive compression*\n\n*Yeachan Kim, Kang-Min Kim, SangKeun Lee*\n\n**TL;DR:** This paper proposes a novel method to adaptively compress word embeddings and shows that the model assigns word to each code-book by considering the significance of tasks.\n\n**Abstract:** Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks. However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices). In this paper, we propose a novel method to adaptively compress word embeddings. We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4). However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks. The proposed method works in two steps. First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks. After selecting the code length, each word learns discrete codes through a neural network with a binary constraint. To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks. Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy. Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 19  (*Influential: 2*)\n\n### 4 related papers from ArXiv\n\n#### 1. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 2. A Hierarchical Attention Based Seq2seq Model for Chinese Lyrics\n  Generation\n\n*From Search Query: hierarchical attention*\n\n*Haoshen Fan, Jie Wang, Bojin Zhuang, Shaojun Wang, Jing Xiao*\n\n**Abstract:** In this paper, we comprehensively study on context-aware generation of\nChinese song lyrics. Conventional text generative models generate a sequence or\nsentence word by word, failing to consider the contextual relationship between\nsentences. Taking account into the characteristics of lyrics, a hierarchical\nattention based Seq2Seq (Sequence-to-Sequence) model is proposed for Chinese\nlyrics generation. With encoding of word-level and sentence-level contextual\ninformation, this model promotes the topic relevance and consistency of\ngeneration. A large Chinese lyrics corpus is also leveraged for model training.\nEventually, results of automatic and human evaluations demonstrate that our\nmodel is able to compose complete Chinese lyrics with one united topic\nconstraint.\n\n**Published:** 2019-06-15T06:58:42Z  (*Updated: 2019-06-15T06:58:42Z*)\n\n\n\n#### 3. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural\n  Architecture Search\n\n*From Search Query: adaptive compression*\n\n*Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, Jingren Zhou*\n\n**Abstract:** Large pre-trained language models such as BERT have shown their effectiveness\nin various natural language processing tasks. However, the huge parameter size\nmakes them difficult to be deployed in real-time applications that require\nquick inference with limited resources. Existing methods compress BERT into\nsmall models while such compression is task-independent, i.e., the same\ncompressed BERT for all different downstream tasks. Motivated by the necessity\nand benefits of task-oriented BERT compression, we propose a novel compression\nmethod, AdaBERT, that leverages differentiable Neural Architecture Search to\nautomatically compress BERT into task-adaptive small models for specific tasks.\nWe incorporate a task-oriented knowledge distillation loss to provide search\nhints and an efficiency-aware loss as search constraints, which enables a good\ntrade-off between efficiency and effectiveness for task-adaptive BERT\ncompression. We evaluate AdaBERT on several NLP tasks, and the results\ndemonstrate that those task-adaptive compressed models are 12.7x to 29.3x\nfaster than BERT in inference time and 11.5x to 17.0x smaller in terms of\nparameter size, while comparable performance is maintained.\n\n**Published:** 2020-01-13T14:03:26Z  (*Updated: 2021-01-22T10:58:24Z*)\n\n\n\n#### 4. UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference\n\n*From Search Query: adaptive compression*\n\n*Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Lingpeng Kong, Ngai Wong*\n\n**Abstract:** Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.\n\n**Published:** 2024-10-04T02:32:36Z  (*Updated: 2024-10-04T02:32:36Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Hardware-efficient entangled measurements for variational quantum algorithms\n\n*From Search Query: hardware efficient implementation*\n\n*Anonymous*\n\n**Abstract:** Variational algorithms have received significant attention in recent years due to their potential to solve practical problems using noisy intermediate-scale quantum (NISQ) devices. A fundamental step of these algorithms is the evaluation of the expected value of Hamiltonians, and hence efficient schemes to perform this task are required. The standard approach employs local measurements of Pauli operators and requires a large number of circuits. An alternative is to make use of entangled measurements, which might introduce additional gates between physically disconnected qubits that harm the performance. As a solution to this problem, we propose hardware-efficient entangled measurements (HEEM), that is, measurements that permit only entanglement between physically connected qubits. We show that this strategy enhances the evaluation of molecular Hamiltonians in NISQ devices by reducing the number of circuits required without increasing their depth. We provide quantitative metrics of how this approach offers better results than local measurements and arbitrarily entangled measurements. We estimate the ground-state energy of the H$_2$O molecule with classical simulators and quantum hardware using the variational quantum eigensolver with HEEM.\n\n**Published:** 2022-02-14\n\n\n\n#### 2. HELP: Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning\n\n*From Search Query: hardware efficient implementation*\n\n*Sung Ju Hwang, Song Chong, Sewoong Lee, Hayeon Lee*\n\n**Abstract:** For deployment, neural architecture search should be hardware-aware, in order to satisfy the device-specific constraints (e.g., memory usage, latency and energy consumption) and enhance the model efficiency. Existing methods on hardware-aware NAS collect a large number of samples (e.g., accuracy and latency) from a target device, either builds a lookup table or a latency estimator. However, such approach is impractical in real-world scenarios as there exist numerous devices with different hardware specifications, and collecting samples from such a large number of devices will require prohibitive computational and monetary cost. To overcome such limitations, we propose Hardware-adaptive Efficient Latency Predictor (HELP), which formulates the device-specific latency estimation problem as a meta-learning problem, such that we can estimate the latency of a model's performance for a given task on an unseen device with a few samples. To this end, we introduce novel hardware embeddings to embed any devices considering them as black-box functions that output latencies, and meta-learn the hardware-adaptive latency predictor in a device-dependent manner, using the hardware embeddings. We validate the proposed HELP for its latency estimation performance on unseen platforms, on which it achieves high estimation performance with as few as 10 measurement samples, outperforming all relevant baselines. We also validate end-to-end NAS frameworks using HELP against ones without it, and show that it largely reduces the total time cost of the base NAS method, in latency-constrained settings. Code is available at https://github.com/HayeonLee/HELP.\n\n**Published:** 2021-06-16\n\n\n\n#### 3. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 4. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 5. Adapting Language Models to Compress Contexts\n\n*From Search Query: adaptive compression*\n\n*Danqi Chen, Anirudh Ajith, Alexander Wettig, Alexis Chevalier*\n\n**Abstract:** Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.\n\n**Published:** 2023-05-24\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on hardware-efficient implementations of hierarchical attention mechanisms and adaptive compression techniques, here are some key findings and strategies from the provided sources:\n\n## Hardware-Efficient Implementations of Hierarchical Attention\n\n### SeerAttention\nThe SeerAttention mechanism introduces a learnable gate to adaptively identify significant blocks in the attention map, allowing for block-level sparsity. This approach integrates well with the tiling computation scheme of FlashAttention, which is crucial for hardware efficiency. SeerAttention customizes the FlashAttention kernel to extract block-wise attention map information without maintaining the full-size attention map, resulting in negligible overhead and significant speedup (up to 5.67\u00d7) during long-context fine-tuning.\n\n### Hardware-Aware Context Sharding\nThe concept of sharding the context heterogeneously among attention heads is highlighted in the work on hardware-aware context sharding. Each attention head attends to a different subset of tokens, collectively covering the full context. This approach, combined with hybrid architectures that mix sparse and dense attention, achieves strong performance on downstream tasks and significant speedups (up to 25.3\u00d7) compared to dense attention baselines. This method ensures efficient memory IO and parallelization, making it hardware-friendly.\n\n## Adaptive Compression Techniques\n\n### Adaptive Sparsity in Attention\nSeerAttention's adaptive sparsity learning is a form of adaptive compression. It learns the block-wise attention sparsity from the model itself, allowing the model to dynamically adjust the attention patterns based on the input. This approach reduces I/O and computation overhead by skipping unimportant blocks, which is essential for efficient state management.\n\n### Hybrid Sparse and Dense Attention\nThe hybrid architecture proposed in the context sharding work also involves adaptive compression. By combining sparse and dense attention, the model can adaptively select which tokens to attend to densely and which to attend to sparsely, balancing efficiency and performance. This hybrid approach ensures that the model can handle long contexts efficiently while maintaining strong downstream performance.\n\n## Integration with Position-Aware Mechanisms\n\n### Hierarchical Passage Encoder\nWhile not directly focused on autoregressive language models, the hierarchical passage encoder (HPE) in the hierarchical answer-aware and context-aware network provides insights into multi-level attention mechanisms. HPE uses two-level answer-aware and context-aware attention with multi-hop reasoning, which could be adapted to improve position-aware attention in autoregressive models. This hierarchical approach can help in modeling long-range dependencies more effectively.\n\n## Practical Implementation Details and Performance Optimizations\n\n### Memristor-Based Acceleration\nFor hardware efficiency, the memristor-based accelerator for transformer self-attention is noteworthy. This design leverages in-memory computing to perform matrix operations efficiently, reducing power consumption and latency significantly. Integrating such hardware innovations can enhance the overall efficiency of autoregressive models, especially in edge devices or high-performance computing clusters.\n\n### Tiling and Hierarchy\nThe use of tiling strategies, as mentioned in the analysis note, is crucial for minimizing memory transfers and optimizing computation. Implementations like SeerAttention and hardware-aware context sharding leverage tiling to handle intra-block and inter-block components efficiently, which is essential for practical implementation and performance optimization.\n\nIn summary, to achieve hardware-efficient implementations of hierarchical attention mechanisms and adaptive compression techniques, researchers should focus on:\n\n- **Adaptive Sparsity Learning**: Implement mechanisms like SeerAttention to learn and leverage intrinsic sparsity in attention maps.\n- **Hardware-Aware Context Sharding**: Design attention mechanisms that shard the context among attention heads to optimize for hardware efficiency and parallelization.\n- **Hybrid Sparse and Dense Attention**: Combine sparse and dense attention to balance efficiency and performance.\n- **Memristor-Based Acceleration**: Explore hardware innovations like memristor-based accelerators to enhance efficiency.\n- **Tiling and Hierarchy**: Utilize tiling strategies to optimize memory IO and computation, integrating hierarchical attention mechanisms for better handling of long sequences.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\n\nThe search results reveal several promising directions for improving FastTTTLinear:\n\na) Attention Mechanisms:\n- Linear attention mechanisms achieving O(n) complexity while maintaining performance\n- Chunk-based processing with selective attention showing strong results\n- Hardware-efficient implementations becoming increasingly important\n\nb) Memory Management:\n- Tiling strategies to minimize memory transfers between HBM and SRAM\n- Adaptive compression techniques for state management\n- Efficient caching strategies for long sequences\n\nc) Position Encoding:\n- Position-aware attention mechanisms improving long-range modeling\n- Skip-wise training enabling better handling of long sequences\n- Adaptive position encoding schemes showing promise\n\n2. Useful References with Excerpts:\n\nFrom \"Lightning Attention-2\":\n\"We leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation [...] utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks.\"\n\nFrom \"CHELA\":\n\"We leverage the thought of tiling and hierarchy [...] implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\nFrom \"SampleAttention\":\n\"Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach.\"\n\n3. Key Insights and Analysis:\n\na) Tiling and Hierarchy:\n- Dividing attention computation into intra-block and inter-block components\n- Using different attention mechanisms for different levels of hierarchy\n- Leveraging hardware-aware implementations for efficiency\n\nb) Adaptive Processing:\n- Content-dependent selection of important tokens\n- Two-stage filtering for key-value pairs\n- Dynamic adjustment of attention patterns\n\nc) Implementation Efficiency:\n- IO-aware algorithms crucial for performance\n- Hardware-specific optimizations needed\n- Balance between computation and memory access\n\n4. Future Search Plan:\nNeed to investigate:\n1. Hardware-efficient implementations of hierarchical attention\n2. Adaptive compression techniques for state management\n3. Integration strategies for position-aware mechanisms\n\n5. References:\n\n1. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n2. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n3. Zhu, Q., Duan, J., Chen, C., Liu, S., Li, X., Feng, G., ... & Yang, C. (2024). Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention. arXiv preprint.\n\n4. Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint."
        },
        {
            "ready": false,
            "query": "profile guided optimization, adaptive state management, efficient test-time training",
            "detail": "Extract information about profile-guided optimization techniques and adaptive state management strategies in language models, focusing on methods that maintain efficiency during test-time training.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nExtract information about profile-guided optimization techniques and adaptive state management strategies in language models, focusing on methods that maintain efficiency during test-time training.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.83)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 33/51 (Score: 0.83)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. [22] A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In In Uncertainty in Artificial Intelligence, pages 148-155. Morgan Kaufmann, 1998. [23] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 2022. [24] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling, 2020. [25] Xinyang Geng. EasyLM: A Simple And Scalable Training Framework for Large Language Models. https://github.com/young-geng/EasyLM, mar 2023. https://github.com/ young-geng/EasyLM. [26] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\u00e0, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. arXiv preprint arXiv:2007.04309, 2020. [29] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models.\n```\n\n#### 2. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.55)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.55)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 3. N-grammer: Augmenting Transformers with latent n-grams (Avg. Score: 0.23)\n\n*Aurko Roy, Rohan Anil, Guangda Lai, Benjamin Lee, Jeffrey Zhao, Shuyuan Zhang, Shibo Wang, Ye Zhang, Shen Wu, Rigel Swavely, Tao Yu, Phuong Dao, Christopher Fifty, Z. Chen, Yonghui Wu*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** The N-Grammer is proposed, a simple yet effective modification to the Transformer architecture inspired by the literature in statistical language modeling, by augmenting the model with n-grams that are constructed from a discrete latent representation of the text sequence.\n\n**Abstract:** Transformer models have recently emerged as one of the foundational models in natural language processing, and as a byproduct, there is significant recent interest and investment in scaling these models. However, the training and inference costs of these large Transformer language models are prohibitive, thus necessitating more research in identifying more efficient variants. In this work, we propose a simple yet effective modification to the Transformer architecture inspired by the literature in statistical language modeling, by augmenting the model with n-grams that are constructed from a discrete latent representation of the text sequence. We evaluate our model, the N-Grammer on language modeling on the C4 data-set as well as text classification on the SuperGLUE data-set, and find that it outperforms several strong baselines such as the Transformer and the Primer. We open-source our model for reproducibility purposes in Jax.\n\n##### *Relevant Chunk: No. 14/21 (Score: 0.23)*\n\n```\n## References\n\nNaman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. 2020. Disentangling adaptive gradient methods from learning rates. arXiv preprint arXiv:2002.11803. Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. Etc: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Leon Bottou and Yoshua Bengio. 1995. Convergence properties of the k-means algorithms. In Advances in neural information processing systems, pages 585-592. Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263311. Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L Mercer. 1992. Classbased n-gram models of natural language. Co\u0442\u0440\u0438tational linguistics, 18(4):467-480. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Stanley F Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech \\& Language, 13(4):359-394. Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17241734. Gon\u00e7alo M Correia, Vlad Niculae, and Andr\u00e9 FT Martins. 2019. Adaptively sparse transformers. arXiv preprint arXiv:1909.00015. Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1). John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7). William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961. Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946-2953. Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. arXiv preprint arXiv:1410.5401. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909. Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415. W Ronny Huang, Tara N Sainath, Cal Peyser, Shankar Kumar, David Rybach, and Trevor Strohman. 2021. Lookup-table recurrent language models for long tail speech recognition. arXiv preprint arXiv:2104.04552. Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2011. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128. \u0141ukasz Kaiser, Aurko Roy, Ashish Vaswani, Niki Pamar, Samy Bengio, Jakob Uszkoreit, and Noam Shazeer. 2018. Fast decoding in sequence models using discrete latent variables. arXiv preprint arXiv:1803.03382. Slava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE transactions on acoustics, speech, and signal processing, 35(3):400-401. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models.\n```\n\n#### 4. Scaling Laws of RoPE-based Extrapolation (Avg. Score: 0.12)\n\n*Xiaoran Liu, Hang Yan, Shuo Zhang, Chen An, Xipeng Qiu, Dahua Lin*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 36  (*Influential: 4*)\n\n**TL;DR:** This work proposes a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length, and achieves extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n**Abstract:** The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.12)*\n\n```\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/ P16-1144. URL https://doi.org/10.18653/v1/p16-1144. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. doi: 10.48550/ARXIV.2309. 00071. URLhttps://doi.org/10.48550/arXiv.2309.00071. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Shawn Presser. Books3, 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, pp. 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https: //doi.org/10.1109/SC41405.2020.00024. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve in-context learning of large language models.\n```\n\n#### 5. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.11)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.11)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: profile guided optimization, adaptive state management, efficient test-time training\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Beyond Reward: Offline Preference-guided Policy Optimization\n\n*From Search Query: profile guided optimization*\n\n*Yachen Kang, Dingxu Shi, Jinxin Liu, Li He, Donglin Wang*\n\n**TL;DR:** The offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function, is proposed.\n\n**Abstract:** This study focuses on the topic of offline preference-based reinforcement learning (PbRL), a variant of conventional reinforcement learning that dispenses with the need for online interaction or specification of reward functions. Instead, the agent is provided with fixed offline trajectories and human preferences between pairs of trajectories to extract the dynamics and task information, respectively. Since the dynamics and task information are orthogonal, a naive approach would involve using preference-based reward learning followed by an off-the-shelf offline RL algorithm. However, this requires the separate learning of a scalar reward function, which is assumed to be an information bottleneck of the learning process. To address this issue, we propose the offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function. OPPO achieves this by introducing an offline hindsight information matching objective for optimizing a contextual policy and a preference modeling objective for finding the optimal context. OPPO further integrates a well-performing decision policy by optimizing the two objectives iteratively. Our empirical results demonstrate that OPPO effectively models offline preferences and outperforms prior competing baselines, including offline RL algorithms performed over either true or pseudo reward function specifications. Our code is available on the project website: https://sites.google.com/view/oppo-icml-2023 .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 4*)\n\n#### 2. Meta-SAGE: Scale Meta-Learning Scheduled Adaptation with Guided Exploration for Mitigating Scale Shift on Combinatorial Optimization\n\n*From Search Query: profile guided optimization*\n\n*Jiwoo Son, Minsu Kim, Hyeon-Seob Kim, Jinkyoo Park*\n\n**TL;DR:** Results show that Meta-SAGE outperforms previous adaptation methods and significantly improves scalability in representative CO tasks.\n\n**Abstract:** This paper proposes Meta-SAGE, a novel approach for improving the scalability of deep reinforcement learning models for combinatorial optimization (CO) tasks. Our method adapts pre-trained models to larger-scale problems in test time by suggesting two components: a scale meta-learner (SML) and scheduled adaptation with guided exploration (SAGE). First, SML transforms the context embedding for subsequent adaptation of SAGE based on scale information. Then, SAGE adjusts the model parameters dedicated to the context embedding for a specific instance. SAGE introduces locality bias, which encourages selecting nearby locations to determine the next location. The locality bias gradually decays as the model is adapted to the target instance. Results show that Meta-SAGE outperforms previous adaptation methods and significantly improves scalability in representative CO tasks. Our source code is available at https://github.com/kaist-silab/meta-sage\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 1*)\n\n#### 3. CAME: Confidence-guided Adaptive Memory Efficient Optimization\n\n*From Search Query: profile guided optimization*\n\n*Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, Yang You*\n\n**TL;DR:** This paper proposes CAME, a confidence-guided strategy to reduce the instability of existing memory efficient optimizers and proposes it to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods.\n\n**Abstract:** Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 4. Adaptive Experimental Design with Temporal Interference: A Maximum Likelihood Approach\n\n*From Search Query: adaptive state management*\n\n*P. Glynn, Ramesh Johari, M. Rasouli*\n\n**TL;DR:** Using a novel application of classical martingale analysis of Markov chains via Poisson's equation, efficient designs are characterized via a succinct convex optimization problem to propose a consistent, efficient online experimental design that adaptively samples the two Markov Chains.\n\n**Abstract:** Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different inventory management algorithms in an online retail site. Standard randomized controlled trials are typically not feasible, since the goal is to estimate policy performance on the entire system. Instead, the typical current practice involves dynamically alternating between the two policies for fixed lengths of time, and comparing the average performance of each over the intervals in which they were run as an estimate of the treatment effect. However, this approach suffers from *temporal interference*: one algorithm alters the state of the system as seen by the second algorithm, biasing estimates of the treatment effect. Further, the simple non-adaptive nature of such designs implies they are not sample efficient. \nWe develop a benchmark theoretical model in which to study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies). We assume estimation of the steady state reward for each chain proceeds via nonparametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efficient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martingale analysis of Markov chains via Poisson's equation, we characterize efficient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efficient online experimental design that adaptively samples the two Markov chains.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 26  (*Influential: 0*)\n\n#### 5. Towards Relational POMDPs for Adaptive Dialogue Management\n\n*From Search Query: adaptive state management*\n\n*Pierre Lison*\n\n**TL;DR:** This paper represents the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models and encoded in a small set of general rules expressed as a Markov Logic network.\n\n**Abstract:** Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions of the state and action spaces.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2010\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 6. Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning\n\n*From Search Query: adaptive state management*\n\n*Shenzhi Wang, Qisen Yang, Jiawei Gao, M. Lin, Hao Chen, Liwei Wu, Ning Jia, Shiji Song, Gao Huang*\n\n**TL;DR:** Theoretically, it is proved that state-adaptive balances are necessary for achieving a higher policy performance upper bound, and extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark.\n\n**Abstract:** Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a balance model to select a suitable policy for each state. Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 7. Efficient Test-Time Model Adaptation without Forgetting\n\n*From Search Query: efficient test-time training*\n\n*Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, S. Zheng, P. Zhao, Mingkui Tan*\n\n**TL;DR:** An active sample selection criterion is proposed to identify reliable and non-redundant samples, on which the model is updated to minimize the entropy loss for test-time adaptation, and a Fisher regularizer is introduced to constrain important model parameters from drastic changes.\n\n**Abstract:** Test-time adaptation (TTA) seeks to tackle potential distribution shifts between training and testing data by adapting a given model w.r.t. any testing sample. This task is particularly important for deep models when the test environment changes frequently. Although some recent attempts have been made to handle this task, we still face two practical challenges: 1) existing methods have to perform backward computation for each test sample, resulting in unbearable prediction cost to many applications; 2) while existing TTA solutions can significantly improve the test performance on out-of-distribution data, they often suffer from severe performance degradation on in-distribution data after TTA (known as catastrophic forgetting). In this paper, we point out that not all the test samples contribute equally to model adaptation, and high-entropy ones may lead to noisy gradients that could disrupt the model. Motivated by this, we propose an active sample selection criterion to identify reliable and non-redundant samples, on which the model is updated to minimize the entropy loss for test-time adaptation. Furthermore, to alleviate the forgetting issue, we introduce a Fisher regularizer to constrain important model parameters from drastic changes, where the Fisher importance is estimated from test samples with generated pseudo labels. Extensive experiments on CIFAR-10-C, ImageNet-C, and ImageNet-R verify the effectiveness of our proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 224  (*Influential: 63*)\n\n#### 8. Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction\n\n*From Search Query: efficient test-time training*\n\n*Zeshuai Deng, Zhuokun Chen, Shuaicheng Niu, Thomas H. Li, Bohan Zhuang, Mingkui Tan*\n\n**TL;DR:** An efficient test-time adaptation framework for SR is presented, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types, and designs a second-order degradation scheme to construct paired data based on the degradation type of the test image, predicted by a pre-trained degradation classifier.\n\n**Abstract:** Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, e.g., Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (e.g., blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 9. Efficient Test Time Adapter Ensembling for Low-resource Language Varieties\n\n*From Search Query: efficient test-time training*\n\n*Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, Graham Neubig*\n\n**TL;DR:** This paper finds that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters, and proposes EMA, a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions.\n\n**Abstract:** Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires training a separate language adapter for every language one wishes to support, which can be impractical for languages with limited data. An intuitive solution is to use a related language adapter for the new language variety, but we observe that this solution can lead to sub-optimal performance. In this paper, we aim to improve the robustness of language adapters to uncovered languages without training new adapters. We find that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters. Building upon this observation, we propose Entropy Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. Experiments on three diverse groups of language varieties show that our method leads to significant improvements on both named entity recognition and part-of-speech tagging across all languages.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 30  (*Influential: 6*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Profile Guided Optimization without Profiles: A Machine Learning Approach\n\n*From Search Query: profile guided optimization*\n\n*Chris Cummins, Nadav Rotem*\n\n**Abstract:** Profile guided optimization is an effective technique for improving the optimization ability of compilers based on dynamic behavior, but collecting profile data is expensive, cumbersome, and requires regular updating to remain fresh. We present a novel statistical approach to inferring branch probabilities that improves the performance of programs that are compiled without profile guided optimizations. We perform offline training using information that is collected from a large corpus of binaries that have branch probabilities information. The learned model is used by the compiler to predict the branch probabilities of regular uninstrumented programs, which the compiler can then use to inform optimization decisions. We integrate our technique directly in LLVM, supplementing the existing human-engineered compiler heuristics. We evaluate our technique on a suite of benchmarks, demonstrating some gains over compiling without profile information. In deployment, our technique requires no profiling runs and has negligible effect on compilation time.\n\n**Published:** 2021-12-24\n\n\n\n#### 2. Coz: Finding Code that Counts with Causal Profiling\n\n*From Search Query: profile guided optimization*\n\n*Emery D. Berger, Charlie Curtsinger*\n\n**Abstract:** Improving performance is a central concern for software developers. To locate optimization opportunities, developers rely on software profilers. However, these profilers only report where programs spent their time: optimizing that code may have no impact on performance. Past profilers thus both waste developer time and make it difficult for them to uncover significant optimization opportunities. This paper introduces causal profiling. Unlike past profiling approaches, causal profiling indicates exactly where programmers should focus their optimization efforts, and quantifies their potential impact. Causal profiling works by running performance experiments during program execution. Each experiment calculates the impact of any potential optimization by virtually speeding up code: inserting pauses that slow down all other code running concurrently. The key insight is that this slowdown has the same relative effect as running that line faster, thus \"virtually\" speeding it up. We present Coz, a causal profiler, which we evaluate on a range of highly-tuned applications: Memcached, SQLite, and the PARSEC benchmark suite. Coz identifies previously unknown optimization opportunities that are both significant and targeted. Guided by Coz, we improve the performance of Memcached by 9%, SQLite by 25%, and accelerate six PARSEC applications by as much as 68%; in most cases, these optimizations involve modifying under 10 lines of code.\n\n**Published:** 2016-08-12\n\n\n\n#### 3. Predicting future state for adaptive clinical pathway management\n\n*From Search Query: adaptive state management*\n\n*Anonymous*\n\n**Abstract:** Clinical decision support systems are assisting physicians in providing care\nto patients. However, in the context of clinical pathway management such\nsystems are rather limited as they only take the current state of the patient\ninto account and ignore the possible evolvement of that state in the future. In\nthe past decade, the availability of big data in the healthcare domain did open\na new era for clinical decision support. Machine learning technologies are now\nwidely used in the clinical domain, nevertheless, mostly as a tool for disease\nprediction. A tool that not only predicts future states, but also enables\nadaptive clinical pathway management based on these predictions is still in\nneed. This paper introduces weighted state transition logic, a logic to model\nstate changes based on actions planned in clinical pathways. Weighted state\ntransition logic extends linear logic by taking weights -- numerical values\nindicating the quality of an action or an entire clinical pathway -- into\naccount. It allows us to predict the future states of a patient and it enables\nadaptive clinical pathway management based on these predictions. We provide an\nimplementation of weighted state transition logic using semantic web\ntechnologies, which makes it easy to integrate semantic data and rules as\nbackground knowledge. Executed by a semantic reasoner, it is possible to\ngenerate a clinical pathway towards a target state, as well as to detect\npotential conflicts in the future when multiple pathways are coexisting. The\ntransitions from the current state to the predicted future state are traceable,\nwhich builds trust from human users on the generated pathway.\n\n**Published:** 2020-10-20\n\n\n\n#### 4. Data Interpreter: An LLM Agent For Data Science\n\n*From Search Query: adaptive state management*\n\n*Zhibin Gou, Taicheng Guo, Lingyao Zhang, Jiaqi Chen, Danyang Li, Chenxing Wei, Ceyao Zhang, Wei Tao, Binhao Wu, Bang Liu, Xiawu Zheng, Min Yang, Li Zhang, Chenglin Wu, Zongze Xu, Yuheng Cheng, Yaying Fei, Xinbing Liang, Xiangtao Lu, Xiangru Tang, Tuo Zhou, Mingchen Zhuge, Jinlin Wang, Jiayi Zhang, Bangbang Liu, Yizhang Lin, Sirui Hong*\n\n**Abstract:** Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.\n\n**Published:** 2024-02-28\n\n\n\n#### 5. Selecting Informative Contexts Improves Language Model Finetuning\n\n*From Search Query: efficient test-time training*\n\n*Nicole Beckage, Richard Antonello, Javier Turek, Alexander Huth*\n\n**Abstract:** Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a test metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning -- we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.\n\n**Published:** 2020-05-01\n\n\n\n#### 6. Learning to Remember Rare Events\n\n*From Search Query: efficient test-time training*\n\n*Ofir Nachum, \u0141ukasz Kaiser, Samy Bengio, Aurko Roy*\n\n**Abstract:** Despite recent advances, memory-augmented deep neural networks are still\nlimited when it comes to life-long and one-shot learning, especially in\nremembering rare events. We present a large-scale life-long memory module for\nuse in deep learning. The module exploits fast nearest-neighbor algorithms for\nefficiency and thus scales to large memory sizes. Except for the\nnearest-neighbor query, the module is fully differentiable and trained\nend-to-end with no extra supervision. It operates in a life-long manner, i.e.,\nwithout the need to reset it during training.\n  Our memory module can be easily added to any part of a supervised neural\nnetwork. To show its versatility we add it to a number of networks, from simple\nconvolutional ones tested on image classification to deep sequence-to-sequence\nand recurrent-convolutional models. In all cases, the enhanced network gains\nthe ability to remember and do life-long one-shot learning. Our module\nremembers training examples shown many thousands of steps in the past and it\ncan successfully generalize from them. We set new state-of-the-art for one-shot\nlearning on the Omniglot dataset and demonstrate, for the first time, life-long\none-shot learning in recurrent neural networks on a large-scale machine\ntranslation task.\n\n**Published:** 2017-03-09\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, focusing on profile-guided optimization and adaptive state management, especially during test-time training, here are some relevant insights and techniques:\n\n## Profile-Guided Optimization\n\n### General Concept\nProfile-guided optimization (PGO) involves using runtime profiling data to optimize the performance of a program. In the context of language models, this can be adapted to optimize attention mechanisms and other components based on actual usage patterns.\n\n- **Dynamic Profiling**: Similar to how PGO is used in compilers, language models can benefit from dynamic profiling to identify frequently executed paths or attention patterns. This information can be used to optimize the model's architecture and training process.\n\n### Application in Language Models\n- **Attention Mechanisms**: Profile-guided optimization can be applied to attention mechanisms by analyzing which attention heads or patterns are most frequently used. This can help in optimizing the attention computation, such as discarding non-special tokens or evicting long-range contexts, as suggested in \"Model Tells You What to Discard\".\n  - For example, targeted profiling can help discern the intrinsic structure of attention modules, allowing for more efficient computation by focusing on local contexts and special tokens.\n\n## Adaptive State Management\n\n### Content-Dependent Selection\n- **Dynamic Compression**: Adaptive state management can involve dynamic compression based on importance metrics. For instance, models can use uncertainty-aware long-context compressors to efficiently manage long contexts during inference, as proposed in \"UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference\".\n  - This approach ensures that only the most relevant information is retained, reducing memory and computational requirements.\n\n### Hierarchical State Representation\n- **Hierarchical Processing**: Hierarchical state representation can reduce the memory footprint and improve efficiency. Techniques like CHELA, which implements linear attention in a divide-and-conquer manner, can be beneficial. This approach allows for global abstraction and data-dependent selection, maintaining real linear complexity.\n  - Tiling and hierarchical processing strategies can also be used for memory optimization, enabling better parallelization and reducing memory movement.\n\n### Adaptive Mechanisms in Test-Time Training\n- **Reinforcement Learning for System Optimization (RLSO)**: The RLSO approach, as described in \"End-to-end Training for Recommendation with Language-based User Profiles,\" can be adapted for optimizing language models. This involves using reinforcement learning to optimize the model based on rewards from downstream tasks, ensuring the model aligns with the desired performance metrics during test-time training.\n  - This method can be extended to optimize the state management and attention patterns in language models, ensuring they are adaptive and efficient during test-time.\n\n## Efficient Test-Time Training\n\n### IO-Aware Algorithms\n- **Block-Based Processing**: IO-aware algorithms and block-based processing can significantly improve performance during test-time training. By processing inputs in blocks and selectively computing only the necessary parts, models can reduce computational overhead and improve parallelization.\n\n### Hardware-Specific Optimizations\n- **NVIDIA NIM**: Utilizing tools like NVIDIA NIM, which provides optimized engines for various LLM architectures and scalable deployment options, can help in achieving efficient test-time training. NIM abstracts away model inference internals and offers high-performance features such as scalable deployment and flexible integration, which are crucial for efficient test-time training.\n\nIn summary, to improve autoregressive language model design, researchers can:\n- Use profile-guided optimization to identify and optimize frequently used attention patterns and model components.\n- Implement adaptive state management techniques such as dynamic compression, hierarchical state representation, and content-dependent selection.\n- Employ reinforcement learning methods like RLSO to optimize the model during test-time training.\n- Leverage IO-aware algorithms, block-based processing, and hardware-specific optimizations to ensure efficient computation and memory management.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\n\nThe search results reveal several promising directions for enhancing FastTTTLinear:\n\na) Hardware-Efficient Attention:\n- Gated Linear Attention (GLA) with hardware-efficient training shows strong potential\n- CHELA's divide-and-conquer approach for linear attention implementation\n- Tiling and hierarchical processing strategies for memory optimization\n\nb) Adaptive Mechanisms:\n- Content-dependent selection for efficient processing\n- Dynamic compression based on importance\n- Hybrid sparse-dense attention patterns\n\nc) Implementation Strategies:\n- IO-aware algorithms crucial for performance\n- Block-based processing with selective computation\n- Hardware-specific optimizations for different platforms\n\n2. Useful References with Excerpts:\n\nFrom \"Gated Linear Attention Transformers\":\n\"FLASHLINEARATTENTION trades off memory movement against parallelizability [...] faster than FLASHATTENTION-2 even on short sequence lengths (e.g., 1K). [...] especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\"\n\nFrom \"CHELA\":\n\"leverage the thought of tiling and hierarchy to propose CHELA [...] implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\nFrom \"Model Tells You What to Discard\":\n\"conduct targeted profiling to discern the intrinsic structure of attention modules [...] evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens.\"\n\n3. Key Insights and Analysis:\n\na) Hardware-Efficient Implementation:\n- Tiling strategies crucial for memory efficiency\n- Block-based processing enables better parallelization\n- IO-aware algorithms can significantly improve performance\n- Hardware-specific optimizations needed for different platforms\n\nb) Adaptive Processing:\n- Content-dependent selection can reduce computation\n- Dynamic compression based on importance metrics\n- Hybrid attention patterns balance efficiency and effectiveness\n- Profile-guided optimization for attention patterns\n\nc) Memory Management:\n- Hierarchical state representation reduces memory footprint\n- Selective caching strategies improve efficiency\n- Block-wise processing enables better memory utilization\n- Adaptive compression reduces memory requirements\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for profile-guided optimization in attention mechanisms\n2. Methods for combining hierarchical and linear attention efficiently\n3. Strategies for adaptive state management in test-time training\n\n5. References:\n\n1. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n2. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n3. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., & Gao, J. (2023). Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. International Conference on Learning Representations.\n\n4. Xiong, J., et al. (2024). UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hierarchical attention, efficient gradient computation, hardware optimization",
            "detail": "Extract information about techniques for combining hierarchical attention mechanisms with efficient gradient computation, focusing on hardware-aware implementations and optimization strategies.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nExtract information about techniques for combining hierarchical attention mechanisms with efficient gradient computation, focusing on hardware-aware implementations and optimization strategies.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.28)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.28)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.21)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.32)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n##### *Relevant Chunk: No. 3/32 (Score: 0.10)*\n\n```\nIt is worth noting that their complexity is essentially quadratic, and the corresponding linear versions both suffer performance degradation (see Fig. 1 left). While it is true that the simple use of a chunk linearization strategy can rival the speed of SSMs, the drop in performance is significant. We, therefore, abandon this strategy and the softmax function to accelerate the linear attention operations of the kernel-based approach. In addition, long convolution (Fu et al., 2023b) is more efficient and easier to implement than the traditional SSMs. We believe combining these two modules could release the true power of the hybrid model with faster speed and better performance. However, the existence of such a dilemma is worth considering: (1) the linear attention in hybrid models is a speed bottleneck for long convolution, the comparison is shown in Fig. 1 right; (2) the instability of long convolution makes attention hard to optimize. Specifically, due to the intense memory access (I/O) on the GPU and cumulative summation (cumsum) operation in a casual setting,\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_542e0cd768b54533ad80g-02.jpg?height=550&width=1454&top_left_y=224&top_left_x=300)\n\nFigure 1. Demonstration of (left) comparison of various popular models on the Long Range Arena Dataset, and (right) speed benchmark on different implementations of attentions. Figure left, despite showing promising performance of SSM-Attention models, the linear version of these models degenerated. Figure right benchmarks attention speed with around 200 M parameters, showing linear attention is not linear with sequence length and is significantly slower than S 4 , which can be a speed bottleneck in these hybrid designs. the notable reduction in complexity from $\\mathcal{O}\\left(L^{2}\\right)$ to $\\mathcal{O}(L)$ in linear attention is only theoretical (Qin et al., 2024); On the other hand, the long convolution needs hand-crafted regularizations to prevent over-fitting high-frequency noise. To escape the dilemma, we have developed a novel model called CHELA, which stands for short-long Convolutional with Hardware-Efficient $\\underline{\\text { Linear }} \\underline{\\text { Attention. This model is }}$ designed to effectively and efficiently capture complex dependencies in long inputs. Inspired by FlashAttention (Dao et al., 2022; Dao, 2023), the model comprises multiple layers of real linear token mixers with hardware-friendly implementation with a gating mechanism that achieves comparable performance to full attention. We have added a stabilizer, reparameterizable (Ding et al., 2022; 2023) short-long convolutions, to the bottom layer of the long convolution to integrate multiple frequencies with inputs. As a result, the top layers of CHELA capture more refined data-dependent information using linear attention, while the convolutions add a structural bias that enhances global abstraction in multi-level frequencies. We demonstrate the efficiency and effectiveness of CHELA on various datasets and tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (LRA) benchmark (Tay et al., 2020b), which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, CHELA is not only significantly faster than the vanilla Transformer but also yields better performance. In all the settings, CHELA outperforms the baselines. Finally, we provide further analysis and ablation experiments to demonstrate the effectiveness. Our contribution can be summarized in three folds:\n\n- Efficient Implementation: We release the potential speed of linear attention in the attention-ssm hybrid model, maintaining a constant rate regardless of sequence length with fixed memory. - New Design: We improve multi-frequency learning with multi-level convolutional hierarchies, leading to better performance and stability for long convolution. - Promising Results: By connecting the above two basic modules, we propose CHELA for efficient long sequence learners are capable of both global information abstraction and fine-grained data-dependent selection.\n```\n\n#### 3. Compositional Attention: Disentangling Search and Retrieval (Avg. Score: 0.14)\n\n*Sarthak Mittal, S. Raparthy, I. Rish, Yoshua Bengio, Guillaume Lajoie*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.\n\n**Abstract:** Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.\n\n##### *Relevant Chunk: No. 23/40 (Score: 0.14)*\n\n```\narXiv preprint arXiv:2012.14601, 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015. ## APPENDIX\n\n## A RELATED WORK\n\nThe advent of transformer-like models have led to advancements on various flavours of attention based models. This revolution first started with augmenting Recurrent Neural Networks (RNNs) with a form of semi-parametric memory structure through attention (Bahdanau et al., 2015) and it soon led to people questioning the need for recurrence. This line of questioning resulted in a famous class of models that get rid of recurrence in favour of just parallel self-attention computations that are quite efficient to do on modern hardware (Vaswani et al., 2017). We briefly discuss the various advances along these lines and distinguish how our proposed attention algorithm is different from them. ## A. 1 ATTENTION\n\nAttention has been a major component of human cognition which allows humans to selectively process relevant information from the plethora of sensory stimulus we receive. The idea of selecting relevant features from a sea of information allows us to make predictions in both a robust as well as compute efficient way. Inspired from neural cognition, there have been a lot of efforts in trying to introduce a notion of attention to relevant states of the input for reliable downstream prediction ( Xu et al., 2015; Luong et al., 2015; Kerg et al., 2020). A major problem in Recurrent Neural Networks based systems is the problem of vanishing and exploding gradients that happens due to improper credit assignment in the model. This is because RNNs model all the information seen up to a certain time through a parametric fixed sized vector which undergoes repeated computations over all time steps. This makes the system brittle to changes in sequence lengths or in presence of long sequence of distracting information. A way to solve this problem was to move away from parametric representations of the entire past and instead rely on dynamic semi-parametric \"memory\" to allow these models to look back whenever needed (Graves et al., 2014; Bahdanau et al., 2015). These works aimed at augmenting recurrence with self-attention and demonstrated that when combined with these cognition-inspired inductive biases, ML systems were able to extrapolate much better to larger sequence lengths. Following this, there has been a lot of recent work that then aimed to remove recurrence between timesteps and rely solely on querying information through self-attention. Recent advances on multiple domains (Vaswani et al., 2017; Dosovitskiy et al., 2020; Ding et al., 2020; Locatello et al., 2020) showcased that removing recurrence from the picture and relying solely on parallel computations not only leads to significant improvements in performance and generalization but is also easier and faster to train on current hardware. Since the advent of these transformer based models built fundamentally on multi-head attention, the role of attention has become increasingly important across various domains like vision, language and reinforcement learning. It has also led to a lot of research on various architectural choices in fully attention-based systems, some of which we discuss in Appendix A.2. It is, however, important to note that there has been some research that highlight the need for recurrence jointly with self-attention for solving certain logical reasoning tasks efficiently (Hudson \\& Manning, 2018; Selvakumar et al., 2018; Webb et al., 2020). ## A. 2 TRANSFORMER VARIANTS\n\nThe ubiquity of self-attention models in the current ML community has led to tremendous research aimed at incorporating different inductive biases in the attention mechanism used; namely in the multi-head attention. Most of these variants aim to alter multi-head attention in a way that would remove the quadratic time complexity computational bottleneck that is present in standard multi-head attention. However, there are certain works that aim more on the fundamental inductive biases that the attention encodes as opposed to computational benefits. We discuss some of these variants here. Reducing Computational Complexity. Given a set of $n$ vectors, the standard multi-head attention aims to create an $n \\times n$ attention matrix that takes quadratic complexity to compute. This bottleneck prevents usage of self-attention when $n$ is large. In light of this, a lot of recent research aims to reduce this quadratic complexity to $n \\log n$ or linear complexity. This is often achieved by either introducing some restrictions in the $n \\times n$ attention matrix through locality sensitive hashing (Kitaev et al., 2020),\nsparsity (Child et al., 2019), low rank approximation (Wang et al., 2020) or through random features for approximation of softmax (Choromanski et al., 2020).\n```\n\n#### 4. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.14)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 14/40 (Score: 0.14)*\n\n```\non pp. 1-4, 12, 16, 19, 29, 30). [13] Songlin Yang et al. \"Gated Linear Attention Transformers with Hardware-Efficient Training\". In: arXiv preprint arXiv:2312.06635 (2023) (cit.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical attention, efficient gradient computation, hardware optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4300  (*Influential: 554*)\n\n#### 2. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n#### 3. HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level\n\n*From Search Query: hierarchical attention*\n\n*Haoran Luo, E. Haihong, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin*\n\n**TL;DR:** A novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention, that addresses the issue of HKG multi-position prediction for the first time and achieves state-of-the-art performance in link prediction tasks on HKG standard datasets.\n\n**Abstract:** Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs\u2019 representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n\n*From Search Query: efficient gradient computation*\n\n*Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu*\n\n**TL;DR:** It is proved that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size.\n\n**Abstract:** Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \\emph{Gradient-based One-Side Sampling} (GOSS) and \\emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \\emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 8733  (*Influential: 936*)\n\n#### 5. Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses\n\n*From Search Query: efficient gradient computation*\n\n*Corinna Cortes, Vitaly Kuznetsov, M. Mohri, Dmitry Storcheus, Scott Yang*\n\n**TL;DR:** Efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses are designed that facilitate efficient gradient computation and enable one to train learning models such as neural networks with complex structured losses.\n\n**Abstract:** Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or $n$-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\\\"{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the $n$-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias\n\n*From Search Query: efficient gradient computation*\n\n*Ryo Karakida, Tomoumi Takase, Tomohiro Hayase, Kazuki Osawa*\n\n**TL;DR:** This study reveals that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational cost of GR and shows that the finite-Difference computation also works better in the sense of generalization performance.\n\n**Abstract:** Gradient regularization (GR) is a method that penalizes the gradient norm of the training loss during training. While some studies have reported that GR can improve generalization performance, little attention has been paid to it from the algorithmic perspective, that is, the algorithms of GR that efficiently improve the performance. In this study, we first reveal that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational cost of GR. Next, we show that the finite-difference computation also works better in the sense of generalization performance. We theoretically analyze a solvable model, a diagonal linear network, and clarify that GR has a desirable implicit bias to so-called rich regime and finite-difference computation strengthens this bias. Furthermore, finite-difference GR is closely related to some other algorithms based on iterative ascent and descent steps for exploring flat minima. In particular, we reveal that the flooding method can perform finite-difference GR in an implicit way. Thus, this work broadens our understanding of GR for both practice and theory.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 11  (*Influential: 4*)\n\n#### 7. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks\n\n*From Search Query: hardware optimization*\n\n*Y. Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi, Vikas Chandra, Yingyan Lin*\n\n**TL;DR:** This work opens up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy, and proposes a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient Dnns that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real- hardware efficiency.\n\n**Abstract:** Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 8. Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces\n\n*From Search Query: hardware optimization*\n\n*Leonard Papenmeier, Luigi Nardi, Matthias Poloczek*\n\n**TL;DR:** This paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality that reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems.\n\n**Abstract:** Impactful applications such as materials discovery, hardware design, neural architecture search, or portfolio optimization require optimizing high-dimensional black-box functions with mixed and combinatorial input spaces. While Bayesian optimization has recently made significant progress in solving such problems, an in-depth analysis reveals that the current state-of-the-art methods are not reliable. Their performances degrade substantially when the unknown optima of the function do not have a certain structure. To fill the need for a reliable algorithm for combinatorial and mixed spaces, this paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality. Comprehensive experiments show that Bounce reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 2. A Hierarchical Attention Based Seq2seq Model for Chinese Lyrics\n  Generation\n\n*From Search Query: hierarchical attention*\n\n*Haoshen Fan, Jie Wang, Bojin Zhuang, Shaojun Wang, Jing Xiao*\n\n**Abstract:** In this paper, we comprehensively study on context-aware generation of\nChinese song lyrics. Conventional text generative models generate a sequence or\nsentence word by word, failing to consider the contextual relationship between\nsentences. Taking account into the characteristics of lyrics, a hierarchical\nattention based Seq2Seq (Sequence-to-Sequence) model is proposed for Chinese\nlyrics generation. With encoding of word-level and sentence-level contextual\ninformation, this model promotes the topic relevance and consistency of\ngeneration. A large Chinese lyrics corpus is also leveraged for model training.\nEventually, results of automatic and human evaluations demonstrate that our\nmodel is able to compose complete Chinese lyrics with one united topic\nconstraint.\n\n**Published:** 2019-06-15T06:58:42Z  (*Updated: 2019-06-15T06:58:42Z*)\n\n\n\n#### 3. The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\n\n*From Search Query: hardware optimization*\n\n*Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei*\n\n**Abstract:** Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.\n\n**Published:** 2024-02-27T18:56:19Z  (*Updated: 2024-02-27T18:56:19Z*)\n\n\n\n#### 4. BrainTransformers: SNN-LLM\n\n*From Search Query: hardware optimization*\n\n*Zhengzheng Tang, Eva Zhu*\n\n**Abstract:** This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.\n\n**Published:** 2024-10-03T14:17:43Z  (*Updated: 2024-10-23T03:05:37Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 2. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 3. Efficient Per-Example Gradient Computations\n\n*From Search Query: efficient gradient computation*\n\n*Ian Goodfellow*\n\n**Abstract:** This technical report describes an efficient technique for computing the norm\nof the gradient of the loss function for a neural network with respect to its\nparameters. This gradient norm can be computed efficiently for every example.\n\n**Published:** 2015-10-07\n\n\n\n#### 4. AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning\n\n*From Search Query: efficient gradient computation*\n\n*Rishabh Iyer, Ganesh Ramakrishnan, Lucian Popa, Alexandre V. Evfimievski, Aakriti, Guttu Sai Abhishek, KrishnaTeja Killamsetty*\n\n**Abstract:** Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset for different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3$\\times$-30$\\times$ while achieving comparable performance to the hyper-parameters found using the entire dataset.\n\n**Published:** 2022-03-15\n\n\n\n#### 5. Data-Driven Offline Optimization For Architecting Hardware Accelerators\n\n*From Search Query: hardware optimization*\n\n*Sergey Levine, Kevin Swersky, Milad Hashemi, Amir Yazdanbakhsh, Aviral Kumar*\n\n**Abstract:** Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a \"simulation-driven\" approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a \"data-driven\", offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators -- tailored towards both single and multiple applications -- improving performance upon state-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x.\n\n**Conference:** data-driven-offline-optimization-for\n\n**Published:** 2021-10-20\n\n\n\n#### 6. Efficient and Modular Implicit Differentiation\n\n*From Search Query: hardware optimization*\n\n*Jean-Philippe Vert, Fabian Pedregosa, Felipe Llinares-L\u00f3pez, Stephan Hoyer, Roy Frostig, Marco Cuturi, Quentin Berthet, Mathieu Blondel*\n\n**Abstract:** Automatic differentiation (autodiff) has revolutionized machine learning. It allows to express complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization layers, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, so far, implicit differentiation remained difficult to use for practitioners, as it often required case-by-case tedious mathematical derivations and implementations. In this paper, we propose automatic implicit differentiation, an efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function $F$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of $F$ and the implicit function theorem to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many existing implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.\n\n**Conference:** efficient-and-modular-implicit-1\n\n**Published:** 2021-05-31\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by combining hierarchical attention mechanisms with efficient gradient computation and focusing on hardware-aware implementations and optimization strategies, here are some key insights and techniques:\n\n## Hierarchical Attention Mechanisms\n\n- **Hierarchical State Representation and Adaptive Compression**: Implementing hierarchical state representation can reduce the memory footprint and improve efficiency. Techniques like those mentioned in the analysis, such as \"hierarchical state representation with adaptive compression\" and \"content-dependent selection mechanisms,\" can be crucial. For example, the \"CHELA\" approach uses a divide-and-conquer method for linear attention, which maintains real linear complexity and allows for data-dependent selection, potentially reducing computational overhead.\n\n- **Chunk-Based Processing and Selective Attention**: Methods like \"Lightning Attention-2\" suggest tiling strategies where intra-block and inter-block components are handled separately. This approach can be extended to hierarchical attention by applying conventional attention within blocks and linear attention kernel tricks for inter-block interactions, which can be more efficient and scalable.\n\n## Efficient Gradient Computation\n\n- **Profile-Guided Optimization and Dynamic Adaptation**: Runtime profiling can help identify frequently used attention patterns, allowing for dynamic adaptation of attention mechanisms based on usage. This can be combined with hardware-specific optimizations to enhance efficiency. For instance, selective computation based on importance metrics can reduce the computational load during training and inference.\n\n- **Adaptive Gradient Management**: Techniques such as adaptive gradient management and Fisher regularization can help in stabilizing the model during test-time training. This ensures that important model parameters do not undergo drastic changes, which can be particularly beneficial when combined with hierarchical attention mechanisms.\n\n## Hardware-Aware Implementations and Optimization\n\n- **Hardware-Efficient Linear Attention**: Implementations like those described in \"Short-Long Convolutions Help Hardware-Efficient Linear Attention\" focus on making linear attention more hardware-efficient. This involves optimizing the attention computation to better utilize hardware resources, such as using convolutions to handle long sequences efficiently.\n\n- **Memristor-Based Acceleration**: The use of memristor-based in-memory computing, as described in the paper on \"Efficient memristor accelerator for transformer self-attention,\" can significantly accelerate attention computations. This approach integrates computation into memory, reducing the need for data movement and enhancing overall efficiency.\n\n## Test-Time Optimization and Memory Efficiency\n\n- **Active Sample Selection and IO-Aware Algorithms**: Techniques like active sample selection and IO-aware algorithms can optimize test-time training by focusing on reliable and non-redundant samples. This, combined with memory-efficient caching strategies, can improve the overall performance and efficiency of the model.\n\n- **Tiling Strategies and Block-Based Processing**: Tiling strategies, as mentioned in \"Lightning Attention-2,\" can help in managing memory efficiently by handling intra-block and inter-block components separately. This approach enables better parallelization and reduces the memory footprint, making it more suitable for hardware-aware implementations.\n\nBy integrating these techniques, researchers can develop autoregressive language models that leverage hierarchical attention mechanisms, efficient gradient computation, and hardware-aware optimizations, leading to improved performance, efficiency, and scalability.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\n\nAfter three rounds of searches, several promising directions have emerged for enhancing FastTTTLinear:\n\na) Efficient State Management:\n- Profile-guided optimization for dynamic attention patterns\n- Hierarchical state representation with adaptive compression\n- Content-dependent selection mechanisms\n- Hardware-aware implementations\n\nb) Test-Time Optimization:\n- Adaptive gradient management\n- Selective computation based on importance\n- Memory-efficient caching strategies\n- IO-aware algorithms\n\nc) Architecture Innovations:\n- Chunk-based processing with selective attention\n- Position-aware mechanisms for better long-range modeling\n- Hybrid attention patterns combining local and global information\n- Hardware-efficient implementations\n\n2. Useful References with Excerpts:\n\nFrom \"Efficient Test-Time Model Adaptation\":\n\"We propose an active sample selection criterion to identify reliable and non-redundant samples [...] introduce a Fisher regularizer to constrain important model parameters from drastic changes.\"\n\nThis suggests potential for selective computation during test-time training.\n\nFrom \"Lightning Attention-2\":\n\"We leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation [...] utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks.\"\n\nThis indicates opportunities for hierarchical attention processing.\n\nFrom \"CHELA\":\n\"implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\nThis shows promise for efficient attention computation.\n\n3. Key Insights and Analysis:\n\na) Profile-Guided Optimization:\n- Runtime profiling can identify frequently used attention patterns\n- Dynamic adaptation of attention mechanisms based on usage\n- Hardware-specific optimizations for different platforms\n- Selective computation based on importance metrics\n\nb) State Management:\n- Hierarchical representation reduces memory footprint\n- Content-dependent selection improves efficiency\n- Adaptive compression based on importance\n- Block-based processing enables better parallelization\n\nc) Test-Time Training:\n- Active sample selection for efficient adaptation\n- Fisher regularization for stability\n- IO-aware algorithms for better performance\n- Memory-efficient caching strategies\n\nd) Implementation Considerations:\n- Tiling strategies for memory efficiency\n- Hardware-aware implementations\n- Balance between computation and memory access\n- Efficient gradient management\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for combining hierarchical attention with test-time training\n2. Methods for efficient gradient computation in adaptive settings\n3. Hardware-specific optimizations for different platforms\n\n5. References:\n\n1. Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., & Tan, M. (2022). Efficient Test-Time Model Adaptation without Forgetting. International Conference on Machine Learning.\n\n2. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n3. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n4. Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint.\n\n5. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n6. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., & Gao, J. (2023). Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "GLA implementation guides, RWKV performance benchmarks, Linear attention theoretical foundations",
            "detail": "Practical implementation challenges and solutions for GLA and RWKV",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nPractical implementation challenges and solutions for GLA and RWKV\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.93)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 42/64 (Score: 0.93)*\n\n```\nSection 9. Experiments for figures 6 and 7. Proofreading full manuscript. Atsushi Saito Section 1, 5, 8.1 and 8.2. Experiments for 8.2. Proofreading and adding citations. Guangyu Song Section 8.2. Initial draft sections 1, 11. Experiments for 8.2. Contributions to table 1 . Proofreading and citations. Haoqin Tu Section 8.4, experiments for Table 5. Proofreading full manuscript. Stanis\u0142aw Wo\u017aniak Experiments with zero-shot evaluation on additional NLP tasks (Sec. F.4). Bart\u0142omiej Koptyra Zero-shot evaluation comparison of Raven and Eagle 7B on additional NLP tasks (Sec. F.4). Aleksander Szcz\u0119sny Conducted experiments on given datasets tasks: TextEntail, GoEmo, PolEmo, WNLI (Sec. F.4). Ruichong Zhang Initial paper structure organization, draft sections 3, 4, 5 and appendices E, G and H. Experiments for music of section 10.1 and alignment of section F.1. Figure 8 and 10. Additional work on section 11 and appendix B. Bingchen Zhao Section F.2, experiments for Figure 11. Proofreading full manuscript. Qihang Zhao Section 2, Tables 1. Proofreading and revisions. Peng Zhou Section 2, Tables 1, appendices C,I. Proofreading and revisions. Jian Zhu Initial draft sections 2 and C. Captions of Table 4, 3 and 8. Fixing citations and formatting the whole manuscript. Proofreading and revisions. Rui-Jie Zhu Optimizing draft Section C, reorganizing Table 8, 12, and 11. Proofreading and revisions. ## B Additional Architecture Details\n\nThe WKV computations of Eagle and Finch can be parallelized across the time dimension using a variety of techniques including associative scan or the parallelization techniques used in FlashAttention. (Dao et al., 2022) The simplest of these, while highly parallel, prove inefficient due to repeated expensive memory transfers between fast SRAM and slower HBM. We take a different approach when training, choosing to parallelize over non-time dimensions only while using a custom CUDA implementation that carefully keeps state operations in fast SRAM, which is simpler yet provides enough breadth for a highly efficient implementation. See Section 9 for kernel experiments. We provide an additional pure PyTorch implementation with similar full-model speed characteristics that parallelizes over the time dimension using an algorithmic approach similar to GLA (Yang et al., 2023). Unlike Transformers, RWKV's recurrence mechanism does not examine tokens more than one time-step old. This allows us to train on and provide inference for unbounded sequence lengths without requiring increased computing power or memory. Another significant advantage is that RWKV does not utilize explicit positional encoding, which allows RWKV to handle contexts of arbitrary length without modification. Finch Token Shift Finch changes the token shift mechanism to become data-dependent.\n```\n\n#### 2. Reinventing RNNs for the Transformer Era (Avg. Score: 0.59)\n\n*Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 251  (*Influential: 27*)\n\n**TL;DR:** This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Abstract:** Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n##### *Relevant Chunk: No. 5/48 (Score: 0.91)*\n\n```\nsoftmax operation improving numerical stability and mitigating vanishing gradients (for rigorous proof, see Appendix H). It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization (Ba et al., 2016) incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues. These design elements not only enhance the training dynamics of deep neural networks but also facilitate the stacking of multiple layers, leading to superior performance over conventional RNN models by capturing complex patterns across different levels of abstraction (see also Appendix I). ### 3.1.1 Token Shift\n\nIn this architecture, all linear projection vectors ( $R, K, V$ in time-mixing, and $R^{\\prime}, K^{\\prime}$ in channelmixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift. The vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n\n$$\n\\begin{aligned}\nr_{t} & =W_{r} \\cdot\\left(\\mu_{r} \\odot x_{t}+\\left(1-\\mu_{r}\\right) \\odot x_{t-1}\\right) \\\\\nk_{t} & =W_{k} \\cdot\\left(\\mu_{k} \\odot x_{t}+\\left(1-\\mu_{k}\\right) \\odot x_{t-1}\\right) \\\\\nv_{t} & =W_{v} \\cdot\\left(\\mu_{v} \\odot x_{t}+\\left(1-\\mu_{v}\\right) \\odot x_{t-1}\\right)\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-04.jpg?height=792&width=764&top_left_y=229&top_left_x=246)\n\nFigure 3: RWKV architecture for language modeling. as are the channel-mixing inputs:\n\n$$\n\\begin{aligned}\nr_{t}^{\\prime} & =W_{r}^{\\prime} \\cdot\\left(\\mu_{r}^{\\prime} \\odot x_{t}+\\left(1-\\mu_{r}^{\\prime}\\right) \\odot x_{t-1}\\right) \\\\\nk_{t}^{\\prime} & =W_{k}^{\\prime} \\cdot\\left(\\mu_{k}^{\\prime} \\odot x_{t}+\\left(1-\\mu_{k}^{\\prime}\\right) \\odot x_{t-1}\\right)\n\\end{aligned}\n$$\n\nThe token shift is implemented as a simple offset in the temporal dimension at each block using the PyTorch (Paszke et al., 2019) library as nn. ZeroPad2d ( $(0,0,1,-1))$. ### 3.1.2 WKV Operator\n\nThe computation of the $W K V$ operator in our model parallels the method used in Attention Free Transformer (AFT) (Zhai et al., 2021). However, unlike AFT where $W$ is a pairwise matrix, our model treats $W$ as a channel-wise vector that is modified by relative position. In our model, this recurrent behavior is defined by the time-dependent update of the $W K V$ vectors, formalized in the following equation:\n\n$$\nw k v_{t}=\\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}} \\odot v_{i}+e^{u+k_{t}} \\odot v_{t}}{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}}+e^{u+k_{t}}}\n$$\n\nTo circumvent any potential degradation of $W$, we introduce a vector $U$ that separately attends to the current token. More information about this can be found in Appendix I. ### 3.1.3 Output Gating\n\nOutput gating is implemented in both time-mixing and channel-mixing blocks using the sigmoid of the receptance, $\\sigma(r)$. The output vector ot post the $W K V$ operator is given by:\n\n$$\no_{t}=W_{o} \\cdot\\left(\\sigma\\left(r_{t}\\right) \\odot w k v_{t}\\right)\n$$\n\nIn the channel-mixing block, a similar operation is performed:\n\n$$\no_{t}^{\\prime}=\\sigma\\left(r_{t}^{\\prime}\\right) \\odot\\left(W_{v}^{\\prime} \\cdot \\max \\left(k_{t}^{\\prime}, 0\\right)^{2}\\right)\n$$\n\nwhere we adopt the squared ReLU activation function (So et al., 2021). ### 3.2 Transformer-like Training\n\nRWKV can be efficiently parallelized using a technique called time-parallel mode, reminiscent of Transformers. The time complexity of processing a batch of sequences in a single layer is $O\\left(B T d^{2}\\right)$, primarily consisting of matrix multiplications $W_{\\lambda}$, where $\\lambda \\in\\{r, k, v, o\\}$ (assuming $B$ sequences, $T$ maximum tokens, and $d$ channels). In contrast, updating attention scores $w k v_{t}$ involves a serial scan (see Appendix D for more detail) and has complexity $O(B T d)$. The matrix multiplications can be parallelized similarly to $W_{\\lambda}$, where $\\lambda \\in\\{Q, K, V, O\\}$ in conventional Transformers. The element-wise $W K V$ computation is time-dependent but can be readily parallelized along the other two dimensions (Lei et al., 2018) ${ }^{3}$. ### 3.3 RNN-like Inference\n\nRecurrent networks commonly utilize the output at state $t$ as input at state $t+1$. This usage is also observed in the autoregressive decoding inference of language models, where each token must be computed before being passed to the next step. RWKV takes advantage of this RNN-like structure, known as time-sequential mode. In this context, RWKV can be conveniently formulated recursively for decoding during inference, as demonstrated in Appendix D. ### 3.4 Additional Optimizations\n\nCustom Kernels To address inefficiencies in the $W K V$ computation arising from the sequential nature of the task when using standard deep learning frameworks, we have developed a custom CUDA\n\n[^2]kernel. This kernel enables the execution of a single compute kernel on training accelerators, while all other parts of the model, such as matrix multiplications and point-wise operations, are already inherently parallelizable and efficient. Small Init Embedding During the initial stage of training a transformer model (Vaswani et al., 2017), we observe that the embedding matrix undergoes slow changes, presenting a challenge for the model to move away from its initial noisy embedding state. To address this issue, we propose an approach that involves initializing the embedding matrix with small values and subsequently applying an additional LayerNorm operation. This accelerates and stabilizes the training process, allowing for the training of deep architectures with post-LN components. The effectiveness of this approach is demonstrated in Figure 9, illustrating improved convergence by enabling the model to quickly transition away from the initially small embedding. This is achieved through small changes occurring in a single step, which subsequently lead to substantial alterations in directions and further notable changes after the LayerNorm operation. Custom Initialization Building on principles from previous works (He et al., 2016; Jumper et al., 2021), we adopt an initialization strategy where parameters are set to values resembling an identity mapping while breaking symmetry to establish a clear information flow. The majority of weights are initialized to zero, and linear layers do not employ biases. Detailed formulas are given in Appendix E. We observe that the choice of initialization plays a crucial role in both the speed and quality of convergence (refer to Appendix F for further details). ### 3.5 Implementation\n\nRWKV is implemented using the PyTorch Deep Learning Library (Paszke et al., 2019). We integrate additional optimization strategies inspired by DeepSpeed (Rasley et al., 2020) into the system, improving its efficiency and scalability. The model begins with an embedding layer, as detailed in Section 3.4. Following this are several identical residual blocks arranged sequentially. These are depicted in Figures 2 and 3 and adheres to the principles outlined in Section 3.1.1. After the last block, a simple output projection head, consisting of a LayerNorm (Ba et al., 2016) and a linear projection, is employed for logits generation for next-token prediction and computation of the cross-entropy loss during training. ## 4 Trained Models and Computing Costs\n\nTo demonstrate the scalability of RWKV, we train six models ranging from 169 million to 14 billion parameters as shown in Table 2. All models are trained for one epoch ( 330 billion tokens) on the Pile (Gao et al., 2020; Biderman et al., 2022). | Name | Layers | Model Dimension | Parameters | FLOP per token |\n| :---: | :---: | :---: | :---: | :---: |\n| 169 M | 12 | 768 | $1.693 \\times 10^{8}$ | $2.613 \\times 10^{8}$ |\n| 430 M | 24 | 1024 | $4.304 \\times 10^{8}$ | $7.573 \\times 10^{8}$ |\n| 1.5 B | 24 | 2048 | $1.515 \\times 10^{9}$ | $2.823 \\times 10^{9}$ |\n| 3 B | 32 | 2560 | $2.985 \\times 10^{9}$ | $5.710 \\times 10^{9}$ |\n| 7B | 32 | 4096 | $7.393 \\times 10^{9}$ | $1.437 \\times 10^{10}$ |\n| 14B | 40 | 5120 | $1.415 \\times 10^{10}$ | $2.778 \\times 10^{10}$ |\n\nTable 2: RWKV model architectures and FLOP counts. Further details of these hyperparameters are elaborated upon in Appendix G. The number of parameters for each model is computed using the formula: \\# parameters = $2 V D+13 D^{2} L+D(11 L+4)$ where $V=50277$ is the vocabulary size, $D$ represents the Model Dimension and $L$ corresponds to the number of layers. FLOPs is for a forward pass for one token. It was calculated as $2\\left(2 V D+13 D^{2} L\\right)$, which is the twice (add and multiply) the number of parameters in linear layers. The backwards pass FLOPs can be approximated as twice that of the forward pass, giving a total of $6\\left(2 V D+13 D^{2} L\\right)$ FLOP per token. Notably, this matches the standard formula for FLOP calculations in transformers Kaplan et al. (2020): FLOP $=6 \\cdot[\\#$ tokens $] \\cdot[\\#$ parameters $]$. ### 4.1 Additional Training Details\n\nFor training, we use the standard Adam optimizer without weight decay, use bfloat 16 precision, and train with a context length of 1024 tokens. Further details on hyperparameters are in Appendix G. Diverting from standard practice for transformers, we apply exponential decay to our learning rate. We also incorporate the auxiliary loss introduced by PaLM (Chowdhery et al., 2022), supplementing the standard cross-entropy loss function. This auxiliary loss encourages the softmax normalizer to approximate zero closely. As for the learning rate schedule, it remains constant for the initial iterations, and subsequently decays exponentially. ### 4.2 Scaling Laws\n\nScaling laws (Kaplan et al., 2020; Henighan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023) in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors. These factors can include the model size $(N)$, dataset size $(D)$, or the optimally allocated compute budget ( $C_{\\mathrm{min}}$ ). Scaling laws are important for two primary reasons: they allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation (Black et al., 2022; Le Scao et al., 2022) and the contexts in which they fail provides rich feedback on important areas for future research (Wei et al., 2022a; Biderman et al., 2023a). Previous work on scaling laws for RNNs has claimed that LSTMs do not strictly follow the same log-log linear scaling that transformers do (Kaplan et al., 2020). We train 45 RWKV models for a variety of pairs (dataset, parameters) and find that RWKV does follow the same general form of the scaling law that is well established for transformers. Figure 4 shows our results for loss as a function of compute, with the linear fit to the Pareto optimal points holding an $r^{2}$ value of 0.994 . Even when we extrapolate our curve an additional order of magnitude (blue), we find an extremely good fit with an $r^{2}$ of 0.875 . ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-06.jpg?height=555&width=763&top_left_y=1704&top_left_x=241)\n\nFigure 4: Scaling laws curves for RWKV models\n\n## 5 Evaluations\n\nHaving demonstrated the scalability of RWKV models in the previous section, we now turn our attention to their competitiveness with traditional transformers. We focus on two questions:\nCompetitiveness Is RWKV competitive against quadratic transformer architectures with the same amount of compute? Long Context Does increasing the context length of RWKV yield better language modeling loss when RWKV models are trained for context lengths that most open-sourced quadratic transformers cannot efficiently process? ### 5.1 NLP Evaluations\n\nTo demonstrate that RWKV is competitive with traditional transformers at NLP tasks, we compare with similarly sized models trained for a similar number of tokens (Pythia (Biderman et al., 2023b), OPT (Zhang et al., 2022) and BLOOM (Scao et al., 2022)). All RWKV models were trained for one epoch on the Pile (330B tokens), which is close but not identical to the amount of tokens the Pythia, OPT, and BLOOM models were trained for. Consequently, we compare our models on a FLOPmatched basis. We avoid comparing with model trained in the Chinchilla-optimal regime (Hoffmann et al., 2022) or the overtrained regime (Touvron et al., 2023) to ensure the most equitable comparison. We report results on ARC (both Easy and Challenge) (Clark et al., 2018), BoolQ (Clark et al., 2019), COPA (Roemmele et al., 2018), HeadQA (Vilares and G\u00f3mez-Rodr\u00edguez, 2019), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), ReCoRD (Zhang et al., 2018), SciQ (Johannes Welbl Nelson F.\n```\n\n##### *Relevant Chunk: No. 4/48 (Score: 0.75)*\n\n```\nof British Columbia ${ }^{28}$ U. of C., Santa Cruz ${ }^{29}$ U. of Electronic Science and Technology of China\n\n\n#### Abstract\n\nTransformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks. ${ }^{1}$\n\n## 1 Introduction\n\nDeep learning has greatly advanced artificial intelligence, impacting a range of scientific and industrial uses. These often involve complex sequential data\n\n[^0]processing tasks such as natural language understanding, conversational AI, time-series analysis, and indirectly sequential formats like images and graphs (Brown et al., 2020; Ismail Fawaz et al., 2019; Wu et al., 2020; Albalak et al., 2022). Predominant among these techniques include RNNs and Transformers (Vaswani et al., 2017), each with specific benefits and drawbacks. RNNs require less memory, particularly for handling long sequences. However, they suffer from the vanishing gradient problem and non-parallelizability in the time dimension during training, limiting their scalability (Hochreiter, 1998; Le and Zuidema, 2016). ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-01.jpg?height=584&width=746&top_left_y=1647&top_left_x=1063)\n\nFigure 1: Average performance of RWKV models compared to transformers across twelve NLP tasks. For further details, see section 5 . Transformers emerged as a powerful alternative, adept at managing local and long-range dependencies and supporting parallelized training (Tay et al., 2022). Models such as GPT-3 (Brown et al., 2020), ChatGPT (OpenAI, 2022; Koco\u0144 et al., 2023),\n\n| Model | Time | Space |\n| :--- | :---: | :---: |\n| Transformer | $O\\left(T^{2} d\\right)$ | $O\\left(T^{2}+T d\\right)$ |\n| Reformer | $O(T \\log T d)$ | $O(T \\log T+T d)$ |\n| Performer | $O\\left(T d^{2} \\log d\\right)$ | $O\\left(T d \\log d+d^{2} \\log d\\right)$ |\n| Linear Transformers | $O\\left(T d^{2}\\right)$ | $O\\left(T d+d^{2}\\right)$ |\n| AFT-full | $O\\left(T^{2} d\\right)$ | $O(T d)$ |\n| AFT-local | $O(T s d)$ | $O(T d)$ |\n| MEGA | $O(c T d)$ | $O(c d)$ |\n| RWKV (ours) | $O($ Td $)$ | $O(\\mathbf{d})$ |\n\nTable 1: Inference complexity comparison with different Transformers. Here $T$ denotes the sequence length, $d$ the feature dimension, $c$ is MEGA's chunk size of quadratic attention, and $s$ is the size of a local window for AFT. LLaMA (Touvron et al., 2023), and Chinchilla (Hoffmann et al., 2022) showcase the potential of Transformers in NLP. However, the self-attention mechanism's quadratic complexity makes it computationally and memory intensive for tasks involving long sequences and constrained resources. This has stimulated research to enhance Transformers' scalability, sometimes sacrificing some of their effectiveness (Wang et al., 2020; Zaheer et al., 2020; Dao et al., 2022a). To tackle these challenges, we introduce the Receptance Weighted Key Value (RWKV) model, combining the strengths of RNNs and Transformers while circumventing key drawbacks. RWKV alleviates memory bottleneck and quadratic scaling associated with Transformers (Katharopoulos et al., 2020) with efficient linear scaling, while maintaining the expressive properties of the Transformer, such as parallelized training and robust scalability. RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity; see Table 1. The motivation behind RWKV is to balance computational efficiency with expressive capacity in neural networks. It offers a solution for handling large-scale models with billions of parameters, exhibiting competitive performance at a reduced computational cost. Experiments suggest RWKV addresses scaling and deployment challenges in AI, especially for sequential data processing, pointing towards more sustainable and efficient AI models. Our contributions in this paper are as follows:\n\n- The introduction of RWKV, a novel architec- ture combining RNNs and Transformer advantages while mitigating their limitations. - Detailed experiments demonstrating RWKV's performance and efficiency on benchmark datasets for large-scale models. - The release of pretrained models, from 169 million to 14 billion parameters, trained on the Pile (Gao et al., 2020; Biderman et al., 2022). ${ }^{2}$\n\n\n## 2 Background\n\nHere we briefly review the fundamentals of RNNs and Transformers. ### 2.1 Recurrent Neural Networks (RNNs)\n\nPopular RNN architectures such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) are characterized by the following formulation (shown for LSTM, others can be reasoned similarly):\n\n$$\n\\begin{aligned}\nf_{t} & =\\sigma_{g}\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right) \\\\\ni_{t} & =\\sigma_{g}\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right) \\\\\no_{t} & =\\sigma_{g}\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right) \\\\\n\\tilde{c}_{t} & =\\sigma_{c}\\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right) \\\\\nc_{t} & =f_{t} \\odot c_{t-1}+i_{t} \\odot \\tilde{c}_{t} \\\\\nh_{t} & =o_{t} \\odot \\sigma_{h}\\left(c_{t}\\right)\n\\end{aligned}\n$$\n\nAlthough RNNs can be factored into two linear blocks ( $W$ and $U$ ) and an RNN-specific block (1)(6), as noted by Bradbury et al. (2017), the data dependency relying on previous time steps prohibits parallelizing these typical RNNs. ### 2.2 Transformers and AFT\n\nIntroduced by Vaswani et al. (2017), Transformers are a class of neural networks that have become the dominant architecture for several NLP tasks. Instead of operating on sequences step-by-step like RNNs, Transformers rely on attention mechanisms to capture relationships between all input and all output tokens:\n\n$$\n\\operatorname{Attn}(Q, K, V)=\\operatorname{softmax}\\left(Q K^{\\top}\\right) V\n$$\n\nwhere the multi-headness and scaling factor $\\frac{1}{\\sqrt{d_{k}}}$ is omitted for convenience. The core $Q K^{\\top}$ multiplication is an ensemble of pairwise attention scores\n\n[^1]between each token in a sequence, which can be decomposed as vector operations:\n$$\n\\operatorname{Attn}(Q, K, V)_{t}=\\frac{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}} \\odot v_{i}}{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}}}\n$$\n\nAFT (Zhai et al., 2021), alternately formulates\n\n$$\n\\operatorname{Attn}^{+}(W, K, V)_{t}=\\frac{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}} \\odot v_{i}}{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}}}\n$$\n\nwhere $\\left\\{w_{t, i}\\right\\} \\in R^{T \\times T}$ is the learned pair-wise position biases, and each $w_{t, i}$ is a scalar. Inspired by AFT, RWKV takes a similar approach. However, for simplicity, it modifies the interaction weights so that it can be transformed into an RNN. Each $w_{t, i}$ in RWKV is a channelwise time decay vector multiplied by the relative position and traced backward from current time as it decays:\n\n$$\nw_{t, i}=-(t-i) w\n$$\n\nwhere $w \\in\\left(R_{\\geq 0}\\right)^{d}$, with $d$ the number of channels. We require $w$ to be non-negative to ensure that $e^{w_{t, i}} \\leq 1$ and the per-channel weights decay backwards in time. ## 3 RWKV\n\nThe RWKV model architecture is defined by four fundamental elements that are intrinsic to the timemixing and channel-mixing blocks:\n\n- $R$ : The Receptance vector acts as the receiver of past information. - $W$ : The Weight signifies the positional weight decay vector, a trainable parameter within the model. - $K$ : The Key vector performs a role analogous to $K$ in traditional attention mechanisms. - $V$ : The Value vector functions similarly to $V$ in conventional attention processes. These core elements interact multiplicatively at each timestep, as depicted in Figure 2. ### 3.1 Architecture\n\nThe RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information. This model uses a unique attention-like score update process, which includes a time-dependent\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-03.jpg?height=829&width=718&top_left_y=248&top_left_x=1086)\n\nFigure 2: Elements within an RWKV block (left) and the complete RWKV residual block, equipped with a final head for language modeling (right).\n```\n\n##### *Relevant Chunk: No. 41/48 (Score: 0.39)*\n\n```\n8, where the hidden states $h$ is the numerator-denominator tuple $(a, b)$. To avoid overflow in calculating $e^{k_{t}}$, a numerical trick is used in the official implementation. Noticing that $a_{1}=e^{k_{1}} \\odot v_{1}$ and $b_{1}=e^{k_{1}}$, we set $a_{1}^{\\prime}=$ $v_{1}, b_{1}^{\\prime}=1, p_{1}=k_{1}$, where $p_{t}$ stores the shared exponents of $a_{t}$ and $b_{t}$. Now the above recursion can be converted into a numerical safe version, for each time step $t>1$ :\n\n$$\n\\begin{aligned}\nq & :=\\max \\left(p_{t-1}, u+k_{t}\\right) \\\\\nw k v_{t} & =\\frac{e^{p_{t-1}-q} \\odot a_{t-1}^{\\prime}+e^{u+k_{t}-q} \\odot v_{t}}{e^{p_{t-1}-q} \\odot b_{t-1}^{\\prime}+e^{u+k_{t}-q}}\n\\end{aligned}\n$$\n\nThe update to $a_{t}^{\\prime}, b_{t}^{\\prime}$, and their shared exponent is also carried out in a similar fashion:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-17.jpg?height=344&width=507&top_left_y=819&top_left_x=1160)\n\nFigure 8: RWKV time-mixing block formulated as an RNN cell. Color codes: yellow $(\\mu)$ denotes the token shift, red (1) denotes the denominator, blue (2) denotes the numerator, and pink (3) denotes the fraction computations in 16. $h$ denotes the numerator-denominator tuple. $$\n\\begin{aligned}\nq^{\\prime} & :=\\max \\left(p_{t-1}-w, k_{t}\\right) \\\\\na_{t}^{\\prime} & =e^{p_{t-1}-w-q^{\\prime}} \\odot a_{t-1}^{\\prime}+e^{k_{t}-q^{\\prime}} \\odot v_{t} \\\\\nb_{t}^{\\prime} & =e^{p_{t-1}-w-q^{\\prime}} \\odot b_{t-1}^{\\prime}+e^{k_{t}-q^{\\prime}} \\\\\np_{t} & =q^{\\prime}\n\\end{aligned}\n$$\n\nThe RWKV model has an internal state that stores some previous information. In each layer, the internal state consists five parts, each of which is a vector with $D$ numbers, where $D$ is the model dimension. The five parts are:\n\n- The current input of the Time-mix block $x_{t}$;\n- The current input of the Channel-mix block $y_{t}$;\n- The numerator of the $W K V$ value $a_{t}^{\\prime}$, as defined in equation (26);\n- The denominator of the $W K V$ value $b_{t}^{\\prime}$, as defined in equation (27);\n- An auxiliary state $p_{t}$ in (28), which is used for $W K V$ computation to maintain numerical precision. Which yields a total size of $5 D L$ parameters. It is worth noting that in algebraic context with infinite precision, the helper state $p_{t}$ can be ignored, and the $W K V$ numerator and denominator can be computed directly using equations (21) and (22), reducing the size of the internal state to $4 D L$. ## E Parameter initializations\n\nWe describe the specific parameter initializations below and motivate the design choices. Parameters belonging to residual blocks are often adjusted by layer depth and total number of layers. Let \\# denote the vocabulary size, $s$ denote the embedding dimension, $d$ denote the hidden size (we use $d=4 s$ ), $L$ the number of layers, $l$ the layer index (from 0 to $L-1$ ), we use the following initializations:\n\n- Embeddings are initialized to $\\mathcal{U}\\left( \\pm 1 \\times 10^{-4}\\right)$ as explained in 3.4\n- For the time-mixing blocks $(11,12,13)$, initializations are $\\mu_{k_{i}}=\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}, \\mu_{v_{i}}=\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}+\\frac{0.3 l}{L-1}$ and $\\mu_{r_{i}}=\\frac{1}{2} \\cdot\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}$\n- For the channel-mixing blocks $(14,15), \\mu_{k_{i}}$ and $\\mu_{r_{i}}$ are initialized to $\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}$\n- $w_{i}(16)$, also known as \"time decay\", is initialized to $-5+8 \\cdot\\left(\\frac{i}{d-1}\\right)^{0.7+\\frac{1.3 l}{L-1}}$.\n```\n\n##### *Relevant Chunk: No. 6/48 (Score: 0.31)*\n\n```\nLiu, 2017), and Winogrande (Zellers et al., 2020). Figure 1 shows the average results across all benchmarks. Some individual benchmarks are shown in Fig 5, with the rest in Appendix J. Additionally, we carried out comparative studies on RWKV and ChatGPT / GPT-4, see Appendix L. They revealed that RWKV is very sensitive to prompt engineering. When the prompts were adjusted (re-ordered) from the ones used for GPT to more suitable for RWKV, the performance (F1) increased even from $44.2 \\%$ to $74.8 \\%$. For sarcasm detection, RWKV outperformed ChatGPT, but was still slightly worse than the SOTA solution. ### 5.2 Extended Context Finetuning\n\nUnlike transformers, RNNs do not have a predefined sequences length when they are created. However in order to efficient make use of compute\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-07.jpg?height=939&width=1556&top_left_y=266&top_left_x=255)\n\nFigure 5: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. Additional plots can be found in Appendix J. we nevertheless need to preprocess the training data into contexts of the same length. We find that we are able to teach the model how to efficiently handle substantially larger batch sizes by finetuning with progressively increasing sequence length. Specifically, we first double the sequence length from 1024 to 2048 and finetune for 10B tokens from the original pretraining corpus, then we double again to 4096 for 100B tokens from the same corpus, and finally double to 8192 tokens for another 100B tokens from the same corpus. In Fig. 6 we show that increasing context length leads to lower test loss on the Pile, an indication that RWKV can make effective use of long contextual information. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-07.jpg?height=318&width=673&top_left_y=2145&top_left_x=286)\n\nFigure 6: RWKV shows decreasing mean test loss as a function of context length on the Pile (Gao et al., 2020)\n\n### 5.3 Long Context Benchmarks\n\nAdditionally, we evaluate our model's ability to handle very long sequences by comparing to stateof-the-art long sequence models on the LongRange Arena (LRA) benchmark (Tay et al., 2021). LRA is designed to assess the performance of models in handling lengthy context situations. It includes a collection of tasks with sequences ranging from 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images, and mathematical expressions. We apply RWKV on the LRA benchmark and the results are in Appendix J.2. The results show that RWKV performs second only to the S 4 model in five datasets. ## 6 Inference Experiments\n\nWe benchmark inference requirements according to size and family. Specifically, we evaluate text generation speed and memory requirements on typical compute platforms including CPU (x86) and GPU (NVIDIA A100 80 GB ). For all of our inference experiments we use float 32 precision and the HuggingFace Transformers (Wolf et al., 2020). We include all model parameters in the parameter count, including both embedding and non-embedding layers. Performance under different quantization setups is left to further work. See Appendix K for\nmore results. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-08.jpg?height=481&width=629&top_left_y=319&top_left_x=308)\n\nFigure 7: Cumulative time on text generation for LLMs. Unlike transformers, RWKV exhibits linear scaling. ## 7 Future Work\n\nThere are several promising directions for future work on the RWKV architecture. Work can be done to increase model expressivity by enhancing the time-decay formulations and exploring initial model states while maintaining efficiency. The RWKV computational efficiency can be further improved by applying a parallel scan in the $w k v_{t}$ step to reduce the computational cost to $O(B \\log (T) d)$. The mechanisms used in RWKV can be applied to encoder-decoder architectures, potentially replacing the cross-attention mechanism. This could be applicable in seq2seq or multimodal settings, thereby enhancing efficiency during both training and inference. RWKV's state (or context) can be leveraged for interpretability, predictability in sequence data, and safety. Manipulating the hidden state could also guide behavior and allow greater customizability through prompt tuning. The RWKV architecture is not perfect, and can be improved via many aspects, such as modifying the formulae or implementing larger internal states. Larger states can enhance the model's memory to previous context and improve performance over various tasks. ## 8 Conclusions\n\nWe introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and longrange dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations. We benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs. RWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters. ## 9 Limitations\n\nWhile our proposed RWKV model has demonstrated promising results regarding training and memory efficiency during inference, some limitations should be acknowledged and addressed in future work. First, the linear attention of RWKV leads to significant efficiency gains but still, it may also limit the model's performance on tasks that require recalling minutiae information over very long contexts. This is due to the funneling of information through a single vector representation over many time steps, compared with the full information maintained by the quadratic attention of standard Transformers. In other words, the model's recurrent architecture inherently limits its ability to \"look back\" at previous tokens, as opposed to traditional self-attention mechanisms. While learned time decay helps prevent the loss of information, it is mechanistically limited compared to full selfattention. Another limitation of this work is the increased importance of prompt engineering in comparison to standard Transformer models. The linear attention mechanism used in RWKV limits the information from the prompt that will be carried over to the model's continuation. As a result, carefully designed prompts may be even more crucial for the model to perform well on tasks. The above RWKV property was confirmed by studies on prompt engineering presented in Appendix L. By changing the order of the information\npieces, we were even able to almost double the RWKV performance for some tasks. ## 10 Ethics Statement\n\nIn this paper, we present a novel architecture for sequential data processing and prove its effectiveness by building a series of LLMs trained on publicly released pretraining data (Gao et al., 2020; Biderman et al., 2022) and later fine-tuned on publicly available instructions (Taori et al., 2023; Chaudhary, 2023; Cheung, 2023; Anand et al., 2023; Anonymous, 2023; Yang, 2023; Ji et al., 2023a,b). As a novel architecture for sequential data, RWKV has the potential to improve sequencebased models across different applications ranging from natural language processing to biomedical data processing or climate modelling. Since the training code is released open source, RWKV contributes to the democratization of AI , levels the playing field, and empowers members of the Open Source community to inspect, study, and finetune RWKV in particular tasks. Moreover, it contributes to advancing the understanding of LLMs capabilities and limitations. A significant amount of work has been devoted to increasing the efficiency of RWKV training so as to minimize its cost and promote accessibility. As LLMs trained on public data, RWKV's lower inference cost compared to Transformer alternatives makes it more suitable for deployment in consumer and edge hardware, which is a step towards the democratization and distribution of LLMs to the general public, creating better privacy and ownership incentives. It also lowers the resource barrier to Chat assistants and text generation for small and/or underrepresented communities. PreTrained model weights for different sizes ranging from 0.1 B to 14B parameters trained on multiple languages are released to increase ease of adoption and allow for the study of emergent phenomena. On the other hand, with lower resource barriers, the spreading of AI-generated text might become more prevalent. Current RWKV LLMs may exhibit and/or reproduce biases and potentially harmful content present in the data used for training. Nonetheless, mitigation and finetuning strategies discussed for other, large Transformer models should be applicable to RWKV as well. ## Acknowledgements\n\nWe thank StabilityAI for the compute used to train our models and for technical support in development of RWKV. We also thank the members of the RWKV and EleutherAI Discord servers for their help and work on further extending the applicability of RWKV to different domains.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: GLA implementation guides, RWKV performance benchmarks, Linear attention theoretical foundations\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: GLA implementation guides*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 2. Generating Signed Language Instructions in Large-Scale Dialogue Systems\n\n*From Search Query: GLA implementation guides*\n\n*Mert Inan, Katherine Atwell, Anthony Sicilia, Lorna Quandt, Malihe Alikhani*\n\n**Abstract:** We introduce a goal-oriented conversational AI system enhanced with American Sign Language (ASL) instructions, presenting the first implementation of such a system on a worldwide multimodal conversational AI platform. Accessible through a touch-based interface, our system receives input from users and seamlessly generates ASL instructions by leveraging retrieval methods and cognitively based gloss translations. Central to our design is a sign translation module powered by Large Language Models, alongside a token-based video retrieval system for delivering instructional content from recipes and wikiHow guides. Our development process is deeply rooted in a commitment to community engagement, incorporating insights from the Deaf and Hard-of-Hearing community, as well as experts in cognitive and ASL learning sciences. The effectiveness of our signing instructions is validated by user feedback, achieving ratings on par with those of the system in its non-signing variant. Additionally, our system demonstrates exceptional performance in retrieval accuracy and text-generation quality, measured by metrics such as BERTScore. We have made our codebase and datasets publicly accessible at https://github.com/Merterm/signed-dialogue, and a demo of our signed instruction video retrieval system is available at https://huggingface.co/spaces/merterm/signed-instructions.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction\n\n*From Search Query: GLA implementation guides*\n\n*Zuobai Zhang, Minghao Xu, A. Lozano, V. Chenthamarakshan, Payel Das, Jian Tang*\n\n**TL;DR:** The DiffPreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling, inspired by the success of denoising diffusion models in generative tasks, and SiamDiff, a method to capture the correlation between different conformers of a protein.\n\n**Abstract:** Self-supervised pre-training methods on proteins have recently gained attention, with most approaches focusing on either protein sequences or structures, neglecting the exploration of their joint distribution, which is crucial for a comprehensive understanding of protein functions by integrating co-evolutionary information and structural characteristics. In this work, inspired by the success of denoising diffusion models in generative tasks, we propose the DiffPreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling. DiffPreT guides the encoder to recover the native protein sequences and structures from the perturbed ones along the joint diffusion trajectory, which acquires the joint distribution of sequences and structures. Considering the essential protein conformational variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory Prediction (SiamDiff) to capture the correlation between different conformers of a protein. SiamDiff attains this goal by maximizing the mutual information between representations of diffusion trajectories of structurally-correlated conformers. We study the effectiveness of DiffPreT and SiamDiff on both atom- and residue-level structure-based protein understanding tasks. Experimental results show that the performance of DiffPreT is consistently competitive on all tasks, and SiamDiff achieves new state-of-the-art performance, considering the mean ranks on all tasks. Our implementation is available at https://github.com/DeepGraphLearning/SiamDiff.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 4. RWKV: Reinventing RNNs for the Transformer Era\n\n*From Search Query: RWKV performance benchmarks*\n\n*Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xingjian Du, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu*\n\n**TL;DR:** This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Abstract:** Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 356  (*Influential: 34*)\n\n#### 5. Investigating Data Contamination in Modern Benchmarks for Large Language Models\n\n*From Search Query: RWKV performance benchmarks*\n\n*Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark B. Gerstein, Arman Cohan*\n\n**TL;DR:** It is found that certain commercial LLMs could surprisingly guess the missing option in various test sets, and in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data.\n\n**Abstract:** Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 21  (*Influential: 0*)\n\n#### 6. When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain\n\n*From Search Query: RWKV performance benchmarks*\n\n*Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, S. Chava, Natraj Raman, Charese Smiley, Jiaao Chen, Diyi Yang*\n\n**TL;DR:** A novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective is proposed, which outperforms those in prior literature on a variety of NLP tasks.\n\n**Abstract:** Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 78  (*Influential: 11*)\n\n#### 7. How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?\n\n*From Search Query: Linear attention theoretical foundations*\n\n*Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, Peter L. Bartlett*\n\n**TL;DR:** A statistical task complexity bound is established for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks and it is proved that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression.\n\n**Abstract:** Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 38  (*Influential: 8*)\n\n#### 8. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: Linear attention theoretical foundations*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 9. Generalizable Multi-linear Attention Network\n\n*From Search Query: Linear attention theoretical foundations*\n\n*Tao Jin, Zhou Zhao*\n\n**TL;DR:** This work proposes a new method called generalizable multi-linear attention network (MAN), which can associate more modalities in acceptable complexity with hierarchical approximation decomposition and introduces the local sequential constraints, which can be combined with ARF conveniently, as positional information.\n\n**Abstract:** The majority of existing multimodal sequential learning methods focus on how to obtain powerful individual representations and neglect to effectively capture the multimodal joint representation. Bilinear attention network (BAN) is a commonly used integration method, which leverages tensor operations to associate the features of different modalities. However, BAN has a poor compatibility for more modalities, since the computational complexity of the attention map increases exponentially with the number of modalities. Based on this concern, we propose a new method called generalizable multi-linear attention network (MAN), which can associate more modalities in acceptable complexity with hierarchical approximation decomposition. Speci\ufb01cally, considering the fact that softmax attention kernels cannot be decomposed as linear operation directly, we adopt the addition random features mechanism to approximate the non-linear softmax functions with enough theoretical analysis. Furthermore, we also introduce the local sequential constraints, which can be combined with ARF conveniently, as positional information. We conduct extensive experiments on several datasets of corresponding tasks, the experimental results show that MAN could achieve competitive results compared with baseline methods, showcasing the effectiveness of our contributions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 4  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence\n\n*From Search Query: RWKV performance benchmarks*\n\n*Rui-Jie Zhu, Jian Zhu, Peng Zhou, Qihang Zhao, Bingchen Zhao, Ruichong Zhang, Stanis\u0142aw Wo\u017aniak, Cahya Wirawan, Haoqin Tu, Guangyu Song, Fares Obeid, Niklas Muennighoff, Jiaju Lin, Atsushi Saito, Xingjian Du, Ronald McClelland Jr., Satyapriya Krishna, Bart\u0142omiej Koptyra, Jan Koco\u0144, Kranthi Kiran GV, Przemys\u0142aw Kazienko, Haowen Hou, Teddy Ferdinan, Eugene Cheah, Stella Biderman, Eric Alcaide, Alon Albalak, Quentin Anthony, Daniel Goldstein, Bo Peng*\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n**Published:** 2024-04-08\n\n\n\n#### 2. Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model\n\n*From Search Query: RWKV performance benchmarks*\n\n*Chen Change Loy, Shuicheng Yan, Ming-Hsuan Yang, Tao Zhang, Lu Qi, Xiangtai Li, Haobo Yuan*\n\n**Abstract:** Transformer-based segmentation methods face the challenge of efficient inference when dealing with high-resolution images. Recently, several linear attention architectures, such as Mamba and RWKV, have attracted much attention as they can process long sequences efficiently. In this work, we focus on designing an efficient segment-anything model by exploring these different architectures. Specifically, we design a mixed backbone that contains convolution and RWKV operation, which achieves the best for both accuracy and efficiency. In addition, we design an efficient decoder to utilize the multiscale tokens to obtain high-quality masks. We denote our method as RWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we build a benchmark containing various high-quality segmentation datasets and jointly train one efficient yet high-quality segmentation model using this benchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding performance in efficiency and segmentation quality compared to transformers and other linear attention models. For example, compared with the same-scale transformer model, RWKV-SAM achieves more than 2x speedup and can achieve better segmentation performance on various datasets. In addition, RWKV-SAM outperforms recent vision Mamba models with better classification and semantic segmentation results. Code and models will be publicly available.\n\n**Published:** 2024-06-27\n\n\n\n#### 3. Retentive Network: A Successor to Transformer for Large Language Models\n\n*From Search Query: Linear attention theoretical foundations*\n\n*Furu Wei, Jianyong Wang, Jilong Xue, Yuqing Xia, Shuming Ma, Shaohan Huang, Li Dong, Yutao Sun*\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n**Published:** 2023-07-17\n\n\n\n#### 4. Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis\n\n*From Search Query: Linear attention theoretical foundations*\n\n*Ilker Hacihaliloglu, Dorit Merhof, Ulas Bagci, Ali Diba, Reza Azad, Soheila Hatami, Afshin Bozorgpour, Bobby Azad, Sanaz Karimijafarbigloo, Sina Ghorbani Kolahi, Moein Heidari*\n\n**Abstract:** Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks. However, the emergence of transformers has altered this paradigm due to their superior performance. Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations. However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation. State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence. Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models. Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging. Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context. Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs. Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field. In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository.\n\n**Published:** 2024-06-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating Gated Linear Attention (GLA) and RWKV architecture, here are some key points and resources that can help address the practical implementation challenges, performance benchmarks, and theoretical foundations:\n\n## Practical Implementation Challenges and Solutions\n\n### GLA Implementation\n- For GLA, the main implementation challenges include managing state transitions and optimizing memory usage. A detailed guide on implementing GLA can be found in papers that discuss the hardware-efficient training of GLA Transformers. For example, the paper \"Gated Linear Attention Transformers with Hardware-Efficient Training\" provides insights into how to implement GLA efficiently, highlighting techniques such as optimizing state size and selectivity mechanisms to avoid local pattern shortcuts.\n\n### RWKV Implementation\n- RWKV implementation involves combining the efficient parallelizable training of transformers with the efficient inference of RNNs. The paper \"Reinventing RNNs for the Transformer Era\" discusses the practical aspects of implementing RWKV, including how to handle long-range dependencies efficiently and how to optimize the linear attention mechanism.\n\n## Performance Benchmarks\n\n### Comparing GLA and RWKV\n- To assess the performance of GLA and RWKV, look for benchmarks that compare these models against other linear attention models and traditional attention-based models. For instance, the paper \"Gated Linear Attention Transformers with Hardware-Efficient Training\" compares GLA against LLaMA-architecture Transformers and other linear-time-inference baselines like RetNet and Mamba. Similarly, \"Reinventing RNNs for the Transformer Era\" compares RWKV's performance with that of Transformers and other RNN-based models.\n\n## Theoretical Foundations\n\n### Linear Attention Mechanisms\n- Theoretical papers on linear attention mechanisms can provide deep insights into their mathematical underpinnings. For example, \"Linear Log-Normal Attention with Unbiased Concentration\" introduces a novel self-attention mechanism that emulates the distribution and concentration behavior of the original self-attention, which can be useful for understanding the theoretical justification of linear attention.\n\n### Online Learning Objectives\n- The use of linear attention mechanisms is often supported by online learning objectives. These objectives help in formulating precise state transition rules and optimizing performance. Papers like \"Luna: Linear Unified Nested Attention\" discuss how linear attention mechanisms can be formulated to achieve linear time and space complexity, providing a theoretical foundation for the efficiency of these models.\n\n## Additional Resources\n\n- While the provided sources do not directly address GLA or RWKV, they do not offer specific implementation guides or theoretical foundations. However, the detailed analysis and references provided in the summary can guide further research:\n  - **Implementation Guides**: Look for repositories or articles that provide code examples and implementation details, such as the advanced techniques discussed in the context of Retrieval Augmented Generation (RAG) which, although not directly related, can offer insights into optimizing document segmentation and query refinement techniques that might be analogous to optimizing attention mechanisms.\n  - **Theoretical Papers**: Search for academic papers and conference proceedings that delve into the mathematical and computational aspects of linear attention mechanisms. These papers will help in understanding the theoretical justifications and potential improvements to the models.\n\nBy focusing on these areas, researchers can gather the necessary information to overcome practical implementation challenges, evaluate performance benchmarks, and understand the theoretical foundations of GLA and RWKV, ultimately enhancing the design of their autoregressive language model.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for **FastTTTLinear** seeks to integrate Gated Linear Attention (GLA) and RWKV architecture concepts into the TTTLinear GAU to enhance computational efficiency and scalability. The search results provide insights into the empirical performance, implementation challenges, and theoretical foundations of GLA and RWKV, as well as comparisons with other models.\n\n### Useful References with Excerpts\n\n1. **Gated Linear Attention Transformers with Hardware-Efficient Training**\n   - \"The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\"\n   - This paper highlights GLA's competitive performance and efficiency in language modeling tasks.\n\n2. **Reinventing RNNs for the Transformer Era**\n   - \"RWKV combines the efficient parallelizable training of transformers with the efficient inference of RNNs, leveraging a linear attention mechanism.\"\n   - This work discusses RWKV's ability to handle long-range dependencies efficiently and its competitive performance compared to Transformers.\n\n3. **Linear Log-Normal Attention with Unbiased Concentration**\n   - \"We propose a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention.\"\n   - This paper introduces an alternative linear attention mechanism, relevant for understanding the landscape of linear attention models.\n\n4. **Luna: Linear Unified Nested Attention**\n   - \"Luna is a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear time and space complexity.\"\n   - Luna's approach to linear attention provides a benchmark for evaluating the efficiency of linear attention mechanisms.\n\n### Key Insights and Detailed Analysis\n\n- **Empirical Performance**: GLA and RWKV have shown competitive performance in language modeling tasks, with GLA excelling in hardware-efficient training and RWKV effectively capturing long-range dependencies.\n\n- **Implementation Challenges**: Implementing GLA and RWKV involves managing state transitions and ensuring efficient memory usage. Challenges include optimizing state size and selectivity mechanisms to avoid local pattern shortcuts.\n\n- **Theoretical Justification**: The use of linear attention mechanisms is supported by online learning objectives, which help in formulating precise state transition rules and optimizing performance.\n\n- **Comparison with Other Models**: GLA and RWKV offer linear complexity and constant memory usage, making them efficient alternatives to traditional attention-based models for long-context tasks. Their expressiveness and adaptability are enhanced by gating mechanisms.\n\n### Future Search Plan\n\n1. **Explore Implementation Details**: Search for detailed implementation guides or case studies on GLA and RWKV to understand practical challenges and solutions.\n\n2. **Benchmarking and Performance Metrics**: Look for benchmarks that compare GLA and RWKV with other linear attention models to assess their relative performance.\n\n3. **Theoretical Foundations**: Investigate theoretical papers that delve into the mathematical underpinnings of linear attention mechanisms and their applications in language models.\n\n### References\n\n- Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*. arXiv.org.\n- Peng, B., et al. (2023). *Reinventing RNNs for the Transformer Era*. Conference on Empirical Methods in Natural Language Processing.\n- Nahshan, Y., Kampeas, D.-J., & Haleva, E. (2023). *Linear Log-Normal Attention with Unbiased Concentration*. International Conference on Learning Representations.\n- Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., & Zettlemoyer, L. (2021). *Luna: Linear Unified Nested Attention*. Neural Information Processing Systems."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "Gated Linear Attention, RWKV architecture, Linear Attention",
            "detail": "Integration of Gated Linear Attention and RWKV in autoregressive models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nIntegration of Gated Linear Attention and RWKV in autoregressive models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training (Avg. Score: 0.99)\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 43  (*Influential: 9*)\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n##### *Relevant Chunk: No. 5/51 (Score: 0.99)*\n\n```\n$w / \\mathrm{m}$. and $w / o m$. denotes using FLASHLINEARATTENTION with or without materialization of hidden states in HBM. and a pure PyTorch (i.e., I/O-unaware) implementation of chunkwise linear attention, showing the benefits of I/O-awareness. ## 4 Gated Linear Attention\n\nThe linear recurrence in Eq. 1 does not have a decay term or a forget gate, which has been shown to be crucial in RNNs (Hochreiter \\& Schmidhuber, 1997; Cho et al., 2014; van der Westhuizen \\& Lasenby, 2018). The lack of a decay term makes it difficult for a model to \"forget\" information, and has been hypothesized to be partially responsible for the instability of linear attention in long-context tasks (Buckman \\& Gelada, 2024). Recent works (Sun et al., 2023a; Qin et al., 2023b) obtain better performance through incorporating a global, non-data-dependent decay factor ${ }^{3} \\gamma \\in(0,1)$ into linear attention: $\\mathbf{S}_{t}=\\gamma \\mathbf{S}_{t-1}+\\boldsymbol{k}_{t}^{\\top} \\boldsymbol{v}_{t}$. The use of a single $\\gamma$ is designed to preserve the attention-style parallel form for efficient training. In this work, we consider a data-dependent gating mechanism for linear attention. We show that despite having a more expressive gating factor, the resulting gated linear attention (GLA) layer still admits a hardware-efficient chunkwise form for efficient training. ### 4.1 Recurrent and Parallel Form of GLA\n\nRecurrent form. GLA has a 2D forget gate $\\mathbf{G}_{t} \\in(0,1)^{d_{k} \\times d_{v}}$ that varies over time:\n\n$$\n\\mathbf{S}_{t}=\\mathbf{G}_{t} \\odot \\mathbf{S}_{t-1}+\\boldsymbol{k}_{t}^{\\top} \\boldsymbol{v}_{t}\n$$\n\nwhere we now allow the hidden state to have varying dimensions. This Hadamard product-based recurrent form is very general and encompasses many recent RNNs with 2D hidden states, as listed in Table 1. Central to the design of gated linear attention is the parameterization of $\\mathbf{G}_{t}$ which requires a balance between parameter-efficiency, state size, and training efficiency. A\n\n[^2]| Model | Parameterization | Learnable parameters |  |\n| :--- | :--- | :--- | :--- |\n| Mamba (Gu \\& Dao, 2023) | $\\mathbf{G}_{t}=\\exp \\left(-\\left(\\mathbf{1}^{\\top} \\boldsymbol{\\alpha}_{t}\\right) \\odot \\exp (\\boldsymbol{A})\\right), \\quad \\boldsymbol{\\alpha}_{t}=\\operatorname{softplus}\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{1}} \\boldsymbol{W}_{\\alpha_{2}}\\right)$ | $\\boldsymbol{A} \\in \\mathbb{R}^{d_{k} \\times d_{v}}, \\quad \\boldsymbol{W}_{\\alpha_{1}} \\in \\mathbb{R}^{d \\times \\frac{d}{16}}, \\quad \\boldsymbol{W}_{\\alpha_{2}} \\in \\mathbb{R}^{\\frac{d}{16} \\times d_{v}}$ |  |\n| Mamba-2 (Dao \\& Gu, 2024) | $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{\\top} \\mathbf{1}, \\quad \\gamma_{t}=\\exp \\left(-\\operatorname{softplus}\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\gamma}\\right) \\exp (a)\\right)$ | $\\boldsymbol{W}_{\\gamma} \\in \\mathbb{R}^{d \\times 1}, \\quad a \\in \\mathbb{R}^{\\top}$ |  |\n| mLSTM (Beck et al., 2024; Peng et al., 2021) | $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{\\top} \\mathbf{1}, \\quad \\gamma_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\gamma}\\right)$ | $\\boldsymbol{W}_{\\gamma} \\in \\mathbb{R}^{d \\times 1}$, |  |\n| Gated Retention (Sun et al., 2024) | $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{\\top} \\mathbf{1}, \\quad \\gamma_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\gamma}\\right)^{\\frac{1}{\\tau}}$ | $\\boldsymbol{W}_{\\gamma} \\in \\mathbb{R}^{d \\times 1}$ |  |\n| DFW (Mao, 2022; Pramanik et al., 2023) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\boldsymbol{\\beta}_{t}, \\quad \\boldsymbol{\\alpha}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha}\\right), \\quad \\boldsymbol{\\beta}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\beta}\\right)$ | $\\boldsymbol{W}_{\\alpha} \\in \\mathbb{R}^{d \\times d_{k}}, \\quad \\boldsymbol{W}_{\\beta} \\in \\mathbb{R}^{d \\times d_{v}}$ |  |\n| GateLoop (Katsch, 2023) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\quad \\boldsymbol{\\alpha}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{1}}\\right) \\exp \\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{2}} \\mathbf{i}\\right)$ | $\\boldsymbol{W}_{\\alpha_{1}} \\in \\mathbb{R}^{d \\times d_{k}}, \\quad \\boldsymbol{W}_{\\alpha_{2}} \\in \\mathbb{R}^{d \\times d_{k}}$ |  |\n| HGRN-2 (Qin et al., 2024b) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\boldsymbol{\\alpha}_{t}=\\boldsymbol{\\gamma}+(1-\\boldsymbol{\\gamma}) \\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha}\\right)$ | $\\boldsymbol{W}_{\\alpha} \\in \\mathbb{R}^{d \\times d_{k}}, \\quad \\boldsymbol{\\gamma} \\in(0,1)^{d_{k}}$ |  |\n| RWKV-6 (Peng et al., 2024) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\boldsymbol{\\alpha}_{t}=\\exp \\left(-\\exp \\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha}\\right)\\right)$ | $\\boldsymbol{W}_{\\alpha} \\in \\mathbb{R}^{d \\times d_{k}}$, | $\\boldsymbol{W}_{\\alpha_{1}} \\in \\mathbb{R}^{d \\times 16}, \\quad \\boldsymbol{W}_{\\alpha_{2}} \\in \\mathbb{R}^{16 \\times d_{k}}$ |\n| Gated Linear Attention (GLA) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\boldsymbol{\\alpha}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{1}} \\boldsymbol{W}_{\\alpha_{2}}\\right)^{\\frac{1}{\\tau}}$ |  |  |\n\nTable 1: Gated linear attention formulation of recent models, which vary in their parameterization of $\\mathbf{G}_{t}$. The bias terms are omitted. na\u00efve mapping $\\boldsymbol{x}_{t} \\mapsto \\mathbf{G}_{t}$ to obtain a data-dependent gating matrix would require a matrix of size $d \\cdot d_{k} \\cdot d_{v}$, which would be parameter-inefficient. Mao (2022) propose a more efficient outer-product-based low-rank parameterization $\\left(\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\boldsymbol{\\beta}_{t}\\right)$, which requires $d \\cdot d_{v}+d \\cdot d_{k}$ parameters. ${ }^{4}$\n\nIn Mamba (Gu \\& Dao, 2023), $\\mathbf{G}_{t}$ is obtained by combining a data-independent learnable matrix $\\boldsymbol{A}$ with a data-dependent vector $\\boldsymbol{\\alpha}_{t}$, which allows the matrix to be full rank. However, this prevents the use of tensor cores because it cannot be reformulated into a matrix-multiply format, as discussed in Dao \\& Gu (2024). The lack of a compact matrix-multiply form necessitates the materialization of each time step's hidden states. To reduce high I/O costs, Gu \\& Dao (2023) develop a hardware-aware algorithm that materializes the hidden states exclusively in SRAM rather than in HBM. Due to limited SRAM capacity, this approach cannot scale to larger hidden states, which, as we will show in our experiments, results in suboptimal performance on recall-intensive tasks. Mamba-2 (Dao \\& Gu, 2024) addresses this limitation with a more restricted gating mechanism: $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{T} \\mathbf{1}$, where $\\gamma_{t} \\in(0,1)$ is a scalar, which makes it possible to to reformulate the recurrence in matrix-multiply form, enabling the use of tensor cores and larger state sizes. This scalar data-dependent gating is also used in Peng et al.\n```\n\n#### 2. RRWKV: Capturing Long-range Dependencies in RWKV (Avg. Score: 0.99)\n\n*Leilei Wang*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** The paper devises the Retrospected Receptance Weighted Key Value architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\n\n**Abstract:** Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\n\n##### *Relevant Chunk: No. 3/11 (Score: 0.99)*\n\n```\n### 2.2.1 Recurrent Neural Networks\n\nRecurrent Neural Networks (RNNs) Chung et al. [2014] are designed to handle sequential data by incorporating a mechanism that enables information to propagate from one step to the next. This process can be succinctly expressed using the following formula:\n\n$$\n\\begin{aligned}\n& h_{t}=g\\left(x_{t}, h_{t-1}\\right) \\\\\n& \\tilde{x}_{t}=f\\left(x_{t}, h_{t}\\right)\n\\end{aligned}\n$$\n\nIn this equation, the functions $g$ and $f$ refer to the reset gate and update gate, respectively. The reset gate function, $g$, determines the extent to which the previous state, $h_{t-1}$, is reset or forgotten based on the current input, $x_{t}$. Subsequently, the update gate function, $f$, governs the proportion of the current input $x_{t}$ and the hidden current state $h_{t}$ that contribute to the computation of the output, $\\tilde{x}_{t}$. By utilizing these reset and update gates, RNNs effectively capture the temporal dependencies inherent in sequential data. ### 2.2.2 Receptance Weighted Key Value Model\n\nReceptance Weighted Key Value (RWKV) Peng et al. [2023] model captures and propagates information in a sequential or auto-regressive mode Graves [2014] as integrating the capabilities of RNNs and attention mechanisms. This model consists of two core blocks, namely the time-mix block and the channel-mix block. Time-Mix Block This block aims to enhance the modeling of dependencies and patterns within a sequence by replacing the conventional weighted sum calculation in attention mechanisms with hidden states. By incorporating hidden states, the time-mix block can effectively propagate and update information across sequential steps. The calculations involved in this block can be expressed as follows:\n\n$$\n\\begin{aligned}\nq_{t} & =\\left(\\mu_{q} \\odot x_{t}+\\left(1-\\mu_{q}\\right) \\odot x_{t-1}\\right) \\cdot W_{q} \\\\\nk_{t} & =\\left(\\mu_{k} \\odot x_{t}+\\left(1-\\mu_{k}\\right) \\odot x_{t-1}\\right) \\cdot W_{k} \\\\\nv_{t} & =\\left(\\mu_{v} \\odot x_{t}+\\left(1-\\mu_{v}\\right) \\odot x_{t-1}\\right) \\cdot W_{v} \\\\\no_{t} & =\\left(\\sigma\\left(q_{t}\\right) \\odot h\\left(k_{t}, v_{t}\\right)\\right) \\cdot W_{o}\n\\end{aligned}\n$$\n\nIn these formulas, the representations of $q_{t}, k_{t}$, and $v_{t}$ are calculated by linearly interpolating between the current input and the input at the previous time for coherent and fluent token representations. The parameter $\\mu$ denotes the token shift, which determines the interpolation weight. Similar to tensor-product attention, this block applies a non-linear activation function $\\sigma$ to $q_{t}$ and combines it with the hidden states $h\\left(k_{t}, v_{t}\\right)$ using element-wise multiplication as an update gate. Furthermore, the hidden states $h\\left(k_{t}, v_{t}\\right)$ serve as both the reset gate and a replacement for the traditional weighted sum value. This process of calculating the hidden states can be described by the following equations:\n\n$$\n\\begin{aligned}\na_{0}, b_{0}, p_{0} & =0,0,0 \\\\\np_{t} & =\\max \\left(p_{t-1}, k_{t}\\right) \\\\\nh_{t} & =\\frac{\\exp \\left(p_{t-1}-p_{t}\\right) \\odot a_{t-1}+\\exp \\left(k_{t}-p_{t}\\right) \\odot v_{t}}{\\exp \\left(p_{t-1}-p_{t}\\right) \\odot b_{t-1}+\\exp \\left(k_{t}-p_{t}\\right)}\n\\end{aligned}\n$$\n\nNote that the position biases are omitted in these equations for convenience, and the division is element-wise division. Intuitively, the hidden states are computed recursively, and the vector $p$ serves as the reset gate in this process. Channel-Mix Block This block is to amplify the outputs of time-mix block, which is given by:\n\n$$\n\\begin{aligned}\nr_{t} & =\\left(\\mu_{r} \\odot o_{t}+\\left(1-\\mu_{r}\\right) \\odot o_{t-1}\\right) \\cdot W_{r} \\\\\nz_{t} & =\\left(\\mu_{z} \\odot o_{t}+\\left(1-\\mu_{z}\\right) \\odot o_{t-1}\\right) \\cdot W_{z} \\\\\n\\tilde{x}_{t} & =\\sigma\\left(r_{t}\\right) \\odot\\left(\\max \\left(z_{t}, 0\\right)^{2} \\cdot W_{v}\\right)\n\\end{aligned}\n$$\n\nwhere squared ReLU activation So et al. [2022] is adopted. In these equations, the output $o_{t}$ contains the history information up to time $t$, and the interpolation weight $\\mu$ is derived from both $o_{t}$ and $o_{t-1}$ same as in time-mix block. Intuitively, this amplification process enhances the representations of historical information. In sum, the RWKV model has achieved the parallelization in a time-parallel mode due to the tensor-product Lei et al. [2018] and contains the linear computational costs for discarding similarity matrix.\n```\n\n#### 3. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.99)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 43/64 (Score: 0.99)*\n\n```\nIntuitively, important information can effectively flag itself for inclusion using this mechanism, and less important information can flag itself to partially or fully avoid entering the data stream, leaving room for more important pre-existing data to remain. Viewed from the perspective of induction heads, we theorize that this could allow for potential misleading matches to be pre-filtered out up front if they are not deemed useful for a given task. Improved WKV (Weighted Key-Value State) Modules The Eagle WKV attention sub-module is similar to the linear attention mechanism found in RetNet, but with learned per-channel decay rates replacing RetNet's static per-head decay rates. Our matrix-valued states feature a geometrically decaying $K^{\\mathrm{T}} V \\in \\mathbb{R}^{(D / h) \\times(D / h)}$ term. This term can be intuitively understood as\na memory bank of values, with $K$ acting as an input gate for rows receiving the current token embedding's value. Each row of this state decays at its own rate via the learned parameter $w$. In Finch, we augment the learned token-shift parameters $\\mu_{r}, \\mu_{k}, \\mu_{\\nu}, \\mu_{w}$ and decay rate parameter $w$ with learned weight matrices. Inspired by Low-Rank Adaptation (LoRA) (Hu et al., 2022), we provide two new learned weight matrices for each such parameter $y$, computing $y^{\\prime}=y+$ $\\tanh (x A) B$. This approach allows us to dynamically generate data-dependent token-shift amounts and decay rates with only modest increases in computational cost and model size. Extra SiLU Gating We remove the Sigmoid activation of receptance in favor of a new SiLU gate on the output of our linear attention calculation. Our receptance term now functions much like the query term in linear attention. ## Eagle and Finch Linear Attention Formula, PyTorch Recurrent Implementation\n\n```\n# r, k, v parameter shape (B,H,1,D//H)\n# w parameter of shape (1,H,1,D///H) for Eagle (RWKV-5)\n# (B,H,1,D//H) for Finch (RWKV-6)\n# u parameter of shape (1,H,1,D///H)\n# wkv_state parameter of shape (B,H,D//H,D//H)\ndef rwkv_5_or__6_recurrent(r, k, v, w, u, wkv_state):\n    kV = k.mT@ V\n    out = r @(wkv_state + u.mT * kv)\n    wkv_state = w.mT * wkv__state + kv\n    return out, wkv__state\n```\n\nEvolution of RWKV Formula in Expanded form Table 7 shows the expansion of terms at each sequence position to illustrate the progression of changes from RWKV-4 through RWKV-6.\n```\n\n#### 4. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.98)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 1/24 (Score: 0.98)*\n\n```\n# A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models \n\nItamar Zimerman*<br>Ameen Ali $^{*}$<br>Lior Wolf<br>The Blavatnik School of Computer Science, Tel Aviv University<br>\\{zimerman1,ameenali\\}@mail.tau.ac.il, wolf@cs.tau.ac.il\n\n\n#### Abstract\n\nRecent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.98)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 59/74 (Score: 0.98)*\n\n```\n- RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free Transformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \"WKV\" mechanism involves LTI recurrences and can be seen as the ratio of two SSMs. We also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformer's MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP blocks. ## B. 3 Relationship to RNNs\n\nRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state. Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016), and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:\n\n- They do not use state expansion $(N=1)$ or selective $B, C$ parameters, both of which are important for performance (Section 4.6). - They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism + discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and initializations (Section 3.6). Additionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991; Hochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The former could be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latter was difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more careful parameterization of the recurrent dynamics inspired by classical SSM theory (e.g.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: Gated Linear Attention, RWKV architecture, Linear Attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Mega: Moving Average Equipped Gated Attention\n\n*From Search Query: Gated Linear Attention*\n\n*Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, Luke Zettlemoyer*\n\n**TL;DR:** This paper introduces Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.\n\n**Abstract:** The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 143  (*Influential: 28*)\n\n#### 2. Transformer Quality in Linear Time\n\n*From Search Query: Gated Linear Attention*\n\n*Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le*\n\n**TL;DR:** This work revisit the design choices in Transformers, and proposes a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss, and a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality.\n\n**Abstract:** We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and 12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on C4 for masked language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 175  (*Influential: 35*)\n\n#### 3. RWKV: Reinventing RNNs for the Transformer Era\n\n*From Search Query: RWKV architecture*\n\n*Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xingjian Du, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu*\n\n**TL;DR:** This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Abstract:** Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 356  (*Influential: 34*)\n\n#### 4. DARTS: Differentiable Architecture Search\n\n*From Search Query: RWKV architecture*\n\n*Hanxiao Liu, K. Simonyan, Yiming Yang*\n\n**TL;DR:** The proposed algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.\n\n**Abstract:** This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 4045  (*Influential: 1246*)\n\n#### 5. MLP-Mixer: An all-MLP Architecture for Vision\n\n*From Search Query: RWKV architecture*\n\n*I. Tolstikhin, N. Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy*\n\n**TL;DR:** It is shown that while convolutions and attention are both sufficient for good performance, neither of them are necessary, and MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs), attains competitive scores on image classification benchmarks.\n\n**Abstract:** Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e.\"mixing\"the per-location features), and one with MLPs applied across patches (i.e.\"mixing\"spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 2214  (*Influential: 326*)\n\n#### 6. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\n\n*From Search Query: Linear Attention*\n\n*Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Franccois Fleuret*\n\n**TL;DR:** This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.\n\n**Abstract:** Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 1343  (*Influential: 178*)\n\n#### 7. Linear attention is (maybe) all you need (to understand transformer optimization)\n\n*From Search Query: Linear Attention*\n\n*Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, A. Jadbabaie, S. Sra*\n\n**TL;DR:** The results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.\n\n**Abstract:** Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 4*)\n\n### 5 related papers from ArXiv\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: Gated Linear Attention*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 2. Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency,\n  Performance, and Adversarial Robustness\n\n*From Search Query: Gated Linear Attention*\n\n*Xiaojing Fan, Chunliang Tao*\n\n**Abstract:** With the increasing demand for practical applications of Large Language\nModels (LLMs), many attention-efficient models have been developed to balance\nperformance and computational cost. However, the adversarial robustness of\nthese models remains under-explored. In this work, we design a framework to\ninvestigate the trade-off between efficiency, performance, and adversarial\nrobustness of LLMs and conduct extensive experiments on three prominent models\nwith varying levels of complexity and efficiency -- Transformer++, Gated Linear\nAttention (GLA) Transformer, and MatMul-Free LM -- utilizing the GLUE and\nAdvGLUE datasets. The AdvGLUE dataset extends the GLUE dataset with adversarial\nsamples designed to challenge model robustness. Our results show that while the\nGLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE\ntasks, they demonstrate higher efficiency and either superior or comparative\nrobustness on AdvGLUE tasks compared to Transformer++ across different attack\nlevels. These findings highlight the potential of simplified architectures to\nachieve a compelling balance between efficiency, performance, and adversarial\nrobustness, offering valuable insights for applications where resource\nconstraints and resilience to adversarial attacks are critical.\n\n**Published:** 2024-08-08T16:54:40Z  (*Updated: 2024-09-14T03:19:10Z*)\n\n\n\n#### 3. Enhancing Transformer RNNs with Multiple Temporal Perspectives\n\n*From Search Query: RWKV architecture*\n\n*Razvan-Gabriel Dumitru, Darius Peteleaza, Mihai Surdeanu*\n\n**Abstract:** We introduce the concept of multiple temporal perspectives, a novel approach\napplicable to Recurrent Neural Network (RNN) architectures for enhancing their\nunderstanding of sequential data. This method involves maintaining diverse\ntemporal views of previously encountered text, significantly enriching the\nlanguage models' capacity to interpret context. To show the efficacy of this\napproach, we incorporate it into the Receptance Weighted Key Value (RWKV)\narchitecture, addressing its inherent challenge of retaining all historical\ninformation within a single hidden state. Notably, this improvement is achieved\nwith a minimal increase in the number of parameters --even as little as\n$0.04\\%$ of the original number of parameters. Further, the additional\nparameters necessary for the multiple temporal perspectives are fine-tuned with\nminimal computational overhead, avoiding the need for a full pre-training. The\nresulting model maintains linear computational complexity during prompt\ninference, ensuring consistent efficiency across various sequence lengths. The\nempirical results and ablation studies included in our research validate the\neffectiveness of our approach, showcasing improved performance across multiple\nbenchmarks. The code, model weights and datasets are open-sourced at:\nhttps://github.com/RazvanDu/TemporalRNNs.\n\n**Published:** 2024-02-04T22:12:29Z  (*Updated: 2024-07-11T20:43:59Z*)\n\n\n\n#### 4. RRWKV: Capturing Long-range Dependencies in RWKV\n\n*From Search Query: RWKV architecture*\n\n*Leilei Wang*\n\n**Abstract:** Owing to the impressive dot-product attention, the Transformers have been the\ndominant architectures in various natural language processing (NLP) tasks.\nRecently, the Receptance Weighted Key Value (RWKV) architecture follows a\nnon-transformer architecture to eliminate the drawbacks of dot-product\nattention, where memory and computational complexity exhibits quadratic scaling\nwith sequence length. Although RWKV has exploited a linearly tensor-product\nattention mechanism and achieved parallelized computations by deploying the\ntime-sequential mode, it fails to capture long-range dependencies because of\nits limitation on looking back at previous information, compared with full\ninformation obtained by direct interactions in the standard transformer.\nTherefore, the paper devises the Retrospected Receptance Weighted Key Value\n(RRWKV) architecture via incorporating the retrospecting ability into the RWKV\nto effectively absorb information, which maintains memory and computational\nefficiency as well.\n\n**Published:** 2023-06-08T13:17:06Z  (*Updated: 2024-09-13T08:58:47Z*)\n\n\n\n#### 5. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry\n\n*From Search Query: Linear Attention*\n\n*Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher R\u00e9*\n\n**Abstract:** Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.\n\n**Published:** 2024-02-06T19:31:26Z  (*Updated: 2024-02-06T19:31:26Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Gated Slot Attention for Efficient Linear-Time Sequence Modeling\n\n*From Search Query: Gated Linear Attention*\n\n*Guohong Fu, Peng Zhou, Wei Bi, Bailin Wang, Freda Shi, Bolun Wang, Yiqiao Wang, Leyang Cui, Yue Zhang, Ruijie Zhu, Songlin Yang, Yu Zhang*\n\n**Abstract:** Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.\n\n**Published:** 2024-09-11\n\n\n\n#### 2. Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures\n\n*From Search Query: RWKV architecture*\n\n*Wenhai Wang, Jifeng Dai, Hongsheng Li, Yu Qiao, Tong Lu, Lewei Lu, Xizhou Zhu, Zhe Chen, Weiyun Wang, Yuchen Duan*\n\n**Abstract:** Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at \\url{https://github.com/OpenGVLab/Vision-RWKV}.\n\n**Published:** 2024-03-04\n\n\n\n#### 3. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence\n\n*From Search Query: RWKV architecture*\n\n*Rui-Jie Zhu, Jian Zhu, Peng Zhou, Qihang Zhao, Bingchen Zhao, Ruichong Zhang, Stanis\u0142aw Wo\u017aniak, Cahya Wirawan, Haoqin Tu, Guangyu Song, Fares Obeid, Niklas Muennighoff, Jiaju Lin, Atsushi Saito, Xingjian Du, Ronald McClelland Jr., Satyapriya Krishna, Bart\u0142omiej Koptyra, Jan Koco\u0144, Kranthi Kiran GV, Przemys\u0142aw Kazienko, Haowen Hou, Teddy Ferdinan, Eugene Cheah, Stella Biderman, Eric Alcaide, Alon Albalak, Quentin Anthony, Daniel Goldstein, Bo Peng*\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n**Published:** 2024-04-08\n\n\n\n#### 4. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\n\n*From Search Query: Linear Attention*\n\n*Mike Lewis, Noah A. Smith, Ofir Press*\n\n**Abstract:** Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.\n\n**Conference:** train-short-test-long-attention-with-linear-1\n\n**Published:** 2021-08-27\n\n\n\n#### 5. Linformer: Self-Attention with Linear Complexity\n\n*From Search Query: Linear Attention*\n\n*Madian Khabsa, Hao Ma, Sinong Wang, Belinda Z. Li, Han Fang*\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n**Published:** 2020-06-08\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models by integrating Gated Linear Attention (GLA) and concepts from the RWKV architecture, here are some key points and references that can guide the research:\n\n## Existing Implementations and Similar Concepts\n\n- The concept of Gated Linear Attention (GLA) has been explored in various state-space models (SSMs). For instance, the Gated Linear RNNs with State Expansion (HGRN2) model uses a similar gating mechanism, although it is more closely related to traditional RNNs rather than linear attention mechanisms.\n- The RWKV architecture itself uses element-wise recurrence and maintains a state in a ratio form, which is updated using separate SSMs. The latest version of RWKV has moved towards an outer product similar to linear attention variants.\n\n## Efficiency and Scalability\n\n- Linear attention mechanisms, such as those used in GLA and RWKV, are designed to achieve linear complexity in both time and space. This is a significant improvement over traditional attention mechanisms, which often have quadratic complexity. For example, the Longhorn model, which is based on state-space models, demonstrates efficient and stable recurrent updates without manually designed gating mechanisms, showing comparable or better performance than state-of-the-art SSMs like Mamba.\n- The Approximate Gated Linear Transformer (AGaLiTe) also reduces space complexity by using a low-rank approximation, making it more efficient in space and time compared to traditional linear transformers.\n\n## Expressiveness and Adaptability\n\n- GLA's data-dependent gates enhance the model's expressiveness by allowing it to control the flow of information more dynamically. This is similar to the gating mechanism in AGaLiTe, which uses a normalized exponential average with element-wise learned decay parameters to smoothly reduce the impact of past information.\n- The RWKV architecture's use of element-wise recurrence and separate updates for the numerator and denominator (or their equivalents) also adds to the model's adaptability and expressiveness.\n\n## Comparison with Existing Models\n\n- Models like Mamba and the original RWKV architecture have been compared in terms of their performance on sequence modeling tasks. Longhorn, for example, outperforms Mamba on certain tasks, indicating that different linear attention mechanisms can have varying advantages.\n- AGaLiTe and GaLiTe are compared with GTrXL, showing improved computational complexities and empirical latency measurements, which can serve as a benchmark for the proposed integration.\n\n## Theoretical Justification\n\n- The theoretical framework for using linear attention mechanisms is grounded in the idea of solving online learning objectives. This perspective helps in streamlining the design of state-space models and justifies the use of linear attention variants like GLA and RWKV.\n- The scaling laws for large language models, such as the Chinchilla scaling law, provide a theoretical basis for understanding how model size, dataset size, and computational cost are related, which is crucial for justifying the scalability of the proposed model.\n\n### Integration Plan\n\n1. **Combine Gating Mechanism with RWKV Updates**:\n   - Integrate the data-dependent gating mechanism of GLA into the RWKV architecture to enhance expressiveness and adaptability. This can involve using the gating mechanism to control the updates of the numerator and denominator in the RWKV state recurrence.\n\n2. **Optimize for Linear Complexity**:\n   - Ensure that the integration maintains linear complexity in both time and space. This can be achieved by using low-rank approximations or efficient recurrence updates as seen in AGaLiTe and Longhorn.\n\n3. **Evaluate on Sequence Modeling Tasks**:\n   - Benchmark the proposed model against existing models like Mamba, RWKV, and AGaLiTe on various sequence modeling tasks to assess its performance and efficiency.\n\n4. **Theoretical Validation**:\n   - Validate the theoretical soundness of the proposed integration by ensuring it aligns with the online learning perspective and scaling laws for large language models.\n\nBy following these steps and leveraging the insights from existing research, the integration of GLA and RWKV concepts into an autoregressive model like FastTTTLinear can be both novel and effective, offering improvements in efficiency, scalability, and expressiveness.\n",
            "analysis": "### Initial Analysis\n\nThe proposal for **FastTTTLinear** aims to enhance the TTTLinear GAU by integrating Gated Linear Attention (GLA) and concepts from the RWKV architecture. The main objectives are to improve computational efficiency and scalability for long sequences while maintaining test-time training capabilities. Key aspects to investigate include:\n\n1. **Novelty**: The integration of GLA and RWKV concepts into TTTLinear is a novel approach. However, similar concepts have been explored in other architectures like Mamba and RWKV. It is crucial to determine if this specific integration within the TTTLinear framework has been explored before.\n\n2. **Efficiency and Scalability**: The proposal claims linear complexity in both time and space, which is a significant improvement over traditional attention mechanisms. Investigating existing implementations of linear attention mechanisms and their effectiveness in real-world scenarios will be important.\n\n3. **Expressiveness and Adaptability**: GLA's data-dependent gates are designed to enhance expressiveness. Understanding how this impacts the model's ability to adapt during test-time training is crucial.\n\n4. **Comparison with Existing Models**: The proposal should be compared with existing models like Mamba, RWKV, and others that utilize linear attention mechanisms to understand its relative advantages and potential drawbacks.\n\n5. **Theoretical Justification**: The theoretical basis for using GLA and RWKV concepts should be examined to ensure that the proposed modifications are sound and likely to achieve the desired improvements.\n\n### Plan for Investigation\n\n1. **Search for Existing Implementations**: Look for existing implementations or research papers that have explored similar integrations of GLA and RWKV concepts in autoregressive models.\n\n2. **Evaluate Efficiency Claims**: Investigate papers and benchmarks that discuss the efficiency and scalability of linear attention mechanisms, especially in the context of long sequences.\n\n3. **Assess Expressiveness**: Explore studies that evaluate the impact of data-dependent gating mechanisms on model expressiveness and adaptability.\n\n4. **Compare with Related Work**: Identify and compare the proposal with similar models like Mamba and RWKV to assess its novelty and potential advantages.\n\n5. **Theoretical Analysis**: Review theoretical papers that justify the use of GLA and RWKV concepts in language models to ensure that the proposal is grounded in sound theory.\n\n### Keywords\n- Gated Linear Attention\n- RWKV architecture\n- Linear Attention Mechanisms\n\n### Detailed Query\n\"Integration of Gated Linear Attention and RWKV in autoregressive models\""
        },
        {
            "ready": false,
            "query": "Empirical evaluation GLA, RWKV implementation challenges, Linear attention benchmarks",
            "detail": "Empirical performance of GLA and RWKV in real-world scenarios",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nEmpirical performance of GLA and RWKV in real-world scenarios\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training (Avg. Score: 0.98)\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 43  (*Influential: 9*)\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n##### *Relevant Chunk: No. 13/51 (Score: 0.98)*\n\n```\nAll evaluations are performed using the LM evaluation harness (Gao et al., 2021). Our main results are shown in Table 2. Compared to RetNet which uses a data-independent decay rate, the GLA Transformer with data-dependent gates shows improved results on all tasks. Both GLA Transformer and Mamba show comparable performance to Transformer++. Recall-intensive tasks. While subquadratic models can achieve competitive language modeling performance to Transformers, Arora et al. (2024) show that they lag behind softmax attention in recall-intensive tasks. We next evaluate GLA on real and synthetic tasks that focus on recall. The synthetic MQAR task (Arora et al., 2023a) is a more challenging multi-query version of the induction head task (Fu et al., 2023b) in which a model has to recall the token following a query token multiple times. We follow Arora et al. (2023a)'s experimental setting and compare GLA against recent subquadractic models, including RetNet (Sun et al., 2023a), Mamba (Gu \\& Dao, 2023), Hyena (Poli et al., 2023) and RWKV-4 (Peng et al., 2023). For RetNet and GLA the number of heads is set to 2 ; for other models we follow the default settings in Arora et al. (2023a). The results are shown\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_fe8f969e3ab84f3972dcg-08.jpg?height=457&width=1441&top_left_y=197&top_left_x=315)\n\nFigure 5: Length extrapolation on the test set of SlimPajama and PG19. We pretrain 1.3B models from scratch on SlimPajama for 100B tokens with different training length. ${ }^{*}$ indicates models using truncated BPTT with over 12 segments that are each of 2 K -length. | Scale | Model | FDA | SWDE | SQUAD |\n| :--- | :--- | :---: | :---: | :---: |\n| 340M Params | Transformer++ | 21.4 | 42.2 | 22.1 |\n| 15B Tokens | RetNet | 2.9 | 13.3 | 27.6 |\n|  | Mamba | 2.1 | 12.4 | 23.0 |\n|  | GLA | 8.1 | 18.6 | 27.2 |\n| 1.3B Params | Transformer++ | 27.4 | 66.6 | 31.5 |\n| 100B Tokens | RetNet | 14.3 | 42.8 | 34.7 |\n|  | Mamba | 6.2 | 41.4 | 35.2 |\n|  | GLA | 19.9 | 50.6 | 42.6 |\n\nTable 3: Comparison of different models in three recall-intensive tasks tested in Arora et al.\n```\n\n#### 2. Reinventing RNNs for the Transformer Era (Avg. Score: 0.84)\n\n*Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 251  (*Influential: 27*)\n\n**TL;DR:** This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Abstract:** Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n##### *Relevant Chunk: No. 6/48 (Score: 0.90)*\n\n```\nLiu, 2017), and Winogrande (Zellers et al., 2020). Figure 1 shows the average results across all benchmarks. Some individual benchmarks are shown in Fig 5, with the rest in Appendix J. Additionally, we carried out comparative studies on RWKV and ChatGPT / GPT-4, see Appendix L. They revealed that RWKV is very sensitive to prompt engineering. When the prompts were adjusted (re-ordered) from the ones used for GPT to more suitable for RWKV, the performance (F1) increased even from $44.2 \\%$ to $74.8 \\%$. For sarcasm detection, RWKV outperformed ChatGPT, but was still slightly worse than the SOTA solution. ### 5.2 Extended Context Finetuning\n\nUnlike transformers, RNNs do not have a predefined sequences length when they are created. However in order to efficient make use of compute\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-07.jpg?height=939&width=1556&top_left_y=266&top_left_x=255)\n\nFigure 5: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. Additional plots can be found in Appendix J. we nevertheless need to preprocess the training data into contexts of the same length. We find that we are able to teach the model how to efficiently handle substantially larger batch sizes by finetuning with progressively increasing sequence length. Specifically, we first double the sequence length from 1024 to 2048 and finetune for 10B tokens from the original pretraining corpus, then we double again to 4096 for 100B tokens from the same corpus, and finally double to 8192 tokens for another 100B tokens from the same corpus. In Fig. 6 we show that increasing context length leads to lower test loss on the Pile, an indication that RWKV can make effective use of long contextual information. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-07.jpg?height=318&width=673&top_left_y=2145&top_left_x=286)\n\nFigure 6: RWKV shows decreasing mean test loss as a function of context length on the Pile (Gao et al., 2020)\n\n### 5.3 Long Context Benchmarks\n\nAdditionally, we evaluate our model's ability to handle very long sequences by comparing to stateof-the-art long sequence models on the LongRange Arena (LRA) benchmark (Tay et al., 2021). LRA is designed to assess the performance of models in handling lengthy context situations. It includes a collection of tasks with sequences ranging from 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images, and mathematical expressions. We apply RWKV on the LRA benchmark and the results are in Appendix J.2. The results show that RWKV performs second only to the S 4 model in five datasets. ## 6 Inference Experiments\n\nWe benchmark inference requirements according to size and family. Specifically, we evaluate text generation speed and memory requirements on typical compute platforms including CPU (x86) and GPU (NVIDIA A100 80 GB ). For all of our inference experiments we use float 32 precision and the HuggingFace Transformers (Wolf et al., 2020). We include all model parameters in the parameter count, including both embedding and non-embedding layers. Performance under different quantization setups is left to further work. See Appendix K for\nmore results. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-08.jpg?height=481&width=629&top_left_y=319&top_left_x=308)\n\nFigure 7: Cumulative time on text generation for LLMs. Unlike transformers, RWKV exhibits linear scaling. ## 7 Future Work\n\nThere are several promising directions for future work on the RWKV architecture. Work can be done to increase model expressivity by enhancing the time-decay formulations and exploring initial model states while maintaining efficiency. The RWKV computational efficiency can be further improved by applying a parallel scan in the $w k v_{t}$ step to reduce the computational cost to $O(B \\log (T) d)$. The mechanisms used in RWKV can be applied to encoder-decoder architectures, potentially replacing the cross-attention mechanism. This could be applicable in seq2seq or multimodal settings, thereby enhancing efficiency during both training and inference. RWKV's state (or context) can be leveraged for interpretability, predictability in sequence data, and safety. Manipulating the hidden state could also guide behavior and allow greater customizability through prompt tuning. The RWKV architecture is not perfect, and can be improved via many aspects, such as modifying the formulae or implementing larger internal states. Larger states can enhance the model's memory to previous context and improve performance over various tasks. ## 8 Conclusions\n\nWe introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and longrange dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations. We benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs. RWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters. ## 9 Limitations\n\nWhile our proposed RWKV model has demonstrated promising results regarding training and memory efficiency during inference, some limitations should be acknowledged and addressed in future work. First, the linear attention of RWKV leads to significant efficiency gains but still, it may also limit the model's performance on tasks that require recalling minutiae information over very long contexts. This is due to the funneling of information through a single vector representation over many time steps, compared with the full information maintained by the quadratic attention of standard Transformers. In other words, the model's recurrent architecture inherently limits its ability to \"look back\" at previous tokens, as opposed to traditional self-attention mechanisms. While learned time decay helps prevent the loss of information, it is mechanistically limited compared to full selfattention. Another limitation of this work is the increased importance of prompt engineering in comparison to standard Transformer models. The linear attention mechanism used in RWKV limits the information from the prompt that will be carried over to the model's continuation. As a result, carefully designed prompts may be even more crucial for the model to perform well on tasks. The above RWKV property was confirmed by studies on prompt engineering presented in Appendix L. By changing the order of the information\npieces, we were even able to almost double the RWKV performance for some tasks. ## 10 Ethics Statement\n\nIn this paper, we present a novel architecture for sequential data processing and prove its effectiveness by building a series of LLMs trained on publicly released pretraining data (Gao et al., 2020; Biderman et al., 2022) and later fine-tuned on publicly available instructions (Taori et al., 2023; Chaudhary, 2023; Cheung, 2023; Anand et al., 2023; Anonymous, 2023; Yang, 2023; Ji et al., 2023a,b). As a novel architecture for sequential data, RWKV has the potential to improve sequencebased models across different applications ranging from natural language processing to biomedical data processing or climate modelling. Since the training code is released open source, RWKV contributes to the democratization of AI , levels the playing field, and empowers members of the Open Source community to inspect, study, and finetune RWKV in particular tasks. Moreover, it contributes to advancing the understanding of LLMs capabilities and limitations. A significant amount of work has been devoted to increasing the efficiency of RWKV training so as to minimize its cost and promote accessibility. As LLMs trained on public data, RWKV's lower inference cost compared to Transformer alternatives makes it more suitable for deployment in consumer and edge hardware, which is a step towards the democratization and distribution of LLMs to the general public, creating better privacy and ownership incentives. It also lowers the resource barrier to Chat assistants and text generation for small and/or underrepresented communities. PreTrained model weights for different sizes ranging from 0.1 B to 14B parameters trained on multiple languages are released to increase ease of adoption and allow for the study of emergent phenomena. On the other hand, with lower resource barriers, the spreading of AI-generated text might become more prevalent. Current RWKV LLMs may exhibit and/or reproduce biases and potentially harmful content present in the data used for training. Nonetheless, mitigation and finetuning strategies discussed for other, large Transformer models should be applicable to RWKV as well. ## Acknowledgements\n\nWe thank StabilityAI for the compute used to train our models and for technical support in development of RWKV. We also thank the members of the RWKV and EleutherAI Discord servers for their help and work on further extending the applicability of RWKV to different domains.\n```\n\n##### *Relevant Chunk: No. 42/48 (Score: 0.89)*\n\n```\nIntuitively, it is the discount factor applied to previous tokens over time. - $u_{i}(16)$, also known as \"bonus\", is set to $0.5 \\cdot(((i+1) \\bmod 3)-1)+\\log 0.3$. It is the special weighting applied to the current token in equation 16. The alternating zigzag pattern initially creates subtle variations in the tensor elements, which are intended to help the model treat different dimensions of the embedding distinctively. - $W_{o}$ (17) (time-mixing) and $W_{v}$ (channel-mixing) are initialized to $\\mathcal{N}\\left(0, \\sqrt{\\frac{d}{s}}=2\\right)$\n- All other $W_{r}, W_{k}, W_{v}$ weights are initialized to 0 so the model can start learning from the beginning without noisy signals. - All LayerNorm weights start from 1 and biases from 0. ## F Small Init Embedding\n\nThis section presents the experimental validation of small initialization embedding. The experimental setup is as follows. In the baseline configuration, the parameters are initialized using a normal distribution with a mean of 0.0 and a standard deviation of 0.02 , which is a commonly used initialization method in models like BERT and GPT. On the other hand, in the small initialization of the embedding (small init emb) experiment, the parameters are initialized using a uniform distribution with a range of $1 \\mathrm{e}-4$, which is slightly different from RWKV where a normal distribution with a standard deviation of 1e-4 is used. However, this difference is negligible and does not affect our conclusions. The experiments were conducted with a batch size of 400 . As depicted in Figure 9, the loss curve for the small init emb exhibits a faster rate of decrease and convergence compared to the traditional initialization using a normal distribution. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-18.jpg?height=712&width=926&top_left_y=1649&top_left_x=542)\n\nFigure 9: Effect of small initialization embedding. ## G Hyperparameters\n\nTo train the models mentioned, we use $\\epsilon=(0.9,0.99)$ without weight decay for the Adam optimizer, and switch batch size dynamically between 128 or 256 sequences, each of 1024 tokens. We further organize\n\n| Model | 169 M | 430 M | 1.5 B | 3 B | 7 B | 14 B |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Init LR | 0.0006 | 0.0004 | 0.0003 | 0.00015 | 0.00015 | 0.0001 |\n| Warmup Mini-Epochs | 361 | 411 | 443 | 451 | 465 | 544 |\n| End LR | 0.00001 | 0.00001 | 0.00001 | 0.00001 | 0.00001 | 0.000007 |\n\nTable 3: Hyperparameters for our learning rate (LR) schedule of the pretrained models. the training into multiple mini-epochs, each of 40320 samples, to guide our learning rate schedule. The training process takes 8043 mini-epochs to make one pass over the Pile. The initial warming up miniepochs have a constant learning rate of \"Init LR\". After the warming up mini-epochs, the learning rate exponentially decays until in the last mini-epoch, in which the model finishes training on the entire Pile, the learning rate arrives at the \"End LR\". The related hyperparameters are shown in Table 3. ## H Gradient Stability in RWKV\n\nIn this section, we present a mathematical description of the gradient stability property in RWKV, focusing specifically on the time-mixing block. By gradient stability we mean that if the inputs $x_{t}$ are bounded and the model parameters are fixed, then the gradients with respect to $W_{k}$ and $W_{v}$ are uniformly bounded for all $T$ (thus not exploding). Consequently, we can control the amount each $x_{t}$ contributes to the gradient at $T$ in a naturally decaying fashion by the weight decay mechanism $w$ (thus not vanishing unless desired). First, we make the simplification that there are no token shifts, this will not affect the final conclusion. In this scenario, $w k v_{T}$ can be written as\n\n$$\nw k v_{T}=\\frac{\\sum_{t=1}^{T} K_{t}^{e} \\odot v_{t}}{\\sum_{t=1}^{T} K_{t}^{e}}=\\mathrm{E}\\left(v_{t}\\right)=\\frac{\\mathrm{S}\\left(v_{t}\\right)}{\\mathrm{S}(1)}\n$$\n\nwhere\n\n$$\n\\begin{gathered}\nv_{t}=W_{v} x_{t}, \\quad \\frac{\\partial\\left(v_{t}\\right)_{i}}{\\partial\\left(W_{v}\\right)_{i, j}}=\\left(x_{t}\\right)_{j} \\\\\nK_{t}^{e}=e^{W_{k} x_{t}+w_{T, t},} \\quad \\frac{\\partial\\left(K_{t}^{e}\\right)_{i}}{\\partial\\left(W_{k}\\right)_{i, j}}=\\left(x_{t}\\right)_{j}\\left(K_{t}^{e}\\right)_{i}\n\\end{gathered}\n$$\n\nand $\\mathrm{S}(\\cdot)$ and $\\mathrm{E}(\\cdot)$ are shorthand for denoting sums and averages over weights $K_{t}^{e}$. The loss function at position $T$ can be written as\n\n$$\nL_{T}=l\\left(f\\left(w k v_{T}\\right), y_{T}\\right)\n$$\n\nBecause $w k v_{T}$ relates to $\\left(W_{k}\\right)_{i, j}$ and $\\left(W_{v}\\right)_{i, j}$ only through the $i$-th channel $\\left(w k v_{T}\\right)_{i}$, we have\n\n$$\n\\frac{\\partial L_{T}}{\\partial\\left(W_{v}\\right)_{i, j}}=\\frac{\\partial L_{T}}{\\partial\\left(w k v_{T}\\right)_{i}} \\frac{\\partial\\left(w k v_{T}\\right)_{i}}{\\partial\\left(W_{v}\\right)_{i, j}}\n$$\n\nThe first part of the above equation contains trivial operations like output layers, and other layers of time-mixing, which can be proven inductively. The second part of the above equation can be bounded as\n\n$$\n\\begin{aligned}\n\\left|\\frac{\\partial\\left(w k v_{T}\\right)_{i}}{\\partial\\left(W_{v}\\right)_{i, j}}\\right| & =\\left|\\frac{\\partial \\mathrm{E}_{i}\\left[\\left(v_{t}\\right)_{i}\\right]}{\\partial\\left(W_{v}\\right)_{i, j}}\\right| \\\\\n& =\\left|\\mathrm{E}_{i}\\left[\\left(x_{t}\\right)_{j}\\right]\\right| \\leq \\max _{t}\\left|\\left(x_{t}\\right)_{j}\\right|\n\\end{aligned}\n$$\n\nwhich is irrelevant to $T$. Similarly,\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\left(w k v_{T}\\right)_{i}}{\\partial\\left(W_{k}\\right)_{i, j}} & =\\partial \\frac{\\mathrm{S}_{i}\\left[\\left(v_{t}\\right)_{i}\\right]}{\\mathrm{S}_{i}(1)} / \\partial\\left(W_{k}\\right)_{i, j} \\\\\n& =\\frac{\\mathrm{S}_{i}\\left[\\left(x_{t}\\right)_{j}\\left(v_{t}\\right)_{i}\\right]}{\\mathrm{S}_{i}(1)}-\\frac{\\mathrm{S}_{i}\\left[\\left(x_{t}\\right)_{j}\\right] \\mathrm{S}_{i}\\left[\\left(v_{t}\\right)_{i}\\right]}{\\mathrm{S}_{i}(1)^{2}} \\\\\n& =\\mathrm{E}_{i}\\left[\\left(x_{t}\\right)_{j}\\left(v_{t}\\right)_{i}\\right]-\\mathrm{E}_{i}\\left[\\left(x_{t}\\right)_{j}\\right] \\mathrm{E}_{i}\\left[\\left(v_{t}\\right)_{i}\\right] \\\\\n& =\\operatorname{cov}_{i}\\left(\\left(x_{t}\\right)_{j},\\left(v_{t}\\right)_{i}\\right)\n\\end{aligned}\n$$\n\ncan also be bounded. Note that $w k v$ 's softmax operation contains at least two non-zero terms ( $u$ and $w$ ), so the above \"covariance\" will not degenerate into 0 . ## I Model Behavior Visualization\n\nThe right plot illustrates the time decays $\\left(e^{-w}\\right)$ in each layer of the RWKV-169M model, sorted along the channel axis. Notably, several decays in the last layers are very close or equal to one, implying that certain information is preserved and propagated throughout the model's temporal context. Meanwhile, many decays in the initial layer are close to zero, which corresponds to local operations in $w k v(16)$, likely to be associated with tasks such as text parsing or lexical analysis. (Note that the local operations in $w k v$ are due to the extra parameter $u$, when $e^{-w}$ is degenerated into 0 .) These patterns of time decays are partly learned, but also come from parameter initialization as it speeds up training. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-20.jpg?height=563&width=715&top_left_y=935&top_left_x=1067)\n\nFigure 10: Model behavior visualizations of RWKV. The plot below shows the information retrieval and propagation path in the RWKV-430M model. The experiment follows the causal trace method introduced by Meng et al. (2022), where we\n\n1. Run the model once, and record all states and activation of each layer during the computation;\n2. Corrupt the input embeddings of the subject using noise (\"The Eiffel Tower\" in this example);\n3. Restore the states and activation of a certain layer at a certain token during the computation, and record the log-probability of the model outputting the correct answer (\"Paris\"). Unlike transformers, RWKV relies on the recursive propagation of information in the time dimension. In this case, the fact that the Eiffel Tower is located in Paris is retrieved in layer 4 just after the model sees \"The Eiffel\". It is then passed down to the subsequent layers. In layer 20, mostly, the information is propagated through time until reaching where it is needed. Finally, at the token \"of\", it is passed down to the last layer for outputting the answer. ## J Additional Evaluations\n\n## J. 1 Further details on NLP tasks\n\nWe evaluate on the following tasks:\nARC (Clark et al., 2018) A dataset designed for multiple-choice question answering, encompassing science exam questions ranging from third grade to ninth grade. It has Easy and Challenge subsets that we report results on separately. BoolQ (Clark et al., 2019) A binary yes/no question answering benchmark. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-21.jpg?height=563&width=1261&top_left_y=244&top_left_x=406)\n\nFigure 11: Model behavior visualizations of the RWKV model. COPA (Roemmele et al., 2018) A dataset to evaluate achievement in open-domain commonsense causal reasoning. HeadQA (Vilares and G\u00f3mez-Rodr\u00edguez, 2019) A benchmark consisting of graduate-level questions encompassing various fields such as medicine, nursing, biology, chemistry, psychology, and pharmacology. HellaSwag (Zellers et al., 2019) A novel benchmark for commonsense Natural Language Inference (NLI) which is build by adversarial filtering against transformer models. LAMBADA (Paperno et al., 2016) A benchmark dataset that evaluates the model's contextual reasoning and language comprehension abilities by presenting context-target pairs, where the objective is to predict the most probable target token. We follow standard practice and use the untokenized version created by OpenAI (Brown et al., 2020). OpenBookQA (Mihaylov et al., 2018) A QA dataset to evaluate human comprehension of a subject by incorporating open book facts, scientific knowledge, and perceptual common sense, drawing inspiration from open book exams. PIQA (Bisk et al., 2020) A benchmark for the task of physical common sense reasoning, which consists of a binary choice task that can be better understood as a set of two pairs, namely (Goal, Solution). ReCoRD (Zhang et al., 2018) A benchmark for evaluating commonsense reasoning in reading comprehension by generating queries from CNN/Daily Mail news articles and requiring text span answers from corresponding summarizing passages. SciQ (Johannes Welbl Nelson F. Liu, 2017) A multiple-choice QA dataset which was created using an innovative approach to gather well-crafted multiple-choice questions that are focused on a specific domain. Winogrande (Zellers et al., 2020) A dataset designed to evaluate the acquisition of common sense reasoning by neural language models, aiming to determine whether we are accurately assessing the true capabilities of machine common sense. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-22.jpg?height=1887&width=1563&top_left_y=493&top_left_x=250)\n\nFigure 12: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. ## J. 2 Evaluation on Long Range Arena\n\nThe Long-Range Arena (LRA) benchmark (Tay et al., 2021) is designed to assess the performance of models in handling lengthy context situations. It includes a collection of tasks with sequences ranging from 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images, and mathematical expressions. We apply RWKV on the LRA benchmark and the report results are in Table 4. Other models' performances are directly cited from Gu et al. (2022); Alam et al. (2023). Table 4: Evaluation on Long Range Arena. Other models reported in the literature (Gu et al., 2022; Alam et al., 2023). Bolded values are the best. | MODEL | LISTOPS | TEXT | RETRIEVAL | IMAGE | PATHFINDER | PATH-X | AVG |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Transformer | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | $\\boldsymbol{x}$ | 53.66 |\n| Reformer | 37.27 | 56.10 | 53.40 | 38.07 | 68.50 | $\\mathbf{x}$ | 50.56 |\n| BigBird | 36.05 | 64.02 | 59.29 | 40.83 | 74.87 | $\\mathbf{x}$ | 54.17 |\n| Linear Trans. | 16.13 | 65.90 | 53.09 | 42.34 | 75.30 | $\\mathbf{x}$ | 50.46 |\n| Performer | 18.01 | 65.40 | 53.82 | 42.77 | 77.05 | $\\boldsymbol{x}$ | 51.18 |\n| FNet | 35.33 | 65.11 | 59.61 | 38.67 | 77.80 | $\\boldsymbol{x}$ | 54.42 |\n| Nystr\u00f6mformer | 37.15 | 65.52 | 79.56 | 41.58 | 70.94 | $\\mathbf{x}$ | 57.46 |\n| Luna-256 | 37.25 | 64.57 | 79.29 | 47.38 | 77.72 | $\\mathbf{x}$ | 59.37 |\n| Hrrformer | 39.98 | 65.38 | 76.15 | 50.45 | 72.17 | $\\boldsymbol{x}$ | 60.83 |\n| S4 | $\\mathbf{5 9 .\n```\n\n##### *Relevant Chunk: No. 44/48 (Score: 0.82)*\n\n```\n2 0}$ | $\\mathbf{9 6 . 3 5}$ | $\\mathbf{8 6 . 0 9}$ |\n| RWKV | 55.88 | 86.04 | 88.34 | 70.53 | 58.42 | $\\mathbf{x}$ | 72.07 |\n\nThe results show that RWKV performs second only to the S4 model in five datasets. While RWKV substantially underpreforms S4 on Image, Pathfinder, and Path-X, on the problems related to natural language and computer code processing RWKV performs on par with S4 or nearly so.\n```\n\n##### *Relevant Chunk: No. 5/48 (Score: 0.75)*\n\n```\nsoftmax operation improving numerical stability and mitigating vanishing gradients (for rigorous proof, see Appendix H). It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization (Ba et al., 2016) incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues. These design elements not only enhance the training dynamics of deep neural networks but also facilitate the stacking of multiple layers, leading to superior performance over conventional RNN models by capturing complex patterns across different levels of abstraction (see also Appendix I). ### 3.1.1 Token Shift\n\nIn this architecture, all linear projection vectors ( $R, K, V$ in time-mixing, and $R^{\\prime}, K^{\\prime}$ in channelmixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift. The vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n\n$$\n\\begin{aligned}\nr_{t} & =W_{r} \\cdot\\left(\\mu_{r} \\odot x_{t}+\\left(1-\\mu_{r}\\right) \\odot x_{t-1}\\right) \\\\\nk_{t} & =W_{k} \\cdot\\left(\\mu_{k} \\odot x_{t}+\\left(1-\\mu_{k}\\right) \\odot x_{t-1}\\right) \\\\\nv_{t} & =W_{v} \\cdot\\left(\\mu_{v} \\odot x_{t}+\\left(1-\\mu_{v}\\right) \\odot x_{t-1}\\right)\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-04.jpg?height=792&width=764&top_left_y=229&top_left_x=246)\n\nFigure 3: RWKV architecture for language modeling. as are the channel-mixing inputs:\n\n$$\n\\begin{aligned}\nr_{t}^{\\prime} & =W_{r}^{\\prime} \\cdot\\left(\\mu_{r}^{\\prime} \\odot x_{t}+\\left(1-\\mu_{r}^{\\prime}\\right) \\odot x_{t-1}\\right) \\\\\nk_{t}^{\\prime} & =W_{k}^{\\prime} \\cdot\\left(\\mu_{k}^{\\prime} \\odot x_{t}+\\left(1-\\mu_{k}^{\\prime}\\right) \\odot x_{t-1}\\right)\n\\end{aligned}\n$$\n\nThe token shift is implemented as a simple offset in the temporal dimension at each block using the PyTorch (Paszke et al., 2019) library as nn. ZeroPad2d ( $(0,0,1,-1))$. ### 3.1.2 WKV Operator\n\nThe computation of the $W K V$ operator in our model parallels the method used in Attention Free Transformer (AFT) (Zhai et al., 2021). However, unlike AFT where $W$ is a pairwise matrix, our model treats $W$ as a channel-wise vector that is modified by relative position. In our model, this recurrent behavior is defined by the time-dependent update of the $W K V$ vectors, formalized in the following equation:\n\n$$\nw k v_{t}=\\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}} \\odot v_{i}+e^{u+k_{t}} \\odot v_{t}}{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}}+e^{u+k_{t}}}\n$$\n\nTo circumvent any potential degradation of $W$, we introduce a vector $U$ that separately attends to the current token. More information about this can be found in Appendix I. ### 3.1.3 Output Gating\n\nOutput gating is implemented in both time-mixing and channel-mixing blocks using the sigmoid of the receptance, $\\sigma(r)$. The output vector ot post the $W K V$ operator is given by:\n\n$$\no_{t}=W_{o} \\cdot\\left(\\sigma\\left(r_{t}\\right) \\odot w k v_{t}\\right)\n$$\n\nIn the channel-mixing block, a similar operation is performed:\n\n$$\no_{t}^{\\prime}=\\sigma\\left(r_{t}^{\\prime}\\right) \\odot\\left(W_{v}^{\\prime} \\cdot \\max \\left(k_{t}^{\\prime}, 0\\right)^{2}\\right)\n$$\n\nwhere we adopt the squared ReLU activation function (So et al., 2021). ### 3.2 Transformer-like Training\n\nRWKV can be efficiently parallelized using a technique called time-parallel mode, reminiscent of Transformers. The time complexity of processing a batch of sequences in a single layer is $O\\left(B T d^{2}\\right)$, primarily consisting of matrix multiplications $W_{\\lambda}$, where $\\lambda \\in\\{r, k, v, o\\}$ (assuming $B$ sequences, $T$ maximum tokens, and $d$ channels). In contrast, updating attention scores $w k v_{t}$ involves a serial scan (see Appendix D for more detail) and has complexity $O(B T d)$. The matrix multiplications can be parallelized similarly to $W_{\\lambda}$, where $\\lambda \\in\\{Q, K, V, O\\}$ in conventional Transformers. The element-wise $W K V$ computation is time-dependent but can be readily parallelized along the other two dimensions (Lei et al., 2018) ${ }^{3}$. ### 3.3 RNN-like Inference\n\nRecurrent networks commonly utilize the output at state $t$ as input at state $t+1$. This usage is also observed in the autoregressive decoding inference of language models, where each token must be computed before being passed to the next step. RWKV takes advantage of this RNN-like structure, known as time-sequential mode. In this context, RWKV can be conveniently formulated recursively for decoding during inference, as demonstrated in Appendix D. ### 3.4 Additional Optimizations\n\nCustom Kernels To address inefficiencies in the $W K V$ computation arising from the sequential nature of the task when using standard deep learning frameworks, we have developed a custom CUDA\n\n[^2]kernel. This kernel enables the execution of a single compute kernel on training accelerators, while all other parts of the model, such as matrix multiplications and point-wise operations, are already inherently parallelizable and efficient. Small Init Embedding During the initial stage of training a transformer model (Vaswani et al., 2017), we observe that the embedding matrix undergoes slow changes, presenting a challenge for the model to move away from its initial noisy embedding state. To address this issue, we propose an approach that involves initializing the embedding matrix with small values and subsequently applying an additional LayerNorm operation. This accelerates and stabilizes the training process, allowing for the training of deep architectures with post-LN components. The effectiveness of this approach is demonstrated in Figure 9, illustrating improved convergence by enabling the model to quickly transition away from the initially small embedding. This is achieved through small changes occurring in a single step, which subsequently lead to substantial alterations in directions and further notable changes after the LayerNorm operation. Custom Initialization Building on principles from previous works (He et al., 2016; Jumper et al., 2021), we adopt an initialization strategy where parameters are set to values resembling an identity mapping while breaking symmetry to establish a clear information flow. The majority of weights are initialized to zero, and linear layers do not employ biases. Detailed formulas are given in Appendix E. We observe that the choice of initialization plays a crucial role in both the speed and quality of convergence (refer to Appendix F for further details). ### 3.5 Implementation\n\nRWKV is implemented using the PyTorch Deep Learning Library (Paszke et al., 2019). We integrate additional optimization strategies inspired by DeepSpeed (Rasley et al., 2020) into the system, improving its efficiency and scalability. The model begins with an embedding layer, as detailed in Section 3.4. Following this are several identical residual blocks arranged sequentially. These are depicted in Figures 2 and 3 and adheres to the principles outlined in Section 3.1.1. After the last block, a simple output projection head, consisting of a LayerNorm (Ba et al., 2016) and a linear projection, is employed for logits generation for next-token prediction and computation of the cross-entropy loss during training. ## 4 Trained Models and Computing Costs\n\nTo demonstrate the scalability of RWKV, we train six models ranging from 169 million to 14 billion parameters as shown in Table 2. All models are trained for one epoch ( 330 billion tokens) on the Pile (Gao et al., 2020; Biderman et al., 2022). | Name | Layers | Model Dimension | Parameters | FLOP per token |\n| :---: | :---: | :---: | :---: | :---: |\n| 169 M | 12 | 768 | $1.693 \\times 10^{8}$ | $2.613 \\times 10^{8}$ |\n| 430 M | 24 | 1024 | $4.304 \\times 10^{8}$ | $7.573 \\times 10^{8}$ |\n| 1.5 B | 24 | 2048 | $1.515 \\times 10^{9}$ | $2.823 \\times 10^{9}$ |\n| 3 B | 32 | 2560 | $2.985 \\times 10^{9}$ | $5.710 \\times 10^{9}$ |\n| 7B | 32 | 4096 | $7.393 \\times 10^{9}$ | $1.437 \\times 10^{10}$ |\n| 14B | 40 | 5120 | $1.415 \\times 10^{10}$ | $2.778 \\times 10^{10}$ |\n\nTable 2: RWKV model architectures and FLOP counts. Further details of these hyperparameters are elaborated upon in Appendix G. The number of parameters for each model is computed using the formula: \\# parameters = $2 V D+13 D^{2} L+D(11 L+4)$ where $V=50277$ is the vocabulary size, $D$ represents the Model Dimension and $L$ corresponds to the number of layers. FLOPs is for a forward pass for one token. It was calculated as $2\\left(2 V D+13 D^{2} L\\right)$, which is the twice (add and multiply) the number of parameters in linear layers. The backwards pass FLOPs can be approximated as twice that of the forward pass, giving a total of $6\\left(2 V D+13 D^{2} L\\right)$ FLOP per token. Notably, this matches the standard formula for FLOP calculations in transformers Kaplan et al. (2020): FLOP $=6 \\cdot[\\#$ tokens $] \\cdot[\\#$ parameters $]$. ### 4.1 Additional Training Details\n\nFor training, we use the standard Adam optimizer without weight decay, use bfloat 16 precision, and train with a context length of 1024 tokens. Further details on hyperparameters are in Appendix G. Diverting from standard practice for transformers, we apply exponential decay to our learning rate. We also incorporate the auxiliary loss introduced by PaLM (Chowdhery et al., 2022), supplementing the standard cross-entropy loss function. This auxiliary loss encourages the softmax normalizer to approximate zero closely. As for the learning rate schedule, it remains constant for the initial iterations, and subsequently decays exponentially. ### 4.2 Scaling Laws\n\nScaling laws (Kaplan et al., 2020; Henighan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023) in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors. These factors can include the model size $(N)$, dataset size $(D)$, or the optimally allocated compute budget ( $C_{\\mathrm{min}}$ ). Scaling laws are important for two primary reasons: they allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation (Black et al., 2022; Le Scao et al., 2022) and the contexts in which they fail provides rich feedback on important areas for future research (Wei et al., 2022a; Biderman et al., 2023a). Previous work on scaling laws for RNNs has claimed that LSTMs do not strictly follow the same log-log linear scaling that transformers do (Kaplan et al., 2020). We train 45 RWKV models for a variety of pairs (dataset, parameters) and find that RWKV does follow the same general form of the scaling law that is well established for transformers. Figure 4 shows our results for loss as a function of compute, with the linear fit to the Pareto optimal points holding an $r^{2}$ value of 0.994 . Even when we extrapolate our curve an additional order of magnitude (blue), we find an extremely good fit with an $r^{2}$ of 0.875 . ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-06.jpg?height=555&width=763&top_left_y=1704&top_left_x=241)\n\nFigure 4: Scaling laws curves for RWKV models\n\n## 5 Evaluations\n\nHaving demonstrated the scalability of RWKV models in the previous section, we now turn our attention to their competitiveness with traditional transformers. We focus on two questions:\nCompetitiveness Is RWKV competitive against quadratic transformer architectures with the same amount of compute? Long Context Does increasing the context length of RWKV yield better language modeling loss when RWKV models are trained for context lengths that most open-sourced quadratic transformers cannot efficiently process? ### 5.1 NLP Evaluations\n\nTo demonstrate that RWKV is competitive with traditional transformers at NLP tasks, we compare with similarly sized models trained for a similar number of tokens (Pythia (Biderman et al., 2023b), OPT (Zhang et al., 2022) and BLOOM (Scao et al., 2022)). All RWKV models were trained for one epoch on the Pile (330B tokens), which is close but not identical to the amount of tokens the Pythia, OPT, and BLOOM models were trained for. Consequently, we compare our models on a FLOPmatched basis. We avoid comparing with model trained in the Chinchilla-optimal regime (Hoffmann et al., 2022) or the overtrained regime (Touvron et al., 2023) to ensure the most equitable comparison. We report results on ARC (both Easy and Challenge) (Clark et al., 2018), BoolQ (Clark et al., 2019), COPA (Roemmele et al., 2018), HeadQA (Vilares and G\u00f3mez-Rodr\u00edguez, 2019), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), ReCoRD (Zhang et al., 2018), SciQ (Johannes Welbl Nelson F.\n```\n\n\n\n---\n## Found 12 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: Empirical evaluation GLA, RWKV implementation challenges, Linear attention benchmarks\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing\n\n*From Search Query: Empirical evaluation GLA*\n\n*Abbas Ghaddar, Yimeng Wu, Sunyam Bagga, Ahmad Rashid, Khalil Bibi, Mehdi Rezagholizadeh, Chao Xing, Yasheng Wang, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Xin Jiang, Qun Liu, P. Langlais*\n\n**TL;DR:** This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields and releases three new Arabic BERT-style models, and achieves a new state-of-the-art performance on discriminative and generative ArabicNLU andNLG tasks.\n\n**Abstract:** There is a growing body of work in recent years to develop pre-trained language models (PLMs) for the Arabic language. This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields. First, existing Arabic PLMs are not well-explored and their pre-training can be improved significantly using a more methodical approach. Second, there is a lack of systematic and reproducible evaluation of these models in the literature. We revisit both the pre-training and evaluation of Arabic PLMs. In terms of pre-training, we explore the impact of the quality of the pretraining data, the size of the model, and the incorporation of character-level information on Arabic PLM. As a result, we release three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and two T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a comprehensive empirical study to systematically evaluate the performance of existing state-of-the-art models on ALUE, a leaderboard-powered benchmark for Arabic NLU tasks, and on a subset of the Arabic generative tasks. We show that our models significantly outperform existing Arabic PLMs and achieve a new state-of-the-art performance on discriminative and generative Arabic NLU and NLG tasks. Our models and source code to reproduce results will be made available upon acceptance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Gated Feedback Recurrent Neural Networks\n\n*From Search Query: Empirical evaluation GLA*\n\n*Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, Yoshua Bengio*\n\n**TL;DR:** The empirical evaluation of different RNN units revealed that the proposed gated-feedback RNN outperforms the conventional approaches to build deep stacked RNNs in the tasks of character-level language modeling and Python program evaluation.\n\n**Abstract:** In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2015\n\n**Citations:** 799  (*Influential: 82*)\n\n#### 3. Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs\n\n*From Search Query: Empirical evaluation GLA*\n\n*Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi*\n\n**TL;DR:** This study defines a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency and believes it can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.\n\n**Abstract:** Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 225  (*Influential: 34*)\n\n#### 4. A path-norm toolkit for modern networks: consequences, promises and challenges\n\n*From Search Query: RWKV implementation challenges*\n\n*Antoine Gonon, Nicolas Brisebarre, E. Riccietti, R\u00e9mi Gribonval*\n\n**Abstract:** This work introduces the first toolkit around path-norms that fully encompasses general DAG ReLU networks with biases, skip connections and any operation based on the extraction of order statistics: max pooling, GroupSort etc. This toolkit notably allows us to establish generalization bounds for modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on layered fully-connected networks compared to the product of operator norms, another complexity measure most commonly used. The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. Video Question Answering: Datasets, Algorithms and Challenges\n\n*From Search Query: RWKV implementation challenges*\n\n*Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Wei Deng, Tat-seng Chua*\n\n**TL;DR:** This survey aims to sort out the recent advances in video question answering (VideoQA) and point towards future directions, including those mainly designed for Factoid QA and those targeted at explicit relation and logic inference.\n\n**Abstract:** This survey aims to sort out the recent advances in video question answering (VideoQA) and point towards future directions. We firstly categorize the datasets into 1) normal VideoQA, multi-modal VideoQA and knowledge-based VideoQA, according to the modalities invoked in the question-answer pairs, or 2) factoid VideoQA and inference VideoQA, according to the technical challenges in comprehending the questions and deriving the correct answers. We then summarize the VideoQA techniques, including those mainly designed for Factoid QA (e.g., the early spatio-temporal attention-based methods and the recently Transformer-based ones) and those targeted at explicit relation and logic inference (e.g., neural modular networks, neural symbolic methods, and graph-structured methods). Aside from the backbone techniques, we delve into the specific models and find out some common and useful insights either for video modeling, question answering, or for cross-modal correspondence learning. Finally, we point out the research trend of studying beyond factoid VideoQA to inference VideoQA, as well as towards the robustness and interpretability. Additionally, we maintain a repository, https://github.com/VRU-NExT/VideoQA, to keep trace of the latest VideoQA papers, datasets, and their open-source implementations if available. With these efforts, we strongly hope this survey could shed light on the follow-up VideoQA research.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 70  (*Influential: 3*)\n\n#### 6. CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges\n\n*From Search Query: RWKV implementation challenges*\n\n*Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin*\n\n**TL;DR:** This work presents CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation and demonstrates CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.\n\n**Abstract:** Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. Real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. To fill this gap, our research pivots towards evaluating LLMs in a more realistic setting -- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. We implement four agent strategies to optimize these tools' usage. Our experiments on CodeAgentBench show that CodeAgent enhances LLM performance significantly, with improvements ranging from 18.1\\% to 250\\%. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 30  (*Influential: 4*)\n\n#### 7. Linear Log-Normal Attention with Unbiased Concentration\n\n*From Search Query: Linear attention benchmarks*\n\n*Yury Nahshan, Dor-Joseph Kampeas, E. Haleva*\n\n**TL;DR:** This work studies the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability and proposes a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention.\n\n**Abstract:** Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. Luna: Linear Unified Nested Attention\n\n*From Search Query: Linear attention benchmarks*\n\n*Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, Luke Zettlemoyer*\n\n**TL;DR:** Luna is proposed, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear time and space complexity.\n\n**Abstract:** The quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 107  (*Influential: 18*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Adaptive Pyramid Context Network for Semantic Segmentation\n\n*From Search Query: Empirical evaluation GLA*\n\n* Yu Qiao,  Yali Wang,  Lei Zhou,  Zhongying Deng, Junjun He*\n\n**Abstract:** Recent studies witnessed that context features can significantly improve the performance of deep semantic segmentation networks. Current context based segmentation methods differ with each other in how to construct context features and perform differently in practice. This paper firstly introduces three desirable properties of context features in segmentation task. Specially, we find that Global-guided Local Affinity (GLA) can play a vital role in constructing effective context features, while this property has been largely ignored in previous works. Based on this analysis, this paper proposes Adaptive Pyramid Context Network (APCNet) for semantic segmentation. APCNet adaptively constructs multi-scale contextual representations with multiple well-designed Adaptive Context Modules (ACMs). Specifically, each ACM leverages a global image representation as a guidance to estimate the local affinity coefficients for each sub-region, and then calculates a context vector with these affinities. We empirically evaluate our APCNet on three semantic segmentation and scene parsing datasets, including PASCAL VOC 2012, Pascal-Context, and ADE20K dataset.  Experimental results show that APCNet achieves state-of-the-art performance on all three benchmarks, and obtains a new record 84.2% on PASCAL VOC 2012 test set without MS COCO pre-trained and any post-processing.\r\n\n**Proceeding:** cvpr-2019-6\n\n**Published:** 2019-06-01\n\n\n\n#### 2. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks\n\n*From Search Query: RWKV implementation challenges*\n\n*Guoqi Li, Jason K. Eshraghian, Qihang Zhao, Rui-Jie Zhu*\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n**Published:** 2023-02-27\n\n\n\n#### 3. Visual Attention Network\n\n*From Search Query: Linear attention benchmarks*\n\n*Shi-Min Hu, Ming-Ming Cheng, Zheng-Ning Liu, Cheng-Ze Lu, Meng-Hao Guo*\n\n**Abstract:** While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN surpasses similar size vision transformers(ViTs) and convolutional neural networks(CNNs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. Code is available at https://github.com/Visual-Attention-Network.\n\n**Published:** 2022-02-20\n\n\n\n#### 4. Mega: Moving Average Equipped Gated Attention\n\n*From Search Query: Linear attention benchmarks*\n\n*Luke Zettlemoyer, Jonathan May, Graham Neubig, Liangke Gui, Junxian He, Xiang Kong, Chunting Zhou, Xuezhe Ma*\n\n**Abstract:** The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.\n\n**Published:** 2022-09-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model incorporating Gated Linear Attention (GLA) and RWKV (Revolutionary Weighted Key-Value) architectures, here are some key insights and findings from the provided sources and analysis:\n\n## Empirical Performance of GLA and RWKV\n\n- **GLA Performance**: GLA has been shown to perform competitively against other strong baselines, including LLaMA-architecture Transformers, RetNet, and Mamba, in moderate-scale language modeling experiments. This competitive performance is highlighted in the context of hardware-efficient training and linear-time inference.\n\n- **RWKV Performance**: RWKV is effective in capturing long-range dependencies while maintaining memory and computational efficiency. The architecture incorporates retrospecting abilities to absorb information effectively, which enhances its performance in sequence modeling tasks.\n\n## Implementation Challenges\n\n- **Linear Attention Variants**: Implementing GLA and other linear attention variants like Linear Attention (LA) and Retention Network (RetNet) involves managing the complexity of state transitions. For example, these models often rely on rank-1 or constant outputs to reduce computational complexity, which can be challenging to optimize without losing information.\n\n- **State Size and Selectivity**: Models like Mamba, which are related to RWKV, face challenges due to their limited recurrent state size and constrained selectivity mechanisms. These limitations can lead to local pattern shortcuts, hindering the model's ability to handle distributed key information. Introducing global selection mechanisms can mitigate these issues.\n\n## Benchmarking and Comparisons\n\n- **Efficiency and Scalability**: Both GLA and RWKV offer linear complexity and constant memory usage, making them more efficient than traditional attention-based models for long-context tasks. Mamba, for instance, achieves fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, which is a significant advantage.\n\n- **Expressiveness and Adaptability**: The gating mechanisms in GLA and RWKV enhance expressiveness by dynamically controlling information flow. This adaptability is crucial for handling diverse sequence modeling tasks. Comparisons with other models like Mamba and traditional Transformers show that these linear attention mechanisms can match or surpass the performance of state-of-the-art models in various benchmarks.\n\n## Theoretical Justification and Online Learning Perspective\n\n- **Online Learning Objectives**: The design of these models can be streamlined through an online learning perspective, where the state-space models are treated as meta-modules for specific online learning problems. This approach helps in formulating precise online learning objectives and deriving state transition rules from optimizing these objectives, which justifies the use of linear attention variants.\n\n## Practical Considerations\n\n- **Global Selection Mechanism**: To address the limitations of local pattern shortcuts in models like Mamba, introducing an input-dependent global gating module can significantly improve performance on complex tasks. This addition can be made with a relatively small increase in parameters (e.g., 4M additional parameters for a 130M-sized Mamba model), leading to substantial performance improvements.\n\nBy considering these points, researchers can design an autoregressive language model that leverages the strengths of GLA and RWKV, ensuring high efficiency, scalability, and expressiveness while addressing the challenges associated with these architectures.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for **FastTTTLinear** integrates Gated Linear Attention (GLA) and RWKV architecture concepts into the TTTLinear GAU to enhance computational efficiency and scalability. The search results provide insights into the current state of research on GLA, RWKV, and related linear attention mechanisms.\n\n### Useful References with Excerpts\n\n1. **Gated Linear Attention Transformers with Hardware-Efficient Training**\n   - \"The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\"\n   - This paper discusses the competitive performance of GLA in comparison to other models, highlighting its efficiency and scalability.\n\n2. **RRWKV: Capturing Long-range Dependencies in RWKV**\n   - \"The paper devises the Retrospected Receptance Weighted Key Value architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\"\n   - This work highlights the RWKV architecture's ability to handle long-range dependencies efficiently.\n\n3. **Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence**\n   - \"Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs.\"\n   - This paper presents advancements in RWKV, focusing on expressivity and efficiency.\n\n4. **A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models**\n   - \"Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties.\"\n   - This paper provides a unified view of models like Mamba and RWKV, emphasizing their efficient scaling properties.\n\n5. **Mamba: Linear-Time Sequence Modeling with Selective State Spaces**\n   - \"Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.\"\n   - This paper discusses Mamba's linear scaling and efficiency, which are relevant to the proposal's goals.\n\n### Key Insights and Detailed Analysis\n\n- **Efficiency and Scalability**: The proposal's focus on linear complexity aligns with current research trends in linear attention mechanisms like GLA and RWKV. These mechanisms are designed to handle long sequences efficiently, which is a key advantage over traditional attention mechanisms.\n\n- **Expressiveness and Adaptability**: GLA's data-dependent gating enhances expressiveness, allowing the model to dynamically control information flow. This is similar to the gating mechanisms in other models like Mamba and RWKV, which also focus on adaptability and expressiveness.\n\n- **Comparison with Existing Models**: The proposal should be compared with models like Mamba and RWKV to assess its relative advantages. The search results indicate that these models have been successful in achieving efficient sequence modeling with linear complexity.\n\n- **Theoretical Justification**: The theoretical framework for using GLA and RWKV concepts is supported by the idea of solving online learning objectives, which justifies the use of linear attention variants.\n\n### Future Search Plan\n\n1. **Explore Empirical Results**: Search for empirical studies that evaluate the performance of GLA and RWKV in real-world scenarios to validate the proposal's claims.\n\n2. **Investigate Implementation Challenges**: Look for papers or reports that discuss the practical challenges of implementing GLA and RWKV, particularly in terms of integration and stability.\n\n3. **Benchmarking and Comparisons**: Search for benchmarks that compare GLA and RWKV with other models to understand their strengths and weaknesses.\n\n### References\n\n- Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*. arXiv.org.\n- Wang, L. (2023). *RRWKV: Capturing Long-range Dependencies in RWKV*. arXiv.org.\n- Peng, B., et al. (2024). *Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence*. arXiv.org.\n- Zimerman, I., Ali, A., & Wolf, L. (2024). *A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models*. arXiv.org.\n- Gu, A., & Dao, T. (2023). *Mamba: Linear-Time Sequence Modeling with Selective State Spaces*. arXiv.org."
        },
        {
            "ready": false,
            "query": "GLA implementation details, RWKV benchmarks, Linear attention theory",
            "detail": "Implementation challenges and solutions for GLA and RWKV",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nImplementation challenges and solutions for GLA and RWKV\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. Reinventing RNNs for the Transformer Era (Avg. Score: 0.65)\n\n*Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 251  (*Influential: 27*)\n\n**TL;DR:** This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Abstract:** Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n##### *Relevant Chunk: No. 5/48 (Score: 0.92)*\n\n```\nsoftmax operation improving numerical stability and mitigating vanishing gradients (for rigorous proof, see Appendix H). It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization (Ba et al., 2016) incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues. These design elements not only enhance the training dynamics of deep neural networks but also facilitate the stacking of multiple layers, leading to superior performance over conventional RNN models by capturing complex patterns across different levels of abstraction (see also Appendix I). ### 3.1.1 Token Shift\n\nIn this architecture, all linear projection vectors ( $R, K, V$ in time-mixing, and $R^{\\prime}, K^{\\prime}$ in channelmixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift. The vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n\n$$\n\\begin{aligned}\nr_{t} & =W_{r} \\cdot\\left(\\mu_{r} \\odot x_{t}+\\left(1-\\mu_{r}\\right) \\odot x_{t-1}\\right) \\\\\nk_{t} & =W_{k} \\cdot\\left(\\mu_{k} \\odot x_{t}+\\left(1-\\mu_{k}\\right) \\odot x_{t-1}\\right) \\\\\nv_{t} & =W_{v} \\cdot\\left(\\mu_{v} \\odot x_{t}+\\left(1-\\mu_{v}\\right) \\odot x_{t-1}\\right)\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-04.jpg?height=792&width=764&top_left_y=229&top_left_x=246)\n\nFigure 3: RWKV architecture for language modeling. as are the channel-mixing inputs:\n\n$$\n\\begin{aligned}\nr_{t}^{\\prime} & =W_{r}^{\\prime} \\cdot\\left(\\mu_{r}^{\\prime} \\odot x_{t}+\\left(1-\\mu_{r}^{\\prime}\\right) \\odot x_{t-1}\\right) \\\\\nk_{t}^{\\prime} & =W_{k}^{\\prime} \\cdot\\left(\\mu_{k}^{\\prime} \\odot x_{t}+\\left(1-\\mu_{k}^{\\prime}\\right) \\odot x_{t-1}\\right)\n\\end{aligned}\n$$\n\nThe token shift is implemented as a simple offset in the temporal dimension at each block using the PyTorch (Paszke et al., 2019) library as nn. ZeroPad2d ( $(0,0,1,-1))$. ### 3.1.2 WKV Operator\n\nThe computation of the $W K V$ operator in our model parallels the method used in Attention Free Transformer (AFT) (Zhai et al., 2021). However, unlike AFT where $W$ is a pairwise matrix, our model treats $W$ as a channel-wise vector that is modified by relative position. In our model, this recurrent behavior is defined by the time-dependent update of the $W K V$ vectors, formalized in the following equation:\n\n$$\nw k v_{t}=\\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}} \\odot v_{i}+e^{u+k_{t}} \\odot v_{t}}{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}}+e^{u+k_{t}}}\n$$\n\nTo circumvent any potential degradation of $W$, we introduce a vector $U$ that separately attends to the current token. More information about this can be found in Appendix I. ### 3.1.3 Output Gating\n\nOutput gating is implemented in both time-mixing and channel-mixing blocks using the sigmoid of the receptance, $\\sigma(r)$. The output vector ot post the $W K V$ operator is given by:\n\n$$\no_{t}=W_{o} \\cdot\\left(\\sigma\\left(r_{t}\\right) \\odot w k v_{t}\\right)\n$$\n\nIn the channel-mixing block, a similar operation is performed:\n\n$$\no_{t}^{\\prime}=\\sigma\\left(r_{t}^{\\prime}\\right) \\odot\\left(W_{v}^{\\prime} \\cdot \\max \\left(k_{t}^{\\prime}, 0\\right)^{2}\\right)\n$$\n\nwhere we adopt the squared ReLU activation function (So et al., 2021). ### 3.2 Transformer-like Training\n\nRWKV can be efficiently parallelized using a technique called time-parallel mode, reminiscent of Transformers. The time complexity of processing a batch of sequences in a single layer is $O\\left(B T d^{2}\\right)$, primarily consisting of matrix multiplications $W_{\\lambda}$, where $\\lambda \\in\\{r, k, v, o\\}$ (assuming $B$ sequences, $T$ maximum tokens, and $d$ channels). In contrast, updating attention scores $w k v_{t}$ involves a serial scan (see Appendix D for more detail) and has complexity $O(B T d)$. The matrix multiplications can be parallelized similarly to $W_{\\lambda}$, where $\\lambda \\in\\{Q, K, V, O\\}$ in conventional Transformers. The element-wise $W K V$ computation is time-dependent but can be readily parallelized along the other two dimensions (Lei et al., 2018) ${ }^{3}$. ### 3.3 RNN-like Inference\n\nRecurrent networks commonly utilize the output at state $t$ as input at state $t+1$. This usage is also observed in the autoregressive decoding inference of language models, where each token must be computed before being passed to the next step. RWKV takes advantage of this RNN-like structure, known as time-sequential mode. In this context, RWKV can be conveniently formulated recursively for decoding during inference, as demonstrated in Appendix D. ### 3.4 Additional Optimizations\n\nCustom Kernels To address inefficiencies in the $W K V$ computation arising from the sequential nature of the task when using standard deep learning frameworks, we have developed a custom CUDA\n\n[^2]kernel. This kernel enables the execution of a single compute kernel on training accelerators, while all other parts of the model, such as matrix multiplications and point-wise operations, are already inherently parallelizable and efficient. Small Init Embedding During the initial stage of training a transformer model (Vaswani et al., 2017), we observe that the embedding matrix undergoes slow changes, presenting a challenge for the model to move away from its initial noisy embedding state. To address this issue, we propose an approach that involves initializing the embedding matrix with small values and subsequently applying an additional LayerNorm operation. This accelerates and stabilizes the training process, allowing for the training of deep architectures with post-LN components. The effectiveness of this approach is demonstrated in Figure 9, illustrating improved convergence by enabling the model to quickly transition away from the initially small embedding. This is achieved through small changes occurring in a single step, which subsequently lead to substantial alterations in directions and further notable changes after the LayerNorm operation. Custom Initialization Building on principles from previous works (He et al., 2016; Jumper et al., 2021), we adopt an initialization strategy where parameters are set to values resembling an identity mapping while breaking symmetry to establish a clear information flow. The majority of weights are initialized to zero, and linear layers do not employ biases. Detailed formulas are given in Appendix E. We observe that the choice of initialization plays a crucial role in both the speed and quality of convergence (refer to Appendix F for further details). ### 3.5 Implementation\n\nRWKV is implemented using the PyTorch Deep Learning Library (Paszke et al., 2019). We integrate additional optimization strategies inspired by DeepSpeed (Rasley et al., 2020) into the system, improving its efficiency and scalability. The model begins with an embedding layer, as detailed in Section 3.4. Following this are several identical residual blocks arranged sequentially. These are depicted in Figures 2 and 3 and adheres to the principles outlined in Section 3.1.1. After the last block, a simple output projection head, consisting of a LayerNorm (Ba et al., 2016) and a linear projection, is employed for logits generation for next-token prediction and computation of the cross-entropy loss during training. ## 4 Trained Models and Computing Costs\n\nTo demonstrate the scalability of RWKV, we train six models ranging from 169 million to 14 billion parameters as shown in Table 2. All models are trained for one epoch ( 330 billion tokens) on the Pile (Gao et al., 2020; Biderman et al., 2022). | Name | Layers | Model Dimension | Parameters | FLOP per token |\n| :---: | :---: | :---: | :---: | :---: |\n| 169 M | 12 | 768 | $1.693 \\times 10^{8}$ | $2.613 \\times 10^{8}$ |\n| 430 M | 24 | 1024 | $4.304 \\times 10^{8}$ | $7.573 \\times 10^{8}$ |\n| 1.5 B | 24 | 2048 | $1.515 \\times 10^{9}$ | $2.823 \\times 10^{9}$ |\n| 3 B | 32 | 2560 | $2.985 \\times 10^{9}$ | $5.710 \\times 10^{9}$ |\n| 7B | 32 | 4096 | $7.393 \\times 10^{9}$ | $1.437 \\times 10^{10}$ |\n| 14B | 40 | 5120 | $1.415 \\times 10^{10}$ | $2.778 \\times 10^{10}$ |\n\nTable 2: RWKV model architectures and FLOP counts. Further details of these hyperparameters are elaborated upon in Appendix G. The number of parameters for each model is computed using the formula: \\# parameters = $2 V D+13 D^{2} L+D(11 L+4)$ where $V=50277$ is the vocabulary size, $D$ represents the Model Dimension and $L$ corresponds to the number of layers. FLOPs is for a forward pass for one token. It was calculated as $2\\left(2 V D+13 D^{2} L\\right)$, which is the twice (add and multiply) the number of parameters in linear layers. The backwards pass FLOPs can be approximated as twice that of the forward pass, giving a total of $6\\left(2 V D+13 D^{2} L\\right)$ FLOP per token. Notably, this matches the standard formula for FLOP calculations in transformers Kaplan et al. (2020): FLOP $=6 \\cdot[\\#$ tokens $] \\cdot[\\#$ parameters $]$. ### 4.1 Additional Training Details\n\nFor training, we use the standard Adam optimizer without weight decay, use bfloat 16 precision, and train with a context length of 1024 tokens. Further details on hyperparameters are in Appendix G. Diverting from standard practice for transformers, we apply exponential decay to our learning rate. We also incorporate the auxiliary loss introduced by PaLM (Chowdhery et al., 2022), supplementing the standard cross-entropy loss function. This auxiliary loss encourages the softmax normalizer to approximate zero closely. As for the learning rate schedule, it remains constant for the initial iterations, and subsequently decays exponentially. ### 4.2 Scaling Laws\n\nScaling laws (Kaplan et al., 2020; Henighan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023) in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors. These factors can include the model size $(N)$, dataset size $(D)$, or the optimally allocated compute budget ( $C_{\\mathrm{min}}$ ). Scaling laws are important for two primary reasons: they allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation (Black et al., 2022; Le Scao et al., 2022) and the contexts in which they fail provides rich feedback on important areas for future research (Wei et al., 2022a; Biderman et al., 2023a). Previous work on scaling laws for RNNs has claimed that LSTMs do not strictly follow the same log-log linear scaling that transformers do (Kaplan et al., 2020). We train 45 RWKV models for a variety of pairs (dataset, parameters) and find that RWKV does follow the same general form of the scaling law that is well established for transformers. Figure 4 shows our results for loss as a function of compute, with the linear fit to the Pareto optimal points holding an $r^{2}$ value of 0.994 . Even when we extrapolate our curve an additional order of magnitude (blue), we find an extremely good fit with an $r^{2}$ of 0.875 . ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-06.jpg?height=555&width=763&top_left_y=1704&top_left_x=241)\n\nFigure 4: Scaling laws curves for RWKV models\n\n## 5 Evaluations\n\nHaving demonstrated the scalability of RWKV models in the previous section, we now turn our attention to their competitiveness with traditional transformers. We focus on two questions:\nCompetitiveness Is RWKV competitive against quadratic transformer architectures with the same amount of compute? Long Context Does increasing the context length of RWKV yield better language modeling loss when RWKV models are trained for context lengths that most open-sourced quadratic transformers cannot efficiently process? ### 5.1 NLP Evaluations\n\nTo demonstrate that RWKV is competitive with traditional transformers at NLP tasks, we compare with similarly sized models trained for a similar number of tokens (Pythia (Biderman et al., 2023b), OPT (Zhang et al., 2022) and BLOOM (Scao et al., 2022)). All RWKV models were trained for one epoch on the Pile (330B tokens), which is close but not identical to the amount of tokens the Pythia, OPT, and BLOOM models were trained for. Consequently, we compare our models on a FLOPmatched basis. We avoid comparing with model trained in the Chinchilla-optimal regime (Hoffmann et al., 2022) or the overtrained regime (Touvron et al., 2023) to ensure the most equitable comparison. We report results on ARC (both Easy and Challenge) (Clark et al., 2018), BoolQ (Clark et al., 2019), COPA (Roemmele et al., 2018), HeadQA (Vilares and G\u00f3mez-Rodr\u00edguez, 2019), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), ReCoRD (Zhang et al., 2018), SciQ (Johannes Welbl Nelson F.\n```\n\n##### *Relevant Chunk: No. 4/48 (Score: 0.83)*\n\n```\nof British Columbia ${ }^{28}$ U. of C., Santa Cruz ${ }^{29}$ U. of Electronic Science and Technology of China\n\n\n#### Abstract\n\nTransformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks. ${ }^{1}$\n\n## 1 Introduction\n\nDeep learning has greatly advanced artificial intelligence, impacting a range of scientific and industrial uses. These often involve complex sequential data\n\n[^0]processing tasks such as natural language understanding, conversational AI, time-series analysis, and indirectly sequential formats like images and graphs (Brown et al., 2020; Ismail Fawaz et al., 2019; Wu et al., 2020; Albalak et al., 2022). Predominant among these techniques include RNNs and Transformers (Vaswani et al., 2017), each with specific benefits and drawbacks. RNNs require less memory, particularly for handling long sequences. However, they suffer from the vanishing gradient problem and non-parallelizability in the time dimension during training, limiting their scalability (Hochreiter, 1998; Le and Zuidema, 2016). ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-01.jpg?height=584&width=746&top_left_y=1647&top_left_x=1063)\n\nFigure 1: Average performance of RWKV models compared to transformers across twelve NLP tasks. For further details, see section 5 . Transformers emerged as a powerful alternative, adept at managing local and long-range dependencies and supporting parallelized training (Tay et al., 2022). Models such as GPT-3 (Brown et al., 2020), ChatGPT (OpenAI, 2022; Koco\u0144 et al., 2023),\n\n| Model | Time | Space |\n| :--- | :---: | :---: |\n| Transformer | $O\\left(T^{2} d\\right)$ | $O\\left(T^{2}+T d\\right)$ |\n| Reformer | $O(T \\log T d)$ | $O(T \\log T+T d)$ |\n| Performer | $O\\left(T d^{2} \\log d\\right)$ | $O\\left(T d \\log d+d^{2} \\log d\\right)$ |\n| Linear Transformers | $O\\left(T d^{2}\\right)$ | $O\\left(T d+d^{2}\\right)$ |\n| AFT-full | $O\\left(T^{2} d\\right)$ | $O(T d)$ |\n| AFT-local | $O(T s d)$ | $O(T d)$ |\n| MEGA | $O(c T d)$ | $O(c d)$ |\n| RWKV (ours) | $O($ Td $)$ | $O(\\mathbf{d})$ |\n\nTable 1: Inference complexity comparison with different Transformers. Here $T$ denotes the sequence length, $d$ the feature dimension, $c$ is MEGA's chunk size of quadratic attention, and $s$ is the size of a local window for AFT. LLaMA (Touvron et al., 2023), and Chinchilla (Hoffmann et al., 2022) showcase the potential of Transformers in NLP. However, the self-attention mechanism's quadratic complexity makes it computationally and memory intensive for tasks involving long sequences and constrained resources. This has stimulated research to enhance Transformers' scalability, sometimes sacrificing some of their effectiveness (Wang et al., 2020; Zaheer et al., 2020; Dao et al., 2022a). To tackle these challenges, we introduce the Receptance Weighted Key Value (RWKV) model, combining the strengths of RNNs and Transformers while circumventing key drawbacks. RWKV alleviates memory bottleneck and quadratic scaling associated with Transformers (Katharopoulos et al., 2020) with efficient linear scaling, while maintaining the expressive properties of the Transformer, such as parallelized training and robust scalability. RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity; see Table 1. The motivation behind RWKV is to balance computational efficiency with expressive capacity in neural networks. It offers a solution for handling large-scale models with billions of parameters, exhibiting competitive performance at a reduced computational cost. Experiments suggest RWKV addresses scaling and deployment challenges in AI, especially for sequential data processing, pointing towards more sustainable and efficient AI models. Our contributions in this paper are as follows:\n\n- The introduction of RWKV, a novel architec- ture combining RNNs and Transformer advantages while mitigating their limitations. - Detailed experiments demonstrating RWKV's performance and efficiency on benchmark datasets for large-scale models. - The release of pretrained models, from 169 million to 14 billion parameters, trained on the Pile (Gao et al., 2020; Biderman et al., 2022). ${ }^{2}$\n\n\n## 2 Background\n\nHere we briefly review the fundamentals of RNNs and Transformers. ### 2.1 Recurrent Neural Networks (RNNs)\n\nPopular RNN architectures such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) are characterized by the following formulation (shown for LSTM, others can be reasoned similarly):\n\n$$\n\\begin{aligned}\nf_{t} & =\\sigma_{g}\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right) \\\\\ni_{t} & =\\sigma_{g}\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right) \\\\\no_{t} & =\\sigma_{g}\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right) \\\\\n\\tilde{c}_{t} & =\\sigma_{c}\\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right) \\\\\nc_{t} & =f_{t} \\odot c_{t-1}+i_{t} \\odot \\tilde{c}_{t} \\\\\nh_{t} & =o_{t} \\odot \\sigma_{h}\\left(c_{t}\\right)\n\\end{aligned}\n$$\n\nAlthough RNNs can be factored into two linear blocks ( $W$ and $U$ ) and an RNN-specific block (1)(6), as noted by Bradbury et al. (2017), the data dependency relying on previous time steps prohibits parallelizing these typical RNNs. ### 2.2 Transformers and AFT\n\nIntroduced by Vaswani et al. (2017), Transformers are a class of neural networks that have become the dominant architecture for several NLP tasks. Instead of operating on sequences step-by-step like RNNs, Transformers rely on attention mechanisms to capture relationships between all input and all output tokens:\n\n$$\n\\operatorname{Attn}(Q, K, V)=\\operatorname{softmax}\\left(Q K^{\\top}\\right) V\n$$\n\nwhere the multi-headness and scaling factor $\\frac{1}{\\sqrt{d_{k}}}$ is omitted for convenience. The core $Q K^{\\top}$ multiplication is an ensemble of pairwise attention scores\n\n[^1]between each token in a sequence, which can be decomposed as vector operations:\n$$\n\\operatorname{Attn}(Q, K, V)_{t}=\\frac{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}} \\odot v_{i}}{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}}}\n$$\n\nAFT (Zhai et al., 2021), alternately formulates\n\n$$\n\\operatorname{Attn}^{+}(W, K, V)_{t}=\\frac{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}} \\odot v_{i}}{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}}}\n$$\n\nwhere $\\left\\{w_{t, i}\\right\\} \\in R^{T \\times T}$ is the learned pair-wise position biases, and each $w_{t, i}$ is a scalar. Inspired by AFT, RWKV takes a similar approach. However, for simplicity, it modifies the interaction weights so that it can be transformed into an RNN. Each $w_{t, i}$ in RWKV is a channelwise time decay vector multiplied by the relative position and traced backward from current time as it decays:\n\n$$\nw_{t, i}=-(t-i) w\n$$\n\nwhere $w \\in\\left(R_{\\geq 0}\\right)^{d}$, with $d$ the number of channels. We require $w$ to be non-negative to ensure that $e^{w_{t, i}} \\leq 1$ and the per-channel weights decay backwards in time. ## 3 RWKV\n\nThe RWKV model architecture is defined by four fundamental elements that are intrinsic to the timemixing and channel-mixing blocks:\n\n- $R$ : The Receptance vector acts as the receiver of past information. - $W$ : The Weight signifies the positional weight decay vector, a trainable parameter within the model. - $K$ : The Key vector performs a role analogous to $K$ in traditional attention mechanisms. - $V$ : The Value vector functions similarly to $V$ in conventional attention processes. These core elements interact multiplicatively at each timestep, as depicted in Figure 2. ### 3.1 Architecture\n\nThe RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information. This model uses a unique attention-like score update process, which includes a time-dependent\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-03.jpg?height=829&width=718&top_left_y=248&top_left_x=1086)\n\nFigure 2: Elements within an RWKV block (left) and the complete RWKV residual block, equipped with a final head for language modeling (right).\n```\n\n##### *Relevant Chunk: No. 6/48 (Score: 0.43)*\n\n```\nLiu, 2017), and Winogrande (Zellers et al., 2020). Figure 1 shows the average results across all benchmarks. Some individual benchmarks are shown in Fig 5, with the rest in Appendix J. Additionally, we carried out comparative studies on RWKV and ChatGPT / GPT-4, see Appendix L. They revealed that RWKV is very sensitive to prompt engineering. When the prompts were adjusted (re-ordered) from the ones used for GPT to more suitable for RWKV, the performance (F1) increased even from $44.2 \\%$ to $74.8 \\%$. For sarcasm detection, RWKV outperformed ChatGPT, but was still slightly worse than the SOTA solution. ### 5.2 Extended Context Finetuning\n\nUnlike transformers, RNNs do not have a predefined sequences length when they are created. However in order to efficient make use of compute\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-07.jpg?height=939&width=1556&top_left_y=266&top_left_x=255)\n\nFigure 5: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. Additional plots can be found in Appendix J. we nevertheless need to preprocess the training data into contexts of the same length. We find that we are able to teach the model how to efficiently handle substantially larger batch sizes by finetuning with progressively increasing sequence length. Specifically, we first double the sequence length from 1024 to 2048 and finetune for 10B tokens from the original pretraining corpus, then we double again to 4096 for 100B tokens from the same corpus, and finally double to 8192 tokens for another 100B tokens from the same corpus. In Fig. 6 we show that increasing context length leads to lower test loss on the Pile, an indication that RWKV can make effective use of long contextual information. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-07.jpg?height=318&width=673&top_left_y=2145&top_left_x=286)\n\nFigure 6: RWKV shows decreasing mean test loss as a function of context length on the Pile (Gao et al., 2020)\n\n### 5.3 Long Context Benchmarks\n\nAdditionally, we evaluate our model's ability to handle very long sequences by comparing to stateof-the-art long sequence models on the LongRange Arena (LRA) benchmark (Tay et al., 2021). LRA is designed to assess the performance of models in handling lengthy context situations. It includes a collection of tasks with sequences ranging from 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images, and mathematical expressions. We apply RWKV on the LRA benchmark and the results are in Appendix J.2. The results show that RWKV performs second only to the S 4 model in five datasets. ## 6 Inference Experiments\n\nWe benchmark inference requirements according to size and family. Specifically, we evaluate text generation speed and memory requirements on typical compute platforms including CPU (x86) and GPU (NVIDIA A100 80 GB ). For all of our inference experiments we use float 32 precision and the HuggingFace Transformers (Wolf et al., 2020). We include all model parameters in the parameter count, including both embedding and non-embedding layers. Performance under different quantization setups is left to further work. See Appendix K for\nmore results. ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-08.jpg?height=481&width=629&top_left_y=319&top_left_x=308)\n\nFigure 7: Cumulative time on text generation for LLMs. Unlike transformers, RWKV exhibits linear scaling. ## 7 Future Work\n\nThere are several promising directions for future work on the RWKV architecture. Work can be done to increase model expressivity by enhancing the time-decay formulations and exploring initial model states while maintaining efficiency. The RWKV computational efficiency can be further improved by applying a parallel scan in the $w k v_{t}$ step to reduce the computational cost to $O(B \\log (T) d)$. The mechanisms used in RWKV can be applied to encoder-decoder architectures, potentially replacing the cross-attention mechanism. This could be applicable in seq2seq or multimodal settings, thereby enhancing efficiency during both training and inference. RWKV's state (or context) can be leveraged for interpretability, predictability in sequence data, and safety. Manipulating the hidden state could also guide behavior and allow greater customizability through prompt tuning. The RWKV architecture is not perfect, and can be improved via many aspects, such as modifying the formulae or implementing larger internal states. Larger states can enhance the model's memory to previous context and improve performance over various tasks. ## 8 Conclusions\n\nWe introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and longrange dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations. We benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs. RWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters. ## 9 Limitations\n\nWhile our proposed RWKV model has demonstrated promising results regarding training and memory efficiency during inference, some limitations should be acknowledged and addressed in future work. First, the linear attention of RWKV leads to significant efficiency gains but still, it may also limit the model's performance on tasks that require recalling minutiae information over very long contexts. This is due to the funneling of information through a single vector representation over many time steps, compared with the full information maintained by the quadratic attention of standard Transformers. In other words, the model's recurrent architecture inherently limits its ability to \"look back\" at previous tokens, as opposed to traditional self-attention mechanisms. While learned time decay helps prevent the loss of information, it is mechanistically limited compared to full selfattention. Another limitation of this work is the increased importance of prompt engineering in comparison to standard Transformer models. The linear attention mechanism used in RWKV limits the information from the prompt that will be carried over to the model's continuation. As a result, carefully designed prompts may be even more crucial for the model to perform well on tasks. The above RWKV property was confirmed by studies on prompt engineering presented in Appendix L. By changing the order of the information\npieces, we were even able to almost double the RWKV performance for some tasks. ## 10 Ethics Statement\n\nIn this paper, we present a novel architecture for sequential data processing and prove its effectiveness by building a series of LLMs trained on publicly released pretraining data (Gao et al., 2020; Biderman et al., 2022) and later fine-tuned on publicly available instructions (Taori et al., 2023; Chaudhary, 2023; Cheung, 2023; Anand et al., 2023; Anonymous, 2023; Yang, 2023; Ji et al., 2023a,b). As a novel architecture for sequential data, RWKV has the potential to improve sequencebased models across different applications ranging from natural language processing to biomedical data processing or climate modelling. Since the training code is released open source, RWKV contributes to the democratization of AI , levels the playing field, and empowers members of the Open Source community to inspect, study, and finetune RWKV in particular tasks. Moreover, it contributes to advancing the understanding of LLMs capabilities and limitations. A significant amount of work has been devoted to increasing the efficiency of RWKV training so as to minimize its cost and promote accessibility. As LLMs trained on public data, RWKV's lower inference cost compared to Transformer alternatives makes it more suitable for deployment in consumer and edge hardware, which is a step towards the democratization and distribution of LLMs to the general public, creating better privacy and ownership incentives. It also lowers the resource barrier to Chat assistants and text generation for small and/or underrepresented communities. PreTrained model weights for different sizes ranging from 0.1 B to 14B parameters trained on multiple languages are released to increase ease of adoption and allow for the study of emergent phenomena. On the other hand, with lower resource barriers, the spreading of AI-generated text might become more prevalent. Current RWKV LLMs may exhibit and/or reproduce biases and potentially harmful content present in the data used for training. Nonetheless, mitigation and finetuning strategies discussed for other, large Transformer models should be applicable to RWKV as well. ## Acknowledgements\n\nWe thank StabilityAI for the compute used to train our models and for technical support in development of RWKV. We also thank the members of the RWKV and EleutherAI Discord servers for their help and work on further extending the applicability of RWKV to different domains.\n```\n\n##### *Relevant Chunk: No. 41/48 (Score: 0.42)*\n\n```\n8, where the hidden states $h$ is the numerator-denominator tuple $(a, b)$. To avoid overflow in calculating $e^{k_{t}}$, a numerical trick is used in the official implementation. Noticing that $a_{1}=e^{k_{1}} \\odot v_{1}$ and $b_{1}=e^{k_{1}}$, we set $a_{1}^{\\prime}=$ $v_{1}, b_{1}^{\\prime}=1, p_{1}=k_{1}$, where $p_{t}$ stores the shared exponents of $a_{t}$ and $b_{t}$. Now the above recursion can be converted into a numerical safe version, for each time step $t>1$ :\n\n$$\n\\begin{aligned}\nq & :=\\max \\left(p_{t-1}, u+k_{t}\\right) \\\\\nw k v_{t} & =\\frac{e^{p_{t-1}-q} \\odot a_{t-1}^{\\prime}+e^{u+k_{t}-q} \\odot v_{t}}{e^{p_{t-1}-q} \\odot b_{t-1}^{\\prime}+e^{u+k_{t}-q}}\n\\end{aligned}\n$$\n\nThe update to $a_{t}^{\\prime}, b_{t}^{\\prime}$, and their shared exponent is also carried out in a similar fashion:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-17.jpg?height=344&width=507&top_left_y=819&top_left_x=1160)\n\nFigure 8: RWKV time-mixing block formulated as an RNN cell. Color codes: yellow $(\\mu)$ denotes the token shift, red (1) denotes the denominator, blue (2) denotes the numerator, and pink (3) denotes the fraction computations in 16. $h$ denotes the numerator-denominator tuple. $$\n\\begin{aligned}\nq^{\\prime} & :=\\max \\left(p_{t-1}-w, k_{t}\\right) \\\\\na_{t}^{\\prime} & =e^{p_{t-1}-w-q^{\\prime}} \\odot a_{t-1}^{\\prime}+e^{k_{t}-q^{\\prime}} \\odot v_{t} \\\\\nb_{t}^{\\prime} & =e^{p_{t-1}-w-q^{\\prime}} \\odot b_{t-1}^{\\prime}+e^{k_{t}-q^{\\prime}} \\\\\np_{t} & =q^{\\prime}\n\\end{aligned}\n$$\n\nThe RWKV model has an internal state that stores some previous information. In each layer, the internal state consists five parts, each of which is a vector with $D$ numbers, where $D$ is the model dimension. The five parts are:\n\n- The current input of the Time-mix block $x_{t}$;\n- The current input of the Channel-mix block $y_{t}$;\n- The numerator of the $W K V$ value $a_{t}^{\\prime}$, as defined in equation (26);\n- The denominator of the $W K V$ value $b_{t}^{\\prime}$, as defined in equation (27);\n- An auxiliary state $p_{t}$ in (28), which is used for $W K V$ computation to maintain numerical precision. Which yields a total size of $5 D L$ parameters. It is worth noting that in algebraic context with infinite precision, the helper state $p_{t}$ can be ignored, and the $W K V$ numerator and denominator can be computed directly using equations (21) and (22), reducing the size of the internal state to $4 D L$. ## E Parameter initializations\n\nWe describe the specific parameter initializations below and motivate the design choices. Parameters belonging to residual blocks are often adjusted by layer depth and total number of layers. Let \\# denote the vocabulary size, $s$ denote the embedding dimension, $d$ denote the hidden size (we use $d=4 s$ ), $L$ the number of layers, $l$ the layer index (from 0 to $L-1$ ), we use the following initializations:\n\n- Embeddings are initialized to $\\mathcal{U}\\left( \\pm 1 \\times 10^{-4}\\right)$ as explained in 3.4\n- For the time-mixing blocks $(11,12,13)$, initializations are $\\mu_{k_{i}}=\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}, \\mu_{v_{i}}=\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}+\\frac{0.3 l}{L-1}$ and $\\mu_{r_{i}}=\\frac{1}{2} \\cdot\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}$\n- For the channel-mixing blocks $(14,15), \\mu_{k_{i}}$ and $\\mu_{r_{i}}$ are initialized to $\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}$\n- $w_{i}(16)$, also known as \"time decay\", is initialized to $-5+8 \\cdot\\left(\\frac{i}{d-1}\\right)^{0.7+\\frac{1.3 l}{L-1}}$.\n```\n\n#### 2. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.57)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 13/64 (Score: 0.57)*\n\n```\nFurther optimization of our Finch CUDA implementation, including algorithmic improvements, are possible, and could lead to speed increases and greater parallelization. However, this optimization is left for future work. ## 10 Multimodal Experiments\n\nIn this section, we explore the capabilities of Eagle when extended to handle multimodal tasks, where the model processes and integrates textual inputs with inputs in a different domain. ### 10.1 RWKV Music Modelling\n\nTo investigate the Eagle architecture's applicability to music modeling, we use the Irishman ABC music sheet dataset (Wu et al., 2023) to train a new RWKV-5-Music model using the same hyperparameters as the existing RWKV-4-Music model. The loss of RWKV-5 is approximately $2 \\%$ lower than that of the previous generation model, and this improvement is primarily observed in the musical score part, indicating that RWKV-5 possesses stronger modeling and generalization capabilities than its predecessor. The model has a total of $L=24$ layers, with a dimension of $D=512$ and uses a byte-level tokenizer with $V=128$ tokens. The training context length is 1024 bytes. We use all 2,162 pieces of music in the validation set and calculate the loss for each position from the start. The loss is averaged across all pieces of music, then Gaussian smoothed over the position in the sequence. The figure 8 shows the loss as a function of position. Note that the first 30-100 bytes of the ABC format are the file header and control codes, followed by the musical scores. The loss of RWKV-5 is approximately $2 \\%$ lower than the previous generation model, and it is shown mainly in the musical score part, indicating that RWKV-5 has stronger modelling and generalization capabilities than its precedent model. ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-15.jpg?height=472&width=1045&top_left_y=1477&top_left_x=540)\n\nFigure 8: Music modelling loss over sequence position. ### 10.2 VisualRWKV\n\nVisualRWKV is the visual-enhanced version of the RWKV language model, enabling RWKV to handle various visual tasks. Our VisualRWKV follows a similar architecture to popular visionlanguage models (Liu et al., 2023a). We present the architecture in Figure 9. It consists of a vision encoder and a language model. Specifically, we use CLIP (Radford et al., 2021) as the vision encoder and Eagle 1.5B and 3B as the language model. We use LLaVA-1.5 dataset (Liu et al., 2023a). To adapt Eagle to this multimodal task, we employ a two-stage instruction-tuning process to enhance model performance. Initially, we conduct pre-training for feature alignment, during which only the projection layer is subjected to updates, while the rest of the model is kept in a frozen state. Following this, we move on to the fine-tuning end-to-end stage, where both the projection layer and the RWKV language model are fine-tuned, and the vision encoder\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-16.jpg?height=781&width=1288&top_left_y=325&top_left_x=402)\n\nFigure 9: VisualRWKV architecture overview. | Method | Vision Encoder | LLM | GQA ( $\\uparrow$ ) | ScienceQA-IMG ( $\\uparrow$ ) | Text-VQA ( $\\uparrow$ ) | POPE ( $\\uparrow$ ) |\n| :--- | :--- | :--- | :---: | :---: | :---: | :---: |\n| BLIP-2 (Li et al., 2023a) | EVA01-CLIP-G | Vicuna-13B | 41.0 | 61.0 | 42.5 |  |\n| BLIP-2 (Li et al., 2023a) | EVA01-CLIP-G | Flan-T5-11B | 44.6 | 64.5 | - |  |\n| InstructBLIP(Dai et al., 2023) | EVA01-CLIP-G | Vicuna-7B | 49.2 | 60.5 | - |  |\n| InstructBLIP(Dai et al., 2023) | EVA01-CLIP-G | Vicuna-13B | 49.5 | 63.1 | - |  |\n| IDEFICS-9B (IDEFICS, 2023) | OpenCLIP-H | LLaMA-7B | 38.4 | - | 50.1 |  |\n| IDEFICS-80B (IDEFICS, 2023) | OpenCLIP-H | LLaMA-65B | 45.2 | 25.9 | -9 |  |\n| TinyGPT-V (Yuan et al., 2023) | EVA01-CLIP-G | Phi-2 (2.7B) | 33.6 | - | 30.9 |  |\n| VisualRWKV | CLIP-L | Eagle-1.5B | 48.5 | - | - |  |\n| VisualRWKV | CLIP-L | Eagle-3B | 49.7 | 46.2 | - |  |\n\nTable 6: A comparison of VisualRWKV to other state-of-the-art Multimodal Large Language\nModels (MLLMs) across 4 distinct benchmarks. We evaluate these models on benchmarks: GQA(Hudson \\& Manning, 2019), ScienceQA-IMG(Lu et al., 2022), Text-VQA(Singh et al., 2019) and POPE(Li et al., 2023c). For POPE, the average F1-score across three distinct categories-random, popular, and adversarial-was computed using the validation set of the MSCOCO dataset. continue to be kept frozen. As shown in Table 6, we demonstrate that VisualRWKV's architecture is powerful for visual understanding and reasoning. With a smaller vision encoder CLIP-L ( 0.4 B ) and modest-sized LLMs of 1.5B and 3B, it achieves results comparable to the combination of CLIP-G (1.0B) and CLIP-H (1.0B) with larger LLMs of 7B and 13B. Moreover, in some benchmarks, it even outperforms larger models. ## 11 Conclusions\n\nIn this work, we introduced Eagle (RWKV-5) and Finch (RWKV-6), marking substantial progress in RNN-based language models by integrating multiheaded matrix-valued states and dynamic data-driven recurrence mechanisms. These models demonstrate exceptional performance on MQAR and diverse linguistic benchmarks, challenging the dominance of traditional Transformer architectures while retaining key RNN advantages. With models publicly available under the Apache 2.0 license and trained on an extensive multilingual corpus, our work not only advances the capabilities of language models but also emphasizes community accessibility and applicability across various domains. While acknowledging the computational and ethical challenges ahead,\nwe hope that Eagle and Finch's efficient new architecture and wide availability will help push the boundaries of language modeling and pave the way for future innovations. Limitations The Eagle and Finch models fall short on certain aspects that can be mitigated and addressed in future work. We experimented with using Eagle as an embedding model on the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023) but were not able to get strong embedding performance.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: GLA implementation details, RWKV benchmarks, Linear attention theory\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation\n\n*From Search Query: GLA implementation details*\n\n*Wenqing Zheng, S. Sharan, Ajay Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, Zhangyang Wang*\n\n**TL;DR:** This work proposes ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes, demonstrating that the progressive generation eases the reasoning procedure and guides the language model to generate higher-quality solutions.\n\n**Abstract:** For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of\"outline-then-detail\". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to jointly encode the natural language descriptions and syntactically aligned I/O data samples. Extensive evaluations show that ChainCoder outperforms state-of-the-arts, demonstrating that our progressive generation eases the reasoning procedure and guides the language model to generate higher-quality solutions. Our codes are available at: https://github.com/VITA-Group/ChainCoder.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 0*)\n\n#### 2. The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation\n\n*From Search Query: GLA implementation details*\n\n*Hao Peng, Xiaozhi Wang, Feng Yao, Kaisheng Zeng, Lei Hou, Juanzi Li, Zhiyuan Liu, Weixing Shen*\n\n**TL;DR:** Three major pitfalls of EE evaluations are identified and a series of remedies are suggested, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results are suggested.\n\n**Abstract:** Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoid these pitfalls, we suggest a series of remedies, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results. To help implement these remedies, we develop a consistent evaluation framework OMNIEVENT, which can be obtained from https://github.com/THU-KEG/OmniEvent.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 3. Implementation Matters in Deep RL: A Case Study on PPO and TRPO\n\n*From Search Query: GLA implementation details*\n\n*Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, F. Janoos, L. Rudolph, A. Madry*\n\n**TL;DR:** The results show that algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm are responsible for most of PPO's gain in cumulative reward over TRPO, and fundamentally change how RL methods function.\n\n**Abstract:** We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms, Proximal Policy Optimization and Trust Region Policy Optimization. We investigate the consequences of \"code-level optimizations:\" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 214  (*Influential: 19*)\n\n#### 4. RWKV: Reinventing RNNs for the Transformer Era\n\n*From Search Query: RWKV benchmarks*\n\n*Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xingjian Du, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu*\n\n**TL;DR:** This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Abstract:** Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 356  (*Influential: 34*)\n\n#### 5. SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality\n\n*From Search Query: RWKV benchmarks*\n\n*Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, Ranjay Krishna*\n\n**TL;DR:** This work introduces SugarCrepe, a new benchmark for vision-language compositionality evaluation that employs large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilizes an adversarial refinement mechanism to maximally reduce biases.\n\n**Abstract:** In the last year alone, a surge of new benchmarks to measure compositional understanding of vision-language models have permeated the machine learning ecosystem. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. Surprisingly, we find significant biases in all these benchmarks rendering them hackable. This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models. To remedy this rampant vulnerability, we introduce SugarCrepe, a new benchmark for vision-language compositionality evaluation. We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases. We re-evaluate state-of-the-art models and recently proposed compositionality inducing strategies, and find that their improvements were hugely overestimated, suggesting that more innovation is needed in this important direction. We release SugarCrepe and the code for evaluation at: https://github.com/RAIVNLab/sugar-crepe.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 77  (*Influential: 24*)\n\n#### 6. Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks\n\n*From Search Query: RWKV benchmarks*\n\n*Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg*\n\n**TL;DR:** Three strategies that can make a difference in preventing data contamination are proposed: test data made public should be encrypted with a public key and licensed to disallow derivative distribution, and demand training exclusion controls from closed API holders.\n\n**Abstract:** Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate without them; (3) avoid data which appears with its solution on the internet, and release the web-page context of internet-derived data along with the data. These strategies are practical and can be effective in preventing data contamination.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 76  (*Influential: 5*)\n\n#### 7. One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention\n\n*From Search Query: Linear attention theory*\n\n*Arvind V. Mahankali, Tatsunori Hashimoto, Tengyu Ma*\n\n**TL;DR:** This work theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data and mathematically shows that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective.\n\n**Abstract:** Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity [Aky\\\"urek et al., 2023], while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective [von Oswald et al., 2022]. However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian distribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss now implements a single step of $\\textit{pre-conditioned}$ GD. However, if only the distribution of the responses is changed, then this does not have a large effect on the learned algorithm: even when the response comes from a more general family of $\\textit{nonlinear}$ functions, the global minimizer of the pre-training loss still implements a single step of GD on a least-squares linear regression objective.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 65  (*Influential: 7*)\n\n#### 8. How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation\n\n*From Search Query: Linear attention theory*\n\n*Josh Alman, Zhao Song*\n\n**TL;DR:** This work studies a generalization of attention which captures triple-wise correlations, and shows that bounded entries are both necessary and sufficient for quickly performing generalized computations and yields a natural tradeoff between the boundedness of the entries, and order of the tensor one may use for more expressive, efficient attention computation.\n\n**Abstract:** In the classical transformer attention scheme, we are given three $n \\times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \\times d$ size matrix $D^{-1} \\exp(QK^\\top) V$ where $D = \\mathrm{diag}( \\exp(QK^\\top) {\\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations: $\\bullet$ On the positive side, if all entries of the input matrices are bounded above by $o(\\sqrt[3]{\\log n})$ then we show how to approximate the ``tensor-type'' attention matrix in $n^{1+o(1)}$ time. $\\bullet$ On the negative side, we show that if the entries of the input matrices may be as large as $\\Omega(\\sqrt[3]{\\log n})$, then there is no algorithm that runs faster than $n^{3-o(1)}$ (assuming the Strong Exponential Time Hypothesis from fine-grained complexity theory). We also show that our construction, algorithms, and lower bounds naturally generalize to higher-order tensors and correlations. Interestingly, the higher the order of the tensors, the lower the bound on the entries needs to be for an efficient algorithm. Our results thus yield a natural tradeoff between the boundedness of the entries, and order of the tensor one may use for more expressive, efficient attention computation.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 2*)\n\n#### 9. An analytic theory of generalization dynamics and transfer learning in deep linear networks\n\n*From Search Query: Linear attention theory*\n\n*Andrew Kyle Lampinen, S. Ganguli*\n\n**TL;DR:** An analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks is developed and reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.\n\n**Abstract:** Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 121  (*Influential: 7*)\n\n### 5 related papers from Papers with Code\n\n#### 1. ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention\n\n*From Search Query: GLA implementation details*\n\n*Chang Huang, Qian Zhang, Lianghui Zhu, Xinggang Wang, Bencheng Liao*\n\n**Abstract:** Recently, linear complexity sequence modeling networks have achieved modeling capabilities similar to Vision Transformers on a variety of computer vision tasks, while using fewer FLOPs and less memory. However, their advantage in terms of actual runtime speed is not significant. To address this issue, we introduce Gated Linear Attention (GLA) for vision, leveraging its superior hardware-awareness and efficiency. We propose direction-wise gating to capture 1D global context through bidirectional modeling and a 2D gating locality injection to adaptively inject 2D local details into 1D global context. Our hardware-aware implementation further merges forward and backward scanning into a single kernel, enhancing parallelism and reducing memory cost and latency. The proposed model, ViG, offers a favorable trade-off in accuracy, parameters, and FLOPs on ImageNet and downstream tasks, outperforming popular Transformer and CNN-based models. Notably, ViG-S matches DeiT-B's accuracy while using only 27% of the parameters and 20% of the FLOPs, running 2$\\times$ faster on $224\\times224$ images. At $1024\\times1024$ resolution, ViG-T uses 5.2$\\times$ fewer FLOPs, saves 90% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7% higher top-1 accuracy than DeiT-T. These results position ViG as an efficient and scalable solution for visual representation learning. Code is available at \\url{https://github.com/hustvl/ViG}.\n\n**Published:** 2024-05-28\n\n\n\n#### 2. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence\n\n*From Search Query: RWKV benchmarks*\n\n*Rui-Jie Zhu, Jian Zhu, Peng Zhou, Qihang Zhao, Bingchen Zhao, Ruichong Zhang, Stanis\u0142aw Wo\u017aniak, Cahya Wirawan, Haoqin Tu, Guangyu Song, Fares Obeid, Niklas Muennighoff, Jiaju Lin, Atsushi Saito, Xingjian Du, Ronald McClelland Jr., Satyapriya Krishna, Bart\u0142omiej Koptyra, Jan Koco\u0144, Kranthi Kiran GV, Przemys\u0142aw Kazienko, Haowen Hou, Teddy Ferdinan, Eugene Cheah, Stella Biderman, Eric Alcaide, Alon Albalak, Quentin Anthony, Daniel Goldstein, Bo Peng*\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n**Published:** 2024-04-08\n\n\n\n#### 3. Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model\n\n*From Search Query: RWKV benchmarks*\n\n*Chen Change Loy, Shuicheng Yan, Ming-Hsuan Yang, Tao Zhang, Lu Qi, Xiangtai Li, Haobo Yuan*\n\n**Abstract:** Transformer-based segmentation methods face the challenge of efficient inference when dealing with high-resolution images. Recently, several linear attention architectures, such as Mamba and RWKV, have attracted much attention as they can process long sequences efficiently. In this work, we focus on designing an efficient segment-anything model by exploring these different architectures. Specifically, we design a mixed backbone that contains convolution and RWKV operation, which achieves the best for both accuracy and efficiency. In addition, we design an efficient decoder to utilize the multiscale tokens to obtain high-quality masks. We denote our method as RWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we build a benchmark containing various high-quality segmentation datasets and jointly train one efficient yet high-quality segmentation model using this benchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding performance in efficiency and segmentation quality compared to transformers and other linear attention models. For example, compared with the same-scale transformer model, RWKV-SAM achieves more than 2x speedup and can achieve better segmentation performance on various datasets. In addition, RWKV-SAM outperforms recent vision Mamba models with better classification and semantic segmentation results. Code and models will be publicly available.\n\n**Published:** 2024-06-27\n\n\n\n#### 4. Asymptotic theory of in-context learning by linear attention\n\n*From Search Query: Linear attention theory*\n\n*Cengiz Pehlevan, Anindita Maiti, Jacob A. Zavatone-Veth, Mary I. Letey, Yue M. Lu*\n\n**Abstract:** Transformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers' success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model's behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures.\n\n**Published:** 2024-05-20\n\n\n\n#### 5. Flowformer: Linearizing Transformers with Conservation Flows\n\n*From Search Query: Linear attention theory*\n\n*Mingsheng Long, Jianmin Wang, Jiehui Xu, Jialong Wu, Haixu Wu*\n\n**Abstract:** Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.\n\n**Published:** 2022-02-13\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating Gated Linear Attention (GLA) and RWKV architecture, here are some key points and resources that can help address the implementation challenges, benchmarks, and theoretical foundations:\n\n## Implementation Challenges and Solutions\n\n### Managing State Transitions and Memory Usage\n- Implementing GLA and RWKV requires careful management of state transitions to avoid local pattern shortcuts and ensure efficient memory usage. For GLA, the gating mechanism helps in controlling the flow of information, which can be optimized using hardware-efficient training methods[6|.\n- For RWKV, the combination of transformer-like parallelizable training with RNN-like efficient inference necessitates optimizing state size and selectivity mechanisms. This can be achieved by fine-tuning the linear attention mechanism to balance between capturing long-range dependencies and maintaining computational efficiency.\n\n### Practical Implementation Guides\n- Look for detailed implementation guides or case studies that provide insights into the practical challenges faced during the implementation of GLA and RWKV. For example, the paper \"Gated Linear Attention Transformers with Hardware-Efficient Training\" provides implementation details on how to train GLA models efficiently on hardware, which can be a valuable resource.\n\n## Benchmarking and Performance Metrics\n\n### Comparative Benchmarks\n- To assess the relative performance of GLA and RWKV, compare them with other linear attention models like Luna (Linear Unified Nested Attention) and Linear Log-Normal Attention. These benchmarks can help in evaluating the efficiency and effectiveness of different linear attention mechanisms.\n- For instance, the paper \"Luna: Linear Unified Nested Attention\" provides a benchmark for evaluating the efficiency of linear attention mechanisms, which can be used to compare the performance of GLA and RWKV.\n\n### Performance Metrics\n- Use metrics such as perplexity, accuracy in downstream tasks, and robustness to variant inputs to evaluate the performance of GLA and RWKV models. These metrics will help in understanding how these models perform in real-world scenarios compared to other state-of-the-art models.\n\n## Theoretical Foundations\n\n### Linear Attention Mechanisms\n- Investigate theoretical papers that delve into the mathematical underpinnings of linear attention mechanisms. For example, the paper \"Linear Log-Normal Attention with Unbiased Concentration\" provides a theoretical justification for linear attention mechanisms by emulating the distribution and concentration behavior of the original self-attention.\n\n### Online Learning Objectives\n- The use of linear attention mechanisms is often supported by online learning objectives, which help in formulating precise state transition rules and optimizing performance. Understanding these theoretical foundations can help in designing more efficient and scalable models.\n\n## Additional Resources\n\n### Research Papers\n- Here are some key papers that provide detailed insights into GLA and RWKV:\n  - \"Gated Linear Attention Transformers with Hardware-Efficient Training\" by Yang et al.\n  - \"Reinventing RNNs for the Transformer Era\" by Peng et al.\n  - \"Linear Log-Normal Attention with Unbiased Concentration\" by Nahshan et al.\n  - \"Luna: Linear Unified Nested Attention\" by Ma et al.\n\n### Implementation Repositories\n- Look for open-source repositories or implementation guides on platforms like GitHub or research institution websites. These repositories often include code snippets, tutorials, and discussions that can help in overcoming implementation challenges.\n\nBy focusing on these areas, researchers can gather the necessary information to improve the design of autoregressive language models using GLA and RWKV, ensuring better performance, efficiency, and scalability.\n\n## References\n\n- Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*. arXiv.org.\n- Peng, B., et al. (2023). *Reinventing RNNs for the Transformer Era*. Conference on Empirical Methods in Natural Language Processing.\n- Nahshan, Y., Kampeas, D.-J., & Haleva, E. (2023). *Linear Log-Normal Attention with Unbiased Concentration*. International Conference on Learning Representations.\n- Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., & Zettlemoyer, L. (2021). *Luna: Linear Unified Nested Attention*. Neural Information Processing Systems.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for **FastTTTLinear** aims to integrate Gated Linear Attention (GLA) and RWKV architecture concepts into the TTTLinear GAU to enhance computational efficiency and scalability. The search results provide insights into the empirical performance, implementation challenges, and theoretical justification of GLA and RWKV, as well as comparisons with other models.\n\n### Useful References with Excerpts\n\n1. **Gated Linear Attention Transformers with Hardware-Efficient Training**\n   - \"The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\"\n   - This paper highlights GLA's competitive performance and efficiency in language modeling tasks.\n\n2. **Reinventing RNNs for the Transformer Era**\n   - \"RWKV combines the efficient parallelizable training of transformers with the efficient inference of RNNs, leveraging a linear attention mechanism.\"\n   - This work discusses RWKV's ability to handle long-range dependencies efficiently and its competitive performance compared to Transformers.\n\n3. **Linear Log-Normal Attention with Unbiased Concentration**\n   - \"We propose a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention.\"\n   - This paper introduces an alternative linear attention mechanism, relevant for understanding the landscape of linear attention models.\n\n4. **Luna: Linear Unified Nested Attention**\n   - \"Luna is a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear time and space complexity.\"\n   - Luna's approach to linear attention provides a benchmark for evaluating the efficiency of linear attention mechanisms.\n\n### Key Insights and Detailed Analysis\n\n- **Empirical Performance**: GLA and RWKV have shown competitive performance in language modeling tasks, with GLA excelling in hardware-efficient training and RWKV effectively capturing long-range dependencies.\n\n- **Implementation Challenges**: Implementing GLA and RWKV involves managing state transitions and ensuring efficient memory usage. Challenges include optimizing state size and selectivity mechanisms to avoid local pattern shortcuts.\n\n- **Theoretical Justification**: The use of linear attention mechanisms is supported by online learning objectives, which help in formulating precise state transition rules and optimizing performance.\n\n- **Comparison with Other Models**: GLA and RWKV offer linear complexity and constant memory usage, making them efficient alternatives to traditional attention-based models for long-context tasks. Their expressiveness and adaptability are enhanced by gating mechanisms.\n\n### Future Search Plan\n\n1. **Explore Implementation Details**: Search for detailed implementation guides or case studies on GLA and RWKV to understand practical challenges and solutions.\n\n2. **Benchmarking and Performance Metrics**: Look for benchmarks that compare GLA and RWKV with other linear attention models to assess their relative performance.\n\n3. **Theoretical Foundations**: Investigate theoretical papers that delve into the mathematical underpinnings of linear attention mechanisms and their applications in language models.\n\n### References\n\n- Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*. arXiv.org.\n- Peng, B., et al. (2023). *Reinventing RNNs for the Transformer Era*. Conference on Empirical Methods in Natural Language Processing.\n- Nahshan, Y., Kampeas, D.-J., & Haleva, E. (2023). *Linear Log-Normal Attention with Unbiased Concentration*. International Conference on Learning Representations.\n- Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., & Zettlemoyer, L. (2021). *Luna: Linear Unified Nested Attention*. Neural Information Processing Systems."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 1.114932,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.8678425000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "TTTLinear",
    "proposal": "# HierarchicalTTTLinear: Enhancing TTTLinear with Hierarchical Attention and Hardware-Efficient Gradient Computation\n\n## Motivation\n\nCurrent language models face significant challenges in efficiently processing long sequences while maintaining adaptability and performance. While FastTTTLinear and its variants address some of these challenges through linear attention and test-time training, they still face limitations in memory efficiency and computational overhead. The motivation behind HierarchicalTTTLinear is to enhance the TTTLinear GAU by:\n\n1. Introducing hierarchical attention mechanisms for efficient processing of long sequences\n2. Implementing adaptive state compression for reduced memory usage\n3. Optimizing gradient computation through hardware-aware implementations\n4. Maintaining test-time training capabilities while improving efficiency\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **Memory Efficiency**:\n   - High memory requirements for long sequences\n   - Inefficient state representation\n   - Limited ability to compress and decompress information\n\n2. **Computational Overhead**:\n   - Uniform computation regardless of content importance\n   - Inefficient gradient computation during test-time training\n   - Limited hardware utilization\n\n3. **Scalability**:\n   - Difficulty handling very long sequences\n   - Challenges in maintaining performance with increased context length\n   - Memory bandwidth bottlenecks\n\n### Proposed Solutions\n\n1. **Hierarchical Attention**:\n   - Chunk-based processing with selective attention\n   - Multi-level state representation\n   - Adaptive compression based on content importance\n\n2. **Efficient Gradient Computation**:\n   - Hardware-aware implementation strategies\n   - Profile-guided optimization\n   - Selective computation based on importance\n\n3. **Memory Management**:\n   - Dynamic state compression\n   - Efficient caching strategies\n   - Block-wise processing\n\n## Core Idea and Philosophy\n\nHierarchicalTTTLinear introduces three key innovations:\n\n1. **Hierarchical State Processing**:\n   - Multi-level attention mechanisms\n   - Content-dependent chunk selection\n   - Adaptive state compression\n\n2. **Hardware-Efficient Computation**:\n   - Profile-guided optimization\n   - Block-based processing\n   - IO-aware algorithms\n\n3. **Adaptive Test-Time Training**:\n   - Selective gradient computation\n   - Efficient state updates\n   - Memory-aware caching\n\n## Design Plan\n\n### Selected GAU to Modify\n```selection TTTLinear```\n\n### Modification Overview\n\n1. **Hierarchical Attention Integration**:\n   - Implement chunk-based processing\n   - Add selective attention mechanisms\n   - Enable multi-level state representation\n\n2. **Gradient Computation Optimization**:\n   - Introduce profile-guided optimization\n   - Implement selective gradient updates\n   - Add hardware-aware computations\n\n3. **Memory Management Enhancement**:\n   - Add dynamic state compression\n   - Implement efficient caching\n   - Enable block-wise processing\n\n### Implementation Guidelines\n\n```python\nclass HierarchicalTTTLinear(nn.Module):\n    def __init__(self, embed_dim, num_heads, chunk_size, compression_ratio):\n        super().__init__()\n        self.chunk_size = chunk_size\n        self.compression_ratio = compression_ratio\n        \n        # Hierarchical attention components\n        self.chunk_attention = ChunkAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            chunk_size=chunk_size\n        )\n        \n        self.global_attention = GlobalAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads\n        )\n        \n        # State compression components\n        self.state_compressor = AdaptiveStateCompressor(\n            embed_dim=embed_dim,\n            compression_ratio=compression_ratio\n        )\n        \n        # Gradient computation components\n        self.gradient_manager = HardwareEfficientGradient(\n            embed_dim=embed_dim\n        )\n\n    def forward(self, x, state=None):\n        # Chunk-based processing\n        chunks = self.split_into_chunks(x)\n        \n        # Local attention within chunks\n        chunk_outputs = []\n        for chunk in chunks:\n            # Profile-guided optimization\n            if self.should_process_chunk(chunk):\n                chunk_output = self.chunk_attention(chunk)\n                chunk_outputs.append(chunk_output)\n        \n        # Global attention across chunks\n        global_output = self.global_attention(chunk_outputs)\n        \n        # State compression and update\n        if state is not None:\n            compressed_state = self.state_compressor(state)\n            new_state = self.update_state(compressed_state, global_output)\n        else:\n            new_state = self.initialize_state(global_output)\n        \n        # Efficient gradient computation\n        if self.training:\n            output = self.gradient_manager(global_output)\n        else:\n            output = global_output\n            \n        return output, new_state\n\nclass ChunkAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, chunk_size):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            batch_first=True\n        )\n        self.chunk_size = chunk_size\n        \n    def forward(self, x):\n        # Implement chunk-level attention\n        return self.attention(x, x, x)[0]\n\nclass GlobalAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attention = LinearAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads\n        )\n        \n    def forward(self, chunk_outputs):\n        # Implement global attention across chunks\n        return self.attention(chunk_outputs)\n\nclass AdaptiveStateCompressor(nn.Module):\n    def __init__(self, embed_dim, compression_ratio):\n        super().__init__()\n        self.compressor = nn.Linear(\n            embed_dim,\n            int(embed_dim * compression_ratio)\n        )\n        \n    def forward(self, state):\n        # Implement adaptive state compression\n        return self.compressor(state)\n\nclass HardwareEfficientGradient(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.grad_scaler = nn.Parameter(torch.ones(1))\n        \n    def forward(self, x):\n        # Implement hardware-efficient gradient computation\n        return x * self.grad_scaler\n```\n\n### Theoretical Analysis\n\n1. **Computational Complexity**:\n   - O(n) complexity through chunk-based processing\n   - Reduced memory overhead through compression\n   - Efficient gradient computation\n\n2. **Memory Usage**:\n   - Adaptive compression reduces memory requirements\n   - Block-wise processing enables better memory utilization\n   - Efficient caching strategies minimize memory bandwidth\n\n3. **Model Capacity**:\n   - Enhanced expressiveness through hierarchical attention\n   - Maintained adaptability through test-time training\n   - Improved long-range dependency modeling\n\n## Expected Benefits\n\n1. **Efficiency Improvements**:\n   - Reduced memory requirements through compression\n   - Faster computation through selective processing\n   - Better hardware utilization\n\n2. **Enhanced Adaptability**:\n   - Improved handling of varied input lengths\n   - Better generalization through hierarchical processing\n   - Efficient test-time training\n\n3. **Scalability**:\n   - Better handling of long sequences\n   - Reduced memory bandwidth requirements\n   - Improved parallel processing capabilities\n\n## References\n\n1. Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n2. Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n3. Qin, Z., et al. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n4. Niu, S., et al. (2022). Efficient Test-Time Model Adaptation without Forgetting. International Conference on Machine Learning.\n\n5. Ge, S., et al. (2023). Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. International Conference on Learning Representations.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "gpt4o_0806",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel enhancement of TTTLinear that introduces hierarchical attention with adaptive state compression and hardware-efficient gradient computation. The design combines chunk-based processing with selective attention mechanisms to improve computational efficiency and memory usage while maintaining test-time training capabilities.",
    "ideation": null,
    "modelname": "hierarchicalttt",
    "suggestions": "1. Conduct empirical studies to validate theoretical claims, focusing on robustness and adaptability.\n2. Provide detailed implementation guidelines for integrating GLA and RWKV.\n3. Explore potential challenges in integration and propose solutions or mitigations.",
    "user_input": ""
}