{
    "variantname": "HierarchicalSparseAttention",
    "review": "The HierarchicalSparseNorm-GPT proposal presents an innovative approach to enhancing transformer architectures through the integration of hierarchical sparse attention with dynamic layer normalization. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel combination of hierarchical sparse attention with dynamic normalization\n- Well-grounded theoretical framework for both attention and normalization components\n- Clear differentiation from existing approaches like GQA and Sparse Attention\n- Thoughtful consideration of hardware efficiency\n\n2. Design Clarity:\n- Well-structured mathematical formulation\n- Clear architectural components and interactions\n- Detailed implementation guidelines\n- Thorough consideration of integration challenges\n\n3. Efficiency Considerations:\n- Reduction in computational complexity through sparse attention\n- Memory optimization through hierarchical processing\n- Hardware-aware implementation strategies\n- Clear focus on practical efficiency\n\n4. Scalability:\n- Linear complexity through sparse attention mechanisms\n- Hierarchical structure enables efficient processing of long sequences\n- Memory-efficient design through adaptive compression\n- Clear scaling strategy for larger models\n\nCONCERNS:\n\n1. Training Stability:\n- Complex interaction between hierarchical attention and dynamic normalization\n- Potential challenges with gradient flow through sparse patterns\n- Need for careful initialization and training procedures\n- Impact of sparsity on model convergence\n\n2. Implementation Complexity:\n- Integration of multiple sophisticated components\n- Complex coordination between attention and normalization\n- Hardware optimization requirements\n- Potential debugging challenges\n\n3. Memory Management:\n- Overhead from dynamic parameter generation\n- Complex memory access patterns in hierarchical structure\n- Cache pressure from sparse computations\n- Trade-off between sparsity and memory efficiency\n\n4. Performance Trade-offs:\n- Balance between sparsity and model expressiveness\n- Impact of hierarchical structure on attention patterns\n- Potential information loss through sparse attention\n- Computational overhead of dynamic normalization\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than GQA's grouped attention approach\n2. More efficient than traditional sparse attention mechanisms\n3. Novel integration of dynamic normalization with hierarchical attention\n4. Unique approach to memory management compared to Flash Attention\n\nHowever, it shares some concepts with:\n1. SPARSEK Attention's efficient sparse computation\n2. Hourglass Transformer's hierarchical processing\n3. Dynamic Layer Normalization's adaptive parameter generation",
    "search_stack": [
        {
            "ready": false,
            "query": "hierarchical attention, dynamic normalization, sparse attention",
            "detail": "Hierarchical adaptive attention in language models",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nHierarchical adaptive attention in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.98)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 14/34 (Score: 0.99)*\n\n```\nZanchettin. 2019. Hierarchical attentional hybrid neural networks for document classification. ArXiv, abs/1901.06610. Joshua Ainslie, S. Onta\u00f1\u00f3n, C. Alberti, V. Cvicek, Zachary Kenneth Fisher, Philip Pham, Anirudh Ravula, S. Sanghai, Qifan Wang, and L. Yang. 2020. Etc: Encoding long and structured inputs in transformers. In EMNLP. Alexei Baevski and M. Auli. 2019. Adaptive input representations for neural language modeling. ArXiv, abs/1809.10853. I. Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. 2019. Attention augmented convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3285-3294. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 6/34 (Score: 0.97)*\n\n```\n(11) and (12) is generally unknown. So gathering empirical evidences seems the only viable path to answer the first question listed above. The ablation studies by (Khandelwal et al., 2018) examine the effect of context words on a language model. Within the context range of about 200 tokens, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. The observation is succinctly summarized by the title of the paper \"sharp nearby, fuzzy far away\". Remarkably, this is in spirit very close to the key insight into the Multilevel Methods. A few recent attention-related studies have explored this direction with some success, such as word-level and sentence-level attentions in (Miculicich et al., 2018; Abreu et al., 2019), and sentence-level and paragraph-level attentions in (Liu and Lapata, 2019). Even though the proposed hierarchical attention in these studies only has two levels, as opposed to ten or more levels typically used by the Multilevel Methods, the reported positive results are quite suggestive. We therefore hypothesize that the same hierarchical low-rank structure as shown in Eq (13) might also hold for the attention matrix in many NLP tasks. And we treat it as the inductive bias in the hierarchical attention mechanism proposed in this paper. As pointed out in (Goyal and Bengio, 2020), inductive biases encourage the learning algorithm to prioritise solutions with certain properties. Hence good benchmark performance delivered by a Transformer-based model with proposed hierarchical attention can be regarded as a positive evidence to support the hierarchical low-rank structure hypothesis. ### 5.2 Informal Exposition of Hierarchical Attention\n\nIn the standard definition of attention in Eq. (3) and (4), there is no preference given to any keys based on the sequence distance between a query and keys. The observation in (Khandelwal et al.,\n2018) clearly suggests that a distance-dependent attention mechanism should be a better alternative. We will take three steps to informally explain the hierarchical attention mechanism. First, the attention matrix blocks for nearby, mid-distance and long-distance attention are separated in section 5.2.1. This is the first step toward the distance-dependent attention mentioned above. Second, a token hierarchy is established in section 5.2.2. Third, the hierarchical attention is constructed in section 5.2.3\n\n### 5.2.1 Attention Partition\n\nConsider a 16-word sentence in Fig. 1. The sentence is partitioned at three segment granularity. This induces a three-level partition of the attention matrix $A$ for the original sequence:\n\n$$\nA=A^{(2)}+A^{(1)}+A^{(0)}\n$$\n\nwhere\n\n$$\nA^{(2)}=\\left[\\begin{array}{c|c}\n0 & A_{12}^{(2)} \\\\\n\\hline A_{21}^{(2)} & 0\n\\end{array}\\right]\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_a848c33bf30dfbc079beg-05.jpg?height=258&width=600&top_left_y=1539&top_left_x=291)\n\n$$\nA^{(0)}=\\left[\\begin{array}{c|c|c|c|c}\nA_{11}^{(0)} & A_{12}^{(0)} & & & \\\\\n\\hline A_{21}^{(0)} & A_{22}^{(0)} & A_{23}^{(0)} & & \\\\\n\\hline & \\ddots & \\ddots & \\ddots & \\\\\n\\hline & & & A_{87}^{(0)} & A_{88}^{(0)}\n\\end{array}\\right]\n$$\n\nNote that the nonzero entries in $A^{(0)}, A^{(1)}$ and $A^{(2)}$ are the same as the corresponding entries of matrix $A$ in Eq.\n```\n\n#### 2. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Avg. Score: 0.88)\n\n*Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Franccois Fleuret*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 1155  (*Influential: 164*)\n\n**TL;DR:** This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.\n\n**Abstract:** Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\n##### *Relevant Chunk: No. 15/28 (Score: 0.88)*\n\n```\n2010. Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. RoBERTa: A robustly optimized BERT pretraining approach, 2020. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In Wallach, H., Larochelle, H., Beygelzimer, A., d' Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 14014-14024. Curran Associates, Inc., 2019. Mnih, A. and Hinton, G. E. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pp. 1081-1088, 2009. Morin, F. and Bengio, Y. Hierarchical probabilistic neural network language model. In Aistats, volume 5, pp. 246252. Citeseer, 2005. Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., and Shlens, J. Stand-alone self-attention in vision models.\n```\n\n#### 3. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.83)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 18/25 (Score: 0.83)*\n\n```\nSmith, and Lingpeng Kong. 2021. Random feature attention. ArXiv, abs/2103.02143. Michael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, S. Baccus, Y. Bengio, Stefano Ermon, and Christopher R\u00e9. 2023. Hyena hierarchy: Towards larger convolutional language models. International Conference on Machine Learning. Zhen Qin, Weixuan Sun, Huicai Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. ArXiv, abs/2202.08791. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers.\n```\n\n#### 4. HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning (Avg. Score: 0.79)\n\n*Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, Sung Ju Hwang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Hierarchically Pruned Attention (HiP) is proposed, a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly, opening up the possibility to many long-context LLM applications previously infeasible.\n\n**Abstract:** In modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering. However, handling long context sequences with LLMs is prohibitively costly due to the conventional attention mechanism's quadratic time and space complexity, and the context window size is limited by the GPU memory. Although recent works have proposed linear and sparse attention mechanisms to address this issue, their real-world applicability is often limited by the need to re-train pre-trained models. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which simultaneously reduces the training and inference time complexity from $O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To this end, we devise a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly. HiP is training-free as it only utilizes the pre-trained attention scores to spot the positions of the top-$k$ most significant elements for each query. Moreover, it ensures that no token is overlooked, unlike the sliding window-based sub-quadratic attention methods, such as StreamingLLM. Extensive experiments on diverse real-world benchmarks demonstrate that HiP significantly reduces prompt (i.e., prefill) and decoding latency and memory usage while maintaining high generation performance with little or no degradation. As HiP allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no additional engineering due to its easy plug-and-play deployment, we believe that our work will have a large practical impact, opening up the possibility to many long-context LLM applications previously infeasible.\n\n##### *Relevant Chunk: No. 1/44 (Score: 0.79)*\n\n```\n# HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning \n\nHeejun Lee ${ }^{1 *}$, Geon Park $^{1 *}$, Youngwan Lee ${ }^{1,2 *}$, Jina Kim ${ }^{1 *}$,<br>Wonyoung Jeong ${ }^{4}$, Myeongjae Jeon ${ }^{3}$, Sung Ju Hwang ${ }^{1,4 \\dagger}$<br>KAIST ${ }^{1}$, ETRI $^{2}$, UNIST $^{3}$, DeepAuto.ai ${ }^{4}$,<br>\\{ainl,geon.park,ywlee88,jinakim\\}@kaist.ac.kr,<br>young@deepauto.ai, mjjeon@unist.ac.kr, sjhwang@kaist.ac.kr\n\n\n#### Abstract\n\nIn modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical attention, dynamic normalization, sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from ArXiv\n\n#### 1. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 2. Bidirectional Context-Aware Hierarchical Attention Network for Document\n  Understanding\n\n*From Search Query: hierarchical attention*\n\n*Jean-Baptiste Remy, Antoine Jean-Pierre Tixier, Michalis Vazirgiannis*\n\n**Abstract:** The Hierarchical Attention Network (HAN) has made great strides, but it\nsuffers a major limitation: at level 1, each sentence is encoded in complete\nisolation. In this work, we propose and compare several modifications of HAN in\nwhich the sentence encoder is able to make context-aware attentional decisions\n(CAHAN). Furthermore, we propose a bidirectional document encoder that\nprocesses the document forwards and backwards, using the preceding and\nfollowing sentences as context. Experiments on three large-scale sentiment and\ntopic classification datasets show that the bidirectional version of CAHAN\noutperforms HAN everywhere, with only a modest increase in computation time.\nWhile results are promising, we expect the superiority of CAHAN to be even more\nevident on tasks requiring a deeper understanding of the input documents, such\nas abstractive summarization. Code is publicly available.\n\n**Published:** 2019-08-16T15:20:04Z  (*Updated: 2019-08-16T15:20:04Z*)\n\n\n\n#### 3. Efficient Social Network Multilingual Classification using Character,\n  POS n-grams and Dynamic Normalization\n\n*From Search Query: dynamic normalization*\n\n*Carlos-Emiliano Gonz\u00e1lez-Gallardo, Juan-Manuel Torres-Moreno, Azucena Montes Rend\u00f3n, Gerardo Sierra*\n\n**Abstract:** In this paper we describe a dynamic normalization process applied to social\nnetwork multilingual documents (Facebook and Twitter) to improve the\nperformance of the Author profiling task for short texts. After the\nnormalization process, $n$-grams of characters and n-grams of POS tags are\nobtained to extract all the possible stylistic information encoded in the\ndocuments (emoticons, character flooding, capital letters, references to other\nusers, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of\nperformance.\n\n**Published:** 2017-02-21T16:26:54Z  (*Updated: 2017-02-21T16:26:54Z*)\n\n\n\n#### 4. Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers\n\n*From Search Query: sparse attention*\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n\n**Published:** 2024-06-24T15:55:59Z  (*Updated: 2024-06-24T15:55:59Z*)\n\n\n\n#### 5. SEA: Sparse Linear Attention with Estimated Attention Mask\n\n*From Search Query: sparse attention*\n\n*Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang*\n\n**Abstract:** The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n\n**Published:** 2023-10-03T03:56:26Z  (*Updated: 2024-03-25T04:04:05Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 2. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 3. Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows\n\n*From Search Query: dynamic normalization*\n\n*Andreas Lehrmann, Greg Mori, Bo Chang, Ruizhi Deng, Marcus A. Brubaker*\n\n**Abstract:** Normalizing flows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation. In this work, we propose a novel type of normalizing flow driven by a differential deformation of the Wiener process. As a result, we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process, such as efficient computation of likelihoods and marginals. Furthermore, our continuous treatment provides a natural framework for irregular time series with an independent arrival process, including straightforward interpolation. We illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior flexibility to variational RNN and latent ODE baselines in a series of experiments on synthetic and real-world data.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-02-24\n\n\n\n#### 4. Dynamic Token Normalization Improves Vision Transformers\n\n*From Search Query: dynamic normalization*\n\n*Ping Luo, Ying Shan, Xiaogang Wang, Xuyuan Xu, Zhaoyang Zhang, Yixiao Ge, Wenqi Shao*\n\n**Abstract:** Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\\%$ - $1.2\\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\\%$ - $3.9\\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\\%$ - $0.8\\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \\url{https://github.com/wqshao126/DTN}\n\n**Conference:** dynamic-token-normalization-improves-vision\n\n**Published:** 2021-12-05\n\n\n\n#### 5. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: sparse attention*\n\n*Bo Dai, Dale Schuurmans, Jure Leskovec, Mengjiao Yang, Zihang Dai, Hanjun Dai, Hongyu Ren*\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-12\n\n\n\n#### 6. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\n\n*From Search Query: sparse attention*\n\n*Lili Qiu, Yuqing Yang, Chin-Yew Lin, Dongsheng Li, Amir H. Abdi, Zhenhua Han, Surin Ahn, Xufang Luo, Qianhui Wu, Chengruidong Zhang, Yucheng Li, Huiqiang Jiang*\n\n**Abstract:** The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\n\n**Published:** 2024-07-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical adaptive attention, dynamic normalization, and sparse attention, here are some key insights and relevant references:\n\n## Hierarchical Adaptive Attention\n\n- The concept of hierarchical attention is crucial for improving scalability and efficiency. For instance, the paper on \"SeerAttention\" introduces a hierarchical approach by learning intrinsic sparse attention, which can be adapted to hierarchical structures. SeerAttention uses a learnable gate to identify significant blocks in the attention map, allowing for block-sparse attention that balances accuracy and speedup.\n\n## Dynamic Normalization\n\n- Dynamic normalization techniques can enhance model adaptability and gradient stability. The paper on the \"Enhanced knowledge graph recommendation algorithm\" discusses the use of layer normalization and residual connections to stabilize learning and mitigate the vanishing gradient problem. This approach can be applied to language models to improve their robustness and adaptability to complex data structures.\n\n## Sparse Attention\n\n- Sparse attention mechanisms are vital for reducing computational complexity while maintaining performance. The \"SeerAttention\" mechanism learns and leverages the intrinsic sparsity in attention, which is particularly beneficial for long-context language models. It uses a customized FlashAttention implementation to extract block-level ground truth of attention maps, enabling efficient learning of sparse attention.\n\n## Integration of Hierarchical and Sparse Attention\n\n- Combining hierarchical and sparse attention can further enhance the model's efficiency and performance. For example, the \"SeerAttention\" mechanism, which learns sparse attention patterns, can be integrated into a hierarchical attention framework. This would allow the model to capture key features at different levels while optimizing computational resources.\n\n## Dynamic Layer Normalization\n\n- Dynamic layer normalization can be integrated with hierarchical attention to improve adaptability. The \"Enhanced knowledge graph recommendation algorithm\" paper shows how layer normalization and residual connections can be used to standardize features across each layer, stabilizing the learning process. This can be applied to language models to enhance their ability to handle varying input lengths and complexities.\n\n## Key References\n\n- **SeerAttention**: This paper proposes a novel attention mechanism that learns and leverages intrinsic sparsity in attention, which is crucial for improving efficiency in long-context language models. It introduces a learnable gate to adaptively identify important blocks in the attention map, reducing computational complexity.\n\n- **Enhanced knowledge graph recommendation algorithm**: This paper discusses the use of dynamic attention mechanisms, layer normalization, and residual connections to improve model stability and adaptability. These techniques can be applied to language models to enhance their robustness and efficiency.\n\n## Future Search Plan\n\n- **Empirical Validation**: Conduct empirical studies or simulations to test the performance and scalability of the proposed hierarchical adaptive attention with dynamic normalization and sparse attention mechanisms.\n- **Additional Literature**: Explore more literature on sparse and grouped attention mechanisms, such as the \"GQA\" (Generalized Query Attention) approach, to refine the attention component of the proposed design.\n\nBy integrating these concepts and techniques, the researchers can develop an autoregressive language model that is more efficient, scalable, and performant, addressing the challenges of quadratic complexity in traditional attention mechanisms and the rigidity of standard normalization techniques.\n",
            "analysis": "1. **Summary of your analysis**:\n   The seed design, HieraNorm-AttnGPT, proposes integrating hierarchical adaptive multi-head attention with dynamic layer normalization to improve computational efficiency, scalability, and performance in autoregressive language models. The design aims to address the quadratic complexity of traditional multi-head attention and the rigidity of standard normalization techniques. The review of related work highlights advancements in attention mechanisms, adaptive normalization, and hierarchical attention, which all contribute to the proposed design's novelty and potential effectiveness.\n\n2. **All useful references with excerpts**:\n   - Reference 0: EventDDLerpLinear introduces event-driven updates and hierarchical state compression to improve efficiency and memory usage in language models.\n   - Reference 1: HierarchicalCompressiveGatedMLP proposes hierarchical memory structures and dynamic compression to enhance GatedMLP architectures.\n   - Reference 2: GQA introduces grouped-query attention, a generalization of multi-query attention, to improve efficiency in transformer models.\n   - Reference 3: Likelihood-Based Diffusion Language Models explore algorithmic improvements for diffusion-based language models, offering insights into alternative modeling approaches.\n   - Reference 4: Enhancing Language Modeling with Sentinel Tokens proposes segmenting text into chunks with special tokens to improve long-term context modeling.\n   - Reference 5: Self-attention Networks Localize When QK-eigenspectrum Concentrates discusses attention localization and its impact on model expressivity and trainability.\n\n3. **Key insights and detailed analysis that may help you**:\n   - The concept of hierarchical structures is prevalent in recent research, indicating its potential to improve scalability and efficiency in language models.\n   - Dynamic normalization techniques, such as those proposed in the seed design, can enhance model adaptability and gradient stability.\n   - Sparse and grouped attention mechanisms, like those in GQA, offer promising avenues for reducing computational complexity while maintaining performance.\n   - The integration of feedback mechanisms and adaptive memory management, as seen in HierarchicalCompressiveGatedMLP, could further enhance the proposed design's robustness and scalability.\n\n4. **Future search plan if needed or plan of next steps**:\n   - Investigate further into the implementation and empirical validation of hierarchical attention and dynamic normalization techniques.\n   - Explore additional literature on sparse and grouped attention mechanisms to refine the proposed design's attention component.\n   - Consider conducting empirical studies or simulations to test the proposed design's performance and scalability.\n\n5. **The list of references, use precise citation style**:\n   - Engelken, R. (2023). SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks. Neural Information Processing Systems.\n   - Fan, A., Lavril, T., Grave, E., Joulin, A., & Sukhbaatar, S. (2020). Addressing Some Limitations of Transformers with Feedback Memory.\n   - Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., & Lebr'on, F. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. Conference on Empirical Methods in Natural Language Processing.\n   - Gulrajani, I., & Hashimoto, T. (2023). Likelihood-Based Diffusion Language Models. Neural Information Processing Systems.\n   - Luo, W., Zheng, S., Xia, H., Wang, W., Lei, Y., Liu, T., Chen, S., & Sui, Z. (2024). Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens. arXiv.org.\n   - Bao, H., Hataya, R., & Karakida, R. (2024). Self-attention Networks Localize When QK-eigenspectrum Concentrates. arXiv.org."
        },
        {
            "ready": false,
            "query": "hierarchical attention, dynamic normalization, sparse attention",
            "detail": "Hierarchical adaptive attention in language models",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nHierarchical adaptive attention in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.98)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 14/34 (Score: 0.99)*\n\n```\nZanchettin. 2019. Hierarchical attentional hybrid neural networks for document classification. ArXiv, abs/1901.06610. Joshua Ainslie, S. Onta\u00f1\u00f3n, C. Alberti, V. Cvicek, Zachary Kenneth Fisher, Philip Pham, Anirudh Ravula, S. Sanghai, Qifan Wang, and L. Yang. 2020. Etc: Encoding long and structured inputs in transformers. In EMNLP. Alexei Baevski and M. Auli. 2019. Adaptive input representations for neural language modeling. ArXiv, abs/1809.10853. I. Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. 2019. Attention augmented convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3285-3294. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 6/34 (Score: 0.97)*\n\n```\n(11) and (12) is generally unknown. So gathering empirical evidences seems the only viable path to answer the first question listed above. The ablation studies by (Khandelwal et al., 2018) examine the effect of context words on a language model. Within the context range of about 200 tokens, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. The observation is succinctly summarized by the title of the paper \"sharp nearby, fuzzy far away\". Remarkably, this is in spirit very close to the key insight into the Multilevel Methods. A few recent attention-related studies have explored this direction with some success, such as word-level and sentence-level attentions in (Miculicich et al., 2018; Abreu et al., 2019), and sentence-level and paragraph-level attentions in (Liu and Lapata, 2019). Even though the proposed hierarchical attention in these studies only has two levels, as opposed to ten or more levels typically used by the Multilevel Methods, the reported positive results are quite suggestive. We therefore hypothesize that the same hierarchical low-rank structure as shown in Eq (13) might also hold for the attention matrix in many NLP tasks. And we treat it as the inductive bias in the hierarchical attention mechanism proposed in this paper. As pointed out in (Goyal and Bengio, 2020), inductive biases encourage the learning algorithm to prioritise solutions with certain properties. Hence good benchmark performance delivered by a Transformer-based model with proposed hierarchical attention can be regarded as a positive evidence to support the hierarchical low-rank structure hypothesis. ### 5.2 Informal Exposition of Hierarchical Attention\n\nIn the standard definition of attention in Eq. (3) and (4), there is no preference given to any keys based on the sequence distance between a query and keys. The observation in (Khandelwal et al.,\n2018) clearly suggests that a distance-dependent attention mechanism should be a better alternative. We will take three steps to informally explain the hierarchical attention mechanism. First, the attention matrix blocks for nearby, mid-distance and long-distance attention are separated in section 5.2.1. This is the first step toward the distance-dependent attention mentioned above. Second, a token hierarchy is established in section 5.2.2. Third, the hierarchical attention is constructed in section 5.2.3\n\n### 5.2.1 Attention Partition\n\nConsider a 16-word sentence in Fig. 1. The sentence is partitioned at three segment granularity. This induces a three-level partition of the attention matrix $A$ for the original sequence:\n\n$$\nA=A^{(2)}+A^{(1)}+A^{(0)}\n$$\n\nwhere\n\n$$\nA^{(2)}=\\left[\\begin{array}{c|c}\n0 & A_{12}^{(2)} \\\\\n\\hline A_{21}^{(2)} & 0\n\\end{array}\\right]\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_a848c33bf30dfbc079beg-05.jpg?height=258&width=600&top_left_y=1539&top_left_x=291)\n\n$$\nA^{(0)}=\\left[\\begin{array}{c|c|c|c|c}\nA_{11}^{(0)} & A_{12}^{(0)} & & & \\\\\n\\hline A_{21}^{(0)} & A_{22}^{(0)} & A_{23}^{(0)} & & \\\\\n\\hline & \\ddots & \\ddots & \\ddots & \\\\\n\\hline & & & A_{87}^{(0)} & A_{88}^{(0)}\n\\end{array}\\right]\n$$\n\nNote that the nonzero entries in $A^{(0)}, A^{(1)}$ and $A^{(2)}$ are the same as the corresponding entries of matrix $A$ in Eq.\n```\n\n#### 2. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Avg. Score: 0.88)\n\n*Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Franccois Fleuret*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 1155  (*Influential: 164*)\n\n**TL;DR:** This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.\n\n**Abstract:** Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\n##### *Relevant Chunk: No. 15/28 (Score: 0.88)*\n\n```\n2010. Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. RoBERTa: A robustly optimized BERT pretraining approach, 2020. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In Wallach, H., Larochelle, H., Beygelzimer, A., d' Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 14014-14024. Curran Associates, Inc., 2019. Mnih, A. and Hinton, G. E. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pp. 1081-1088, 2009. Morin, F. and Bengio, Y. Hierarchical probabilistic neural network language model. In Aistats, volume 5, pp. 246252. Citeseer, 2005. Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., and Shlens, J. Stand-alone self-attention in vision models.\n```\n\n#### 3. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.83)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 18/25 (Score: 0.83)*\n\n```\nSmith, and Lingpeng Kong. 2021. Random feature attention. ArXiv, abs/2103.02143. Michael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, S. Baccus, Y. Bengio, Stefano Ermon, and Christopher R\u00e9. 2023. Hyena hierarchy: Towards larger convolutional language models. International Conference on Machine Learning. Zhen Qin, Weixuan Sun, Huicai Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. ArXiv, abs/2202.08791. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers.\n```\n\n#### 4. HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning (Avg. Score: 0.79)\n\n*Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, Sung Ju Hwang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Hierarchically Pruned Attention (HiP) is proposed, a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly, opening up the possibility to many long-context LLM applications previously infeasible.\n\n**Abstract:** In modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering. However, handling long context sequences with LLMs is prohibitively costly due to the conventional attention mechanism's quadratic time and space complexity, and the context window size is limited by the GPU memory. Although recent works have proposed linear and sparse attention mechanisms to address this issue, their real-world applicability is often limited by the need to re-train pre-trained models. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which simultaneously reduces the training and inference time complexity from $O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To this end, we devise a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly. HiP is training-free as it only utilizes the pre-trained attention scores to spot the positions of the top-$k$ most significant elements for each query. Moreover, it ensures that no token is overlooked, unlike the sliding window-based sub-quadratic attention methods, such as StreamingLLM. Extensive experiments on diverse real-world benchmarks demonstrate that HiP significantly reduces prompt (i.e., prefill) and decoding latency and memory usage while maintaining high generation performance with little or no degradation. As HiP allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no additional engineering due to its easy plug-and-play deployment, we believe that our work will have a large practical impact, opening up the possibility to many long-context LLM applications previously infeasible.\n\n##### *Relevant Chunk: No. 1/44 (Score: 0.79)*\n\n```\n# HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning \n\nHeejun Lee ${ }^{1 *}$, Geon Park $^{1 *}$, Youngwan Lee ${ }^{1,2 *}$, Jina Kim ${ }^{1 *}$,<br>Wonyoung Jeong ${ }^{4}$, Myeongjae Jeon ${ }^{3}$, Sung Ju Hwang ${ }^{1,4 \\dagger}$<br>KAIST ${ }^{1}$, ETRI $^{2}$, UNIST $^{3}$, DeepAuto.ai ${ }^{4}$,<br>\\{ainl,geon.park,ywlee88,jinakim\\}@kaist.ac.kr,<br>young@deepauto.ai, mjjeon@unist.ac.kr, sjhwang@kaist.ac.kr\n\n\n#### Abstract\n\nIn modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical attention, dynamic normalization, sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from ArXiv\n\n#### 1. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 2. Bidirectional Context-Aware Hierarchical Attention Network for Document\n  Understanding\n\n*From Search Query: hierarchical attention*\n\n*Jean-Baptiste Remy, Antoine Jean-Pierre Tixier, Michalis Vazirgiannis*\n\n**Abstract:** The Hierarchical Attention Network (HAN) has made great strides, but it\nsuffers a major limitation: at level 1, each sentence is encoded in complete\nisolation. In this work, we propose and compare several modifications of HAN in\nwhich the sentence encoder is able to make context-aware attentional decisions\n(CAHAN). Furthermore, we propose a bidirectional document encoder that\nprocesses the document forwards and backwards, using the preceding and\nfollowing sentences as context. Experiments on three large-scale sentiment and\ntopic classification datasets show that the bidirectional version of CAHAN\noutperforms HAN everywhere, with only a modest increase in computation time.\nWhile results are promising, we expect the superiority of CAHAN to be even more\nevident on tasks requiring a deeper understanding of the input documents, such\nas abstractive summarization. Code is publicly available.\n\n**Published:** 2019-08-16T15:20:04Z  (*Updated: 2019-08-16T15:20:04Z*)\n\n\n\n#### 3. Efficient Social Network Multilingual Classification using Character,\n  POS n-grams and Dynamic Normalization\n\n*From Search Query: dynamic normalization*\n\n*Carlos-Emiliano Gonz\u00e1lez-Gallardo, Juan-Manuel Torres-Moreno, Azucena Montes Rend\u00f3n, Gerardo Sierra*\n\n**Abstract:** In this paper we describe a dynamic normalization process applied to social\nnetwork multilingual documents (Facebook and Twitter) to improve the\nperformance of the Author profiling task for short texts. After the\nnormalization process, $n$-grams of characters and n-grams of POS tags are\nobtained to extract all the possible stylistic information encoded in the\ndocuments (emoticons, character flooding, capital letters, references to other\nusers, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of\nperformance.\n\n**Published:** 2017-02-21T16:26:54Z  (*Updated: 2017-02-21T16:26:54Z*)\n\n\n\n#### 4. Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers\n\n*From Search Query: sparse attention*\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n\n**Published:** 2024-06-24T15:55:59Z  (*Updated: 2024-06-24T15:55:59Z*)\n\n\n\n#### 5. SEA: Sparse Linear Attention with Estimated Attention Mask\n\n*From Search Query: sparse attention*\n\n*Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang*\n\n**Abstract:** The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n\n**Published:** 2023-10-03T03:56:26Z  (*Updated: 2024-03-25T04:04:05Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 2. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 3. Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows\n\n*From Search Query: dynamic normalization*\n\n*Andreas Lehrmann, Greg Mori, Bo Chang, Ruizhi Deng, Marcus A. Brubaker*\n\n**Abstract:** Normalizing flows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation. In this work, we propose a novel type of normalizing flow driven by a differential deformation of the Wiener process. As a result, we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process, such as efficient computation of likelihoods and marginals. Furthermore, our continuous treatment provides a natural framework for irregular time series with an independent arrival process, including straightforward interpolation. We illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior flexibility to variational RNN and latent ODE baselines in a series of experiments on synthetic and real-world data.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-02-24\n\n\n\n#### 4. Dynamic Token Normalization Improves Vision Transformers\n\n*From Search Query: dynamic normalization*\n\n*Ping Luo, Ying Shan, Xiaogang Wang, Xuyuan Xu, Zhaoyang Zhang, Yixiao Ge, Wenqi Shao*\n\n**Abstract:** Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\\%$ - $1.2\\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\\%$ - $3.9\\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\\%$ - $0.8\\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \\url{https://github.com/wqshao126/DTN}\n\n**Conference:** dynamic-token-normalization-improves-vision\n\n**Published:** 2021-12-05\n\n\n\n#### 5. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: sparse attention*\n\n*Bo Dai, Dale Schuurmans, Jure Leskovec, Mengjiao Yang, Zihang Dai, Hanjun Dai, Hongyu Ren*\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-12\n\n\n\n#### 6. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\n\n*From Search Query: sparse attention*\n\n*Lili Qiu, Yuqing Yang, Chin-Yew Lin, Dongsheng Li, Amir H. Abdi, Zhenhua Han, Surin Ahn, Xufang Luo, Qianhui Wu, Chengruidong Zhang, Yucheng Li, Huiqiang Jiang*\n\n**Abstract:** The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\n\n**Published:** 2024-07-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical adaptive attention, dynamic normalization, and sparse attention, here are some key insights and references that can be useful:\n\n## Hierarchical Attention Mechanisms\n\n- Hierarchical attention mechanisms can significantly reduce computational complexity while maintaining or improving performance. For instance, the concept of hierarchical attention is similar to the multi-hierarchical complementary feature interaction network (MHCFIN) described in, where a dual-branch encoder-decoder structure with multi-attention mechanisms captures key features at different levels. This approach can be adapted to language models to handle long-range dependencies efficiently.\n\n## Dynamic Normalization Techniques\n\n- Dynamic normalization techniques are crucial for enhancing model stability and adaptability. The enhanced knowledge graph recommendation algorithm discussed in uses layer normalization and residual connections to stabilize learning and mitigate the vanishing gradient problem. These techniques can be integrated into language models to improve their robustness and ability to handle complex data structures.\n\n## Sparse Attention Mechanisms\n\n- Sparse attention mechanisms are essential for reducing computational complexity while maintaining performance. The concept of dynamic sparse attention with hierarchical attention pruning, as in HiP Attention, can be particularly useful. This approach reduces complexity by selectively attending to significant blocks in the attention map, which is akin to the intrinsic sparse attention mechanism in SeerAttention.\n\n## Key Insights and Detailed Analysis\n\n- **Hierarchical Attention**: Hierarchical attention can capture key features at different levels, similar to the multi-modal feature interaction module in MHCFIN. This can be applied to language models to improve their ability to handle hierarchical structures in text data.\n- **Dynamic Normalization**: Techniques like layer normalization and residual connections can enhance the stability and adaptability of language models, as seen in the enhanced knowledge graph recommendation algorithm.\n- **Sparse Attention**: Mechanisms like HiP Attention and SeerAttention can reduce computational complexity while maintaining performance by selectively attending to significant blocks in the attention map.\n\n## Future Search Plan and Next Steps\n\n- **Implementation and Validation**: Further investigate the implementation and empirical validation of hierarchical attention and dynamic normalization techniques in the context of autoregressive language models.\n- **Additional Literature**: Explore additional literature on sparse and grouped attention mechanisms to refine the proposed design's attention component. For example, studying how linear transformers with learnable kernel functions can enhance in-context learning abilities.\n- **Empirical Studies**: Conduct empirical studies or simulations to test the proposed design's performance and scalability. This could involve comparing the new model against existing state-of-the-art models in terms of perplexity, accuracy in downstream tasks, robustness, and efficiency.\n\n## References\n\n- **MHCFIN**: Wu et al. \"A Multi-Hierarchical Complementary Feature Interaction Network for Multi-Modal Image Reconstruction\".\n  - This paper provides insights into hierarchical attention mechanisms and their application in capturing key features at different levels.\n- **Enhanced Knowledge Graph Recommendation Algorithm**: Zhang et al. \"Enhanced knowledge graph recommendation algorithm based on dynamic attention mechanism\".\n  - This paper discusses the use of dynamic attention mechanisms, layer normalization, and residual connections to enhance model stability and adaptability.\n- **HiP Attention and SeerAttention**: Lee et al. \"HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning\" and Lou et al. \"Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers\".\n  - These papers introduce dynamic sparse attention mechanisms with hierarchical attention pruning, which can be adapted to improve the efficiency and performance of autoregressive language models.\n\n### Precise Citation Style\n\n- Wu, J.; Yang, S.; Wang, X.; Pei, Y.; Wang, S.; Song, C. \"A Multi-Hierarchical Complementary Feature Interaction Network for Multi-Modal Image Reconstruction.\" *MDPI*, 2024, 14(21), 9764. DOI: 10.3390/app14219764.\n- Zhang, Y.; Li, H.; Gao, J.; Xie, M.; Yu, Z. \"Enhanced knowledge graph recommendation algorithm based on dynamic attention mechanism.\" *Nature*, 2024. DOI: 10.1038/s41598-024-74516-z.\n- Lee, H.; Park, G.; Lee, Y.; Kim, J.; Jeong, W.; Jeon, M.; Hwang, S. J. \"HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning.\" *arXiv.org*, 2024.\n- Lou, C.; Jia, Z.; Zheng, Z.; Tu, K. \"Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers.\" *arXiv.org*, 2024.\n\nBy integrating these concepts, researchers can develop more efficient, scalable, and robust autoregressive language models that outperform existing state-of-the-art models.\n",
            "analysis": "1. **Summary of your analysis**:\n   The search results provide insights into hierarchical adaptive attention, dynamic normalization, and sparse attention mechanisms. Hierarchical attention is highlighted for its ability to improve scalability and efficiency, while dynamic normalization techniques enhance model adaptability and gradient stability. Sparse attention mechanisms are crucial for reducing computational complexity while maintaining performance. Integrating these concepts can lead to more efficient and scalable language models.\n\n2. **All useful references with excerpts**:\n   - **H-Transformer-1D**: Describes an efficient hierarchical method to compute attention in the Transformer architecture with linear run time and memory complexity.\n   - **Transformers are RNNs**: Expresses self-attention as a linear dot-product of kernel feature maps, reducing complexity from O(N\u00b2) to O(N).\n   - **Linear Transformers with Learnable Kernel Functions**: Presents a model that enhances in-context learning abilities with a linear transformer and kernel functions.\n   - **HiP Attention**: Proposes a dynamic sparse attention mechanism with hierarchical attention pruning to reduce complexity and maintain performance.\n   - **SeerAttention**: Introduces a hierarchical approach with intrinsic sparse attention, using a learnable gate to identify significant blocks in the attention map.\n\n3. **Key insights and detailed analysis that may help you**:\n   - Hierarchical attention mechanisms can significantly reduce the computational complexity of traditional attention mechanisms while maintaining or improving performance.\n   - Dynamic normalization techniques, such as those discussed in the Enhanced knowledge graph recommendation algorithm, can improve model stability and adaptability.\n   - Sparse attention mechanisms, like those in SeerAttention and HiP Attention, offer promising avenues for reducing computational complexity while maintaining performance.\n   - The integration of hierarchical and sparse attention can enhance model efficiency and performance by capturing key features at different levels while optimizing computational resources.\n\n4. **Future search plan if needed or plan of next steps**:\n   - Further investigate the implementation and empirical validation of hierarchical attention and dynamic normalization techniques.\n   - Explore additional literature on sparse and grouped attention mechanisms to refine the proposed design's attention component.\n   - Conduct empirical studies or simulations to test the proposed design's performance and scalability.\n\n5. **The list of references, use precise citation style**:\n   - Zhu, Z., & Soricut, R. (2021). H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. Annual Meeting of the Association for Computational Linguistics.\n   - Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. International Conference on Machine Learning.\n   - Aksenov, Y., Balagansky, N., Lo Cicero Vaina, S. M., Shaposhnikov, B., Gorbatovski, A., & Gavrilov, D. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv.org.\n   - Lee, H., Park, G., Lee, Y., Kim, J., Jeong, W., Jeon, M., & Hwang, S. J. (2024). HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning. arXiv.org.\n   - Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv.org."
        },
        {
            "ready": false,
            "query": "hierarchical attention, dynamic normalization, sparse attention",
            "detail": "Hierarchical adaptive attention in language models",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nHierarchical adaptive attention in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.98)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 14/34 (Score: 0.99)*\n\n```\nZanchettin. 2019. Hierarchical attentional hybrid neural networks for document classification. ArXiv, abs/1901.06610. Joshua Ainslie, S. Onta\u00f1\u00f3n, C. Alberti, V. Cvicek, Zachary Kenneth Fisher, Philip Pham, Anirudh Ravula, S. Sanghai, Qifan Wang, and L. Yang. 2020. Etc: Encoding long and structured inputs in transformers. In EMNLP. Alexei Baevski and M. Auli. 2019. Adaptive input representations for neural language modeling. ArXiv, abs/1809.10853. I. Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. 2019. Attention augmented convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3285-3294. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 6/34 (Score: 0.97)*\n\n```\n(11) and (12) is generally unknown. So gathering empirical evidences seems the only viable path to answer the first question listed above. The ablation studies by (Khandelwal et al., 2018) examine the effect of context words on a language model. Within the context range of about 200 tokens, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. The observation is succinctly summarized by the title of the paper \"sharp nearby, fuzzy far away\". Remarkably, this is in spirit very close to the key insight into the Multilevel Methods. A few recent attention-related studies have explored this direction with some success, such as word-level and sentence-level attentions in (Miculicich et al., 2018; Abreu et al., 2019), and sentence-level and paragraph-level attentions in (Liu and Lapata, 2019). Even though the proposed hierarchical attention in these studies only has two levels, as opposed to ten or more levels typically used by the Multilevel Methods, the reported positive results are quite suggestive. We therefore hypothesize that the same hierarchical low-rank structure as shown in Eq (13) might also hold for the attention matrix in many NLP tasks. And we treat it as the inductive bias in the hierarchical attention mechanism proposed in this paper. As pointed out in (Goyal and Bengio, 2020), inductive biases encourage the learning algorithm to prioritise solutions with certain properties. Hence good benchmark performance delivered by a Transformer-based model with proposed hierarchical attention can be regarded as a positive evidence to support the hierarchical low-rank structure hypothesis. ### 5.2 Informal Exposition of Hierarchical Attention\n\nIn the standard definition of attention in Eq. (3) and (4), there is no preference given to any keys based on the sequence distance between a query and keys. The observation in (Khandelwal et al.,\n2018) clearly suggests that a distance-dependent attention mechanism should be a better alternative. We will take three steps to informally explain the hierarchical attention mechanism. First, the attention matrix blocks for nearby, mid-distance and long-distance attention are separated in section 5.2.1. This is the first step toward the distance-dependent attention mentioned above. Second, a token hierarchy is established in section 5.2.2. Third, the hierarchical attention is constructed in section 5.2.3\n\n### 5.2.1 Attention Partition\n\nConsider a 16-word sentence in Fig. 1. The sentence is partitioned at three segment granularity. This induces a three-level partition of the attention matrix $A$ for the original sequence:\n\n$$\nA=A^{(2)}+A^{(1)}+A^{(0)}\n$$\n\nwhere\n\n$$\nA^{(2)}=\\left[\\begin{array}{c|c}\n0 & A_{12}^{(2)} \\\\\n\\hline A_{21}^{(2)} & 0\n\\end{array}\\right]\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_a848c33bf30dfbc079beg-05.jpg?height=258&width=600&top_left_y=1539&top_left_x=291)\n\n$$\nA^{(0)}=\\left[\\begin{array}{c|c|c|c|c}\nA_{11}^{(0)} & A_{12}^{(0)} & & & \\\\\n\\hline A_{21}^{(0)} & A_{22}^{(0)} & A_{23}^{(0)} & & \\\\\n\\hline & \\ddots & \\ddots & \\ddots & \\\\\n\\hline & & & A_{87}^{(0)} & A_{88}^{(0)}\n\\end{array}\\right]\n$$\n\nNote that the nonzero entries in $A^{(0)}, A^{(1)}$ and $A^{(2)}$ are the same as the corresponding entries of matrix $A$ in Eq.\n```\n\n#### 2. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Avg. Score: 0.88)\n\n*Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Franccois Fleuret*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 1155  (*Influential: 164*)\n\n**TL;DR:** This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.\n\n**Abstract:** Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\n##### *Relevant Chunk: No. 15/28 (Score: 0.88)*\n\n```\n2010. Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. RoBERTa: A robustly optimized BERT pretraining approach, 2020. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In Wallach, H., Larochelle, H., Beygelzimer, A., d' Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 14014-14024. Curran Associates, Inc., 2019. Mnih, A. and Hinton, G. E. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pp. 1081-1088, 2009. Morin, F. and Bengio, Y. Hierarchical probabilistic neural network language model. In Aistats, volume 5, pp. 246252. Citeseer, 2005. Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., and Shlens, J. Stand-alone self-attention in vision models.\n```\n\n#### 3. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.83)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 18/25 (Score: 0.83)*\n\n```\nSmith, and Lingpeng Kong. 2021. Random feature attention. ArXiv, abs/2103.02143. Michael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, S. Baccus, Y. Bengio, Stefano Ermon, and Christopher R\u00e9. 2023. Hyena hierarchy: Towards larger convolutional language models. International Conference on Machine Learning. Zhen Qin, Weixuan Sun, Huicai Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. ArXiv, abs/2202.08791. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers.\n```\n\n#### 4. HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning (Avg. Score: 0.79)\n\n*Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, Sung Ju Hwang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Hierarchically Pruned Attention (HiP) is proposed, a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly, opening up the possibility to many long-context LLM applications previously infeasible.\n\n**Abstract:** In modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering. However, handling long context sequences with LLMs is prohibitively costly due to the conventional attention mechanism's quadratic time and space complexity, and the context window size is limited by the GPU memory. Although recent works have proposed linear and sparse attention mechanisms to address this issue, their real-world applicability is often limited by the need to re-train pre-trained models. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which simultaneously reduces the training and inference time complexity from $O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To this end, we devise a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly. HiP is training-free as it only utilizes the pre-trained attention scores to spot the positions of the top-$k$ most significant elements for each query. Moreover, it ensures that no token is overlooked, unlike the sliding window-based sub-quadratic attention methods, such as StreamingLLM. Extensive experiments on diverse real-world benchmarks demonstrate that HiP significantly reduces prompt (i.e., prefill) and decoding latency and memory usage while maintaining high generation performance with little or no degradation. As HiP allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no additional engineering due to its easy plug-and-play deployment, we believe that our work will have a large practical impact, opening up the possibility to many long-context LLM applications previously infeasible.\n\n##### *Relevant Chunk: No. 1/44 (Score: 0.79)*\n\n```\n# HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning \n\nHeejun Lee ${ }^{1 *}$, Geon Park $^{1 *}$, Youngwan Lee ${ }^{1,2 *}$, Jina Kim ${ }^{1 *}$,<br>Wonyoung Jeong ${ }^{4}$, Myeongjae Jeon ${ }^{3}$, Sung Ju Hwang ${ }^{1,4 \\dagger}$<br>KAIST ${ }^{1}$, ETRI $^{2}$, UNIST $^{3}$, DeepAuto.ai ${ }^{4}$,<br>\\{ainl,geon.park,ywlee88,jinakim\\}@kaist.ac.kr,<br>young@deepauto.ai, mjjeon@unist.ac.kr, sjhwang@kaist.ac.kr\n\n\n#### Abstract\n\nIn modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical attention, dynamic normalization, sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from ArXiv\n\n#### 1. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 2. Bidirectional Context-Aware Hierarchical Attention Network for Document\n  Understanding\n\n*From Search Query: hierarchical attention*\n\n*Jean-Baptiste Remy, Antoine Jean-Pierre Tixier, Michalis Vazirgiannis*\n\n**Abstract:** The Hierarchical Attention Network (HAN) has made great strides, but it\nsuffers a major limitation: at level 1, each sentence is encoded in complete\nisolation. In this work, we propose and compare several modifications of HAN in\nwhich the sentence encoder is able to make context-aware attentional decisions\n(CAHAN). Furthermore, we propose a bidirectional document encoder that\nprocesses the document forwards and backwards, using the preceding and\nfollowing sentences as context. Experiments on three large-scale sentiment and\ntopic classification datasets show that the bidirectional version of CAHAN\noutperforms HAN everywhere, with only a modest increase in computation time.\nWhile results are promising, we expect the superiority of CAHAN to be even more\nevident on tasks requiring a deeper understanding of the input documents, such\nas abstractive summarization. Code is publicly available.\n\n**Published:** 2019-08-16T15:20:04Z  (*Updated: 2019-08-16T15:20:04Z*)\n\n\n\n#### 3. Efficient Social Network Multilingual Classification using Character,\n  POS n-grams and Dynamic Normalization\n\n*From Search Query: dynamic normalization*\n\n*Carlos-Emiliano Gonz\u00e1lez-Gallardo, Juan-Manuel Torres-Moreno, Azucena Montes Rend\u00f3n, Gerardo Sierra*\n\n**Abstract:** In this paper we describe a dynamic normalization process applied to social\nnetwork multilingual documents (Facebook and Twitter) to improve the\nperformance of the Author profiling task for short texts. After the\nnormalization process, $n$-grams of characters and n-grams of POS tags are\nobtained to extract all the possible stylistic information encoded in the\ndocuments (emoticons, character flooding, capital letters, references to other\nusers, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of\nperformance.\n\n**Published:** 2017-02-21T16:26:54Z  (*Updated: 2017-02-21T16:26:54Z*)\n\n\n\n#### 4. Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers\n\n*From Search Query: sparse attention*\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n\n**Published:** 2024-06-24T15:55:59Z  (*Updated: 2024-06-24T15:55:59Z*)\n\n\n\n#### 5. SEA: Sparse Linear Attention with Estimated Attention Mask\n\n*From Search Query: sparse attention*\n\n*Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang*\n\n**Abstract:** The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n\n**Published:** 2023-10-03T03:56:26Z  (*Updated: 2024-03-25T04:04:05Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 2. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 3. Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows\n\n*From Search Query: dynamic normalization*\n\n*Andreas Lehrmann, Greg Mori, Bo Chang, Ruizhi Deng, Marcus A. Brubaker*\n\n**Abstract:** Normalizing flows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation. In this work, we propose a novel type of normalizing flow driven by a differential deformation of the Wiener process. As a result, we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process, such as efficient computation of likelihoods and marginals. Furthermore, our continuous treatment provides a natural framework for irregular time series with an independent arrival process, including straightforward interpolation. We illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior flexibility to variational RNN and latent ODE baselines in a series of experiments on synthetic and real-world data.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-02-24\n\n\n\n#### 4. Dynamic Token Normalization Improves Vision Transformers\n\n*From Search Query: dynamic normalization*\n\n*Ping Luo, Ying Shan, Xiaogang Wang, Xuyuan Xu, Zhaoyang Zhang, Yixiao Ge, Wenqi Shao*\n\n**Abstract:** Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\\%$ - $1.2\\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\\%$ - $3.9\\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\\%$ - $0.8\\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \\url{https://github.com/wqshao126/DTN}\n\n**Conference:** dynamic-token-normalization-improves-vision\n\n**Published:** 2021-12-05\n\n\n\n#### 5. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: sparse attention*\n\n*Bo Dai, Dale Schuurmans, Jure Leskovec, Mengjiao Yang, Zihang Dai, Hanjun Dai, Hongyu Ren*\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-12\n\n\n\n#### 6. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\n\n*From Search Query: sparse attention*\n\n*Lili Qiu, Yuqing Yang, Chin-Yew Lin, Dongsheng Li, Amir H. Abdi, Zhenhua Han, Surin Ahn, Xufang Luo, Qianhui Wu, Chengruidong Zhang, Yucheng Li, Huiqiang Jiang*\n\n**Abstract:** The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\n\n**Published:** 2024-07-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on hierarchical adaptive attention, dynamic normalization, and sparse attention mechanisms is crucial. Here are the key insights and references that can help achieve this goal:\n\n## Hierarchical Attention\nHierarchical attention mechanisms can significantly reduce computational complexity while maintaining or improving performance. For instance:\n- **SeerAttention** introduces a hierarchical approach with intrinsic sparse attention, using a learnable gate to identify significant blocks in the attention map. This method enhances efficiency for long-context language models by adaptively selecting important blocks and skipping unimportant ones.\n\n## Dynamic Normalization\nDynamic normalization techniques are essential for improving model stability and adaptability:\n- The **Enhanced knowledge graph recommendation algorithm** uses dynamic attention mechanisms along with layer normalization and residual connections. These techniques stabilize learning, mitigate the vanishing gradient problem, and enhance the model's capability to process complex data structures.\n\n## Sparse Attention\nSparse attention mechanisms are vital for reducing computational complexity:\n- **SeerAttention** leverages the intrinsic sparsity in attention maps, using a learnable gate to adaptively identify significant blocks. This approach maintains near-lossless accuracy even at high sparsity levels and is particularly effective for long-context language models.\n- **HiP Attention** proposes a dynamic sparse attention mechanism with hierarchical attention pruning, which reduces complexity and maintains performance. This method involves sparse sub-quadratic attention that is efficient and scalable (though not directly cited, the concept aligns with the need for hierarchical and sparse attention).\n\n## Integration and Future Directions\nIntegrating hierarchical and sparse attention mechanisms can enhance model efficiency and performance:\n- Combining hierarchical attention with dynamic normalization and sparse attention can capture key features at different levels while optimizing computational resources. For example, using a hierarchical structure to reduce the dimensionality of the attention space and then applying sparse attention mechanisms to further reduce computational complexity.\n- Further research should focus on the empirical validation of these integrated mechanisms. Conducting experiments to test the performance and scalability of models incorporating these features would be crucial.\n\n## Key References\n- **SeerAttention**: Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs. arXiv.org.\n- **Enhanced knowledge graph recommendation algorithm**: Nature article on dynamic attention mechanism and layer normalization for knowledge graph recommendation.\n- **HiP Attention**: While not directly cited here, the concept is similar to what is proposed in other sparse attention mechanisms. For a similar concept, see: Lee, H., Park, G., Lee, Y., Kim, J., Jeong, W., Jeon, M., & Hwang, S. J. (2024). HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning. arXiv.org.\n\n## Future Search Plan\n- Investigate the implementation details of hierarchical attention mechanisms, such as H-Transformer-1D, and how they can be integrated with sparse attention.\n- Explore additional literature on dynamic normalization techniques to enhance model stability and adaptability.\n- Conduct empirical studies or simulations to test the performance and scalability of the proposed integrated design.\n\nBy focusing on these areas, researchers can develop more efficient, scalable, and performant autoregressive language models.\n",
            "analysis": "1. **Summary of your analysis**:\n   The search results provide insights into hierarchical adaptive attention, dynamic normalization, and sparse attention mechanisms. Hierarchical attention is highlighted for its ability to improve scalability and efficiency, while dynamic normalization techniques enhance model adaptability and gradient stability. Sparse attention mechanisms are crucial for reducing computational complexity while maintaining performance. Integrating these concepts can lead to more efficient and scalable language models.\n\n2. **All useful references with excerpts**:\n   - **H-Transformer-1D**: Describes an efficient hierarchical method to compute attention in the Transformer architecture with linear run time and memory complexity.\n   - **Transformers are RNNs**: Expresses self-attention as a linear dot-product of kernel feature maps, reducing complexity from O(N\u00b2) to O(N).\n   - **Linear Transformers with Learnable Kernel Functions**: Presents a model that enhances in-context learning abilities with a linear transformer and kernel functions.\n   - **HiP Attention**: Proposes a dynamic sparse attention mechanism with hierarchical attention pruning to reduce complexity and maintain performance.\n   - **SeerAttention**: Introduces a hierarchical approach with intrinsic sparse attention, using a learnable gate to identify significant blocks in the attention map.\n\n3. **Key insights and detailed analysis that may help you**:\n   - Hierarchical attention mechanisms can significantly reduce the computational complexity of traditional attention mechanisms while maintaining or improving performance.\n   - Dynamic normalization techniques, such as those discussed in the Enhanced knowledge graph recommendation algorithm, can improve model stability and adaptability.\n   - Sparse attention mechanisms, like those in SeerAttention and HiP Attention, offer promising avenues for reducing computational complexity while maintaining performance.\n   - The integration of hierarchical and sparse attention can enhance model efficiency and performance by capturing key features at different levels while optimizing computational resources.\n\n4. **Future search plan if needed or plan of next steps**:\n   - Further investigate the implementation and empirical validation of hierarchical attention and dynamic normalization techniques.\n   - Explore additional literature on sparse and grouped attention mechanisms to refine the proposed design's attention component.\n   - Conduct empirical studies or simulations to test the proposed design's performance and scalability.\n\n5. **The list of references, use precise citation style**:\n   - Zhu, Z., & Soricut, R. (2021). H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. Annual Meeting of the Association for Computational Linguistics.\n   - Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. International Conference on Machine Learning.\n   - Aksenov, Y., Balagansky, N., Lo Cicero Vaina, S. M., Shaposhnikov, B., Gorbatovski, A., & Gavrilov, D. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv.org.\n   - Lee, H., Park, G., Lee, Y., Kim, J., Jeong, W., Jeon, M., & Hwang, S. J. (2024). HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning. arXiv.org.\n   - Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv.org."
        },
        {
            "ready": true,
            "query": "hierarchical attention, dynamic normalization, sparse attention",
            "detail": "Hierarchical adaptive attention in language models",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nHierarchical adaptive attention in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.98)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 14/34 (Score: 0.99)*\n\n```\nZanchettin. 2019. Hierarchical attentional hybrid neural networks for document classification. ArXiv, abs/1901.06610. Joshua Ainslie, S. Onta\u00f1\u00f3n, C. Alberti, V. Cvicek, Zachary Kenneth Fisher, Philip Pham, Anirudh Ravula, S. Sanghai, Qifan Wang, and L. Yang. 2020. Etc: Encoding long and structured inputs in transformers. In EMNLP. Alexei Baevski and M. Auli. 2019. Adaptive input representations for neural language modeling. ArXiv, abs/1809.10853. I. Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. 2019. Attention augmented convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3285-3294. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 6/34 (Score: 0.97)*\n\n```\n(11) and (12) is generally unknown. So gathering empirical evidences seems the only viable path to answer the first question listed above. The ablation studies by (Khandelwal et al., 2018) examine the effect of context words on a language model. Within the context range of about 200 tokens, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. The observation is succinctly summarized by the title of the paper \"sharp nearby, fuzzy far away\". Remarkably, this is in spirit very close to the key insight into the Multilevel Methods. A few recent attention-related studies have explored this direction with some success, such as word-level and sentence-level attentions in (Miculicich et al., 2018; Abreu et al., 2019), and sentence-level and paragraph-level attentions in (Liu and Lapata, 2019). Even though the proposed hierarchical attention in these studies only has two levels, as opposed to ten or more levels typically used by the Multilevel Methods, the reported positive results are quite suggestive. We therefore hypothesize that the same hierarchical low-rank structure as shown in Eq (13) might also hold for the attention matrix in many NLP tasks. And we treat it as the inductive bias in the hierarchical attention mechanism proposed in this paper. As pointed out in (Goyal and Bengio, 2020), inductive biases encourage the learning algorithm to prioritise solutions with certain properties. Hence good benchmark performance delivered by a Transformer-based model with proposed hierarchical attention can be regarded as a positive evidence to support the hierarchical low-rank structure hypothesis. ### 5.2 Informal Exposition of Hierarchical Attention\n\nIn the standard definition of attention in Eq. (3) and (4), there is no preference given to any keys based on the sequence distance between a query and keys. The observation in (Khandelwal et al.,\n2018) clearly suggests that a distance-dependent attention mechanism should be a better alternative. We will take three steps to informally explain the hierarchical attention mechanism. First, the attention matrix blocks for nearby, mid-distance and long-distance attention are separated in section 5.2.1. This is the first step toward the distance-dependent attention mentioned above. Second, a token hierarchy is established in section 5.2.2. Third, the hierarchical attention is constructed in section 5.2.3\n\n### 5.2.1 Attention Partition\n\nConsider a 16-word sentence in Fig. 1. The sentence is partitioned at three segment granularity. This induces a three-level partition of the attention matrix $A$ for the original sequence:\n\n$$\nA=A^{(2)}+A^{(1)}+A^{(0)}\n$$\n\nwhere\n\n$$\nA^{(2)}=\\left[\\begin{array}{c|c}\n0 & A_{12}^{(2)} \\\\\n\\hline A_{21}^{(2)} & 0\n\\end{array}\\right]\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_a848c33bf30dfbc079beg-05.jpg?height=258&width=600&top_left_y=1539&top_left_x=291)\n\n$$\nA^{(0)}=\\left[\\begin{array}{c|c|c|c|c}\nA_{11}^{(0)} & A_{12}^{(0)} & & & \\\\\n\\hline A_{21}^{(0)} & A_{22}^{(0)} & A_{23}^{(0)} & & \\\\\n\\hline & \\ddots & \\ddots & \\ddots & \\\\\n\\hline & & & A_{87}^{(0)} & A_{88}^{(0)}\n\\end{array}\\right]\n$$\n\nNote that the nonzero entries in $A^{(0)}, A^{(1)}$ and $A^{(2)}$ are the same as the corresponding entries of matrix $A$ in Eq.\n```\n\n#### 2. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Avg. Score: 0.88)\n\n*Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Franccois Fleuret*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 1155  (*Influential: 164*)\n\n**TL;DR:** This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.\n\n**Abstract:** Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\n##### *Relevant Chunk: No. 15/28 (Score: 0.88)*\n\n```\n2010. Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. RoBERTa: A robustly optimized BERT pretraining approach, 2020. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In Wallach, H., Larochelle, H., Beygelzimer, A., d' Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 14014-14024. Curran Associates, Inc., 2019. Mnih, A. and Hinton, G. E. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pp. 1081-1088, 2009. Morin, F. and Bengio, Y. Hierarchical probabilistic neural network language model. In Aistats, volume 5, pp. 246252. Citeseer, 2005. Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., and Shlens, J. Stand-alone self-attention in vision models.\n```\n\n#### 3. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.83)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 18/25 (Score: 0.83)*\n\n```\nSmith, and Lingpeng Kong. 2021. Random feature attention. ArXiv, abs/2103.02143. Michael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, S. Baccus, Y. Bengio, Stefano Ermon, and Christopher R\u00e9. 2023. Hyena hierarchy: Towards larger convolutional language models. International Conference on Machine Learning. Zhen Qin, Weixuan Sun, Huicai Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. ArXiv, abs/2202.08791. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers.\n```\n\n#### 4. HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning (Avg. Score: 0.79)\n\n*Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, Sung Ju Hwang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Hierarchically Pruned Attention (HiP) is proposed, a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly, opening up the possibility to many long-context LLM applications previously infeasible.\n\n**Abstract:** In modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering. However, handling long context sequences with LLMs is prohibitively costly due to the conventional attention mechanism's quadratic time and space complexity, and the context window size is limited by the GPU memory. Although recent works have proposed linear and sparse attention mechanisms to address this issue, their real-world applicability is often limited by the need to re-train pre-trained models. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which simultaneously reduces the training and inference time complexity from $O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To this end, we devise a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly. HiP is training-free as it only utilizes the pre-trained attention scores to spot the positions of the top-$k$ most significant elements for each query. Moreover, it ensures that no token is overlooked, unlike the sliding window-based sub-quadratic attention methods, such as StreamingLLM. Extensive experiments on diverse real-world benchmarks demonstrate that HiP significantly reduces prompt (i.e., prefill) and decoding latency and memory usage while maintaining high generation performance with little or no degradation. As HiP allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no additional engineering due to its easy plug-and-play deployment, we believe that our work will have a large practical impact, opening up the possibility to many long-context LLM applications previously infeasible.\n\n##### *Relevant Chunk: No. 1/44 (Score: 0.79)*\n\n```\n# HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning \n\nHeejun Lee ${ }^{1 *}$, Geon Park $^{1 *}$, Youngwan Lee ${ }^{1,2 *}$, Jina Kim ${ }^{1 *}$,<br>Wonyoung Jeong ${ }^{4}$, Myeongjae Jeon ${ }^{3}$, Sung Ju Hwang ${ }^{1,4 \\dagger}$<br>KAIST ${ }^{1}$, ETRI $^{2}$, UNIST $^{3}$, DeepAuto.ai ${ }^{4}$,<br>\\{ainl,geon.park,ywlee88,jinakim\\}@kaist.ac.kr,<br>young@deepauto.ai, mjjeon@unist.ac.kr, sjhwang@kaist.ac.kr\n\n\n#### Abstract\n\nIn modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical attention, dynamic normalization, sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from ArXiv\n\n#### 1. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 2. Bidirectional Context-Aware Hierarchical Attention Network for Document\n  Understanding\n\n*From Search Query: hierarchical attention*\n\n*Jean-Baptiste Remy, Antoine Jean-Pierre Tixier, Michalis Vazirgiannis*\n\n**Abstract:** The Hierarchical Attention Network (HAN) has made great strides, but it\nsuffers a major limitation: at level 1, each sentence is encoded in complete\nisolation. In this work, we propose and compare several modifications of HAN in\nwhich the sentence encoder is able to make context-aware attentional decisions\n(CAHAN). Furthermore, we propose a bidirectional document encoder that\nprocesses the document forwards and backwards, using the preceding and\nfollowing sentences as context. Experiments on three large-scale sentiment and\ntopic classification datasets show that the bidirectional version of CAHAN\noutperforms HAN everywhere, with only a modest increase in computation time.\nWhile results are promising, we expect the superiority of CAHAN to be even more\nevident on tasks requiring a deeper understanding of the input documents, such\nas abstractive summarization. Code is publicly available.\n\n**Published:** 2019-08-16T15:20:04Z  (*Updated: 2019-08-16T15:20:04Z*)\n\n\n\n#### 3. Efficient Social Network Multilingual Classification using Character,\n  POS n-grams and Dynamic Normalization\n\n*From Search Query: dynamic normalization*\n\n*Carlos-Emiliano Gonz\u00e1lez-Gallardo, Juan-Manuel Torres-Moreno, Azucena Montes Rend\u00f3n, Gerardo Sierra*\n\n**Abstract:** In this paper we describe a dynamic normalization process applied to social\nnetwork multilingual documents (Facebook and Twitter) to improve the\nperformance of the Author profiling task for short texts. After the\nnormalization process, $n$-grams of characters and n-grams of POS tags are\nobtained to extract all the possible stylistic information encoded in the\ndocuments (emoticons, character flooding, capital letters, references to other\nusers, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of\nperformance.\n\n**Published:** 2017-02-21T16:26:54Z  (*Updated: 2017-02-21T16:26:54Z*)\n\n\n\n#### 4. Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers\n\n*From Search Query: sparse attention*\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n\n**Published:** 2024-06-24T15:55:59Z  (*Updated: 2024-06-24T15:55:59Z*)\n\n\n\n#### 5. SEA: Sparse Linear Attention with Estimated Attention Mask\n\n*From Search Query: sparse attention*\n\n*Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang*\n\n**Abstract:** The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n\n**Published:** 2023-10-03T03:56:26Z  (*Updated: 2024-03-25T04:04:05Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 2. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 3. Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows\n\n*From Search Query: dynamic normalization*\n\n*Andreas Lehrmann, Greg Mori, Bo Chang, Ruizhi Deng, Marcus A. Brubaker*\n\n**Abstract:** Normalizing flows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation. In this work, we propose a novel type of normalizing flow driven by a differential deformation of the Wiener process. As a result, we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process, such as efficient computation of likelihoods and marginals. Furthermore, our continuous treatment provides a natural framework for irregular time series with an independent arrival process, including straightforward interpolation. We illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior flexibility to variational RNN and latent ODE baselines in a series of experiments on synthetic and real-world data.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-02-24\n\n\n\n#### 4. Dynamic Token Normalization Improves Vision Transformers\n\n*From Search Query: dynamic normalization*\n\n*Ping Luo, Ying Shan, Xiaogang Wang, Xuyuan Xu, Zhaoyang Zhang, Yixiao Ge, Wenqi Shao*\n\n**Abstract:** Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\\%$ - $1.2\\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\\%$ - $3.9\\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\\%$ - $0.8\\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \\url{https://github.com/wqshao126/DTN}\n\n**Conference:** dynamic-token-normalization-improves-vision\n\n**Published:** 2021-12-05\n\n\n\n#### 5. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: sparse attention*\n\n*Bo Dai, Dale Schuurmans, Jure Leskovec, Mengjiao Yang, Zihang Dai, Hanjun Dai, Hongyu Ren*\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-12\n\n\n\n#### 6. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\n\n*From Search Query: sparse attention*\n\n*Lili Qiu, Yuqing Yang, Chin-Yew Lin, Dongsheng Li, Amir H. Abdi, Zhenhua Han, Surin Ahn, Xufang Luo, Qianhui Wu, Chengruidong Zhang, Yucheng Li, Huiqiang Jiang*\n\n**Abstract:** The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\n\n**Published:** 2024-07-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on hierarchical adaptive attention, dynamic normalization, and sparse attention mechanisms is crucial. Here are the key insights and references that can help achieve this goal:\n\n## Hierarchical Attention Mechanisms\n- **SeerAttention** introduces a hierarchical approach with intrinsic sparse attention, using a learnable gate to identify significant blocks in the attention map. This method enhances efficiency and scalability, especially for long-context language models, by adaptively selecting important blocks and reducing computational complexity.\n\n## Dynamic Normalization Techniques\n- The **Enhanced knowledge graph recommendation algorithm** highlights the use of dynamic attention mechanisms combined with layer normalization and residual connections. These techniques improve model stability and adaptability, particularly when dealing with complex and sparse data structures. This approach can be adapted to language models to enhance their robustness and performance.\n\n## Sparse Attention Mechanisms\n- **SeerAttention** also leverages sparse attention by learning the intrinsic sparsity in attention maps rather than relying on predefined patterns or heuristics. This approach allows for significant efficiency improvements without compromising accuracy, especially in long-context settings.\n- **HiP Attention** proposes a sparse sub-quadratic attention mechanism with hierarchical attention pruning. This method reduces computational complexity while maintaining performance, making it a viable option for efficient autoregressive language models (though not directly cited, the concept aligns with the needed improvements).\n\n## Integration and Future Directions\n- Integrating hierarchical and sparse attention mechanisms can significantly enhance model efficiency and performance. For instance, combining the hierarchical approach of **SeerAttention** with the dynamic normalization techniques from the knowledge graph recommendation algorithm could lead to a more robust and scalable language model.\n- Further research should focus on the empirical validation of these integrated mechanisms. Conducting experiments on benchmarks like LongBench and evaluating the model's performance in terms of perplexity, accuracy in downstream tasks, and robustness to variant inputs would be essential.\n\n## Key References\n- **SeerAttention**: Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs. arXiv.org.\n- **Enhanced knowledge graph recommendation algorithm**: The dynamic attention mechanism and normalization techniques discussed here can be adapted for language models. [Author's Name], [Year]. Enhanced knowledge graph recommendation algorithm based on dynamic attention mechanism and contrastive learning. *Nature*, [Volume], [Pages].\n\n## Future Search Plan\n- Investigate other hierarchical attention mechanisms, such as **H-Transformer-1D**, which computes attention with linear run time and memory complexity, to see if they can be integrated with sparse attention methods (conceptually similar, though not directly cited).\n- Explore additional literature on sparse and grouped attention mechanisms to refine the proposed design's attention component.\n- Conduct empirical studies or simulations to test the proposed design's performance and scalability in various contexts, including long-context fine-tuning and downstream tasks.\n\nBy combining these insights, researchers can develop an autoregressive language model that is more efficient, scalable, and robust, ultimately achieving better overall performance with more data and larger models.\n",
            "analysis": "1. **Summary of your analysis**:\n   The search results provide a comprehensive overview of hierarchical adaptive attention, dynamic normalization, and sparse attention mechanisms. Hierarchical attention is highlighted for its ability to reduce computational complexity while maintaining performance. Dynamic normalization techniques enhance model stability and adaptability, and sparse attention mechanisms are crucial for reducing computational complexity. Integrating these concepts can lead to more efficient and scalable language models.\n\n2. **All useful references with excerpts**:\n   - **H-Transformer-1D**: Describes an efficient hierarchical method to compute attention in the Transformer architecture with linear run time and memory complexity.\n   - **Transformers are RNNs**: Expresses self-attention as a linear dot-product of kernel feature maps, reducing complexity from O(N\u00b2) to O(N).\n   - **Linear Transformers with Learnable Kernel Functions**: Presents a model that enhances in-context learning abilities with a linear transformer and kernel functions.\n   - **HiP Attention**: Proposes a dynamic sparse attention mechanism with hierarchical attention pruning to reduce complexity and maintain performance.\n   - **SeerAttention**: Introduces a hierarchical approach with intrinsic sparse attention, using a learnable gate to identify significant blocks in the attention map.\n\n3. **Key insights and detailed analysis that may help you**:\n   - Hierarchical attention mechanisms can significantly reduce the computational complexity of traditional attention mechanisms while maintaining or improving performance.\n   - Dynamic normalization techniques, such as those discussed in the Enhanced knowledge graph recommendation algorithm, can improve model stability and adaptability.\n   - Sparse attention mechanisms, like those in SeerAttention and HiP Attention, offer promising avenues for reducing computational complexity while maintaining performance.\n   - The integration of hierarchical and sparse attention can enhance model efficiency and performance by capturing key features at different levels while optimizing computational resources.\n\n4. **Future search plan if needed or plan of next steps**:\n   - Further investigate the implementation and empirical validation of hierarchical attention and dynamic normalization techniques.\n   - Explore additional literature on sparse and grouped attention mechanisms to refine the proposed design's attention component.\n   - Conduct empirical studies or simulations to test the proposed design's performance and scalability.\n\n5. **The list of references, use precise citation style**:\n   - Zhu, Z., & Soricut, R. (2021). H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. Annual Meeting of the Association for Computational Linguistics.\n   - Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. International Conference on Machine Learning.\n   - Aksenov, Y., Balagansky, N., Lo Cicero Vaina, S. M., Shaposhnikov, B., Gorbatovski, A., & Gavrilov, D. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv.org.\n   - Lee, H., Park, G., Lee, Y., Kim, J., Jeong, W., Jeon, M., & Hwang, S. J. (2024). HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning. arXiv.org.\n   - Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs. arXiv.org."
        },
        {
            "ready": false,
            "query": "\"training stability sparse transformers\", \"dynamic normalization attention\", \"hardware optimization hierarchical\"",
            "detail": "Search for papers and technical documents discussing:\n\n1. Training stability in sparse and hierarchical transformers:\n- Gradient flow analysis\n- Convergence properties\n- Normalization techniques\n- Practical solutions\n\n2. Dynamic normalization in attention mechanisms:\n- Adaptive parameter generation\n- Integration with sparse attention\n- Stability considerations\n- Performance impact\n\n3. Hardware optimization for hierarchical architectures:\n- Memory access patterns\n- Cache utilization\n- Parallel processing\n- Implementation strategies",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents discussing:\n\n1. Training stability in sparse and hierarchical transformers:\n- Gradient flow analysis\n- Convergence properties\n- Normalization techniques\n- Practical solutions\n\n2. Dynamic normalization in attention mechanisms:\n- Adaptive parameter generation\n- Integration with sparse attention\n- Stability considerations\n- Performance impact\n\n3. Hardware optimization for hierarchical architectures:\n- Memory access patterns\n- Cache utilization\n- Parallel processing\n- Implementation strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.26)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.26)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Staircase Attention for Recurrent Processing of Sequences (Avg. Score: 0.08)\n\n*Da Ju, Stephen Roller, Sainbayar Sukhbaatar, J. Weston*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 10  (*Influential: 2*)\n\n**TL;DR:** Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, and to provide improved modeling power compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.\n\n**Abstract:** Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.\n\n##### *Relevant Chunk: No. 9/21 (Score: 0.08)*\n\n```\narXiv preprint arXiv:1804.09849, 2018. [14] Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for transformer. arXiv preprint arXiv:1904.03092, 2019. [15] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL (1), pages 2978-2988. Association for Computational Linguistics, 2019. [16] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [17] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019. [18] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [19] Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. [20] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021. [21] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models.\n```\n\n#### 3. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.06)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 24/32 (Score: 0.06)*\n\n```\n[RSVG20b] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. International Conference On Topology, Algebra And Categories In Logic, 2020. $\\left[\\mathrm{RZW}^{+}\\right.$22] Liliang Ren, Zixuan Zhang, Han Wang, Clare Voss, ChengXiang Zhai, and Heng Ji. Language model pre-training with sparse latent typing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 14801494, Abu Dhabi, United Arab Emirates, dec 2022. Association for Computational Linguistics. [SGBJ19] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019. [SJP+ 21] Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur D.\n```\n\n#### 4. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.05)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 13/30 (Score: 0.05)*\n\n```\narXiv preprint arXiv:2304.07327, 2023. Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers, 2022. Heejun Lee, Minki Kang, Youngwan Lee, and Sung Ju Hwang. Sparse token transformer with attention back tracking. In The Eleventh International Conference on Learning Representations, 2023. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019. Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022. Andr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo. Sparse and continuous attention mechanisms. Advances in Neural Information Processing Systems, 33:20989-21001, 2020. Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.\n```\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.04)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"training stability sparse transformers\", \"dynamic normalization attention\", \"hardware optimization hierarchical\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from ArXiv\n\n#### 1. Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT\n\n*From Search Query: \"training stability sparse transformers\"*\n\n*James Lee-Thorp, Joshua Ainslie*\n\n**Abstract:** We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the\nspeed and stability of linear, mixing transformations to design the Sparse\nMixer encoder model. Sparse Mixer slightly outperforms (<1%) BERT on GLUE and\nSuperGLUE, but more importantly trains 65% faster and runs inference 61%\nfaster. We also present a faster variant, prosaically named Fast Sparse Mixer,\nthat marginally underperforms BERT on SuperGLUE, but trains and runs nearly\ntwice as fast. We justify the design of these two models by carefully ablating\nthrough various mixing mechanisms, MoE configurations and hyperparameters.\nSparse Mixer overcomes many of the latency and stability concerns of MoE models\nand offers the prospect of serving sparse student models, without resorting to\ndistilling them to dense variants.\n\n**Published:** 2022-05-24T23:08:54Z  (*Updated: 2022-10-13T17:47:14Z*)\n\n\n\n#### 2. Tracking Universal Features Through Fine-Tuning and Model Merging\n\n*From Search Query: \"training stability sparse transformers\"*\n\n*Niels Horn, Desmond Elliott*\n\n**Abstract:** We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.\n\n**Published:** 2024-10-16T09:18:39Z  (*Updated: 2024-10-16T09:18:39Z*)\n\n\n\n#### 3. Can Neural Image Captioning be Controlled via Forced Attention?\n\n*From Search Query: \"dynamic normalization attention\"*\n\n*Philipp Sadler, Tatjana Scheffler, David Schlangen*\n\n**Abstract:** Learned dynamic weighting of the conditioning signal (attention) has been\nshown to improve neural language generation in a variety of settings. The\nweights applied when generating a particular output sequence have also been\nviewed as providing a potentially explanatory insight into the internal\nworkings of the generator. In this paper, we reverse the direction of this\nconnection and ask whether through the control of the attention of the model we\ncan control its output. Specifically, we take a standard neural image\ncaptioning model that uses attention, and fix the attention to pre-determined\nareas in the image. We evaluate whether the resulting output is more likely to\nmention the class of the object in that area than the normally generated\ncaption. We introduce three effective methods to control the attention and find\nthat these are producing expected results in up to 28.56% of the cases.\n\n**Published:** 2019-11-10T14:00:27Z  (*Updated: 2019-11-10T14:00:27Z*)\n\n\n\n#### 4. Clustering in pure-attention hardmax transformers and its role in\n  sentiment analysis\n\n*From Search Query: \"dynamic normalization attention\"*\n\n*Albert Alcalde, Giovanni Fantuzzi, Enrique Zuazua*\n\n**Abstract:** Transformers are extremely successful machine learning models whose\nmathematical properties remain poorly understood. Here, we rigorously\ncharacterize the behavior of transformers with hardmax self-attention and\nnormalization sublayers as the number of layers tends to infinity. By viewing\nsuch transformers as discrete-time dynamical systems describing the evolution\nof points in a Euclidean space, and thanks to a geometric interpretation of the\nself-attention mechanism based on hyperplane separation, we show that the\ntransformer inputs asymptotically converge to a clustered equilibrium\ndetermined by special points called leaders. We then leverage this theoretical\nunderstanding to solve sentiment analysis problems from language processing\nusing a fully interpretable transformer model, which effectively captures\n`context' by clustering meaningless words around leader words carrying the most\nmeaning. Finally, we outline remaining challenges to bridge the gap between the\nmathematical analysis of transformers and their real-life implementation.\n\n**Published:** 2024-06-26T16:13:35Z  (*Updated: 2024-06-26T16:13:35Z*)\n\n\n\n#### 5. FastMoE: A Fast Mixture-of-Expert Training System\n\n*From Search Query: \"hardware optimization hierarchical\"*\n\n*Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, Jie Tang*\n\n**Abstract:** Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of\nlanguage model to trillions of parameters. However, training trillion-scale MoE\nrequires algorithm and system co-design for a well-tuned high performance\ndistributed training system. Unfortunately, the only existing platform that\nmeets the requirements strongly depends on Google's hardware (TPU) and software\n(Mesh Tensorflow) stack, and is not open and available to the public,\nespecially GPU and PyTorch communities.\n  In this paper, we present FastMoE, a distributed MoE training system based on\nPyTorch with common accelerators. The system provides a hierarchical interface\nfor both flexible model design and easy adaption to different applications,\nsuch as Transformer-XL and Megatron-LM. Different from direct implementation of\nMoE models using PyTorch, the training speed is highly optimized in FastMoE by\nsophisticated high-performance acceleration skills. The system supports placing\ndifferent experts on multiple GPUs across multiple nodes, enabling enlarging\nthe number of experts linearly against the number of GPUs. The source of\nFastMoE is available at https://github.com/laekov/fastmoe under Apache-2\nlicense.\n\n**Published:** 2021-03-24T15:27:15Z  (*Updated: 2021-03-24T15:27:15Z*)\n\n\n\n#### 6. ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training\n  Quantization Framework for W8A8 Transformers\n\n*From Search Query: \"hardware optimization hierarchical\"*\n\n*Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He*\n\n**Abstract:** Quantization techniques are pivotal in reducing the memory and computational\ndemands of deep neural network inference. Existing solutions, such as\nZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook\ncrucial memory-bounded operators and the complexities of per-token\nquantization. Addressing these gaps, we present a novel, fully\nhardware-enhanced robust optimized post-training W8A8 quantization framework,\nZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and\ncompute-intensive operators, aiming for optimal hardware performance.\nAdditionally, it offers flexibility by allowing specific INT8 modules to switch\nto FP16/BF16 mode, enhancing accuracy.\n\n**Published:** 2023-10-26T18:34:41Z  (*Updated: 2023-10-26T18:34:41Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: \"training stability sparse transformers\"*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 2. Self-Attention Generative Adversarial Networks\n\n*From Search Query: \"dynamic normalization attention\"*\n\n*Han Zhang, Augustus Odena, Ian Goodfellow, Dimitris Metaxas*\n\n**Abstract:** In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.\n\n**Conference:** self-attention-generative-adversarial-1\n\n**Published:** 2018-05-21\n\n\n\n#### 3. TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up\n\n*From Search Query: \"dynamic normalization attention\"*\n\n*Zhangyang Wang, Shiyu Chang, Yifan Jiang*\n\n**Abstract:** The recent explosive interest on transformers has suggested their potential to become powerful \"universal\" models for computer vision tasks, such as classification, detection, and segmentation. While those attempts mainly study the discriminative models, we explore transformers on some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs). Our goal is to conduct the first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed TransGAN, consists of a memory-friendly transformer-based generator that progressively increases feature resolution, and correspondingly a multi-scale discriminator to capture simultaneously semantic contexts and low-level textures. On top of them, we introduce the new module of grid self-attention for alleviating the memory bottleneck further, in order to scale up TransGAN to high-resolution generation. We also develop a unique training recipe including a series of techniques that can mitigate the training instability issues of TransGAN, such as data augmentation, modified normalization, and relative position encoding. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs using convolutional backbones. Specifically, TransGAN sets new state-of-the-art inception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2. When it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual examples with high fidelity and impressive texture details. In addition, we dive deep into the transformer-based generation models to understand how their behaviors differ from convolutional ones, by visualizing training dynamics. The code is available at https://github.com/VITA-Group/TransGAN.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-02-14\n\n\n\n#### 4. Alpaka - An Abstraction Library for Parallel Kernel Acceleration\n\n*From Search Query: \"hardware optimization hierarchical\"*\n\n*Andreas Kn\u00fcpfer, Ren\u00e9 Widera, Erik Zenker, Wolfgang E. Nagel, Michael Bussmann, Benjamin Worpitz, Guido Juckeland, Axel Huebl*\n\n**Abstract:** Porting applications to new hardware or programming models is a tedious and\nerror prone process. Every help that eases these burdens is saving developer\ntime that can then be invested into the advancement of the application itself\ninstead of preserving the status-quo on a new platform.\n  The Alpaka library defines and implements an abstract hierarchical redundant\nparallelism model. The model exploits parallelism and memory hierarchies on a\nnode at all levels available in current hardware. By doing so, it allows to\nachieve platform and performance portability across various types of\naccelerators by ignoring specific unsupported levels and utilizing only the\nones supported on a specific accelerator. All hardware types (multi- and\nmany-core CPUs, GPUs and other accelerators) are supported for and can be\nprogrammed in the same way. The Alpaka C++ template interface allows for\nstraightforward extension of the library to support other accelerators and\nspecialization of its internals for optimization.\n  Running Alpaka applications on a new (and supported) platform requires the\nchange of only one source code line instead of a lot of \\#ifdefs.\n\n**Published:** 2016-02-26\n\n\n\n#### 5. JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu\n\n*From Search Query: \"hardware optimization hierarchical\"*\n\n*Haoyi Xiong, Dejing Dou, daxiang dong, Zhiwei Zha, Guobao Yang, Wenlin Wang, Guangxing Chen, Hao Xiong, Xiaochao Liao, Jiang Li, Qian Gao, Hao liu*\n\n**Abstract:** In modern internet industries, deep learning based recommender systems have became an indispensable building block for a wide spectrum of applications, such as search engine, news feed, and short video clips. However, it remains challenging to carry the well-trained deep models for online real-time inference serving, with respect to the time-varying web-scale traffics from billions of users, in a cost-effective manner. In this work, we present JIZHI - a Model-as-a-Service system - that per second handles hundreds of millions of online inference requests to huge deep models with more than trillions of sparse parameters, for over twenty real-time recommendation services at Baidu, Inc. In JIZHI, the inference workflow of every recommendation request is transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the pipeline refers to a staged computation or I/O intensive task processor. With traffics of real-time inference requests arrived, each modularized processor can be run in a fully asynchronized way and managed separately. Besides, JIZHI introduces heterogeneous and hierarchical storage to further accelerate the online inference process by reducing unnecessary computations and potential data access latency induced by ultra-sparse model parameters. Moreover, an intelligent resource manager has been deployed to maximize the throughput of JIZHI over the shared infrastructure by searching the optimal resource allocation plan from historical logs and fine-tuning the load shedding policies over intermediate system feedback. Extensive experiments have been done to demonstrate the advantages of JIZHI from the perspectives of end-to-end service latency, system-wide throughput, and resource consumption. JIZHI has helped Baidu saved more than ten million US dollars in hardware and utility costs while handling 200% more traffics without sacrificing inference efficiency.\n\n**Published:** 2021-06-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on sparse and hierarchical transformers, here are some key findings and suggestions based on the provided keywords and analysis:\n\n### Training Stability in Sparse and Hierarchical Transformers\n\n**Gradient Flow Analysis and Convergence Properties:**\n- The paper on V:N:M sparsity for efficient Transformer inference highlights the importance of training stability in sparse Transformers. It suggests that reducing the update frequency of sparse masks can enhance training stability and improve final accuracy. This is achieved through techniques like SR-STE, which suppresses the weights that are masked out, allowing the updated masks to become progressively consistent as training advances.\n\n**Normalization Techniques:**\n- Normalization is crucial for maintaining stability in deep hierarchical networks. The use of techniques like SR-STE and dynamic mask training helps in balancing the exploration and exploitation of masks, which is essential for stable training. For example, the three-staged LoRA training technique proposed in the V:N:M sparsity paper ensures that the masks are updated less frequently to maintain stability.\n\n**Practical Solutions:**\n- The integration of sparse attention mechanisms with hierarchical structures requires careful handling of gradient flow. The SeerAttention mechanism, which learns intrinsic sparse attention, demonstrates near-lossless accuracy even at high sparsity levels during long-context fine-tuning. This approach involves training the attention gate using a small set of calibration data and fine-tuning the entire model, ensuring stable training.\n\n### Dynamic Normalization in Attention Mechanisms\n\n**Adaptive Parameter Generation:**\n- SeerAttention introduces an adaptive attention mechanism where the attention gate is learned using a 2D max-pooled attention map from full attention as ground truth. This allows for flexible usage and adjustment of the Top-k ratio to balance accuracy and efficiency.\n\n**Integration with Sparse Attention:**\n- The SeerAttention mechanism integrates well with sparse attention by learning block-level sparsity. This approach enables the model to adapt to various context lengths and sparsity ratios, maintaining high performance even at high sparsity levels.\n\n**Stability Considerations:**\n- Dynamic normalization in attention mechanisms, such as SeerAttention, involves stabilizing the training process by initializing the attention gate using post-training methods before fine-tuning the entire model. This ensures that the model converges stably and maintains near-lossless performance.\n\n**Performance Impact:**\n- The performance impact of dynamic normalization is significant, as it allows for efficient and adaptive sparse attention. SeerAttention outperforms previous sparse attention methods and maintains high accuracy even with 90% sparsity during long-context fine-tuning.\n\n### Hardware Optimization for Hierarchical Architectures\n\n**Memory Access Patterns and Cache Utilization:**\n- The use of block-sparse computation and FlashAttention techniques can optimize memory access patterns and cache utilization. For instance, the SeerAttention mechanism extends FlashAttention to accommodate a large class of attention sparsity patterns, leading to efficient implementations with no computational complexity overhead.\n\n**Parallel Processing:**\n- Memristor-based hardware accelerators can significantly improve the parallel processing capabilities of transformer networks. The implementation of matrix-matrix multiplication using memristor-based in-memory computing shows a 10\u00d7 acceleration compared to digital counterparts, highlighting the potential for parallel processing optimization.\n\n**Implementation Strategies:**\n- For hierarchical architectures, block-based processing is crucial for memory efficiency. The V:N:M sparsity paper proposes a three-staged LoRA training technique that balances the exploration and exploitation of masks, which is essential for efficient and stable training under memory constraints.\n\nIn summary:\n\n- **Training Stability:** Techniques like SR-STE and dynamic mask training are essential for maintaining stability in sparse and hierarchical transformers. SeerAttention's adaptive attention mechanism also ensures stable training by learning intrinsic sparse attention.\n- **Dynamic Normalization:** SeerAttention's adaptive parameter generation and integration with sparse attention enable flexible and efficient attention mechanisms. This approach ensures stable training and high performance even at high sparsity levels.\n- **Hardware Optimization:** Memristor-based accelerators and block-sparse computation techniques can significantly optimize memory access patterns, cache utilization, and parallel processing for hierarchical architectures.\n\nThese insights and techniques can help researchers improve the design of autoregressive language models by enhancing training stability, integrating dynamic normalization with sparse attention, and optimizing hardware for hierarchical architectures.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights into three key aspects of the HierarchicalSparseNorm-GPT proposal:\n\na) Sparse Attention Mechanisms:\n- SPARSEK Attention demonstrates efficient sparse attention with linear complexity\n- Hardware-efficient implementations through block-sparse computation\n- Integration possibilities with hierarchical structures\n- Trade-offs between sparsity and performance\n\nb) Hierarchical Transformers:\n- Hourglass architecture shows benefits of explicit hierarchical structure\n- Importance of proper upsampling and downsampling techniques\n- Memory efficiency through hierarchical processing\n- Stability considerations in deep hierarchical networks\n\nc) Memory Optimization:\n- Flash Attention techniques for sparse patterns\n- Block-based processing for memory efficiency\n- Hardware-aware implementation strategies\n- Cache optimization approaches\n\n2. Useful References with Excerpts:\n\nFrom SPARSEK paper:\n\"SPARSEK Attention offers linear time complexity and constant memory footprint during generation [...] outperforms previous sparse attention methods and provides significant speed improvements\"\n\nFrom Hierarchical Transformers:\n\"Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently\"\n\nFrom Sparse Flash Attention:\n\"extends FlashAttention to accommodate a large class of attention sparsity patterns [...] leads to implementations with no computational complexity overhead\"\n\n3. Key Insights:\n\na) Attention Design:\n- Sparse attention can maintain performance while reducing complexity\n- Hierarchical structures improve efficiency for long sequences\n- Dynamic sparsity patterns outperform static approaches\n- Hardware-aware implementation crucial for real benefits\n\nb) Memory Management:\n- Block-based processing reduces memory overhead\n- Cache optimization important for performance\n- Hardware-specific optimizations necessary\n- Trade-off between memory usage and computation\n\nc) Stability Considerations:\n- Gradient flow in hierarchical structures needs careful handling\n- Normalization crucial for deep hierarchical networks\n- Balance between sparsity and stability required\n- Training dynamics affected by attention patterns\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Training stability in sparse hierarchical networks\n2. Integration of dynamic normalization with sparse attention\n3. Hardware optimization techniques for combined sparse-hierarchical architectures\n\n5. References:\n\n[1] Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv preprint.\n\n[2] Nawrot, P., Tworkowski, S., Tyrolski, M., Kaiser, L., Wu, Y., Szegedy, C., & Michalewski, H. (2021). Hierarchical Transformers Are More Efficient Language Models. NAACL-HLT.\n\n[3] Pagliardini, M., Paliotta, D., Jaggi, M., & Fleuret, F. (2023). Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. arXiv preprint.\n\n[4] Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. Neural Information Processing Systems.\n\n[5] Yuan, Y., Hu, H., Yang, Y., Zheng, N., Peng, H., & Liu, X. (2023). EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention. CVPR."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"hierarchical sparse attention\", \"dynamic layer normalization\", \"adaptive attention\"",
            "detail": "Search for papers and technical documents that discuss:\n\n1. Hierarchical attention mechanisms in transformer architectures, focusing on:\n- Multi-scale attention implementations\n- Sparse attention patterns\n- Adaptive gating mechanisms\n- Memory efficiency optimizations\n\n2. Dynamic and adaptive normalization techniques, particularly:\n- Context-dependent parameter generation\n- Gradient stability in deep networks\n- Integration with attention mechanisms\n- Performance impact on large models\n\n3. Hardware-efficient implementations of:\n- Sparse attention computation\n- Dynamic parameter generation\n- Memory access optimization\n- Training stability solutions",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents that discuss:\n\n1. Hierarchical attention mechanisms in transformer architectures, focusing on:\n- Multi-scale attention implementations\n- Sparse attention patterns\n- Adaptive gating mechanisms\n- Memory efficiency optimizations\n\n2. Dynamic and adaptive normalization techniques, particularly:\n- Context-dependent parameter generation\n- Gradient stability in deep networks\n- Integration with attention mechanisms\n- Performance impact on large models\n\n3. Hardware-efficient implementations of:\n- Sparse attention computation\n- Dynamic parameter generation\n- Memory access optimization\n- Training stability solutions\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Hierarchical Transformers Are More Efficient Language Models (Avg. Score: 0.48)\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, H. Michalewski*\n\n**Published in:** NAACL-HLT (2021)\t**Cited by** 40  (*Influential: 4*)\n\n**TL;DR:** Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.\n\n##### *Relevant Chunk: No. 17/25 (Score: 0.48)*\n\n```\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient content-based sparse attention with routing transformers. Jianlin $\\mathrm{Su}, \\mathrm{Yu} \\mathrm{Lu}$, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, and Y-Lan Boureau. 2020. Multi-scale transformer language models. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers.\n```\n\n#### 2. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.39)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 27/35 (Score: 0.39)*\n\n```\nIn Proceedings of the 2013 Conference on\n\nEmpirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331-335, Florence, Italy. Association for Computational Linguistics. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10183-10192. PMLR. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021b. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. ArXiv preprint, abs/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.\n```\n\n#### 3. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.38)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 28/38 (Score: 0.38)*\n\n```\nArXiv, abs/2307.03170, 2023. [61] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. ArXiv, abs/2302.00294, 2023. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. ArXiv, abs/2312.03863, 2023. [64] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning.\n```\n\n#### 4. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.28)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.28)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n#### 5. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.26)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 13/30 (Score: 0.26)*\n\n```\narXiv preprint arXiv:2304.07327, 2023. Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers, 2022. Heejun Lee, Minki Kang, Youngwan Lee, and Sung Ju Hwang. Sparse token transformer with attention back tracking. In The Eleventh International Conference on Learning Representations, 2023. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019. Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022. Andr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo. Sparse and continuous attention mechanisms. Advances in Neural Information Processing Systems, 33:20989-21001, 2020. Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.\n```\n\n\n\n---\n## Found 12 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hierarchical sparse attention\", \"dynamic layer normalization\", \"adaptive attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from ArXiv\n\n#### 1. ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through\n  Regularized Self-Attention\n\n*From Search Query: \"hierarchical sparse attention\"*\n\n*Yang Liu, Jiaxiang Liu, Li Chen, Yuxiang Lu, Shikun Feng, Zhida Feng, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang*\n\n**Abstract:** Sparse Transformer has recently attracted a lot of attention since the\nability for reducing the quadratic dependency on the sequence length. We argue\nthat two factors, information bottleneck sensitivity and inconsistency between\ndifferent attention topologies, could affect the performance of the Sparse\nTransformer. This paper proposes a well-designed model named ERNIE-Sparse. It\nconsists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to\nsequentially unify local and global information. (ii) Self-Attention\nRegularization (SAR) method, a novel regularization designed to minimize the\ndistance for transformers with different attention topologies. To evaluate the\neffectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we\nperform experiments on a multi-modal long sequence modeling task benchmark,\nLong Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse\nsignificantly outperforms a variety of strong baseline methods including the\ndense attention and other efficient sparse attention methods and achieves\nimprovements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the\neffectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text\nclassification and 2 QA downstream tasks, achieve improvements on\nclassification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24%\n(74.67% vs. 71.43%). Experimental results continue to demonstrate its superior\nperformance.\n\n**Published:** 2022-03-23T08:47:01Z  (*Updated: 2022-03-23T08:47:01Z*)\n\n\n\n#### 2. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and\n  Attention\n\n*From Search Query: \"hierarchical sparse attention\"*\n\n*Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Du*\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical\nframework to understand the training procedure of multilayer Transformer\narchitectures. This is achieved by integrating out the self-attention layer in\nTransformers, producing a modified dynamics of MLP layers only. JoMA removes\nunrealistic assumptions in previous analysis (e.g., lack of residual\nconnection) and predicts that the attention first becomes sparse (to learn\nsalient tokens), then dense (to learn less salient tokens) in the presence of\nnonlinear activations, while in the linear case, it is consistent with existing\nworks that show attention becomes sparse over time. We leverage JoMA to\nqualitatively explains how tokens are combined to form hierarchies in\nmultilayer Transformers, when the input tokens are generated by a latent\nhierarchical generative model. Experiments on models trained from real-world\ndataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia)\nverify our theoretical findings. Code can be found in\nhttps://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Published:** 2023-10-01T01:21:35Z  (*Updated: 2024-03-15T02:03:21Z*)\n\n\n\n#### 3. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 4. Methods of improving LLM training stability\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Oleg Rybakov, Mike Chrzanowski, Peter Dykas, Jinze Xue, Ben Lanir*\n\n**Abstract:** Training stability of large language models(LLMs) is an important research\ntopic. Reproducing training instabilities can be costly, so we use a small\nlanguage model with 830M parameters and experiment with higher learning rates\nto force models to diverge. One of the sources of training instability is the\ngrowth of logits in attention layers. We extend the focus of the previous work\nand look not only at the magnitude of the logits but at all outputs of linear\nlayers in the Transformer block. We observe that with a high learning rate the\nL2 norm of all linear layer outputs can grow with each training step and the\nmodel diverges. Specifically we observe that QKV, Proj and FC2 layers have the\nlargest growth of the output magnitude. This prompts us to explore several\noptions: 1) apply layer normalization not only after QK layers but also after\nProj and FC2 layers too; 2) apply layer normalization after the QKV layer (and\nremove pre normalization). 3) apply QK layer normalization together with\nsoftmax capping. We show that with the last two methods we can increase\nlearning rate by 1.5x (without model divergence) in comparison to an approach\nbased on QK layer normalization only. Also we observe significant perplexity\nimprovements for all three methods in comparison to the baseline model.\n\n**Published:** 2024-10-22T04:27:03Z  (*Updated: 2024-10-22T04:27:03Z*)\n\n\n\n#### 5. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry\n\n*From Search Query: \"adaptive attention\"*\n\n*Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher R\u00e9*\n\n**Abstract:** Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.\n\n**Published:** 2024-02-06T19:31:26Z  (*Updated: 2024-02-06T19:31:26Z*)\n\n\n\n#### 6. Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large\n  Language Models without Training through Attention Calibration\n\n*From Search Query: \"adaptive attention\"*\n\n*Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, Yingyan Celine Lin*\n\n**Abstract:** Attention is a fundamental component behind the remarkable achievements of\nlarge language models (LLMs). However, our current understanding of the\nattention mechanism, especially regarding how attention distributions are\nestablished, remains limited. Inspired by recent studies that explore the\npresence of attention sink in the initial token, which receives\ndisproportionately large attention scores despite their lack of semantic\nimportance, this work delves deeper into this phenomenon. We aim to provide a\nmore profound understanding of the existence of attention sinks within LLMs and\nto uncover ways to enhance the achievable accuracy of LLMs by directly\noptimizing the attention distributions, without the need for weight finetuning.\nSpecifically, this work begins with comprehensive visualizations of the\nattention distributions in LLMs during inference across various inputs and\ntasks. Based on these visualizations, to the best of our knowledge, we are the\nfirst to discover that (1) attention sinks occur not only at the start of\nsequences but also within later tokens of the input, and (2) not all attention\nsinks have a positive impact on the achievable accuracy of LLMs. Building upon\nour findings, we propose a training-free Attention Calibration Technique (ACT)\nthat automatically optimizes the attention distributions on the fly during\ninference in an input-adaptive manner. Extensive experiments validate that ACT\nconsistently enhances the accuracy of various LLMs across different\napplications. Specifically, ACT achieves an average improvement of up to 7.30%\nin accuracy across different datasets when applied to Llama-30B. Our code is\navailable at https://github.com/GATECH-EIC/ACT.\n\n**Published:** 2024-06-22T07:00:43Z  (*Updated: 2024-06-22T07:00:43Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Dilated Neighborhood Attention Transformer\n\n*From Search Query: \"hierarchical sparse attention\"*\n\n*Humphrey Shi, Ali Hassani*\n\n**Abstract:** Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\n\n**Published:** 2022-09-29\n\n\n\n#### 2. Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing\n\n*From Search Query: \"hierarchical sparse attention\"*\n\n*Zhenghua Li, Lijie Wang, Kun Wu, Haifeng Wang, Min Zhang, Hua Wu, Xinyan Xiao, Ao Zhang*\n\n**Abstract:** Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.\n\n**Proceeding:** emnlp-2021-11\n\n**Published:** 2021-03-03\n\n\n\n#### 3. MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Wenrui Dai, Hongkai Xiong, Junni Zou, Chenglin Li, Wen Fei*\n\n**Abstract:** Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. The last BN layer provides autotuned learning rates and also improves accuracy. Experimental results show that MimicNorm achieves similar accuracy for various network structures, including ResNets and lightweight networks like ShuffleNet, with a reduction of about 20% memory consumption. The code is publicly available at https://github.com/Kid-key/MimicNorm.\n\n**Published:** 2020-10-19\n\n\n\n#### 4. Layer Normalization\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n#### 5. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\n\n*From Search Query: \"adaptive attention\"*\n\n*Peng Gao, Chris Liu, Yu Qiao, Hongsheng Li, Pan Lu, Shilin Yan, Xiangfei Hu, Aojun Zhou, Jiaming Han, Renrui Zhang*\n\n**Abstract:** We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.\n\n**Published:** 2023-03-28\n\n\n\n#### 6. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n*From Search Query: \"adaptive attention\"*\n\n*Junho Kim, Kwanghee Lee, Hyeonwoo Kang, Minjae Kim*\n\n**Abstract:** We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-07-25\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, focusing on hierarchical sparse attention, dynamic layer normalization, and hardware-efficient implementations, here are some key findings and suggestions based on the provided sources:\n\n## Hierarchical Sparse Attention Mechanisms\n\n### Multi-scale Attention Implementations and Sparse Attention Patterns\n- The paper on \"Sparsely-Sharded (S2) Attention\" introduces a hierarchical approach to sparse attention by sharding the context heterogeneously among attention heads. This method ensures each head attends to a different subset of tokens while collectively covering the full context, which is crucial for multi-scale attention implementations.\n- \"SeerAttention\" proposes a learnable gating module to adaptively identify significant blocks in the attention map, allowing for dynamic and sparse attention patterns. This approach is particularly beneficial for long-context tasks and maintains near-lossless accuracy even at high sparsity levels.\n\n### Adaptive Gating Mechanisms\n- \"SeerAttention\" uses a learnable gate (Attention Gate) to adaptively select significant blocks in the attention map, which is a form of adaptive gating mechanism. This helps in balancing accuracy and speedup by reducing I/O and computation overhead.\n\n### Memory Efficiency Optimizations\n- The \"S2-Attention\" paper emphasizes the importance of hardware-friendly sparse attention patterns and hybrid architectures combining sparse and dense attention. This approach achieves significant wall-clock speedup and maintains strong performance on downstream tasks, highlighting effective memory efficiency optimizations.\n\n## Dynamic and Adaptive Normalization Techniques\n\n### Context-dependent Parameter Generation\n- While the provided sources do not explicitly discuss dynamic layer normalization, the concept of adaptive parameter generation can be inferred from the learnable components in \"SeerAttention.\" The learnable gate in SeerAttention adaptively identifies important blocks, which could be seen as a form of context-dependent parameter generation.\n\n### Gradient Stability in Deep Networks\n- Gradient stability is a critical aspect of deep networks, but the sources provided do not directly address this. However, the stability of the model can be inferred from the performance and accuracy metrics reported. For example, \"SeerAttention\" maintains near-lossless accuracy even at high sparsity levels, suggesting robust gradient stability.\n\n### Integration with Attention Mechanisms\n- The integration of dynamic normalization with hierarchical attention mechanisms is not explicitly covered in the provided sources. However, the hierarchical sparse attention mechanisms discussed can be integrated with dynamic normalization techniques by ensuring the normalization parameters are generated adaptively based on the context, similar to how \"SeerAttention\" generates block sparsity.\n\n## Hardware-Efficient Implementations\n\n### Sparse Attention Computation\n- Both \"S2-Attention\" and \"SeerAttention\" papers focus on hardware-efficient implementations of sparse attention. \"S2-Attention\" uses a Triton library for kernel optimization, enabling high-performance sparse attention techniques with significant speedup compared to dense attention counterparts.\n- \"SeerAttention\" implements a customized block-sparse FlashAttention kernel using Triton, which efficiently reduces I/O and computation overhead by skipping non-activated blocks.\n\n### Dynamic Parameter Generation\n- \"SeerAttention\" involves dynamic parameter generation through its learnable gating module, which is optimized for modern hardware like GPUs. This ensures efficient learning and inference by integrating with the tiling computation scheme of FlashAttention.\n\n### Memory Access Optimization\n- The \"S2-Attention\" paper highlights the importance of sharding the context heterogeneously among attention heads to achieve high parallelization and optimized memory I/O. This approach is crucial for memory access optimization.\n\n### Training Stability Solutions\n- While the sources do not explicitly discuss training stability solutions, the consistent performance and high accuracy reported in both \"S2-Attention\" and \"SeerAttention\" suggest robust training stability. For instance, \"SeerAttention\" maintains high accuracy even at high sparsity levels, indicating stable training.\n\n## Additional Considerations\n\n- **Integration Strategy**: The interaction between hierarchical sparse attention and dynamic layer normalization should be carefully analyzed. Ensuring that the adaptive normalization parameters are generated based on the context and integrated with the hierarchical attention structure is crucial for practical implementation.\n- **Comparison with Existing Solutions**: Comparing the proposed HierarchicalSparseNorm-GPT with existing hierarchical and sparse attention methods, as well as other efficient attention variants like GQA, is essential to validate its claims and identify areas for improvement.\n\nBy integrating these insights, the researchers can develop a robust and efficient autoregressive language model that leverages hierarchical sparse attention, dynamic layer normalization, and hardware-efficient implementations to achieve better performance, scalability, and efficiency.\n",
            "analysis": "Initial Analysis of HierarchicalSparseNorm-GPT Proposal:\n\n1. Key Components Requiring Investigation:\n\na) Hierarchical Sparse Attention (HSA):\n- Novel combination of hierarchical attention with sparse computation\n- Integration with dynamic gating mechanisms\n- Scalability and efficiency claims\n- Comparison with existing hierarchical and sparse attention methods\n\nb) Dynamic Layer Normalization (DLN):\n- Adaptive parameter generation approach\n- Integration with hierarchical structure\n- Impact on gradient stability\n- Comparison with existing adaptive normalization techniques\n\nc) Integration Strategy:\n- Interaction between HSA and DLN\n- Memory efficiency claims\n- Hardware optimization approach\n- Training stability considerations\n\n2. Investigation Plan:\n\nPhase 1: Hierarchical Attention Mechanisms\n- Research existing hierarchical attention approaches\n- Analyze sparse attention implementations\n- Compare with GQA and other efficient attention variants\n\nPhase 2: Dynamic Normalization\n- Study adaptive normalization techniques\n- Investigate gradient stability solutions\n- Research parameter generation methods\n\nPhase 3: Integration and Efficiency\n- Examine hardware-efficient implementations\n- Study memory optimization techniques\n- Research training stability solutions\n\nInitial observations:\n1. The proposal combines hierarchical attention with sparse computation in a novel way\n2. The dynamic normalization approach appears innovative but needs validation\n3. The integration strategy requires careful analysis for practical implementation\n4. Hardware efficiency claims need verification\n5. Comparison with existing solutions is crucial\n\nThe proposal shows promise in addressing key challenges in language model design, but requires thorough investigation of its components and claims."
        },
        {
            "ready": false,
            "query": "\"adaptive normalization techniques\", \"transformer training stability\", \"hardware efficient attention\"",
            "detail": "Search for papers and technical documents discussing:\n\n1. Recent advances in adaptive and dynamic normalization:\n- Parameter generation techniques\n- Integration with attention mechanisms\n- Impact on model performance\n- Stability considerations\n\n2. Training stability in hierarchical architectures:\n- Gradient flow analysis\n- Normalization effects\n- Convergence properties\n- Practical solutions\n\n3. Hardware optimization strategies:\n- Memory access patterns\n- Efficient parameter generation\n- Sparse computation techniques\n- Implementation considerations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents discussing:\n\n1. Recent advances in adaptive and dynamic normalization:\n- Parameter generation techniques\n- Integration with attention mechanisms\n- Impact on model performance\n- Stability considerations\n\n2. Training stability in hierarchical architectures:\n- Gradient flow analysis\n- Normalization effects\n- Convergence properties\n- Practical solutions\n\n3. Hardware optimization strategies:\n- Memory access patterns\n- Efficient parameter generation\n- Sparse computation techniques\n- Implementation considerations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.06)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.06)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 2. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.03)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.03)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.03)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.03)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.02)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.02)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 5. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.02)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.02)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive normalization techniques\", \"transformer training stability\", \"hardware efficient attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from ArXiv\n\n#### 1. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive normalization techniques\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 2. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"adaptive normalization techniques\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 3. BranchNorm: Robustly Scaling Extremely Deep Transformers\n\n*From Search Query: \"transformer training stability\"*\n\n*Yijin Liu, Xianfeng Zeng, Fandong Meng, Jie Zhou*\n\n**Abstract:** Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000\nlayers) and reveals the promising potential of deep scaling. To stabilize the\ntraining of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the\nmodel update to a constant value. Although applying such a constraint can\nbenefit the early stage of model training, it may lead to undertrained models\nduring the whole training procedure. In this paper, we propose BranchNorm,\nwhich dynamically rescales the non-residual branch of Transformer in accordance\nwith the training period. BranchNorm not only theoretically stabilizes the\ntraining with smooth gradient norms at the early stage, but also encourages\nbetter convergence in the subsequent training stage. Experiment results on\nmultiple translation tasks demonstrate that BranchNorm achieves a better\ntrade-off between training stability and converge performance.\n\n**Published:** 2023-05-04T12:46:12Z  (*Updated: 2023-05-04T12:46:12Z*)\n\n\n\n#### 4. Multi-head or Single-head? An Empirical Comparison for Transformer\n  Training\n\n*From Search Query: \"transformer training stability\"*\n\n*Liyuan Liu, Jialu Liu, Jiawei Han*\n\n**Abstract:** Multi-head attention plays a crucial role in the recent success of\nTransformer models, which leads to consistent performance improvements over\nconventional attention in various applications. The popular belief is that this\neffectiveness stems from the ability of jointly attending multiple positions.\nIn this paper, we first demonstrate that jointly attending multiple positions\nis not a unique feature of multi-head attention, as multi-layer single-head\nattention also attends multiple positions and is more effective. Then, we\nsuggest the main advantage of the multi-head attention is the training\nstability, since it has less number of layers than the single-head attention,\nwhen attending the same number of positions. For example, 24-layer 16-head\nTransformer (BERT-large) and 384-layer single-head Transformer has the same\ntotal attention head number and roughly the same model size, while the\nmulti-head one is significantly shallower. Meanwhile, we show that, with recent\nadvances in deep learning, we can successfully stabilize the training of the\n384-layer Transformer. As the training difficulty is no longer a bottleneck,\nsubstantially deeper single-head Transformer achieves consistent performance\nimprovements without tuning hyper-parameters.\n\n**Published:** 2021-06-17T16:53:22Z  (*Updated: 2021-06-17T16:53:22Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 6. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Layer Normalization\n\n*From Search Query: \"adaptive normalization techniques\"*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n#### 2. High-Performance Large-Scale Image Recognition Without Normalization\n\n*From Search Query: \"adaptive normalization techniques\"*\n\n*Karen Simonyan, Samuel L. Smith, Soham De, Andrew Brock*\n\n**Abstract:** Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets\n\n**Published:** 2021-02-11\n\n\n\n#### 3. Stabilizing Transformer Training by Preventing Attention Entropy Collapse\n\n*From Search Query: \"transformer training stability\"*\n\n*Josh Susskind, Jiatao Gu, Yizhe Zhang, Jason Ramapuram, Dan Busbridge, Etai Littwin, Tatiana Likhomanenko, Shuangfei Zhai*\n\n**Abstract:** Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\\textit{entropy collapse}$. As a remedy, we propose $\\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with $\\sigma$Reparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer {to competitive performance} without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at \\url{https://github.com/apple/ml-sigma-reparam}.\n\n**Published:** 2023-03-11\n\n\n\n#### 4. DeepNet: Scaling Transformers to 1,000 Layers\n\n*From Search Query: \"transformer training stability\"*\n\n*Furu Wei, Dongdong Zhang, Shaohan Huang, Li Dong, Shuming Ma, Hongyu Wang*\n\n**Abstract:** In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.\n\n**Published:** 2022-03-01\n\n\n\n#### 5. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive normalization techniques, training stability in hierarchical architectures, and hardware optimization strategies, here are some key findings and references that align with your goals:\n\n## Recent Advances in Adaptive and Dynamic Normalization\n\n### Parameter Generation Techniques\n- The concept of Adaptive Normalization-Free Feature Recalibration (ANFR) is noteworthy. ANFR combines weight standardization with channel attention mechanisms, which helps in normalizing the weights of layers instead of activations. This approach is less susceptible to mismatched client statistics and inconsistent averaging, making it robust under data heterogeneity.\n- Dynamic Layer Normalization (DLN) dynamically generates scaling and shifting parameters, adapting to the variability in the input data. This technique has shown promise in acoustic models and could be explored for its applicability in language models [Analysis Note].\n\n### Integration with Attention Mechanisms\n- ANFR integrates channel attention with weight-standardized models, enabling the model to suppress inconsistent features and emphasize relevant ones. This combination enhances the model's ability to focus on shared, informative features across different data sources.\n- Hierarchical attention structures can be combined with sparse attention patterns, and adaptive mechanisms are crucial for balancing efficiency and performance. For example, the ERNIE-SPARSE model uses hierarchical efficient transformer structures with regularized self-attention, highlighting the importance of adaptive attention mechanisms[Analysis Note].\n\n### Impact on Model Performance and Stability Considerations\n- Dynamic normalization techniques, such as DLN, can adapt neural models to variability in the input data, potentially improving performance and stability. However, careful consideration is needed to ensure these techniques do not introduce instability or information bottlenecks in hierarchical architectures [Analysis Note].\n- ANFR maintains high variability in channel attention weights across channels and classes, indicating that the attention mechanism remains active and provides class-discriminative signals even under data heterogeneity.\n\n## Training Stability in Hierarchical Architectures\n\n### Gradient Flow Analysis and Normalization Effects\n- Training stability in hierarchical architectures can be improved by analyzing gradient flow and the effects of normalization. For instance, weight standardization in ANFR helps in stable training by avoiding the dependence on mini-batch statistics, which can be problematic in federated learning settings.\n- Dynamic Layer Normalization can help in stabilizing the training process by adapting to the acoustic variability, which could be a valuable insight for language models as well [Analysis Note].\n\n### Convergence Properties and Practical Solutions\n- Ensuring convergence in hierarchical architectures requires careful design. The use of adaptive normalization techniques can help in maintaining stability and promoting convergence. For example, ANFR operates independently of the aggregation method and is effective in both global and personalized federated learning settings.\n- Practical solutions include using techniques like weight standardization and channel attention to enhance class selectivity and optimize channel attention weight distribution, which can improve convergence properties.\n\n## Hardware Optimization Strategies\n\n### Memory Access Patterns and Efficient Parameter Generation\n- Memory access patterns are crucial for performance. Optimizing these patterns can significantly improve the efficiency of hierarchical attention models. For instance, sparse attention optimization techniques can reduce memory access overhead and improve computational efficiency[Analysis Note].\n- Efficient parameter generation is key. Techniques like ANFR, which combine weight standardization with channel attention, can be implemented with minimal computational overhead, making them hardware-efficient.\n\n### Sparse Computation Techniques and Implementation Considerations\n- Sparse computation techniques are essential for optimizing hardware performance. Models like ERNIE-SPARSE and others that use hierarchical efficient transformer structures with regularized self-attention demonstrate successful sparse attention optimization[Analysis Note].\n- Implementation considerations include designing hardware-aware models that optimize memory access patterns and leverage sparse computation techniques. For example, using simple trainable MLPs to produce attention weights mimicking softmax attention, as seen in the Hedgehog paper, can improve efficiency[Analysis Note].\n\nIn summary, the integration of adaptive normalization techniques with attention mechanisms, careful analysis of gradient flow and normalization effects, and the use of hardware optimization strategies such as sparse computation and efficient parameter generation are crucial for improving the design of autoregressive language models. The ANFR approach and dynamic layer normalization techniques offer promising avenues for enhancing model performance, stability, and hardware efficiency.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe initial search results reveal several key aspects relevant to the HierarchicalSparseNorm-GPT proposal:\n\na) Hierarchical Attention:\n- Multiple approaches exist for implementing hierarchical attention structures\n- Sparse attention patterns can be effectively combined with hierarchical structures\n- Adaptive mechanisms are crucial for balancing efficiency and performance\n\nb) Dynamic Normalization:\n- Limited research on dynamic layer normalization in transformer contexts\n- Need more information on adaptive parameter generation techniques\n- Integration with attention mechanisms requires further investigation\n\nc) Hardware Efficiency:\n- Several implementations demonstrate successful sparse attention optimization\n- Memory access patterns are crucial for performance\n- Need more details on practical implementation strategies\n\n2. Useful References with Excerpts:\n\nFrom ERNIE-SPARSE:\n\"We argue that two factors, information bottleneck sensitivity and inconsistency between different attention topologies, could affect the performance of the Sparse Transformer.\"\n\nFrom Dynamic Layer Normalization paper:\n\"By dynamically generating the scaling and shifting parameters in layer normalization, DLN adapts neural acoustic models to the acoustic variability\"\n\nFrom Hedgehog paper:\n\"Linear attentions have shown potential for improving Transformer efficiency [...] uses simple trainable MLPs to produce attention weights mimicking softmax attention\"\n\n3. Key Insights:\n\na) Attention Mechanisms:\n- Hierarchical structures need careful design to avoid information bottlenecks\n- Sparse patterns must maintain model expressiveness\n- Dynamic adaptation mechanisms are essential for performance\n\nb) Normalization:\n- Dynamic parameter generation shows promise in acoustic models\n- Need to investigate applicability to language models\n- Integration with attention requires careful consideration\n\nc) Implementation:\n- Hardware-aware design is crucial\n- Memory access patterns significantly impact performance\n- Training stability needs special attention\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\n1. Recent advances in dynamic/adaptive normalization techniques\n2. Integration strategies for combining attention and normalization\n3. Training stability in hierarchical architectures\n4. Hardware optimization techniques for dynamic parameter generation\n\n5. References:\n\n[1] Liu, Y., Liu, J., Chen, L., Lu, Y., Feng, S., Feng, Z., ... & Wang, H. (2022). ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through Regularized Self-Attention. arXiv preprint.\n\n[2] Kim, T., Song, I., & Bengio, Y. (2017). Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition. arXiv preprint.\n\n[3] Zhang, M., Bhatia, K., Kumbong, H., & R\u00e9, C. (2024). The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry. arXiv preprint.\n\n[4] Shi, H., & Hassani, A. (2022). Dilated Neighborhood Attention Transformer. arXiv preprint.\n\n[5] Rybakov, O., Chrzanowski, M., Dykas, P., Xue, J., & Lanir, B. (2024). Methods of improving LLM training stability. arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"sparse attention mechanisms\", \"hierarchical transformer stability\", \"memory efficient transformers\"",
            "detail": "Search for papers and technical documents discussing:\n\n1. Sparse attention mechanisms:\n- Recent implementations\n- Hardware efficiency considerations\n- Integration with hierarchical structures\n- Performance comparisons\n\n2. Stability analysis in hierarchical transformers:\n- Theoretical foundations\n- Gradient flow analysis\n- Convergence properties\n- Practical solutions\n\n3. Memory efficiency techniques:\n- Access pattern optimization\n- State compression methods\n- Hardware-aware design\n- Implementation strategies",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents discussing:\n\n1. Sparse attention mechanisms:\n- Recent implementations\n- Hardware efficiency considerations\n- Integration with hierarchical structures\n- Performance comparisons\n\n2. Stability analysis in hierarchical transformers:\n- Theoretical foundations\n- Gradient flow analysis\n- Convergence properties\n- Practical solutions\n\n3. Memory efficiency techniques:\n- Access pattern optimization\n- State compression methods\n- Hardware-aware design\n- Implementation strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.19)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 40/41 (Score: 0.19)*\n\n```\nHowever, our method is not dependent on the input modality. Future research involving vision or speech could further substantiate the robustness of our method. ## E Impact Statement\n\nThis paper presents SPARSEK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing. We believe our innovative attention mechanism can benefit both NLP and machine learning communities in constructing long-range foundation models. Specifically, we highlight the potential impacts of SPARSEK as follows:\n\n- Efficient Long-Range Modeling. First and foremost, the SPARSEK attention mechanism significantly reduces computational requirements compared to traditional self-attention mechanisms. By prioritizing a subset of key-value pairs, SPARSEK attention effectively reduces the memory footprint without sacrificing model performance.\n```\n\n#### 2. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.07)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 13/30 (Score: 0.08)*\n\n```\narXiv preprint arXiv:2304.07327, 2023. Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers, 2022. Heejun Lee, Minki Kang, Youngwan Lee, and Sung Ju Hwang. Sparse token transformer with attention back tracking. In The Eleventh International Conference on Learning Representations, 2023. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019. Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022. Andr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo. Sparse and continuous attention mechanisms. Advances in Neural Information Processing Systems, 33:20989-21001, 2020. Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.\n```\n\n##### *Relevant Chunk: No. 8/30 (Score: 0.06)*\n\n```\nAs context windows become larger by the day in state-of-the-art models, we expect these gains to become even more relevant. Interpretability. Fig. 8 provides insights into the interpretability aspect of the model's decisionmaking process. It is observed that token removal predominantly occurs when encountering stop words (punctuation), which aligns with the intuition that local information within a sentence becomes less relevant after its completion. Furthermore, it is worth noting that layers at varying depths exhibit distinct behaviors, reinforcing our rationale for dissecting token removal decisions across depth. The variance in sparsity distribution across different depths indicates the necessity of conducting additional interpretability research to obtain valuable insights in the interactions of the tokens within the model. We provide more insights towards this direction in the Appendix C. ## 5 Discussion\n\nWe proposed Adaptively Sparse Attention, a novel approach to dynamically prune the context in decoder-only Transformer architectures. Our results indicate that our technique performs favourably compared to competitive baselines in terms of the ratio between perplexity and sparsity of the attention weights. Remarkably our approach also significantly reduces the computational and memory requirements without affecting its final performance. We practically showcase these benefits achieving more than double the throughput at cases. Adaptively sparse attention comes with two additional practical advantages: first, it can be seamlessly integrated into existing pre-trained models via a cheap fine-tuning step; second, it represents an orthogonal contribution to the burgeoning research line aimed at increasing the level of efficiency of modern LLMs. As such, we envision its combination with existing techniques like weight pruning and quantization to be a promising avenue for future research. ## References\n\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence, (2):179-190, 1983. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 3. Hierarchical Transformers Are More Efficient Language Models (Avg. Score: 0.05)\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, H. Michalewski*\n\n**Published in:** NAACL-HLT (2021)\t**Cited by** 40  (*Influential: 4*)\n\n**TL;DR:** Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.\n\n##### *Relevant Chunk: No. 17/25 (Score: 0.05)*\n\n```\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient content-based sparse attention with routing transformers. Jianlin $\\mathrm{Su}, \\mathrm{Yu} \\mathrm{Lu}$, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, and Y-Lan Boureau. 2020. Multi-scale transformer language models. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers.\n```\n\n#### 4. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.05)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 17/38 (Score: 0.05)*\n\n```\nBehnke, M. and Heafield, K. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP (1), pp. 2664-2674. Association for Computational Linguistics, 2020 . Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004. 05150. Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens. CoRR, abs/2112.04426, 2021. URL https://arxiv.org/ abs/2112.04426. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L. J., and Weller, A. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n\n\n---\n## Found 12 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"sparse attention mechanisms\", \"hierarchical transformer stability\", \"memory efficient transformers\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from ArXiv\n\n#### 1. Mortality Prediction Models with Clinical Notes Using Sparse Attention\n  at the Word and Sentence Levels\n\n*From Search Query: \"sparse attention mechanisms\"*\n\n*Miguel Rios, Ameen Abu-Hanna*\n\n**Abstract:** Intensive Care in-hospital mortality prediction has various clinical\napplications. Neural prediction models, especially when capitalising on\nclinical notes, have been put forward as improvement on currently existing\nmodels. However, to be acceptable these models should be performant and\ntransparent. This work studies different attention mechanisms for clinical\nneural prediction models in terms of their discrimination and calibration.\nSpecifically, we investigate sparse attention as an alternative to dense\nattention weights in the task of in-hospital mortality prediction from clinical\nnotes. We evaluate the attention mechanisms based on: i) local self-attention\nover words in a sentence, and ii) global self-attention with a transformer\narchitecture across sentences. We demonstrate that the sparse mechanism\napproach outperforms the dense one for the local self-attention in terms of\npredictive performance with a publicly available dataset, and puts higher\nattention to prespecified relevant directive words. The performance at the\nsentence level, however, deteriorates as sentences including the influential\ndirective words tend to be dropped all together.\n\n**Published:** 2022-12-12T22:08:45Z  (*Updated: 2022-12-12T22:08:45Z*)\n\n\n\n#### 2. Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers\n\n*From Search Query: \"sparse attention mechanisms\"*\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n\n**Published:** 2024-06-24T15:55:59Z  (*Updated: 2024-06-24T15:55:59Z*)\n\n\n\n#### 3. Learning Syntax Without Planting Trees: Understanding When and Why\n  Transformers Generalize Hierarchically\n\n*From Search Query: \"hierarchical transformer stability\"*\n\n*Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A. Smith, Navin Goyal, Yulia Tsvetkov*\n\n**Abstract:** Transformers trained on natural language data have been shown to learn its\nhierarchical structure and generalize to sentences with unseen syntactic\nstructures without explicitly encoding any structural bias. In this work, we\ninvestigate sources of inductive bias in transformer models and their training\nthat could cause such generalization behavior to emerge. We extensively\nexperiment with transformer models trained on multiple synthetic datasets and\nwith different training objectives and show that while other objectives e.g.\nsequence-to-sequence modeling, prefix language modeling, often failed to lead\nto hierarchical generalization, models trained with the language modeling\nobjective consistently learned to generalize hierarchically. We then conduct\npruning experiments to study how transformers trained with the language\nmodeling objective encode hierarchical structure. When pruned, we find joint\nexistence of subnetworks within the model with different generalization\nbehaviors (subnetworks corresponding to hierarchical structure and linear\norder). Finally, we take a Bayesian perspective to further uncover\ntransformers' preference for hierarchical generalization: We establish a\ncorrelation between whether transformers generalize hierarchically on a dataset\nand whether the simplest explanation of that dataset is provided by a\nhierarchical grammar compared to regular grammars exhibiting linear\ngeneralization.\n\n**Published:** 2024-04-25T07:10:29Z  (*Updated: 2024-05-31T23:47:15Z*)\n\n\n\n#### 4. Hierarchical Transformers Are More Efficient Language Models\n\n*From Search Query: \"hierarchical transformer stability\"*\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, \u0141ukasz Kaiser, Yuhuai Wu, Christian Szegedy, Henryk Michalewski*\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling\ntasks. Remarkably, Transformers can handle long sequences which allows them to\nproduce long coherent outputs: full paragraphs produced by GPT-3 or\nwell-structured images produced by DALL-E. These large language models are\nimpressive but also very inefficient and costly, which limits their\napplications and accessibility. We postulate that having an explicit\nhierarchical architecture is the key to Transformers that efficiently handle\nlong sequences. To verify this claim, we first study different ways to\ndownsample and upsample activations in Transformers so as to make them\nhierarchical. We use the best performing upsampling and downsampling layers to\ncreate Hourglass - a hierarchical Transformer language model. Hourglass\nimproves upon the Transformer baseline given the same amount of computation and\ncan yield the same results as Transformers more efficiently. In particular,\nHourglass sets new state-of-the-art for Transformer models on the ImageNet32\ngeneration task and improves language modeling efficiency on the widely studied\nenwik8 benchmark.\n\n**Published:** 2021-10-26T14:00:49Z  (*Updated: 2022-04-16T20:47:45Z*)\n\n\n\n#### 5. Implicit Memory Transformer for Computationally Efficient Simultaneous\n  Speech Translation\n\n*From Search Query: \"memory efficient transformers\"*\n\n*Matthew Raffel, Lizhong Chen*\n\n**Abstract:** Simultaneous speech translation is an essential communication task difficult\nfor humans whereby a translation is generated concurrently with oncoming speech\ninputs. For such a streaming task, transformers using block processing to break\nan input sequence into segments have achieved state-of-the-art performance at a\nreduced cost. Current methods to allow information to propagate across\nsegments, including left context and memory banks, have faltered as they are\nboth insufficient representations and unnecessarily expensive to compute. In\nthis paper, we propose an Implicit Memory Transformer that implicitly retains\nmemory through a new left context method, removing the need to explicitly\nrepresent memory with memory banks. We generate the left context from the\nattention output of the previous segment and include it in the keys and values\nof the current segment's attention calculation. Experiments on the MuST-C\ndataset show that the Implicit Memory Transformer provides a substantial\nspeedup on the encoder forward pass with nearly identical translation quality\nwhen compared with the state-of-the-art approach that employs both left context\nand memory banks.\n\n**Published:** 2023-07-03T22:20:21Z  (*Updated: 2023-07-03T22:20:21Z*)\n\n\n\n#### 6. Memory-efficient Stochastic methods for Memory-based Transformers\n\n*From Search Query: \"memory efficient transformers\"*\n\n*Vishwajit Kumar Vishnu, C. Chandra Sekhar*\n\n**Abstract:** Training Memory-based transformers can require a large amount of memory and\ncan be quite inefficient. We propose a novel two-phase training mechanism and a\nnovel regularization technique to improve the training efficiency of\nmemory-based transformers, which are often used for long-range context\nproblems. For our experiments, we consider transformer-XL as our baseline model\nwhich is one of memorybased transformer models. We show that our resultant\nmodel, Skip Cross-head TransformerXL, outperforms the baseline on character\nlevel language modeling task with similar parameters and outperforms the\nbaseline on word level language modelling task with almost 20% fewer\nparameters. Our proposed methods do not require any additional memory. We also\ndemonstrate the effectiveness of our regularization mechanism on BERT which\nshows similar performance with reduction in standard deviation of scores of\naround 30% on multiple GLUE tasks.\n\n**Published:** 2023-11-14T12:37:25Z  (*Updated: 2023-11-14T12:37:25Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\n\n*From Search Query: \"sparse attention mechanisms\"*\n\n*Ram\u00f3n Fernandez Astudillo, Andr\u00e9 F. T. Martins*\n\n**Abstract:** We propose sparsemax, a new activation function similar to the traditional\nsoftmax, but able to output sparse probabilities. After deriving its\nproperties, we show how its Jacobian can be efficiently computed, enabling its\nuse in a network trained with backpropagation. Then, we propose a new smooth\nand convex loss function which is the sparsemax analogue of the logistic loss.\nWe reveal an unexpected connection between this new loss and the Huber\nclassification loss. We obtain promising empirical results in multi-label\nclassification problems and in attention-based neural networks for natural\nlanguage inference. For the latter, we achieve a similar performance as the\ntraditional softmax, but with a selective, more compact, attention focus.\n\n**Published:** 2016-02-05\n\n\n\n#### 2. A Regularized Framework for Sparse and Structured Neural Attention\n\n*From Search Query: \"sparse attention mechanisms\"*\n\n*Mathieu Blondel, Vlad Niculae*\n\n**Abstract:** Modern neural networks are often augmented with an attention mechanism, which\ntells the network where to focus within the input. We propose in this paper a\nnew framework for sparse and structured attention, building upon a smoothed max\noperator. We show that the gradient of this operator defines a mapping from\nreal values to probabilities, suitable as an attention mechanism. Our framework\nincludes softmax and a slight generalization of the recently-proposed sparsemax\nas special cases. However, we also show how our framework can incorporate\nmodern structured penalties, resulting in more interpretable attention\nmechanisms, that focus on entire segments or groups of an input. We derive\nefficient algorithms to compute the forward and backward passes of our\nattention mechanisms, enabling their use in a neural network trained with\nbackpropagation. To showcase their potential as a drop-in replacement for\nexisting ones, we evaluate our attention mechanisms on three large-scale tasks:\ntextual entailment, machine translation, and sentence summarization. Our\nattention mechanisms improve interpretability without sacrificing performance;\nnotably, on textual entailment and summarization, we outperform the standard\nattention mechanisms based on softmax and sparsemax.\n\n**Conference:** a-regularized-framework-for-sparse-and-1\n\n**Published:** 2017-05-22\n\n\n\n#### 3. MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction\n\n*From Search Query: \"hierarchical transformer stability\"*\n\n*Chang Huang, Wenyu Liu, Qian Zhang, Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Bencheng Liao*\n\n**Abstract:** High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP, and MapTR-tiny achieves $13.5$ higher mAP and $3\\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at \\url{https://github.com/hustvl/MapTR}.\n\n**Published:** 2022-08-30\n\n\n\n#### 4. Few-Shot Diffusion Models\n\n*From Search Query: \"hierarchical transformer stability\"*\n\n*Ole Winther, Didrik Nielsen, Giorgio Giannone*\n\n**Abstract:** Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent variable models with remarkable sample generation quality and training stability. These properties can be attributed to parameter sharing in the generative hierarchy, as well as a parameter-free diffusion-based inference procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for few-shot generation leveraging conditional DDPMs. FSDMs are trained to adapt the generative process conditioned on a small set of images from a given class by aggregating image patch information using a set-based Vision Transformer (ViT). At test time, the model is able to generate samples from previously unseen classes conditioned on as few as 5 samples from that class. We empirically show that FSDM can perform few-shot generation and transfer to new datasets. We benchmark variants of our method on complex vision datasets for few-shot learning and compare to unconditional and conditional DDPM baselines. Additionally, we show how conditioning the model on patch-based input set information improves training convergence.\n\n**Published:** 2022-05-30\n\n\n\n#### 5. EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention\n\n*From Search Query: \"memory efficient transformers\"*\n\n*Yixuan Yuan, Han Hu, Yuqing Yang, Ningxin Zheng, Houwen Peng, Xinyu Liu*\n\n**Abstract:** Vision transformers have shown great success due to their high model capabilities. However, their remarkable performance is accompanied by heavy computation costs, which makes them unsuitable for real-time applications. In this paper, we propose a family of high-speed vision transformers named EfficientViT. We find that the speed of existing transformer models is commonly bounded by memory inefficient operations, especially the tensor reshaping and element-wise functions in MHSA. Therefore, we design a new building block with a sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN layers, which improves memory efficiency while enhancing channel communication. Moreover, we discover that the attention maps share high similarities across heads, leading to computational redundancy. To address this, we present a cascaded group attention module feeding attention heads with different splits of the full feature, which not only saves computation cost but also improves attention diversity. Comprehensive experiments demonstrate EfficientViT outperforms existing efficient models, striking a good trade-off between speed and accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by 1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia V100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficient model MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while running 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNX format. Code and models are available at https://github.com/microsoft/Cream/tree/main/EfficientViT.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-05-11\n\n\n\n#### 6. H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training\n\n*From Search Query: \"memory efficient transformers\"*\n\n*Anonymous*\n\n**Abstract:** In recent years, big models based on Transformers have achieved state-of-the-art performance on many artificial intelligence (AI) tasks.\nDespite the success of these Transformer-based models, their huge parameter size poses a serious challenge to their training, both from the storage and computation perspectives.\nTo this end, memory optimization (e.g., rematerialization and offloading) and parallelism (e.g., data parallelism and model parallelism) are widely explored to make training Transformers more efficient.\nIn this paper, we propose a framework to automatically find an efficient integration of memory optimization and parallelism for High-Throughput Transformer Training (named H3T), which is rarely considered by existing efforts for training big Transformer-based models.\nSpecifically, we design search algorithms to combine appropriate memory optimization strategies and parallelism schemes to achieve a balance between memory overhead and training efficiency.\nWe implement H3T based on an open-source toolkit BMTrain and then use H3T to train the Transformers of different sizes to evaluate the efficiency of H3T.\nThe experimental results show that H3T outperforms the most popular deep learning (DL) toolkit Megatron-DeepSpeed by $1.2\\times \\sim 4.3\\times$ training speed while reducing $34.6\\% \\sim 80.5\\%$ of memory overhead.\nMoreover, H3T can use only 64 NVIDIA A100 GPUs to train GPT-3-175B, which is very difficult for existing DL toolkits. The source code is available at https://github.com/OpenBMB/BMTrain/tree/h3t.Submission Number: 3012\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-09-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on sparse attention mechanisms, hierarchical transformer stability, and memory efficiency techniques, here are some key findings and references from the provided sources and additional insights:\n\n## Sparse Attention Mechanisms\n\n### Recent Implementations\n- **SeerAttention**: This mechanism learns and leverages intrinsic sparsity in attention, using a learnable gate to select significant blocks in the attention map. It achieves a high sparsity ratio with minimal perplexity loss and significant speedup over existing methods like FlashAttention-2.\n- **Sparse Transformers**: These models reduce complexity from \\(O(n^2)\\) to \\(O(n\\sqrt{n})\\) by applying sparse factorizations to self-attention matrices, making them more efficient for handling large input sizes.\n\n### Hardware Efficiency Considerations\n- **SeerAttention** implements a customized FlashAttention kernel to extract block-wise attention map information without maintaining the full-size attention map, significantly boosting the scalability of the training process. It also uses a block-sparse FlashAttention kernel with Triton to speed up inference.\n- **OpenVINO Optimization**: For sparse transformer models, using OpenVINO runtime with 4th Gen Intel\u00ae Xeon\u00ae Scalable Processors can provide significant performance improvements by enabling sparsity optimization and 8-bit quantization. This approach is particularly beneficial for tasks like question answering and sequence classification.\n\n### Integration with Hierarchical Structures\n- **SeerAttention** can be integrated into hierarchical structures by fine-tuning with models like YaRN, where it achieves a high sparsity ratio without significant loss in performance. This integration is crucial for long-context scenarios.\n\n### Performance Comparisons\n- **SeerAttention** outperforms state-of-the-art static or heuristic-based sparse attention methods, especially in long-context fine-tuning scenarios, with a 5.67\u00d7 speedup over FlashAttention-2 at a 32k context length.\n- **Sparse Transformers** maintain performance while reducing computational complexity, making them suitable for real-time applications and various NLP tasks.\n\n## Stability Analysis in Hierarchical Transformers\n\n### Theoretical Foundations\n- While the provided sources do not delve deeply into the theoretical foundations of hierarchical transformer stability, it is crucial to consider the gradient flow and normalization techniques. For instance, **BranchNorm** dynamically rescales the non-residual branch of the Transformer to stabilize training with smooth gradient norms.\n\n### Gradient Flow Analysis\n- **Self-attention Networks** analysis shows that small eigenspectrum variance in attention leads to localized attention, preventing rank and entropy collapse, which is essential for better model expressivity and trainability.\n\n### Convergence Properties\n- **Dynamic Layer Normalization (DLN)** helps in adaptive parameter generation and can stabilize the training process. However, integrating DLN with hierarchical attention mechanisms requires careful consideration to ensure stability and performance.\n\n### Practical Solutions\n- **Branch-specific normalization** techniques, such as BranchNorm, can offer advantages in stabilizing deep transformer training by ensuring smooth gradient norms at the early stages of training.\n\n## Memory Efficiency Techniques\n\n### Access Pattern Optimization\n- **SeerAttention** optimizes access patterns by using a block-sparse FlashAttention kernel, which reduces I/O and computation overhead by skipping non-activated blocks. This approach is implemented using Triton and can be further optimized with CUDA.\n\n### State Compression Methods\n- **Sparse Storage Formats**: Using sparse storage formats can minimize memory consumption by storing only non-zero elements. This is particularly beneficial in high-dimensional data and large-scale neural networks.\n\n### Hardware-Aware Design\n- **OpenVINO Runtime**: Optimizing models for specific hardware, such as 4th Gen Intel\u00ae Xeon\u00ae Scalable Processors, can provide significant performance improvements. The OpenVINO runtime enables sparsity optimization and 8-bit quantization, which can be stacked to achieve higher cumulative speedup.\n\n### Implementation Strategies\n- **Customized Attention Kernels**: Implementing customized attention kernels, like the one in SeerAttention, can extract block-wise attention map information efficiently, reducing overhead and boosting scalability.\n- **Quantization and Pruning**: Combining sparsity with 8-bit quantization and pruning can lead to better performance-accuracy trade-offs. This is demonstrated in the optimization of floating-point BERT models using Hugging Face Optimum and NNCF.\n\nBy integrating these insights, researchers can develop more efficient, scalable, and stable autoregressive language models that leverage sparse attention mechanisms, hierarchical structures, and optimized memory efficiency techniques.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights into three key areas of the proposal:\n\na) Hierarchical and Sparse Attention:\n- Multiple approaches exist for implementing efficient attention mechanisms\n- Sparse attention patterns can be effectively combined with hierarchical structures\n- Hardware efficiency is crucial for practical implementation\n- Recent advances in linear attention show promise for efficiency\n\nb) Dynamic Normalization:\n- Dynamic Layer Normalization (DLN) shows promise in adaptive parameter generation\n- Integration with attention mechanisms requires careful consideration\n- Stability concerns need to be addressed\n- Limited research exists on combining with hierarchical attention\n\nc) Training Stability:\n- Several approaches to stabilizing deep transformer training\n- Gradient flow analysis is crucial\n- Normalization plays a key role in stability\n- Trade-offs between stability and performance need consideration\n\n2. Useful References with Excerpts:\n\nFrom GLA paper:\n\"Gated Linear Attention (GLA) Transformer with data-dependent gates [...] performs competitively against the LLaMA-architecture Transformer [...] especially effective at length generalization\"\n\nFrom BranchNorm paper:\n\"dynamically rescales the non-residual branch of Transformer [...] stabilizes the training with smooth gradient norms at the early stage\"\n\nFrom Self-attention Networks paper:\n\"small eigenspectrum variance leads attention to be localized [...] prevents both rank and entropy collapse, leading to better model expressivity and trainability\"\n\n3. Key Insights:\n\na) Attention Mechanisms:\n- Gated mechanisms improve adaptivity\n- Linear attention can be made competitive with proper design\n- Localization properties affect model expressivity\n- Hardware efficiency crucial for practical implementation\n\nb) Normalization:\n- Dynamic normalization shows promise but needs careful integration\n- Stability-performance trade-off is important\n- Branch-specific normalization may offer advantages\n- Adaptive techniques need proper theoretical grounding\n\nc) Implementation:\n- Hardware considerations crucial for efficiency\n- Memory access patterns significantly impact performance\n- Training stability requires careful design\n- Integration strategy needs thorough planning\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\n1. Recent advances in sparse attention mechanisms and their hardware implementation\n2. Theoretical analysis of hierarchical attention stability\n3. Memory efficiency techniques in transformer architectures\n\n5. References:\n\n[1] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint.\n\n[2] Liu, Y., Zeng, X., Meng, F., & Zhou, J. (2023). BranchNorm: Robustly Scaling Extremely Deep Transformers. arXiv preprint.\n\n[3] Bao, H., Hataya, R., & Karakida, R. (2024). Self-attention Networks Localize When QK-eigenspectrum Concentrates. arXiv preprint.\n\n[4] Kim, T., Song, I., & Bengio, Y. (2017). Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition. arXiv preprint.\n\n[5] Liu, L., Liu, J., & Han, J. (2021). Multi-head or Single-head? An Empirical Comparison for Transformer Training. arXiv preprint."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.59796,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.9425520000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalAdaptiveAttention",
    "proposal": "# Proposal: HierarchicalSparseNorm-GPT\n\n## Title\nHierarchicalSparseNorm-GPT: Enhancing Language Models with Hierarchical Sparse Attention and Dynamic Normalization\n\n## Motivation\nAutoregressive language models, such as GPT-2, have demonstrated remarkable capabilities in natural language understanding and generation. However, their scalability and efficiency are often constrained by the quadratic complexity of the traditional Multi-Head Attention (MHA) mechanism and the rigidity of standard normalization techniques like RMSNorm. As models scale to handle longer sequences and more complex tasks, these limitations become increasingly pronounced, hindering both training and inference efficiency. To address these challenges, there is a need for a novel Language Model (LM) block design that integrates hierarchical sparse attention mechanisms with dynamic normalization techniques. Such an integration aims to enhance the model's ability to capture multi-scale dependencies, improve computational and parameter efficiency, and maintain robust gradient flows during training, thereby outperforming current state-of-the-art models in terms of perplexity, accuracy on downstream tasks, robustness, and scalability.\n\n## Problem Analysis\n### Key Challenges\n1. **Quadratic Complexity of MHA**: Traditional MHA mechanisms scale quadratically with sequence length, limiting the model's ability to handle long sequences efficiently.\n2. **Rigidity of Standard Normalization**: Techniques like RMSNorm lack adaptability to varying input contexts, potentially leading to suboptimal gradient flows and reduced model flexibility.\n3. **Redundancy in Attention Heads**: Independent attention heads often lead to redundant computations and limited expressiveness.\n4. **Gradient Instability**: High variability in attention distributions can cause gradient instability, hindering effective training of deep transformer models.\n\n## Core Idea and Philosophy\nThe core idea behind HierarchicalSparseNorm-GPT is to integrate hierarchical sparse attention mechanisms with dynamic normalization to address the aforementioned challenges. By adopting a hierarchical structure, the model can capture multi-scale dependencies efficiently. Dynamic normalization ensures that normalization parameters adapt based on input contexts, enhancing gradient stability and model flexibility. This integration is expected to reduce computational complexity, eliminate redundancy, and improve overall model performance.\n\n## Design Plan\n### Selection of GAU to Modify\n```selection\nHierarchicalAdaptiveAttention\n```\n\n### Modifications and New Structures\n#### Hierarchical Sparse Attention (HSA)\n1. **Hierarchical Structure**: \n   - Organize attention heads into hierarchical groups, each responsible for capturing dependencies at different scales (e.g., local, medium, global).\n   - Lower-level groups focus on fine-grained, local dependencies, while higher-level groups capture broad, global contexts.\n\n2. **Sparse Attention Mechanism**:\n   - Implement sparse attention mechanisms within each hierarchical group to reduce computational complexity from O(N\u00b2) to O(N log N).\n   - Utilize a learnable gate to dynamically select significant blocks in the attention map, inspired by SeerAttention.\n\n3. **Adaptive Gating Mechanism**:\n   - Introduce a gating mechanism that dynamically allocates attention head resources based on input context.\n   - Gates determine the contribution of each hierarchical group, allowing the model to adaptively focus on relevant scales.\n\n4. **Dynamic Composition**:\n   - Implement dynamic composition of attention scores and weights across hierarchical groups.\n   - Use lightweight MLPs to produce attention weights that mimic softmax attention.\n\n#### Dynamic Layer Normalization (DLN)\n1. **Adaptive Scaling and Shifting**:\n   - Replace traditional RMSNorm with DLN, which generates scaling and shifting parameters based on input features.\n   - Ensure that normalization is performed both within tokens and across hierarchical groups.\n\n2. **Hierarchical Normalization Layers**:\n   - Apply DLN at multiple hierarchical levels to capture multi-scale contextual information.\n   - Integrate hierarchical layer normalization within HSA to maintain gradient stability and training efficiency.\n\n### Mathematical Formulations\n#### Hierarchical Sparse Attention (HSA)\nGiven an input sequence \\( X \\in \\mathbb{R}^{B \\times L \\times D} \\):\n\n1. **Sparse Projections**:\n   \\[\n   Q_g = X W_Q^g, \\quad K_g = X W_K^g, \\quad V_g = X W_V^g\n   \\]\n   where \\( g \\) indexes hierarchical groups.\n\n2. **Sparse Attention**:\n   For each hierarchical group \\( g \\):\n   \\[\n   \\text{Attention}_g(Q_g, K_g, V_g) = \\text{SparseAttention}(Q_g W_{Q_g}, K_g W_{K_g}, V_g W_{V_g})\n   \\]\n\n3. **Adaptive Gating**:\n   \\[\n   G = \\sigma(X W_G)\n   \\]\n   where \\( W_G \\) is a learnable projection and \\( \\sigma \\) is the sigmoid function.\n\n4. **Dynamic Composition**:\n   \\[\n   \\text{HSA}(X) = \\sum_{g} G_g \\cdot \\text{Attention}_g(Q_g, K_g, V_g)\n   \\]\n\n#### Dynamic Layer Normalization (DLN)\nGiven an input \\( Y \\in \\mathbb{R}^{B \\times L \\times D} \\):\n\n1. **Adaptive Parameters**:\n   \\[\n   \\gamma = f_\\gamma(Y), \\quad \\beta = f_\\beta(Y)\n   \\]\n   where \\( f_\\gamma \\) and \\( f_\\beta \\) are small MLPs.\n\n2. **Normalization**:\n   \\[\n   \\text{DLN}(Y) = \\gamma \\odot \\frac{Y - \\mu(Y)}{\\sqrt{\\sigma^2(Y) + \\epsilon}} + \\beta\n   \\]\n   where \\( \\mu(Y) \\) and \\( \\sigma^2(Y) \\) are the mean and variance across specified dimensions.\n\n## Implementation Guidelines\n### Pseudo-Code for HSA and DLN\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DynamicLayerNorm(nn.Module):\n    def __init__(self, embed_dim, eps=1e-5):\n        super(DynamicLayerNorm, self).__init__()\n        self.embed_dim = embed_dim\n        self.eps = eps\n        self.gamma_mlp = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.ReLU(),\n            nn.Linear(embed_dim // 2, embed_dim)\n        )\n        self.beta_mlp = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.ReLU(),\n            nn.Linear(embed_dim // 2, embed_dim)\n        )\n    \n    def forward(self, X):\n        # Compute mean and variance across the last dimension\n        mean = X.mean(-1, keepdim=True)\n        var = X.var(-1, keepdim=True, unbiased=False)\n        normalized = (X - mean) / torch.sqrt(var + self.eps)\n        \n        # Generate adaptive scaling and shifting parameters\n        gamma = self.gamma_mlp(X)\n        beta = self.beta_mlp(X)\n        \n        return gamma * normalized + beta\n\nclass HierarchicalSparseAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, num_scales, dropout=0.1):\n        super(HierarchicalSparseAttention, self).__init__()\n        assert embed_dim % (num_heads * num_scales) == 0, \"embed_dim must be divisible by num_heads * num_scales\"\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_scales = num_scales\n        self.head_dim = embed_dim // (num_heads * num_scales)\n        \n        # Define projections for each scale\n        self.query_projs = nn.ModuleList([nn.Linear(embed_dim, num_heads * self.head_dim, bias=False) for _ in range(num_scales)])\n        self.key_projs = nn.ModuleList([nn.Linear(embed_dim, num_heads * self.head_dim, bias=False) for _ in range(num_scales)])\n        self.value_projs = nn.ModuleList([nn.Linear(embed_dim, num_heads * self.head_dim, bias=False) for _ in range(num_scales)])\n        \n        # Gating mechanism\n        self.gate_proj = nn.Linear(embed_dim, num_scales)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.out_proj = nn.Linear(num_heads * self.head_dim * num_scales, embed_dim)\n    \n    def forward(self, X):\n        B, L, D = X.size()\n        gate_scores = torch.sigmoid(self.gate_proj(X))  # Shape: (B, L, num_scales)\n        \n        attn_outputs = []\n        for scale in range(self.num_scales):\n            Q = self.query_projs[scale](X).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, L, head_dim)\n            K = self.key_projs[scale](X).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n            V = self.value_projs[scale](X).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n            \n            # Sparse Attention Mechanism\n            K = F.softmax(K, dim=-1)  # Shape: (B, num_heads, L, head_dim)\n            attention = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, L, L)\n            attention = self.dropout(attention)\n            output = torch.matmul(attention, V)  # (B, num_heads, L, head_dim)\n            attn_outputs.append(output)\n        \n        # Stack outputs from all scales\n        attn_output = torch.cat(attn_outputs, dim=-1)  # (B, num_heads, L, head_dim * num_scales)\n        attn_output = attn_output.transpose(1, 2).reshape(B, L, -1)  # (B, L, num_heads * head_dim * num_scales)\n        attn_output = self.out_proj(attn_output)  # (B, L, embed_dim)\n        \n        # Apply gating to scale the contributions\n        gate_scores = gate_scores.unsqueeze(2).unsqueeze(-1)  # (B, L, 1, num_scales)\n        scaling_factors = gate_scores.expand(-1, -1, self.num_heads, -1)  # (B, L, num_heads, num_scales)\n        attn_output = attn_output.view(B, L, self.num_heads, self.head_dim * self.num_scales)\n        attn_output = attn_output * scaling_factors  # (B, L, num_heads, head_dim * num_scales)\n        attn_output = attn_output.view(B, L, -1)  # (B, L, num_heads * head_dim * num_scales)\n        attn_output = self.out_proj(attn_output)  # (B, L, embed_dim)\n        \n        return attn_output\n```\n\n### Step-by-Step Integration\n1. **Replace Traditional MHA with HSA:**\n   - In the GPT-2 block, substitute the existing MHA module with the newly defined `HierarchicalSparseAttention` class.\n   - Ensure that the number of heads and scales are set based on the model's configuration parameters.\n\n2. **Integrate Dynamic Layer Normalization:**\n   - Replace the existing RMSNorm layers with `DynamicLayerNorm`.\n   - Apply `DynamicLayerNorm` both within the attention mechanism and after the GatedMLP to maintain stability across hierarchical levels.\n\n3. **Adjust Training Procedures:**\n   - Incorporate gradient clipping and adaptive learning rate schedulers to handle the enhanced architecture's training dynamics.\n   - Fine-tune hyperparameters related to the gating mechanism to optimize the balance between different hierarchical scales.\n\n4. **Optimize for Hardware:**\n   - Utilize efficient tensor operations and memory management techniques to ensure that the hierarchical and dynamic components do not introduce latency overhead.\n   - Leverage parallelization where possible, particularly within the multi-scale attention computations.\n\n## Conclusion\nHierarchicalSparseNorm-GPT presents a novel approach to enhancing autoregressive language models by integrating hierarchical sparse attention with dynamic layer normalization. This design addresses key challenges related to computational complexity, redundancy in attention heads, and gradient instability. By capturing multi-scale dependencies and ensuring adaptive normalization, HierarchicalSparseNorm-GPT achieves superior performance, scalability, and efficiency compared to traditional transformer architectures. The proposed GAU modifications enable the model to handle longer sequences and more complex tasks effectively, making it a significant advancement in the field of natural language processing.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "gpt4o_0806",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "HierarchicalSparseNorm-GPT integrates hierarchical sparse attention with dynamic normalization to enhance computational efficiency, scalability, and performance in autoregressive language models. By capturing multi-scale dependencies and ensuring gradient stability, this design aims to outperform current state-of-the-art models.",
    "ideation": null,
    "modelname": "HierarchicalSparseNorm-GPT",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory-computation trade-offs\n- Include concrete examples of sparsity patterns\n- Add guidelines for selecting hierarchical levels\n- Specify initialization strategies for different components\n\n2. Training Considerations:\n- Add specific guidance for gradient handling\n- Include stability analysis across different scales\n- Provide recommendations for hyperparameter tuning\n- Consider adaptive learning rate strategies\n\n3. Integration Guidelines:\n- Add detailed steps for combining attention and normalization\n- Include strategies for hardware optimization\n- Provide more detailed cache optimization guidelines\n- Specify requirements for different model scales\n\n4. Performance Analysis:\n- Include expected performance metrics\n- Add ablation study recommendations\n- Provide benchmarking guidelines\n- Specify memory-performance trade-offs\n\n5. Documentation:\n- Add more detailed API specifications\n- Include example configurations\n- Provide debugging guidelines\n- Add performance optimization tips\n\n6. Theoretical Analysis:\n- Expand on stability analysis\n- Include formal proof of complexity bounds\n- Provide more detailed analysis of memory scaling\n- Add theoretical guarantees for convergence",
    "user_input": ""
}