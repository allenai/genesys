{
    "variantname": "KernelDDLerpLinear",
    "review": "The KernelDDLerpLinear proposal presents an innovative approach to enhancing RWKV6's state transformations through kernel methods and hierarchical state representations. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\n1. CLARITY (4.5/5)\nThe proposal is exceptionally well-structured and articulated, with:\n- Clear objectives for improving state transformations\n- Detailed mathematical formulations\n- Well-defined implementation guidelines\n- Comprehensive documentation of components and their interactions\n\n2. INNOVATION (4.0/5)\nThe proposal demonstrates significant innovation through:\n- Novel combination of CPD kernels with hierarchical states\n- Unique approach to selective state updates\n- Integration with RWKV6's DDLerpLinear architecture\nComparison with existing research shows:\n- Builds upon KERPLE's kernel-based position embeddings\n- Extends beyond traditional state space models\n- Introduces new concepts for state management\n\n3. FEASIBILITY (3.5/5)\nImplementation feasibility shows promise but faces challenges:\n+ Well-defined mathematical foundation\n+ Clear implementation guidelines\n+ Maintains linear complexity\n- Complex kernel computations may require optimization\n- Hierarchical state management needs careful implementation\n\n4. SCALABILITY (4.0/5)\nThe design shows strong potential for scaling:\n+ Linear complexity maintained\n+ Selective updates reduce memory requirements\n+ Hierarchical structure supports efficient processing\n- May need optimization for very large models\n\n5. ACCURACY AND ROBUSTNESS (4.0/5)\nExpected improvements in:\n+ Enhanced expressivity through kernel transformations\n+ Better handling of long-range dependencies\n+ Improved position-aware processing\n+ Adaptive state updates for diverse inputs\n\n6. EFFICIENCY (3.5/5)\nEfficiency considerations:\n+ Selective update mechanism reduces computation\n+ Hierarchical state representation optimizes memory usage\n- Kernel computations may introduce overhead\n- Memory requirements for state hierarchy\n\nSTRENGTHS:\n1. Theoretically sound integration of kernel methods\n2. Novel hierarchical state representation\n3. Efficient selective update mechanism\n4. Maintained linear complexity\n5. Clear implementation guidelines\n6. Strong potential for improved expressivity\n\nCONCERNS:\n1. Computational overhead from kernel operations\n2. Memory requirements for hierarchical states\n3. Implementation complexity\n4. Training stability considerations\n5. Hardware optimization challenges",
    "search_stack": [
        {
            "ready": false,
            "query": "kernel methods recurrent neural networks",
            "detail": "Find papers discussing the application of kernel methods in recurrent neural networks, particularly focusing on state representation and transformation techniques.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing the application of kernel methods in recurrent neural networks, particularly focusing on state representation and transformation techniques.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Liquid Structural State-Space Models (Avg. Score: 0.97)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 49/54 (Score: 0.97)*\n\n```\nW. Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb. Time series extrinsic regression. Data Mining and Knowledge Discovery, 35(3):1032-1060, 2021. Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438-9447. PMLR, 2020a. Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020b. T. Trinh, A. Dai, T. Luong, and Q. Le. Learning longer-term dependencies in rnns with auxiliary losses. In International Conference on Machine Learning, pages 4965-4974. PMLR, 2018. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. A. Voelker, I. Kaji\u0107, and C. Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. C. Vorbach, R. Hasani, A. Amini, M. Lechner, and D. Rus. Causal navigation by continuous-time neural networks. Advances in Neural Information Processing Systems, 34, 2021. C. Wang and M. Niepert. State-regularized recurrent neural networks. In International Conference on Machine Learning, pages 6596-6606, 2019. S. Wang, B. Z. Li, M. Khabsa, H.\n```\n\n##### *Relevant Chunk: No. 11/54 (Score: 0.97)*\n\n```\nS. G. Brush. History of the lenz-ising model. Reviews of modern physics, 39(4):883, 1967. S. Chang, Y. Zhang, W. Han, M. Yu, X. Guo, W. Tan, X. Cui, M. Witbrock, M. A. Hasegawa-Johnson, and T. S. Huang. Dilated recurrent neural networks. Advances in neural information processing systems, 30, 2017. B. Charlier, J. Feydy, J. A. Glaun\u00e8s, F.-D. Collin, and G. Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1-6, 2021. URL http:// jmlr.org/papers/v22/20-275.html. Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):1-12, 2018. D. Chen, L. Jacob, and J. Mairal. Recurrent kernel networks. In Advances in Neural Information Processing Systems, pages 13431-13442, 2019.\n```\n\n#### 2. Theoretical Foundations of Deep Selective State-Space Models (Avg. Score: 0.93)\n\n*Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, C. Salvi, Terry Lyons*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** Theoretical grounding is given to this recent finding that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.\n\n**Abstract:** Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.\n\n##### *Relevant Chunk: No. 28/45 (Score: 0.93)*\n\n```\nMartin, E. and Cundy, C. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. Morrill, J., Salvi, C., Kidger, P., and Foster, J. Neural rough differential equations for long time series. In International Conference on Machine Learning, pp. 7829-7838. PMLR, 2021. Nguyen, E., Goel, K., Gu, A., Downs, G. W., Shah, P., Dao, T., Baccus, S. A., and R\u00e9, C. S4nd: Modeling images and videos as multidimensional signals using state spaces. $A d$ vances in Neural Information Processing Systems, 2022. Orvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith, S. L. On the universality of linear recurrences followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023a. Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023b. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Salvi, C., Cass, T., Foster, J., Lyons, T., and Yang, W. The signature kernel is the solution of a goursat pde. SIAM Journal on Mathematics of Data Science, 3(3):873-899,\n2021a. doi: 10.1137/20M1366794. URL https: //doi.org/10.1137/20M1366794. Salvi, C., Lemercier, M., Cass, T., Bonilla, E.\n```\n\n#### 3. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.86)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 17/24 (Score: 0.86)*\n\n```\narXiv preprint arXiv:2209.10655, 2022. [36] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. [37] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. [38] Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf, and Seong-Whan Lee. Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $2501-2508,2020$. [39] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [40] Badri Narayana Patro and Vijay Srinivas Agneeswaran. Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges. arXiv preprint arXiv:2404.16112, 2024. [41] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [42] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys\u0142aw Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.\n```\n\n#### 4. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.86)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 12/30 (Score: 0.86)*\n\n```\nZenodo, Sept. 2021. [17] Felix A. Gers, J\u00fcrgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):2451-2471, 2000. [18] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pages 571-575, 2021. [19] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00edk, Bas R. Steunebrink, and J\u00fcrgen Schmidhuber. Lstm: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28:2222-2232, 2015. [20] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In NeurIPS, 2022. [21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [23] Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3800-3809. PMLR, 2020. [24] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 572-585, 2021. [25] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state-space layers, 2021. [26] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [27] Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns. CoRR, abs/2212.00768, 2022. [28] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. [29] Hongyu He and Marko Kabic. A unified view of long-sequence models towards modeling million-scale dependencies. CoRR, abs/2302.06218, 2023. [30] Sepp Hochreiter and Yoshua Bengio. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: kernel methods recurrent neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Recurrent Kernel Networks\n\n*From Search Query: kernel methods recurrent neural networks*\n\n*Dexiong Chen, Laurent Jacob, J. Mairal*\n\n**TL;DR:** This work generalizes convolutional kernel networks to model gaps in sequences, resulting in a new type of recurrent neural network which can be trained end-to-end with backpropagation, or without supervision by using kernel approximation techniques.\n\n**Abstract:** Substring kernels are classical tools for representing biological sequences or text. However, when large amounts of annotated data is available, models that allow end-to-end training such as neural networks are often prefered. Links between recurrent neural networks (RNNs) and substring kernels have recently been drawn, by formally showing that RNNs with specific activation functions were points in a reproducing kernel Hilbert space (RKHS). In this paper, we revisit this link by generalizing convolutional kernel networks---originally related to a relaxation of the mismatch kernel---to model gaps in sequences. It results in a new type of recurrent neural network which can be trained end-to-end with backpropagation, or without supervision by using kernel approximation techniques. We experimentally show that our approach is well suited to biological sequences, where it outperforms existing methods for protein classification tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 13  (*Influential: 1*)\n\n#### 2. Kernel similarity matching with Hebbian networks\n\n*From Search Query: kernel methods recurrent neural networks*\n\n*Kyle L. Luther, H. Seung*\n\n**TL;DR:** The algorithm proceeds by deriving and then minimizing an upper bound for the sum of squared errors between output and input kernel similarities, which leads to online correlation-based learning rules which can be implemented with a 1 layer recurrent neural network.\n\n**Abstract:** Recent works have derived neural networks with online correlation-based learning rules to perform kernel similarity matching . These works applied existing linear similarity matching algorithms to nonlinear features generated with random Fourier methods. In this paper we attempt to perform kernel similarity matching by directly learning the nonlinear features. Our algorithm proceeds by deriving and then minimizing an upper bound for the sum of squared errors between output and input kernel similarities. The construction of our upper bound leads to online correlation-based learning rules which can be implemented with a 1 layer recurrent neural network. In addition to generating high-dimensional linearly separable representations, we show that our upper bound naturally yields representations which are sparse and selective for specific input patterns. We compare the approximation quality of our method to neural random Fourier method and variants of the popular but non-biological \u201cNystr\u00f6m\u201d method for approximating the kernel matrix. Our method appears to be comparable or better than randomly sampled Nystr\u00f6m methods when the outputs are relatively low dimensional (although still potentially higher dimensional than the inputs) but less faithful when the outputs are very high dimensional.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Reservoir Computing meets Recurrent Kernels and Structured Transforms\n\n*From Search Query: kernel methods recurrent neural networks*\n\n*Jonathan Dong, Ruben Ohana, M. Rafayelyan, Florent Krzakala*\n\n**TL;DR:** The rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence, and shows how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate.\n\n**Abstract:** Reservoir Computing is a class of simple yet efficient Recurrent Neural Networks where internal weights are fixed at random and only a linear output layer is trained. In the large size limit, such random neural networks have a deep connection with kernel methods. Our contributions are threefold: a) We rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence. b) We test our models on chaotic time series prediction, a classic but challenging benchmark in Reservoir Computing, and show how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate. c) When the number of samples is too large, we leverage the success of structured Random Features for kernel approximation by introducing Structured Reservoir Computing. The two proposed methods, Recurrent Kernel and Structured Reservoir Computing, turn out to be much faster and more memory-efficient than conventional Reservoir Computing.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 4. Demystifying Orthogonal Monte Carlo and Beyond\n\n*From Search Query: kernel methods recurrent neural networks*\n\n*Han Lin, Haoxian Chen, Tianyi Zhang, Cl\u00e9ment Laroche, K. Choromanski*\n\n**TL;DR:** NOMC is the first algorithm consistently outperforming OMC in applications ranging from kernel methods to approximating distances in probabilistic metric spaces, and is proposed as a novel extensions of the method leveraging number theory techniques and particle algorithms.\n\n**Abstract:** Orthogonal Monte Carlo (OMC) is a very effective sampling algorithm imposing structural geometric conditions (orthogonality) on samples for variance reduction. Due to its simplicity and superior performance as compared to its Quasi Monte Carlo counterparts, OMC is used in a wide spectrum of challenging machine learning applications ranging from scalable kernel methods to predictive recurrent neural networks, generative models and reinforcement learning. However theoretical understanding of the method remains very limited. In this paper we shed new light on the theoretical principles behind OMC, applying theory of negatively dependent random variables to obtain several new concentration results. We also propose a novel extensions of the method leveraging number theory techniques and particle algorithms, called Near-Orthogonal Monte Carlo (NOMC). We show that NOMC is the first algorithm consistently outperforming OMC in applications ranging from kernel methods to approximating distances in probabilistic metric spaces.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 5. Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods\n\n*From Search Query: kernel methods recurrent neural networks*\n\n*Shunta Akiyama, Taiji Suzuki*\n\n**TL;DR:** This work investigates the excess risk of two-layer ReLU neural networks in a teacher-student regression model, in which a student network learns an unknown teacher network through its outputs, and shows that the student network provably reaches a near-global optimal solution and outperforms any kernel methods estimator, including neural tangent kernel approach, random feature model, and other kernel methods, in a sense of the minimax optimal rate.\n\n**Abstract:** While deep learning has outperformed other methods for various tasks, theoretical frameworks that explain its reason have not been fully established. To address this issue, we investigate the excess risk of two-layer ReLU neural networks in a teacher-student regression model, in which a student network learns an unknown teacher network through its outputs. Especially, we consider the student network that has the same width as the teacher network and is trained in two phases: first by noisy gradient descent and then by the vanilla gradient descent. Our result shows that the student network provably reaches a near-global optimal solution and outperforms any kernel methods estimator (more generally, linear estimators), including neural tangent kernel approach, random feature model, and other kernel methods, in a sense of the minimax optimal rate. The key concept inducing this superiority is the non-convexity of the neural network models. Even though the loss landscape is highly non-convex, the student network adaptively learns the teacher neurons.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Non-Local Neural Networks With Grouped Bilinear Attentional Transforms\n\n*From Search Query: kernel methods recurrent neural networks*\n\n* Changhu Wang,  Yadong Mu,  Zehuan Yuan, Lu Chi*\n\n**Abstract:** Modeling spatial or temporal long-range dependency plays a key role in deep neural networks. Conventional dominant solutions include recurrent operations on sequential data or deeply stacking convolutional layers with small kernel size. Recently, a number of non-local operators (such as self-attention based) have been devised. They are typically generic and can be plugged into many existing network pipelines for globally computing among any two neurons in a feature map. This work proposes a novel non-local operator. It is inspired by the attention mechanism of human visual system, which can quickly attend to important local parts in sight and suppress other less-relevant information. The core of our method is learnable and data-adaptive bilinear attentional transform (BA-Transform), whose merits are three-folds: first, BA-Transform is versatile to model a wide spectrum of local or global attentional operations, such as emphasizing specific local regions. Each BA-Transform is learned in a data-adaptive way; Secondly, to address the discrepancy among features, we further design grouped BA-Transforms, which essentially apply different attentional operations to different groups of feature channels; Thirdly, many existing non-local operators are computation-intensive. The proposed BA-Transform is implemented by simple matrix multiplication and admits better efficacy. For empirical evaluation, we perform comprehensive experiments on two large-scale benchmarks, ImageNet and Kinetics, for image / video classification respectively. The achieved accuracies and various ablation experiments consistently demonstrate significant improvement by large margins.\r\n\n**Proceeding:** cvpr-2020-6\n\n**Published:** 2020-06-01\n\n\n\n#### 2. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\n\n*From Search Query: kernel methods recurrent neural networks*\n\n*Ran El-Yaniv, Yoshua Bengio, Matthieu Courbariaux, Itay Hubara, Daniel Soudry*\n\n**Abstract:** We introduce a method to train Quantized Neural Networks (QNNs) --- neural\nnetworks with extremely low precision (e.g., 1-bit) weights and activations, at\nrun-time. At train-time the quantized weights and activations are used for\ncomputing the parameter gradients. During the forward pass, QNNs drastically\nreduce memory size and accesses, and replace most arithmetic operations with\nbit-wise operations. As a result, power consumption is expected to be\ndrastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and\nImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to\ntheir 32-bit counterparts. For example, our quantized version of AlexNet with\n1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover,\nwe quantize the parameter gradients to 6-bits as well which enables gradients\ncomputation using only bit-wise operation. Quantized recurrent neural networks\nwere tested over the Penn Treebank dataset, and achieved comparable accuracy as\ntheir 32-bit counterparts using only 4-bits. Last but not least, we programmed\na binary matrix multiplication GPU kernel with which it is possible to run our\nMNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering\nany loss in classification accuracy. The QNN code is available online.\n\n**Published:** 2016-09-22\n\n\n\n#### 3. A Bag-of-Words Equivalent Recurrent Neural Network for Action Recognition\n\n*From Search Query: kernel methods recurrent neural networks*\n\n*Alexander Richard, Juergen Gall*\n\n**Abstract:** The traditional bag-of-words approach has found a wide range of applications\nin computer vision. The standard pipeline consists of a generation of a visual\nvocabulary, a quantization of the features into histograms of visual words, and\na classification step for which usually a support vector machine in combination\nwith a non-linear kernel is used. Given large amounts of data, however, the\nmodel suffers from a lack of discriminative power. This applies particularly\nfor action recognition, where the vast amount of video features needs to be\nsubsampled for unsupervised visual vocabulary generation. Moreover, the kernel\ncomputation can be very expensive on large datasets. In this work, we propose a\nrecurrent neural network that is equivalent to the traditional bag-of-words\napproach but enables for the application of discriminative training. The model\nfurther allows to incorporate the kernel computation into the neural network\ndirectly, solving the complexity issue and allowing to represent the complete\nclassification system within a single network. We evaluate our method on four\nrecent action recognition benchmarks and show that the conventional model as\nwell as sparse coding methods are outperformed.\n\n**Published:** 2017-03-23\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on the application of kernel methods in recurrent neural networks (RNNs) for state representation and transformation techniques, here are some key insights and references:\n\n## Kernel Methods in RNNs\n\n### Recurrent Kernels\nThe concept of Recurrent Kernels, as discussed in, offers a promising approach. Recurrent Kernels are a variant of kernel methods that dynamically update scalar products over time based on changes in the input data. This method can be seen as an alternative to large-scale Reservoir Computing and has shown state-of-the-art performance in tasks like chaotic time series prediction.\n\n- **Application to RNNs**: By defining Recurrent Kernels for different Reservoir Computing topologies (e.g., Leaky, Sparse, Deep), you can extend this approach to RNNs. This involves computing the Gram matrix for the Recurrent Kernels, which can capture the temporal dependencies in the data more effectively than traditional RNNs.\n\n### Neural Tangent Kernel (NTK)\nThe Neural Tangent Kernel (NTK) provides a connection between neural networks and kernel methods. In the context of RNNs, NTK can help in understanding how the network's parameters evolve during training.\n\n- **Kernel Interpretation**: The NTK shows that in the infinite-width limit, the behavior of a neural network is equivalent to kernel regression with the NTK. This can be extended to RNNs by interpreting their state transitions through a kernel framework, potentially improving stability and generalization.\n\n## State Representation and Transformation\n\n### Kernelized Position Embedding\nThe idea of kernelizing positional differences, as proposed in KERPLE (Kernelized Relative Positional Embedding), can be adapted for RNNs. This approach uses conditionally positive definite (CPD) kernels to generalize relative position embedding, which can enhance length extrapolation capabilities.\n\n- **Adaptation to RNNs**: By incorporating kernel-based position embedding into the recurrent architecture, you can improve the model's ability to handle long-range dependencies and extrapolate to longer sequence lengths.\n\n### Dynamic State Handling\nUsing kernel methods to transform and manage the state in RNNs can be beneficial. For instance, the dynamic recurrence in RWKV6 could be enhanced by incorporating position-aware scaling using kernel-based transformations.\n\n- **Kernel-Based State Transformations**: By applying kernel methods to the state transitions in RNNs, you can potentially improve the model's ability to capture complex temporal dependencies and enhance memory efficiency through hierarchical state representation.\n\n## Efficiency and Scalability\n\n### Memory-Efficient Attention Mechanisms\nCombining kernel methods with efficient attention mechanisms, such as the two-level attention schema in Poolingformer, can improve memory efficiency.\n\n- **Hierarchical Attention**: Implementing a hierarchical attention mechanism within the recurrent framework, using kernel-based transformations to aggregate information, can help in handling long-range dependencies more efficiently.\n\n### Computational Efficiency\nUsing Recurrent Kernels or NTK can provide analytical formulations that are well-suited for theoretical studies and can help in optimizing the computational efficiency of the model.\n\n- **Gram Matrix Computation**: While Recurrent Kernels require computing scalar products between all pairs of input points, this can be optimized by using efficient linear algebra techniques and leveraging the deterministic limit provided by these kernels.\n\nBy integrating these approaches, the researchers can develop a more robust and efficient autoregressive language model that leverages the strengths of kernel methods in handling state representation, transformation, and temporal dependencies.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis:\nFrom analyzing the seed design (RWKV6) and initial references, several key observations emerge:\n\na) Current State and Limitations:\n- RWKV6 uses a dual-path architecture with attention and feedforward paths\n- Matrix-valued states and dynamic recurrence are key features\n- The design faces challenges in length extrapolation and efficiency\n- Current implementation relies heavily on linear transformations (LerpLinear, DDLerpLinear)\n\nb) Potential Areas for Innovation:\n- Position embedding and length extrapolation\n- Memory efficiency in attention mechanisms\n- Dynamic state handling\n- Computational efficiency in recurrent structures\n\n2. Useful References with Excerpts:\n\nFrom \"KERPLE: Kernelized Relative Positional Embedding\":\n> \"We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences using conditionally positive definite (CPD) kernels\"\n> \"The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way\"\n\nThis suggests potential improvements in position handling using kernel methods.\n\nFrom \"Scaling Laws of RoPE-based Extrapolation\":\n> \"fine-tuning a RoPE-based LM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance\"\n> \"achieve extrapolation up to 1 million context length within only 16K training length\"\n\nIndicates possibilities for improving length extrapolation capabilities.\n\nFrom \"Poolingformer\":\n> \"two-level attention schema... first level uses a smaller sliding window pattern to aggregate information from neighbors. Its second level employs a larger window to increase receptive fields with pooling attention\"\n\nSuggests efficient approaches to handling long-range dependencies.\n\n3. Key Insights and Analysis:\n\na) Position Embedding Innovation:\n- Current RWKV6 could benefit from more sophisticated position handling\n- KERPLE's kernel-based approach could be adapted for the recurrent setting\n- RoPE's scaling insights could inform dynamic position handling\n\nb) Attention Mechanism Improvements:\n- Poolingformer's hierarchical approach could inspire more efficient attention\n- Two-level processing could be adapted to work within RWKV's recurrent framework\n- Memory efficiency could be improved through strategic pooling\n\nc) State Management:\n- Current matrix-valued states could be enhanced with kernel-based transformations\n- Dynamic recurrence could incorporate position-aware scaling\n- Memory efficiency could be improved through hierarchical state representation\n\n4. Future Search Plan:\nNeed to investigate:\n1. Kernel methods in recurrent architectures\n2. Memory-efficient attention mechanisms\n3. State compression techniques\n\n5. References:\n\n[1] Chi, T. C., Fan, T. H., Ramadge, P., & Rudnicky, A. I. (2022). KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. Neural Information Processing Systems.\n\n[2] Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., & Lin, D. (2023). Scaling Laws of RoPE-based Extrapolation. arXiv preprint.\n\n[3] Zhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., & Chen, W. (2021). Poolingformer: Long Document Modeling with Pooling Attention. International Conference on Machine Learning.\n\n[4] Peng, B., et al. (2024). Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence. arXiv preprint."
        },
        {
            "ready": false,
            "query": "liquid state space models selective attention",
            "detail": "Find papers discussing liquid state space models and selective attention mechanisms, particularly focusing on their application in recurrent neural networks and language models.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing liquid state space models and selective attention mechanisms, particularly focusing on their application in recurrent neural networks and language models.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 1.00)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 3/35 (Score: 1.00)*\n\n```\nFinally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code ${ }^{1}$ and pre-trained model checkpoints ${ }^{2}$ are publicly available. ## 2 Background\n\n### 2.1 Attention Mechanism\n\nSuppose the input to the layer is $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, where $L$ is the sequence length and $d$ is the embedding dimension, then the attention mechanism outputs\n\n$$\n\\operatorname{Attn}(\\mathbf{X})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}$. Here $\\mathbf{W}_{q}, \\mathbf{W}_{k}, \\mathbf{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix $\\mathbf{A}=$ $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\in \\mathbb{R}^{L \\times L}$. Then, $\\mathbf{A}_{i j}$ captures the alignment between the $i$-th and the $j$-th input tokens. ### 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a 1-dimensional input signal $u(t)$ to a $d_{s}$-dimensional latent state $x(t)$, after which $x(t)$ is mapped to a 1-dimensional output signal $y(t)$. Concretely,\n\n$$\nx^{\\prime}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nHere, $\\mathbf{A} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{B} \\in \\mathbb{R}^{d_{s}}$ and $\\mathbf{C} \\in \\mathbb{R}^{d_{s}}$. Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$\n\n[^1]cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, highorder polynomial projection operators) are proposed to initialize A. The HiPPO matrices are designed such that the state $x(t)$ at time $t$ can memorize the history of the input $u(t)$ up to time $t$. Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, where $L$ is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size $\\Delta$, such that\n\n$$\n\\begin{aligned}\n& x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k} \\\\\n& \\text { where } \\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1}(\\mathbf{I}+\\Delta / 2 \\cdot \\mathbf{A}) \\\\\n& \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1} \\Delta \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n\\end{aligned}\n$$\n\nWe unroll the above recurrent representation, after which we have\n\n$$\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\cdots+\\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C B}} u_{k}\n$$\n\nThis can be written as a convolutional representation $y=\\overline{\\mathbf{K}} * u$, where the convolution kernel\n\n$$\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\cdots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nHere, \" $*$ \" is the discrete convolution operator, $u$ represents the input sequence $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, and $y$ represents the corresponding output sequence $\\left(y_{0}, y_{1}, \\cdots, y_{L}\\right)$.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.99)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.99)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 4. Resurrecting Recurrent Neural Networks for Long Sequences  (Avg. Score: 0.99)\n\n*Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 146  (*Influential: 26*)\n\n**TL;DR:** This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Abstract:** Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n##### *Relevant Chunk: No. 31/71 (Score: 0.99)*\n\n```\nR. Hasani, M. Lechner, T.-H. Wang, M. Chahine, A. Amini, and D. Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. K. Helfrich, D. Willmott, and Q. Ye. Orthogonal recurrent neural networks with scaled cayley transform. In International Conference on Machine Learning. PMLR, 2018. T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github . com /deepmind/dm-haiku. S. Hochreiter. Untersuchungen zu dynamischen neuronales netzen. Diploma thesis, Institut f'ur Informatik, Technische Universit\"at M\"unchen, 1991. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 1997. J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 1982.\n```\n\n#### 5. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.98)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.98)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: liquid state space models selective attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Liquid Structural State-Space Models\n\n*From Search Query: liquid state space models selective attention*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: liquid state space models selective attention*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: liquid state space models selective attention*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 268  (*Influential: 21*)\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: liquid state space models selective attention*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 154  (*Influential: 36*)\n\n#### 5. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: liquid state space models selective attention*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using liquid state space models and selective attention mechanisms, here are some key findings and references that align with your goals:\n\n## Liquid State Space Models\n\nLiquid state space models, particularly those with input-dependent dynamics, show promise in enhancing sequence modeling capabilities.\n\n- The **S7 model** introduced in the papers and builds upon previous state-space models (SSMs) like S4, S5, and Mamba. S7 incorporates input-dependent dynamics, allowing the model to adapt state transitions based on the current input. This feature enables the model to capture long-range dependencies and perform content-based reasoning efficiently. The input-dependent state transitions and stable reparameterization in S7 can be particularly beneficial for autoregressive language models, as they improve the model's ability to handle complex sequences.\n\n## Selective Attention Mechanisms\n\nSelective attention mechanisms are crucial for focusing on relevant parts of the input sequence, which can enhance the performance of autoregressive language models.\n\n- The concept of **selective state spaces** in models like Mamba and Mamba-2 involves making the transition matrices and other parameters functions of the input. This selective mechanism allows the model to filter information and retain only the relevant parts, which is similar to how selective attention works in other models. This approach can be integrated into autoregressive language models to improve their ability to capture context and long-term dependencies.\n\n## Integration with Recurrent Neural Networks\n\nFor autoregressive language models, integrating liquid state space models with selective attention mechanisms can be done within the framework of recurrent neural networks (RNNs).\n\n- **Hierarchical Gating Mechanisms** as discussed in the analysis note can be combined with liquid state-space models. Hierarchical gating allows different layers to model different temporal scales, which can be enhanced by the adaptive state transitions of liquid state-space models.\n- **Kernel Methods** can also be integrated to enhance state representations. The kernel structure in liquid state-space models can take into account the similarities of the input sequence samples, which can improve the model's performance in capturing context and dependencies.\n\n## Specific Implementations and Future Directions\n\n- **S7 Model**: The S7 model's ability to dynamically adjust state transitions based on input content and its stable reparameterization makes it a strong candidate for improving autoregressive language models. Its application in long-sequence modeling tasks and its efficiency in handling complex dependencies are particularly relevant.\n- **Mamba and Mamba-2**: These models' selective state spaces and input-dependent adaptations can be adapted for language modeling tasks, especially in handling multivariate sequence data and capturing extended dependencies.\n\nBy combining these elements\u2014liquid state space models with input-dependent dynamics, selective attention mechanisms, and hierarchical gating\u2014the researcher can develop an autoregressive language model that is more efficient, adaptable, and capable of capturing long-range dependencies, thereby improving overall performance.\n",
            "analysis": "Analysis Note: Kernel Methods and State Space Models\n\n1. Summary of Analysis:\nThe search results reveal promising directions for enhancing RWKV6's architecture through kernel methods and state space modeling:\n\na) Key Areas of Interest:\n- Kernel-based state representations\n- Liquid state-space models\n- Hierarchical gating mechanisms\n- Efficient sequence modeling approaches\n\nb) Potential Innovation Paths:\n- Integration of kernel methods with state transformations\n- Liquid structural adaptations\n- Hierarchical gating for improved memory management\n- Selective state space modeling\n\n2. Useful References with Excerpts:\n\nFrom \"Liquid Structural State-Space Models\":\n> \"LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference\"\n> \"The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference\"\n\nFrom \"Theoretical Foundations of Deep Selective State-Space Models\":\n> \"when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input\"\n\nFrom \"Hierarchically Gated Recurrent Neural Network\":\n> \"includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers\"\n> \"allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies\"\n\n3. Key Insights and Analysis:\n\na) State Space Innovation:\n- Liquid state-space models offer adaptive state transitions\n- Selective state mechanisms can improve efficiency\n- Hierarchical gating can better manage different temporal scales\n\nb) Kernel Integration Opportunities:\n- Kernel methods can enhance state representations\n- Input-dependent adaptations improve model flexibility\n- Signature-based projections offer theoretical guarantees\n\nc) Architectural Considerations:\n- Two-level processing can improve efficiency\n- Hierarchical structures can better handle dependencies\n- Adaptive mechanisms can enhance model capabilities\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific implementations of liquid state-space models\n2. Selective attention mechanisms in recurrent architectures\n3. Efficient kernel approximation techniques\n\n5. References:\n\n[1] Hasani, R. M., et al. (2022). Liquid Structural State-Space Models. International Conference on Learning Representations.\n\n[2] Cirone, N. M., et al. (2024). Theoretical Foundations of Deep Selective State-Space Models. arXiv preprint.\n\n[3] Qin, Z., Yang, S., & Zhong, Y. (2023). Hierarchically Gated Recurrent Neural Network for Sequence Modeling. Neural Information Processing Systems.\n\n[4] Zimerman, I., Ali, A., & Wolf, L. (2024). A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hierarchical state space models implementation",
            "detail": "Find papers discussing hierarchical architectures in state space models, particularly focusing on implementation details and performance analysis.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing hierarchical architectures in state space models, particularly focusing on implementation details and performance analysis.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.98)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.98)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n#### 2. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.98)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 1/27 (Score: 0.98)*\n\n```\n# State Space Models as Foundation Models: A Control Theoretic Overview \n\nCarmen Amo Alonso*, Jerome Sieber*, and Melanie N. Zeilinger\n\n\n#### Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences. Index Terms-Machine learning, Linear systems, Timevarying systems. ## I. INTRODUCTION\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models' capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window ${ }^{1}$ and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the\n\n[^0]attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inferencein contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section $\\Pi$, we provide an overview of the essential components and considerations in SSMs. In Section III we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. ## II. State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. ## A. Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\n$$\ny(k)=f(u(k), \\ldots, u(k-T) ; \\theta)\n$$\n\nwhere at each time $k$, the output $y(k)$ is produced after evaluating an input signal of length $k-T$, i.e., $u(k), \\ldots, u(k-$ $T)$, and a set of parameters $\\theta$. The parameters $\\theta$ are task dependent, and can be fine-tuned accordingly. Since the search space of general $f(\\cdot ; \\theta)$ is too broad, different parameterizations of $f(\\cdot ; \\theta)$ can be used to render the problem tractable. For instance, the model $f(\\cdot ; \\theta)$ can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of $f(\\cdot ; \\theta)$ is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model $f(\\cdot ; \\theta)$ is presented with input-output pairs $(u(k), y(k)) \\forall k$, where $\\theta$ represents the parameters. The parameters $\\theta$ are then iteratively updated to minimize a loss function $\\mathscr{L}(\\cdot)$, i.e., iteratively solving the following optimization problem\n\n$$\n\\min _{\\theta} \\mathscr{L}(y-f(u ; \\theta))\n$$\n\nFor a language model the inputs $u$ are tokenized ${ }^{2}$ sentences and the outputs $y$ are a shifted version of the same inputs, i.e., an auto-regressive setup. ## B. Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\n$$\n\\begin{aligned}\n& \\dot{x}(t)=A x(t)+B u(t) \\\\\n& y(t)=C x(t)+D u(t)\n\\end{aligned}\n$$\n\nwhere $x \\in \\mathbb{C}^{p}$ represents the complex-valued state, $u, y \\in \\mathbb{R}^{q}$ are the input and the output, respectively, and $t$ denotes the continuous-time index. We note that the input fed into the system denoted as $u$ is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. $A, B, C, D$ are complex-valued matrices of appropriate dimensions and in representation (3), these matrices\n\n[^1]are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., $A_{t}, B_{t}, C_{t}, D_{t}$. In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input $u(t)$ represents a signal or input token fed into the model at a given time $t$. The state $x(t)$ represents the hidden state that stores the relevant information about the current and previous inputs up to time $t$, and $y(t)$ is the output of the model at time $t$. In a learning setup, the matrices $A, B, C, D$ are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix $A$ is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption 2.1: The dynamic matrix in dynamics (3) has a diagonal structure, i.e., $A=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)$ with $\\lambda_{i} \\in \\mathbb{C} \\forall i$. Although initial proposals [11], [12] deviate slightly from Assumption 2.1, most of the Structured SSMs literature assumes a diagonal $A$ matrix. Specific choices will be discussed in Section III\n\n## C. Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\n$$\n\\begin{aligned}\nx(k+1) & =\\bar{A} x(k)+\\bar{B} u(k) \\\\\ny(k) & =\\bar{C} x(k)+\\bar{D} u(k)\n\\end{aligned}\n$$\n\nwhere $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ are the discrete-time dynamic matrices discretized with time-step $\\Delta \\in \\mathbb{R}$, possibly with complexvalued components, and $k$ denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III\nWe note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. ## D. Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix $A$ have a significant impact on the performance after training: on a simple classification task, performance increases from $67 \\%$ when $A$ is randomly initialized, to $80 \\%$ when $A$ is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state $x(k)$ being able to capture the recent history of the inputs $u(k), \\ldots, u(k-T)$ for some time horizon $T$. This property is referred to as memory in the standard SSM literature. As is well-known in control\ntheory, the memory of system (4) is directly linked to the eigenvalues of matrix $A$. Lemma 2.2: (Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of $A$ are inside the unit circle and very close to the unit circumference, i.e. $|\\operatorname{eig}(A)| \\leq 1$ and $|\\operatorname{eig}(A)| \\approx 1 \\forall \\operatorname{eig}(A)$. Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned $A$ matrix is approximately equal to (but not bigger than) 1 . For the initialization of the other matrices, i.e., $B, C$, and $D$, standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of $B, C$, and $D$ in the following and refer the reader to the original papers [14], [15]. ## E. Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation $u(1)$ and zero initial state $x(1)=0$. In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a noncausal representation since input-output pairs $(u(k), y(k))$ are available for all $k$. Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\n$$\ny(k)=\\sum_{\\tau=0}^{k} \\bar{C} \\bar{A}^{k-\\tau} \\bar{B} u(\\tau)\n$$\n\nThis convolutional representation allows for faster learning because the complete input sequence $u(k) \\forall k$ can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. ## F. Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input $u$ and postprocessing of the output $y$ is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding\nA\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-3.jpg?height=776&width=874&top_left_y=211&top_left_x=1078)\n\nFig. 1: A. General scaffolding of a SSM. The dynamical model (4) is represented in green. The input to the SSM is pre-processed and forked off in a skip connection (lower signal). The nature of the pre-processing map (linear or nonlinear) depends on the specific scaffolding. The output of the recursion is then post-processed with a nonlinear gate. B. Overall architecture of a SSM. Each of the SSMs including its scaffolding (Fig. 1.A.) is structured in a layered fashion, where the output from one layer is the input to the next. the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition 2.3. In general, a linear or nonlinear map is performed on the input $\\bar{u}$ before it is fed into system (4). Once the output $y$ has been computed, a gating operation is generally performed to control the flow of information from the input $\\tilde{u}$ to the output $\\tilde{y}$. Intuitively, the gate $g(\\tilde{y}, \\tilde{u})$ controls which outputs $\\tilde{y}$ are set to zero based on the inputs $\\tilde{u}$ via the softmax operation. Definition 2.3: Given two vectors $x_{1}, x_{2} \\in \\mathbb{R}^{p}$, a gating operation is defined as $g\\left(x_{1}, x_{2}\\right):=x_{1} \\odot \\sigma\\left(W x_{2}\\right)$, where $W \\in \\mathbb{R}^{p \\times p}$, $\\odot$ is the element-wise multiplication, and $\\sigma$ is the softmax operation ${ }^{3}$\n\nAs is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input $y$ and output $u$ are of the same dimension $\\mathbb{R}^{q}$. For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matri-\n\n[^2]ces), and the size of the systems ranges in $p \\in[64,512], q \\in$ [32, 1024]. For language modelling the number of layers and system size can be significantly larger.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.95)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.95)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.90)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.90)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.87)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.87)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical state space models implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: hierarchical state space models implementation*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 2. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: hierarchical state space models implementation*\n\n*Paul Mattes, Rainer Schlosser, R. Herbrich*\n\n**TL;DR:** Hieros is a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space that allows for more efficient training than RNN- based world models and more efficient imagination than Transformer-based world models.\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: hierarchical state space models implementation*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Robustifying State-space Models for Long Sequences via Approximate Diagonalization\n\n*From Search Query: hierarchical state space models implementation*\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 5. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: hierarchical state space models implementation*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 209  (*Influential: 37*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures\n\n*From Search Query: hierarchical state space models implementation*\n\n*Risi Kondor, Truong Son Hy, Nhat Khang Ngo*\n\n**Abstract:** Contemporary graph learning algorithms are not well-defined for large molecules since they do not consider the hierarchical interactions among the atoms, which are essential to determine the molecular properties of macromolecules. In this work, we propose Multiresolution Graph Transformers (MGT), the first graph transformer architecture that can learn to represent large molecules at multiple scales. MGT can learn to produce representations for the atoms and group them into meaningful functional groups or repeating units. We also introduce Wavelet Positional Encoding (WavePE), a new positional encoding method that can guarantee localization in both spectral and spatial domains. Our proposed model achieves competitive results on two macromolecule datasets consisting of polymers and peptides, and one drug-like molecule dataset. Importantly, our model outperforms other state-of-the-art methods and achieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO and LUMO) calculated by Density Functional Theory (DFT) in the polymers dataset. Furthermore, the visualizations, including clustering results on macromolecules and low-dimensional spaces of their representations, demonstrate the capability of our methodology in learning to represent long-range and hierarchical structures. Our PyTorch implementation is publicly available at https://github.com/HySonLab/Multires-Graph-Transformer\n\n**Published:** 2023-02-17\n\n\n\n#### 2. A no-regret generalization of hierarchical softmax to extreme multi-label classification\n\n*From Search Query: hierarchical state space models implementation*\n\n*Krzysztof Dembczy\u0144ski, R\u00f3bert Busa-Fekete, Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov*\n\n**Abstract:** Extreme multi-label classification (XMLC) is a problem of tagging an instance\nwith a small subset of relevant labels chosen from an extremely large pool of\npossible labels. Large label spaces can be efficiently handled by organizing\nlabels as a tree, like in the hierarchical softmax (HSM) approach commonly used\nfor multi-class problems. In this paper, we investigate probabilistic label\ntrees (PLTs) that have been recently devised for tackling XMLC problems. We\nshow that PLTs are a no-regret multi-label generalization of HSM when\nprecision@k is used as a model evaluation metric. Critically, we prove that\npick-one-label heuristic - a reduction technique from multi-label to\nmulti-class that is routinely used along with HSM - is not consistent in\ngeneral. We also show that our implementation of PLTs, referred to as\nextremeText (XT), obtains significantly better results than HSM with the\npick-one-label heuristic and XML-CNN, a deep network specifically designed for\nXMLC problems. Moreover, XT is competitive to many state-of-the-art approaches\nin terms of statistical performance, model size and prediction time which makes\nit amenable to deploy in an online system.\n\n**Conference:** a-no-regret-generalization-of-hierarchical-1\n\n**Published:** 2018-10-27\n\n\n\n#### 3. The generalized Hierarchical Gaussian Filter\n\n*From Search Query: hierarchical state space models implementation*\n\n*Christoph Mathys, Klaas Enno Stephan, Anna Hedvig M\u00f8ller, Nicolas Legrand, Peter Thestrup Waade, Lilian Aline Weber*\n\n**Abstract:** Hierarchical Bayesian models of perception and learning feature prominently in contemporary cognitive neuroscience where, for example, they inform computational concepts of mental disorders. This includes predictive coding and hierarchical Gaussian filtering (HGF), which differ in the nature of hierarchical representations. Predictive coding assumes that higher levels in a given hierarchy influence the state (value) of lower levels. In HGF, however, higher levels determine the rate of change at lower levels. Here, we extend the space of generative models underlying HGF to include a form of nonlinear hierarchical coupling between state values akin to predictive coding and artificial neural networks in general. We derive the update equations corresponding to this generalization of HGF and conceptualize them as connecting a network of (belief) nodes where parent nodes either predict the state of child nodes or their rate of change. This enables us to (1) create modular architectures with generic computational steps in each node of the network, and (2) disclose the hierarchical message passing implied by generalized HGF models and to compare this to comparable schemes under predictive coding. We find that the algorithmic architecture instantiated by the generalized HGF is largely compatible with that of predictive coding but extends it with some unique predictions which arise from precision and volatility related computations. Our developments enable highly flexible implementations of hierarchical Bayesian models for empirical data analysis and are available as open source software.\n\n**Published:** 2023-05-18\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using hierarchical state space models, here are some key findings and implementation details from the provided sources:\n\n## Hierarchical State Space Models\n\n### Efficient State Space Model (eSSM)\nThe paper on \"Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization\" introduces a hierarchical approach to state space models. The eSSM model is designed to handle long sequences efficiently by using a convolutional representation of multi-input multi-output (MIMO) SSMs. This model employs:\n- **Block Diagonalization**: To reduce the number of model parameters and improve model flexibility.\n- **Fast Tensor Convolution**: Utilizing the fast Fourier transform (FFT) to enhance computational efficiency.\n- **Hierarchical Structure**: Stacking eSSM layers to form a deep model, which improves performance and efficiency.\n\n### SC-MAMBA2\nSC-MAMBA2 integrates state-space models (SSMs) with the MAMBA framework, particularly for single-cell ultra-long transcriptome data. This model:\n- **Bidirectional Architecture**: Uses a bidirectional approach to learn relationships among genes efficiently, which can be adapted for hierarchical processing in sequence modeling.\n- **State Space Duality**: Simplifies the matrix A into a scalar, allowing for both linear and quadratic forms of selective state space models. This simplification can enhance the efficiency and scalability of the model.\n\n## Implementation Details and Performance Analysis\n\n### Efficient Parallel Computation\nBoth eSSM and SC-MAMBA2 emphasize the importance of efficient parallel computation. For instance:\n- **eSSM**: Utilizes convolutional representations and fast tensor convolution via FFT to enable efficient training with parallel computing.\n- **SC-MAMBA2**: Implements parameters that are input-independent, addressing limitations in previous Linear Time Invariant (LTI) SSM models and improving adaptability and performance through convolutional methods.\n\n### Selective Mechanisms and Kernel Methods\n- **Selective State Spaces**: SC-MAMBA2 leverages selective state space models, which can improve efficiency by focusing on relevant information. This is achieved through the State Space Duality concept, simplifying the matrix A into a scalar.\n- **Kernel Methods**: The use of convolution kernels in both eSSM and SC-MAMBA2 enhances the representation power of the models. For example, eSSM uses fast tensor convolution, and SC-MAMBA2 employs a convolutional manner for implementing state transitions.\n\n### Hardware-Aware Implementation\nThe implementation of these models considers hardware constraints, which is crucial for efficient training and inference. For instance:\n- **eSSM**: The diagonalization and block diagonalization strategies reduce the number of model parameters, improving computational and storage efficiency.\n- **SC-MAMBA2**: The model is designed to scale linearly with sequence length, making it efficient for handling large-scale biological data.\n\n## Future Directions\n\nTo further improve autoregressive language model design, consider the following:\n\n- **Hierarchical Processing**: Implement hierarchical state space models similar to eSSM and SC-MAMBA2, where each layer processes different aspects of the sequence data.\n- **Kernel Methods**: Leverage kernel-based methods to enhance the representation power of the model, as seen in the convolutional representations used in eSSM and SC-MAMBA2.\n- **Selective Mechanisms**: Incorporate selective state space models to improve efficiency by focusing on relevant information in the sequence data.\n- **Hardware-Aware Optimization**: Optimize the model for hardware constraints to ensure efficient training and inference, using techniques such as re-parameterization and optimized CUDA kernels.\n\nBy integrating these elements, the researcher can develop a more efficient, scalable, and performant autoregressive language model.\n",
            "analysis": "Analysis Note: State Space Models and Selective Mechanisms\n\n1. Summary of Analysis:\nThe search results reveal important developments in state space models (SSMs) and selective mechanisms that could inform our LM block design:\n\na) Key Architectural Advances:\n- Input-dependent state transitions\n- Selective state spaces\n- Hierarchical processing\n- Kernel-based methods\n- Efficient parallel computation\n\nb) Performance Considerations:\n- Trade-off between compression and expressivity\n- Balance between local and global information\n- Importance of hardware-aware implementation\n- Scaling behavior with sequence length\n\n2. Useful References with Excerpts:\n\nFrom \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\":\n> \"a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning\"\n> \"letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n\nFrom \"Understanding the differences in Foundation Models\":\n> \"The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models\"\n> \"state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model\"\n\nFrom \"Liquid Structural State-Space Models\":\n> \"LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module\"\n> \"The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples\"\n\n3. Key Insights and Analysis:\n\na) State Space Innovation:\n- Input-dependent dynamics allow adaptive processing\n- Selective mechanisms can improve efficiency\n- Kernel methods can enhance representation power\n- Hardware-aware implementation is crucial\n\nb) Architectural Considerations:\n- Need to balance compression vs expressivity\n- Importance of selective information flow\n- Potential for hierarchical processing\n- Trade-off between parallel and sequential computation\n\nc) Implementation Strategy:\n- Consider hardware constraints\n- Focus on efficient state transitions\n- Leverage kernel methods for enhanced representation\n- Incorporate selective mechanisms\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific implementations of hierarchical processing in SSMs\n2. Kernel methods for state representation\n3. Hardware-efficient implementations of selective mechanisms\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[2] Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint.\n\n[3] Hasani, R. M., et al. (2022). Liquid Structural State-Space Models. International Conference on Learning Representations.\n\n[4] Zuo, S., et al. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint.\n\n[5] Smith, J. T. H., et al. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "kernel methods RWKV implementation",
            "detail": "Find papers discussing the implementation of kernel methods in RWKV-like architectures, particularly focusing on state representation and transformation techniques.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing the implementation of kernel methods in RWKV-like architectures, particularly focusing on state representation and transformation techniques.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Reinventing RNNs for the Transformer Era (Avg. Score: 0.88)\n\n*Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 251  (*Influential: 27*)\n\n**TL;DR:** This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n**Abstract:** Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n\n##### *Relevant Chunk: No. 40/48 (Score: 0.92)*\n\n```\nFigures (2, 3, 3, 8). Experiments section 6. Appendices E, K. Contributions to Appendix M. Quentin Anthony Manuscript (organization, initial draft sections 1, C, 2; revision and proofreading; final version). Alon Albalak Manuscript (abstract and sections 1, 9, and 7; proofreading and revision). Samuel Arcadinho Contributions to Figures 7, 13, and 14. Contributions to Appendix K. Stella Biderman Performed the scaling laws analysis and evaluated competitor models on benchmark tasks. Huanqi Cao Manuscript (contributions to 3.2 and 3.3; proofreading and revision). Experiments for Appendix I. Xin Cheng Manuscript (proofreading and revision). Contributions to Appendix M, J. Michael Chung Manuscript (contributions to section I; proofreading and revision). Xingjian Du Evaluation on Long Range Arena Benchmark (TBD until 5.31). Matteo Grella Manuscript (sections H, I, 8; contributions to sections 1, 7 and 9; proofreading and revision). Contributions to Appendix D. Kranthi Kiran GV Manuscript (sections C and 5; contributions to section 2; revision and proofreading). Tables K and K . Appendix 4. Xuzheng He Manuscript (contributions to section 2; proofreading and revision). Contributions to Figure8. Appendix I. Contributions to appendix H. Haowen Hou Figure 9. Appendix F. Jiaju Lin RWKV on LRA benchmarking\nPrzemys\u0142aw Kazienko Manuscript (proofreading and revision). Contributions to Section 6, 9, and Appendix L. Jan Kocon Manuscript (Section 1; proofreading and revision). Contributions to Appendix L. Jiaming Kong Manuscript (revision and proofreading). Appendix H. Barttomiej Koptyra Manuscript (revision and proofreading) Contributions to Appendix L. Hayden Lau Manuscript (contributions to section 1 and 9; proofreading and revision). Contributions to Appendix M. Krishna Sri Ipsit Mantri Figure 12\nFerdinand Mom Manuscript (contributions to section 1, C, 3.3, I; proofreading and revision). Contributions to Appendix D. Atsushi Saito Manuscript (sections 2 and 5; contributions to section C). Contributions to Appendix J\nGuangyu Song Manuscript (rewrote section 3; final version). Initial draft Ethics Statement). Xiangru Tang Manuscript (sections C and 2; contributions to abstract; revision and proofreading). Contributions to Appendix M. Bolun Wang Contributions to Tables 1. Johan S. Wind RWKV performance optimizations (CUDA), Contributions to Appendix 4. Stanis\u0142aw Wo\u017aniak Contributions to Appendix L. Ruichong Zhang Manuscript (proofreading and revision); Contributions to Figure 6 and Appendix M. Zhenyuan Zhang Manuscript (revision and proofreading). Figure 3. Experiments Appendix I. Contributions to Appendices D and M. Qihang Zhao Manuscript (proofreading and revision). Contributions to Table 5. Peng Zhou Contributions to Tables 1 and Table 5. Qinghua Zhou Manuscript (Proofreading and revision of section 3; Add missing citations in 3.3). Revision of Figures 2 and 12. Jian Zhu Manuscript (section C; proofreading and revision). Figures 3 and 6. Rui-Jie Zhu Tables 1 and 5. Experiments for table 5. ## C Additional Related Work\n\nRecently, a number of techniques have been proposed to address the limitations of transformers. Optimizing Attention Mechanism Many transformer variants (\"x-formers\") have been introduced to reduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020; Kitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al., 2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other efficient methods (Katharopoulos et al., 2020; Jaegle et al., 2021). Some recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022; Jang et al., 2019) share similarities with RWKV's chunked computation scheme. Despite being memory-efficient, their time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV achieves better space and time complexity during inference by formulating a linear attention as an RNN. Attention Free Models Another line of research replaces the attention mechanism with other modules to scale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021; Liu et al., 2021) propose replacing attention by Multi-Layer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer (AFT) (Zhai et al., 2021) and HrrFormer (Alam et al., 2023) replaces dot-product self-attention with a computationally efficient alternative. None of these models have been successfully scaled to the point where drawing comparisons with transformer-based large language models makes sense. There has also been substantial research into state space models (SSM) (Gu et al., 2021) and its variants (Dao et al., 2022b; Gupta et al., 2022; Poli et al., 2023). In contrast to the preceding models, SSM and its successors have shown substantial progress towards efficient scaling. Simultaneously with this work, Poli et al. (2023) train SSM-based models with 125 million and 355 million parameters and show that the performance is on-par with a transformer that uses a mix of local and global attention (Black et al., 2021). Advances in RNNs Inspired by the success of transformers, RNN-style (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) recursive components have also been modified to increase context length, such as the Recurrent Memory Transformer (Bulatov et al., 2022, 2023) and Linear Recurrent Units (Orvieto et al., 2023). Most similar to our work, the Quasi-Recurrent neural network (QRNN) (Bradbury et al., 2017) uses both convolutional layers and recurrent pooling functions across timesteps and channels. While QRNN utilizes convolutional filters with fixed sizes, RWKV employs a time-mixing module as an attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN, RWKV includes a parametrized channel-mixing module that is parallelizable. ## D Time-Mixing Block as an RNN Cell\n\nAs stated in 3.3, the RWKV time-mixing block can be formulated as an RNN, as the $W K V$ computation can be written in such a recursive form:\n\n$$\n\\begin{aligned}\na_{0}, b_{0} & =0 \\\\\nw k v_{t} & =\\frac{a_{t-1}+e^{u+k_{t}} \\odot v_{t}}{b_{t-1}+e^{u+k_{t}}} \\\\\na_{t} & =e^{-w} \\odot a_{t-1}+e^{k_{t}} \\odot v_{t} \\\\\nb_{t} & =e^{-w} \\odot b_{t-1}+e^{k_{t}}\n\\end{aligned}\n$$\n\nThe dataflow of the RNN-like time-mixing is shown in Fig.\n```\n\n##### *Relevant Chunk: No. 4/48 (Score: 0.84)*\n\n```\nof British Columbia ${ }^{28}$ U. of C., Santa Cruz ${ }^{29}$ U. of Electronic Science and Technology of China\n\n\n#### Abstract\n\nTransformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks. ${ }^{1}$\n\n## 1 Introduction\n\nDeep learning has greatly advanced artificial intelligence, impacting a range of scientific and industrial uses. These often involve complex sequential data\n\n[^0]processing tasks such as natural language understanding, conversational AI, time-series analysis, and indirectly sequential formats like images and graphs (Brown et al., 2020; Ismail Fawaz et al., 2019; Wu et al., 2020; Albalak et al., 2022). Predominant among these techniques include RNNs and Transformers (Vaswani et al., 2017), each with specific benefits and drawbacks. RNNs require less memory, particularly for handling long sequences. However, they suffer from the vanishing gradient problem and non-parallelizability in the time dimension during training, limiting their scalability (Hochreiter, 1998; Le and Zuidema, 2016). ![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-01.jpg?height=584&width=746&top_left_y=1647&top_left_x=1063)\n\nFigure 1: Average performance of RWKV models compared to transformers across twelve NLP tasks. For further details, see section 5 . Transformers emerged as a powerful alternative, adept at managing local and long-range dependencies and supporting parallelized training (Tay et al., 2022). Models such as GPT-3 (Brown et al., 2020), ChatGPT (OpenAI, 2022; Koco\u0144 et al., 2023),\n\n| Model | Time | Space |\n| :--- | :---: | :---: |\n| Transformer | $O\\left(T^{2} d\\right)$ | $O\\left(T^{2}+T d\\right)$ |\n| Reformer | $O(T \\log T d)$ | $O(T \\log T+T d)$ |\n| Performer | $O\\left(T d^{2} \\log d\\right)$ | $O\\left(T d \\log d+d^{2} \\log d\\right)$ |\n| Linear Transformers | $O\\left(T d^{2}\\right)$ | $O\\left(T d+d^{2}\\right)$ |\n| AFT-full | $O\\left(T^{2} d\\right)$ | $O(T d)$ |\n| AFT-local | $O(T s d)$ | $O(T d)$ |\n| MEGA | $O(c T d)$ | $O(c d)$ |\n| RWKV (ours) | $O($ Td $)$ | $O(\\mathbf{d})$ |\n\nTable 1: Inference complexity comparison with different Transformers. Here $T$ denotes the sequence length, $d$ the feature dimension, $c$ is MEGA's chunk size of quadratic attention, and $s$ is the size of a local window for AFT. LLaMA (Touvron et al., 2023), and Chinchilla (Hoffmann et al., 2022) showcase the potential of Transformers in NLP. However, the self-attention mechanism's quadratic complexity makes it computationally and memory intensive for tasks involving long sequences and constrained resources. This has stimulated research to enhance Transformers' scalability, sometimes sacrificing some of their effectiveness (Wang et al., 2020; Zaheer et al., 2020; Dao et al., 2022a). To tackle these challenges, we introduce the Receptance Weighted Key Value (RWKV) model, combining the strengths of RNNs and Transformers while circumventing key drawbacks. RWKV alleviates memory bottleneck and quadratic scaling associated with Transformers (Katharopoulos et al., 2020) with efficient linear scaling, while maintaining the expressive properties of the Transformer, such as parallelized training and robust scalability. RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity; see Table 1. The motivation behind RWKV is to balance computational efficiency with expressive capacity in neural networks. It offers a solution for handling large-scale models with billions of parameters, exhibiting competitive performance at a reduced computational cost. Experiments suggest RWKV addresses scaling and deployment challenges in AI, especially for sequential data processing, pointing towards more sustainable and efficient AI models. Our contributions in this paper are as follows:\n\n- The introduction of RWKV, a novel architec- ture combining RNNs and Transformer advantages while mitigating their limitations. - Detailed experiments demonstrating RWKV's performance and efficiency on benchmark datasets for large-scale models. - The release of pretrained models, from 169 million to 14 billion parameters, trained on the Pile (Gao et al., 2020; Biderman et al., 2022). ${ }^{2}$\n\n\n## 2 Background\n\nHere we briefly review the fundamentals of RNNs and Transformers. ### 2.1 Recurrent Neural Networks (RNNs)\n\nPopular RNN architectures such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) are characterized by the following formulation (shown for LSTM, others can be reasoned similarly):\n\n$$\n\\begin{aligned}\nf_{t} & =\\sigma_{g}\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right) \\\\\ni_{t} & =\\sigma_{g}\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right) \\\\\no_{t} & =\\sigma_{g}\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right) \\\\\n\\tilde{c}_{t} & =\\sigma_{c}\\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right) \\\\\nc_{t} & =f_{t} \\odot c_{t-1}+i_{t} \\odot \\tilde{c}_{t} \\\\\nh_{t} & =o_{t} \\odot \\sigma_{h}\\left(c_{t}\\right)\n\\end{aligned}\n$$\n\nAlthough RNNs can be factored into two linear blocks ( $W$ and $U$ ) and an RNN-specific block (1)(6), as noted by Bradbury et al. (2017), the data dependency relying on previous time steps prohibits parallelizing these typical RNNs. ### 2.2 Transformers and AFT\n\nIntroduced by Vaswani et al. (2017), Transformers are a class of neural networks that have become the dominant architecture for several NLP tasks. Instead of operating on sequences step-by-step like RNNs, Transformers rely on attention mechanisms to capture relationships between all input and all output tokens:\n\n$$\n\\operatorname{Attn}(Q, K, V)=\\operatorname{softmax}\\left(Q K^{\\top}\\right) V\n$$\n\nwhere the multi-headness and scaling factor $\\frac{1}{\\sqrt{d_{k}}}$ is omitted for convenience. The core $Q K^{\\top}$ multiplication is an ensemble of pairwise attention scores\n\n[^1]between each token in a sequence, which can be decomposed as vector operations:\n$$\n\\operatorname{Attn}(Q, K, V)_{t}=\\frac{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}} \\odot v_{i}}{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}}}\n$$\n\nAFT (Zhai et al., 2021), alternately formulates\n\n$$\n\\operatorname{Attn}^{+}(W, K, V)_{t}=\\frac{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}} \\odot v_{i}}{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}}}\n$$\n\nwhere $\\left\\{w_{t, i}\\right\\} \\in R^{T \\times T}$ is the learned pair-wise position biases, and each $w_{t, i}$ is a scalar. Inspired by AFT, RWKV takes a similar approach. However, for simplicity, it modifies the interaction weights so that it can be transformed into an RNN. Each $w_{t, i}$ in RWKV is a channelwise time decay vector multiplied by the relative position and traced backward from current time as it decays:\n\n$$\nw_{t, i}=-(t-i) w\n$$\n\nwhere $w \\in\\left(R_{\\geq 0}\\right)^{d}$, with $d$ the number of channels. We require $w$ to be non-negative to ensure that $e^{w_{t, i}} \\leq 1$ and the per-channel weights decay backwards in time. ## 3 RWKV\n\nThe RWKV model architecture is defined by four fundamental elements that are intrinsic to the timemixing and channel-mixing blocks:\n\n- $R$ : The Receptance vector acts as the receiver of past information. - $W$ : The Weight signifies the positional weight decay vector, a trainable parameter within the model. - $K$ : The Key vector performs a role analogous to $K$ in traditional attention mechanisms. - $V$ : The Value vector functions similarly to $V$ in conventional attention processes. These core elements interact multiplicatively at each timestep, as depicted in Figure 2. ### 3.1 Architecture\n\nThe RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information. This model uses a unique attention-like score update process, which includes a time-dependent\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_adcbc90dd79b80c126adg-03.jpg?height=829&width=718&top_left_y=248&top_left_x=1086)\n\nFigure 2: Elements within an RWKV block (left) and the complete RWKV residual block, equipped with a final head for language modeling (right).\n```\n\n#### 2. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.84)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 21/29 (Score: 0.84)*\n\n```\nSo, and Quoc V. Le. Pay attention to mlps, 2021. Huanru Henry Mao. Fine-tuning pre-trained transformers into decaying fast weights. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 10236-10242, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / 2022$.emnlp-main.697. Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. 5th International Conference on Learning Representations, ICLR, Toulon, France, 2017. Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 26670-26698. PMLR, 2023. URL https://proceedings.mlr.press/v202/ orvieto23a.html. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran G. V., Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: reinventing rnns for the transformer era. CoRR, abs/2305.13048, 2023. doi: 10.48550/ARXIV.2305.13048. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys l aw Kazienko, G Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and Ruijie Zhu. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.\n```\n\n#### 3. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.82)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 48/97 (Score: 0.82)*\n\n```\nArXiv, 2306.01116, 2023. B. Peng, E. Alcaide, Q. Anthony, et al. RWKV: Reinventing RNNs for the transformer era.\n```\n\n#### 4. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.79)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 26/64 (Score: 0.79)*\n\n```\narXiv preprint arXiv:1606.06031, 2016. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\u0142aw Wo\u017aniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 14048-14077, Singapore, December 2023. Association for Computational Linguistics. doi: $10.18653 / v 1 / 2023$. findings-emnlp.936. URL https://aclanthology.org/2023. findings-emnlp. 936. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: kernel methods RWKV implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Kernel methods through the roof: handling billions of points efficiently\n\n*From Search Query: kernel methods RWKV implementation*\n\n*Giacomo Meanti, Luigi Carratino, L. Rosasco, Alessandro Rudi*\n\n**TL;DR:** This work designed a preconditioned gradient solver for kernel methods exploiting both GPU acceleration and parallelization with multiple GPUs, implementing out-of-core variants of common linear algebra operations to guarantee optimal hardware utilization.\n\n**Abstract:** Kernel methods provide an elegant and principled approach to nonparametric learning, but so far could hardly be used in large scale problems, since naive implementations scale poorly with data size. Recent advances have shown the benefits of a number of algorithmic ideas, for example combining optimization, numerical linear algebra and random projections. Here, we push these efforts further to develop and test a solver that takes full advantage of GPU hardware. Towards this end, we designed a preconditioned gradient solver for kernel methods exploiting both GPU acceleration and parallelization with multiple GPUs, implementing out-of-core variants of common linear algebra operations to guarantee optimal hardware utilization. Further, we optimize the numerical precision of different operations and maximize efficiency of matrix-vector multiplications. As a result we can experimentally show dramatic speedups on datasets with billions of points, while still guaranteeing state of the art performance. Additionally, we make our software available as an easy to use library.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 100  (*Influential: 5*)\n\n#### 2. Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation\n\n*From Search Query: kernel methods RWKV implementation*\n\n*Yingyi Chen, Qinghua Tao, F. Tonin, J. Suykens*\n\n**TL;DR:** This work provides a primal-dual representation for the asymmetric kernel in self-attention and successfully applies it to modeling and optimization, and demonstrates that the deployed KSVD optimization regularizes Primal-Attention with a sharper singular value decay than that of the canonical self-ATTention.\n\n**Abstract:** Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stationary solution to the KSVD optimization in Primal-Attention yields a zero-value objective. In this manner, KSVD optimization can be implemented by simply minimizing a regularization loss, so that low-rank property is promoted without extra decomposition. Numerical experiments show state-of-the-art performance of our Primal-Attention with improved efficiency. Moreover, we demonstrate that the deployed KSVD optimization regularizes Primal-Attention with a sharper singular value decay than that of the canonical self-attention, further verifying the great potential of our method. To the best of our knowledge, this is the first work that provides a primal-dual representation for the asymmetric kernel in self-attention and successfully applies it to modeling and optimization.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 3. Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters\n\n*From Search Query: kernel methods RWKV implementation*\n\n*Brian Cho, Yaroslav Mukhin, Kyra Gan, Ivana Malenica*\n\n**TL;DR:** This work uses the targeted maximum likelihood estimation (TMLE) framework to propose a novel method named kernel debiased plug-in estimation (KDPE), which simultaneously debiases all pathwise differentiable target parameters that satisfy the regularity conditions.\n\n**Abstract:** When estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce \"plug-in bias.\" Traditional methods addressing this suboptimal bias-variance trade-off rely on the influence function (IF) of the target parameter. When estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding IFs, which poses analytical and computational challenges. In this work, we leverage the targeted maximum likelihood estimation (TMLE) framework to propose a novel method named kernel debiased plug-in estimation (KDPE). KDPE refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on reproducing kernel Hilbert spaces. We show that KDPE: (i) simultaneously debiases all pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the IF for implementation, and (iii) remains computationally tractable. We numerically illustrate the use of KDPE and validate our theoretical results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 4. Fast Approximation of Similarity Graphs with Kernel Density Estimation\n\n*From Search Query: kernel methods RWKV implementation*\n\n*Peter Macgregor, He Sun*\n\n**TL;DR:** This work presents a new algorithmic framework that constructs a sparse approximation of the fully connected similarity graph while preserving its cluster structure, based on the kernel density estimation problem, and is applicable for arbitrary kernel functions.\n\n**Abstract:** Constructing a similarity graph from a set $X$ of data points in $\\mathbb{R}^d$ is the first step of many modern clustering algorithms. However, typical constructions of a similarity graph have high time complexity, and a quadratic space dependency with respect to $|X|$. We address this limitation and present a new algorithmic framework that constructs a sparse approximation of the fully connected similarity graph while preserving its cluster structure. Our presented algorithm is based on the kernel density estimation problem, and is applicable for arbitrary kernel functions. We compare our designed algorithm with the well-known implementations from the scikit-learn library and the FAISS library, and find that our method significantly outperforms the implementation from both libraries on a variety of datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. Out-of-Distribution Detection via Conditional Kernel Independence Model\n\n*From Search Query: kernel methods RWKV implementation*\n\n*Yu Wang, Jingjing Zou, Jingyang Lin, Qing-Hua Ling, Yingwei Pan, Ting Yao, Tao Mei*\n\n**TL;DR:** This paper probes an alternative hypothesis on OOD detection by constructing a novel latent variable model based on independent component analysis (ICA) techniques, and builds upon the probabilistic formulation, and applies the Hilbert-Schmidt Independence Criteria that offers a convenient solution for optimizing variable dependencies.\n\n**Abstract:** Recently, various methods have been introduced to address the OOD detection problem with training outlier exposure. These methods usually count on discriminative softmax metric or energy method to screen OOD samples. In this paper, we probe an alternative hypothesis on OOD detection by constructing a novel latent variable model based on independent component analysis (ICA) techniques. This novel method named Conditional-i builds upon the probabilistic formulation, and applies the Hilbert-Schmidt Independence Criteria that offers a convenient solution for optimizing variable dependencies. Conditional-i exclusively encodes the useful class condition into the probabilistic model, which provides the desired convenience in delivering theoretical support for the OOD detection task. To facilitate the implementation of the Conditional-i model, we construct unique memory bank architectures that allow for convenient end-to-end training within a tractable budget. Empirical results demonstrate an evident performance boost on benchmarks against SOTA methods. We also provide valuable theoretical justifications that our training strategy is guaranteed to bound the error in the context of OOD detection. Code is available at: https://github.com/OODHSIC/conditional-i.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 0*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on the implementation of kernel methods in RWKV-like architectures and state representation and transformation techniques, here are some key findings and references:\n\n## Kernel Methods in RWKV Architectures\n\n1. **SeerAttention and Block-Sparse FlashAttention**:\n   The paper on SeerAttention introduces a method that could be adapted for RWKV-like architectures. It discusses the use of block-sparse FlashAttention kernels, which can be optimized using Triton or CUDA. This approach reduces both I/O and computation overhead by skipping non-activated blocks, which is crucial for efficient state representation and transformation.\n\n## State Representation and Transformation Techniques\n\n1. **Hierarchical State Space Models**:\n   The concept of hierarchical state space models, as discussed in the analysis, can be integrated with RWKV architectures. For example, HiSS models stack structured state-space models to create a temporal hierarchy, which can be beneficial for long-range dependency modeling. This hierarchical approach can enhance the state representation power of RWKV models[Analysis Note].\n\n2. **Selective State Mechanisms**:\n   Selective state mechanisms, such as those in Mamba, can be implemented in RWKV models to improve efficiency. These mechanisms allow input-dependent parameters and selective state updates, which can be done without traditional attention mechanisms. This aligns with the need for efficient processing and hardware-aware optimization[Analysis Note].\n\n## Hardware-Efficient Implementations\n\n1. **CUDA Kernels for RWKV Operations**:\n   The `llama.cpp` repository provides insights into how CUDA kernels can be used for efficient parallel processing in RWKV-like models. The implementation includes CUDA kernels for RWKV-WKV operations, reduction, and argmax operations, which are essential for optimizing state transitions and representations on GPU hardware.\n\n## Integration and Optimization\n\n1. **Quantization and Low-Rank Approximations**:\n   The OpenMOSE/RWKV-LM-RLHF repository discusses various quantization techniques (e.g., FP8, Int8, NF4) and low-rank approximations (e.g., LoRA) that can be applied to RWKV models. These methods can help in optimizing the state representation and transformation processes, making them more hardware-efficient.\n\n## Future Directions\n\n- **Integrating Hierarchical and Selective Mechanisms**:\n  Future work should focus on integrating hierarchical processing with selective state mechanisms within RWKV architectures. This could involve stacking state-space models and implementing input-dependent parameters to enhance both efficiency and representation power.\n\n- **Hardware-Aware Optimization**:\n  Optimizing the implementation using hardware-specific backends (like CUDA, Metal, Vulkan) and efficient memory management strategies will be crucial for practical deployment.\n\nBy combining these approaches, researchers can develop more efficient and powerful autoregressive language models that leverage kernel methods, hierarchical and selective state space models, and hardware-efficient implementations.\n",
            "analysis": "Analysis Note: Hierarchical and Selective State Space Models\n\n1. Summary of Analysis:\n\nAfter three rounds of searches, several key themes and potential innovation directions have emerged:\n\na) Core Technical Advances:\n- Hierarchical state space architectures\n- Selective state mechanisms\n- Kernel-based methods\n- Hardware-efficient implementations\n- Adaptive state transitions\n\nb) Design Opportunities:\n- Integration of hierarchical processing with RWKV6\n- Enhancement of state representation through kernel methods\n- Implementation of selective mechanisms for efficient processing\n- Hardware-aware optimization of state transitions\n\n2. Useful References with Excerpts:\n\nFrom \"Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\":\n> \"HiSS stacks structured state-space models on top of each other to create a temporal hierarchy\"\n> \"outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE\"\n\nFrom \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\":\n> \"letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n> \"selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks\"\n\nFrom \"On the Parameterization and Initialization of Diagonal State Space Models\":\n> \"While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance\"\n> \"diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension\"\n\nFrom \"Robustifying State-space Models\":\n> \"perturb-then-diagonalize (PTD) methodology, which is based on the pseudospectral theory of non-normal operators\"\n> \"shows resilience to Fourier-mode noise-perturbed inputs\"\n\n3. Key Insights and Analysis:\n\na) Hierarchical Processing:\n- Stacking SSMs creates effective temporal hierarchies\n- Different layers can handle different temporal scales\n- Hierarchical structures improve long-range dependency modeling\n\nb) Selective Mechanisms:\n- Input-dependent parameters enhance discrete modality handling\n- Selective state updates improve efficiency\n- Can be implemented without attention mechanisms\n\nc) Implementation Considerations:\n- Initialization is crucial for performance\n- Diagonal forms can maintain performance while improving efficiency\n- Hardware-aware implementation is essential for practical deployment\n\nd) State Representation:\n- Kernel methods can enhance representation power\n- PTD methodology improves robustness\n- Diagonal restrictions can preserve important properties\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific implementations of kernel methods in RWKV architectures\n2. Hardware optimization techniques for state space models\n3. Integration methods for hierarchical and selective mechanisms\n\n5. References:\n\n[1] Bhirangi, R. M., et al. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. International Conference on Machine Learning.\n\n[2] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[3] Gu, A., et al. (2022). On the Parameterization and Initialization of Diagonal State Space Models. Neural Information Processing Systems.\n\n[4] Yu, A., et al. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization. International Conference on Learning Representations.\n\n[5] Lu, P., et al. (2023). Efficient Classification of Long Documents via State-Space Models. Conference on Empirical Methods in Natural Language Processing.\n\n[6] Mattes, P., et al. (2023). Hieros: Hierarchical Imagination on Structured State Space Sequence World Models. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "kernel methods neural networks memory efficient",
            "detail": "Search for papers discussing:\n1. Memory-efficient kernel methods in neural networks\n2. Hardware-optimized kernel implementations\n3. Efficient kernel approximations for state transformations\n4. Trade-offs between kernel expressivity and computational efficiency",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory-efficient kernel methods in neural networks\n2. Hardware-optimized kernel implementations\n3. Efficient kernel approximations for state transformations\n4. Trade-offs between kernel expressivity and computational efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.24)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 21/35 (Score: 0.24)*\n\n```\nIn International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=Byt3oJ-0W. [32] Munkhoeva, M., Kapushev, Y., Burnaev, E., and Oseledets, I. Quadrature-based features for kernel approximation. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 9165-9174. Curran Associates, Inc., 2018. [33] Neyshabur, B. and Panigrahy, R. Sparse matrix factorization. arXiv preprint arXiv:1311.3315, 2013. [34] Olshevsky, V. and Shokrollahi, M. A. Matrix-vector product for confluent cauchy-like matrices with application to confluent rational interpolation. In Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, May 21-23, 2000, Portland, OR, USA, pp. 573-581, 2000. doi: 10.1145/335305.335380. URL http://doi.acm.org/10.1145/335305.335380. [35] Pan, V. Y. Structured Matrices and Polynomials: Unified Superfast Algorithms. Springer-Verlag New York, Inc., New York, NY, USA, 2001. ISBN 0-8176-4240-4. [36] Parker, D. S. Random butterfly transformations with applications in computational linear algebra. 1995. [37] Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks.\n```\n\n#### 2. Random Feature Attention (Avg. Score: 0.22)\n\n*Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 292  (*Influential: 26*)\n\n**TL;DR:** RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, is proposed and explored, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.\n\n**Abstract:** Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.\n\n##### *Relevant Chunk: No. 13/40 (Score: 0.22)*\n\n```\nIn Proc. of EMNLP, 2014. Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Proc. of NeurIPS, 2009. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers. In Proc. of ICLR, 2020. Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs).\n```\n\n#### 3. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation (Avg. Score: 0.15)\n\n*Ta-Chung Chi, Ting-Han Fan, P. Ramadge, Alexander I. Rudnicky*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 39  (*Influential: 7*)\n\n**TL;DR:** KERPLE is proposed, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences using conditionally positive definite (CPD) kernels, and it is shown that a CPD kernel can be transformed into a PD kernel by adding a constant offset.\n\n**Abstract:** Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\\url{https://github.com/chijames/KERPLE.git}.\n\n##### *Relevant Chunk: No. 21/37 (Score: 0.15)*\n\n```\nIn Biocomputing 2002, pages 564-575. World Scientific, 2001. Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral clustering and normalized cuts. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 551-556, 2004. Hiroyuki Takeda, Sina Farsiu, and Peyman Milanfar. Kernel regression for image processing and reconstruction. IEEE Transactions on image processing, 16(2):349-366, 2007. Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 4344-4353, Hong Kong, China, November 2019. Association for Computational Linguistics. Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. DA-transformer: Distance-aware transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2059-2068, Online, June 2021b. Association for Computational Linguistics. Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. Advances in Neural Information Processing Systems, 34, 2021. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.10)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 10/53 (Score: 0.10)*\n\n```\nIn Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics, Mexico City, Mexico, 2021. Association for Computational Linguistics. [8] Benjamin Charlier, Jean Feydy, Joan Alexis Glaun\u00e8s, Fran\u00e7ois-David Collin, and Ghislain Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1-6, 2021. URL http://jmlr.org/papers/v22/20-275.html. [9] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention.\n```\n\n#### 5. Liquid Structural State-Space Models (Avg. Score: 0.07)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 11/54 (Score: 0.07)*\n\n```\nS. G. Brush. History of the lenz-ising model. Reviews of modern physics, 39(4):883, 1967. S. Chang, Y. Zhang, W. Han, M. Yu, X. Guo, W. Tan, X. Cui, M. Witbrock, M. A. Hasegawa-Johnson, and T. S. Huang. Dilated recurrent neural networks. Advances in neural information processing systems, 30, 2017. B. Charlier, J. Feydy, J. A. Glaun\u00e8s, F.-D. Collin, and G. Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1-6, 2021. URL http:// jmlr.org/papers/v22/20-275.html. Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):1-12, 2018. D. Chen, L. Jacob, and J. Mairal. Recurrent kernel networks. In Advances in Neural Information Processing Systems, pages 13431-13442, 2019.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: kernel methods neural networks memory efficient\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Reservoir Computing meets Recurrent Kernels and Structured Transforms\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Jonathan Dong, Ruben Ohana, M. Rafayelyan, Florent Krzakala*\n\n**TL;DR:** The rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence, and shows how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate.\n\n**Abstract:** Reservoir Computing is a class of simple yet efficient Recurrent Neural Networks where internal weights are fixed at random and only a linear output layer is trained. In the large size limit, such random neural networks have a deep connection with kernel methods. Our contributions are threefold: a) We rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence. b) We test our models on chaotic time series prediction, a classic but challenging benchmark in Reservoir Computing, and show how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate. c) When the number of samples is too large, we leverage the success of structured Random Features for kernel approximation by introducing Structured Reservoir Computing. The two proposed methods, Recurrent Kernel and Structured Reservoir Computing, turn out to be much faster and more memory-efficient than conventional Reservoir Computing.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 2. Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Shunta Akiyama, Taiji Suzuki*\n\n**TL;DR:** This work investigates the excess risk of two-layer ReLU neural networks in a teacher-student regression model, in which a student network learns an unknown teacher network through its outputs, and shows that the student network provably reaches a near-global optimal solution and outperforms any kernel methods estimator, including neural tangent kernel approach, random feature model, and other kernel methods, in a sense of the minimax optimal rate.\n\n**Abstract:** While deep learning has outperformed other methods for various tasks, theoretical frameworks that explain its reason have not been fully established. To address this issue, we investigate the excess risk of two-layer ReLU neural networks in a teacher-student regression model, in which a student network learns an unknown teacher network through its outputs. Especially, we consider the student network that has the same width as the teacher network and is trained in two phases: first by noisy gradient descent and then by the vanilla gradient descent. Our result shows that the student network provably reaches a near-global optimal solution and outperforms any kernel methods estimator (more generally, linear estimators), including neural tangent kernel approach, random feature model, and other kernel methods, in a sense of the minimax optimal rate. The key concept inducing this superiority is the non-convexity of the neural network models. Even though the loss landscape is highly non-convex, the student network adaptively learns the teacher neurons.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 3. Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Ziyi Huang, H. Lam, Haofeng Zhang*\n\n**TL;DR:** This work creates statistically guaranteed schemes to principally characterize and remove the uncertainty of over-parameterized neural networks with very low computation effort and builds several approaches to construct asymptotically exact-coverage confidence intervals using as low as four trained networks without additional overheads.\n\n**Abstract:** Uncertainty quantification (UQ) is important for reliability assessment and enhancement of machine learning models. In deep learning, uncertainties arise not only from data, but also from the training procedure that often injects substantial noises and biases. These hinder the attainment of statistical guarantees and, moreover, impose computational challenges on UQ due to the need for repeated network retraining. Building upon the recent neural tangent kernel theory, we create statistically guaranteed schemes to principally \\emph{characterize}, and \\emph{remove}, the uncertainty of over-parameterized neural networks with very low computation effort. In particular, our approach, based on what we call a procedural-noise-correcting (PNC) predictor, removes the procedural uncertainty by using only \\emph{one} auxiliary network that is trained on a suitably labeled dataset, instead of many retrained networks employed in deep ensembles. Moreover, by combining our PNC predictor with suitable light-computation resampling methods, we build several approaches to construct asymptotically exact-coverage confidence intervals using as low as four trained networks without additional overheads.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Maria Refinetti, Sebastian Goldt, Florent Krzakala, Lenka Zdeborov'a*\n\n**TL;DR:** It is theoretically shown that two-layer neural networks (2LNN) with only a few hidden neurons can beat the performance of kernel learning on a simple Gaussian mixture classification task and illustrates how over-parametrising the neural network leads to faster convergence, but does not improve its final performance.\n\n**Abstract:** A recent series of theoretical works showed that the dynamics of neural networks with a certain initialisation are well-captured by kernel methods. Concurrent empirical work demonstrated that kernel methods can come close to the performance of neural networks on some image classification tasks. These results raise the question of whether neural networks only learn successfully if kernels also learn successfully, despite neural networks being more expressive. Here, we show theoretically that two-layer neural networks (2LNN) with only a few hidden neurons can beat the performance of kernel learning on a simple Gaussian mixture classification task. We study the high-dimensional limit where the number of samples is linearly proportional to the input dimension, and show that while small 2LNN achieve near-optimal performance on this task, lazy training approaches such as random features and kernel methods do not. Our analysis is based on the derivation of a closed set of equations that track the learning dynamics of the 2LNN and thus allow to extract the asymptotic performance of the network as a function of signal-to-noise ratio and other hyperparameters. We finally illustrate how over-parametrising the neural network leads to faster convergence, but does not improve its final performance.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 66  (*Influential: 2*)\n\n#### 5. LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Yi-Lin Sung, Jaemin Cho, Mohit Bansal*\n\n**TL;DR:** LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections, and achieves higher accuracy than Adapter and LoRA in a low-memory regime.\n\n**Abstract:** Fine-tuning large pre-trained models on downstream tasks has been adopted in a variety of domains recently. However, it is costly to update the entire parameter set of large pre-trained models. Although recently proposed parameter-efficient transfer learning (PETL) techniques allow updating a small subset of parameters (e.g. only using 2% of parameters) inside a pre-trained backbone network for a new task, they only reduce the training memory requirement by up to 30%. This is because the gradient computation for the trainable parameters still requires backpropagation through the large pre-trained backbone model. To address this, we propose Ladder Side-Tuning (LST), a new PETL technique that can reduce training memory requirements by more substantial amounts. Unlike existing parameter-efficient methods that insert additional parameters inside backbone networks, we train a ladder side network, a small and separate network that takes intermediate activations as input via shortcut connections (called ladders) from backbone networks and makes predictions. LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections. We evaluate our method with various models (T5 and CLIP-T5) on both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the whole network, while other methods only save 26% of that in similar parameter usages (hence, 2.7x more memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA in a low-memory regime. To further show the advantage of this better memory efficiency, we also apply LST to larger T5 models, attaining better GLUE performance than full fine-tuning and other PETL methods. The accuracy-efficiency trade-off also holds on VL tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 176  (*Influential: 27*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Auto-Keras: An Efficient Neural Architecture Search System\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Qingquan Song, Haifeng Jin, Xia Hu*\n\n**Abstract:** Neural architecture search (NAS) has been proposed to automatically tune deep\nneural networks, but existing search algorithms, e.g., NASNet, PNAS, usually\nsuffer from expensive computational cost. Network morphism, which keeps the\nfunctionality of a neural network while changing its neural architecture, could\nbe helpful for NAS by enabling more efficient training during the search. In\nthis paper, we propose a novel framework enabling Bayesian optimization to\nguide the network morphism for efficient neural architecture search. The\nframework develops a neural network kernel and a tree-structured acquisition\nfunction optimization algorithm to efficiently explores the search space.\nIntensive experiments on real-world benchmark datasets have been done to\ndemonstrate the superior performance of the developed framework over the\nstate-of-the-art methods. Moreover, we build an open-source AutoML system based\non our method, namely Auto-Keras. The system runs in parallel on CPU and GPU,\nwith an adaptive search strategy for different GPU memory limits.\n\n**Published:** 2018-06-27\n\n\n\n#### 2. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Ran El-Yaniv, Matthieu Courbariaux, Yoshua Bengio, Itay Hubara, Daniel Soudry*\n\n**Abstract:** We introduce a method to train Binarized Neural Networks (BNNs) - neural\nnetworks with binary weights and activations at run-time. At training-time the\nbinary weights and activations are used for computing the parameters gradients.\nDuring the forward pass, BNNs drastically reduce memory size and accesses, and\nreplace most arithmetic operations with bit-wise operations, which is expected\nto substantially improve power-efficiency. To validate the effectiveness of\nBNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On\nboth, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10\nand SVHN datasets. Last but not least, we wrote a binary matrix multiplication\nGPU kernel with which it is possible to run our MNIST BNN 7 times faster than\nwith an unoptimized GPU kernel, without suffering any loss in classification\naccuracy. The code for training and running our BNNs is available on-line.\n\n**Published:** 2016-02-09\n\n\n\n#### 3. Binarized Neural Networks\n\n*From Search Query: kernel methods neural networks memory efficient*\n\n*Ran El Yaniv, Itay Hubara, Daniel Soudry*\n\n**Abstract:** We introduce a method to train Binarized Neural Networks (BNNs) - neural\nnetworks with binary weights and activations at run-time and when computing the\nparameters' gradient at train-time. We conduct two sets of experiments, each\nbased on a different framework, namely Torch7 and Theano, where we train BNNs\non MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results.\nDuring the forward pass, BNNs drastically reduce memory size and accesses, and\nreplace most arithmetic operations with bit-wise operations, which might lead\nto a great increase in power-efficiency. Last but not least, we wrote a binary\nmatrix multiplication GPU kernel with which it is possible to run our MNIST BNN\n7 times faster than with an unoptimized GPU kernel, without suffering any loss\nin classification accuracy. The code for training and running our BNNs is\navailable.\n\n**Conference:** binarized-neural-networks-1\n\n**Published:** 2016-02-08\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on memory-efficient kernel methods, hardware-optimized kernel implementations, efficient kernel approximations, and the trade-offs between kernel expressivity and computational efficiency, here are some key insights and references from the provided sources and additional context:\n\n## Memory-Efficient Kernel Methods in Neural Networks\n\n- **Mixed-Input Quantization**: The use of mixed-input quantization, as discussed in the Machete kernel implementation, can significantly reduce memory requirements. By quantizing weights to 4-bit and activations to 16-bit, a 4x reduction in memory needed for weight storage can be achieved, which is particularly beneficial for large language models (LLMs).\n\n## Hardware-Optimized Kernel Implementations\n\n- **Machete Kernel**: This kernel, optimized for NVIDIA Hopper GPUs, employs techniques such as weight pre-shuffling and interleaving data to optimize memory bandwidth and latency. This approach ensures efficient on-the-fly upconversion of weights and hides latency by overlapping it with compute and data movement, making it highly efficient for both compute-bound and memory-bound scenarios.\n\n- **PyGim for GNNs**: Although focused on Graph Neural Networks (GNNs), the cooperative acceleration model (CoA) in PyGim demonstrates how to efficiently map and execute compute-intensive and memory-intensive kernels on different hardware (PIM cores and Host CPU/GPU). This approach can be insightful for optimizing kernel executions in other neural network architectures by leveraging the strengths of different hardware components.\n\n## Efficient Kernel Approximations for State Transformations\n\n- **Kernel Decomposition Approach**: The D3L method described in the resource-efficient model for Deep Kernel Learning involves decomposing the operators and the network. This approach allows for solving smaller problems in parallel, which can be beneficial for state transformations in autoregressive models. By localizing the predictive function and reducing the error function with regularization constraints, this method can improve accuracy and scalability.\n\n## Trade-Offs Between Kernel Expressivity and Computational Efficiency\n\n- **Quantization Trade-Offs**: The use of mixed-input quantization, as in the Machete kernel, involves a trade-off between model expressivity and computational efficiency. While quantizing weights reduces memory and can speed up computations, it may also affect the model's ability to capture complex patterns. However, the Machete kernel shows competitive performance with FP16 at higher batch sizes, indicating that these trade-offs can be managed effectively.\n\n- **Blockwise Computation**: Implementing blockwise computation can reduce memory overhead, which is crucial for maintaining both memory efficiency and computational efficiency. This approach is mentioned in the context of state space models, where IO-aware algorithms and blockwise computation are highlighted as key strategies for improving efficiency.\n\n## Additional Considerations\n\n- **Hierarchical State Representations**: From the analysis summary, hierarchical state representations and selective state updates are crucial for improving information flow and efficiency in state space models. These concepts can be applied to autoregressive language models to enhance memory efficiency and computational performance.\n\n- **Memory Hierarchy Awareness**: Being aware of the memory hierarchy and optimizing algorithms accordingly is vital. Techniques like those used in PyGim, which leverage the strengths of different hardware components, can be adapted to optimize kernel methods in autoregressive models.\n\nBy integrating these insights, researchers can develop more efficient and scalable autoregressive language models that balance kernel expressivity with computational and memory efficiency.\n",
            "analysis": "### Analysis Summary\n\nThe search results provide valuable insights into state space models (SSMs) and their applications in language modeling, particularly focusing on recent innovations like Mamba and hybrid architectures. This helps evaluate the proposed KernelDDLerpLinear design.\n\n### Key References and Excerpts\n\n1. From DenseMamba:\n\"State space model (SSM) is a new type of foundational network architecture offering lower computational complexity [...] DenseSSM retains fine-grained information crucial for the final output.\"\n\n2. From Spectral State Space Models:\n\"We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [...] have provable robustness properties.\"\n\n3. From Understanding Differences in Foundation Models:\n\"Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class [...] conditions under which softmax attention can be approximated.\"\n\n### Key Insights\n\n1. State Management:\n- Hierarchical state representations improve information flow\n- Selective state updates enhance efficiency\n- Memory-compute trade-offs are crucial\n\n2. Kernel Methods:\n- CPD kernels show promise for position-aware transformations\n- Hardware-efficient implementations are essential\n- Memory hierarchy awareness impacts performance\n\n3. Implementation Considerations:\n- IO-aware algorithms improve efficiency\n- Blockwise computation reduces memory overhead\n- Trade-offs between state size and model capabilities\n\n### Areas Needing Further Investigation\n\n1. Kernel Implementations:\n- Efficient kernel approximations\n- Hardware-specific optimizations\n- Memory-efficient kernel computations\n\n2. State Space Architecture:\n- Hierarchical state management techniques\n- Memory-efficient state representations\n- Selective update mechanisms\n\n### Future Search Plan\n\nNeed additional searches focusing on:\n1. Kernel methods in neural networks\n2. Memory-efficient kernel implementations\n3. Hardware optimization techniques\n\n### References\n\n1. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection. arXiv:2403.00818.\n\n2. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n3. Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models. arXiv:2402.19427.\n\n4. Chen, W., Liang, C., Shen, Y., Lu, Y., Liu, Y., & Ren, L. (2024). Samba: Simple Hybrid State Space Models. arXiv preprint.\n\n5. Dao, T., & Gu, A. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "kernel position embedding language models",
            "detail": "Search for papers discussing:\n1. Applications of kernel methods in language model architectures\n2. Hierarchical state representations in neural networks\n3. Position-aware transformations using CPD kernels\n4. Selective or sparse update mechanisms in state space models\n5. Memory-efficient implementations of kernel computations\n6. Long-range dependency modeling through kernel methods",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Applications of kernel methods in language model architectures\n2. Hierarchical state representations in neural networks\n3. Position-aware transformations using CPD kernels\n4. Selective or sparse update mechanisms in state space models\n5. Memory-efficient implementations of kernel computations\n6. Long-range dependency modeling through kernel methods\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.90)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.90)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.47)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.47)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 3. DeciMamba: Exploring the Length Extrapolation Potential of Mamba (Avg. Score: 0.43)\n\n*Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, Raja Giryes*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** DeciMamba is introduced, a context-extension method specifically designed for Mamba that can extrapolate to context lengths that are 25x times longer than the ones seen during training, and does so without utilizing additional computational resources.\n\n**Abstract:** Long-range sequence processing poses a significant challenge for Transformers due to their quadratic complexity in input length. A promising alternative is Mamba, which demonstrates high performance and achieves Transformer-level capabilities while requiring substantially fewer computational resources. In this paper we explore the length-generalization capabilities of Mamba, which we find to be relatively limited. Through a series of visualizations and analyses we identify that the limitations arise from a restricted effective receptive field, dictated by the sequence length used during training. To address this constraint, we introduce DeciMamba, a context-extension method specifically designed for Mamba. This mechanism, built on top of a hidden filtering mechanism embedded within the S6 layer, enables the trained model to extrapolate well even without additional training. Empirical experiments over real-world long-range NLP tasks show that DeciMamba can extrapolate to context lengths that are 25x times longer than the ones seen during training, and does so without utilizing additional computational resources. We will release our code and models.\n\n##### *Relevant Chunk: No. 22/27 (Score: 0.43)*\n\n```\nYue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. 2024b. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. 2022. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. Preprint, arXiv:1609.07843. Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. 2023a. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Maciej Pi\u00f3ro, Kamil Ciebiera, Krystian Kr\u00f3l, Jan Ludziejewski, and Sebastian Jaszczur. 2024. Moemamba: Efficient selective state space models with mixture of experts. arXiv preprint arXiv:2401.04081. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. 2023. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 28043-28078. PMLR. Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. Preprint, arXiv:1806.03822.\n```\n\n#### 4. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.28)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 43/97 (Score: 0.28)*\n\n```\nArXiv, 2312.10523, 2023. H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces. ArXiv, 2206.13947, 2022. S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In International Conference on Learning Representations (ICRL), 2017. URL https://openreview. net/ forum?id=Byj72udxe. W. Merrill and A. Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531-545, 2023. doi: 10.1162/ tacl_a_00562. W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. ArXiv, 2404.08819, 2024. M. Milakov and N. Gimelshein. Online normalizer calculation for softmax. ArXiv, 1805.02867, 2018. K. Nakano. Associatron - a model of associative memory. IEEE Transactions on Systems, Man, and Cybernetics, SMC-2(3):380-388, 1972. doi: 10.1109/TSMC.1972.4309133. G. Nearing, D. Cohen, V. Dube, M. Gauch, O. Gilon, S. Harrigan, A. Hassidim, D. Klotz, F. Kratzert, A. Metzger, S. Nevo, F. Pappenberger, C. Prudhomme, G. Shalev, S. Shenzis, T. Y. Tekalign, D. Weitzner, and Y.\n```\n\n#### 5. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.26)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 17/24 (Score: 0.26)*\n\n```\narXiv preprint arXiv:2209.10655, 2022. [36] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. [37] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. [38] Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf, and Seong-Whan Lee. Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $2501-2508,2020$. [39] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [40] Badri Narayana Patro and Vijay Srinivas Agneeswaran. Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges. arXiv preprint arXiv:2404.16112, 2024. [41] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [42] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys\u0142aw Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: kernel position embedding language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning\n\n*From Search Query: kernel position embedding language models*\n\n*Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner*\n\n**TL;DR:** This work uses VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, and finds that VLM-RMs are remarkably robust as long as the VLM is large enough.\n\n**Abstract:** Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second\"baseline\"prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 36  (*Influential: 4*)\n\n#### 2. Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use\n\n*From Search Query: kernel position embedding language models*\n\n*Yuhan Chen, Ang Lv, Ting-En Lin, C. Chen, Yuchuan Wu, Fei Huang, Yongbin Li, Rui Yan*\n\n**TL;DR:** This paper proposes a novel inference method named Attention Buckets, which allows LLMs to process their input through multiple parallel processes and enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information.\n\n**Abstract:** In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art performance, comparable to that of GPT-4. On other benchmarks and some RAG tasks, which also demand a thorough understanding of contextual content, Attention Buckets also exhibited notable enhancements in performance.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 3. CLEX: Continuous Length Extrapolation for Large Language Models\n\n*From Search Query: kernel position embedding language models*\n\n*Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Li Bing*\n\n**TL;DR:** Continuous Length EXtrapolation (CLEX) is proposed, which generalises the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths.\n\n**Abstract:** Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4x or almost 8x training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 1*)\n\n#### 4. CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending\n\n*From Search Query: kernel position embedding language models*\n\n*Shiyi Zhu, Jingting Ye, Wei Jiang, Siqiao Xue, Qi Zhang, Yifan Wu, Jianguo Li*\n\n**TL;DR:** This work proposes a novel attention mechanism, CoCA (Collinear Constrained Attention), which enforces a collinear constraint between Q and K to seamlessly integrate RoPE and self-attention, and provides an optimized implementation, making it a drop-in replacement for any existing transformer-based models.\n\n**Abstract:** Self-attention and position embedding are two key modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors harming long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention unveiled by our work. To address this issue, we propose a novel attention mechanism, CoCA (Collinear Constrained Attention). Specifically, we enforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE and self-attention. While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models. Extensive experiments show that CoCA performs extraordinarily well in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can seamlessly extend the context window up to 32K (60$\\times$), without any fine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve extrapolation up to 32K within only 2K training length. Our code is publicly available at: https://github.com/codefuse-ai/Collinear-Constrained-Attention\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. Advancing Beyond Identification: Multi-bit Watermark for Large Language Models\n\n*From Search Query: kernel position embedding language models*\n\n*Kiyoon Yoo, Wonhyuk Ahn, N. Kwak*\n\n**TL;DR:** The proposed Multi-bit Watermark via Position Allocation via Position Allocation is proposed, embedding traceable multi-bit information during language model generation, and enables robust extraction of the watermark without any model access, embedding and extraction of long messages without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time.\n\n**Abstract:** We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages (\\geq 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n### 2 related papers from Papers with Code\n\n#### 1. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation\n\n*From Search Query: kernel position embedding language models*\n\n*Alexander I. Rudnicky, Peter J. Ramadge, Ting-Han Fan, Ta-Chung Chi*\n\n**Abstract:** Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\\url{https://github.com/chijames/KERPLE.git}.\n\n**Published:** 2022-05-20\n\n\n\n#### 2. Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel\n\n*From Search Query: kernel position embedding language models*\n\n*Louis-Philippe Morency, Makoto Yamada, Yao-Hung Hubert Tsai, Shaojie Bai, Ruslan Salakhutdinov*\n\n**Abstract:** Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer's attention. As an example, we propose a new variant of Transformer's attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.\n\n**Proceeding:** emnlp-2019-11\n\n**Published:** 2019-08-30\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve autoregressive language model design using kernel methods, hierarchical state representations, and other specified techniques, here are some key findings and insights from the provided sources:\n\n## Applications of Kernel Methods in Language Model Architectures\n\n- **Rotary Position Embedding (RoPE):** This is a novel approach to positional encoding that uses kernel methods to incorporate relative positional information into the attention mechanism. RoPE enhances the performance of large language models (LLMs) on long text inputs by dynamically adjusting the embedding based on the token's position. This method reduces computational overhead and improves contextual awareness, making it scalable for large models like LLaMA.\n\n## Position-Aware Transformations Using CPD Kernels\n\n- While the sources do not explicitly mention \"CPD kernels,\" the concept of using kernel methods for position-aware transformations is well-represented by RoPE. RoPE uses a rotation matrix derived from the token's position to modify the query and key vectors in the attention mechanism, effectively integrating positional information into the model.\n\n## Hierarchical State Representations in Neural Networks\n\n- The concept of hierarchical state representations is not directly discussed in the provided sources, but the idea of managing state transformations efficiently is relevant. For instance, the Unsloth library optimizes state transformations through techniques like low-rank adaptations (LoRA) and gradient checkpointing, which can be seen as a form of hierarchical state management to enhance efficiency and reduce memory overhead.\n\n## Selective or Sparse Update Mechanisms in State Space Models\n\n- The Unsloth library implements optimized kernels and utilities that include efficient update mechanisms. For example, the use of chunked logit and gradient computation and the application of gradient checkpointing can be seen as sparse update strategies that aim to reduce memory consumption and improve efficiency during training.\n\n## Memory-Efficient Implementations of Kernel Computations\n\n- The Liger Kernel implementation and the Unsloth library both focus on memory-efficient kernel computations. The Liger Kernel fuses operations like RoPE and SwiGLU into single kernels, reducing overhead and memory consumption. Similarly, the Unsloth library uses optimized Triton-accelerated kernels and caching mechanisms to minimize memory overheads and reduce execution time.\n\n## Long-Range Dependency Modeling Through Kernel Methods\n\n- RoPE is particularly effective in modeling long-range dependencies by incorporating relative positional information directly into the attention mechanism. This approach enhances the model's ability to capture relationships between tokens over long sequences, which is crucial for autoregressive language models.\n\n### Additional Insights\n\n- **Efficiency and Scalability:** The use of kernel methods like RoPE and optimized kernels in the Liger and Unsloth implementations shows promise in maintaining efficiency and scalability. These methods reduce computational overhead and memory consumption, making them suitable for large-scale language models.\n\n- **Hierarchical State Management:** While not explicitly discussed, the optimization techniques used in the Unsloth library (e.g., LoRA, gradient checkpointing) can serve as a foundation for exploring hierarchical state representations that balance memory overhead with performance benefits.\n\n- **Selective Updates:** The chunked logit and gradient computation, along with other sparse update strategies, can help in achieving efficient memory usage and maintaining model accuracy, which is crucial for the selective update mechanism proposed in the KernelDDLerpLinear approach.\n\nBy integrating these insights, the researchers can develop a more efficient and effective autoregressive language model that leverages kernel methods, hierarchical state representations, and selective update mechanisms to enhance performance, scalability, and memory efficiency.\n",
            "analysis": "Initial Analysis of KernelDDLerpLinear Proposal:\n\n1. Key Components Requiring Investigation:\n\na) Kernel-based State Transformations:\n- The use of CPD kernels for position-aware transformations\n- Integration with existing RWKV6 architecture\n- Impact on computational efficiency\n- Comparison with other kernel-based approaches in LMs\n\nb) Hierarchical State Representation:\n- Effectiveness of multi-level state management\n- Memory overhead vs. performance benefits\n- Comparison with existing hierarchical approaches\n- Impact on long-range dependency modeling\n\nc) Selective Update Mechanism:\n- Efficiency of importance-based updates\n- Threshold adaptation strategies\n- Impact on model convergence\n- Trade-offs between sparsity and accuracy\n\n2. Investigation Plan:\n\nPhase 1: Kernel Methods in LMs\n- Research existing kernel-based approaches in LMs\n- Analyze CPD kernel applications\n- Study position-aware kernel transformations\n\nPhase 2: Hierarchical State Architectures\n- Investigate hierarchical state representations\n- Compare with existing state space models\n- Analyze memory-performance trade-offs\n\nPhase 3: Selective Update Mechanisms\n- Research sparse update strategies\n- Study threshold adaptation methods\n- Analyze efficiency-accuracy trade-offs\n\nInitial Observations:\n\n1. Novelty Assessment:\n- The combination of kernel methods with hierarchical states appears novel\n- Integration with RWKV6's DDLerpLinear is unique\n- Selective update mechanism shows promise for efficiency\n\n2. Potential Strengths:\n- Enhanced expressivity through kernel transformations\n- Improved long-range dependency modeling\n- Efficient memory usage through selective updates\n- Maintained linear complexity\n\n3. Potential Concerns:\n- Computational overhead of kernel computations\n- Memory requirements of hierarchical states\n- Complexity of implementation\n- Training stability\n\n4. Research Alignment:\n- Aligns with trends in efficient attention mechanisms\n- Builds on recent work in kernel-based position embeddings\n- Addresses known limitations in state transformations\n\nThe proposal appears to be distinct from the previous sibling design (RWKV-Lightning) which focused on attention mechanisms. This proposal instead focuses on enhancing state transformations through kernel methods and hierarchical representations.\n\nCompared to the similar design (TTTLinear-AuxAct), this proposal takes a fundamentally different approach, focusing on kernel methods and hierarchical states rather than auxiliary activations and structured matrices.\n\nThe proposal shows promise in addressing key challenges in RWKV6's state transformations while maintaining efficiency. Further investigation is needed to fully assess its potential impact and implementation feasibility."
        },
        {
            "ready": false,
            "query": "memory efficient kernel neural networks",
            "detail": "Search for papers discussing:\n1. Memory-efficient implementations of kernel methods in neural networks\n2. Techniques for reducing memory overhead in kernel computations\n3. Hardware-efficient kernel implementations\n4. Memory optimization strategies for state transformations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory-efficient implementations of kernel methods in neural networks\n2. Techniques for reducing memory overhead in kernel computations\n3. Hardware-efficient kernel implementations\n4. Memory optimization strategies for state transformations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.69)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.69)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 2. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.48)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.48)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.45)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.45)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. Memformer: A Memory-Augmented Transformer for Sequence Modeling (Avg. Score: 0.22)\n\n*Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, A. Geramifard, Zhou Yu*\n\n**Published in:** AACL/IJCNLP (2020)\t**Cited by** 36  (*Influential: 2*)\n\n**TL;DR:** This work presents Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information, and proposes a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back- Propagation through time with a significantly reduced memory requirement.\n\n**Abstract:** Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.\n\n##### *Relevant Chunk: No. 11/21 (Score: 0.22)*\n\n```\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. CoRR, abs/1604.06174. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. URL https://openai.com/blog/sparse-transformers.\n```\n\n#### 5. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.21)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.21)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: memory efficient kernel neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Tianxin Wei, Zeming Guo, Yifan Chen, Jingrui He*\n\n**TL;DR:** The neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM is investigated and proposed to coin a lightweight PLM through NTK-approximating MLP fusion, which is surprisingly shown to well approximate the NTK of the original PLM.\n\n**Abstract:** Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural language understanding (NLU) and generation (NLG) tasks are provided to verify the effectiveness of the proposed method MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Reservoir Computing meets Recurrent Kernels and Structured Transforms\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Jonathan Dong, Ruben Ohana, M. Rafayelyan, Florent Krzakala*\n\n**TL;DR:** The rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence, and shows how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate.\n\n**Abstract:** Reservoir Computing is a class of simple yet efficient Recurrent Neural Networks where internal weights are fixed at random and only a linear output layer is trained. In the large size limit, such random neural networks have a deep connection with kernel methods. Our contributions are threefold: a) We rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence. b) We test our models on chaotic time series prediction, a classic but challenging benchmark in Reservoir Computing, and show how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate. c) When the number of samples is too large, we leverage the success of structured Random Features for kernel approximation by introducing Structured Reservoir Computing. The two proposed methods, Recurrent Kernel and Structured Reservoir Computing, turn out to be much faster and more memory-efficient than conventional Reservoir Computing.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 3. Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Shuai Zhang, M. Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miaoyuan Liu*\n\n**TL;DR:** This paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error and proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexityand improve convergence without compromising the test accuracy.\n\n**Abstract:** Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include \\textit{graph sparsification} that samples a subgraph to reduce the amount of data aggregation and \\textit{model sparsification} that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 4*)\n\n#### 4. Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Zhiqi Bu, J. Mao, Shiyun Xu*\n\n**TL;DR:** An efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, is proposed that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy.\n\n**Abstract:** Large convolutional neural networks (CNN) can be difficult to train in the differentially private (DP) regime, since the optimization algorithms require a computationally expensive operation, known as the per-sample gradient clipping. We propose an efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy. The improvement in efficiency is rigorously studied through the first complexity analysis for the mixed ghost clipping and existing DP training algorithms. Extensive experiments on vision classification tasks, with large ResNet, VGG, and Vision Transformers, demonstrate that DP training with mixed ghost clipping adds $1\\sim 10\\%$ memory overhead and $<2\\times$ slowdown to the standard non-private training. Specifically, when training VGG19 on CIFAR10, the mixed ghost clipping is $3\\times$ faster than state-of-the-art Opacus library with $18\\times$ larger maximum batch size. To emphasize the significance of efficient DP training on convolutional layers, we achieve 96.7\\% accuracy on CIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous best results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy engine (\\url{https://github.com/woodyx218/private_vision}) that implements DP training of CNN with a few lines of code.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 36  (*Influential: 3*)\n\n#### 5. Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Ziyi Huang, H. Lam, Haofeng Zhang*\n\n**TL;DR:** This work creates statistically guaranteed schemes to principally characterize and remove the uncertainty of over-parameterized neural networks with very low computation effort and builds several approaches to construct asymptotically exact-coverage confidence intervals using as low as four trained networks without additional overheads.\n\n**Abstract:** Uncertainty quantification (UQ) is important for reliability assessment and enhancement of machine learning models. In deep learning, uncertainties arise not only from data, but also from the training procedure that often injects substantial noises and biases. These hinder the attainment of statistical guarantees and, moreover, impose computational challenges on UQ due to the need for repeated network retraining. Building upon the recent neural tangent kernel theory, we create statistically guaranteed schemes to principally \\emph{characterize}, and \\emph{remove}, the uncertainty of over-parameterized neural networks with very low computation effort. In particular, our approach, based on what we call a procedural-noise-correcting (PNC) predictor, removes the procedural uncertainty by using only \\emph{one} auxiliary network that is trained on a suitably labeled dataset, instead of many retrained networks employed in deep ensembles. Moreover, by combining our PNC predictor with suitable light-computation resampling methods, we build several approaches to construct asymptotically exact-coverage confidence intervals using as low as four trained networks without additional overheads.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Auto-Keras: An Efficient Neural Architecture Search System\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Qingquan Song, Haifeng Jin, Xia Hu*\n\n**Abstract:** Neural architecture search (NAS) has been proposed to automatically tune deep\nneural networks, but existing search algorithms, e.g., NASNet, PNAS, usually\nsuffer from expensive computational cost. Network morphism, which keeps the\nfunctionality of a neural network while changing its neural architecture, could\nbe helpful for NAS by enabling more efficient training during the search. In\nthis paper, we propose a novel framework enabling Bayesian optimization to\nguide the network morphism for efficient neural architecture search. The\nframework develops a neural network kernel and a tree-structured acquisition\nfunction optimization algorithm to efficiently explores the search space.\nIntensive experiments on real-world benchmark datasets have been done to\ndemonstrate the superior performance of the developed framework over the\nstate-of-the-art methods. Moreover, we build an open-source AutoML system based\non our method, namely Auto-Keras. The system runs in parallel on CPU and GPU,\nwith an adaptive search strategy for different GPU memory limits.\n\n**Published:** 2018-06-27\n\n\n\n#### 2. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Ran El-Yaniv, Matthieu Courbariaux, Yoshua Bengio, Itay Hubara, Daniel Soudry*\n\n**Abstract:** We introduce a method to train Binarized Neural Networks (BNNs) - neural\nnetworks with binary weights and activations at run-time. At training-time the\nbinary weights and activations are used for computing the parameters gradients.\nDuring the forward pass, BNNs drastically reduce memory size and accesses, and\nreplace most arithmetic operations with bit-wise operations, which is expected\nto substantially improve power-efficiency. To validate the effectiveness of\nBNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On\nboth, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10\nand SVHN datasets. Last but not least, we wrote a binary matrix multiplication\nGPU kernel with which it is possible to run our MNIST BNN 7 times faster than\nwith an unoptimized GPU kernel, without suffering any loss in classification\naccuracy. The code for training and running our BNNs is available on-line.\n\n**Published:** 2016-02-09\n\n\n\n#### 3. Fast Finite Width Neural Tangent Kernel\n\n*From Search Query: memory efficient kernel neural networks*\n\n*Samuel S. Schoenholz, Jascha Sohl-Dickstein, Roman Novak*\n\n**Abstract:** The Neural Tangent Kernel (NTK), defined as $\\Theta_\\theta^f(x_1, x_2) = \\left[\\partial f(\\theta, x_1)\\big/\\partial \\theta\\right] \\left[\\partial f(\\theta, x_2)\\big/\\partial \\theta\\right]^T$ where $\\left[\\partial f(\\theta, \\cdot)\\big/\\partial \\theta\\right]$ is a neural network (NN) Jacobian, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency. Our algorithms can be applied in a black box fashion to any differentiable function, including those implementing neural networks. We open-source our implementations within the Neural Tangents package (arXiv:1912.02803) at https://github.com/google/neural-tangents.\n\n**Conference:** fast-finite-width-neural-tangent-kernel\n\n**Published:** 2022-06-17\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on memory-efficient kernel implementations, reducing memory overhead, hardware-efficient kernel implementations, and memory optimization strategies for state transformations, here are some key findings and references:\n\n## Memory-Efficient Implementations of Kernel Methods\n\n- The paper on reducing memory footprint in neural network training highlights a method to save only the output tensor instead of the input tensor during the forward pass in pointwise nonlinearity layers. This approach is particularly beneficial for transformer-based architectures like GPT, BERT, and Llama, reducing the total memory required by storing only one tensor between layers.\n\n## Techniques for Reducing Memory Overhead in Kernel Computations\n\n- Mixed-input quantization is a technique that can significantly reduce memory overhead. For example, the Machete kernel by Neural Magic uses 4-bit quantized weights and 16-bit activations, achieving a ~4x reduction in memory needed for weight storage. This approach is particularly useful for large language models (LLMs) where model weights consume the majority of GPU memory.\n\n## Hardware-Efficient Kernel Implementations\n\n- The Machete kernel is optimized for NVIDIA Hopper GPUs and leverages the Tensor Memory Accelerator (TMA) to reduce register pressure, simplify address calculations, and overlap memory transfers with computations. This results in significant performance improvements, especially in both compute and memory-bound regimes.\n\n- Custom CUDA and Triton implementations, such as those for highly fused MLP blocks or optimized GeGLU layers, can also reduce memory overhead and improve computational speed. These implementations are tailored to specific hardware architectures and can provide substantial performance improvements.\n\n## Memory Optimization Strategies for State Transformations\n\n- The proposal to modify the handling of activation tensors in pointwise nonlinearity layers can be extended to state transformations. By saving the output tensor instead of the input tensor, similar memory savings can be achieved in state update mechanisms. This involves using the inverse function of the nonlinearity during the backward pass, which can be approximated using simpler functions to maintain training performance.\n\n- Hierarchical state representations, as discussed in the context of DenseMamba, can also inform memory optimization strategies. Selective state integration and update mechanisms can help in reducing the memory footprint by only updating and storing necessary state information.\n\n## Additional Insights\n\n- Kernel methods, such as those using conditionally positive definite (CPD) kernels, can be implemented efficiently while maintaining linear complexity. This is relevant for position-aware transformations in language models and can be optimized further using the techniques mentioned above.\n\nBy integrating these strategies, researchers can develop more memory-efficient autoregressive language models that scale better with larger data and model sizes, while maintaining or improving performance.\n",
            "analysis": "### Summary of Analysis\n\nThe initial search results provide valuable insights into kernel methods, state space models, and position-aware transformations in language models. The proposal for KernelDDLerpLinear appears to combine several emerging trends in efficient language model design.\n\n### Key References and Excerpts\n\n1. From KERPLE paper:\n\"We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences using conditionally positive definite (CPD) kernels\" - This directly relates to the kernel-based approach proposed in KernelDDLerpLinear.\n\n2. From DenseMamba:\n\"State space model (SSM) is a new type of foundational network architecture offering lower computational complexity\" - Provides context for efficient state transformations.\n\n3. From Linear Transformers with Learnable Kernel Functions:\nShows how kernel methods can enhance in-context learning capabilities, relevant to the proposal's goal of improving expressivity.\n\n### Key Insights\n\n1. Kernel Methods:\n- CPD kernels offer a principled approach to position-aware transformations\n- Kernel methods can be efficiently implemented while maintaining linear complexity\n- Need to investigate more about memory-efficient kernel implementations\n\n2. State Representations:\n- Hierarchical state structures appear novel in this context\n- DenseMamba's approach to selective state integration could inform our analysis\n- More research needed on state update mechanisms\n\n3. Implementation Efficiency:\n- Memory efficiency is crucial for practical applications\n- Need to explore more about efficient kernel computations\n- Further investigation needed on selective update mechanisms\n\n### Future Search Plan\n\nNeed additional searches focusing on:\n\n1. Memory-efficient kernel implementations in neural networks\n2. Hierarchical state representations in recurrent architectures\n3. Selective update mechanisms in language models\n\n### References\n\n1. Chi, T. C., Fan, T. H., Rudnicky, A. I., & Ramadge, P. J. (2022). KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. NeurIPS 2022.\n\n2. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv:2403.00818.\n\n3. Aksenov, Y., Balagansky, N., Vaina, S. M. L. C., Shaposhnikov, B., Gorbatovski, A., & Gavrilov, D. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv:2402.04248.\n\n4. Ben-Kish, A., Zimerman, I., Abu-Hussein, S., Cohen, N., Globerson, A., Wolf, L., & Giryes, R. (2024). DeciMamba: Exploring the Length Extrapolation Potential of Mamba. arXiv:2401.08818."
        },
        {
            "ready": false,
            "query": "state space models efficient implementation",
            "detail": "Search for papers discussing:\n1. Recent advances in state space models for language modeling\n2. Memory-efficient implementations of state space architectures\n3. Hierarchical state representations in neural networks\n4. Hardware-optimized state space computations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Recent advances in state space models for language modeling\n2. Memory-efficient implementations of state space architectures\n3. Hierarchical state representations in neural networks\n4. Hardware-optimized state space computations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 1.00)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 1.00)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 1.00)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 4. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 1.00)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 1.00)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 5. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.99)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.99)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space models efficient implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. State Space Expectation Propagation: Efficient Inference Schemes for Temporal Gaussian Processes\n\n*From Search Query: state space models efficient implementation*\n\n*William J. Wilkinson, Paul E. Chang, M. R. Andersen, A. Solin*\n\n**TL;DR:** This work forms approximate Bayesian inference in non-conjugate temporal and spatio-temporal Gaussian process models as a simple parameter update rule applied during Kalman smoothing, providing extensive empirical analysis demonstrating the efficacy of various algorithms under this unifying framework.\n\n**Abstract:** We formulate approximate Bayesian inference in non-conjugate temporal and spatio-temporal Gaussian process models as a simple parameter update rule applied during Kalman smoothing. This viewpoint encompasses most inference schemes, including expectation propagation (EP), the classical (Extended, Unscented, etc.) Kalman smoothers, and variational inference. We provide a unifying perspective on these algorithms, showing how replacing the power EP moment matching step with linearisation recovers the classical smoothers. EP provides some benefits over the traditional methods via introduction of the so-called cavity distribution, and we combine these benefits with the computational efficiency of linearisation, providing extensive empirical analysis demonstrating the efficacy of various algorithms under this unifying framework. We provide a fast implementation of all methods in JAX.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 12  (*Influential: 1*)\n\n#### 2. Discriminative State Space Models\n\n*From Search Query: state space models efficient implementation*\n\n*Vitaly Kuznetsov, M. Mohri*\n\n**TL;DR:** This paper provides data-dependent generalization guarantees for learning Discriminative State-Space Models for forecasting non-stationary time series based on the recently introduced notion of discrepancy and an in-depth analysis of the complexity of such models.\n\n**Abstract:** In this paper, we introduce and analyze Discriminative State-Space Models for forecasting non-stationary time series. We provide data-dependent generalization guarantees for learning these models based on the recently introduced notion of discrepancy. We provide an in-depth analysis of the complexity of such models. Finally, we also study the generalization guarantees for several structural risk minimization approaches to this problem and provide an efficient implementation for one of them which is based on a convex objective.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: state space models efficient implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Region-Based Semantic Factorization in GANs\n\n*From Search Query: state space models efficient implementation*\n\n*Jiapeng Zhu, Yujun Shen, Yinghao Xu, Deli Zhao, Qifeng Chen*\n\n**TL;DR:** This work revisits the task of local manipulation with pre-trained GANs and formulate region-based semantic discovery as a dual optimization problem through an appropriately defined generalized Rayleigh quotient, and manages to solve such a problem without any annotations or training.\n\n**Abstract:** Despite the rapid advancement of semantic discovery in the latent space of Generative Adversarial Networks (GANs), existing approaches either are limited to finding global attributes or rely on a number of segmentation masks to identify local attributes. In this work, we present a highly efficient algorithm to factorize the latent semantics learned by GANs concerning an arbitrary image region. Concretely, we revisit the task of local manipulation with pre-trained GANs and formulate region-based semantic discovery as a dual optimization problem. Through an appropriately defined generalized Rayleigh quotient, we manage to solve such a problem without any annotations or training. Experimental results on various state-of-the-art GAN models demonstrate the effectiveness of our approach, as well as its superiority over prior arts regarding precise control, region robustness, speed of implementation, and simplicity of use.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 30  (*Influential: 2*)\n\n#### 5. ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs\n\n*From Search Query: state space models efficient implementation*\n\n*Yang Bai, Wenqian Zhao, Shuo Yin, Zixiao Wang, Bei Yu*\n\n**TL;DR:** ATFormer is presented, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space and can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks.\n\n**Abstract:** The training and inference ef\ufb01ciency of ever-larger deep neural networks highly rely on the performance of tensor operators on speci\ufb01c hardware platforms. Therefore, a compilation-based optimization \ufb02ow with automatic tensor generation and parameter tuning is necessary for ef\ufb01cient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and poor transferability among different hardware platforms. This paper presents ATFormer, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, ATFormer can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks. Furthermore, AT-Former with pre-trained parameters can quickly adapt to different workloads and hardware via transfer learning.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: state space models efficient implementation*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model\n\n*From Search Query: state space models efficient implementation*\n\n*Fahad Shahbaz Khan, Juergen Gall, Salman Khan, Syed Talal Wasim, Abdelrahman Shaker*\n\n**Abstract:** Recent advancements in state-space models (SSMs) have showcased effective performance in modeling long-range dependencies with subquadratic complexity. However, pure SSM-based models still face challenges related to stability and achieving optimal performance on computer vision tasks. Our paper addresses the challenges of scaling SSM-based models for computer vision, particularly the instability and inefficiency of large model sizes. To address this, we introduce a Modulated Group Mamba layer which divides the input channels into four groups and applies our proposed SSM-based efficient Visual Single Selective Scanning (VSSS) block independently to each group, with each VSSS block scanning in one of the four spatial directions. The Modulated Group Mamba layer also wraps the four VSSS blocks into a channel modulation operator to improve cross-channel communication. Furthermore, we introduce a distillation-based training objective to stabilize the training of large models, leading to consistent performance gains. Our comprehensive experiments demonstrate the merits of the proposed contributions, leading to superior performance over existing methods for image classification on ImageNet-1K, object detection, instance segmentation on MS-COCO, and semantic segmentation on ADE20K. Our tiny variant with 23M parameters achieves state-of-the-art performance with a classification top-1 accuracy of 83.3% on ImageNet-1K, while being 26% efficient in terms of parameters, compared to the best existing Mamba design of same model size. Our code and models are available at: https://github.com/Amshaker/GroupMamba.\n\n**Published:** 2024-07-18\n\n\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space models efficient implementation*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using state space models, here are some key findings and insights from the provided sources:\n\n## Recent Advances in State Space Models for Language Modeling\n\nState Space Models (SSMs) have recently emerged as a promising alternative to traditional Transformer architectures for language modeling. Models like Mamba and its variants have shown significant improvements:\n\n- Mamba uses selective state spaces within a simplified neural network architecture to improve content-based reasoning, particularly in language modeling. It achieves fast inference with 5 times higher throughput than Transformers and linear scaling in sequence length.\n- The introduction of Taipan, a hybrid architecture combining Mamba-2 with Selective Attention Layers (SALs), enhances the ability to capture complex, non-Markovian dependencies while maintaining computational efficiency.\n\n## Memory-Efficient Implementations of State Space Architectures\n\nSSMs offer several memory-efficient features:\n- Mamba and its variants achieve linear time efficiency and constant memory usage, which is a significant improvement over the quadratic computational complexity and linear memory growth of Transformers.\n- The use of selective state spaces allows for selective propagation or forgetting of information based on the current token, reducing unnecessary memory usage.\n- Taipan's selective attention mechanism ensures that attention is applied only to tokens that benefit from it, optimizing resource allocation and maintaining computational efficiency.\n\n## Hierarchical State Representations in Neural Networks\n\nHierarchical state representations are integrated in models like Taipan:\n- Taipan combines Mamba-2 blocks with periodically inserted Selective Attention Layers (SALs), creating a hierarchical structure that refines key token representations. This hybrid approach allows for capturing complex dependencies while leveraging the efficiency of Mamba.\n- The SALs in Taipan identify important tokens, refine their features, and augment their representations via softmax attention, which enhances the model's ability to handle long-range dependencies.\n\n## Hardware-Optimized State Space Computations\n\nHardware optimization is a key aspect of these models:\n- Mamba incorporates a hardware-aware parallel algorithm, which enables fast inference and linear scaling in sequence length. This approach is designed to optimize performance on hardware, making it more efficient than traditional Transformer models.\n- The design of SSMs like Mamba and Taipan considers the memory hierarchy and IO-awareness, similar to the principles highlighted in FlashAttention and Ring Attention. This ensures that the models are optimized for real-time processing and analysis of sequential data.\n\n### Additional Insights\n\n- **Long-Range Dependency Modeling**: SSMs excel in capturing long-range dependencies, which is crucial for understanding nuanced meanings in text processing tasks.\n- **Parallel Processing**: SSMs can be trained in parallel, reducing training time and improving efficiency compared to traditional RNNs.\n- **Resource Efficiency**: Recent innovations, such as quantization methods for SSMs, aim to make these models more deployable on resource-limited hardware, addressing a significant challenge in their widespread adoption.\n\nThese findings and insights suggest that state space models, particularly those like Mamba and Taipan, offer promising avenues for improving autoregressive language model design by combining efficiency, scalability, and the ability to handle complex dependencies.\n",
            "analysis": "### Summary of Analysis\n\nThe search results provide valuable insights into memory-efficient implementations of kernel methods and state transformations. The proposal for KernelDDLerpLinear appears to combine several promising approaches while introducing novel elements.\n\n### Key References and Excerpts\n\n1. From FlashAttention:\n\"We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory.\" - This highlights the importance of hardware-aware implementation.\n\n2. From Ring Attention:\n\"leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication\" - Provides insights for efficient state management.\n\n3. From BASED:\n\"By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve\" - Important for understanding memory-performance tradeoffs.\n\n### Key Insights\n\n1. Memory Efficiency:\n- IO-aware algorithms are crucial for performance\n- Blockwise computation can reduce memory overhead\n- Trade-offs between state size and model capabilities need careful consideration\n\n2. Kernel Implementation:\n- CPD kernels need efficient hardware implementation\n- Memory hierarchy awareness is critical\n- Selective update mechanisms show promise\n\n3. State Management:\n- Hierarchical representations need careful design\n- Memory-compute trade-offs are important\n- Hardware-specific optimizations are crucial\n\n### Areas Needing Further Investigation\n\n1. Efficient Kernel Computation:\n- Need more details on hardware-efficient kernel implementations\n- Optimization strategies for CPD kernels\n- Memory-efficient kernel approximations\n\n2. State Space Models:\n- Recent advances in state space architectures\n- Memory-efficient state representations\n- Hierarchical state management techniques\n\n### Future Search Plan\n\nNeed additional searches focusing on:\n1. State space models and efficient implementations\n2. Hardware-efficient kernel approximations\n3. Memory-efficient hierarchical representations\n\n### References\n\n1. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS 2022.\n\n2. Liu, H., Zaharia, M., & Abbeel, P. (2023). Ring Attention with Blockwise Transformers for Near-Infinite Context. arXiv:2307.xxxxx.\n\n3. Arora, S., et al. (2024). Simple linear attention language models balance the recall-throughput tradeoff. arXiv:2401.xxxxx.\n\n4. Wei, T., et al. (2023). NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. ICML 2023."
        }
    ],
    "rating": 3.8,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": false,
    "costs": {
        "DESIGN_PROPOSER": 0.91374,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.9093810000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "DDLerpLinear",
    "proposal": "## KernelDDLerpLinear: Kernel-Enhanced Dynamic State Transformations for RWKV6\n\n### 1. Title\nKernelDDLerpLinear: Kernel-Enhanced Dynamic State Transformations with Selective Updates\n\n### 2. Motivation\nCurrent RWKV6 architectures face challenges in:\n- Limited expressivity in state transformations\n- Inefficient handling of long-range dependencies\n- Suboptimal position-aware state updates\n- High memory requirements for state representation\n\n### 3. Problem Analysis\nDDLerpLinear in RWKV6 uses linear transformations with dynamic updates but lacks:\n- Sophisticated position-aware state representations\n- Efficient mechanisms for selective state updates\n- Hierarchical state processing capabilities\n- Kernel-based feature transformations\n\n### 4. Core Idea and Philosophy\nEnhance DDLerpLinear by incorporating:\n1. Kernel-based state transformations using CPD kernels\n2. Hierarchical state representations\n3. Selective update mechanisms\n4. Position-aware kernel functions\n\n### 5. Design Plan\n\n#### A. Core Components\n\n1. Kernel Transformation Module:\n```python\ndef kernel_transform(x, positions, kernel_params):\n    # CPD kernel transformation\n    diff = positions[:, None] - positions[None, :]\n    K = -kernel_params.p * torch.log(1 + kernel_params.a * torch.abs(diff))\n    return torch.einsum('ij,jd->id', K, x)\n```\n\n2. Hierarchical State Structure:\n```python\nclass HierarchicalState:\n    def __init__(self, levels, dim):\n        self.states = [torch.zeros(dim) for _ in range(levels)]\n        self.importance = nn.Parameter(torch.ones(levels))\n    \n    def update(self, new_state):\n        # Update states hierarchically\n        for i in range(len(self.states)):\n            decay = torch.sigmoid(self.importance[i])\n            self.states[i] = decay * self.states[i] + (1-decay) * new_state\n```\n\n3. Selective Update Mechanism:\n```python\ndef selective_update(x, state, threshold):\n    # Compute update importance\n    importance = torch.norm(x - state, dim=-1)\n    mask = importance > threshold\n    return mask\n```\n\n#### B. Modified DDLerpLinear Implementation:\n\n```python\nclass KernelDDLerpLinear(nn.Module):\n    def __init__(self, embed_dim, output_dim, n_levels=3):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.output_dim = output_dim\n        self.kernel_params = nn.Parameter(torch.ones(2))  # [p, a]\n        self.hierarchical_state = HierarchicalState(n_levels, embed_dim)\n        self.update_threshold = nn.Parameter(torch.zeros(1))\n        \n        # Original DDLerpLinear components\n        self.linear = nn.Linear(embed_dim, output_dim)\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        \n    def forward(self, x, mu, delta=None):\n        if delta is None:\n            shifted = self.time_shift(x)\n            delta = shifted - x\n            \n        # Apply kernel transformation\n        positions = torch.arange(x.shape[1], device=x.device)\n        x_kernel = kernel_transform(x, positions, self.kernel_params)\n        \n        # Selective update\n        update_mask = selective_update(x_kernel, \n                                    self.hierarchical_state.states[-1],\n                                    self.update_threshold)\n        \n        # Update hierarchical states\n        self.hierarchical_state.update(x_kernel * update_mask[..., None])\n        \n        # Combine with original transformation\n        x_combined = x + delta * mu\n        x_with_state = x_combined + self.hierarchical_state.get_weighted_state()\n        \n        return self.linear(x_with_state)\n```\n\n### 6. Implementation Guidelines\n\n1. Initialization:\n- Initialize kernel parameters with small positive values\n- Set hierarchical state levels based on sequence length\n- Initialize update threshold conservatively\n\n2. Training:\n- Use gradient clipping for kernel parameters\n- Apply layer normalization after state updates\n- Implement warmup for selective update threshold\n\n3. Optimization:\n- Cache kernel computations when possible\n- Use sparse operations for selective updates\n- Implement efficient CUDA kernels for state operations\n\n### 7. Research Summary\n\nKey findings from research:\n1. KERPLE demonstrates effectiveness of kernel-based position embeddings\n2. Hierarchical state spaces improve long-range modeling\n3. Selective mechanisms enhance efficiency\n4. Hardware-aware implementation crucial for performance\n\n### 8. Evolution of Design\n\nThe design evolved through several stages:\n1. Initial kernel-only enhancement\n2. Addition of hierarchical states\n3. Integration of selective mechanisms\n4. Optimization for hardware efficiency\n\n### 9. Theoretical Analysis\n\nA. Complexity Analysis:\n- Time complexity: O(T*d) maintained\n- Space complexity: O(d + L*d), where L is number of levels\n\nB. Advantages:\n1. Enhanced expressivity through kernel transformations\n2. Improved long-range dependency modeling\n3. Efficient selective updates\n4. Hardware-friendly implementation\n\n### 10. Conclusion\n\nKernelDDLerpLinear enhances RWKV6's capabilities through:\n- Kernel-based state transformations\n- Hierarchical state representation\n- Selective update mechanisms\n- Efficient implementation\n\n### 11. References\n\n[1] Chi, T. C., et al. (2022). KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation.\n\n[2] Peng, B., et al. (2024). Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\n\n[3] Meanti, G., et al. (2020). Kernel methods through the roof: handling billions of points efficiently.\n\n[4] Bhirangi, R. M., et al. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "We propose KernelDDLerpLinear, an enhancement to DDLerpLinear that incorporates kernel methods and hierarchical state representations to improve the expressivity and efficiency of RWKV6's state transformations. The design leverages conditionally positive definite kernels for position-aware state updates and introduces a selective state mechanism for efficient processing. This modification maintains the linear complexity of RWKV while enhancing its ability to capture long-range dependencies.",
    "ideation": null,
    "modelname": "RWKV6-Kernel",
    "suggestions": "1. IMPLEMENTATION OPTIMIZATION\n- Consider providing specific hardware optimization strategies for kernel computations\n- Detail memory management techniques for hierarchical states\n- Include benchmarking guidelines for performance evaluation\n\n2. THEORETICAL FOUNDATIONS\n- Expand on the theoretical analysis of CPD kernel properties\n- Provide more detailed stability analysis\n- Include convergence guarantees for the selective update mechanism\n\n3. PRACTICAL CONSIDERATIONS\n- Add guidelines for hyperparameter tuning\n- Include ablation study recommendations\n- Provide failure case analysis\n\n4. ARCHITECTURE INTEGRATION\n- Detail integration with existing RWKV6 components\n- Specify initialization strategies\n- Include gradient flow analysis\n\n5. PERFORMANCE OPTIMIZATION\n- Add caching strategies for kernel computations\n- Specify sparsity patterns for selective updates\n- Include memory optimization techniques",
    "user_input": ""
}