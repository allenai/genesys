{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HG_GAT_Block(GAUBase):\n    \"\"\"\n    Hierarchical Gated Graph Attention Block (HG_GAT_Block)\n\n    This GAU integrates hierarchical multi-scale normalization with dynamic gating \n    mechanisms and efficient graph-based attention to capture both local and global \n    dependencies in language models. It consists of two main components:\n\n    1. Hierarchical Gated RMSNorm\n    2. Graph Attention Layer\n\n    **Code Example:**\n\n                from hg_gat_block import HG_GAT_Block\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        gau = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Z = {}\n        Y, Z = gau(X, **Z)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm = HierarchicalGatedRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.graph_attention = GraphAttentionLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X_norm, Z = self.norm(X, **Z)\n        Y, Z = self.graph_attention(X_norm, **Z)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(GAUBase):\n    \"\"\"\n    Graph Attention Layer.\n\n    This unit performs multi-head self-attention, capturing global dependencies efficiently.\n    It supports causal attention by applying appropriate masking.\n\n    **Code Example:**\n\n                from graph_attention_layer import GraphAttentionLayer\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        attn_layer = GraphAttentionLayer(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = attn_layer(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        dropout (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, dropout=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.causal = True\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        Q = self.q_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        K = self.k_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        V = self.v_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device,\n                dtype=torch.bool), diagonal=1)\n            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).\n                unsqueeze(0), float('-inf'))\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        attn_output = torch.matmul(attn_probs, V)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        return Y, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HierarchicalGatedRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Gated RMSNorm.\n\n    This unit performs multi-scale RMS normalization with gating mechanisms.\n    It processes input embeddings at multiple scales and applies dynamic gating \n    to control the influence of each scale.\n\n    **Code Example:**\n\n                from hierarchical_gated_rmsnorm import HierarchicalGatedRMSNorm\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        norm = HierarchicalGatedRMSNorm(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = norm(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        scales (list of int, optional): List of scales to use. Defaults to [1, 2, 4].\n        eps (float, optional): Epsilon for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, scales=[1, 2, 4], eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in scales})\n        self.gate_weights = nn.ModuleDict({f'g{s}': nn.Linear(embed_dim,\n            embed_dim, **self.factory_kwargs) for s in scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(scales), **self.\n            factory_kwargs))\n\n    def _decompose_scales(self, X):\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_scales[s] = self._causal_downsample(X, s)\n        return x_scales\n\n    def _causal_downsample(self, X, scale):\n        batch_size, seq_len, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale), **self.factory_kwargs\n            ) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _causal_upsample(self, y_s, scale, target_length):\n        upsampled = y_s.repeat_interleave(scale, dim=1)\n        return upsampled[:, :target_length, :]\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = []\n        for idx, s in enumerate(self.scales):\n            x_s = x_scales[s]\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            gamma_s = self.gammas[f's{s}']\n            gated = torch.sigmoid(self.gate_weights[f'g{s}'](x_s))\n            y_s = gated * (x_s / rms_s * gamma_s)\n            upsampled_y_s = self._causal_upsample(y_s, s, X.size(1))\n            y_scales.append(upsampled_y_s)\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = sum(w * y_s for w, y_s in zip(weights, y_scales))\n        return Y, {}\n\n\ngab_config = {'scales': [1, 2, 4], 'eps': 1e-05, 'num_heads': 8, 'dropout': 0.1\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HG_GAT_Block(GAUBase):\n    \"\"\"\n    Hierarchical Gated Graph Attention Block (HG_GAT_Block)\n\n    This GAU integrates hierarchical multi-scale normalization with dynamic gating \n    mechanisms and efficient graph-based attention to capture both local and global \n    dependencies in language models. It consists of two main components:\n\n    1. Hierarchical Gated RMSNorm\n    2. Graph Attention Layer\n\n    **Code Example:**\n\n                from hg_gat_block import HG_GAT_Block\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        gau = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Z = {}\n        Y, Z = gau(X, **Z)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm = HierarchicalGatedRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.graph_attention = GraphAttentionLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X_norm, Z = self.norm(X, **Z)\n        Y, Z = self.graph_attention(X_norm, **Z)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(GAUBase):\n    \"\"\"\n    Graph Attention Layer.\n\n    This unit performs multi-head self-attention, capturing global dependencies efficiently.\n    It supports causal attention by applying appropriate masking.\n\n    **Code Example:**\n\n                from graph_attention_layer import GraphAttentionLayer\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        attn_layer = GraphAttentionLayer(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = attn_layer(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        dropout (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, dropout=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.causal = True\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        Q = self.q_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        K = self.k_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        V = self.v_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device,\n                dtype=torch.bool), diagonal=1)\n            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).\n                unsqueeze(0), float('-inf'))\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        attn_output = torch.matmul(attn_probs, V)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        return Y, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HierarchicalGatedRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Gated RMSNorm.\n\n    This unit performs multi-scale RMS normalization with gating mechanisms.\n    It processes input embeddings at multiple scales and applies dynamic gating \n    to control the influence of each scale.\n\n    **Code Example:**\n\n                from hierarchical_gated_rmsnorm import HierarchicalGatedRMSNorm\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        norm = HierarchicalGatedRMSNorm(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = norm(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        scales (list of int, optional): List of scales to use. Defaults to [1, 2, 4].\n        eps (float, optional): Epsilon for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, scales=[1, 2, 4], eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in scales})\n        self.gate_weights = nn.ModuleDict({f'g{s}': nn.Linear(embed_dim,\n            embed_dim, **self.factory_kwargs) for s in scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(scales), **self.\n            factory_kwargs))\n\n    def _decompose_scales(self, X):\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_scales[s] = self._causal_downsample(X, s)\n        return x_scales\n\n    def _causal_downsample(self, X, scale):\n        batch_size, seq_len, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale), **self.factory_kwargs\n            ) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _causal_upsample(self, y_s, scale, target_length):\n        upsampled = y_s.repeat_interleave(scale, dim=1)\n        return upsampled[:, :target_length, :]\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = []\n        for idx, s in enumerate(self.scales):\n            x_s = x_scales[s]\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            gamma_s = self.gammas[f's{s}']\n            gated = torch.sigmoid(self.gate_weights[f'g{s}'](x_s))\n            y_s = gated * (x_s / rms_s * gamma_s)\n            upsampled_y_s = self._causal_upsample(y_s, s, X.size(1))\n            y_scales.append(upsampled_y_s)\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = sum(w * y_s for w, y_s in zip(weights, y_scales))\n        return Y, {}\n\n\ngab_config = {'scales': [1, 2, 4], 'eps': 1e-05, 'num_heads': 8, 'dropout': 0.1\n    }\n\n\n\nautoconfig = {\n    'd_model': 1536,\n    'n_block': 33\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HG_GAT_Block(GAUBase):\n    \"\"\"\n    Hierarchical Gated Graph Attention Block (HG_GAT_Block)\n\n    This GAU integrates hierarchical multi-scale normalization with dynamic gating \n    mechanisms and efficient graph-based attention to capture both local and global \n    dependencies in language models. It consists of two main components:\n\n    1. Hierarchical Gated RMSNorm\n    2. Graph Attention Layer\n\n    **Code Example:**\n\n                from hg_gat_block import HG_GAT_Block\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        gau = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Z = {}\n        Y, Z = gau(X, **Z)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm = HierarchicalGatedRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.graph_attention = GraphAttentionLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X_norm, Z = self.norm(X, **Z)\n        Y, Z = self.graph_attention(X_norm, **Z)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(GAUBase):\n    \"\"\"\n    Graph Attention Layer.\n\n    This unit performs multi-head self-attention, capturing global dependencies efficiently.\n    It supports causal attention by applying appropriate masking.\n\n    **Code Example:**\n\n                from graph_attention_layer import GraphAttentionLayer\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        attn_layer = GraphAttentionLayer(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = attn_layer(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        dropout (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, dropout=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.causal = True\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        Q = self.q_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        K = self.k_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        V = self.v_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device,\n                dtype=torch.bool), diagonal=1)\n            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).\n                unsqueeze(0), float('-inf'))\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        attn_output = torch.matmul(attn_probs, V)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        return Y, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HierarchicalGatedRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Gated RMSNorm.\n\n    This unit performs multi-scale RMS normalization with gating mechanisms.\n    It processes input embeddings at multiple scales and applies dynamic gating \n    to control the influence of each scale.\n\n    **Code Example:**\n\n                from hierarchical_gated_rmsnorm import HierarchicalGatedRMSNorm\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        norm = HierarchicalGatedRMSNorm(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = norm(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        scales (list of int, optional): List of scales to use. Defaults to [1, 2, 4].\n        eps (float, optional): Epsilon for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, scales=[1, 2, 4], eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in scales})\n        self.gate_weights = nn.ModuleDict({f'g{s}': nn.Linear(embed_dim,\n            embed_dim, **self.factory_kwargs) for s in scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(scales), **self.\n            factory_kwargs))\n\n    def _decompose_scales(self, X):\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_scales[s] = self._causal_downsample(X, s)\n        return x_scales\n\n    def _causal_downsample(self, X, scale):\n        batch_size, seq_len, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale), **self.factory_kwargs\n            ) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _causal_upsample(self, y_s, scale, target_length):\n        upsampled = y_s.repeat_interleave(scale, dim=1)\n        return upsampled[:, :target_length, :]\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = []\n        for idx, s in enumerate(self.scales):\n            x_s = x_scales[s]\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            gamma_s = self.gammas[f's{s}']\n            gated = torch.sigmoid(self.gate_weights[f'g{s}'](x_s))\n            y_s = gated * (x_s / rms_s * gamma_s)\n            upsampled_y_s = self._causal_upsample(y_s, s, X.size(1))\n            y_scales.append(upsampled_y_s)\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = sum(w * y_s for w, y_s in zip(weights, y_scales))\n        return Y, {}\n\n\ngab_config = {'scales': [1, 2, 4], 'eps': 1e-05, 'num_heads': 8, 'dropout': 0.1\n    }\n\n\n\nautoconfig = {\n    'd_model': 512,\n    'n_block': 7\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HG_GAT_Block(GAUBase):\n    \"\"\"\n    Hierarchical Gated Graph Attention Block (HG_GAT_Block)\n\n    This GAU integrates hierarchical multi-scale normalization with dynamic gating \n    mechanisms and efficient graph-based attention to capture both local and global \n    dependencies in language models. It consists of two main components:\n\n    1. Hierarchical Gated RMSNorm\n    2. Graph Attention Layer\n\n    **Code Example:**\n\n                from hg_gat_block import HG_GAT_Block\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        gau = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Z = {}\n        Y, Z = gau(X, **Z)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm = HierarchicalGatedRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.graph_attention = GraphAttentionLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X_norm, Z = self.norm(X, **Z)\n        Y, Z = self.graph_attention(X_norm, **Z)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(GAUBase):\n    \"\"\"\n    Graph Attention Layer.\n\n    This unit performs multi-head self-attention, capturing global dependencies efficiently.\n    It supports causal attention by applying appropriate masking.\n\n    **Code Example:**\n\n                from graph_attention_layer import GraphAttentionLayer\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        attn_layer = GraphAttentionLayer(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = attn_layer(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        dropout (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, dropout=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.causal = True\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        Q = self.q_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        K = self.k_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        V = self.v_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device,\n                dtype=torch.bool), diagonal=1)\n            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).\n                unsqueeze(0), float('-inf'))\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        attn_output = torch.matmul(attn_probs, V)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        return Y, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HierarchicalGatedRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Gated RMSNorm.\n\n    This unit performs multi-scale RMS normalization with gating mechanisms.\n    It processes input embeddings at multiple scales and applies dynamic gating \n    to control the influence of each scale.\n\n    **Code Example:**\n\n                from hierarchical_gated_rmsnorm import HierarchicalGatedRMSNorm\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        norm = HierarchicalGatedRMSNorm(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = norm(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        scales (list of int, optional): List of scales to use. Defaults to [1, 2, 4].\n        eps (float, optional): Epsilon for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, scales=[1, 2, 4], eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in scales})\n        self.gate_weights = nn.ModuleDict({f'g{s}': nn.Linear(embed_dim,\n            embed_dim, **self.factory_kwargs) for s in scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(scales), **self.\n            factory_kwargs))\n\n    def _decompose_scales(self, X):\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_scales[s] = self._causal_downsample(X, s)\n        return x_scales\n\n    def _causal_downsample(self, X, scale):\n        batch_size, seq_len, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale), **self.factory_kwargs\n            ) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _causal_upsample(self, y_s, scale, target_length):\n        upsampled = y_s.repeat_interleave(scale, dim=1)\n        return upsampled[:, :target_length, :]\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = []\n        for idx, s in enumerate(self.scales):\n            x_s = x_scales[s]\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            gamma_s = self.gammas[f's{s}']\n            gated = torch.sigmoid(self.gate_weights[f'g{s}'](x_s))\n            y_s = gated * (x_s / rms_s * gamma_s)\n            upsampled_y_s = self._causal_upsample(y_s, s, X.size(1))\n            y_scales.append(upsampled_y_s)\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = sum(w * y_s for w, y_s in zip(weights, y_scales))\n        return Y, {}\n\n\ngab_config = {'scales': [1, 2, 4], 'eps': 1e-05, 'num_heads': 8, 'dropout': 0.1\n    }\n\n\n\nautoconfig = {\n    'd_model': 2048,\n    'n_block': 33\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HG_GAT_Block(GAUBase):\n    \"\"\"\n    Hierarchical Gated Graph Attention Block (HG_GAT_Block)\n\n    This GAU integrates hierarchical multi-scale normalization with dynamic gating \n    mechanisms and efficient graph-based attention to capture both local and global \n    dependencies in language models. It consists of two main components:\n\n    1. Hierarchical Gated RMSNorm\n    2. Graph Attention Layer\n\n    **Code Example:**\n\n                from hg_gat_block import HG_GAT_Block\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        gau = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Z = {}\n        Y, Z = gau(X, **Z)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm = HierarchicalGatedRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.graph_attention = GraphAttentionLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X_norm, Z = self.norm(X, **Z)\n        Y, Z = self.graph_attention(X_norm, **Z)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(GAUBase):\n    \"\"\"\n    Graph Attention Layer.\n\n    This unit performs multi-head self-attention, capturing global dependencies efficiently.\n    It supports causal attention by applying appropriate masking.\n\n    **Code Example:**\n\n                from graph_attention_layer import GraphAttentionLayer\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        attn_layer = GraphAttentionLayer(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = attn_layer(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        dropout (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, dropout=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.causal = True\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        Q = self.q_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        K = self.k_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        V = self.v_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device,\n                dtype=torch.bool), diagonal=1)\n            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).\n                unsqueeze(0), float('-inf'))\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        attn_output = torch.matmul(attn_probs, V)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        return Y, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HierarchicalGatedRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Gated RMSNorm.\n\n    This unit performs multi-scale RMS normalization with gating mechanisms.\n    It processes input embeddings at multiple scales and applies dynamic gating \n    to control the influence of each scale.\n\n    **Code Example:**\n\n                from hierarchical_gated_rmsnorm import HierarchicalGatedRMSNorm\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        norm = HierarchicalGatedRMSNorm(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = norm(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        scales (list of int, optional): List of scales to use. Defaults to [1, 2, 4].\n        eps (float, optional): Epsilon for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, scales=[1, 2, 4], eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in scales})\n        self.gate_weights = nn.ModuleDict({f'g{s}': nn.Linear(embed_dim,\n            embed_dim, **self.factory_kwargs) for s in scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(scales), **self.\n            factory_kwargs))\n\n    def _decompose_scales(self, X):\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_scales[s] = self._causal_downsample(X, s)\n        return x_scales\n\n    def _causal_downsample(self, X, scale):\n        batch_size, seq_len, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale), **self.factory_kwargs\n            ) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _causal_upsample(self, y_s, scale, target_length):\n        upsampled = y_s.repeat_interleave(scale, dim=1)\n        return upsampled[:, :target_length, :]\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = []\n        for idx, s in enumerate(self.scales):\n            x_s = x_scales[s]\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            gamma_s = self.gammas[f's{s}']\n            gated = torch.sigmoid(self.gate_weights[f'g{s}'](x_s))\n            y_s = gated * (x_s / rms_s * gamma_s)\n            upsampled_y_s = self._causal_upsample(y_s, s, X.size(1))\n            y_scales.append(upsampled_y_s)\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = sum(w * y_s for w, y_s in zip(weights, y_scales))\n        return Y, {}\n\n\ngab_config = {'scales': [1, 2, 4], 'eps': 1e-05, 'num_heads': 8, 'dropout': 0.1\n    }\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 16\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HG_GAT_Block(GAUBase):\n    \"\"\"\n    Hierarchical Gated Graph Attention Block (HG_GAT_Block)\n\n    This GAU integrates hierarchical multi-scale normalization with dynamic gating \n    mechanisms and efficient graph-based attention to capture both local and global \n    dependencies in language models. It consists of two main components:\n\n    1. Hierarchical Gated RMSNorm\n    2. Graph Attention Layer\n\n    **Code Example:**\n\n                from hg_gat_block import HG_GAT_Block\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        gau = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Z = {}\n        Y, Z = gau(X, **Z)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm = HierarchicalGatedRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.graph_attention = GraphAttentionLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X_norm, Z = self.norm(X, **Z)\n        Y, Z = self.graph_attention(X_norm, **Z)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(GAUBase):\n    \"\"\"\n    Graph Attention Layer.\n\n    This unit performs multi-head self-attention, capturing global dependencies efficiently.\n    It supports causal attention by applying appropriate masking.\n\n    **Code Example:**\n\n                from graph_attention_layer import GraphAttentionLayer\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        attn_layer = GraphAttentionLayer(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = attn_layer(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        dropout (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, dropout=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.causal = True\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        Q = self.q_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        K = self.k_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        V = self.v_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device,\n                dtype=torch.bool), diagonal=1)\n            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).\n                unsqueeze(0), float('-inf'))\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        attn_output = torch.matmul(attn_probs, V)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        return Y, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HierarchicalGatedRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Gated RMSNorm.\n\n    This unit performs multi-scale RMS normalization with gating mechanisms.\n    It processes input embeddings at multiple scales and applies dynamic gating \n    to control the influence of each scale.\n\n    **Code Example:**\n\n                from hierarchical_gated_rmsnorm import HierarchicalGatedRMSNorm\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        norm = HierarchicalGatedRMSNorm(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = norm(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        scales (list of int, optional): List of scales to use. Defaults to [1, 2, 4].\n        eps (float, optional): Epsilon for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, scales=[1, 2, 4], eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in scales})\n        self.gate_weights = nn.ModuleDict({f'g{s}': nn.Linear(embed_dim,\n            embed_dim, **self.factory_kwargs) for s in scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(scales), **self.\n            factory_kwargs))\n\n    def _decompose_scales(self, X):\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_scales[s] = self._causal_downsample(X, s)\n        return x_scales\n\n    def _causal_downsample(self, X, scale):\n        batch_size, seq_len, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale), **self.factory_kwargs\n            ) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _causal_upsample(self, y_s, scale, target_length):\n        upsampled = y_s.repeat_interleave(scale, dim=1)\n        return upsampled[:, :target_length, :]\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = []\n        for idx, s in enumerate(self.scales):\n            x_s = x_scales[s]\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            gamma_s = self.gammas[f's{s}']\n            gated = torch.sigmoid(self.gate_weights[f'g{s}'](x_s))\n            y_s = gated * (x_s / rms_s * gamma_s)\n            upsampled_y_s = self._causal_upsample(y_s, s, X.size(1))\n            y_scales.append(upsampled_y_s)\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = sum(w * y_s for w, y_s in zip(weights, y_scales))\n        return Y, {}\n\n\ngab_config = {'scales': [1, 2, 4], 'eps': 1e-05, 'num_heads': 8, 'dropout': 0.1\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HG_GAT_Block(GAUBase):\n    \"\"\"\n    Hierarchical Gated Graph Attention Block (HG_GAT_Block)\n\n    This GAU integrates hierarchical multi-scale normalization with dynamic gating \n    mechanisms and efficient graph-based attention to capture both local and global \n    dependencies in language models. It consists of two main components:\n\n    1. Hierarchical Gated RMSNorm\n    2. Graph Attention Layer\n\n    **Code Example:**\n\n                from hg_gat_block import HG_GAT_Block\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        gau = HG_GAT_Block(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Z = {}\n        Y, Z = gau(X, **Z)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm = HierarchicalGatedRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.graph_attention = GraphAttentionLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X_norm, Z = self.norm(X, **Z)\n        Y, Z = self.graph_attention(X_norm, **Z)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(GAUBase):\n    \"\"\"\n    Graph Attention Layer.\n\n    This unit performs multi-head self-attention, capturing global dependencies efficiently.\n    It supports causal attention by applying appropriate masking.\n\n    **Code Example:**\n\n                from graph_attention_layer import GraphAttentionLayer\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        attn_layer = GraphAttentionLayer(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = attn_layer(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        dropout (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, dropout=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.causal = True\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        Q = self.q_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        K = self.k_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        V = self.v_proj(X).view(B, L, self.num_heads, self.head_dim).transpose(\n            1, 2)\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device,\n                dtype=torch.bool), diagonal=1)\n            attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).\n                unsqueeze(0), float('-inf'))\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        attn_output = torch.matmul(attn_probs, V)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        return Y, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HierarchicalGatedRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Gated RMSNorm.\n\n    This unit performs multi-scale RMS normalization with gating mechanisms.\n    It processes input embeddings at multiple scales and applies dynamic gating \n    to control the influence of each scale.\n\n    **Code Example:**\n\n                from hierarchical_gated_rmsnorm import HierarchicalGatedRMSNorm\n\n        embed_dim = 128\n        block_loc = (0, 0)\n        kwarg_all = {}\n        norm = HierarchicalGatedRMSNorm(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all)\n\n        X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n        Y, Z = norm(X)\n        print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n        scales (list of int, optional): List of scales to use. Defaults to [1, 2, 4].\n        eps (float, optional): Epsilon for numerical stability. Defaults to 1e-5.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, scales=[1, 2, 4], eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in scales})\n        self.gate_weights = nn.ModuleDict({f'g{s}': nn.Linear(embed_dim,\n            embed_dim, **self.factory_kwargs) for s in scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(scales), **self.\n            factory_kwargs))\n\n    def _decompose_scales(self, X):\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_scales[s] = self._causal_downsample(X, s)\n        return x_scales\n\n    def _causal_downsample(self, X, scale):\n        batch_size, seq_len, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale), **self.factory_kwargs\n            ) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _causal_upsample(self, y_s, scale, target_length):\n        upsampled = y_s.repeat_interleave(scale, dim=1)\n        return upsampled[:, :target_length, :]\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = []\n        for idx, s in enumerate(self.scales):\n            x_s = x_scales[s]\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            gamma_s = self.gammas[f's{s}']\n            gated = torch.sigmoid(self.gate_weights[f'g{s}'](x_s))\n            y_s = gated * (x_s / rms_s * gamma_s)\n            upsampled_y_s = self._causal_upsample(y_s, s, X.size(1))\n            y_scales.append(upsampled_y_s)\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = sum(w * y_s for w, y_s in zip(weights, y_scales))\n        return Y, {}\n\n\ngab_config = {'scales': [1, 2, 4], 'eps': 1e-05, 'num_heads': 8, 'dropout': 0.1\n    }\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 32\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}