{
    "acronym": "streamingllm",
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "seed_ids": [
        "flashattn",
        "alibi",
        "roformer",
        "compressivetransformer"
    ],
    "s2id": "fdc53c2c10742464087c0525f77e32604827a21d",
    "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
    "authors": [
        "Guangxuan Xiao",
        "Yuandong Tian",
        "Beidi Chen",
        "Song Han",
        "Mike Lewis"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.",
    "citationCount": 227,
    "influentialCitationCount": 41,
    "code": "import math\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\nimport torch.utils.checkpoint\n\nimport torch.nn.functional as F\n\nfrom transformers.models.gpt_neox.modeling_gpt_neox import (\n    apply_rotary_pos_emb,\n    rotate_half,\n    GPTNeoXAttention,\n)\nimport types\n\n__all__ = [\"enable_gpt_neox_pos_shift_attention\"]\n\n\n# streaming_llm.pos_shift.modify_gpt_neox:\n\n###### Modify GPT-NeoX to support positional shift attention ######\n\ndef apply_rotary_pos_emb_single(x, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    x_embed = (x * cos) + (rotate_half(x) * sin)\n    return x_embed\n\n\ndef gpt_neox_pos_shift_attention_forward(\n    self,\n    hidden_states: torch.FloatTensor,\n    attention_mask: torch.FloatTensor,\n    position_ids: torch.LongTensor,\n    head_mask: Optional[torch.FloatTensor] = None,\n    layer_past: Optional[Tuple[torch.Tensor]] = None,\n    use_cache: Optional[bool] = False,\n    output_attentions: Optional[bool] = False,\n):\n    has_layer_past = layer_past is not None\n\n    # Compute QKV\n    # Attention heads [batch, seq_len, hidden_size]\n    #   --> [batch, seq_len, (np * 3 * head_size)]\n    qkv = self.query_key_value(hidden_states)\n\n    # [batch, seq_len, (num_heads * 3 * head_size)]\n    #   --> [batch, seq_len, num_heads, 3 * head_size]\n    new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n    qkv = qkv.view(*new_qkv_shape)\n\n    # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n    query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n    key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n    value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n\n    # Compute rotary embeddings on rotary_ndims\n    query_rot = query[..., : self.rotary_ndims]\n    query_pass = query[..., self.rotary_ndims :]\n\n    # Compute token offset for rotary embeddings (when decoding)\n    seq_len = key.shape[-2]\n    if has_layer_past:\n        seq_len += layer_past[0].shape[-2]\n    cos, sin = self.rotary_emb(value, seq_len=seq_len)\n    query = apply_rotary_pos_emb_single(query_rot, cos, sin, position_ids)\n    query = torch.cat((query, query_pass), dim=-1)\n\n    # Cache QKV values\n    if has_layer_past:\n        past_key = layer_past[0]\n        past_value = layer_past[1]\n        key = torch.cat((past_key, key), dim=-2)\n        value = torch.cat((past_value, value), dim=-2)\n\n    present = (key, value) if use_cache else None\n\n    key_rot = key[..., : self.rotary_ndims]\n    key_pass = key[..., self.rotary_ndims :]\n    key_position_ids = torch.arange(seq_len, device=position_ids.device).unsqueeze(0)\n    key = apply_rotary_pos_emb_single(key_rot, cos, sin, key_position_ids)\n    key = torch.cat((key, key_pass), dim=-1)\n\n    # Compute attention\n    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n    # Reshape outputs\n    attn_output = self._merge_heads(\n        attn_output, self.num_attention_heads, self.head_size\n    )\n    attn_output = self.dense(attn_output)\n\n    outputs = (attn_output, present)\n    if output_attentions:\n        outputs += (attn_weights,)\n\n    return outputs\n\n\ndef enable_gpt_neox_pos_shift_attention(model):\n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            enable_gpt_neox_pos_shift_attention(\n                module,\n            )\n\n        if isinstance(module, GPTNeoXAttention):\n            module.forward = types.MethodType(\n                gpt_neox_pos_shift_attention_forward, module\n            )\n\n###### Similar for LLama and other models ######\n\n\n\n\ndef slice2d(x, start, end):\n    return x[:, :, start:end, ...]\n\n\ndef slice3d(x, start, end):\n    return x[:, :, :, start:end, ...]\n\n\ndef slice1d(x, start, end):\n    return x[:, start:end, ...]\n\n\nDIM_TO_SLICE = {\n    1: slice1d,\n    2: slice2d,\n    3: slice3d,\n}\n\n\nclass StartRecentKVCache:\n    def __init__(\n        self,\n        start_size=4,\n        recent_size=512,\n        k_seq_dim=2,\n        v_seq_dim=2,\n    ):\n        print(f\"StartRecentKVCache: {start_size}, {recent_size}\")\n        self.start_size = start_size\n        self.recent_size = recent_size\n        self.cache_size = start_size + recent_size\n        self.k_seq_dim = k_seq_dim\n        self.v_seq_dim = v_seq_dim\n        self.k_slice = DIM_TO_SLICE[k_seq_dim]\n        self.v_slice = DIM_TO_SLICE[v_seq_dim]\n\n    def __call__(self, past_key_values):\n        if past_key_values is None:\n            return None\n        seq_len = past_key_values[0][0].size(self.k_seq_dim)\n        if seq_len <= self.cache_size:\n            return past_key_values\n        return [\n            [\n                torch.cat(\n                    [\n                        self.k_slice(k, 0, self.start_size),\n                        self.k_slice(k, seq_len - self.recent_size, seq_len),\n                    ],\n                    dim=self.k_seq_dim,\n                ),\n                torch.cat(\n                    [\n                        self.v_slice(v, 0, self.start_size),\n                        self.v_slice(v, seq_len - self.recent_size, seq_len),\n                    ],\n                    dim=self.v_seq_dim,\n                ),\n            ]\n            for k, v in past_key_values\n        ]\n\n    def evict_for_space(self, past_key_values, num_coming):\n        if past_key_values is None:\n            return None\n        seq_len = past_key_values[0][0].size(self.k_seq_dim)\n        if seq_len + num_coming <= self.cache_size:\n            return past_key_values\n        return [\n            [\n                torch.cat(\n                    [\n                        self.k_slice(k, 0, self.start_size),\n                        self.k_slice(\n                            k, seq_len - self.recent_size + num_coming, seq_len\n                        ),\n                    ],\n                    dim=self.k_seq_dim,\n                ),\n                torch.cat(\n                    [\n                        self.v_slice(v, 0, self.start_size),\n                        self.v_slice(\n                            v, seq_len - self.recent_size + num_coming, seq_len\n                        ),\n                    ],\n                    dim=self.v_seq_dim,\n                ),\n            ]\n            for k, v in past_key_values\n        ]\n\n    def evict_range(self, past_key_values, start, end):\n        if past_key_values is None:\n            return None\n        seq_len = past_key_values[0][0].size(self.k_seq_dim)\n        assert start <= end and end <= seq_len\n        return [\n            [\n                torch.cat(\n                    [\n                        self.k_slice(k, 0, start),\n                        self.k_slice(k, end, seq_len),\n                    ],\n                    dim=self.k_seq_dim,\n                ),\n                torch.cat(\n                    [\n                        self.v_slice(v, 0, start),\n                        self.v_slice(v, end, seq_len),\n                    ],\n                    dim=self.v_seq_dim,\n                ),\n            ]\n            for k, v in past_key_values\n        ]\n\ndef enable_streaming_llm(model, start_size, recent_size):\n    if \"llama\" in model.config.model_type:\n        k_seq_dim = v_seq_dim = 2\n        from streaming_llm.pos_shift.modify_llama import (\n            enable_llama_pos_shift_attention,\n        )\n\n        enable_llama_pos_shift_attention(model)\n    elif \"mpt\" in model.config.model_type:\n        v_seq_dim = 2\n        k_seq_dim = 3\n    elif \"gpt_neox\" in model.config.model_type:\n        k_seq_dim = v_seq_dim = 2\n        from streaming_llm.pos_shift.modify_gpt_neox import (\n            enable_gpt_neox_pos_shift_attention,\n        )\n\n        enable_gpt_neox_pos_shift_attention(model)\n    elif \"falcon\" in model.config.model_type:\n        v_seq_dim = 1\n        k_seq_dim = 1\n        from streaming_llm.pos_shift.modify_falcon import (\n            enable_falcon_pos_shift_attention,\n        )\n\n        enable_falcon_pos_shift_attention(model)\n    else:\n        raise ValueError(f\"got {model.config.model_type}\")\n    kv_cache = StartRecentKVCache(\n        start_size=start_size,\n        recent_size=recent_size,\n        k_seq_dim=k_seq_dim,\n        v_seq_dim=v_seq_dim,\n    )\n    return kv_cache",
    "description": null,
    "url": null
}