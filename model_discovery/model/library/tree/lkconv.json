{
    "acronym": "lkconv",
    "title": "Time-aware Large Kernel Convolutions",
    "seed_ids": [
        "lighdynconv",
        "transformer"
    ],
    "s2id": "af34ea4242ca8725ea739ec1bef674bec10c1fa9",
    "abstract": "To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.",
    "authors": [
        "Vasileios Lioutas",
        "Yuhong Guo"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "tldr": "Time-aware Large Kernel (TaLK) Convolutions is introduced, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix.",
    "citationCount": 28,
    "influentialCitationCount": 0,
    "code": "import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqIncrementalDecoder,\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import (\n    AdaptiveSoftmax,\n    LayerNorm,\n    PositionalEmbedding,\n    MultiheadAttention,\n)\n\nimport talkconv_cuda\n\ntry:\n    from apex import amp\nexcept ImportError:\n    raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n\n\nclass TaLKConvolutionEncoderFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, input_x, offset_left, offset_right, max_left, max_right):\n    output = talkconv_cuda.talk_convolution_encoder_forward(input_x, offset_left, offset_right, max_left, max_right)\n\n    ctx.save_for_backward(input_x, offset_left, offset_right)\n    ctx.max_left = max_left\n    ctx.max_right = max_right\n\n    return output\n\n  @staticmethod\n  @amp.float_function\n  def backward(ctx, grad_output):\n    input_x, offset_left, offset_right = ctx.saved_tensors\n    max_left = ctx.max_left\n    max_right = ctx.max_right\n\n    retval = talkconv_cuda.talk_convolution_encoder_backward(input_x, offset_left, offset_right, max_left, max_right, grad_output.contiguous())\n\n    return tuple([retval[0], retval[1], retval[2], None, None])\n\n\nclass TaLKConvolutionDecoderFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, input_x, offset_left, max_left):\n    output = talkconv_cuda.talk_convolution_decoder_forward(input_x, offset_left, max_left)\n\n    ctx.save_for_backward(input_x, offset_left)\n    ctx.max_left = max_left\n\n    return output\n\n  @staticmethod\n  @amp.float_function\n  def backward(ctx, grad_output):\n    input_x, offset_left = ctx.saved_tensors\n    max_left = ctx.max_left\n\n    retval = talkconv_cuda.talk_convolution_decoder_backward(input_x, offset_left, max_left, grad_output.contiguous())\n\n    return tuple([retval[0], retval[1], None])\n  \n\nclass TaLKConv(nn.Module):\n  def __init__(self, hid_dim, offsets_dropout=0.0, decode=False, num_heads=1, min_len_left=1, min_len_right=1):\n    super().__init__()\n\n    self.hid_dim = hid_dim\n    self.decode = decode\n\n    self.num_heads = num_heads\n    self.R = self.hid_dim // self.num_heads\n\n    self.min_len_left = min_len_left\n    self.min_len_right = min_len_right\n\n    if not self.decode:\n        self.offsets = nn.Linear(self.hid_dim, self.num_heads * 2, bias=True)\n    else:\n        self.offsets = nn.Linear(self.hid_dim, self.num_heads, bias=True)\n\n    self.do = nn.Dropout(offsets_dropout)\n\n\n  def forward(self, x, incremental_state=None, mask=None):\n\n    _, B, C = x.size()\n    H = self.num_heads\n    R = C // H\n    K = self.min_len_left + self.min_len_right + 1\n\n\n    if incremental_state is not None:\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is None:\n            input_buffer = x * (1/K)\n        else:\n            input_buffer = torch.cat([input_buffer, (x * (1/K)) + input_buffer[-1:]], dim=0)\n\n        self._set_input_buffer(incremental_state, input_buffer)\n        x_sum = input_buffer.view(-1, B*H, R)\n\n        T = x.shape[0]\n    else:\n        T = x.shape[0]\n\n        x_sum = torch.cumsum(x.view(T, B*H, R)*(1/K), 0)\n\n\n    x_offsets = torch.sigmoid(self.offsets(x))\n    x_offsets = self.do(x_offsets)\n\n    if not self.decode:\n        x_offset_left, x_offset_right = x_offsets[:,:,:H].contiguous().view(T, B*H), x_offsets[:,:,H:].contiguous().view(T, B*H)\n    else:\n        x_offset_left = x_offsets.view(T, B*H)\n\n\n    if incremental_state is not None:\n        x_output = talkconv_cuda.talk_convolution_decoder_inference_forward(x_sum, x_offset_left.squeeze(-1), self.min_len_left)\n    else:\n        if not self.decode:\n            x_output = TaLKConvolutionEncoderFunction.apply(x_sum, x_offset_left.squeeze(-1), x_offset_right.squeeze(-1), self.min_len_left, self.min_len_right)\n        else:\n            x_output = TaLKConvolutionDecoderFunction.apply(x_sum, x_offset_left.squeeze(-1), self.min_len_left)\n\n\n    x_output = x_output.view(T, B, C)\n\n    return x_output\n\n  def _get_input_buffer(self, incremental_state):\n    return utils.get_incremental_state(self, incremental_state, 'input_buffer')\n\n  def _set_input_buffer(self, incremental_state, new_buffer):\n    return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)\n\n  def reorder_incremental_state(self, incremental_state, new_order):\n    input_buffer = self._get_input_buffer(incremental_state)\n    if input_buffer is not None:\n      input_buffer = input_buffer.index_select(1, new_order)\n      self._set_input_buffer(incremental_state, input_buffer)\n\n\nclass TaLKConvDecoder(FairseqIncrementalDecoder):\n    \"\"\"\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs.\n            Default: ``False``\n    \"\"\"\n\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False, final_norm=True):\n        super().__init__(dictionary)\n        self.dropout = args.dropout\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n\n        input_embed_dim = embed_tokens.embedding_dim\n        embed_dim = args.decoder_embed_dim\n        output_embed_dim = args.decoder_output_dim\n\n        padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_scale = math.sqrt(embed_dim)  # todo: try with input_embed_dim\n\n        self.project_in_dim = Linear(input_embed_dim, embed_dim, bias=False) if embed_dim != input_embed_dim else None\n\n        self.embed_positions = PositionalEmbedding(\n            args.max_target_positions, embed_dim, padding_idx,\n            learned=args.decoder_learned_pos,\n        ) if not args.no_token_positional_embeddings else None\n\n        self.layers = nn.ModuleList([])\n        self.layers.extend([\n            TaLKConvDecoderLayer(args, no_encoder_attn, kernel_size=args.decoder_kernel_size_list[i])\n            for i in range(args.decoder_layers)\n        ])\n\n        self.adaptive_softmax = None\n\n        self.project_out_dim = Linear(embed_dim, output_embed_dim, bias=False) \\\n            if embed_dim != output_embed_dim and not args.tie_adaptive_weights else None\n\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dictionary),\n                output_embed_dim,\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.adaptive_softmax_dropout,\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\n                factor=args.adaptive_softmax_factor,\n                tie_proj=args.tie_adaptive_proj,\n            )\n        elif not self.share_input_output_embed:\n            self.embed_out = nn.Parameter(torch.Tensor(len(dictionary), output_embed_dim))\n            nn.init.normal_(self.embed_out, mean=0, std=output_embed_dim ** -0.5)\n        self.register_buffer('version', torch.Tensor([2]))\n        self.normalize = args.decoder_normalize_before and final_norm\n        if self.normalize:\n            self.layer_norm = LayerNorm(embed_dim)\n\n        self.acts_reg = []\n\n    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):\n        \"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"\n        # embed positions\n        positions = self.embed_positions(\n            prev_output_tokens,\n            incremental_state=incremental_state,\n        ) if self.embed_positions is not None else None\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            if positions is not None:\n                positions = positions[:, -1:]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        if positions is not None:\n            x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        attn = None\n\n        inner_states = [x]\n\n        # decoder layers\n        for layer in self.layers:\n            x, attn = layer(\n                x,\n                encoder_out['encoder_out'] if encoder_out is not None else None,\n                encoder_out['encoder_padding_mask'] if encoder_out is not None else None,\n                incremental_state\n            )\n            inner_states.append(x)\n\n        if self.normalize:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        if self.adaptive_softmax is None:\n            # project back to size of vocabulary\n            if self.share_input_output_embed:\n                x = F.linear(x, self.embed_tokens.weight)\n            else:\n                x = F.linear(x, self.embed_out)\n\n        return x, {'attn': attn, 'inner_states': inner_states}\n\n    def max_positions(self):\n        \"\"\"Maximum output length supported by the decoder.\"\"\"\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return min(self.max_target_positions, self.embed_positions.max_positions)\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        if not hasattr(self, '_future_mask') or self._future_mask is None or self._future_mask.device != tensor.device:\n            self._future_mask = torch.triu(utils.fill_with_neg_inf(tensor.new(dim, dim)), 1)\n        if self._future_mask.size(0) < dim:\n            self._future_mask = torch.triu(utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1)\n        return self._future_mask[:dim, :dim]\n\n\nclass TaLKConvDecoderLayer(nn.Module):\n    \"\"\"Decoder layer block.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs.\n            Default: ``False``\n        kernel_size: kernel size of the convolution\n    \"\"\"\n\n    def __init__(self, args, no_encoder_attn=False, kernel_size=1):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.conv_dim = args.decoder_conv_dim\n        if args.decoder_glu:\n            self.linear1 = Linear(self.embed_dim, 2*self.conv_dim)\n            self.act = nn.GLU()\n        else:\n            self.linear1 = Linear(self.embed_dim, self.conv_dim)\n            self.act = None\n\n        self.conv = TaLKConv(self.conv_dim, offsets_dropout=args.weight_dropout, decode=True, num_heads=args.decoder_attention_heads, min_len_left=kernel_size, min_len_right=0)\n\n        self.linear2 = Linear(self.conv_dim, self.embed_dim)\n\n        self.dropout = args.dropout\n        self.relu_dropout = args.relu_dropout\n        self.input_dropout = args.input_dropout\n        self.normalize_before = args.decoder_normalize_before\n\n        self.conv_layer_norm = LayerNorm(self.embed_dim)\n\n        if no_encoder_attn:\n            self.encoder_attn = None\n            self.encoder_attn_layer_norm = None\n        else:\n            self.encoder_attn = MultiheadAttention(\n                self.embed_dim, args.decoder_attention_heads,\n                dropout=args.attention_dropout, encoder_decoder_attention=True\n            )\n            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n\n        self.fc1 = Linear(self.embed_dim, args.decoder_ffn_embed_dim)\n        self.fc2 = Linear(args.decoder_ffn_embed_dim, self.embed_dim)\n\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n        self.need_attn = True\n\n    def forward(self, x, encoder_out, encoder_padding_mask, incremental_state,\n                prev_conv_state=None, prev_attn_state=None, conv_mask=None,\n                conv_padding_mask=None):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"\n        residual = x\n        x = self.maybe_layer_norm(self.conv_layer_norm, x, before=True)\n        if prev_conv_state is not None:\n            if incremental_state is None:\n                incremental_state = {}\n            self.conv._set_input_buffer(incremental_state, prev_conv_state)\n        x = F.dropout(x, p=self.input_dropout, training=self.training)\n        x = self.linear1(x)\n        if self.act is not None:\n            x = self.act(x)\n\n        x = self.conv(x, incremental_state=incremental_state)\n\n        x = self.linear2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.conv_layer_norm, x, after=True)\n\n        attn = None\n        if self.encoder_attn is not None:\n            residual = x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, before=True)\n            if prev_attn_state is not None:\n                if incremental_state is None:\n                    incremental_state = {}\n                prev_key, prev_value = prev_attn_state\n                saved_state = {\"prev_key\": prev_key, \"prev_value\": prev_value}\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n            x, attn = self.encoder_attn(\n                query=x,\n                key=encoder_out,\n                value=encoder_out,\n                key_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=(not self.training and self.need_attn),\n            )\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = residual + x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, after=True)\n\n        residual = x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n        x = swish(self.fc1(x))\n        x = F.dropout(x, p=self.relu_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n        return x, attn\n\n    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n        assert before ^ after\n        if after ^ self.normalize_before:\n            return layer_norm(x)\n        else:\n            return x\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n    def extra_repr(self):\n        return 'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'.format(\n            self.dropout, self.relu_dropout, self.input_dropout, self.normalize_before)\n\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.)\n    return m\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))\n\ndef swish(x, beta=1):\n    return x * torch.sigmoid(beta * x)\n",
    "description": null,
    "url": null
}