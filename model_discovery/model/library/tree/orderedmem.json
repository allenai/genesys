{
    "acronym": "orderedmem",
    "title": "Ordered Memory ",
    "seed_ids": [],
    "s2id": "eb48aae0bd3a6311cdaf38b4d95aca41538a2d0c",
    "abstract": "Natural language semantics can be modeled using the phrase-structured model, which can be represented using a tree-type architecture. As a result, recent advances in natural language processing have been made utilising recursive neural networks using memory models that allow them to infer tree-type representations of the input sentence sequence. These new tree models have allowed for improvements in sentiment analysis and semantic recognition. Here we review the Ordered Memory model proposed by Shen et al. (2019) at the NeurIPS 2019 conference, and try to either create baselines that can perform better or create simpler models that can perform equally as well. We found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.",
    "authors": [
        "Daniel Borisov",
        "Matthew D\u2019Iorio",
        "Jeffrey Hyacinthe"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "It is found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\nclass Distribution(nn.Module):\n    def __init__(self, nslot, hidden_size, dropout):\n        super(Distribution, self).__init__()\n\n        self.query = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n        )\n\n        self.key = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n        )\n\n        self.beta = nn.Sequential(\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, 1),\n        )\n\n        self.hidden_size = hidden_size\n\n    def init_p(self, bsz, nslot):\n        return None\n\n    @staticmethod\n    def process_softmax(beta, prev_p):\n        if prev_p is None:\n            return torch.zeros_like(beta), torch.ones_like(beta), torch.zeros_like(beta)\n\n        beta_normalized = beta - beta.max(dim=-1)[0][:, None]\n        x = torch.exp(beta_normalized)\n\n        prev_cp = torch.cumsum(prev_p, dim=1)\n        mask = prev_cp[:, 1:]\n        mask = mask.masked_fill(mask < 1e-5, 0.)\n        mask = F.pad(mask, (0, 1), value=1)\n\n        x_masked = x * mask\n\n        p = F.normalize(x_masked, p=1)\n        cp = torch.cumsum(p, dim=1)\n        rcp = torch.cumsum(p.flip([1]), dim=1).flip([1])\n        return cp, rcp, p\n\n    def forward(self, in_val, prev_out_M, prev_p):\n        query = self.query(in_val)\n        key = self.key(prev_out_M)\n        beta = self.beta(query[:, None, :] + key).squeeze(dim=2)\n        beta = beta / math.sqrt(self.hidden_size)\n        cp, rcp, p = self.process_softmax(beta, prev_p)\n        return cp, rcp, p\n\n\nclass Cell(nn.Module):\n    def __init__(self, hidden_size, dropout, activation=None):\n        super(Cell, self).__init__()\n        self.hidden_size = hidden_size\n        self.cell_hidden_size = 4 * hidden_size\n\n        self.input_t = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size * 2, self.cell_hidden_size),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(self.cell_hidden_size, hidden_size * 4),\n        )\n\n        self.gates = nn.Sequential(\n            nn.Sigmoid(),\n        )\n\n        assert activation is not None\n        self.activation = activation\n\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, vi, hi):\n        input = torch.cat([vi, hi], dim=-1)\n\n        g_input, cell = self.input_t(input).split(\n            (self.hidden_size * 3, self.hidden_size),\n            dim=-1\n        )\n\n        gates = self.gates(g_input)\n        vg, hg, cg = gates.chunk(3, dim=1)\n        output = self.activation(vg * vi + hg * hi + cg * cell)\n        return output\n\n\nclass OrderedMemoryRecurrent(nn.Module):\n    def __init__(self, input_size, slot_size, nslot,\n                 dropout=0.2, dropoutm=0.2):\n        super(OrderedMemoryRecurrent, self).__init__()\n\n        self.activation = nn.LayerNorm(slot_size)\n        self.input_projection = nn.Sequential(\n            nn.Linear(input_size, slot_size),\n            self.activation\n        )\n\n        self.distribution = Distribution(nslot, slot_size, dropoutm)\n\n        self.cell = Cell(slot_size, dropout, activation=self.activation)\n\n        self.nslot = nslot\n        self.slot_size = slot_size\n        self.input_size = input_size\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        zeros = weight.new(bsz, self.nslot, self.slot_size).zero_()\n        p = self.distribution.init_p(bsz, self.nslot)\n        return (zeros, zeros, p)\n\n    def omr_step(self, in_val, prev_M, prev_out_M, prev_p):\n        batch_size, nslot, slot_size = prev_M.size()\n        _batch_size, slot_size = in_val.size()\n\n        assert self.slot_size == slot_size\n        assert self.nslot == nslot\n        assert batch_size == _batch_size\n\n        cp, rcp, p = self.distribution(in_val, prev_out_M, prev_p)\n\n        curr_M = prev_M * (1 - rcp)[:, :, None] + prev_out_M * rcp[:, :, None]\n\n        M_list = []\n        h = in_val\n        for i in range(nslot):\n            if i == nslot - 1 or cp[:, i + 1].max() > 0:\n                h = self.cell(h, curr_M[:, i, :])\n                h = in_val * (1 - cp)[:, i, None] + h * cp[:, i, None]\n            M_list.append(h)\n        out_M = torch.stack(M_list, dim=1)\n\n        output = out_M[:, -1]\n        return output, curr_M, out_M, p\n\n    def forward(self, X, hidden, mask=None):\n        prev_M, prev_memory_output, prev_p = hidden\n        output_list = []\n        p_list = []\n        X_projected = self.input_projection(X)\n        if mask is not None:\n            padded = ~mask\n        for t in range(X_projected.size(0)):\n            output, prev_M, prev_memory_output, prev_p = self.omr_step(\n                X_projected[t], prev_M, prev_memory_output, prev_p)\n            if mask is not None:\n                padded_1 = padded[t, :, None]\n                padded_2 = padded[t, :, None, None]\n                output = output.masked_fill(padded_1, 0.)\n                prev_p = prev_p.masked_fill(padded_1, 0.)\n                prev_M = prev_M.masked_fill(padded_2, 0.)\n                prev_memory_output = prev_memory_output.masked_fill(padded_2, 0.)\n            output_list.append(output)\n            p_list.append(prev_p)\n\n        output = torch.stack(output_list)\n        probs = torch.stack(p_list)\n\n        return (output,\n                probs,\n                (prev_M, prev_memory_output, prev_p))\n\n\nclass OrderedMemory(nn.Module):\n    def __init__(self, config):\n        super(OrderedMemory, self).__init__()\n\n        input_size = config[\"hidden_size\"]\n        slot_size = config[\"hidden_size\"]\n        nslot = config[\"memory_slots\"]\n        if not self.training:\n            nslot = 2 * nslot\n        dropoutm = config[\"memory_dropout\"]\n        dropout = config[\"dropout\"]\n        bidirection = False #config[\"bidirection\"]\n        self.output_last = False #config[\"output_last\"]\n\n        self.OM_forward = OrderedMemoryRecurrent(input_size, slot_size, nslot,\n                                                 dropout=dropout, dropoutm=dropoutm)\n        if bidirection:\n            self.OM_backward = OrderedMemoryRecurrent(input_size, slot_size, nslot,\n                                                      dropout=dropout, dropoutm=dropoutm)\n\n        self.bidirection = bidirection\n\n    def init_hidden(self, bsz):\n        return self.OM_forward.init_hidden(bsz)\n\n    def forward(self, X, input_mask, **kwargs):\n\n        X = X.permute(1, 0, 2).contiguous()\n        mask = input_mask.permute(1, 0).contiguous()\n        mask = mask.bool()\n\n        output_last = False #self.output_last\n\n        bsz = X.size(1)\n        lengths = mask.sum(0)\n        init_hidden = self.init_hidden(bsz)\n\n        output_list = []\n        prob_list = []\n\n        om_output_forward, prob_forward, _ = self.OM_forward(X, init_hidden, mask)\n        if output_last:\n            output_list.append(om_output_forward[-1])\n        else:\n            output_list.append(om_output_forward[lengths - 1, torch.arange(bsz).long()])\n        prob_list.append(prob_forward)\n\n        if self.bidirection:\n            om_output_backward, prob_backward, _ = self.OM_backward(X.flip([0]), init_hidden, mask.flip([0]))\n            output_list.append(om_output_backward[-1])\n            prob_list.append(prob_backward.flip([0]))\n\n        output = torch.cat(output_list, dim=-1)\n        self.probs = prob_list[0]\n\n        return {\"global_state\": output, \"sequence\": om_output_forward.permute(1, 0, 2).contiguous(),\n                \"input_mask\": input_mask}\n    ",
    "description": null,
    "url": null
}