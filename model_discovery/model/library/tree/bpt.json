{
    "acronym": "bpt",
    "title": "Blockwise Parallel Transformer for Large Context Models",
    "seed_ids": [
        "mega",
        "flashattn",
        "mea",
        "linformer",
        "reformer",
        "sparsetransformer",
        "transformer"
    ],
    "s2id": "86b6e42e2ce957f6497d4aa578c9bb4d2b4e4ba3",
    "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.",
    "authors": [
        "Hao Liu",
        "P. Abbeel"
    ],
    "venue": "",
    "year": 2023,
    "tldr": "This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.",
    "citationCount": 5,
    "influentialCitationCount": 1,
    "code": "import functools\nimport json\nimport math\nfrom functools import partial\nfrom typing import Callable, NamedTuple, Optional\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom einops import rearrange\nfrom flax.linen import combine_masks, make_causal_mask\nfrom jax import lax\nfrom jax import numpy as jnp\n\n\ndef quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)\n\nACT2FN = {\n    \"gelu\": partial(nn.gelu, approximate=False),\n    \"relu\": nn.relu,\n    \"silu\": nn.swish,\n    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,\n}\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n    }[name]\n\nMASK_VALUE = -1e10\n\nQ_CHUNK_SIZE = 1024\nK_CHUNK_SIZE = 1024\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)\n\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass _AttentionBlock(nn.Module):\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    float32_logits: bool = False\n\n    def setup(self):\n        self.embed_dim = self.hidden_size\n        self.head_dim = self.embed_dim // self.num_heads\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\n        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n        self.fc_in = nn.Dense(self.intermediate_size,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.fc_out = nn.Dense(self.embed_dim,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.act = ACT2FN[self.activation_function]\n        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            pos_embd_dim = self.rotary_dim\n        else:\n            pos_embd_dim = self.embed_dim // self.num_heads\n        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    def attn_out_proj(self, attn_output, deterministic):\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        return attn_output\n\n    def forward_qkv(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        query = self.q_proj(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim :]\n\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim :]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n            query = apply_rotary_pos_emb(query, sincos)\n\n        if self.float32_logits:\n            query = query.astype(jnp.float32)\n            key = key.astype(jnp.float32)\n\n        return query, key, value\n\n    def forward_ffn(\n        self,\n        hidden_states,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_2(hidden_states)\n        hidden_states = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc_out(hidden_states)\n        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\n        return hidden_states\n\n\nclass AttentionBlock(nn.Module):\n    q_chunk_size: int\n    k_chunk_size: int\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    attn_pdrop: float = 0.0\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    policy: str = 'nothing_saveable'\n    prevent_cse: bool = False\n    float32_logits: bool = False\n\n    def setup(self):\n        self.attn = _AttentionBlock(\n            self.hidden_size,\n            self.num_heads,\n            self.rotary_dim,\n            self.intermediate_size,\n            self.layer_norm_epsilon,\n            self.activation_function,\n            self.resid_pdrop,\n            self.max_position_embeddings,\n            self.dtype,\n            self.causal,\n            self.float32_logits,\n        )\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        This function takes projected key, value states from a single input token and concatenates the states to cached\n        states from previous steps. This function is slighly adapted from the official Flax repository:\n        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n        \"\"\"\n        # detect if we're initializing by absence of existing cache data.\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        position_ids,\n        deterministic: bool = True,\n        init_cache: bool = False,\n    ):\n        query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n        query = query / jnp.sqrt(query.shape[-1])\n\n        dropout_rng = None\n        if not deterministic and self.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n        )\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n            # use standard dot product attention since query length is 1\n            attn_weights = nn.dot_product_attention_weights(\n                query,\n                key,\n                bias=attention_bias,\n                dropout_rng=dropout_rng,\n                dropout_rate=self.config.attn_pdrop,\n                deterministic=deterministic,\n                dtype=self.dtype,\n                precision=None,\n            )\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n            outputs = attn_output + ffn_output + hidden_states\n        else:\n            attn_output = blockwise_compute_attn(\n                query,\n                key,\n                value,\n                bias=attention_bias,\n                deterministic=not deterministic,\n                dropout_rng=dropout_rng,\n                attn_pdrop=self.attn_pdrop,\n                causal_mask=self.causal,\n                query_chunk_size=self.q_chunk_size,\n                key_chunk_size=self.k_chunk_size,\n                dtype=self.dtype,\n                policy=self.policy,\n                precision=None,\n                prevent_cse=self.prevent_cse,\n            )\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = blockwise_compute_ffn(\n                self.attn,\n                hidden_states + attn_output,\n                chunk_size=self.q_chunk_size,\n                deterministic=deterministic,\n                policy=self.policy,\n                prevent_cse=self.prevent_cse,\n            )\n            outputs = ffn_output + hidden_states + attn_output\n        return outputs\n\n\ndef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n            query_chunk_idx, key_chunk_idx):\n    query_offset = query_chunk_idx * query_chunk_size\n    key_offset = key_chunk_idx * key_chunk_size\n    chunk_bias = jnp.zeros((1, 1, 1, 1))\n    if bias is not None:\n        chunk_bias = lax.dynamic_slice(\n            bias,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n        )\n\n    if causal_mask:\n        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n        offset = query_offset - key_offset\n        query_idx += offset\n        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_slice = lax.dynamic_slice(\n            attn_dropout,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(\n                *attn_dropout.shape[:2],\n                min(attn_dropout.shape[-2], query_chunk_size),\n                min(attn_dropout.shape[-1], key_chunk_size),\n            ),\n        )\n        chunk_bias -= attn_dropout_slice * 1e6\n    return chunk_bias\n\nclass Carry(NamedTuple):\n    numerator: jax.Array\n    denominator: jax.Array\n    max_so_far: jax.Array\n\ndef blockwise_compute_attn(query, key, value,\n        bias=None,\n        deterministic=False,\n        dropout_rng=None,\n        attn_pdrop=0.0,\n        causal_mask=True,\n        query_chunk_size=None,\n        key_chunk_size=None,\n        dtype=jnp.float32,\n        policy='nothing_saveable',\n        precision=lax.Precision.HIGHEST,\n        prevent_cse=False,):\n    q_len = query.shape[1]\n    kv_len = key.shape[1]\n    query = rearrange(query, 'b (n c) h q -> b n c h q', c=query_chunk_size)\n    key, value = map(lambda t: rearrange(t, 'b (n c) h v -> b n c h v', c=key_chunk_size), (key, value))\n    query, key, value = map(lambda t: rearrange(t, 'b n c h d -> n b c h d'), (query, key, value))\n    num_q, batch, _, num_heads, dim_per_head = query.shape\n    num_kv, _, _, _, _ = key.shape\n\n    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n        assert bias_dim == 1 or bias_dim == broadcast_dim\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n    else:\n        attn_dropout = None\n\n    _chunk_bias_fn = functools.partial(\n        _chunk_attention_bias,\n        query_chunk_size, key_chunk_size,\n        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\n    def _query_chunk_attention(args):\n        query_chunk, query_chunk_idx = args\n\n        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse,\n                           policy=get_gradient_checkpoint_policy(policy))\n        def summarize_chunk(carry, args):\n            key_chunk, value_chunk, key_chunk_idx = args\n            (numerator, denominator, prev_max_score) = carry\n            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n            attn_weights = attn_weights + bias_chunk\n\n            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n            max_score = jnp.maximum(prev_max_score, max_score)\n            max_score = jax.lax.stop_gradient(max_score)\n            exp_weights = jnp.exp(attn_weights - max_score)\n            exp_values = jnp.einsum(\n                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n            )\n            correction = jnp.exp(prev_max_score - max_score)\n            numerator = numerator * correction + exp_values\n            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n            return Carry(numerator, denominator, max_score), None\n\n        init_carry = Carry(\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n        )\n        (numerator, denominator, max_score), _ = lax.scan(\n            summarize_chunk, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n        )\n        outputs = (numerator / denominator).astype(dtype)\n        return outputs\n\n    _, res = lax.scan(\n        lambda _, x: ((), _query_chunk_attention(x)),\n        (), xs=(query, jnp.arange(0, num_q))\n    )\n    res = rearrange(res, 'n b c h d -> b (n c) h d')\n    return res\n\ndef blockwise_compute_ffn(cell, inputs, chunk_size, deterministic, policy, prevent_cse):\n    inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=chunk_size)\n    inputs = rearrange(inputs, 'b n c d -> n b c d')\n    num_q, _, _, _ = inputs.shape\n    def ffn(cell, _, hidden_states):\n        outputs = cell.forward_ffn(hidden_states, deterministic=deterministic)\n        return _, outputs\n    ffn_remat = nn.remat(\n        ffn,\n        variables=\"params\",\n        rngs={\"params\" : False},\n        prevent_cse=prevent_cse,\n        policy=get_gradient_checkpoint_policy(policy),\n    )\n    _, res = nn.scan(\n        ffn_remat,\n        variable_broadcast=\"params\",\n        split_rngs={\"params\": False},\n        in_axes=0,\n        out_axes=0,\n        length=num_q,\n    )(cell, None, inputs)\n    res = rearrange(res, 'n b c d -> b (n c) d')\n    return res\n",
    "description": null,
    "url": null
}