{
    "acronym": "hyperattention",
    "title": "HyperAttention: Long-context Attention in Near-Linear Time",
    "seed_ids": [
        "scatterbrain",
        "reformer"
    ],
    "s2id": "93e58491830abe1eb965ab37ec64fa97263f6048",
    "abstract": "We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50\\% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.",
    "authors": [
        "Insu Han",
        "Rajesh Jayaram",
        "Amin Karbasi",
        "V. Mirrokni",
        "David P. Woodruff",
        "A. Zandieh"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "An approximate attention mechanism named HyperAttention is presented to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models, and is able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank.",
    "citationCount": 29,
    "influentialCitationCount": 4,
    "code": "import math\nimport torch\nfrom einops import rearrange\n\nimport math\nimport torch\ntry:\n    from flash_attn import flash_attn_func as flash_attn_func_cuda\nexcept ImportError:\n    flash_attn_func_cuda = None\n\nfrom .flash_attn_triton_for_hyper import flash_attn_func\n\n\ndef indexing(x, indices, chunk_size=-1):\n    \"\"\" \n    inputs:\n        - x: 4d-tensor with shape [b, h, n, d] \n        - indices: 3d-tensor with shape [b, h, s] where each entry should be in [0, n-1]\n    output:\n        - out: 4d-tensor with shape [b, h, s, d] where out[i,j] = x[i,j][indices[i,j],:]\n    \n    A naive implementation:\n        out = torch.zeros(b, h, s, d)\n        for i in range(b):\n            for j in range(h):\n                out[i,j] = x[i,j][idx[i,j],:]\n        return out\n    \"\"\"\n    if chunk_size < 0 or (chunk_size > 0 and x.shape[-2] % chunk_size == 0):\n        return x.gather(2, indices.unsqueeze(-1).expand(-1, -1, -1, x.shape[-1]))\n    else:\n        x = x.gather(2, indices.unsqueeze(-1).expand(-1, -1, -1, x.shape[-1]))\n        new_n = math.ceil(x.shape[2] / chunk_size) * chunk_size\n        if new_n <= 0 or new_n - x.shape[2] <= 0:\n            import pdb; pdb.set_trace();\n        return torch.nn.functional.pad(x, (0,0,0,new_n-x.shape[2]), mode='constant',value=0.)\n\n\ndef add_self_attentions(attn1, lse1, attn2, lse2):\n    \"\"\"\n    inputs:\n        - attn1, attn2: 4d-tensors with shape [b, h, n, d]\n        - lse1, lse2: 4d-tensors of log-sum-exp with shape [b, h, n, 1]\n    output:\n        - attn\n        = (attn1 * exp(lse1) + attn2 * exp(lse2)) / (exp(lse1) + exp(lse2))\n        = (attn1 + attn2 * exp(lse2 - lse1)) / (1 + exp(lse2-lse1))\n        = attn1 * c + attn2 * (1-c), where c=1/(1 + exp(lse2-lse1)),\n        - lse \n        = log(exp(lse1) + exp(lse2)) \n        = log(exp(lse1) * (1 + exp(lse2 - lse1))) \n        = lse1 + log(1 + exp(lse2 - lse1)) = lse1 - log(c)\n    \"\"\"\n    c = (1 / (1 + (lse2 - lse1).exp())).to(dtype=attn1.dtype)\n    attn = c * attn1 + (1-c) * attn2\n    lse = lse1 - (c + torch.finfo(lse1.dtype).eps).log()\n    return attn, lse\n\n\ndef exact_attention(query, key, value, softmax_scale, causal=False, bias=None):\n    if query.dtype not in [torch.bfloat16, torch.float16]:\n        qk = query @ key.transpose(-1,-2) * softmax_scale\n        if causal:\n            qk += (torch.ones(query.shape[2], key.shape[2], device=query.device) * torch.finfo(query.dtype).min).triu(1).reshape(1,1,query.shape[2], key.shape[2])\n        out = qk.softmax(dim=-1) @ value\n        lse = torch.logsumexp(qk, dim=-1, keepdim=True)\n        return out, lse\n\n    out, lse = flash_attn_func(\n        query.transpose(1,2), key.transpose(1,2), value.transpose(1,2),\n        bias, causal, softmax_scale)\n    out = out.transpose(1,2)\n    \n    lse = lse.detach()\n    if lse.shape[2] != out.shape[2]:\n        lse = lse[:,:,:out.shape[2]]\n    lse = lse.unsqueeze(-1)\n    return out, lse\n    \n\ndef exact_attention_cuda(query, key, value, softmax_scale, causal, bias=None):\n    if flash_attn_func_cuda is None:\n        raise ImportError(\"Please install flash_attn (pip install flash-attn --no-build-isolation)\")\n    out, lse, _ = flash_attn_func_cuda(\n        query.transpose(1,2), key.transpose(1,2), value.transpose(1,2),\n        softmax_scale=softmax_scale, causal=causal, return_attn_probs=True)\n    out = out.transpose(1,2)\n    lse = lse.unsqueeze(-1)\n    return out, lse\n\n\nclass AngularLSH(torch.nn.Module):\n\n    def __init__(self, num_projs, dim, rng=None):\n        super().__init__()\n        self.num_projs = num_projs\n\n        if num_projs > 0:\n            self.register_buffer('proj_dir', torch.randn(dim + (num_projs,), generator=rng), persistent=False)\n            self.register_buffer('perm', self._unit_hamming_distance_array(self.num_projs), persistent=False)\n            self.register_buffer('enc_vec', 2 ** torch.arange(self.num_projs).view(1, 1, 1, -1), persistent=False)\n            \n    def _unit_hamming_distance_array(self, size_n):\n        if size_n == 1:\n            return torch.tensor([0, 1])\n        a = self._unit_hamming_distance_array(size_n - 1)\n        return torch.concat([a, torch.flip(a, dims=[0]) + 2 ** (size_n - 1)], 0)\n\n    def hash(self, mat):\n        if self.num_projs < 0:\n            return torch.zeros(mat.shape[:-1], device=mat.device, dtype=torch.int32)\n        mask = torch.einsum('...nd,...dr -> ...nr', mat, self.proj_dir)\n        mask = mask > 0\n        bin_ids = (mask * self.enc_vec).sum(-1)\n        return self.perm[bin_ids]\n    \n    def __repr__(self):\n        return f\"AngularLSH(num_proj={self.num_projs}, proj_dir.shape={self.proj_dir.shape})\"\n    \n\nclass HyperAttention(torch.nn.Module):\n\n    def __init__(self, input_dim=64, lsh_num_projs=7, block_size=256, sample_size=256, min_seq_len=4096, cuda=False):\n        super().__init__()\n        self.input_dim = input_dim\n        self.lsh_num_projs = lsh_num_projs\n        self.block_size = block_size\n        self.sample_size = sample_size\n        self.min_seq_len = min_seq_len\n        self.cuda = cuda\n        self.lsh = AngularLSH(num_projs=self.lsh_num_projs, dim=(1, 1, input_dim))\n\n        \n    def forward(self, query: torch.tensor, key: torch.tensor, value: torch.tensor, scale=None, causal=False, return_lse=False):\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n\n        n_query = query.shape[2]\n        batch_size, n_heads, n_key, dim = key.shape\n        scale = dim ** (-0.5) if scale is None else scale\n        \n        # Without causal masking\n        if not causal: \n            attn, lse = self.forward_no_causal_mask(query, key, value, scale)\n\n        # With causal masking\n        else:\n            if n_key <= self.min_seq_len:\n                if self.cuda:\n                    attn, lse = exact_attention_cuda(query, key, value, scale, causal=True)\n                else:\n                    attn, lse = exact_attention(query, key, value, scale, causal=True)\n            else:\n            \n                # If n_query is odd we pad inputs by adding all-zero rows\n                if n_query % 2:\n                    query = torch.nn.functional.pad(query, (0,0,0,1), mode='constant',value=0.)\n                    key = torch.nn.functional.pad(key, (0,0,0,1), mode='constant',value=0.)\n                    value = torch.nn.functional.pad(value, (0,0,0,1), mode='constant',value=0.)\n\n                q_bd = query.view(batch_size, 2*n_heads, query.shape[2]//2, query.shape[-1])\n                k_bd = key.view(batch_size, 2*n_heads, key.shape[2]//2, key.shape[-1])\n                v_bd = value.view(batch_size, 2*n_heads, key.shape[2]//2, value.shape[-1])\n        \n                attn_bd, lse_bd = self.forward(q_bd, k_bd, v_bd, scale, True, True)\n                \n                if attn_bd.shape[2] not in attn_bd.stride():\n                    attn_bd = attn_bd.contiguous()\n                attn_bd = attn_bd.view(batch_size, n_heads, -1, dim)\n\n                if lse_bd.shape[2] not in lse_bd.stride():\n                    lse_bd = lse_bd.contiguous()\n                lse_bd = lse_bd.view(batch_size, n_heads, -1, 1)\n\n                attn_unmasked, lse_unmasked = self.forward_no_causal_mask(\n                    query[:, :, key.shape[2]//2:, :],\n                    key[:, :, :key.shape[2]//2, :], \n                    value[:, :, :key.shape[2]//2, :], scale)\n\n                attn_up, lse_up = attn_bd[:,:,:query.shape[2]//2,:], lse_bd[:,:,:query.shape[2]//2,:]\n                attn_down, lse_down = add_self_attentions(\n                    attn_bd[:,:,query.shape[2]//2:,:],\n                    lse_bd[:,:,query.shape[2]//2:,:],\n                    attn_unmasked,\n                    lse_unmasked)\n\n                attn = torch.cat((attn_up, attn_down), dim=-2)\n                lse = torch.cat((lse_up, lse_down), dim=-2)\n\n                # If n_query was odd exclude the last rows\n                if n_query % 2:\n                    attn = attn[:,:,:-1,:]\n                    lse = lse[:,:,:-1,:]\n\n        if not return_lse:\n            return attn\n        else:\n            return attn, lse\n\n\n    def forward_no_causal_mask(self, query, key, value, scale):\n\n        batch_size, head_size, n_query, dim = query.shape\n        n_key = key.shape[2]\n\n        if self.min_seq_len > n_query:\n            if self.cuda:\n                return exact_attention_cuda(query, key, value, scale, causal=False)\n            else:\n                return exact_attention(query, key, value, scale, causal=False)\n\n        # 1. Sorted block-diagonal via sortLSH\n        _, query_sort_idx = torch.sort(self.lsh.hash(query), dim=2, stable=True) # batch_size x head_size x n\n        _, key_sort_idx = torch.sort(self.lsh.hash(key), dim=2, stable=True)\n        query_sort_idx_inv = torch.argsort(query_sort_idx, dim=2, stable=True) # for recovering the row order\n\n        key_block_size = self.block_size\n\n        query_sorted = indexing(query, query_sort_idx, key_block_size)\n        key_sorted = indexing(key, key_sort_idx, key_block_size)\n        value_sorted = indexing(value, key_sort_idx, key_block_size)\n\n        if key_block_size > 0:\n\n            num_blocks = key_sorted.shape[2] // key_block_size\n            query_block_size = query_sorted.shape[2] // num_blocks\n\n            # Reshape tensors to [batch_size*head_size, 1, block_size, dim] as Flash-attn only allows 4d-tensors\n            query_split_per_block = query_sorted.view(-1, 1, query_block_size, dim)\n            key_split_per_block = key_sorted.view(-1, 1, key_block_size, dim)\n            value_split_per_block = value_sorted.view(-1, 1, key_block_size, dim)\n\n            if self.cuda:\n                attn_block, lse_block = exact_attention_cuda(\n                    query_split_per_block, key_split_per_block, value_split_per_block,\n                    softmax_scale=scale, causal=False)\n            else:\n                attn_block, lse_block = exact_attention(\n                    query_split_per_block, key_split_per_block, value_split_per_block,\n                    softmax_scale=scale, causal=False)\n\n            if attn_block.shape[2] not in attn_block.stride():\n                attn_block = attn_block.contiguous()\n            attn_block = attn_block.view(batch_size, head_size, query_sorted.shape[2], -1)\n\n            if lse_block.shape[2] not in lse_block.stride():\n                lse_block = lse_block.contiguous()\n            lse_block = lse_block.view(batch_size, head_size, query_sorted.shape[2], -1)\n\n            # When inputs are padded, then unpad them\n            if query_sorted.shape[2] != n_query: #query.shape[2]:\n                attn_block, lse_block = attn_block[:,:,:n_query,:], lse_block[:,:,:n_query,:]\n                query_sorted = query_sorted[:,:,:n_query,:]\n                key_sorted = key_sorted[:,:,:n_key,:]\n                value_sorted = value_sorted[:,:,:n_key,:]\n\n        else:\n            query_block_size = -1\n            query_block_size = -1\n            attn_block, lse_block = 0, 0\n\n        # 2. Residual low-rank part via uniform sampling\n        # Sample indices uniformly at random\n        sample_size = self.sample_size\n        if sample_size > 0 and (n_query > query_block_size) and (n_key > key_block_size):\n            sampled_set = torch.randint(n_key, size=(batch_size, head_size, sample_size), device=query_sorted.device)\n            \n            # Compute mask for hiding A_ij computed in block-diagonal attention\n            offset_n = rearrange(torch.arange(n_query, device=query_sorted.device), 'n -> 1 n 1')\n            weights = n_key / sample_size\n            value_subset = indexing(value_sorted, sampled_set)\n            key_subset = indexing(key_sorted, sampled_set)\n\n            if not self.cuda:\n                block_mask = (offset_n // query_block_size) == (sampled_set // key_block_size).view(-1, 1, sample_size)\n                block_mask = block_mask.view(batch_size, head_size, -1, sample_size)\n                block_mask = block_mask.to(query_sorted.dtype)\n                block_mask *= torch.finfo(query_sorted.dtype).min # adding -inf added to QK^T\n\n                attn_res, lse_res = exact_attention(query_sorted, key_subset, value_subset, scale, causal=False, bias=block_mask)\n            else:\n                attn_res, lse_res = exact_attention_cuda(query_sorted, key_subset, value_subset, scale, causal=False)\n            lse_res = lse_res + math.log(weights)\n\n            # Add two attentions\n            if key_block_size > 0:\n                attn, lse = add_self_attentions(attn_block, lse_block, attn_res, lse_res)\n            else:\n                attn, lse = attn_res, lse_res\n        else:\n            attn, lse = attn_block, lse_block\n\n        # Re-order rows with the inverse order for query_sorted -> query\n        attn = indexing(attn, query_sort_idx_inv)\n        lse = indexing(lse, query_sort_idx_inv)\n        return attn, lse",
    "description": null,
    "url": null
}