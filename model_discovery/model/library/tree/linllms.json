{
    "acronym": "linllms",
    "title": "Linearizing Large Language Models",
    "seed_ids": [
        "mamba",
        "yarn",
        "retnet",
        "roformer",
        "lineartransformer"
    ],
    "s2id": "862479f7bd78a69c52a0691766848caa8eb4660f",
    "abstract": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.",
    "authors": [
        "Jean-Pierre Mercat",
        "Igor Vasiljevic",
        "Sedrick Scott Keh",
        "Kushal Arora",
        "Achal Dave",
        "Adrien Gaidon",
        "Thomas Kollar"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "import math\nfrom typing import Callable, Tuple\nfrom functools import partial\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom lightning_attn import lightning_attn_func as lightning_attn_ops\nfrom .utils import get_pos_embed\n\n\n############### Linear attention addition ################\n\n\ndef no_slope_tensor(n_attention_heads: int, device: torch.device, dtype: torch.dtype):\n    \"\"\"\n    This function returns a tensor of zeros, which is equivalent to not using any decay.\n    n_attention_heads: number of attention heads\n    device: device to use\n    dtype: data type to use\n    \"\"\"\n    slopes = torch.zeros(n_attention_heads, 1, 1, device=device, dtype=dtype)\n\n    return slopes\n\n\ndef get_slopes_power_of_2(n, start):\n    \"\"\"\n    This function returns a list of slopes for the linear attention function given a power of 3 number of heads.\n    It is taken from the lightning attention code.\n    n: number of attention heads\n    start: (optional) start value for the slope tensor\n    \"\"\"\n    ratio = 2 ** (-(2 ** -(math.log2(n) - 3)))\n    if start is None:\n        start = ratio\n    return [start * ratio ** i for i in range(n)]\n\n\ndef get_slopes(n, start):\n    \"\"\"\n    This function returns a list of slopes for the linear attention function.\n    It is taken from the lightning attention code.\n    n: number of attention heads\n    start: (optional) start value for the slope tensor\n    \"\"\"\n    if math.log2(n).is_integer():\n        return get_slopes_power_of_2(\n            n, start\n        )  # In the paper, we only train models that have 2^a heads for some a. This function has\n    else:  # some good properties that only occur when the input is a power of 2. To maintain that even\n        closest_power_of_2 = 2 ** math.floor(\n            math.log2(n)\n        )  # when the number of heads is not a power of 2, we use this workaround.\n        return (\n            get_slopes_power_of_2(closest_power_of_2, start)\n            + get_slopes(2 * closest_power_of_2)[0::2][: n - closest_power_of_2]\n        )\n\n\ndef get_slope_tensor(\n    n_attention_heads: int,\n    start: float = None,\n    use_retnet_slopes: bool = False,\n    device: torch.device = None,\n    dtype: torch.dtype = None,\n):\n    \"\"\"\n    This function returns a tensor of slopes for the linear attention function. This determines the decay of the attention function.\n    n_attention_heads: number of attention heads\n    start: (optional) start value for the slope tensor\n    use_retnet_slopes: (optional) use the RetNet slopes instead of the default lightning attention ones\n    device: (optional) device to use\n    dtype: (optional) data type to use\n    \"\"\"\n    if use_retnet_slopes:\n        head_count = torch.arange(n_attention_heads, device=device, dtype=dtype)\n        gamma = 1 - torch.exp2(-5 - head_count.float())\n        slopes = -torch.log(gamma.unsqueeze(-1))\n    else:\n        # h, 1, 1\n        slopes = torch.tensor(get_slopes(n_attention_heads, start), dtype=dtype, device=device).reshape(\n            n_attention_heads,\n            1,\n        )\n    return slopes\n\n\ndef recurrent_forward(\n    queries, keys, vals, s, qk_scale=1, start=None, use_decay=False, use_retnet_slopes=False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    This function computes the output of the linear attention function in a recurrent manner.\n    Its result is equivalent to the parallel computation of the linear attention function.\n    queries: queries, shape (batch_size, num_heads, seq_len, dim_qk)\n    keys: keys, shape (batch_size, num_heads, seq_len, dim_qk)\n    vals: values, shape (batch_size, num_heads, seq_len, dim_v)\n    s: current state of the RNN, shape (batch_size, num_heads, dim_qk, dim_v)\n    use_decay: (optional) use the decaying factor on the distance between queries and keys\n    use_retnet_slopes: (optional) use the RetNet slopes instead of the default lightning attention ones\n    qk_scale: scale factor for queries and keys\n    start: (optional) start value for the slope tensor in case decay is used\n    \"\"\"\n    if use_decay:\n        slope = get_slope_tensor(queries.shape[1], start, use_retnet_slopes, queries.device, queries.dtype)\n        gamma = torch.exp(-slope).reshape(1, queries.shape[1], 1, 1)\n    else:\n        gamma = 1.0\n    s_n = s + (keys.transpose(-1, -2) * qk_scale) @ vals\n    output = queries @ s_n\n    return output, gamma * s_n\n\n\ndef lightning_attn_func(\n    q, k, v, qk_scale: float, start: float = None, use_decay: bool = True, use_retnet_slopes=False\n) -> torch.Tensor:\n    \"\"\"\n    This is the lightning attention function, which is a kernel linear approximation of the softmax function\n    Almost the same as linear_attn_func but using the triton kernel from lightning_attn and a decaying factor (from RetNet paper https://arxiv.org/pdf/2307.08621.pdf)\n    as defined by the depth_slope_tensor function (using no_slope_tensor is equivalent to linear_attn_func).\n    Args:\n        q: queries, shape (batch_size, num_heads, seq_len, dim_qk)\n        k: keys, shape (batch_size, num_heads, seq_len, dim_qk)\n        v: values, shape (batch_size, num_heads, seq_len, dim_v)\n        qk_scale: scale factor for queries and keys\n        start: (optional) start value for the slope tensor in case decay is used\n        use_decay: (optional) use the decaying factor on the distance between queries and keys\n        use_retnet_slopes: (optional) use the RetNet slopes instead of the default lightning attention ones\n    \"\"\"\n    h = q.shape[1]\n    if use_decay:\n        s = get_slope_tensor(h, start, use_retnet_slopes, q.device, torch.float32)\n    else:\n        s = no_slope_tensor(h, q.device, q.dtype)\n    output = lightning_attn_ops(q, k * qk_scale, v, s)\n\n    return output\n\n\nclass LinearAttn(nn.Module):\n    \"\"\"\n    This class implements the linear attention layer.\n    It can be used as a drop-in replacement for the CustomAttn class.\n    The forward method can be run in parallel or recurrent mode depending on the use_cache parameter,\n    which folows the same logic as the CustomAttn class with qk_cache or without it.\n    \"\"\"\n\n    def __init__(self, layer_id, args):\n        super().__init__()\n        self.params = args\n        self.n_heads = args.n_heads\n        self.n_heads_kv = args.n_heads_kv\n        self.qk_head_dim = args.qk_head_dim\n        self.qk_in_head_dim = args.qk_head_dim\n        self.v_head_dim = args.v_head_dim\n\n        self.qk_in_head_dim = args.dim // args.n_heads\n        self.in_proj = nn.Linear(\n            args.dim,\n            (\n                args.n_heads * self.qk_in_head_dim\n                + self.n_heads_kv * self.qk_in_head_dim\n                + self.n_heads_kv * self.v_head_dim\n            ),\n            bias=False,\n        )\n\n        self.out_proj = nn.Linear(args.n_heads * self.v_head_dim, args.dim, bias=False)\n\n        self.pos_embed = get_pos_embed(args)\n\n        self.apply_qk_norm = args.apply_qk_norm\n\n        self._totrain_gn = nn.GroupNorm(\n            num_groups=args.n_heads, num_channels=args.n_heads * self.v_head_dim, affine=False\n        )\n\n        self._totrain_embed = nn.Linear(\n            args.n_heads * self.qk_in_head_dim,\n            args.n_heads * self.qk_head_dim,\n        )\n\n        if args.n_heads != args.n_heads_kv:\n            self._totrain_embed_kv = nn.Linear(\n                args.n_heads_kv * self.qk_in_head_dim,\n                args.n_heads_kv * self.qk_head_dim,\n            )\n\n        self.linear_attn_fn = partial(\n            lightning_attn_func,\n            use_decay=args.use_decay,\n            use_retnet_slopes=args.use_retnet_slopes,\n            start=args.decay_start,\n        )\n        self.recurrent_forward_fn = partial(\n            recurrent_forward,\n            use_decay=args.use_decay,\n            use_retnet_slopes=args.use_retnet_slopes,\n            start=args.decay_start,\n        )\n\n        self.mask = None\n\n        # initialize weights by trunc_normal(1/sqrt(fan_in))\n        std = 1.0 / math.sqrt(args.dim)\n        torch.nn.init.trunc_normal_(self.in_proj.weight, std=std, a=-3 * std, b=3 * std)\n\n        # scale init by depth as in https://arxiv.org/abs/1908.11365 -- worked slightly better.\n        std = std / math.sqrt(2 * (layer_id + 1))\n        torch.nn.init.trunc_normal_(self.out_proj.weight, std=std, a=-3 * std, b=3 * std)\n\n        norm_dim = self.n_heads * self.qk_head_dim\n\n        # initialize norm layers for queries and keys if needed\n        NormClass = args.norm_type\n        self._totrain_q_norm = (\n            NormClass(\n                norm_dim,\n                eps=args.norm_eps,\n            )\n            if self.apply_qk_norm\n            else nn.Identity()\n        )\n        self._totrain_k_norm = (\n            NormClass(\n                norm_dim,\n                eps=args.norm_eps,\n            )\n            if self.apply_qk_norm\n            else nn.Identity()\n        )\n        self.qk_scale = 1.0 / math.sqrt(self.qk_head_dim)\n\n    def repeat_kv(self, hidden_states, n_rep):\n        \"\"\"\n        This function repeats the key and value tensors to match the number of queries.\n        This is needed when the number of key-value heads is different from the number of query heads (GQA or MQA).\n        \"\"\"\n        if n_rep == 1:\n            return hidden_states\n        hidden_states2 = hidden_states.transpose(1, 2)\n        batch, num_key_value_heads, slen, head_dim = hidden_states2.shape\n        hidden_states2 = hidden_states2[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n        return hidden_states2.reshape(batch, num_key_value_heads * n_rep, slen, head_dim).transpose(1, 2)\n\n    def _set_mask(self, seqlen: int, device):\n        if self.mask is None or self.mask.shape[-1] < seqlen:\n            self.mask = torch.tril(torch.ones(1, 1, seqlen, seqlen, requires_grad=False), diagonal=0).to(device)\n\n    def _get_qkv(self, x: torch.Tensor, offset=0):\n        \"\"\"\n        This function computes the queries, keys, and values for the linear attention function.\n        It re-uses the projection layer from a usual transformer model and applies the kernels to the queries and keys (one layer + relu).\n        \"\"\"\n        batchsize, seqlen, _ = x.shape\n        queries, keys, vals = self.in_proj(x).split(\n            [\n                self.n_heads * self.qk_in_head_dim,\n                self.n_heads_kv * self.qk_in_head_dim,\n                self.n_heads_kv * self.v_head_dim,\n            ],\n            dim=-1,\n        )\n        vals = vals.view(batchsize, seqlen, self.n_heads_kv, self.v_head_dim)\n\n        queries = F.relu(self._totrain_embed(queries.view(batchsize, seqlen, self.n_heads * self.qk_in_head_dim))).view(\n            batchsize, seqlen, self.n_heads, self.qk_head_dim\n        )\n        if self.n_heads != self.n_heads_kv:\n            keys = F.relu(\n                self._totrain_embed_kv(keys.view(batchsize, seqlen, self.n_heads_kv * self.qk_in_head_dim))\n            ).view(batchsize, seqlen, self.n_heads_kv, self.qk_head_dim)\n        else:\n            keys = F.relu(\n                self._totrain_embed(keys.view(batchsize, seqlen, self.n_heads_kv * self.qk_in_head_dim))\n            ).view(batchsize, seqlen, self.n_heads_kv, self.qk_head_dim)\n\n        keys = self.repeat_kv(keys, n_rep=self.n_heads // self.n_heads_kv)\n        vals = self.repeat_kv(vals, n_rep=self.n_heads // self.n_heads_kv)\n\n        queries = self._totrain_q_norm(queries.view(batchsize, seqlen, self.n_heads * self.qk_head_dim)).view(\n            batchsize, seqlen, self.n_heads, self.qk_head_dim\n        )\n        keys = self._totrain_k_norm(keys.reshape(batchsize, seqlen, self.n_heads * self.qk_head_dim)).view(\n            batchsize, seqlen, self.n_heads, self.qk_head_dim\n        )\n\n        queries, keys, vals = self.pos_embed(\n            queries,\n            keys,\n            vals,\n            offset=offset,\n        )\n\n        queries = queries.transpose(1, 2).contiguous()\n        keys = keys.transpose(1, 2).contiguous()\n        vals = vals.transpose(1, 2).contiguous()\n        return queries, keys, vals\n\n    def _output(self, output: torch.Tensor):\n        \"\"\"\n        This function computes the output of the linear attention function.\n        It applies the group normalization and the output projection layer.\n        \"\"\"\n        output = output.transpose(1, 2).contiguous()\n        batchsize, seqlen = output.shape[:2]\n\n        output = self._totrain_gn(output.reshape(batchsize * seqlen, self.v_head_dim * self.n_heads))\n\n        output = output.view(batchsize, seqlen, self.v_head_dim * self.n_heads)\n\n        output = self.out_proj(output)\n\n        return output\n\n    def forward(\n        self, x: torch.Tensor, is_causal: bool = True, past_key_value=None, use_cache=False, attention_mask=None\n    ):\n        \"\"\"\n        Run the linear attention function either in parallel (use_cache=False) or recurrent mode (use_cache=True).\n        x: [B, T, D]\n        is_causal: bool must be True\n        past_key_value: None or tuple of (state, offset), this is a hack to repurpose the key_value cache for recurrent inference\n        use_cache: bool if set to true, run the model in recurrent mode, else run parallel mode\n        attention_mask: None,\n        \"\"\"\n        assert is_causal, \"LinearAttn class only supports causal mode\"\n        if attention_mask is not None and attention_mask.all():\n            attention_mask = None\n        increment = x.shape[1] if attention_mask is None else attention_mask.sum(dim=1)\n\n        if not use_cache:\n            output = self.forward_parallel(x, is_causal, attention_mask=attention_mask)\n        else:\n            if past_key_value is None:\n                past_key_value = (None, 0)\n            output, s = self.forward_recurrent(x, past_key_value[0], past_key_value[1])\n            past_key_value = (s, past_key_value[1] + increment)\n\n        return output, past_key_value\n\n    def forward_parallel(self, x: torch.Tensor, causal, attention_mask=None):\n        \"\"\"\n        Use the linear attention function to compute the output in parallel.\n        x: [B, T, D]\n        causal: bool must be True\n        attention_mask: None, not supported for linear attention in parallel mode\n        \"\"\"\n        assert attention_mask is None, \"Attention mask not supported for linear attention\"\n        queries, keys, vals = self._get_qkv(x)\n        output = self.linear_attn_fn(queries, keys, vals, self.qk_scale)\n        return self._output(output)\n\n    def forward_recurrent(\n        self,\n        x: torch.Tensor,\n        s: torch.Tensor = None,\n        offset=0,\n    ):\n        \"\"\"\n        Use the linear attention function to compute the output in recurrent mode.\n        Loops over the sequence length and computes the output and the state update at each step.\n        x: [B, sequence_length, D] input features\n        s: [B, head, h_dim, h_dim] (optional) input recurrent state\n        offset: int or [B,] (optional) sequence offset for positional embedding, encodes the current position of x in the sequence.\n        \"\"\"\n        if s is None:\n            s = torch.zeros(\n                x.shape[0],\n                self.n_heads,\n                self.qk_head_dim,\n                self.v_head_dim,\n                device=x.device,\n                dtype=x.dtype,\n            )\n        queries, keys, vals = self._get_qkv(x, offset)\n\n        out = []\n        for i in range(x.shape[1]):\n            output, s = self.recurrent_forward_fn(\n                queries[:, :, i : i + 1], keys[:, :, i : i + 1], vals[:, :, i : i + 1], s, qk_scale=self.qk_scale\n            )\n            out.append(output)\n\n        output = torch.cat(out, dim=2)\n        return self._output(output), s\n\n",
    "description": null,
    "url": null
}