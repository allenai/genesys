{
    "acronym": "stm",
    "title": "Efficient Long-Text Understanding with Short-Text Models",
    "seed_ids": [
        "dssm",
        "rfa",
        "longformer",
        "reformer",
        "transformer"
    ],
    "s2id": "732e3faec4e5be4d144256f2c379b9dc49f0b227",
    "abstract": "Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.",
    "authors": [
        "Maor Ivgi",
        "Uri Shaham",
        "Jonathan Berant"
    ],
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": "This work proposes SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs and finds that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.",
    "citationCount": 49,
    "influentialCitationCount": 5,
    "code": "import abc\nimport inspect\nimport warnings\n\nimport torch\nfrom makefun import create_function\nfrom torch import nn\nfrom transformers import PreTrainedModel, AutoModel, WEIGHTS_NAME\nfrom transformers.utils import logging\n\nfrom .modules import MockTensorForConditionalGeneration\nfrom .utils import _extract_keyword_args, _fix_args, _fix_kwargs, _stack_args, _stack_kwargs\n\nfrom .configuration_sled import SledConfig\n\nlogger = logging.get_logger(__name__)\n\n\nPREFIX_KEY = 'prefix_length'\n\n\ndef _unstack_encoder_outputs(stacked_output, n, bs):\n    if isinstance(stacked_output, tuple):\n        return [tuple(v if not isinstance(v, torch.Tensor) else v[i * bs:(i + 1) * bs] for v in stacked_output)\n                for i in range(n)]\n    # works for dict as well as structured outputs\n    return [type(stacked_output)(**{k: v if not isinstance(v, torch.Tensor) else v[i*bs:(i+1)*bs]\n                                    for k, v in stacked_output.items()})\n            for i in range(n)]\n\n\ndef _slice_tensor(v, start, end, prefix_length=None):\n    prefix_length = prefix_length or 0\n    return v[:, start+prefix_length:end+prefix_length]\n\n\ndef _merge_encoder_outputs(encoder_outputs_list):\n    # a list of 4-tuples, first value is the returned value from the encoder, then the start and end indices inside the\n    # tensors that we should take, and finally prefix_length (None if was not used)\n\n    # presumed order of returned tuple from encoders: last_hidden_state, hidden_states, attentions\n\n    # the first output, as returned by the underlying model on the first window\n    resulting_output = encoder_outputs_list[0][0]\n    if isinstance(resulting_output, tuple):  # not in return dict mode:\n        resulting_list = []\n        for i in range(len(resulting_output)):\n            if resulting_output[i] is None:\n                resulting_list.append(None)\n            elif (\n                    isinstance(resulting_output[i], (int, float, tuple, MockTensorForConditionalGeneration))\n                    or resulting_output[i].dim() != 3\n            ):\n                continue\n            else:\n                assert isinstance(resulting_output[i], torch.Tensor)\n                # tensors are of of size (N, w, d), N the batch size, w the current window size and d the hidden\n                # state size/logits dimension these are the only parts in the encoder output that we need to merge\n                # between windows\n                resulting_list.append(\n                    torch.cat(tuple(_slice_tensor(out[i], start, end, prefix_length)\n                                    for out, start, end, prefix_length in encoder_outputs_list), dim=1)\n                )  # requires extra GPU memory because it doesn't dump the old copy of the tensors yet\n        resulting_output = tuple(resulting_list)\n    else:\n        for key in resulting_output.keys():\n            if resulting_output[key] is None:\n                continue\n            if isinstance(resulting_output[key], tuple):\n                resulting_output[key] = None  # encoder outputs are not tuples, only the decoders\n            else:\n                assert isinstance(resulting_output[key], torch.Tensor)\n                if resulting_output[key].dim() != 3:\n                    continue  # decoder outputs may be 4d tensors\n                # tensors are of of size (N, w, d), N the batch size, w the current window size and d the hidden\n                # state size/logits dimension\n                resulting_output[key] = torch.cat(\n                    tuple(_slice_tensor(out[key], start, end, prefix_length)\n                          for out, start, end, prefix_length in encoder_outputs_list), dim=1\n                )\n\n    return resulting_output\n\n\nclass MockDecoder(nn.Module):\n    def forward(self, *_, **__):\n        return tuple()\n\n    def to(self, *_, **__):\n        return self\n\n\nclass SledPretrainedModel(PreTrainedModel, metaclass=abc.ABCMeta):\n    config_class = SledConfig\n    auto_model_loader = AutoModel\n    IGNORE_CONFIG_KEYS = {'model_type', '_name_or_path'}  # config keys we allow to be mismatched between the\n    # SledConfig and the underlying model's config\n\n    def __init__(self, underlying_model: PreTrainedModel, config: SledConfig):\n        \"\"\"\n\n        :param underlying_model: The backbone model to use.\n                                Warning - once given, it should not be used directly, as it may cause unexpected behaviours\n        :param config:\n        \"\"\"\n        super(SledPretrainedModel, self).__init__(config)\n        self._underlying_model = (\n            underlying_model  # crucial this will be before any calls to members that is in the base model\n        )\n\n        self._window_fraction = config.window_fraction\n        self._context_size = config.context_size\n        self._window_margin = int(config.context_size * (1 - config.window_fraction) / 2)\n        self._sliding_method = config.sliding_method or 'dynamic'\n        assert self._sliding_method in {'loop', 'stacked', 'dynamic', 'decoderonly'}\n\n        for override_member in ['is_parallelizable', 'supports_gradient_checkpointing']:\n            setattr(self, override_member, getattr(underlying_model, override_member))\n\n        # setting the base_model_prefix to return the correct underlying model and link to some methods\n        # implemented in the base\n        self.base_model_prefix = \"sled_base_model_prefix\"\n        self.sled_base_model_prefix = self._underlying_model.base_model\n\n        # override generation preparation functions that may be overridden by underlying models but will be\n        # found in our wrapper. We wished we could do it a follows:\n        # for method_name, _ in inspect.getmembers(PreTrainedModel, predicate=inspect.isfunction):\n        #     if method_name not in {\"_replicate_for_data_parallel\", 'modules'}:\n        #         setattr(self, method_name, getattr(underlying_model, method_name))\n        # However, the above is too broad and dangerous, so we will do it directly\n        for method_name in {\"_init_weights\", \"prepare_inputs_for_generation\"}:\n            if hasattr(underlying_model, method_name):\n                setattr(self, method_name, getattr(underlying_model, method_name))\n\n        # set the resize_token_embeddings\n        vocab_size = underlying_model.get_input_embeddings().weight.size(0)\n        assert hasattr(self.config, 'vocab_size'), 'Underlying models must have a vocab_size config'\n        assert underlying_model.config.vocab_size == vocab_size\n        self.resize_token_embeddings(vocab_size)  # the underlying model may have a different vocab size compared to its base config\n\n        self._verified_config_consistency = False\n        self._verify_config_consistency()\n        self._verified_config_consistency = False  # We would like to do it later again (before the first forward)\n\n        self._prepend_prefix = config.prepend_prefix\n        self._encode_prefix = config.encode_prefix\n\n        # now, let's create the forward function\n        self._create_forward_function()\n\n    @property\n    def underlying_model(self):\n        return self._underlying_model\n\n    def resize_token_embeddings(self, new_num_tokens=None):\n        res = self.underlying_model.resize_token_embeddings(new_num_tokens)\n        self.config.vocab_size = self.underlying_model.vocab_size  # sync them\n        return res\n\n    @property\n    def _ignore_keys(self):\n        return self.IGNORE_CONFIG_KEYS\n\n    def _replicate_for_data_parallel(self):\n        replica = super()._replicate_for_data_parallel()\n        replica.forward = create_function(self._signature, replica._forward)\n        return replica\n\n    def _create_forward_function(self):\n        # https://stackoverflow.com/a/15638720\n        self._underlying_model_signature = inspect.signature(self._underlying_model.forward)\n        self._forward_kwargs_names = [param.name for param in self._underlying_model_signature.parameters.values()]\n        assert PREFIX_KEY not in self._forward_kwargs_names\n        # if we want to prepend questions in every window, we need to set the forward signature to expect the\n        # input_prefix (e.g. question) as a separate input sequence\n\n        # we want to remove any typing information as it may cause issues in the custom function build do to\n        # non-imported modules. It is ugly and shouldn't be done like that, but it works..\n        params = [self._underlying_model_signature.parameters[p].replace(annotation=inspect.Parameter.empty)\n                  for p in self._underlying_model_signature.parameters]\n        params.append(inspect.Parameter(name=PREFIX_KEY, default=None, kind=params[-1].kind))\n        self._signature = str(self._underlying_model_signature.replace(parameters=params,\n                                                                       return_annotation=inspect.Signature.empty))\n\n        # HF trainer uses the signature to choose which parts to take from a dataset, so we need to make sure our\n        # wrapped forward function has the correct signature (dynamically creating it here)\n        self.forward = create_function(self._signature, self._forward)\n\n    def __getattr__(self, item):\n        try:\n            return super().__getattr__(item)\n        except AttributeError:\n            try:\n                return self._underlying_model.__getattribute__(item)\n            except AttributeError:\n                return self._underlying_model.__getattr__(item)\n\n    @abc.abstractmethod\n    def _forward(self, *args, **kwargs):\n        # the actual forward implementation of the model.\n        raise NotImplementedError\n\n    def _set_underlying_model_attr(self, attr_name, new_val):\n        if hasattr(self._underlying_model, attr_name):\n            setattr(self._underlying_model, attr_name, new_val)\n        elif hasattr(self._underlying_model, \"model\") and hasattr(self._underlying_model.model, attr_name):\n            setattr(self._underlying_model.model, attr_name, new_val)\n        else:\n            raise ValueError(f\"Cannot use this model as we cannot set its {attr_name}\")\n\n    def _run_sliding_window_forward(self, args_tensor_inds, kwargs_tensor_keys, s, *args,\n                                    prefix_length=None, **kwargs):\n        sm = self._sliding_method if self._sliding_method != 'dynamic' else \\\n            ('loop' if not self.training else 'stacked')\n        try:\n            if sm == 'decoderonly':\n                return self._skip_forward_for_decoder_only(args_tensor_inds, kwargs_tensor_keys, s, *args,\n                                                           prefix_length=prefix_length, **kwargs)\n            if sm == 'loop':\n                return self._run_sliding_window_forward_loop(args_tensor_inds, kwargs_tensor_keys, s, *args,\n                                        prefix_length=prefix_length, **kwargs)\n            return self._run_sliding_window_forward_stacked(args_tensor_inds, kwargs_tensor_keys, s, *args,\n                                                         prefix_length=prefix_length, **kwargs)\n        finally:\n            # so that if the model crashes halfway through it will be restored to working order\n            pass\n\n    def _skip_forward_for_decoder_only(self, args_tensor_inds, kwargs_tensor_keys, s, *args,\n                                       prefix_length=None, **kwargs):\n        # NOTE - this will work probably only with BART.\n        embeder = self if hasattr(self, 'embed_tokens') else self.get_encoder() # account for sled encoder\n        return (embeder.embed_tokens(kwargs['input_ids']), )\n\n\n    def _run_sliding_window_forward_loop(self, args_tensor_inds, kwargs_tensor_keys, s, *args,\n                                    prefix_length=None, **kwargs):\n        forward_kwargs = _extract_keyword_args(kwargs, self._forward_kwargs_names, None)\n        encoder_outputs_list = []\n        if prefix_length is not None and self._prepend_prefix:\n            # we were given prefixes in the input, and we are expected to treat them\n            prefix_length, s = self._handle_prefix(prefix_length, s)\n\n            if self._encode_prefix:\n                # encode the question as well, if needed\n                context_start_ind, context_end_ind, update_start_ind, update_end_ind = 0, prefix_length, 0, prefix_length\n\n                encoder_outputs = self._underlying_model.forward(\n                    *_fix_args(args, args_tensor_inds, context_start_ind, context_end_ind, None),\n                    **_fix_kwargs(forward_kwargs, kwargs_tensor_keys, context_start_ind, context_end_ind, None),\n                )\n                encoder_outputs_list.append((encoder_outputs, update_start_ind, update_end_ind, None))\n                # we will need to make sure all input tensors will also drop everything with the prefix\n        else:\n            prefix_length = None  # we need to ignore the prefix and treat the entire input as one long document\n\n        for context_start_ind, context_end_ind, update_start_ind, update_end_ind in self._window_indices(s):\n            encoder_outputs = self._underlying_model.forward(\n                *_fix_args(args, args_tensor_inds, context_start_ind, context_end_ind, prefix_length),\n                **_fix_kwargs(forward_kwargs, kwargs_tensor_keys, context_start_ind, context_end_ind, prefix_length),\n            )\n            encoder_outputs_list.append((encoder_outputs, update_start_ind, update_end_ind, prefix_length))\n\n        return _merge_encoder_outputs(encoder_outputs_list)\n\n    def _handle_prefix(self, prefix_length, s):\n        prefix_length_ = prefix_length[0].detach().cpu().item()\n        assert torch.all(prefix_length == prefix_length_).item(), \\\n            'Using different length prefixes in the same batch is not supported. Either group your batch by ' \\\n            'prefix length, or pad the prefixes to match in length (and do not forget to set the attention ' \\\n            'mask to 0 where appropriate)'\n        if hasattr(self.underlying_model.config, 'max_position_embeddings'):\n            assert self._context_size + prefix_length_ <= self.underlying_model.config.max_position_embeddings, \\\n                f'The prefix length + SLEDs chunk size must be at most the max length that the backbone model can handle'\n        return prefix_length_, s-prefix_length_\n\n    def _run_sliding_window_forward_stacked(self, args_tensor_inds, kwargs_tensor_keys, s, *args,\n                                    prefix_length=None, **kwargs):\n        forward_kwargs = _extract_keyword_args(kwargs, self._forward_kwargs_names, None)\n        stacks_args = []\n        stacks_kwargs = []\n        stacks_info = []\n\n        if prefix_length is not None and self._prepend_prefix:\n            # we were given prefixes in the input, and we are expected to treat them\n            prefix_length, s = self._handle_prefix(prefix_length, s)\n\n            if self._encode_prefix:\n                # encode the question as well, if needed\n                context_start_ind, context_end_ind, update_start_ind, update_end_ind = 0, prefix_length, 0, prefix_length\n                # need to pad it to match the seq len of the rest\n                # we may have too short samples as well so don't want to pad too much\n                pad = min(s, self._context_size)\n                assert pad >= 0, f'We have a weird situation. pad={pad}, s={s}, ' \\\n                                 f'prefix_length={prefix_length} and self._context_size={self._context_size}'\n                stacks_args.append(_fix_args(args, args_tensor_inds, context_start_ind, context_end_ind, None, pad))\n                stacks_kwargs.append(_fix_kwargs(forward_kwargs, kwargs_tensor_keys, context_start_ind,\n                                                 context_end_ind, None, pad))\n                stacks_info.append([None, update_start_ind, update_end_ind, None])\n        else:\n            prefix_length = None  # we need to ignore the prefix and treat the entire input as one long document\n\n        for context_start_ind, context_end_ind, update_start_ind, update_end_ind in self._window_indices(s):\n            stacks_args.append(_fix_args(args, args_tensor_inds, context_start_ind, context_end_ind, prefix_length))\n            stacks_kwargs.append(_fix_kwargs(forward_kwargs, kwargs_tensor_keys, context_start_ind,\n                                             context_end_ind, prefix_length))\n            stacks_info.append([None, update_start_ind, update_end_ind, prefix_length])\n\n        encoder_outputs2 = self._underlying_model.forward(\n            *_stack_args(stacks_args, args_tensor_inds),\n            **_stack_kwargs(stacks_kwargs, kwargs_tensor_keys))\n        bs = forward_kwargs[kwargs_tensor_keys[0]].size()[0] if len(kwargs_tensor_keys) > 0 else \\\n            args[args_tensor_inds[0]].size()[0]\n        for si, eo in zip(stacks_info, _unstack_encoder_outputs(encoder_outputs2, len(stacks_info), bs)):\n            si[0] = eo\n        res = _merge_encoder_outputs(stacks_info)\n\n        return res\n\n    def _window_indices(self, total_seq_len):\n        \"\"\"\n        when total_seq_len is smaller than our desired context length, we do not do sliding window at all.\n        However, if it is longer, then we ALWAYS require the context length to be maximal, even if some windows have\n        a lot of overlap.\n        Also, first window will always update from the start, and last window will always update until the end.\n        when applied, returns a generator that in each iteration produces for numbers:\n        context_start_ind, context_end_ind, update_start_ind, update_end_ind\n\n        context_start_ind, context_end_ind are indices in [0, total_seq_len],\n        where context_end_ind > context_start_ind and when\n        total_seq_len <= context_length then always context_end_ind = context_start_ind+context_length.\n        The sequence of context_start_ind is strictly monotonic and same for context_end_ind.\n        context_start_ind always start in 0 and\n        context_end_ind will always end in total_seq_len.\n        Gives us what token indices to take from the long input.\n\n        update_start_ind, update_end_ind are indices in [0, min(total_seq_len, context_length)],\n        where update_end_ind > update_start_ind\n        and for all windows that are not in the edges (i.e. first/last window) we have\n        update_end_ind-update_start_ind=context_length*window_fraction.\n        For first window update_start_ind is always 0, and for last window,\n        update_end_ind is always min(total_seq_len, context_length).\n        They represents the start and end indices from the selected window of\n        which tokens should be taken out for the final encoding\n\n        When doing a full itartion, accounting for the fact that\n        update_start_ind, update_end_ind are shifted by context_start_ind, we hould get that all indices in\n        [0, total_seq_len] were covered exactly once\n\n        Examples\n        >>> from transformers import T5Tokenizer, T5Model\n        >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n        >>> model_ = T5Model.from_pretrained('t5-small')\n        >>> model = SledModel(model_, 512)  # testing with padding of 50% and context of 512\n\n        >>> list(model._window_indices(256))  # List of: (context_start, context_end, update_start, update_end). short sequence\n        [(0, 256, 0, 256)]\n        >>> list(model._window_indices(510))  # another short sequence\n        [(0, 510, 0, 510)]\n        >>> list(model._window_indices(512))  # sequence of exactly the context size\n        [(0, 512, 0, 512)]\n        >>> list(model._window_indices(514))  # sequence of slightly more than the context size\n        [(0, 512, 0, 384), (2, 514, 382, 512)]\n        >>> list(model._window_indices(766))  # long sequence that does not require a full stride (update in the last chunk is smaller than what is possible)\n        [(0, 512, 0, 384), (254, 766, 130, 512)]\n        >>> list(model._window_indices(768))  # long sequence for exactly two perfect chunks\n        [(0, 512, 0, 384), (256, 768, 128, 512)]\n        >>> list(model._window_indices(780))  # very long sequence that does not require a full stride (update in the last chunk is smaller than what is possible)\n        [(0, 512, 0, 384), (256, 768, 128, 384), (268, 780, 372, 512)]\n        >>> windows = list(model._window_indices(1050))\n        >>> windows\n        [(0, 512, 0, 384), (256, 768, 128, 384), (512, 1024, 128, 384), (538, 1050, 358, 512)]\n        >>> windows = sum([list(range(us+cs, ue+cs)) for cs, _, us, ue in windows], [])  # verify it covers exactly all the indices, each once\n        >>> windows[:10]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        >>> windows[500:510]\n        [500, 501, 502, 503, 504, 505, 506, 507, 508, 509]\n        >>> len(windows)\n        1050\n        >>> len(set(windows))\n        1050\n        >>> model = SledModel(model_, 256, window_fraction=0.75)  # now testing with padding of 25% and context of 256\n\n        >>> list(model._window_indices(128))  # List of: (context_start, context_end, update_start, update_end). short sequence\n        [(0, 128, 0, 128)]\n        >>> list(model._window_indices(254))  # another short sequence\n        [(0, 254, 0, 254)]\n        >>> list(model._window_indices(256))  # sequence of exactly the context size\n        [(0, 256, 0, 256)]\n        >>> list(model._window_indices(258))  # sequence of slightly more than the context size. margin is 256/8 -> 32\n        [(0, 256, 0, 224), (2, 258, 222, 256)]\n        >>> list(model._window_indices(446))  # long sequence that does not require a full stride (update in the last chunk is smaller than what is possible). stride should be 256-64=192\n        [(0, 256, 0, 224), (190, 446, 34, 256)]\n        >>> list(model._window_indices(448))  # long sequence for exactly two perfect chunks\n        [(0, 256, 0, 224), (192, 448, 32, 256)]\n        >>> list(model._window_indices(500))  # very long sequence that does not require a full stride (update in the last chunk is smaller than what is possible)\n        [(0, 256, 0, 224), (192, 448, 32, 224), (244, 500, 172, 256)]\n        >>> windows = list(model._window_indices(1050))\n        >>> windows\n        [(0, 256, 0, 224), (192, 448, 32, 224), (384, 640, 32, 224), (576, 832, 32, 224), (768, 1024, 32, 224), (794, 1050, 198, 256)]\n        >>> windows = sum([list(range(us+cs, ue+cs)) for cs, _, us, ue in windows], [])  # verify it covers exactly all the indices, each once\n        >>> windows[:10]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        >>> windows[500:510]\n        [500, 501, 502, 503, 504, 505, 506, 507, 508, 509]\n        >>> len(windows)\n        1050\n        >>> len(set(windows))\n        1050\n        \"\"\"\n        if total_seq_len <= self._context_size:\n            yield 0, total_seq_len, 0, total_seq_len\n        else:\n            stride = self._context_size - 2 * self._window_margin\n            context_start = update_start_ind = 0\n            context_end = self._context_size\n            update_end_ind = context_end - self._window_margin\n            yield context_start, context_end, update_start_ind, update_end_ind  # first window always should update from the beginning\n            while context_end < total_seq_len:\n                context_end = min(total_seq_len, context_end + stride)\n                context_start = (\n                    context_start + stride if context_end < total_seq_len else total_seq_len - self._context_size\n                )\n                update_start_ind = max(update_start_ind + stride, update_end_ind)\n                # last window always should update until the end\n                update_end_ind = (\n                    min(total_seq_len, update_end_ind + stride) if context_end < total_seq_len else total_seq_len\n                )\n\n                cs, ce, us, ue = context_start, context_end, update_start_ind - context_start, \\\n                                 update_end_ind - context_start\n\n                yield cs, ce, us, ue\n\n    def _fill_prefix_inputs(self, kwargs, kwargs_tensor_keys):\n        prefix_inputs = {}\n        k = PREFIX_KEY\n        if PREFIX_KEY in kwargs:\n            if self._prepend_prefix:\n                if k not in kwargs_tensor_keys:\n                    warnings.warn(f'{k} is missing from kwargs_tensor_keys (though expected for SLED prefix prepending)')\n                else:\n                    kwargs_tensor_keys.remove(k)\n                    prefix_inputs[k] = kwargs.pop(k)\n            elif k in kwargs_tensor_keys:\n                warnings.warn(f'{k} is given in kwargs_tensor_keys even though sled should not prepend prefix, '\n                              f'that would mean the prefix would be ignored and the entire input will be treated '\n                              f'as a single long document, which is probably not what you meant')\n        return prefix_inputs\n\n    @staticmethod\n    def _prep_attention_mask_for_cross_attention(encode_prefix, attention_mask, prefix_length=None):\n        # if we need to drop the prefix encodings, we also need to adjust the attention mask before decoding\n        if not encode_prefix and prefix_length is not None:\n            prefix_length = int(prefix_length[0])\n            return attention_mask[..., prefix_length:]\n        return attention_mask\n\n\nclass SledModel(SledPretrainedModel):\n    \"\"\"\n    >>> from transformers import T5Tokenizer, T5Model, BartModel, BartTokenizer\n\n    >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n    >>> model_ = T5Model.from_pretrained('t5-small')\n    >>> model = SledModel(model_, 4)\n\n    >>> input_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\n    >>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n    >>> outputs = model(input_ids=input_ids.clone(), decoder_input_ids=decoder_input_ids.clone())\n    >>> outputs = model(input_ids=input_ids.clone(), decoder_input_ids=decoder_input_ids.clone(), return_dict=True)\n    >>> outputs = model(input_ids=input_ids.clone(), decoder_input_ids=decoder_input_ids.clone(), return_dict=False)\n    \"\"\"\n\n    def __init__(self, underlying_model: PreTrainedModel, config: SledConfig):\n        super(SledModel, self).__init__(underlying_model, config)\n        # validate the model can be used\n        self._decoder_attr_name = getattr(underlying_model, \"get_decoder_attr_name\", lambda: \"decoder\")()\n        self._encoder_attr_name = getattr(underlying_model, \"get_encoder_attr_name\", lambda: \"encoder\")()\n        self._set_underlying_model_attr(self._decoder_attr_name, self.get_decoder())\n        self._mock_decoder = MockDecoder()\n        assert \"return_dict\" in self._forward_kwargs_names\n        assert \"encoder_outputs\" in self._forward_kwargs_names\n\n    def _forward(self, *args, **kwargs):\n        self._verify_config_consistency()\n        kwargs, args = _fill_kwargs_with_args(self._forward_kwargs_names, *args, **kwargs)\n        kwargs.setdefault(\"encoder_outputs\", None)\n        return_dict = kwargs.setdefault(\"return_dict\", None)\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        labels = kwargs.get(\"labels\", None)\n        kwargs[\"labels\"] = None\n        kwargs[\"return_dict\"] = False\n        kwargs.setdefault(\"labels\", None)\n        args_tensor_inds, kwargs_tensor_keys, s = _find_tensor_inds_and_size(*args, **kwargs)\n        prefix_inputs = self._fill_prefix_inputs(kwargs, kwargs_tensor_keys)\n\n        forward_kwargs = _extract_keyword_args(kwargs, self._forward_kwargs_names, None)\n        if forward_kwargs[\"encoder_outputs\"] is None:\n            # encode, but first let's set decoder to be a mock, no reason to apply it over partial windows\n            self._prep_for_encoding()  # todo - add try catch every time we 'prep' something to rever the state on fail?\n\n            forward_kwargs[\"encoder_outputs\"] = self._run_sliding_window_forward(\n                args_tensor_inds, kwargs_tensor_keys, s, *args, **prefix_inputs, **forward_kwargs\n            )\n            forward_kwargs['attention_mask'] = self._prep_attention_mask_for_cross_attention(self._encode_prefix,\n                forward_kwargs['attention_mask'], prefix_inputs.get('prefix_length', None))\n\n        # now, let's decode\n        forward_kwargs[\"return_dict\"] = return_dict\n        forward_kwargs[\"labels\"] = labels\n        self._fix_post_encoding()\n        if 'decoder_input_ids' in self._forward_kwargs_names and \\\n            forward_kwargs.get('decoder_input_ids', None) is None and \\\n                hasattr(self, 'prepare_decoder_input_ids_from_labels') :\n            logger.warning('Passing a batch through the model without the decoder_input_ids is likely to cause issues. '\n                           'If you encounter cuda errors, make sure you use the prepare_decoder_input_ids_from_labels '\n                           'function of the model correctly before passing the input. '\n                           'If you are only performing prediction without training, you can safely ignore this message')\n        res = self._underlying_model.forward(\n            *args, **_extract_keyword_args(forward_kwargs, self._forward_kwargs_names)\n        )\n\n        return res\n\n    def _prep_for_encoding(self):\n        if not getattr(self, '_preped_for_encoding', False):\n            self._preped_for_encoding = True\n            self._decoder = self.get_decoder()\n            self._mock_decoder.first_device = getattr(self._decoder, \"first_device\", None)\n            self._set_underlying_model_attr(self._decoder_attr_name, self._mock_decoder)\n\n    def _fix_post_encoding(self):\n        assert self._preped_for_encoding\n        self._preped_for_encoding = False\n        self._set_underlying_model_attr(self._decoder_attr_name, self._decoder)\n",
    "description": null,
    "url": null
}