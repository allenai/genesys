{
    "acronym": "lstransformer",
    "title": "Long-Short Transformer: Efficient Transformers for Language and Vision",
    "seed_ids": [
        "rfa",
        "nystromformer",
        "performer",
        "bigbird",
        "lineartransformer",
        "linformer",
        "etc",
        "longformer",
        "sinkhorn",
        "reformer",
        "blockbert",
        "axialattn",
        "sparsetransformer",
        "transformerxl"
    ],
    "s2id": "1a883522f3c0051d70be1f8cbdb8989a77395006",
    "abstract": "Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .",
    "authors": [
        "Chen Zhu",
        "Wei Ping",
        "Chaowei Xiao",
        "M. Shoeybi",
        "T. Goldstein",
        "Anima Anandkumar",
        "Bryan Catanzaro"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": "This paper proposes Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks, and proposes a dual normalization strategy to account for the scale mismatch between the two attention mechanisms.",
    "citationCount": 106,
    "influentialCitationCount": 13,
    "code": "from math import gcd, ceil\nimport functools\n\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n\nfrom einops import rearrange, repeat\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef lcm(*numbers):\n    return int(functools.reduce(lambda x, y: int((x * y) / gcd(x, y)), numbers, 1))\n\ndef pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n    seqlen = tensor.shape[dim]\n    m = seqlen / multiple\n\n    if m.is_integer():\n        return tensor\n\n    remainder = ceil(m) * multiple - seqlen\n    pad_offset = (0,) * (-1 - dim) * 2\n    return F.pad(tensor, (*pad_offset, 0, remainder), value=value)\n\ndef look_around(x, backward = 1, forward = 0, pad_value = -1, dim = 2):\n    t = x.shape[1]\n    dims = (len(x.shape) - dim) * (0, 0)\n    padded_x = F.pad(x, (*dims, backward, forward), value= pad_value)\n    tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)]\n    return torch.cat(tensors, dim=dim)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * mult, dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass LongShortAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        heads = 8,\n        dim_head = 64,\n        causal = True,\n        window_size = 128,\n        pos_emb = None,\n        segment_size = 16,\n        r = 1,\n        dropout = 0.\n    ):\n        super().__init__()\n        assert not (causal and r >= segment_size), 'r should be less than segment size, if autoregressive'\n\n        inner_dim = heads * dim_head\n        self.scale = dim_head ** -0.5\n\n        self.heads = heads\n        self.causal = causal\n\n        self.window_size = window_size\n        self.segment_size = segment_size\n        self.pad_to_multiple = window_size if not causal else lcm(window_size, segment_size)\n\n        self.to_dynamic_proj = nn.Linear(dim_head, r, bias = False)\n        self.local_norm = nn.LayerNorm(dim_head)\n        self.global_norm = nn.LayerNorm(dim_head)\n\n        self.pos_emb = default(pos_emb, RotaryEmbedding(dim_head))\n\n        self.attn_dropout = nn.Dropout(dropout)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n    def forward(self, x, mask = None):\n        b, n, *_, h, device, causal, w, s = *x.shape, self.heads, x.device, self.causal, self.window_size, self.segment_size\n\n        # pad input sequence to multiples of window size (or window size and segment length if causal)\n\n        x = pad_to_multiple(x, self.pad_to_multiple, dim = -2, value = 0.)\n\n        # derive from variables\n\n        padded_len = x.shape[-2]\n        windows = padded_len // w\n        is_padded = padded_len != n\n\n        mask_value = -torch.finfo(x.dtype).max\n\n        # handle mask if padding was needed and mask was not given\n\n        if is_padded:\n            mask = default(mask, torch.ones((b, n), device = device).bool())\n            mask = pad_to_multiple(mask, w, dim = -1, value = False)\n\n        # get queries, keys, values\n\n        qkv = (self.to_q(x), self.to_kv(x))\n\n        # get sequence range, for calculating mask\n\n        seq_range = torch.arange(padded_len, device = device)\n\n        # split heads\n\n        q, kv = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), qkv)\n\n        # rotary embedding\n\n        if exists(self.pos_emb):\n            rotary_emb = self.pos_emb(seq_range, cache_key = padded_len)\n            rotary_emb = rearrange(rotary_emb, 'n d -> () n d')\n            q, kv = map(lambda t: apply_rotary_emb(rotary_emb, t), (q, kv))\n\n        # scale queries\n\n        q = q * self.scale\n\n        # get local queries and keys similarity scores\n\n        window_fn = lambda t: rearrange(t, 'b (w n) d -> b w n d', n = w)\n        lq, lkv = map(window_fn, (q, kv))\n\n        lookaround_kwargs = {'backward': 1, 'forward': (0 if causal else 1)}\n        lkv = look_around(lkv, **lookaround_kwargs)\n\n        lkv = self.local_norm(lkv)\n        lsim = einsum('b w i d, b w j d -> b w i j', lq, lkv)\n\n        # prepare global key / values\n\n        if self.causal:\n            # autoregressive global attention is handled in segments\n            # later on, these segments are carefully masked to prevent leakage\n\n            gkv = rearrange(kv, 'b (n s) d -> b n s d', s = s)\n            pkv = self.to_dynamic_proj(gkv)\n\n            if exists(mask):\n                pmask = repeat(mask, 'b (n s) -> (b h) n s', s = s, h = h)\n                pkv.masked_fill_(~pmask[..., None], mask_value)\n\n            pkv = pkv.softmax(dim = -2)\n\n            gkv = einsum('b n s d, b n s r -> b n r d', gkv, pkv)\n            gkv = rearrange(gkv, 'b n r d -> b (n r) d')\n        else:\n            # equation (3) in the paper\n\n            pkv = self.to_dynamic_proj(kv)\n\n            if exists(mask):\n                pkv.masked_fill_(~mask[..., None], mask_value)\n\n            pkv = pkv.softmax(dim = -2)\n\n            gkv = einsum('b n d, b n r -> b r d', kv, pkv)\n\n        # calculate global queries and keys similarity scores\n\n        gkv = self.global_norm(gkv)\n        gsim = einsum('b n d, b r d -> b n r', q, gkv)\n\n        # concat values together (same as keys)\n\n        gkv = repeat(gkv, 'b r d -> b w r d', w = windows)\n        v = torch.cat((gkv, lkv), dim = -2)\n\n        # masking\n\n        buckets, i, j = lsim.shape[-3:]\n\n        if exists(mask):\n            mask = repeat(mask, 'b (w n) -> (b h) w n', n = w, h = h)\n            mask = look_around(mask, pad_value = False, **lookaround_kwargs)\n            mask = rearrange(mask, 'b w n -> b w () n')\n            lsim.masked_fill_(~mask, mask_value)\n\n        # mask out padding\n\n        seq_range_windowed = rearrange(seq_range, '(w n) -> () w n', w = windows)\n        pad_mask = look_around(seq_range_windowed, pad_value = -1, **lookaround_kwargs) == -1\n        lsim.masked_fill_(pad_mask[:, :, None], mask_value)\n\n        # calculate causal masking for both global and local\n\n        if self.causal:\n            g_range = rearrange(seq_range, '(n s) -> n s', s = s)\n            g_range_max = g_range.amax(dim = -1)\n            g_mask = seq_range[:, None] >= g_range_max[None, :]\n            g_mask = rearrange(g_mask, 'i j -> () i j')\n            gsim.masked_fill_(~g_mask, mask_value)\n\n            causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n            causal_mask = repeat(causal_mask, 'i j -> () u i j', u = buckets)\n            lsim.masked_fill_(causal_mask, mask_value)\n\n        # concat local and global similarities together to ready for attention\n\n        gsim = rearrange(gsim, 'b (w n) r -> b w n r', w = windows)\n        sim = torch.cat((gsim, lsim), dim = -1)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n        attn = self.attn_dropout(attn)\n\n        # aggregate values (same as keys, since tied) and project out\n\n        out = einsum('b w i j, b w j d -> b w i d', attn, v)\n        out = rearrange(out, '(b h) w n d -> b (w n) (h d)', h = h)\n        out = out[:, :n]\n        return self.to_out(out)\n\n# main class\n\nclass LongShortTransformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        depth,\n        max_seq_len,\n        window_size = 128,\n        causal = True,\n        dim_head = 64,\n        heads = 8,\n        ff_mult = 4,\n        segment_size = None,\n        r = None,\n        ff_dropout = 0.,\n        attn_dropout = 0.\n    ):\n        super().__init__()\n        self.max_seq_len = max_seq_len\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        pos_emb = RotaryEmbedding(dim_head)\n\n        # handle autoregressive default variables differently\n        # specifically, segments are only used for autoregressive case\n        # r is the projected r << n in the non-autoregressive case, and the projected r per segment for the autoregressive case\n        # yea, it is confusing, i know\n\n        segment_size = default(segment_size, 16 if causal else None)\n        r = default(r, 1 if causal else 128)\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, LongShortAttention(dim = dim, heads = heads, dim_head = dim_head, window_size = window_size, causal = causal, pos_emb = pos_emb, segment_size = segment_size, r = r, dropout = attn_dropout)),\n                PreNorm(dim, FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout))\n            ]))\n\n        self.to_logits = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_tokens)\n        )\n\n    def forward(self, x, mask = None):\n        x = self.token_emb(x)\n\n        for attn, ff in self.layers:\n            x = attn(x, mask = mask) + x\n            x = ff(x) + x\n\n        return self.to_logits(x)",
    "description": null,
    "url": null
}