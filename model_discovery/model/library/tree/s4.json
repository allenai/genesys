{
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "acronym": "s4",
    "s2id": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
    "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": "The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.",
    "citationCount": 744,
    "influentialCitationCount": 119,
    "seed_ids": [
        "hippo",
        "lkconv",
        "sparsetransformer",
        "lighdynconv",
        "transformer"
    ],
    "code": "import torch\nimport torch.nn as nn\nfrom src.models.nn import LinearActivation, Activation, DropoutNd\nfrom einops import rearrange, repeat\nimport opt_einsum as oe\n\nimport math\nclass OurModule(nn.Module):\n    def __init__(self): super().__init__()\n\n    def register(self, name, tensor, trainable=False, lr=None, wd=None):\n        \"\"\"Utility method: register a tensor as a buffer or trainable parameter\"\"\"\n\n        if trainable:\n            self.register_parameter(name, nn.Parameter(tensor))\n        else:\n            self.register_buffer(name, tensor)\n\n        optim = {}\n        if trainable and lr is not None: optim[\"lr\"] = lr\n        if trainable and wd is not None: optim[\"weight_decay\"] = wd\n        if len(optim) > 0: setattr(getattr(self, name), \"_optim\", optim)\n\n#\n# This is intended to match np.convolve(x,w)[:len(w)]\n# That is, (u \\ast v)[k] = sum_{j} u[k-j]v[j]\n# Here y = (u \\ask v) on return.\n# We assume the inputs are:\n# u (B H L)\n# v (C H L)\n# and we want to produce y that is (B C H L)\n#\n\n\ndef fft_conv(u,v):\n    L   = u.shape[-1]\n    u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n    v_f = torch.fft.rfft(v, n=2*L) # (C H L)\n   \n    y_f = oe.contract('bhl,chl->bchl', u_f, v_f) \n    y   = torch.fft.irfft(y_f, n=2*L)[..., :L] # (B C H L)\n    return y\n\ndef normalize_param(a, method, norm_const=None):\n        if method == \"l1\":\n            if norm_const is not None:\n                return a/((1+norm_const)*torch.linalg.norm(a,ord=1,dim=2).unsqueeze(2))\n            return a/torch.linalg.norm(a,ord=1,dim=2).unsqueeze(2)\n        if method == \"l2\":\n            return a/torch.linalg.norm(a,ord=2,dim=2).unsqueeze(2)\n        if method == \"max\":\n            return 0.1*a/torch.max(a,dim=2)[0].unsqueeze(2)\n        if method == \"none\":\n            return a\n        raise ValueError(f\"{method} normalization not implemented\")\n\nclass SimpleS4(OurModule):\n    def __init__(self,\n            nHippos,\n            d_state=64,\n            channels=1, \n            use_initial=True, # Use the initial state?\n            zero_order_hold=False, # Use zero-order hold approximation\n            trap_rule=True,\n            dt_min=0.001,\n            dt_max=0.1,\n            lr=None, # Hook to set LR of SSM parameters differently\n            learn_a=True,\n            learn_theta=True,\n            learn_dt=False, # whether to learn separate dt for each hippo\n            theta_scale=False,\n            skip_connection=True,\n            repr='cont', # representation to use: ['cont','disc','comp'] \n            param_norm = 'none', # for normalizing parameters for stability\n            **kernel_args,): # Use the trapezoid rule\n        super().__init__()\n        # H is number of hippos\n        # D is the dimension (also shockingly n other places)\n        # B is the batch\n        # L is the length\n        self.h = nHippos\n        self.d = d_state // 2    \n        self.channels = channels\n        self.use_initial = use_initial\n        self.zero_order_hold = zero_order_hold\n        #\n        # Use the trapezoid rule correct or just do zero-order hold.\n        self.trap_rule = trap_rule\n        self.repr = repr\n        self.learn_dt = learn_dt\n        self.shift = 'shift' in self.repr\n        self.param_norm = param_norm\n\n        _fp    = (self.channels, self.h, self.d)\n        \n        # Chebyshev initialization\n        h_scale  = torch.exp(torch.arange(self.h)/self.h * math.log(dt_max/dt_min))\n        angles   = torch.arange(self.d)*torch.pi\n        t_scale  = h_scale if theta_scale else torch.ones(self.h)\n        theta    = oe.contract('c,h,d->chd', torch.ones(self.channels), t_scale, angles)\n        if self.repr == 'disc':\n            # discrete diagonal representation\n            a = torch.randn(*_fp).abs()\n            #a = 2*torch.rand(*_fp)-1 # init randomly from [-1,1]\n        else:\n            # default continuous diagonal representation\n            a = -repeat(h_scale, 'h -> c h d', c=self.channels, d=self.d)\n                                            \n        self.register(\"theta\", theta,learn_theta,lr=lr, wd=None)\n        self.register(\"a\", a, learn_a,lr=lr, wd=None)\n\n        if self.learn_dt:\n            log_dt = torch.rand(self.h) * (\n                math.log(dt_max) - math.log(dt_min)\n            ) + math.log(dt_min)\n            self.register(\"log_dt\", log_dt, True,lr=lr, wd=None)\n\n        # The other maps \n        if not skip_connection:\n            self.register(\"D\", torch.zeros((channels, self.h)), False)\n        else:\n            self.D = nn.Parameter(torch.randn(channels, self.h))\n        \n        if use_initial or 'comp' in self.repr:\n            if self.shift:\n                b = torch.zeros(*_fp)\n                b[:,:,0] = 1\n                self.register(\"b\", b, False)\n            else:\n                self.b = nn.Parameter(torch.randn(*_fp))\n            self.c = nn.Parameter(torch.randn(*_fp))\n            self.x0 = nn.Parameter(torch.randn(*_fp))\n        else:\n            # This is an optimization that we combine q = c * b\n            # It's as if we're setting x0 = 0.\n            self.q = nn.Parameter(torch.randn(*_fp))\n\n\n    def quadrature_method(self, u, horizon):\n        # The input is now Batch x Hippos x Length\n        l  = u.size(-1)\n\n        dt = 1/(l-1) # the step size\n        if self.learn_dt:\n            dt = torch.exp(self.log_dt).view(1,-1,1, 1)\n\n        # q and a are both C x H x D\n        # zk is of length l we want a C x H x L matrix\n        zk = dt*torch.arange(l, device=u.device).view(1,1,-1,1)\n\n        if self.repr == 'disc':\n            # discrete diagonal representation\n            a_ = (self.a).abs()\n            base_term = 2 * dt * torch.pow(a_.unsqueeze(2), zk) * torch.cos(self.theta.unsqueeze(2) * zk)\n        else:\n            # continuous diagonal representation\n            a_ = self.a #/torch.linalg.norm(self.a,ord=1,dim=2).unsqueeze(2)\n            a_ = -a_.abs()\n            # a_ = -self.a.abs()\n            base_term = 2*dt*torch.exp(a_.unsqueeze(2) * zk)*torch.cos(   self.theta.unsqueeze(2) * zk)\n\n        q  = self.b*self.c if self.use_initial else self.q\n        f  = (q.unsqueeze(2)*base_term).sum(-1)\n\n        y = fft_conv(u,f)\n        # Add in the skip connection with per-channel D matrix\n        y = y + oe.contract('bhl,ch->bchl', u, self.D)\n        # Add back the initial state\n        if self.use_initial:\n            y = y + (2*(self.c*self.x0).unsqueeze(2)*base_term).sum(-1)\n\n        return rearrange(y, 'b c h l-> b (c h) l'), None # flatten the channels.\n\n    def forward(self, u, horizon=None):\n        return self.quadrature_method(u, horizon)\n\n\n# Below here are standard wrapper classes to handle\n# (1) Non-linearity\n# (2) Integration with the Hippo Code base\nclass NonLinear(nn.Module):\n    def __init__(self, h, channels, \n                ln=False, # Extra normalization\n                transposed=True,\n                dropout=0.0, \n                postact=None, # activation after FF\n                activation='gelu', # activation in between SS and FF\n                initializer=None, # initializer on FF\n                weight_norm=False, # weight normalization on FF\n                ):\n            super().__init__()\n            dropout_fn = DropoutNd # nn.Dropout2d bugged in PyTorch 1.11\n            dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n            #norm = Normalization(h*channels, transposed=transposed) if ln else nn.Identity()\n\n            activation_fn = Activation(activation)\n\n            output_linear = LinearActivation(\n                h*channels,\n                h,\n                transposed=transposed, \n                initializer=initializer,\n                activation=postact,\n                activate=True,\n                weight_norm=weight_norm,\n            )\n            #self.f = nn.Sequential(activation_fn, dropout, norm, output_linear)\n            self.f = nn.Sequential(activation_fn, dropout, output_linear)\n    def forward(self,x):  # Always (B H L)\n        return self.f(x)\n\nclass SimpleS4Wrapper(nn.Module):\n    def __init__(\n            self,\n            d_model,\n            d_state=64,\n            channels=1,\n            bidirectional=False,\n            dropout=0.0,\n            transposed=True, # axis ordering (B, L, D) or (B, D, L)\n            ln=True, # IGNORED: Extra normalization\n            postact=None, # activation after FF\n            activation='gelu', # activation in between SS and FF\n            initializer=None, # initializer on FF\n            weight_norm=False, # weight normalization on FF\n            linear=False,\n            # SSM Kernel arguments\n            **kernel_args,\n        ):\n        super().__init__()\n        self.h = d_model\n        self.d = d_state\n        self.channels = channels\n        #self.shift = shift\n        #self.linear = linear\n        self.out_d = self.h\n        self.transposed = transposed\n        self.bidirectional = bidirectional\n        assert not bidirectional, f\"Bidirectional NYI\"\n        self.s4 = SimpleS4(nHippos=d_model, d_state=d_state, \n                            channels=channels, **kernel_args)\n        # the mapping\n        # We transpose if it's not in the forward.\n        nl          =  NonLinear(self.h, channels=self.channels, ln=ln, # Extra normalization\n                        dropout=dropout, postact=postact, activation=activation, transposed=True,\n                        initializer=initializer, weight_norm=weight_norm)\n        self.out = nn.Identity() if linear else nl\n\n    def forward(self, u, *w, state=None, horizon=None):\n        #  u: (B H L) if self.transposed else (B L H)\n        if not self.transposed: u = u.transpose(-1, -2)\n        # We only pass BHL, and it is as if transposed is True.\n        y, state = self.s4(u,horizon=horizon)\n        ret = self.out(y)\n        if not self.transposed: ret = ret.transpose(-1, -2)\n        return ret, state\n\n    @property\n    def d_state(self): return self.h * self.d \n\n    @property\n    def d_output(self): return self.out_d  "
}