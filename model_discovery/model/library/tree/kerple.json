{
    "acronym": "kerple",
    "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation",
    "seed_ids": [
        "alibi",
        "roformer",
        "performer",
        "transformer",
        "bert",
        "gpt2",
        "gpt"
    ],
    "s2id": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
    "abstract": "Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\\url{https://github.com/chijames/KERPLE.git}.",
    "authors": [
        "Ta-Chung Chi",
        "Ting-Han Fan",
        "P. Ramadge",
        "Alexander I. Rudnicky"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": "KERPLE is proposed, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences using conditionally positive definite (CPD) kernels, and it is shown that a CPD kernel can be transformed into a PD kernel by adding a constant offset.",
    "citationCount": 39,
    "influentialCitationCount": 7,
    "code": "# Copyright (c) 2021, EleutherAI contributors\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Parts of the code here are adapted from PyTorch\n# repo: https://github.com/pytorch/pytorch\n\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn.parameter import Parameter\n\nfrom .initialize import get_model_parallel_rank\nfrom .initialize import get_model_parallel_world_size\n\n\n\n\nclass ParallelKerpleLog(torch.nn.Module):\n    \"\"\"Kernelized T5 Relative Position Bias parallelized in the heads dimension\"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n    ):\n        super().__init__()\n        self.heads = neox_args.num_attention_heads\n        self.model_parallel_size = get_model_parallel_world_size()\n        self.model_parallel_rank = get_model_parallel_rank()\n        self.num_heads_per_partition = self.heads // self.model_parallel_size\n        self.pos_emb = neox_args.pos_emb\n        self.eps = 1e-2\n        \n        # megatron splits across heads, so we need to make sure each head receives the correct matrix\n        assert self.model_parallel_size <= self.heads and self.model_parallel_rank <= self.model_parallel_size\n        \n        # Allocate weights and initialize.\n        # The kernel has the form -p*log(1+a*|m-n|)\n        def get_parameter(scale, init_method):\n            if init_method == 'ones':\n                return Parameter(torch.ones(\n                               self.num_heads_per_partition,\n                               device=torch.cuda.current_device(),\n                               dtype=neox_args.params_dtype,\n                               )[:,None,None]*scale )\n            elif init_method == 'uniform':\n                return Parameter(torch.rand(\n                               self.num_heads_per_partition,\n                               device=torch.cuda.current_device(),\n                               dtype=neox_args.params_dtype,\n                               )[:,None,None]*scale )\n        \n        self.bias_p = get_parameter(2, 'uniform')\n        self.bias_a = get_parameter(1, 'uniform')\n\n        self.cached_matrix = None\n        self.cached_seq_len = None\n    \n    def stats(self):\n        def get_stats(name, obj):\n            return {name+'_mean': obj.mean().detach().cpu(),\n                    name+'_std': obj.std().detach().cpu(),\n                    name+'_max': obj.max().detach().cpu(),\n                    name+'_min': obj.min().detach().cpu()}\n        dd = {}\n        self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n        dd.update(get_stats('bias_a', self.bias_a))\n        self.bias_p.data = self.bias_p.data.clamp(min=self.eps)\n        dd.update(get_stats('bias_p', self.bias_p))\n        return dd\n    \n    def forward(self, x):\n        # [b, np, sq, sk]\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n        if self.cached_seq_len != seq_len_k:\n            diff = torch.tril(\n                torch.arange(seq_len_k, device=x.device).view(seq_len_k, 1).repeat(1, seq_len_k)\n                + torch.arange(0, -seq_len_k, -1, device=x.device)\n            )\n            diff = diff.to(x.dtype)\n            self.cached_seq_len = seq_len_k\n            self.cached_matrix = diff\n        else:\n            diff = self.cached_matrix\n        \n        self.bias_p.data = self.bias_p.data.clamp(min=self.eps)\n        self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n        bias = -self.bias_p*torch.log(1+self.bias_a*diff) # log kernel\n        \n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n            \n            if type(bias) != float:\n                # seq_len_k - 1 points to the last token index in the current inference batch.\n                bias = bias[:, seq_len_k - 1, :].view(bias.shape[0], 1, bias.shape[2])\n\n        return x + bias\n\n\nclass ParallelKerplePower(torch.nn.Module):\n    \"\"\"Kernelized Alibi Relative Position Bias parallelized in the heads dimension\"\"\"\n\n    def __init__(\n        self,\n        neox_args,\n    ):\n        super().__init__()\n        self.heads = neox_args.num_attention_heads\n        self.model_parallel_size = get_model_parallel_world_size()\n        self.model_parallel_rank = get_model_parallel_rank()\n        self.num_heads_per_partition = self.heads // self.model_parallel_size\n        self.pos_emb = neox_args.pos_emb\n        self.eps = 1e-2\n        \n        # megatron splits across heads, so we need to make sure each head receives the correct matrix\n        assert self.model_parallel_size <= self.heads and self.model_parallel_rank <= self.model_parallel_size\n        \n        # Allocate weights and initialize.\n        # bias_kernel = -bias_a*|m-n|^bias_p\n        # weight_kernel = exp(-wei_a*|m-n|^wei_p)\n        def get_parameter(scale, init_method):\n            if init_method == 'ones':\n                return Parameter(torch.ones(\n                               self.num_heads_per_partition,\n                               device=torch.cuda.current_device(),\n                               dtype=neox_args.params_dtype,\n                               )[:,None,None]*scale )\n            elif init_method == 'uniform':\n                return Parameter(torch.rand(\n                               self.num_heads_per_partition,\n                               device=torch.cuda.current_device(),\n                               dtype=neox_args.params_dtype,\n                               )[:,None,None]*scale )\n        \n        self.bias_a, self.bias_p, self.wei_a, self.wei_p = None, None, None, None\n        \n        if self.pos_emb.endswith('original'):\n            slopes = torch.Tensor(self._get_slopes(self.heads))[\n                self.model_parallel_rank * self.num_heads_per_partition : (self.model_parallel_rank + 1) * self.num_heads_per_partition\n            ][:,None,None]\n            slopes = slopes.to(torch.cuda.current_device()).to(neox_args.params_dtype)\n            self.bias_a = Parameter(slopes, requires_grad=False)\n        else:\n            bias_arg, wei_arg = self.pos_emb.split('_')[-2:]\n            self.bias_p = get_parameter(2, 'uniform') if 'p' in bias_arg else None\n            self.bias_a = get_parameter(1, 'uniform') if 'a' in bias_arg else None\n            self.wei_p = get_parameter(2, 'uniform') if 'p' in wei_arg else None\n            self.wei_a = get_parameter(1, 'uniform') if 'a' in wei_arg else None\n\n        self.cached_matrix = None\n        self.cached_seq_len = None\n    \n    def _get_slopes(self, n):\n        \"\"\"\n        Get slopes for Alibi positional embedding\n        n : int = number of heads.\n        For best performance, restrict n to a power of 2.\n        \"\"\"\n\n        def get_slopes_power_of_2(n):\n            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n            ratio = start\n            return [start * ratio ** i for i in range(n)]\n\n        if math.log2(n).is_integer():\n            return get_slopes_power_of_2(n)\n        else:\n            closest_power_of_2 = 2 ** math.floor(math.log2(n))\n            return (\n                get_slopes_power_of_2(closest_power_of_2)\n                + self._get_slopes(2 * closest_power_of_2)[0::2][\n                    : n - closest_power_of_2\n                ]\n            )\n\n    def stats(self):\n        def get_stats(name, obj):\n            return {name+'_mean': obj.mean().detach().cpu(),\n                    name+'_std': obj.std().detach().cpu(),\n                    name+'_max': obj.max().detach().cpu(),\n                    name+'_min': obj.min().detach().cpu()}\n        dd = {}\n        if self.bias_a is not None:\n            self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n            dd.update(get_stats('bias_a', self.bias_a))\n        if self.bias_p is not None:\n            self.bias_p.data = self.bias_p.data.clamp(min=self.eps, max=2)\n            dd.update(get_stats('bias_p', self.bias_p))\n        if self.wei_a is not None:\n            self.wei_a.data = self.wei_a.data.clamp(min=self.eps)\n            dd.update(get_stats('wei_a', self.wei_a))\n        if self.wei_p is not None:\n            self.wei_p.data = self.wei_p.data.clamp(min=self.eps, max=2)\n            dd.update(get_stats('wei_p', self.wei_p))\n        return dd\n    \n    def forward(self, x):\n        # [b, np, sq, sk]\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n        if self.cached_seq_len != seq_len_k:\n            diff = torch.tril(\n                torch.arange(seq_len_k, device=x.device).view(seq_len_k, 1).repeat(1, seq_len_k)\n                + torch.arange(0, -seq_len_k, -1, device=x.device)\n            )\n            diff = diff.to(x.dtype)\n            self.cached_seq_len = seq_len_k\n            self.cached_matrix = diff\n        else:\n            diff = self.cached_matrix\n        \n        # get bias matrix\n        if self.bias_p is None and self.bias_a is None:\n            bias = 0.0\n        else:\n            if self.bias_p is not None:\n                self.bias_p.data = self.bias_p.data.clamp(min=self.eps, max=2)\n                bias = diff.pow(self.bias_p)\n            else:\n                bias = diff\n            if self.bias_a is not None:\n                self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n                bias = -bias*self.bias_a\n            else:\n                bias = -bias\n\n        # get weight matrix\n        if self.wei_p is None and self.wei_a is None:\n            wei = 1.0\n        else:\n            if self.wei_p is not None:\n                self.wei_p.data = self.wei_p.data.clamp(min=self.eps, max=2)\n                wei = diff.pow(self.wei_p)\n            else:\n                wei = diff\n            if self.wei_a is not None:\n                self.wei_a.data = self.wei_a.data.clamp(min=self.eps)\n                wei = (-wei*self.wei_a).exp()\n            else:\n                wei = (-wei).exp()\n        \n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n            \n            if type(bias) != float:\n                # seq_len_k - 1 points to the last token index in the current inference batch.\n                bias = bias[:, seq_len_k - 1, :].view(bias.shape[0], 1, bias.shape[2])\n            if type(wei) != float:\n                wei = wei[:, seq_len_k - 1, :].view(wei.shape[0], 1, wei.shape[2])\n\n        return x*wei + bias\n\n",
    "description": null,
    "url": null
}