{
    "acronym": "longheads",
    "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
    "seed_ids": [
        "pi",
        "landmarkattn",
        "compressivetransformer"
    ],
    "s2id": "f6440a16ccc5c13d2a86af91b76e078685abfd16",
    "abstract": "Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that relies on the inherent correlation between the query and the key representations, efficiently distributing context chunks to different heads. In this way, each head ensures it can effectively process attended tokens within the trained length, while different heads in different layers can collectively process longer contexts. LongHeads works efficiently in linear time, fits seamlessly with many LLMs that use relative positional encoding. LongHeads achieves 100% accuracy at the 128k length on passkey retrieval task, verifying LongHeads's efficacy in extending the usable context window for existing models. We release our code at https://github.com/LuLuLuyi/LongHeads .",
    "authors": [
        "Yi Lu",
        "Xin Zhou",
        "Wei He",
        "Jun Zhao",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "LongHeads is a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential, and achieves 100% accuracy at the 128k length on passkey retrieval task, verifying LongHeads's efficacy in extending the usable context window for existing models.",
    "citationCount": 2,
    "influentialCitationCount": 2,
    "code": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nfrom transformers.utils import logging\nfrom .configuration_llama import LlamaConfig\nfrom .modules import LlamaRotaryEmbedding, apply_rotary_pos_emb, apply_rotary_pos_emb_for_relative_keys, apply_rotary_pos_emb_for_relative_query\n# from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\nfrom flash_attn import flash_attn_qkvpacked_func, flash_attn_func, flash_attn_with_kvcache\nfrom flash_attn.modules.mha import FlashSelfAttention\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _pad_to_chunk_size(tensor, window_size, past_key_value_length=0, dim=-2, pad_value=0):\n    assert dim < 0  # only accept ``dim'' index in a reverse manner\n    seqlen = int(tensor.shape[dim])\n    seqlen += past_key_value_length\n    m = seqlen / window_size\n    if m.is_integer():\n        return tensor\n    remainder = math.ceil(m) * window_size - seqlen  # get padding size\n    pad_offset = (0,) * (-1 - dim) * 2  #\n    padded_res = F.pad(tensor, (*pad_offset, 0, remainder), value=pad_value)\n    return padded_res\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\n        # longheads config\n        self.cpu_offload = config.cpu_offload\n        self.batch_encoding = config.batch_encoding\n        self.encoding_batch_size = config.encoding_batch_size\n        self.atten_length = config.atten_length\n        self.begin_selective_length = config.begin_selective_length\n        self.window_size = config.window_size\n        self.attn_scale = self.head_dim ** -0.5\n        self.causal_attn = FlashSelfAttention(causal=True, softmax_scale=self.head_dim ** -0.5)\n        self.attn = FlashSelfAttention(causal=False, softmax_scale=self.head_dim ** -0.5)\n        \n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def _nonoverlap_window_1d_partition(self, x):\n        # cut the chunk along the seq_len dimension\n        return rearrange(x, '... (g w) d -> ... g w d', w=self.window_size)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,  # [bsz, 1, tgt_seq_len, src_seq_len]\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        past_query: Optional[List[torch.FloatTensor]] = None,\n        past_global_key_value: Optional[List[torch.FloatTensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # check valid hyper param\n        assert attention_mask is not None\n        assert self.atten_length % 256 == 0\n        past_key_value_length = 0\n        if past_key_value is not None:\n            past_key_value_length = past_key_value[0].shape[-2]\n        sequence_length = position_ids[0,-1] + 1\n        ori_len = hidden_states.shape[-2]\n        \n        atten_length = self.atten_length\n        begin_selective_length = self.begin_selective_length\n        if ori_len > 1:\n            atten_chunk_num = atten_length // self.window_size\n            \n            hidden_states = _pad_to_chunk_size(hidden_states, self.window_size,\n                                            dim=-2, past_key_value_length=past_key_value_length)\n            attention_mask = _pad_to_chunk_size(attention_mask, self.window_size,\n                                            dim=-1, past_key_value_length=past_key_value_length)\n            local_position_ids = _pad_to_chunk_size(position_ids, self.window_size,\n                                            dim=-1, past_key_value_length=past_key_value_length)\n            local_position_ids = local_position_ids % self.window_size  # apply rope within chunk\n            bsz, q_len, _ = hidden_states.size()\n            attention_mask = 1 - attention_mask\n\n            query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n            key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n            value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n            kv_seq_len = key_states.shape[-2]\n            if past_key_value is not None:\n                kv_seq_len += past_key_value_length\n            # decide where to add the rope, inside or outside chunk\n            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n            query_states_ori = query_states\n            key_states_ori = key_states\n            if sequence_length > begin_selective_length:\n                normal_query_states, normal_key_states = apply_rotary_pos_emb(query_states[:,:,:begin_selective_length], key_states[:,:,:begin_selective_length], cos, sin, position_ids[:,:begin_selective_length])\n                normal_value_states = value_states[:,:,:begin_selective_length]\n            else:\n                normal_query_states, normal_key_states = apply_rotary_pos_emb(query_states[:,:,:sequence_length], key_states[:,:,:sequence_length], cos, sin, position_ids[:,:sequence_length])\n                normal_value_states = value_states[:,:,:sequence_length]\n            local_query_states, local_key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, local_position_ids)\n            # [bsz, nh, t, hd]\n\n            if past_key_value is not None:\n                # reuse k, v, self_attention\n                key_states = torch.cat([past_key_value[0], key_states], dim=2)\n                value_states = torch.cat([past_key_value[1], value_states], dim=2)\n            chunk_num = key_states.shape[-2] / self.window_size\n            assert chunk_num.is_integer()\n            \n            if self.cpu_offload:\n                past_key_value = (key_states[:,:,:ori_len].to('cpu'), value_states[:,:,:ori_len].to('cpu')) if use_cache else None\n                past_query = (query_states[:,:,:ori_len].to('cpu'),) if use_cache else None\n            else:\n                past_key_value = (key_states[:,:,:ori_len], value_states[:,:,:ori_len]) if use_cache else None\n                past_query = (query_states[:,:,:ori_len],) if use_cache else None\n\n            # partition to get global chunk\n            # [bsz, nh, t, hd] = > [bsz, nh, g, w, hd]\n            rf_w_q = self._nonoverlap_window_1d_partition(local_query_states)\n            rf_w_k = self._nonoverlap_window_1d_partition(local_key_states)\n            rf_w_v = self._nonoverlap_window_1d_partition(value_states)\n\n            rf_w_k_ori = self._nonoverlap_window_1d_partition(key_states_ori)\n\n            # compute global feature\n            global_chunk_mask = attention_mask.unsqueeze(1).unsqueeze(-1).to(torch.bool)  # [bsz, 1, t, 1]\n            rf_w_mask = self._nonoverlap_window_1d_partition(global_chunk_mask)  # [bsz, 1, g, w, 1]\n            rf_w_q = rf_w_q.masked_fill(rf_w_mask, 0.)\n            rf_w_k = rf_w_k.masked_fill(rf_w_mask, 0.)\n            rf_w_v = rf_w_v.masked_fill(rf_w_mask, 0.)\n            rf_w_k_ori = rf_w_k_ori.masked_fill(rf_w_mask, 0.)\n\n            \n            b, h, g, w, d = rf_w_q.shape\n            attn_rf_w_q = rearrange(rf_w_q, \"b h g w d -> b (h g) w d\") \n            attn_rf_w_k = rearrange(rf_w_k, \"b h g w d -> b (h g) w d\") \n            attn_rf_w_v = rearrange(rf_w_v, \"b h g w d -> b (h g) w d\") \n            qkv = torch.stack([attn_rf_w_q, attn_rf_w_k, attn_rf_w_v], dim=2)\n            \n            chunk_features = self.attn(qkv.transpose(1, 3))\n            chunk_features = chunk_features.transpose(1, 2)\n            chunk_features = rearrange(chunk_features, \"b (h g) w d -> b h g w d\", h=h)  \n            chunk_features_value = chunk_features.mean(dim=-2)  # [bsz, nh, g, hd]\n\n            chunk_features_key = F.scaled_dot_product_attention(query=chunk_features_value.unsqueeze(-2),\n                                                            key=rf_w_k,\n                                                            value=rf_w_k).squeeze(-2) # [bsz, nh, g, 1, hd]\n            if self.cpu_offload:\n                past_global_key_value = (chunk_features_key[:,:,:-1].to('cpu'), chunk_features_value[:,:,:-1].to('cpu')) if use_cache else None\n            else:\n                past_global_key_value = (chunk_features_key[:,:,:-1], chunk_features_value[:,:,:-1]) if use_cache else None\n            past_cache = (past_key_value, past_query, past_global_key_value)\n            if sequence_length >= begin_selective_length:\n                selective_query_ori = query_states_ori[:,:,begin_selective_length:]\n                selective_query_position_ids = torch.arange(0, selective_query_ori.size(-2), dtype=position_ids.dtype).unsqueeze(0) % self.window_size + (atten_chunk_num - 1) * self.window_size\n                selective_query = apply_rotary_pos_emb_for_relative_query(selective_query_ori, cos, sin, selective_query_position_ids)\n                \n                global_qk = torch.einsum('bhsd,bhgd->bhsg', selective_query_ori, chunk_features_key)\n                global_qk = rearrange(global_qk, \"b h (g w) d -> b h g w d\", h=h, w=self.window_size)\n                global_qk = global_qk.transpose(-2, -3)\n                global_qk_mask = torch.ones_like(global_qk, dtype=query_states.dtype).bool().triu(begin_selective_length//self.window_size+1)\n                global_qk_select = torch.zeros_like(global_qk, dtype=query_states.dtype).bool()\n                global_qk_select[:,:,:,:,0] = True\n                for idx in range(global_qk.size(-2)):\n                    global_qk_select[:,:,:,idx, idx + begin_selective_length//self.window_size-1: idx + begin_selective_length//self.window_size+1] = True\n                mask_value = torch.finfo(global_qk.dtype).min\n                select_value = torch.finfo(global_qk.dtype).max\n                global_qk = global_qk.masked_fill(global_qk_mask, mask_value)\n                global_qk = global_qk.masked_fill(global_qk_select, select_value)\n                global_qk = global_qk.transpose(-2, -3)\n                global_qk = rearrange(global_qk, \"b h g w d -> b h (g w) d\", h=h, w=self.window_size)\n                \n                atten_chunks = torch.topk(global_qk, k=atten_chunk_num, dim=-1, largest=True)[1]\n                sorted_atten_chunks, indices = torch.sort(atten_chunks, dim=-1)\n                \n                selective_query = selective_query[:,:, :ori_len - begin_selective_length]\n                # allocate space for atten output\n                select_atten_output = torch.zeros_like(selective_query, dtype=query_states.dtype)\n                \n                if self.batch_encoding == True:\n                    # batch encoding implementation\n                    # allocate space for selected kv list\n                    selective_sequence_length = selective_query.size(2)\n    \n                    encoding_batch_size = self.encoding_batch_size\n                    batch_num = selective_sequence_length // encoding_batch_size\n                    last_batch_size = selective_sequence_length % encoding_batch_size\n                    \n                    \n                    for batch_idx in range(batch_num):\n                        st = batch_idx * encoding_batch_size\n                        ed = (batch_idx + 1) * encoding_batch_size\n                        \n                        # init selected kv cache for every query in batch, position information is prepared in selected qkv cache\n                        selected_keys_cache = torch.zeros([encoding_batch_size, self.num_heads, atten_length, self.head_dim], dtype=query_states.dtype, device=query_states.device)\n                        selected_values_cache = torch.zeros([encoding_batch_size, self.num_heads, atten_length, self.head_dim], dtype=query_states.dtype, device=query_states.device)\n                        cache_seqlens = torch.zeros([encoding_batch_size], dtype=torch.int32, device=query_states.device)\n                        for relative_idx, query_idx in enumerate(range(st, ed)):\n                            relative_position =  (atten_chunk_num - 1) * self.window_size + query_idx % self.window_size\n                            cache_seqlens[relative_idx] = relative_position\n                            relative_position_ids = torch.arange(0, relative_position, dtype=position_ids.dtype).unsqueeze(0)\n                            selected_keys = torch.gather(rf_w_k_ori.transpose(-1,-3),-1, sorted_atten_chunks[:,:,query_idx].unsqueeze(-2).unsqueeze(-2).repeat(1, 1,self.head_dim,self.window_size,1)).transpose(-1,-3)\n                            selected_values = torch.gather(rf_w_v.transpose(-1,-3),-1, sorted_atten_chunks[:,:,query_idx].unsqueeze(-2).unsqueeze(-2).repeat(1, 1,self.head_dim,self.window_size,1)).transpose(-1,-3)\n                            selected_keys = rearrange(selected_keys, \"b h g w d -> b h (g w) d\")\n                            selected_values = rearrange(selected_values, \"b h g w d -> b h (g w) d\")\n                            # add rope to selected keys\n                            selected_keys[:,:,:relative_position] = apply_rotary_pos_emb_for_relative_keys(selected_keys[:,:,:relative_position], cos, sin, relative_position_ids)\n                            selected_keys_cache[relative_idx,:,:relative_position] = selected_keys[:,:,:relative_position]\n                            selected_values_cache[relative_idx,:,:relative_position] = selected_values[:,:,:relative_position]\n                        batch_queries = selective_query[:, :, st : ed].transpose(0, 2).transpose(1, 2)\n                        batch_keys_list = selected_keys_cache.transpose(1, 2)\n                        batch_values_list = selected_values_cache.transpose(1, 2)\n                        batch_atten_output = flash_attn_with_kvcache(batch_queries, k_cache=batch_keys_list, v_cache=batch_values_list, causal=False, cache_seqlens=cache_seqlens)\n                        select_atten_output[:, :, st : ed] = batch_atten_output.transpose(1, 2).transpose(0, 2)\n                    \n                    if last_batch_size != 0:\n                        st = batch_num * encoding_batch_size\n                        ed = st + last_batch_size\n                        assert ed==selective_sequence_length\n                        \n                        selected_keys_cache = torch.zeros([last_batch_size, self.num_heads, atten_length, self.head_dim], dtype=query_states.dtype, device=query_states.device)\n                        selected_values_cache = torch.zeros([last_batch_size, self.num_heads, atten_length, self.head_dim], dtype=query_states.dtype, device=query_states.device)\n                        cache_seqlens = torch.zeros([last_batch_size], dtype=torch.int32, device=query_states.device)\n                        for relative_idx, query_idx in enumerate(range(st, ed)):\n                            relative_position =  (atten_chunk_num - 1) * self.window_size + query_idx % self.window_size\n                            relative_position_ids = torch.arange(0, relative_position, dtype=position_ids.dtype).unsqueeze(0)\n                            cache_seqlens[relative_idx] = relative_position\n                            selected_keys = torch.gather(rf_w_k_ori.transpose(-1,-3),-1, sorted_atten_chunks[:,:,query_idx].unsqueeze(-2).unsqueeze(-2).repeat(1, 1,self.head_dim,self.window_size,1)).transpose(-1,-3)\n                            selected_values = torch.gather(rf_w_v.transpose(-1,-3),-1, sorted_atten_chunks[:,:,query_idx].unsqueeze(-2).unsqueeze(-2).repeat(1, 1,self.head_dim,self.window_size,1)).transpose(-1,-3)\n                            selected_keys = rearrange(selected_keys, \"b h g w d -> b h (g w) d\")\n                            selected_values = rearrange(selected_values, \"b h g w d -> b h (g w) d\")\n                            # add rope to selected keys\n                            selected_keys[:,:,:relative_position] = apply_rotary_pos_emb_for_relative_keys(selected_keys[:,:,:relative_position], cos, sin, relative_position_ids)\n                            selected_keys_cache[relative_idx,:,:relative_position] = selected_keys[:,:,:relative_position]\n                            selected_values_cache[relative_idx,:,:relative_position] = selected_values[:,:,:relative_position]\n                        batch_queries = selective_query[:, :, st : ed].transpose(0, 2).transpose(1, 2)\n                        batch_keys_list = selected_keys_cache.transpose(1, 2)\n                        batch_values_list = selected_values_cache.transpose(1, 2)\n                        batch_atten_output = flash_attn_with_kvcache(batch_queries, k_cache=batch_keys_list, v_cache=batch_values_list, causal=False, cache_seqlens=cache_seqlens)\n                        select_atten_output[:, :, st : ed] = batch_atten_output.transpose(1, 2).transpose(0, 2)\n                else:\n                    # sequencial encoding implementation\n                    for query_idx in range(selective_query.size(2)):\n                        current_query = selective_query[:,:,query_idx]\n                        relative_position =  (atten_chunk_num - 1) * self.window_size + query_idx % self.window_size\n                        relative_position_ids = torch.arange(0, relative_position, dtype=position_ids.dtype).unsqueeze(0)\n                        # TODO: use mask for every query and kv to speed up\n                        selected_keys = torch.gather(rf_w_k_ori.transpose(-1,-3),-1, sorted_atten_chunks[:,:,query_idx].unsqueeze(-2).unsqueeze(-2).repeat(1, 1,self.head_dim,self.window_size,1)).transpose(-1,-3)\n                        selected_values = torch.gather(rf_w_v.transpose(-1,-3),-1, sorted_atten_chunks[:,:,query_idx].unsqueeze(-2).unsqueeze(-2).repeat(1, 1,self.head_dim,self.window_size,1)).transpose(-1,-3)\n                        selected_keys = rearrange(selected_keys, \"b h g w d -> b h (g w) d\")\n                        selected_values = rearrange(selected_values, \"b h g w d -> b h (g w) d\")\n                        selected_keys = selected_keys[:,:,:relative_position]\n                        selected_values = selected_values[:,:,:relative_position]\n                        # add rope to selected keys\n                        selected_keys = apply_rotary_pos_emb_for_relative_keys(selected_keys, cos, sin, relative_position_ids)\n                        # implemenation with flash-attention 2\n                        current_query = current_query.unsqueeze(1)\n                        selected_keys = selected_keys.transpose(1, 2)\n                        selected_values = selected_values.transpose(1, 2)\n                        current_attn_output = flash_attn_with_kvcache(current_query, k_cache=selected_keys, v_cache=selected_values, softmax_scale=self.attn_scale, causal=False)\n                        current_attn_output = current_attn_output.squeeze(1)\n                        \n                        select_atten_output[:,:,query_idx] = current_attn_output\n            \n            \n            # calculate previous feature with flash-attention 2\n            normal_qkv = torch.stack([normal_query_states, normal_key_states, normal_value_states], dim=2)\n            normal_qkv = normal_qkv.transpose(1, 3)\n            normal_attn_output = flash_attn_qkvpacked_func(normal_qkv, softmax_scale=self.attn_scale, causal=True)\n            normal_attn_output = normal_attn_output.transpose(1, 2)\n            \n            if sequence_length >= begin_selective_length:\n                attn_output = torch.cat([normal_attn_output, select_atten_output], dim=-2)\n            else:\n                attn_output = normal_attn_output\n            attn_output = attn_output[:,:, past_key_value_length:past_key_value_length+ori_len].transpose(1, 2).reshape(bsz, ori_len, -1)\n            attn_output = self.o_proj(attn_output)\n            if not output_attentions:\n                attn_weights = None\n            return attn_output, attn_weights, past_cache\n        elif sequence_length >= begin_selective_length:\n            atten_chunk_num = atten_length // self.window_size\n            bsz, q_len, _ = hidden_states.size()\n            query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n            key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n            value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n            kv_seq_len = key_states.shape[-2]\n            if past_key_value is not None:\n                kv_seq_len += past_key_value_length\n                # decide where to add the rope, inside or outside chunk\n            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n            if past_key_value is not None:\n                # reuse k, v, self_attention\n                if self.cpu_offload:\n                    query_states = query_states.to('cpu')\n                    key_states = key_states.to('cpu')\n                    value_states = value_states.to('cpu')\n                key_states = torch.cat([past_key_value[0], key_states], dim=2)\n                value_states = torch.cat([past_key_value[1], value_states], dim=2)\n            # update past qkv\n            past_key_value = (key_states, value_states) if use_cache else None\n            past_query = (torch.cat([past_query[0], query_states], dim=2),)\n            \n            selection_length = sequence_length - sequence_length % self.window_size\n            last_length = sequence_length % self.window_size\n            last_keys = key_states[:,:,-last_length:] if last_length > 0 else None\n            last_values = value_states[:,:,-last_length:] if last_length > 0 else None\n            \n            rf_w_k = self._nonoverlap_window_1d_partition(key_states[:,:, :selection_length])\n            rf_w_v = self._nonoverlap_window_1d_partition(value_states[:,:, :selection_length])\n            b, h, g, w, d = rf_w_k.shape\n            \n            relative_position =  (atten_chunk_num - 1) * self.window_size + sequence_length % self.window_size\n            relative_position_ids = torch.arange(0, relative_position, dtype=position_ids.dtype).unsqueeze(0)\n            current_query_position_ids = relative_position_ids[:, -1].unsqueeze(0)\n            \n            chunk_features_key = past_global_key_value[0] if past_global_key_value is not None else None\n            current_query_ori = query_states[:,:,ori_len-1]\n            \n            if chunk_features_key is None or chunk_features_key.size()[-2] < 1:\n                position_ids_for_local_atten = torch.arange(0, selection_length, dtype=position_ids.dtype).unsqueeze(0)\n                position_ids_for_local_atten = position_ids_for_local_atten % self.window_size\n                all_query_states_for_local_atten = past_query[0][:,:, :selection_length]\n                all_key_states_for_local_atten = key_states[:,:, :selection_length]\n                if self.cpu_offload:\n                    all_query_states_for_local_atten = all_query_states_for_local_atten.to(hidden_states.device)\n                    all_key_states_for_local_atten = all_key_states_for_local_atten.to(hidden_states.device)\n                    \n                all_query_states_for_local_atten, all_key_states_for_local_atten = apply_rotary_pos_emb(all_query_states_for_local_atten, all_key_states_for_local_atten, cos, sin, position_ids_for_local_atten)\n                rf_w_q_rope = self._nonoverlap_window_1d_partition(all_query_states_for_local_atten)\n                rf_w_k_rope = self._nonoverlap_window_1d_partition(all_key_states_for_local_atten)\n                \n                attn_rf_w_q = rearrange(rf_w_q_rope, \"b h g w d -> b (h g) w d\") \n                attn_rf_w_k = rearrange(rf_w_k_rope, \"b h g w d -> b (h g) w d\") \n                if self.cpu_offload:\n                    attn_rf_w_v = rearrange(rf_w_v, \"b h g w d -> b (h g) w d\")\n                    attn_rf_w_v = attn_rf_w_v.to(hidden_states.device)\n                else:\n                    attn_rf_w_v = rearrange(rf_w_v, \"b h g w d -> b (h g) w d\") \n                qkv = torch.stack([attn_rf_w_q, attn_rf_w_k, attn_rf_w_v], dim=2)\n                \n                chunk_features = self.attn(qkv.transpose(1, 3))\n                chunk_features = chunk_features.transpose(1, 2)\n                chunk_features = rearrange(chunk_features, \"b (h g) w d -> b h g w d\", h=h)  \n                chunk_features_value = chunk_features.mean(dim=-2)  # [bsz, nh, g, hd]\n\n                chunk_features_key = F.scaled_dot_product_attention(query=chunk_features_value.unsqueeze(-2), key=rf_w_k_rope, value=rf_w_k_rope).squeeze(-2) # [bsz, nh, g, 1, hd]\n                \n                past_global_key_value = (chunk_features_key, chunk_features_value) if use_cache else None\n                \n            elif g > chunk_features_key.size()[-2]:\n                position_ids_for_local_atten = torch.arange(0, self.window_size, dtype=position_ids.dtype).unsqueeze(0)\n                all_query_states_for_local_atten = past_query[0][:,:, selection_length - self.window_size:selection_length]\n                all_key_states_for_local_atten = key_states[:,:, selection_length - self.window_size:selection_length]\n                if self.cpu_offload:\n                    all_query_states_for_local_atten = all_query_states_for_local_atten.to(hidden_states.device)\n                    all_key_states_for_local_atten = all_key_states_for_local_atten.to(hidden_states.device)\n                    \n                all_query_states_for_local_atten, all_key_states_for_local_atten = apply_rotary_pos_emb(all_query_states_for_local_atten, all_key_states_for_local_atten, cos, sin, position_ids_for_local_atten)\n                rf_w_q_rope = self._nonoverlap_window_1d_partition(all_query_states_for_local_atten)\n                rf_w_k_rope = self._nonoverlap_window_1d_partition(all_key_states_for_local_atten)\n                \n                attn_rf_w_q = rearrange(rf_w_q_rope, \"b h g w d -> b (h g) w d\") \n                attn_rf_w_k = rearrange(rf_w_k_rope, \"b h g w d -> b (h g) w d\")\n                if self.cpu_offload:\n                    attn_rf_w_v = rearrange(rf_w_v[:,:,-1:], \"b h g w d -> b (h g) w d\")\n                    attn_rf_w_v = attn_rf_w_v.to(hidden_states.device)\n                else:    \n                    attn_rf_w_v = rearrange(rf_w_v[:,:,-1:], \"b h g w d -> b (h g) w d\") \n                qkv = torch.stack([attn_rf_w_q, attn_rf_w_k, attn_rf_w_v], dim=2)\n                \n                chunk_features = self.attn(qkv.transpose(1, 3))\n                chunk_features = chunk_features.transpose(1, 2)\n                chunk_features = rearrange(chunk_features, \"b (h g) w d -> b h g w d\", h=h)  \n                new_chunk_features_value = chunk_features.mean(dim=-2)  # [bsz, nh, g, hd]\n\n                new_chunk_features_key = F.scaled_dot_product_attention(query=new_chunk_features_value.unsqueeze(-2), key=rf_w_k_rope, value=rf_w_k_rope).squeeze(-2) # [bsz, nh, g, 1, hd]\n                \n                if self.cpu_offload:\n                    chunk_features_key = torch.cat([past_global_key_value[0], new_chunk_features_key.to('cpu')], dim=2)\n                    chunk_features_value = torch.cat([past_global_key_value[1], new_chunk_features_value.to('cpu')], dim=2)\n                else:\n                    chunk_features_key = torch.cat([past_global_key_value[0], new_chunk_features_key], dim=2)\n                    chunk_features_value = torch.cat([past_global_key_value[1], new_chunk_features_value], dim=2)\n                past_global_key_value = (chunk_features_key, chunk_features_value) if use_cache else None\n                \n                \n            if chunk_features_key.size()[-2] != g:\n                raise ValueError(\n                    f\"`chunk_features_key` should be have {g} chunks, but only have\"\n                    f\" {chunk_features_key.size()[-2]} chunks\"\n                )\n            if self.cpu_offload:\n                global_qk = torch.einsum('bhd,bhgd->bhg', current_query_ori.to(hidden_states.device), chunk_features_key[:,:,1:-1].to(hidden_states.device))\n            else:\n                global_qk = torch.einsum('bhd,bhgd->bhg', current_query_ori, chunk_features_key[:,:,1:-1])\n\n            atten_chunks = torch.topk(global_qk, k=atten_chunk_num - 3, dim=-1, largest=True)[1] + 1\n            sorted_atten_chunks, indices = torch.sort(atten_chunks, dim=-1)\n            # chunk N-1 is the last chunk, and the last part of seq is denote as last key and last value\n            first_chunk = torch.full(sorted_atten_chunks[...,-1:].size(), 0, dtype=sorted_atten_chunks.dtype, device=sorted_atten_chunks.device)\n            last_chunk = torch.full(sorted_atten_chunks[...,-1:].size(), g-1, dtype=sorted_atten_chunks.dtype, device=sorted_atten_chunks.device)\n            if self.cpu_offload:\n                sorted_atten_chunks = torch.cat([first_chunk, sorted_atten_chunks, last_chunk], dim=-1).to('cpu')\n            else:\n                sorted_atten_chunks = torch.cat([first_chunk, sorted_atten_chunks, last_chunk], dim=-1)\n            \n            selected_keys = torch.gather(rf_w_k.transpose(-1,-3),-1, sorted_atten_chunks.unsqueeze(-2).unsqueeze(-2).repeat(1, 1, self.head_dim,self.window_size, 1)).transpose(-1,-3)\n            selected_values = torch.gather(rf_w_v.transpose(-1,-3),-1, sorted_atten_chunks.unsqueeze(-2).unsqueeze(-2).repeat(1, 1, self.head_dim,self.window_size, 1)).transpose(-1,-3)\n            selected_keys = rearrange(selected_keys, \"b h g w d -> b h (g w) d\")\n            selected_values = rearrange(selected_values, \"b h g w d -> b h (g w) d\")\n            atten_keys = torch.cat([selected_keys, last_keys], dim=-2) if last_keys is not None else selected_keys\n            atten_values = torch.cat([selected_values, last_values], dim=-2) if last_values is not None else selected_values\n            if self.cpu_offload:\n                atten_keys = atten_keys.to(hidden_states.device)\n                atten_values = atten_values.to(hidden_states.device)\n                query_states = query_states.to(hidden_states.device)\n            # apply rope\n            query_states = apply_rotary_pos_emb_for_relative_query(query_states, cos, sin, current_query_position_ids)\n            atten_keys = apply_rotary_pos_emb_for_relative_keys(atten_keys, cos, sin, relative_position_ids)\n\n            # implementation with flash-attention 2\n            query_states = query_states.transpose(1, 2)\n            atten_keys = atten_keys.transpose(1, 2)\n            atten_values = atten_values.transpose(1, 2)\n            attn_output = flash_attn_with_kvcache(query_states, k_cache=atten_keys, v_cache=atten_values, softmax_scale=self.attn_scale, causal=False)\n            attn_output = attn_output.reshape(bsz, ori_len, -1)\n            attn_output = self.o_proj(attn_output)\n            \n            past_cache = (past_key_value, past_query, past_global_key_value)\n            if not output_attentions:\n                attn_weights = None\n            return attn_output, attn_weights, past_cache\n        else:\n            bsz, q_len, _ = hidden_states.size()\n            query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n            key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n            value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n            kv_seq_len = key_states.shape[-2]\n            if past_key_value is not None:\n                kv_seq_len += past_key_value_length\n                # decide where to add the rope, inside or outside chunk\n            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n            if past_key_value is not None:\n                # reuse k, v, self_attention\n                if self.cpu_offload:\n                    key_states = torch.cat([past_key_value[0].to(hidden_states.device), key_states], dim=2)\n                    value_states = torch.cat([past_key_value[1].to(hidden_states.device), value_states], dim=2)\n                else:\n                    key_states = torch.cat([past_key_value[0], key_states], dim=2)\n                    value_states = torch.cat([past_key_value[1], value_states], dim=2)\n                \n            # update past qkv\n            if self.cpu_offload:\n                past_key_value = (key_states.to('cpu'), value_states.to('cpu')) if use_cache else None\n                past_query = (torch.cat([past_query[0], query_states.to('cpu')], dim=2),) if past_query is not None else (query_states.to('cpu'),)\n            else:\n                past_key_value = (key_states, value_states) if use_cache else None\n                past_query = (torch.cat([past_query[0], query_states], dim=2),) if past_query is not None else (query_states,)\n            # add rope\n            key_position_ids = torch.arange(0, sequence_length, dtype=position_ids.dtype).unsqueeze(0)\n            query_states = apply_rotary_pos_emb_for_relative_query(query_states, cos, sin, position_ids)\n            key_states = apply_rotary_pos_emb_for_relative_keys(key_states, cos, sin, key_position_ids)\n            \n            # implementation with flash-attention 2\n            query_states = query_states.transpose(1, 2)\n            key_states = key_states.transpose(1, 2)\n            value_states = value_states.transpose(1, 2)\n            attn_output = flash_attn_with_kvcache(query_states, k_cache=key_states, v_cache=value_states, softmax_scale=self.attn_scale, causal=False)\n            \n            attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n            attn_output = self.o_proj(attn_output)\n            \n            past_cache = (past_key_value, past_query, past_global_key_value)\n            if not output_attentions:\n                attn_weights = None\n\n            return attn_output, attn_weights, past_cache\n       \n\n",
    "description": null,
    "url": null
}