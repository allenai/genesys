{
    "acronym": "gmlp",
    "title": "Pay Attention to MLPs",
    "seed_ids": [
        "transformer",
        "bert"
    ],
    "s2id": "e3a3e85c5a32af29e13b3561f6cf070de70651de",
    "abstract": "Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.",
    "authors": [
        "Hanxiao Liu",
        "Zihang Dai",
        "David R. So",
        "Quoc V. Le"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": "This work proposes a simple network architecture, gMLP, based on MLPs with gating, and shows that it can perform as well as Transformers in key language and vision applications and can scale as much as Transformers over increased data and compute.",
    "citationCount": 526,
    "influentialCitationCount": 70,
    "code": "from random import randrange\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange, Reduce\n\n# functions\n\ndef exists(val):\n    return val is not None\n\ndef pair(val):\n    return (val, val) if not isinstance(val, tuple) else val\n\ndef dropout_layers(layers, prob_survival):\n    if prob_survival == 1:\n        return layers\n\n    num_layers = len(layers)\n    to_drop = torch.zeros(num_layers).uniform_(0., 1.) > prob_survival\n\n    # make sure at least one layer makes it\n    if all(to_drop):\n        rand_index = randrange(num_layers)\n        to_drop[rand_index] = False\n\n    layers = [layer for (layer, drop) in zip(layers, to_drop) if not drop]\n    return layers\n\ndef shift(t, amount, mask = None):\n    if amount == 0:\n        return t\n    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n\n# helper classes\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(x) + x\n\nclass PreShiftTokens(nn.Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n\n    def forward(self, x, **kwargs):\n        if self.shifts == (0,):\n            return self.fn(x, **kwargs)\n\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments\n        splitted = x.split(feats_per_shift, dim = -1)\n        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n        segments_to_shift = list(map(lambda args: shift(*args), zip(segments_to_shift, shifts)))\n        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n        return self.fn(x, **kwargs)\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\nclass Attention(nn.Module):\n    def __init__(self, dim_in, dim_out, dim_inner, causal = False):\n        super().__init__()\n        self.scale = dim_inner ** -0.5\n        self.causal = causal\n\n        self.to_qkv = nn.Linear(dim_in, dim_inner * 3, bias = False)\n        self.to_out = nn.Linear(dim_inner, dim_out)\n\n    def forward(self, x):\n        device = x.device\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n\n        if self.causal:\n            mask = torch.ones(sim.shape[-2:], device = device).triu(1).bool()\n            sim.masked_fill_(mask[None, ...], -torch.finfo(q.dtype).max)\n\n        attn = sim.softmax(dim = -1)\n        out = einsum('b i j, b j d -> b i d', attn, v)\n        return self.to_out(out)\n\nclass SpatialGatingUnit(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_seq,\n        causal = False,\n        act = nn.Identity(),\n        heads = 1,\n        init_eps = 1e-3,\n        circulant_matrix = False\n    ):\n        super().__init__()\n        dim_out = dim // 2\n        self.heads = heads\n        self.causal = causal\n        self.norm = nn.LayerNorm(dim_out)\n\n        self.act = act\n\n        # parameters\n\n        if circulant_matrix:\n            self.circulant_pos_x = nn.Parameter(torch.ones(heads, dim_seq))\n            self.circulant_pos_y = nn.Parameter(torch.ones(heads, dim_seq))\n\n        self.circulant_matrix = circulant_matrix\n        shape = (heads, dim_seq,) if circulant_matrix else (heads, dim_seq, dim_seq)\n        weight = torch.zeros(shape)\n\n        self.weight = nn.Parameter(weight)\n        init_eps /= dim_seq\n        nn.init.uniform_(self.weight, -init_eps, init_eps)\n\n        self.bias = nn.Parameter(torch.ones(heads, dim_seq))\n\n    def forward(self, x, gate_res = None):\n        device, n, h = x.device, x.shape[1], self.heads\n\n        res, gate = x.chunk(2, dim = -1)\n        gate = self.norm(gate)\n\n        weight, bias = self.weight, self.bias\n\n        if self.circulant_matrix:\n            # build the circulant matrix\n\n            dim_seq = weight.shape[-1]\n            weight = F.pad(weight, (0, dim_seq), value = 0)\n            weight = repeat(weight, '... n -> ... (r n)', r = dim_seq)\n            weight = weight[:, :-dim_seq].reshape(h, dim_seq, 2 * dim_seq - 1)\n            weight = weight[:, :, (dim_seq - 1):]\n\n            # give circulant matrix absolute position awareness\n\n            pos_x, pos_y = self.circulant_pos_x, self.circulant_pos_y\n            weight = weight * rearrange(pos_x, 'h i -> h i ()') * rearrange(pos_y, 'h j -> h () j')\n\n        if self.causal:\n            weight, bias = weight[:, :n, :n], bias[:, :n]\n            mask = torch.ones(weight.shape[-2:], device = device).triu_(1).bool()\n            mask = rearrange(mask, 'i j -> () i j')\n            weight = weight.masked_fill(mask, 0.)\n\n        gate = rearrange(gate, 'b n (h d) -> b h n d', h = h)\n\n        gate = einsum('b h n d, h m n -> b h m d', gate, weight)\n        gate = gate + rearrange(bias, 'h n -> () h n ()')\n\n        gate = rearrange(gate, 'b h n d -> b n (h d)')\n\n        if exists(gate_res):\n            gate = gate + gate_res\n\n        return self.act(gate) * res\n\nclass gMLPBlock(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_ff,\n        seq_len,\n        heads = 1,\n        attn_dim = None,\n        causal = False,\n        act = nn.Identity(),\n        circulant_matrix = False\n    ):\n        super().__init__()\n        self.proj_in = nn.Sequential(\n            nn.Linear(dim, dim_ff),\n            nn.GELU()\n        )\n\n        self.attn = Attention(dim, dim_ff // 2, attn_dim, causal) if exists(attn_dim) else None\n\n        self.sgu = SpatialGatingUnit(dim_ff, seq_len, causal, act, heads, circulant_matrix = circulant_matrix)\n        self.proj_out = nn.Linear(dim_ff // 2, dim)\n\n    def forward(self, x):\n        gate_res = self.attn(x) if exists(self.attn) else None\n        x = self.proj_in(x)\n        x = self.sgu(x, gate_res = gate_res)\n        x = self.proj_out(x)\n        return x\n\n# main classes\n\nclass gMLP(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens = None,\n        dim,\n        depth,\n        seq_len,\n        heads = 1,\n        ff_mult = 4,\n        attn_dim = None,\n        prob_survival = 1.,\n        causal = False,\n        circulant_matrix = False,\n        shift_tokens = 0,\n        act = nn.Identity()\n    ):\n        super().__init__()\n        assert (dim % heads) == 0, 'dimension must be divisible by number of heads'\n\n        dim_ff = dim * ff_mult\n        self.seq_len = seq_len\n        self.prob_survival = prob_survival\n\n        self.to_embed = nn.Embedding(num_tokens, dim) if exists(num_tokens) else nn.Identity()\n\n        token_shifts = tuple(range(0 if causal else -shift_tokens, shift_tokens + 1))\n        self.layers = nn.ModuleList([Residual(PreNorm(dim, PreShiftTokens(token_shifts, gMLPBlock(dim = dim, heads = heads, dim_ff = dim_ff, seq_len = seq_len, attn_dim = attn_dim, causal = causal, act = act, circulant_matrix = circulant_matrix)))) for i in range(depth)])\n\n        self.to_logits = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_tokens)\n        ) if exists(num_tokens) else nn.Identity()\n\n    def forward(self, x):\n        x = self.to_embed(x)\n        layers = self.layers if not self.training else dropout_layers(self.layers, self.prob_survival)\n        out = nn.Sequential(*layers)(x)\n        return self.to_logits(out)\n\nclass gMLPVision(nn.Module):\n    def __init__(\n        self,\n        *,\n        image_size,\n        patch_size,\n        num_classes,\n        dim,\n        depth,\n        heads = 1,\n        ff_mult = 4,\n        channels = 3,\n        attn_dim = None,\n        prob_survival = 1.\n    ):\n        super().__init__()\n        assert (dim % heads) == 0, 'dimension must be divisible by number of heads'\n\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n        assert (image_height % patch_height) == 0 and (image_width % patch_width) == 0, 'image height and width must be divisible by patch size'\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n\n        dim_ff = dim * ff_mult\n\n        self.to_patch_embed = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1 = patch_height, p2 = patch_width),\n            nn.Linear(channels * patch_height * patch_width, dim)\n        )\n\n        self.prob_survival = prob_survival\n\n        self.layers = nn.ModuleList([Residual(PreNorm(dim, gMLPBlock(dim = dim, heads = heads, dim_ff = dim_ff, seq_len = num_patches, attn_dim = attn_dim))) for i in range(depth)])\n\n        self.to_logits = nn.Sequential(\n            nn.LayerNorm(dim),\n            Reduce('b n d -> b d', 'mean'),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.to_patch_embed(x)\n        layers = self.layers if not self.training else dropout_layers(self.layers, self.prob_survival)\n        x = nn.Sequential(*layers)(x)\n        return self.to_logits(x)",
    "description": null,
    "url": null
}