{
    "acronym": "ssdlm",
    "title": "Semi-autoregressive Simplex-based Diffusion Language Model ",
    "seed_ids": [
        "analogbits",
        "diffusionlm",
        "d3pms",
        "gpt2",
        "bert"
    ],
    "s2id": "0b9770a377b3f96cef9f268cee1791d39a0d4893",
    "abstract": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM\u2014a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
    "authors": [
        "Xiaochuang Han",
        "Sachin Kumar",
        "Yulia Tsvetkov"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": "SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates, and is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation.",
    "citationCount": 52,
    "influentialCitationCount": 7,
    "code": "#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...)\non a text file or a dataset without using HuggingFace Trainer.\n\nHere is the full list of checkpoints on the hub that can be fine-tuned by this script:\nhttps://huggingface.co/models?filter=masked-lm\n\"\"\"\n# You can also adapt this script on your own mlm task. Pointers for this are left as comments.\n\nimport logging\nimport math\n\nimport torch\nfrom tqdm.auto import tqdm\n\nfrom transformers import (\n    MODEL_MAPPING,\n    AutoModelForSequenceClassification,\n)\nfrom transformers.utils.versions import require_version\n\nfrom termcolor import colored\n\n\nlogger = logging.getLogger(__name__)\nrequire_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\nMODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n\ndef get_time_variables(t, total_t, device): # according to https://arxiv.org/pdf/2102.09672.pdf\n\n    def ft(small_t, big_t, s=1e-4):\n        return torch.cos((small_t / big_t + s) / (1 + s) * math.pi / 2) ** 2\n\n    alpha_t_bar = ft(t, total_t) / ft(torch.zeros(t.shape).to(device), total_t)\n    alpha_t_minus_bar = ft(t-1, total_t) / ft(torch.zeros(t.shape).to(device), total_t)\n    beta_t = 1 - (alpha_t_bar / alpha_t_minus_bar)\n    beta_t_til = (1 - alpha_t_minus_bar) / (1 - alpha_t_bar) * beta_t\n    alpha_t = 1 - beta_t\n    return alpha_t_bar, alpha_t_minus_bar, beta_t, beta_t_til, alpha_t\n\n\ndef apply_controlling_drift(args, perturbed_inputs_diralpha):\n    if args.decode_ctr_lr <= 0:\n        args.ctr_loss = -1\n        return perturbed_inputs_diralpha\n\n    if args.ctr_model is None:\n        ctr_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n        args.ctr_model = AutoModelForSequenceClassification.from_pretrained(ctr_model_name).to(args.accelerator.device)\n    optimizing_label_index = 2\n\n    for ctr_i in range(1):\n        with torch.enable_grad():\n            perturbed_inputs_diralpha_4ctr = perturbed_inputs_diralpha.clone()\n            perturbed_inputs_diralpha_4ctr.requires_grad_()\n            perturbed_inputs_simplex_4ctr = torch.nn.functional.softmax(perturbed_inputs_diralpha_4ctr, dim=-1)\n            perturbed_inputs_embeds_4ctr = torch.nn.functional.linear(perturbed_inputs_simplex_4ctr, args.ctr_model.get_input_embeddings().weight.t())\n            ctr_loss = -torch.nn.functional.log_softmax(args.ctr_model(inputs_embeds=perturbed_inputs_embeds_4ctr).logits, dim=-1)[:,optimizing_label_index].mean()\n            args.ctr_loss = ctr_loss\n            ctr_delta = -torch.autograd.grad(ctr_loss, perturbed_inputs_diralpha_4ctr)[0] # indexing 0 because the return is a tuple\n\n        perturbed_inputs_diralpha = perturbed_inputs_diralpha + args.decode_ctr_lr * ctr_delta\n    \n    return perturbed_inputs_diralpha\n\n\ndef logits_projection(logits, top_p, one_hot_value):\n    assert len(logits.size()) == 3\n    very_low_value = -10000\n\n    # get top-p indices\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)\n    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)\n    nucleus = cum_sum_probs < top_p\n    nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)\n    valid_indices = nucleus.scatter(2, indices, nucleus)\n\n    logits = logits.masked_fill(valid_indices == 0, very_low_value - one_hot_value)\n    return torch.clamp(logits, max=very_low_value + one_hot_value) - very_low_value\n\n\ndef decode(args, batch_input_ids, dec_depth, total_t, model_embedding_lut, embedding_sum_layer, timestep_layer, model, tokenizer):\n    batch_size = args.per_device_eval_batch_size\n    if args.decode_truncate_len > 0:\n        diffusion_input_ids = batch_input_ids[:, args.context_size:-args.decode_truncate_len]\n    else:\n        diffusion_input_ids = batch_input_ids[:, args.context_size:]\n    \n    # for each decode step\n    assert (args.max_seq_length - args.context_size - args.decode_truncate_len) % dec_depth == 0\n    unit_seq_len = int((args.max_seq_length - args.context_size - args.decode_truncate_len) / dec_depth)\n    if args.context_size > 0:\n        unit_context_input_ids = batch_input_ids[:, :args.context_size].clone()\n    else:\n        unit_context_input_ids = None\n    history_decode_ids = None\n\n    for i in range(dec_depth):\n        unit_noise = args.noise_manual_scale * args.one_hot_value * torch.normal(0, 1, size=(batch_size, unit_seq_len, args.vocab_size)).to(args.accelerator.device)\n        xt = unit_noise\n\n        if unit_context_input_ids is not None:\n            context_inputs_embeds = model_embedding_lut(unit_context_input_ids)\n        else:\n            context_inputs_embeds = None\n\n        t_range = list(range(1, args.sigma_num_steps+1))\n        t_range.reverse()\n        progress_bar = tqdm(range(len(t_range)), disable=not args.accelerator.is_local_main_process)\n        \n        for t in t_range:\n            selected_t = torch.FloatTensor([t]).repeat(batch_size).to(args.accelerator.device)\n            alpha_t_bar, alpha_t_minus_bar, beta_t, beta_t_til, alpha_t = get_time_variables(selected_t, total_t, args.accelerator.device)\n            beta_t_til = beta_t_til.view(batch_size, 1, 1)\n            zt = args.noise_manual_scale * args.one_hot_value * torch.normal(0, 1, size=(batch_size, unit_seq_len, args.vocab_size)).to(args.accelerator.device)\n            \n            perturbed_inputs_diralpha = xt\n            \n            mean_or_protect_for_nan = True # (HACK: for the nan issue)\n            if mean_or_protect_for_nan:\n                perturbed_inputs_simplex = torch.nn.functional.softmax(perturbed_inputs_diralpha, dim=-1)\n            else:\n                perturbed_inputs_diralpha = torch.exp(perturbed_inputs_diralpha)\n                dir_model = torch.distributions.dirichlet.Dirichlet(perturbed_inputs_diralpha)\n                perturbed_inputs_simplex = dir_model.sample()\n\n            # pass to the model, conditioned on the timestep as well\n            perturbed_inputs_embeds = embedding_sum_layer(perturbed_inputs_simplex)\n            t_progress = selected_t / total_t\n            timestep_embeds = timestep_layer(t_progress.view(batch_size,1,1).repeat(1,unit_seq_len,1))\n\n            diffusion_embeds = perturbed_inputs_embeds + timestep_embeds\n            if context_inputs_embeds is not None:\n                diffusion_embeds = torch.cat((context_inputs_embeds, diffusion_embeds), dim=1)\n            outputs = model(inputs_embeds=diffusion_embeds, output_hidden_states=False)\n            equivalent_score = outputs.logits\n            if unit_context_input_ids is not None:\n                equivalent_score = equivalent_score[:, unit_context_input_ids.size(1):].contiguous()\n\n            equivalent_score = apply_controlling_drift(args, equivalent_score)\n            \n            if t > 1:\n                sigma_t = torch.sqrt(beta_t_til)\n            else:\n                sigma_t = 0\n            if args.loss_mode == \"l2_on_z\":\n                raise NotImplementedError(\"l2_on_z samping is not implemented yet\")\n            else:\n                projected_logits = logits_projection(equivalent_score, top_p=args.projection_top_p, one_hot_value=args.one_hot_value)\n                xt = torch.sqrt(alpha_t_minus_bar).view(-1, 1, 1) * projected_logits\n                xt = xt + torch.sqrt(1 - alpha_t_minus_bar).view(-1, 1, 1) * zt\n\n            progress_bar.update(1)\n\n            if t % 200 == 0 or t == 1:\n                simplex = torch.nn.functional.softmax(xt, dim=-1)\n                logger.info(f\"sigma_t={sigma_t}, training_coef_at_t={torch.sqrt(1 - alpha_t_bar)}\")\n                logger.info(f\"predicted simplex's entropy={torch.distributions.categorical.Categorical(logits=equivalent_score).entropy()}, logit_max,min,mean={torch.max(equivalent_score)},{torch.min(equivalent_score)},{torch.mean(equivalent_score)}\")\n\n                if unit_context_input_ids is not None:\n                    context_sequences = tokenizer.batch_decode(unit_context_input_ids.detach().to('cpu'))\n                    logger.info(f\"context: {context_sequences}\")\n                \n                real_token_ids_list = torch.argmax(simplex, dim=-1).view(batch_size, unit_seq_len)\n                sampled_sequences = tokenizer.batch_decode(real_token_ids_list.clone().detach().to('cpu'))\n                logger.info(f\"t={t}: {colored(str(sampled_sequences), 'red')}\")\n\n                simplex = equivalent_score\n                real_token_ids_list = torch.argmax(simplex, dim=-1).view(batch_size, unit_seq_len)\n                sampled_sequences = tokenizer.batch_decode(real_token_ids_list.clone().detach().to('cpu'))\n                logger.info(f\"t={t} (before +z): {colored(str(sampled_sequences), 'green')}\")\n\n                alt_i = 1 # look at the second best candidate\n                alt_real_token_ids_list = torch.topk(simplex, alt_i+1, dim=-1).indices[:, :, alt_i].view(batch_size, unit_seq_len)\n                alt_sampled_sequences = tokenizer.batch_decode(alt_real_token_ids_list.clone().detach().to('cpu'))\n                logger.info(f\"t={t} (alt{alt_i+1}): {colored(str(alt_sampled_sequences), 'blue')}\")\n\n                logger.info(f\"ctr loss: {args.ctr_loss}\")\n                logger.info(f\"non-zero vocab: {torch.count_nonzero(projected_logits > -args.one_hot_value+0.0001) / simplex.size(0) / simplex.size(1)} out of {torch.numel(projected_logits) / simplex.size(0) / simplex.size(1)}\")\n        \n        unit_context_input_ids = torch.cat((unit_context_input_ids, real_token_ids_list), dim=1)\n        if history_decode_ids is None:\n            history_decode_ids = real_token_ids_list\n        else:\n            history_decode_ids = torch.cat((history_decode_ids, real_token_ids_list), dim=1)\n\n    if args.context_size > 0:\n        init_context_input_ids = batch_input_ids[:, :args.context_size].clone()\n        context_sequences = tokenizer.batch_decode(init_context_input_ids.detach().to('cpu'))\n    else:\n        init_context_input_ids = None\n        context_sequences = None\n    gold_sequences = tokenizer.batch_decode(diffusion_input_ids.clone().detach().to('cpu'))\n    sampled_sequences = tokenizer.batch_decode(history_decode_ids.clone().detach().to('cpu'))\n    logger.info(f\"context: {context_sequences}\")\n    logger.info(f\"gold: {colored(str(gold_sequences), 'yellow')}\")\n    logger.info(f\"t={t}: {colored(str(sampled_sequences), 'red')}\")\n\n    return history_decode_ids, init_context_input_ids, diffusion_input_ids, sampled_sequences, context_sequences, gold_sequences\n\n",
    "description": null,
    "url": null
}