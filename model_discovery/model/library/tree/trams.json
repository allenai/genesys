{
    "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling",
    "acronym": "trams",
    "s2id": "482ffdfc1e3594f5a22f51e3b9ee31e1c74f85de",
    "abstract": "The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": "A plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric, and the results indicate an improvement without having additional training or adding additional parameters.",
    "citationCount": 2,
    "influentialCitationCount": 0,
    "seed_ids": [
        "unlimiformer",
        "routingtransformer",
        "reformer",
        "transformerxl"
    ],
    "code": null
}