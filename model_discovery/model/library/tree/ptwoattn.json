{
    "acronym": "ptwoattn",
    "title": "Pretraining without attention",
    "seed_ids": [
        "h3",
        "mega",
        "gssm",
        "s4d",
        "flash",
        "s4",
        "fnet",
        "longformer",
        "bert"
    ],
    "s2id": "a128b1c47e6842605fb95bceae930d2135fc38fc",
    "abstract": "Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://github.com/jxiw/BiGS.",
    "authors": [
        "Junxiong Wang",
        "J. Yan",
        "Albert Gu",
        "Alexander M. Rush"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": "This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs), and proposes a proposed model, Bidirectional Gated SSM (BiGS), that combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures.",
    "citationCount": 35,
    "influentialCitationCount": 0,
    "code": "# coding=utf-8\n# Copyright 2022. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch BiGS model. \"\"\"\nimport math\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.utils import (\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithNoAttention,\n    BaseModelOutputWithPooling,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel, SequenceSummary\nfrom transformers.utils import logging\nfrom .configuration_bigs import BiGSConfig\n\nfrom einops import repeat\nfrom torch.linalg import eigh\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"bigs\"\n_CONFIG_FOR_DOC = \"BiGSConfig\"\n_TOKENIZER_FOR_DOC = \"BiGSTokenizer\"\n\n_c2r = torch.view_as_real\n_r2c = torch.view_as_complex\n\nBIGS_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"JunxiongWang/BiGS_128\",\n    \"JunxiongWang/BiGS_512\",\n    \"JunxiongWang/BiGS_1024\",\n    \"JunxiongWang/BiGS_4096\",\n    # See all BiGS models at https://huggingface.co/models?filter=BiGS\n]\n\n\ndef log_step_initializer(H=1024, dt_min=0.01, dt_max=1):\n    # Generate dt\n    log_dt = torch.rand(H) * (\n            math.log(dt_max) - math.log(dt_min)\n    ) + math.log(dt_min)\n    return log_dt\n\ntry:\n    import pykeops\n    from pykeops.torch import Genred\n    has_pykeops = True\n    print(\"Pykeops installation found.\")\n\n    def _broadcast_dims(*tensors):\n        max_dim = max([len(tensor.shape) for tensor in tensors])\n        tensors = [tensor.view((1,)*(max_dim-len(tensor.shape))+tensor.shape) for tensor in tensors]\n        return tensors\n\n    def cauchy_keops(v, z, w):\n        expr_num = 'z * ComplexReal(v) - Real2Complex(Sum(v * w))'\n        expr_denom = 'ComplexMult(z-w, z-Conj(w))'\n\n        cauchy_mult = Genred(\n            f'ComplexDivide({expr_num}, {expr_denom})',\n            [\n                'v = Vj(2)',\n                'z = Vi(2)',\n                'w = Vj(2)',\n            ],\n            reduction_op='Sum',\n            axis=1,\n        )\n\n        v, z, w = _broadcast_dims(v, z, w)\n        v = _c2r(v)\n        z = _c2r(z)\n        w = _c2r(w)\n\n        r = 2*cauchy_mult(v, z, w, backend='GPU')\n        return _r2c(r)\n\n    def log_vandermonde_keops(v, x, L):\n        expr = 'ComplexMult(v, ComplexExp(ComplexMult(x, l)))'\n        vandermonde_mult = Genred(\n            expr,\n            [\n                'v = Vj(2)',\n                'x = Vj(2)',\n                'l = Vi(2)',\n            ],\n            reduction_op='Sum',\n            axis=1,\n        )\n\n        l = torch.arange(L).to(x)\n        v, x, l = _broadcast_dims(v, x, l)\n        v = _c2r(v)\n        x = _c2r(x)\n        l = _c2r(l)\n\n        r = vandermonde_mult(v, x, l, backend='GPU')\n        return _r2c(r).real\n\n    def log_vandermonde_transpose_keops(u, v, x, L):\n        \"\"\"\n        u: ... H L\n        v: ... H N\n        x: ... H N\n        Returns: ... H N\n\n        V = Vandermonde(a, L) : (H N L)\n        contract_L(V * u * v)\n        \"\"\"\n        expr = 'ComplexMult(ComplexMult(v, u), ComplexExp(ComplexMult(x, l)))'\n        vandermonde_mult = Genred(\n            expr,\n            [\n                'u = Vj(2)',\n                'v = Vi(2)',\n                'x = Vi(2)',\n                'l = Vj(2)',\n            ],\n            reduction_op='Sum',\n            axis=1,\n        )\n\n        l = torch.arange(L).to(x)\n        u, v, x, l = _broadcast_dims(u, v, x, l)\n        u = _c2r(u)\n        v = _c2r(v)\n        x = _c2r(x)\n        l = _c2r(l)\n\n        r = vandermonde_mult(u, v, x, l, backend='GPU')\n        return _r2c(r)\n\nexcept ImportError:\n    has_pykeops = False\n    print(\"Switch to torch vandermonde kernel.\")\n\nclass BiGSEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        self.register_buffer(\n            \"token_type_ids\",\n            torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n            persistent=False,\n        )\n\n    def forward(\n            self, input_ids=None, token_type_ids=None, position_ids=None,\n    ):\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, :seq_length]\n\n        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n        # issue #5664\n        if token_type_ids is None:\n            if hasattr(self, \"token_type_ids\"):\n                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n\n        inputs_embeds = self.word_embeddings(input_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\n# S4D Kernel module\nclass S4DKernel(nn.Module):\n    \"\"\"Wrapper around SSKernelDiag that generates the diagonal SSM parameters\n        dt_min, dt_max ==> initialize log steps\n    \"\"\"\n\n    def __init__(self, N=64, use_pykeops_kernel=False, dt_min=0.01, dt_max=1):\n        super().__init__()\n\n        log_step = log_step_initializer(1, dt_min, dt_max)\n        self.C = nn.Parameter(torch.normal(0, 0.5 ** 0.5, (N, 2)))\n\n        A_re = -0.5 * torch.ones(N)\n        A_im = math.pi * torch.arange(N)\n\n        self.register_parameter(\"log_step\", nn.Parameter(log_step))\n        self.register_parameter(\"A_re\", nn.Parameter(A_re))\n        self.register_parameter(\"A_im\", nn.Parameter(A_im))\n        self.use_pykeops_kernel = use_pykeops_kernel\n\n    def forward(self, L):\n        \"\"\"\n        returns: (..., c, L) where c is number of channels (default 1)\n        \"\"\"\n        # Materialize parameters\n        dt = torch.exp(self.log_step)  # (H)\n        A = torch.clamp(self.A_re, None, -1e-4) + 1j * self.A_im\n        C = (self.C[..., 0] + 1j * self.C[..., 1]).unsqueeze(0)\n\n        # Vandermonde multiplication\n        dtA = A * dt.unsqueeze(-1) # (H N)\n        C = C * (torch.exp(dtA)-1.) / A\n\n        if has_pykeops and self.use_pykeops_kernel:\n            K = log_vandermonde_keops(C, dtA, L)\n        else:\n            K = dtA.unsqueeze(-1) * torch.arange(L, device=dtA.device)  # (H N L)\n            K = torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n        \n        return K\n\n\n## S4D module\nclass S4dLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # the simplest ssm\n        self.N = config.num_ssm\n        self.D = nn.Parameter(torch.randn(1))\n        self.kernel = S4DKernel(N=self.N, use_pykeops_kernel=config.use_pykeops_kernel)\n\n    def forward(self, u):\n        \"\"\" Input shape (B, L, H) \"\"\"\n        \"\"\" Output shape (B, L, H) \"\"\"\n        # convert into (B H L)\n        u = u.transpose(-1, -2)\n        L = u.size(-1)  # u is the input\n        # Compute SSM Kernel\n        k = self.kernel(L=L)  # (H L)\n        # Convolution\n        k_f = torch.fft.rfft(k, n=2 * L)  # (H L)\n        u_f = torch.fft.rfft(u, n=2 * L)  # (B H L)\n        y = torch.fft.irfft(u_f * k_f, n=2 * L)[..., :L]  # (B H L)\n        # Compute D term in state space equation - essentially a skip connection\n        y = y + u * self.D\n        # convert back to B, H, L\n        y = y.transpose(-1, -2)\n        return y\n\n\nclass BiGSLayer(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.num_ssm = config.num_ssm\n        self.max_seq_length = config.max_position_embeddings\n        self.pre_norm = config.pre_norm\n        self.decode = config.decode\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        # ssm layers\n        self.fs4 = S4dLayer(config)\n        self.bs4 = S4dLayer(config)\n        # dense layers\n        self.dv = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.du_forward = nn.Linear(config.hidden_size, config.hidden_size)\n        self.du_backward = nn.Linear(config.hidden_size, config.hidden_size)\n        self.duc_forward = nn.Linear(config.hidden_size, config.hidden_size)\n        self.duc_backward =  nn.Linear(config.hidden_size, config.hidden_size)\n        self.dol = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.do = nn.Linear(config.intermediate_size, config.hidden_size)\n\n    def __call__(\n        self,\n        hidden_states\n    ):\n        hidden_residual = hidden_states\n        hidden_states =  self.LayerNorm(hidden_states)\n        # gating\n        v = nn.functional.gelu(self.dv(hidden_states))\n        u_forward = nn.functional.gelu(self.du_forward(hidden_states))\n        u_backward = nn.functional.gelu(self.du_backward(torch.flip(hidden_states, dims=[1])))\n        # s4 layers\n        fs4_output = self.fs4(u_forward)\n        bs4_output = self.bs4(u_backward)\n        # instead of sum, we use multiplication\n        uc_forward = self.duc_forward(fs4_output)\n        uc_backward = torch.flip(self.duc_backward(bs4_output), dims=[1])\n        hidden_states = self.do(nn.functional.gelu(self.dol(uc_forward * uc_backward)) * v)\n        hidden_states = hidden_residual + hidden_states\n        return hidden_states\n\n\nclass BiGSEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([BiGSLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n            self,\n            hidden_states,\n            attention_mask=None,\n            output_hidden_states=False,\n            return_dict=True,\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        \n        for i, layer_module in enumerate(self.layer):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                )\n            else:\n                hidden_states = layer_module(\n                    hidden_states,\n                )\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    all_hidden_states\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithNoAttention(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n        )\n",
    "description": null,
    "url": null
}