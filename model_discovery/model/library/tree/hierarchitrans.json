{
    "acronym": "hierarchitrans",
    "title": "Hierarchical Transformers Are More Efficient Language Models",
    "seed_ids": [
        "roformer",
        "funneltransformer",
        "routingtransformer",
        "reformer",
        "axialattn",
        "sparsetransformer",
        "transformerxl"
    ],
    "s2id": "231e768f0cd280faa0f725bb353262cb4fed08d1",
    "abstract": "Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.",
    "authors": [
        "Piotr Nawrot",
        "Szymon Tworkowski",
        "Micha\u0142 Tyrolski",
        "Lukasz Kaiser",
        "Yuhuai Wu",
        "Christian Szegedy",
        "H. Michalewski"
    ],
    "venue": "NAACL-HLT",
    "year": 2021,
    "tldr": "Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.",
    "citationCount": 40,
    "influentialCitationCount": 4,
    "code": "import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom einops import rearrange, reduce, repeat\nimport math\n\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef pad_to_multiple(tensor, multiple, dim = -1, value = 0):\n    seq_len = tensor.shape[dim]\n    m = seq_len / multiple\n    if m.is_integer():\n        return tensor\n    remainder = math.ceil(m) * multiple - seq_len\n    pad_offset = (0,) * (-1 - dim) * 2\n    return F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n\ndef cast_tuple(val, depth = 1):\n    return val if isinstance(val, tuple) else ((val,) * depth)\n\n# factory\n\ndef get_hourglass_transformer(\n    dim,\n    *,\n    depth,\n    shorten_factor,\n    attn_resampling,\n    updown_sample_type,\n    **kwargs\n):\n    assert isinstance(depth, int) or (isinstance(depth, tuple)  and len(depth) == 3), 'depth must be either an integer or a tuple of 3, indicating (pre_transformer_depth, <nested-hour-glass-config>, post_transformer_depth)'\n    assert not (isinstance(depth, int) and shorten_factor), 'there does not need to be a shortening factor when only a single transformer block is indicated (depth of one integer value)'\n\n    if isinstance(depth, int):\n        return Transformer(dim = dim, depth = depth, **kwargs)\n\n    return HourglassTransformer(dim = dim, depth = depth, shorten_factor = shorten_factor, attn_resampling = attn_resampling, updown_sample_type = updown_sample_type, **kwargs)\n\n# up and down sample classes\n\nclass NaiveDownsample(nn.Module):\n    def __init__(self, shorten_factor):\n        super().__init__()\n        self.shorten_factor = shorten_factor\n\n    def forward(self, x):\n        return reduce(x, 'b (n s) d -> b n d', 'mean', s = self.shorten_factor)\n\nclass NaiveUpsample(nn.Module):\n    def __init__(self, shorten_factor):\n        super().__init__()\n        self.shorten_factor = shorten_factor\n\n    def forward(self, x):\n        return repeat(x, 'b n d -> b (n s) d', s = self.shorten_factor)\n\nclass LinearDownsample(nn.Module):\n    def __init__(self, dim, shorten_factor):\n        super().__init__()\n        self.proj = nn.Linear(dim * shorten_factor, dim)\n        self.shorten_factor = shorten_factor\n\n    def forward(self, x):\n        x = rearrange(x, 'b (n s) d -> b n (s d)', s = self.shorten_factor)\n        return self.proj(x)\n\nclass LinearUpsample(nn.Module):\n    def __init__(self, dim, shorten_factor):\n        super().__init__()\n        self.proj = nn.Linear(dim, dim * shorten_factor)\n        self.shorten_factor = shorten_factor\n\n    def forward(self, x):\n        x = self.proj(x)\n        return rearrange(x, 'b n (s d) -> b (n s) d', s = self.shorten_factor)\n\n# classes\n\nclass PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs) + x\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        heads = 8,\n        dim_head = 64,\n        dropout = 0.,\n        causal = False\n    ):\n        super().__init__()\n        self.heads = heads\n        self.causal = causal\n        self.scale = dim_head ** -0.5\n        inner_dim = heads * dim_head\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, context = None, mask = None):\n        h, device = self.heads, x.device\n        kv_input = default(context, x)\n\n        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        q = q * self.scale\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        mask_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = rearrange(mask, 'b j -> b () () j')\n            sim = sim.masked_fill(~mask, mask_value)\n\n        if self.causal:\n            i, j = sim.shape[-2:]\n            mask = torch.ones(i, j, device = device, dtype = torch.bool).triu_(j - i + 1)\n            mask = rearrange(mask, 'i j -> () () i j')\n            sim = sim.masked_fill(mask, mask_value)\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n        return self.to_out(out)\n\ndef FeedForward(dim, mult = 4, dropout = 0.):\n    return nn.Sequential(\n        nn.Linear(dim, dim * mult),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        nn.Linear(dim * mult, dim)\n    )\n\n# transformer classes\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth,\n        causal = False,\n        heads = 8,\n        dim_head = 64,\n        attn_dropout = 0.,\n        ff_mult = 4,\n        ff_dropout = 0.,\n        norm_out = False\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNormResidual(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout, causal = causal)),\n                PreNormResidual(dim, FeedForward(dim, mult = ff_mult, dropout = ff_dropout))\n            ]))\n\n        self.norm = nn.LayerNorm(dim) if norm_out else nn.Identity()\n\n    def forward(self, x, context = None, mask = None):\n        for attn, ff in self.layers:\n            x = attn(x, context = context, mask = mask)\n            x = ff(x)\n\n        return self.norm(x)\n\nclass HourglassTransformer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth,\n        shorten_factor = 2,\n        attn_resampling = True,\n        updown_sample_type = 'naive',\n        heads = 8,\n        dim_head = 64,\n        causal = False,\n        norm_out = False\n    ):\n        super().__init__()\n        assert len(depth) == 3, 'depth should be a tuple of length 3'\n        assert updown_sample_type in {'naive', 'linear'}, 'downsample / upsample type must be either naive (average pool and repeat) or linear (linear projection and reshape)'\n\n        pre_layers_depth, valley_depth, post_layers_depth = depth\n\n        if isinstance(shorten_factor, (tuple, list)):\n            shorten_factor, *rest_shorten_factor = shorten_factor\n        elif isinstance(valley_depth, int):\n            shorten_factor, rest_shorten_factor = shorten_factor, None\n        else:\n            shorten_factor, rest_shorten_factor = shorten_factor, shorten_factor\n\n        transformer_kwargs = dict(\n            dim = dim,\n            heads = heads,\n            dim_head = dim_head\n        )\n\n        self.causal = causal\n        self.shorten_factor = shorten_factor\n\n        if updown_sample_type == 'naive':\n            self.downsample = NaiveDownsample(shorten_factor)\n            self.upsample   = NaiveUpsample(shorten_factor)\n        elif updown_sample_type == 'linear':\n            self.downsample = LinearDownsample(dim, shorten_factor)\n            self.upsample   = LinearUpsample(dim, shorten_factor)\n        else:\n            raise ValueError(f'unknown updown_sample_type keyword value - must be either naive or linear for now')\n\n        self.valley_transformer = get_hourglass_transformer(\n            shorten_factor = rest_shorten_factor,\n            depth = valley_depth,\n            attn_resampling = attn_resampling,\n            updown_sample_type = updown_sample_type,\n            causal = causal,\n            **transformer_kwargs\n        )\n\n        self.attn_resampling_pre_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n        self.attn_resampling_post_valley = Transformer(depth = 1, **transformer_kwargs) if attn_resampling else None\n\n        self.pre_transformer = Transformer(depth = pre_layers_depth, causal = causal, **transformer_kwargs)\n        self.post_transformer = Transformer(depth = post_layers_depth, causal = causal, **transformer_kwargs)\n        self.norm_out = nn.LayerNorm(dim) if norm_out else nn.Identity()\n\n    def forward(self, x, mask = None):\n        # b : batch, n : sequence length, d : feature dimension, s : shortening factor\n\n        s, b, n = self.shorten_factor, *x.shape[:2]\n\n        # top half of hourglass, pre-transformer layers\n\n        x = self.pre_transformer(x, mask = mask)\n\n        # pad to multiple of shortening factor, in preparation for pooling\n\n        x = pad_to_multiple(x, s, dim = -2)\n\n        if exists(mask):\n            padded_mask = pad_to_multiple(mask, s, dim = -1, value = False)\n\n        # save the residual, and for \"attention resampling\" at downsample and upsample\n\n        x_residual = x.clone()\n\n        # if autoregressive, do the shift by shortening factor minus one\n\n        if self.causal:\n            shift = s - 1\n            x = F.pad(x, (0, 0, shift, -shift), value = 0.)\n\n            if exists(mask):\n                padded_mask = F.pad(padded_mask, (shift, -shift), value = False)\n\n        # naive average pool\n\n        downsampled = self.downsample(x)\n\n        if exists(mask):\n            downsampled_mask = reduce(padded_mask, 'b (n s) -> b n', 'sum', s = s) > 0\n        else:\n            downsampled_mask = None\n\n        # pre-valley \"attention resampling\" - they have the pooled token in each bucket attend to the tokens pre-pooled\n\n        if exists(self.attn_resampling_pre_valley):\n            if exists(mask):\n                attn_resampling_mask = rearrange(padded_mask, 'b (n s) -> (b n) s', s = s)\n            else:\n                attn_resampling_mask = None\n\n            downsampled = self.attn_resampling_pre_valley(\n                rearrange(downsampled, 'b n d -> (b n) () d'),\n                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n                mask = attn_resampling_mask\n            )\n\n            downsampled = rearrange(downsampled, '(b n) () d -> b n d', b = b)\n\n        # the \"valley\" - either a regular transformer or another hourglass\n\n        x = self.valley_transformer(downsampled, mask = downsampled_mask)\n\n        valley_out = x.clone()\n\n        # naive repeat upsample\n\n        x = self.upsample(x)\n\n        # add the residual\n\n        x = x + x_residual\n\n        # post-valley \"attention resampling\"\n\n        if exists(self.attn_resampling_post_valley):\n            x = self.attn_resampling_post_valley(\n                rearrange(x, 'b (n s) d -> (b n) s d', s = s),\n                rearrange(valley_out, 'b n d -> (b n) () d')\n            )\n\n            x = rearrange(x, '(b n) s d -> b (n s) d', b = b)\n\n        # bring sequence back to original length, if it were padded for pooling\n\n        x = x[:, :n]\n\n        # post-valley transformers\n\n        x = self.post_transformer(x, mask = mask)\n        return self.norm_out(x)\n\n# main class\n\nclass HourglassTransformerLM(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        max_seq_len,\n        depth,\n        shorten_factor = None,\n        heads = 8,\n        dim_head = 64,\n        attn_resampling = True,\n        updown_sample_type = 'naive',\n        causal = True\n    ):\n        super().__init__()\n        self.max_seq_len = max_seq_len\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.transformer = get_hourglass_transformer(\n            dim = dim,\n            depth = depth,\n            shorten_factor = shorten_factor,\n            attn_resampling = attn_resampling,\n            updown_sample_type = updown_sample_type,\n            dim_head = dim_head,\n            heads = heads,\n            causal = causal,\n            norm_out = True\n        )\n\n        self.to_logits = nn.Linear(dim, num_tokens)\n\n    def forward(self, x, mask = None):\n        device = x.device\n        x = self.token_emb(x)\n        pos_emb = self.pos_emb(torch.arange(x.shape[-2], device = device))\n        x = x + rearrange(pos_emb, 'n d -> () n d')\n\n        x = self.transformer(x, mask = mask)\n        return self.to_logits(x)",
    "description": null,
    "url": null
}