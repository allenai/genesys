{
    "title": "HyperMixer: An MLP-based Low Cost Alternative to Transformers ",
    "acronym": "hypermixer",
    "s2id": "fb7e324729be2931dc463a572566283802d8aff2",
    "abstract": "Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": "This paper proposes a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks, and demonstrates that this model performs better than alternative MLP-based models, and on par with Transformers.",
    "citationCount": 7,
    "influentialCitationCount": 2,
    "seed_ids": [
        "metaformer",
        "gmlp",
        "fnet",
        "lineartransformer",
        "transformer"
    ],
    "code": null
}