{
    "acronym": "hypermixer",
    "title": "HyperMixer: An MLP-based Low Cost Alternative to Transformers ",
    "seed_ids": [
        "metaformer",
        "gmlp",
        "fnet",
        "lineartransformer",
        "transformer"
    ],
    "s2id": "fb7e324729be2931dc463a572566283802d8aff2",
    "abstract": "Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.",
    "authors": [
        "Florian Mai",
        "Arnaud Pannatier",
        "Fabio Fehr",
        "Haolin Chen",
        "Fran\u00e7ois Marelli",
        "F. Fleuret",
        "J. Henderson"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": "This paper proposes a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks, and demonstrates that this model performs better than alternative MLP-based models, and on par with Transformers.",
    "citationCount": 7,
    "influentialCitationCount": 2,
    "code": "# Copyright \u00a9 <2023> Idiap Research Institute <contact@idiap.ch>\n#\n# SPDX-FileContributor: Florian MAI <florian.ren.mai@googlemail.com>\n# SPDX-FileContributor: Arnaud Pannatier <arnaud.pannatier@idiap.ch>\n# SPDX-FileContributor: Fabio Fehr <fabio.fehr@idiap.ch>\n# SPDX-FileContributor: Juan Pablo Zuluaga <juan-pablo.zuluaga@idiap.ch>\n#\n# SPDX-License-Identifier: MIT\n\"\"\"This module mixes information from different tokens via HyperMixing.\n\nIt can be viewed as a linear-time drop-in replacement for (self-)attention.\n\nsource: https://arxiv.org/abs/2203.03691\n\nAuthors\n * Florian Mai 2023\n * Arnaud Pannatier 2023\n * Juan Pablo Zuluaga 2023\n * Fabio Fehr 2023\n\"\"\"\nimport math\nfrom typing import Callable, Optional\n\nimport torch\nfrom torch import nn\n\n\nclass HyperMixing(nn.Module):\n    \"\"\"This class implements multi-head HyperMixing.\n\n    It is an implementation of the token-mixing component in HyperMixer, a linear\n    time drop-in replacement for (self-)attention. In contrast to the original HyperMixer,\n    this module supports multiple heads, which improves the expressiveness of the model\n    while decreasing the number of parameters.\n\n    Reference: https://arxiv.org/abs/2203.03691\n\n    Example\n    -------\n    >>> inputs = torch.rand([8, 60, 512])\n    >>> net = HyperMixing(512, 2048, num_heads=8)\n    >>> outputs = net(inputs, inputs, inputs)\n    >>> outputs.shape\n    torch.Size([8, 60, 512])\n    \"\"\"\n\n    def __init__(\n        self,\n        input_output_dim: int,\n        hypernet_size: int,\n        tied: bool = False,\n        num_heads: int = 1,\n        max_length: int = 3000,\n        token_information: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n        use_layer_norm: bool = True,\n        hypernetwork_bias: bool = False,\n    ) -> None:\n        \"\"\"Initialize multi-head HyperMixing.\n\n        Arguments\n        ----------\n        input_output_dim : int\n            number of features in keys, queries, and values\n        hypernet_size : int\n            determines the size of the hidden layer of the token-mixing MLP.\n        tied : bool\n            If True, then the generated weight matrices of the token-mixing MLP are tied.\n        num_heads : int\n            parallel token-mixing MLPs.\n        max_length : int\n            Maximum number of input tokens. Needed for generating sufficiently large position embeddings.\n        token_information : Callable\n            Function that encodes token information. Defaults to positional encoding.\n        use_layer_norm : bool\n            If True, layer normalization is used.\n        \"\"\"\n        super().__init__()\n        self.input_output_dim = input_output_dim\n        self.hypernetwork_bias = hypernetwork_bias\n        self.hyper = _HyperNetwork(\n            input_output_dim,\n            hypernet_size,\n            tied=tied,\n            num_heads=num_heads,\n            keep_output_size=False,\n            hypernetwork_bias=hypernetwork_bias,\n        )\n        self.activation = nn.GELU()\n        self.num_heads = num_heads\n        self.use_layer_norm = use_layer_norm\n        if self.use_layer_norm:\n            self.layer_norm = nn.LayerNorm(input_output_dim)\n\n        # add pos encoding or a custom token_information function\n        if token_information is None:\n            self.token_information = _PositionalEncoding(input_output_dim, max_length)\n        else:\n            self.token_information = token_information\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        query_padding_mask: Optional[torch.Tensor] = None,\n        key_padding_mask: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Forward pass through HyperMixer.\n\n        Arguments\n        ----------\n        query : torch.Tensor\n            (B, M, E) where M is the number of queries, B is the batch size,\n            E is the embedding dimension.\n        key : torch.Tensor\n            (B, N, E) where N is the number of keys/values, B is the batch size,\n            E is the embedding dimension.\n        value : torch.Tensor\n            (B, N, E) where N is the number of keys/values, B is the batch size,\n            E is the embedding dimension.\n        query_padding_mask : torch.Tensor\n            (B, M) where B is the batch size, M is the number of queries.\n            If a ByteTensor is provided, the non-zero positions will be ignored\n            while the position with the zero positions will be unchanged. If a\n            BoolTensor is provided, the positions with the value of True will be\n            ignored while the position with the value of False will be unchanged.\n        key_padding_mask : torch.Tensor\n            (B, N) where B is the batch size, N is the number of keys/values.\n            If a ByteTensor is provided, the non-zero positions will be ignored\n            while the position with the zero positions will be unchanged. If a\n            BoolTensor is provided, the positions with the value of True will be\n            ignored while the position with the value of False will be unchanged.\n        Returns\n        -------\n        torch.Tensor\n            output tensor\n        \"\"\"\n        bsize = query.size(0)\n        query_len = query.size(1)\n        key_len = key.size(1)\n\n        if value.size(1) != key.size(1):\n            raise ValueError(\"Number of values not equal to number of keys!\")\n\n        if key_padding_mask is not None:\n            key_float_mask = torch.logical_not(key_padding_mask).unsqueeze(-1).float()\n            key = key * key_float_mask\n            value = value * key_float_mask\n        if query_padding_mask is not None:\n            query_float_mask = (\n                torch.logical_not(query_padding_mask).unsqueeze(-1).float()\n            )\n            query = query * query_float_mask\n\n        # add token information (like position) before passing to hypernetwork\n        hyp_in = self.token_information(key)\n        hyp_out = self.token_information(query)\n        # [bsize, num_heads, key_len/query_len, hypernet_size // num_heads]\n        W1, W2 = self.hyper(hyp_in, hyp_out)\n\n        # mask the weights\n        if key_padding_mask is not None:\n            W1 = W1 * key_float_mask.unsqueeze(1)\n        if query_padding_mask is not None:\n            W2 = W2 * query_float_mask.unsqueeze(1)\n\n        # reshape the num_heads into the batch dimension for parallelizing\n        value = value.transpose(1, 2)  # [bsize, input_output_dim, key_len]\n        value = value.reshape(\n            (bsize * self.num_heads, self.input_output_dim // self.num_heads, key_len)\n        )  # [bsize * num_heads, input_output_dim // num_heads, key_len]\n        W1 = W1.reshape((bsize * self.num_heads, key_len, -1))\n        W2 = W2.reshape((bsize * self.num_heads, query_len, -1))\n\n        # we stick the token-mixing MLP together manually\n        out = _mlp_pass_from_components(value, W1, W2, self.activation)\n\n        # concatenate heads\n        out = out.reshape((bsize, self.input_output_dim, query_len))\n\n        # transpose back\n        out = out.transpose(1, 2)\n\n        # apply layer norm on outputs of the TM-MLP\n        if self.use_layer_norm:\n            out = self.layer_norm(out)\n\n        return out\n\n\nclass _HyperNetwork(nn.Module):\n    def __init__(\n        self,\n        input_output_dim: int,\n        hypernet_size: int,\n        tied: bool = False,\n        num_heads: int = 1,\n        keep_output_size: bool = True,\n        hypernetwork_bias: bool = False,\n    ) -> None:\n        super().__init__()\n\n        self.tied = tied\n        self.w1_gen = _ParallelMLPs(\n            input_output_dim,\n            input_output_dim,\n            output_size=hypernet_size,\n            num_mlps=num_heads,\n            keep_output_size=keep_output_size,\n            bias=hypernetwork_bias,\n        )\n        if self.tied:\n            self.w2_gen = self.w1_gen\n        else:\n            self.w2_gen = _ParallelMLPs(\n                input_output_dim,\n                input_output_dim,\n                output_size=hypernet_size,\n                num_mlps=num_heads,\n                keep_output_size=keep_output_size,\n                bias=hypernetwork_bias,\n            )\n\n    def forward(self, w1_input: torch.Tensor, w2_input: torch.Tensor):\n        \"\"\"Forward pass through the HyperNetwork.\n\n        input_tensor : [batchsize, max_positions, d]\n        The HyperNetwork is supposed to generate an MLP of the form W_2(GELU(W1^T x)), where\n        W1 : N -> k and W2 : M -> k, so it has to return W1 and W2\n        \"\"\"\n        W1 = self.w1_gen(w1_input)\n        if self.tied:\n            if not torch.eq(w1_input, w2_input):\n                raise ValueError(\"Tied weights but queries != keys\")\n\n            W2 = W1\n        else:\n            W2 = self.w2_gen(w2_input)\n\n        return W1, W2\n\n\nclass _ParallelMLPs(nn.Module):\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        output_size: Optional[int] = None,\n        num_mlps: int = 1,\n        keep_output_size: bool = True,\n        bias: bool = False,\n    ) -> None:\n        super().__init__()\n\n        if output_size is None:\n            output_size = input_size\n\n        self.original_in_size = input_size\n        self.original_out_size = output_size\n\n        if input_size % num_mlps != 0:\n            raise ValueError(\"Input size must be divisible by num_mlps\")\n        if output_size % num_mlps != 0:\n            raise ValueError(\"Output size must be divisible by num_mlps\")\n        if hidden_size % num_mlps != 0:\n            raise ValueError(\"Hidden size must be divisible by num_mlps\")\n\n        input_size = input_size // num_mlps\n\n        if not keep_output_size:\n            output_size = output_size // num_mlps\n        hidden_size = hidden_size // num_mlps\n\n        self.input_size = input_size\n        self.output_size = output_size\n\n        self.num_mlps = num_mlps\n\n        self.fc1_weights = nn.Parameter(torch.empty(num_mlps, hidden_size, input_size))\n        self.fc2_weights = nn.Parameter(torch.empty(num_mlps, output_size, hidden_size))\n        nn.init.xavier_uniform_(self.fc1_weights, gain=math.sqrt(2.0))\n        nn.init.xavier_uniform_(self.fc2_weights, gain=math.sqrt(2.0))\n\n        self.bias = bias\n        if self.bias:\n            self.fc1_biases = nn.Parameter(torch.empty(num_mlps, hidden_size))\n            self.fc2_biases = nn.Parameter(torch.empty(num_mlps, output_size))\n            nn.init.xavier_uniform_(self.fc2_biases, gain=math.sqrt(2.0))\n            nn.init.xavier_uniform_(self.fc1_biases, gain=math.sqrt(2.0))\n\n        self.activation = nn.GELU()\n\n    def forward(self, x: torch.Tensor):\n        # x [bsize, seq_len, num_features]\n\n        bsize = x.size(0)\n        seq_len = x.size(1)\n\n        x = x.reshape((bsize, seq_len, self.num_mlps, self.input_size))\n        x = torch.einsum(\"blmf,mhf->bmlh\", x, self.fc1_weights)\n        if self.bias:\n            x = x + self.fc1_biases.unsqueeze(0).unsqueeze(2)\n        x = self.activation(x)\n        x = torch.einsum(\"bmlh,mfh->bmlf\", x, self.fc2_weights)\n        if self.bias:\n            x = x + self.fc2_biases.unsqueeze(0).unsqueeze(2)\n\n        return x\n\n\ndef _mlp_pass_from_components(\n    out, W1: torch.Tensor, W2: torch.Tensor, activation: nn.Module\n) -> torch.Tensor:\n    # we stick the token MLP together manually\n    out = torch.bmm(out, W1)\n    out = activation(out)\n    out = torch.bmm(out, W2.transpose(1, 2))\n    return out\n\n\nclass _PositionalEncoding(nn.Module):\n    \"\"\"Adds sinoidal position embeddings to the input.\"\"\"\n\n    def __init__(self, d_model: int, max_seq_len: int = 4001) -> None:\n        super().__init__()\n        self.d_model = d_model\n        pe = torch.zeros(max_seq_len, d_model)\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n                if i + 1 < d_model:\n                    pe[pos, i + 1] = math.cos(\n                        pos / (10000 ** ((2 * (i + 1)) / d_model))\n                    )\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x * math.sqrt(self.d_model)\n        seq_len = x.size(1)\n        pe = self.pe[:, 1 : seq_len + 1]\n        pe = pe.expand_as(x)\n        x = x + pe\n        return x",
    "description": null,
    "url": null
}