{
    "acronym": "xlstm",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "seed_ids": [],
    "s2id": "98372f2e164a4ae44c390a72a39bd6d7675cae89",
    "abstract": "In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.",
    "authors": [
        "Maximilian Beck",
        "Korbinian Poppel",
        "M. Spanring",
        "Andreas Auer",
        "Oleksandra Prudnikova",
        "Michael K Kopp",
        "G. Klambauer",
        "Johannes Brandstetter",
        "Sepp Hochreiter"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.",
    "citationCount": 14,
    "influentialCitationCount": 2,
    "code": "\"\"\"\nxLSTM Block Implementation\n\nThis module implements the xLSTM block as described in the paper:\n\"xLSTM: Extended Long Short-Term Memory\" by Beck et al. (2024).\n\nThe xLSTM block combines either sLSTM or mLSTM with layer normalization,\nresidual connections, and additional linear projections.\n\nAuthor: Mudit Bhargava\nDate: June 2024\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\n\n\"\"\"\nmLSTM: Matrix Long Short-Term Memory\n\nThis module implements the mLSTM (matrix LSTM) cell and layer as described in the paper:\n\"xLSTM: Extended Long Short-Term Memory\" by Beck et al. (2024).\n\nThe mLSTM extends the traditional LSTM by using a matrix memory state and exponential gating,\nallowing for enhanced storage capacities and improved performance on long-range dependencies.\n\nAuthor: Mudit Bhargava\nDate: June 2024\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass mLSTM(nn.Module):\n    \"\"\"\n    mLSTM layer implementation.\n\n    This layer applies multiple mLSTM cells in sequence, with optional dropout between layers.\n\n    Args:\n        input_size (int): Size of input features.\n        hidden_size (int): Size of hidden state.\n        num_layers (int): Number of mLSTM layers.\n        dropout (float, optional): Dropout probability between layers. Default: 0.0.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0):\n        super(mLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.layers = nn.ModuleList([mLSTMCell(input_size if i == 0 else hidden_size, hidden_size) \n                                     for i in range(num_layers)])\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, input_seq, hidden_state=None):\n        \"\"\"\n        Forward pass of the mLSTM layer.\n\n        Args:\n            input_seq (Tensor): Input sequence of shape (batch_size, seq_length, input_size).\n            hidden_state (tuple of Tensors, optional): Initial hidden state. Default: None.\n\n        Returns:\n            tuple: Output sequence and final hidden state.\n        \"\"\"\n        batch_size, seq_length, _ = input_seq.size()\n        \n        if hidden_state is None:\n            hidden_state = self.init_hidden(batch_size)\n        \n        outputs = []\n        for t in range(seq_length):\n            x = input_seq[:, t, :]\n            for layer_idx, layer in enumerate(self.layers):\n                h, C = hidden_state[layer_idx]\n                h, C = layer(x, (h, C))\n                hidden_state[layer_idx] = (h, C)\n                x = self.dropout_layer(h) if layer_idx < self.num_layers - 1 else h\n            outputs.append(x)\n        \n        return torch.stack(outputs, dim=1), hidden_state\n\n    def init_hidden(self, batch_size):\n        \"\"\"Initialize hidden state for all layers.\"\"\"\n        return [(torch.zeros(batch_size, self.hidden_size, device=self.layers[0].weight_ih.device),\n                 torch.zeros(batch_size, self.hidden_size, self.hidden_size, device=self.layers[0].weight_ih.device))\n                for _ in range(self.num_layers)]\n\nclass mLSTMCell(nn.Module):\n    \"\"\"\n    mLSTM cell implementation.\n\n    This cell uses a matrix memory state and exponential gating as described in the xLSTM paper.\n\n    Args:\n        input_size (int): Size of input features.\n        hidden_size (int): Size of hidden state.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size):\n        super(mLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.weight_ih = nn.Parameter(torch.randn(3 * hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.randn(3 * hidden_size, hidden_size))\n        self.bias = nn.Parameter(torch.randn(3 * hidden_size))\n        \n        self.W_q = nn.Linear(input_size, hidden_size)\n        self.W_k = nn.Linear(input_size, hidden_size)\n        self.W_v = nn.Linear(input_size, hidden_size)\n        \n        self.reset_parameters()\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters using Xavier uniform initialization.\"\"\"\n        nn.init.xavier_uniform_(self.weight_ih)\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.zeros_(self.bias)\n        nn.init.xavier_uniform_(self.W_q.weight)\n        nn.init.xavier_uniform_(self.W_k.weight)\n        nn.init.xavier_uniform_(self.W_v.weight)\n        nn.init.zeros_(self.W_q.bias)\n        nn.init.zeros_(self.W_k.bias)\n        nn.init.zeros_(self.W_v.bias)\n\n    def forward(self, input, hx):\n        \"\"\"\n        Forward pass of the mLSTM cell.\n\n        Args:\n            input (Tensor): Input tensor of shape (batch_size, input_size).\n            hx (tuple of Tensors): Previous hidden state and cell state.\n\n        Returns:\n            tuple: New hidden state and cell state.\n        \"\"\"\n        h, C = hx\n        gates = F.linear(input, self.weight_ih, self.bias) + F.linear(h, self.weight_hh)\n        \n        i, f, o = gates.chunk(3, 1)\n        \n        i = torch.exp(i)  # Exponential input gate\n        f = torch.exp(f)  # Exponential forget gate\n        o = torch.sigmoid(o)\n        \n        q = self.W_q(input)\n        k = self.W_k(input)\n        v = self.W_v(input)\n        \n        C = f.unsqueeze(2) * C + i.unsqueeze(2) * torch.bmm(v.unsqueeze(2), k.unsqueeze(1))\n        h = o * torch.bmm(q.unsqueeze(1), C).squeeze(1)\n        \n        return h, C\n    \n\"\"\"\nsLSTM: Scalar Long Short-Term Memory\n\nThis module implements the sLSTM (scalar LSTM) cell and layer as described in the paper:\n\"xLSTM: Extended Long Short-Term Memory\" by Beck et al. (2024).\n\nThe sLSTM extends the traditional LSTM by using exponential gating and a new memory mixing technique,\nallowing for improved performance on various sequence modeling tasks.\n\nAuthor: Mudit Bhargava\nDate: June 2024\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass sLSTM(nn.Module):\n    \"\"\"\n    sLSTM layer implementation.\n\n    This layer applies multiple sLSTM cells in sequence, with optional dropout between layers.\n\n    Args:\n        input_size (int): Size of input features.\n        hidden_size (int): Size of hidden state.\n        num_layers (int): Number of sLSTM layers.\n        dropout (float, optional): Dropout probability between layers. Default: 0.0.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0):\n        super(sLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.layers = nn.ModuleList([sLSTMCell(input_size if i == 0 else hidden_size, hidden_size) \n                                     for i in range(num_layers)])\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, input_seq, hidden_state=None):\n        \"\"\"\n        Forward pass of the sLSTM layer.\n\n        Args:\n            input_seq (Tensor): Input sequence of shape (batch_size, seq_length, input_size).\n            hidden_state (tuple of Tensors, optional): Initial hidden state. Default: None.\n\n        Returns:\n            tuple: Output sequence and final hidden state.\n        \"\"\"\n        batch_size, seq_length, _ = input_seq.size()\n        \n        if hidden_state is None:\n            hidden_state = self.init_hidden(batch_size)\n        \n        outputs = []\n        for t in range(seq_length):\n            x = input_seq[:, t, :]\n            for layer_idx, layer in enumerate(self.layers):\n                h, c = hidden_state[layer_idx]\n                h, c = layer(x, (h, c))\n                hidden_state[layer_idx] = (h, c)\n                x = self.dropout_layer(h) if layer_idx < self.num_layers - 1 else h\n            outputs.append(x)\n        \n        return torch.stack(outputs, dim=1), hidden_state\n\n    def init_hidden(self, batch_size):\n        \"\"\"Initialize hidden state for all layers.\"\"\"\n        return [(torch.zeros(batch_size, self.hidden_size, device=self.layers[0].weight_ih.device),\n                 torch.zeros(batch_size, self.hidden_size, device=self.layers[0].weight_ih.device))\n                for _ in range(self.num_layers)]\n\nclass sLSTMCell(nn.Module):\n    \"\"\"\n    sLSTM cell implementation.\n\n    This cell uses exponential gating as described in the xLSTM paper.\n\n    Args:\n        input_size (int): Size of input features.\n        hidden_size (int): Size of hidden state.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size):\n        super(sLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n        self.bias = nn.Parameter(torch.randn(4 * hidden_size))\n        \n        self.reset_parameters()\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters using Xavier uniform initialization.\"\"\"\n        nn.init.xavier_uniform_(self.weight_ih)\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, input, hx):\n        \"\"\"\n        Forward pass of the sLSTM cell.\n\n        Args:\n            input (Tensor): Input tensor of shape (batch_size, input_size).\n            hx (tuple of Tensors): Previous hidden state and cell state.\n\n        Returns:\n            tuple: New hidden state and cell state.\n        \"\"\"\n        h, c = hx\n        gates = F.linear(input, self.weight_ih, self.bias) + F.linear(h, self.weight_hh)\n        \n        i, f, g, o = gates.chunk(4, 1)\n        \n        i = torch.exp(i)  # Exponential input gate\n        f = torch.exp(f)  # Exponential forget gate\n        g = torch.tanh(g)\n        o = torch.sigmoid(o)\n        \n        c = f * c + i * g\n        h = o * torch.tanh(c)\n        \n        return h, c\n\nclass xLSTMBlock(nn.Module):\n    \"\"\"\n    xLSTM block implementation.\n\n    This block can use either sLSTM or mLSTM as its core, surrounded by\n    normalization, activation, and projection layers.\n\n    Args:\n        input_size (int): Size of input features.\n        hidden_size (int): Size of hidden state in LSTM.\n        num_layers (int): Number of LSTM layers.\n        dropout (float, optional): Dropout probability. Default: 0.0.\n        lstm_type (str, optional): Type of LSTM to use ('slstm' or 'mlstm'). Default: 'slstm'.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, lstm_type=\"slstm\"):\n        super(xLSTMBlock, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.lstm_type = lstm_type\n\n        if lstm_type == \"slstm\":\n            self.lstm = sLSTM(input_size, hidden_size, num_layers, dropout)\n        elif lstm_type == \"mlstm\":\n            self.lstm = mLSTM(input_size, hidden_size, num_layers, dropout)\n        else:\n            raise ValueError(f\"Invalid LSTM type: {lstm_type}\")\n\n        self.norm = nn.LayerNorm(hidden_size)\n        self.activation = nn.GELU()\n        self.dropout_layer = nn.Dropout(dropout)\n        self.proj = nn.Linear(hidden_size, input_size)\n\n    def forward(self, input_seq, hidden_state=None):\n        \"\"\"\n        Forward pass of the xLSTM block.\n\n        Args:\n            input_seq (Tensor): Input sequence of shape (batch_size, seq_length, input_size).\n            hidden_state (tuple of Tensors, optional): Initial hidden state. Default: None.\n\n        Returns:\n            tuple: Output sequence and final hidden state.\n        \"\"\"\n        lstm_output, hidden_state = self.lstm(input_seq, hidden_state)\n        output = self.activation(lstm_output)\n        output = self.norm(output)\n        output = self.proj(output)\n        output = self.dropout_layer(output + input_seq)  # Residual connection\n        return output, hidden_state\n    \n    \nclass xLSTM(nn.Module):\n    \"\"\"\n    xLSTM model implementation.\n\n    This model uses a combination of sLSTM and mLSTM blocks in a residual architecture.\n\n    Args:\n        vocab_size (int): Size of the vocabulary.\n        embedding_size (int): Size of the token embeddings.\n        hidden_size (int): Size of the hidden state in LSTM blocks.\n        num_layers (int): Number of LSTM layers in each block.\n        num_blocks (int): Number of xLSTM blocks.\n        dropout (float, optional): Dropout probability. Default: 0.0.\n        lstm_type (str, optional): Type of LSTM to use ('slstm' or 'mlstm'). Default: 'slstm'.\n    \"\"\"\n\n    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, num_blocks,\n                 dropout=0.0, lstm_type=\"slstm\"):\n        super(xLSTM, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.num_blocks = num_blocks\n        self.dropout = dropout\n        self.lstm_type = lstm_type\n\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.blocks = nn.ModuleList([\n            xLSTMBlock(embedding_size, hidden_size, num_layers, dropout, lstm_type)\n            for _ in range(num_blocks)\n        ])\n        self.output_layer = nn.Linear(embedding_size, vocab_size)\n\n    def forward(self, input_seq, hidden_states=None):\n        \"\"\"\n        Forward pass of the xLSTM model.\n\n        Args:\n            input_seq (Tensor): Input sequence of token indices.\n            hidden_states (list of tuples, optional): Initial hidden states for each block. Default: None.\n\n        Returns:\n            tuple: Output logits and final hidden states.\n        \"\"\"\n        embedded_seq = self.embedding(input_seq)\n        \n        if hidden_states is None:\n            hidden_states = [None] * self.num_blocks\n        \n        output_seq = embedded_seq\n        for i, block in enumerate(self.blocks):\n            output_seq, hidden_states[i] = block(output_seq, hidden_states[i])\n        \n        output_seq = self.output_layer(output_seq)\n        return output_seq, hidden_states",
    "description": null,
    "url": null
}