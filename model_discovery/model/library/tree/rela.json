{
    "acronym": "rela",
    "title": "Sparse Attention with Linear Units",
    "seed_ids": [
        "reformer",
        "sparsetransformer",
        "transformer"
    ],
    "s2id": "a7721b6523971394a8bd4bfda139122ef59b22cd",
    "abstract": "Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. \u2018switch off\u2019) for some queries, which is not possible with sparsified softmax alternatives.",
    "authors": [
        "Biao Zhang",
        "Ivan Titov",
        "Rico Sennrich"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": "This work introduces a novel, simple method for achieving sparsity in attention: it replaces the softmax activation with a ReLU, and shows that sparsity naturally emerges from such a formulation.",
    "citationCount": 30,
    "influentialCitationCount": 4,
    "code": "# coding: utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport tensorflow as tf\n\nimport func\nfrom models import model\nfrom utils import util, dtype\n\n\n# coding: utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport func\nfrom utils import util, dtype\n\n\ndef dot_attention(query, memory, mem_mask, hidden_size,\n                  ln=False, num_heads=1, cache=None, dropout=None,\n                  out_map=True, scope=None):\n    \"\"\"\n    dotted attention model\n    :param query: [batch_size, qey_len, dim]\n    :param memory: [batch_size, seq_len, mem_dim] or None\n    :param mem_mask: [batch_size, seq_len]\n    :param hidden_size: attention space dimension\n    :param ln: whether use layer normalization\n    :param num_heads: attention head number\n    :param dropout: attention dropout, default disable\n    :param out_map: output additional mapping\n    :param cache: cache-based decoding\n    :param scope:\n    :return: a value matrix, [batch_size, qey_len, mem_dim]\n    \"\"\"\n    with tf.variable_scope(scope or \"dot_attention\", reuse=tf.AUTO_REUSE,\n                           dtype=tf.as_dtype(dtype.floatx())):\n        if memory is None:\n            # suppose self-attention from queries alone\n            h = func.linear(query, hidden_size * 3, ln=ln, scope=\"qkv_map\")\n            q, k, v = tf.split(h, 3, -1)\n\n            if cache is not None:\n                k = tf.concat([cache['k'], k], axis=1)\n                v = tf.concat([cache['v'], v], axis=1)\n                cache = {\n                    'k': k,\n                    'v': v,\n                }\n        else:\n            q = func.linear(query, hidden_size, ln=ln, scope=\"q_map\")\n            if cache is not None and ('mk' in cache and 'mv' in cache):\n                k, v = cache['mk'], cache['mv']\n            else:\n                k = func.linear(memory, hidden_size, ln=ln, scope=\"k_map\")\n                v = func.linear(memory, hidden_size, ln=ln, scope=\"v_map\")\n\n            if cache is not None:\n                cache['mk'] = k\n                cache['mv'] = v\n\n        q = func.split_heads(q, num_heads)\n        k = func.split_heads(k, num_heads)\n        v = func.split_heads(v, num_heads)\n\n        q *= (hidden_size // num_heads) ** (-0.5)\n\n        # q * k => attention weights\n        logits = tf.matmul(q, k, transpose_b=True)\n\n        # convert the mask to 0-1 form and multiply to logits\n        if mem_mask is not None:\n            zero_one_mask = tf.to_float(tf.equal(mem_mask, 0.0))\n            logits *= zero_one_mask\n\n        # replace softmax with relu\n        # weights = tf.nn.softmax(logits)\n        weights = tf.nn.relu(logits)\n\n        dweights = util.valid_apply_dropout(weights, dropout)\n\n        # weights * v => attention vectors\n        o = tf.matmul(dweights, v)\n        o = func.combine_heads(o)\n\n        # perform RMSNorm to stabilize running\n        o = gated_rms_norm(o, scope=\"post\")\n\n        if out_map:\n            o = func.linear(o, hidden_size, ln=ln, scope=\"o_map\")\n\n        results = {\n            'weights': weights,\n            'output': o,\n            'cache': cache\n        }\n\n        return results\n\n\ndef gated_rms_norm(x, eps=None, scope=None):\n    \"\"\"RMS-based Layer normalization layer\"\"\"\n    if eps is None:\n        eps = dtype.epsilon()\n    with tf.variable_scope(scope or \"rms_norm\",\n                           dtype=tf.as_dtype(dtype.floatx())):\n        layer_size = util.shape_list(x)[-1]\n\n        scale = tf.get_variable(\"scale\", [layer_size], initializer=tf.ones_initializer())\n        gate = tf.get_variable(\"gate\", [layer_size], initializer=None)\n\n        ms = tf.reduce_mean(x ** 2, -1, keep_dims=True)\n\n        # adding gating here which slightly improves quality\n        return scale * x * tf.rsqrt(ms + eps) * tf.nn.sigmoid(gate * x)\n    \n\n\ndef decoder(target, state, params):\n    mask = dtype.tf_to_float(tf.cast(target, tf.bool))\n    hidden_size = params.hidden_size\n    initializer = tf.random_normal_initializer(0.0, hidden_size ** -0.5)\n\n    is_training = ('decoder' not in state)\n\n    if is_training:\n        target, mask = util.remove_invalid_seq(target, mask)\n\n    embed_name = \"embedding\" if params.shared_source_target_embedding \\\n        else \"tgt_embedding\"\n    tgt_emb = tf.get_variable(embed_name,\n                              [params.tgt_vocab.size(), params.embed_size],\n                              initializer=initializer)\n    tgt_bias = tf.get_variable(\"bias\", [params.embed_size])\n\n    inputs = tf.gather(tgt_emb, target) * (hidden_size ** 0.5)\n    inputs = tf.nn.bias_add(inputs, tgt_bias)\n\n    # shift\n    if is_training:\n        inputs = tf.pad(inputs, [[0, 0], [1, 0], [0, 0]])\n        inputs = inputs[:, :-1, :]\n        inputs = func.add_timing_signal(inputs)\n    else:\n        inputs = tf.cond(tf.reduce_all(tf.equal(target, params.tgt_vocab.pad())),\n                         lambda: tf.zeros_like(inputs),\n                         lambda: inputs)\n        mask = tf.ones_like(mask)\n        inputs = func.add_timing_signal(inputs, time=dtype.tf_to_float(state['time']))\n\n    inputs = util.valid_apply_dropout(inputs, params.dropout)\n\n    with tf.variable_scope(\"decoder\"):\n        x = inputs\n        for layer in range(params.num_decoder_layer):\n            if params.deep_transformer_init:\n                layer_initializer = tf.variance_scaling_initializer(\n                    params.initializer_gain * (layer + 1) ** -0.5,\n                    mode=\"fan_avg\",\n                    distribution=\"uniform\")\n            else:\n                layer_initializer = None\n            with tf.variable_scope(\"layer_{}\".format(layer), initializer=layer_initializer):\n                with tf.variable_scope(\"self_attention\"):\n                    y = dot_attention(\n                        x,\n                        None,\n                        func.attention_bias(tf.shape(mask)[1], \"causal\"),\n                        hidden_size,\n                        num_heads=params.num_heads,\n                        dropout=params.attention_dropout,\n                        cache=None if is_training else\n                        state['decoder']['state']['layer_{}'.format(layer)]\n                    )\n                    if not is_training:\n                        # k, v\n                        state['decoder']['state']['layer_{}'.format(layer)] \\\n                            .update(y['cache'])\n\n                    y = y['output']\n                    x = func.residual_fn(x, y, dropout=params.residual_dropout)\n                    x = func.layer_norm(x)\n\n                with tf.variable_scope(\"feed_forward\"):\n                    y = func.ffn_layer(\n                        x,\n                        params.filter_size,\n                        hidden_size,\n                        dropout=params.relu_dropout,\n                    )\n\n                    x = func.residual_fn(x, y, dropout=params.residual_dropout)\n                    x = func.layer_norm(x)\n    feature = x\n    if 'dev_decode' in state:\n        feature = x[:, -1, :]\n\n    embed_name = \"tgt_embedding\" if params.shared_target_softmax_embedding \\\n        else \"softmax_embedding\"\n    embed_name = \"embedding\" if params.shared_source_target_embedding \\\n        else embed_name\n    softmax_emb = tf.get_variable(embed_name,\n                                  [params.tgt_vocab.size(), params.embed_size],\n                                  initializer=initializer)\n    feature = tf.reshape(feature, [-1, params.embed_size])\n    logits = tf.matmul(feature, softmax_emb, False, True)\n\n    logits = tf.cast(logits, tf.float32)\n\n    soft_label, normalizer = util.label_smooth(\n        target,\n        util.shape_list(logits)[-1],\n        factor=params.label_smooth)\n    centropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=logits,\n        labels=soft_label\n    )\n    centropy -= normalizer\n    centropy = tf.reshape(centropy, tf.shape(target))\n\n    mask = tf.cast(mask, tf.float32)\n    per_sample_loss = tf.reduce_sum(centropy * mask, -1) / tf.reduce_sum(mask, -1)\n    loss = tf.reduce_mean(per_sample_loss)\n\n    # these mask tricks mainly used to deal with zero shapes, such as [0, 1]\n    loss = tf.cond(tf.equal(tf.shape(target)[0], 0),\n                   lambda: tf.constant(0, tf.float32),\n                   lambda: loss)\n\n    return loss, logits, state, per_sample_loss\n\n\n",
    "description": null,
    "url": null
}