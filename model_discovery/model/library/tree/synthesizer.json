{
    "acronym": "synthesizer",
    "title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "a238109c3969ae681eee0d4f1bf2012f28850593",
    "abstract": "The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \\textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).",
    "authors": [
        "Yi Tay",
        "Dara Bahri",
        "Donald Metzler",
        "Da-Cheng Juan",
        "Zhe Zhao",
        "Che Zheng"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": "The true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models is investigated and a model that learns synthetic attention weights without token-token interactions is proposed, called Synthesizer.",
    "citationCount": 70,
    "influentialCitationCount": 12,
    "code": "''' Define the sublayers in encoder/decoder layer '''\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom synthesizer.modules import ScaledDotProductAttention, DenseAttention\n\n__author__ = \"Tenzin Singhay Bhotia, Yu-Hsiang Huang\"\n\n\nclass RandomAttention(nn.Module):\n    def __init__(self, batch_size, n_head, max_seq_len, attn_dropout = 0.1):\n        super(RandomAttention, self).__init__()\n        #device = torch.device(\"GPU\"),\n        self.random_attn = torch.randn(batch_size, n_head, max_seq_len, max_seq_len, requires_grad = True)\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, v, len_q, mask=None):\n\n        # b x n x max_len x max_len -> b x n x lq x lq\n        random_attn = self.random_attn[:mask.shape[0],:,:len_q,:len_q]\n        random_attn = random_attn.to(torch.device('cuda' if mask.is_cuda else 'cpu'))\n\n        if mask is not None:\n            random_attn = random_attn.masked_fill(mask == 0, -1e9)\n\n        random_attn = self.dropout(F.softmax(random_attn, dim=-1))\n        output = torch.matmul(random_attn, v)\n        \n        return output, random_attn\n\nclass FactorizedRandomAttention(nn.Module):\n    def __init__(self, batch_size, n_head, f,  max_seq_len, attn_dropout = 0.1):\n        super(RandomAttention, self).__init__()\n        #device = torch.device(\"GPU\"),\n        self.random_attn_1 = torch.randn(batch_size, n_head, max_seq_len, f, requires_grad = True)\n        self.random_attn_2 = torch.randn(batch_size, n_head, f, max_seq_len, requires_grad = True)\n        self.dropout = nn.Dropout(attn_dropout)\n    def forward(self, v, len_q, mask=None, factorize=False):\n        # b x n x max_len x max_len -> b x n x lq x lq #[:,:,:len_q,:len_q]\n        random_attn = torch.matmul(self.random_attn_1, self.random_attn_2)[:mask.shape[0],:,:len_q,:len_q]\n\n        if mask is not None:\n            random_attn = random_attn.masked_fill(mask == 0, -1e9)\n\n        random_attn = self.dropout(F.softmax(random_attn, dim=-1))\n        output = torch.matmul(random_attn, v)\n        \n        return output, random_attn\n\nclass MultiHeadAttention(nn.Module):\n    ''' Multi-Head Attention module '''\n\n    def __init__(self, max_seq_len, batch_size, n_head, d_model, d_k, d_v, attn_type, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.batch_size = batch_size\n        self.max_seq_len = max_seq_len\n        \n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        \n        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n        self.attn_type = attn_type.lower()\n        self._init_attn()\n        \n        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n                \n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def _init_attn(self, ):\n        # print(self.attn_type)\n        if self.attn_type == \"vanilla\":\n            self.w_ks = nn.Linear(self.d_model, self.n_head * self.d_k, bias=False)\n            self.attention = ScaledDotProductAttention(temperature=self.d_k ** 0.5)\n        elif self.attn_type == \"dense\":\n            self.attention = DenseAttention(self.max_seq_len, self.d_k,)\n        elif self.attn_type == \"random\":\n            self.attention = RandomAttention(self.batch_size, self.n_head, self.max_seq_len,)\n\n\n    def forward(self, q, k, v, mask=None, factorize=False):\n\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n\n        residual = q\n\n        # Pass through the pre-attention projection: b x lq x (n*dv)\n        # Separate different heads: b x lq x n x dv\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        # Transpose for attention dot product: b x n x lq x dv\n        q, v = q.transpose(1, 2), v.transpose(1, 2)\n\n        # For head axis broadcasting.\n        if mask is not None:\n            mask = mask.unsqueeze(1) \n\n        # Attention type specific input pre-processing \n        \n        if self.attn_type == \"vanilla\":\n            k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n            k = k.transpose(1, 2)\n            q, attn = self.attention(q, k, v, mask=mask)\n\n        elif self.attn_type == \"dense\":\n            q, attn = self.attention(q, v, len_q, mask=mask)\n        elif self.attn_type == \"random\":\n            q, attn = self.attention(v, len_q, mask=mask)\n\n        # elif self.atten_type == \"Random\":\n\n        # elif self.atten_type == \"CNN\":\n\n\n        # Transpose to move the head dimension back: b x lq x n x dv\n        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n\n        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n        # print('q: ', q.shape)\n        # q = q.contiguous()\n        # print('q_2: ', q.shape)\n        # print('okkk: ', sz_b, len_q)\n        # q = q.view(sz_b, len_q, -1)\n        \n        q = self.dropout(self.fc(q))\n        q += residual\n\n        q = self.layer_norm(q)\n\n        return q, attn\n\n\nclass PositionwiseFeedForward(nn.Module):\n    ''' A two-feed-forward-layer module '''\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        residual = x\n\n        x = self.w_2(F.relu(self.w_1(x)))\n        x = self.dropout(x)\n        x += residual\n\n        x = self.layer_norm(x)\n\n        return x\n\nclass DecoderLayer(nn.Module):\n    ''' Compose with three layers '''\n\n    def __init__(self, max_seq_len, batch_size, d_model, d_inner, n_head, d_k, d_v, attn_type, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(max_seq_len, batch_size, n_head, d_model, d_k, d_v, attn_type, dropout=dropout)\n        self.enc_attn = MultiHeadAttention(max_seq_len, batch_size, n_head, d_model, d_k, d_v, attn_type='vanilla', dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(\n            self, dec_input, enc_output,\n            slf_attn_mask=None, dec_enc_attn_mask=None):\n        dec_output, dec_slf_attn = self.slf_attn(\n            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n        dec_output, dec_enc_attn = self.enc_attn(\n            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n        dec_output = self.pos_ffn(dec_output)\n        return dec_output, dec_slf_attn, dec_enc_attn",
    "description": null,
    "url": null
}