{
    "acronym": "srt",
    "title": "Self Reasoning Tokens",
    "seed_ids": [
        "gpt3"
    ],
    "s2id": null,
    "abstract": null,
    "authors": [
        "Felipe Sens Bonetto"
    ],
    "venue": null,
    "year": 2024,
    "tldr": null,
    "citationCount": null,
    "influentialCitationCount": null,
    "code": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import Module, ModuleList\n\nfrom einops import einsum, rearrange, repeat, reduce\nfrom einops.layers.torch import Rearrange\n\nfrom x_transformers import (\n    RMSNorm,\n    FeedForward\n)\n\n\nimport torch\nfrom torch.autograd.function import Function\n\nfrom einops import einsum, rearrange\n\ndef exists(val):\n    return val is not None\n\n# custom function\n\nclass StopGraddableAttentionFunction(Function):\n\n    @staticmethod\n    @torch.no_grad()\n    def forward(\n        ctx,\n        q,\n        k,\n        v,\n        mask,\n        attn_mask,\n        causal: bool,\n        q_stop_grad_mask,\n        k_stop_grad_mask,\n        v_stop_grad_mask,\n    ):\n        scale = q.shape[-1] ** -0.5\n\n        sim = einsum(q, k, 'b h i d, b h j d -> b h i j') * scale\n\n        max_neg_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            mask = rearrange(col_mask, 'b j -> b 1 1 j')\n            sim.masked_fill_(~mask, max_neg_value)\n\n        if exists(attn_mask):\n            sim.masked_fill_(~attn_mask, max_neg_value)\n\n        if causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones((i, j), dtype = torch.bool, device = sim.device).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, max_neg_value)\n\n        attn = sim.softmax(dim = -1)\n\n        out = einsum(attn, v, 'b h i j, b h j d -> b h i d')\n\n        ctx.args = (\n            causal,\n            scale,\n            mask,\n            q_stop_grad_mask,\n            k_stop_grad_mask,\n            v_stop_grad_mask\n        )\n\n        ctx.save_for_backward(\n            q, k, v,\n            attn,\n            out\n        )\n\n        return out\n\n    @staticmethod\n    @torch.no_grad()\n    def backward(ctx, do):\n\n        (\n            causal,\n            scale,\n            mask,\n            q_stop_grad_mask,\n            k_stop_grad_mask,\n            v_stop_grad_mask\n        ) = ctx.args\n\n        q, k, v, p, o = ctx.saved_tensors\n\n        # stop grad masks are either type bool, with True indicating stop grad, or can be type float, in which case it will scale the gradients\n\n        if q_stop_grad_mask.dtype == torch.bool:\n            q_stop_grad_mask = (~q_stop_grad_mask).float()\n\n        if k_stop_grad_mask.dtype == torch.bool:\n            k_stop_grad_mask = (~k_stop_grad_mask).float()\n\n        if v_stop_grad_mask.dtype == torch.bool:\n            v_stop_grad_mask = (~v_stop_grad_mask).float()\n\n        # softmax D\n\n        D = (do * o).sum(dim = -1, keepdims = True)        \n\n        # stop grad for values\n\n        p_v = p\n\n        if exists(v_stop_grad_mask):\n            p_v.mul_(v_stop_grad_mask)\n\n        # dv\n\n        dv = einsum(p_v, do, 'b h i j, b h i d -> b h j d')\n\n        # prep for dq and dk\n\n        dp = einsum(do, v, 'b h i d, b h j d -> b h i j')\n        ds = p * scale * (dp - D)\n\n        # handle stop grad masking for queries and keys\n\n        ds_q = ds_k = ds\n\n        if exists(q_stop_grad_mask):\n            ds_q.mul_(q_stop_grad_mask)\n\n        if exists(k_stop_grad_mask):            \n            ds_k.mul_(k_stop_grad_mask)\n\n        # dq and dk\n\n        dq = einsum(ds_q, k, 'b h i j, b h j d -> b h i d')\n        dk = einsum(ds_k, q, 'b h i j, b h i d -> b h j d')\n\n        return dq, dk, dv, None, None, None, None, None, None\n\n# convenience method with defaults\n\nstop_graddable_attn_ = StopGraddableAttentionFunction.apply\n\ndef stop_graddable_attn(\n    q, k, v,\n    mask = None,\n    attn_mask = None,\n    causal = False,\n    q_stop_grad_mask = None,\n    k_stop_grad_mask = None,\n    v_stop_grad_mask = None\n):\n    return stop_graddable_attn_(q, k, v, mask, attn_mask, causal, q_stop_grad_mask, k_stop_grad_mask, v_stop_grad_mask)\n\n\n# helper functions\n\ndef exists(v):\n    return v is not None\n\ndef default(v, d):\n    return v if exists(v) else d\n\n# attention\n\nclass CausalAttention(Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 64,\n        heads = 8\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        dim_inner = dim_head * heads\n\n        self.to_qkv = nn.Sequential(\n            RMSNorm(dim),\n            nn.Linear(dim, dim_inner * 3, bias = False),\n            Rearrange('b n (qkv h d) -> qkv b h n d', qkv = 3, h = heads)\n        )\n\n        self.to_out = nn.Sequential(\n            Rearrange('b h n d -> b n (h d)'),\n            nn.Linear(dim_inner, dim, bias = False)\n        )\n\n    def forward(\n        self,\n        x,\n        attn_mask = None,\n        stop_grad_attn_mask = None\n    ):\n        seq, device = x.shape[-2], x.device\n\n        q, k, v = self.to_qkv(x)\n\n        if exists(stop_grad_attn_mask):\n            if not isinstance(stop_grad_attn_mask, tuple):\n                stop_grad_attn_mask = (None, stop_grad_attn_mask, stop_grad_attn_mask)\n\n            assert len(stop_grad_attn_mask) == 3, 'stop_grad_attn_mask must be either a stop grad mask (implicit for key / values) or a tuple of 3 Tensor for individual stop grads of queries, keys, values'\n\n            q_stop_grad, k_stop_grad, v_stop_grad = stop_grad_attn_mask\n\n            out = stop_graddable_attn(\n                q, k, v,\n                attn_mask = attn_mask,\n                q_stop_grad_mask = q_stop_grad,\n                k_stop_grad_mask = k_stop_grad,\n                v_stop_grad_mask = v_stop_grad\n            )\n\n        else:\n            q = q * self.scale\n            sim = einsum(q, k, 'b h i d, b h j d -> b h i j')\n\n            causal_mask = torch.ones((seq, seq), device = device, dtype = torch.bool).triu(1)\n\n            mask_value = -torch.finfo(sim.dtype).max\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n            if exists(attn_mask):\n                sim = sim.masked_fill(~attn_mask, mask_value)\n\n            attn = sim.softmax(dim = -1)\n\n            out = einsum(attn, v, 'b h i j, b h j d -> b h i d')\n\n        # combine heads\n\n        return self.to_out(out)\n\n# transformer\n\nclass Transformer(Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        num_tokens,\n        depth,\n        max_seq_len = 2048,\n        max_reason_seq_len = 4,\n        dim_head = 64,\n        heads = 8,\n        ignore_index = -1,\n        stop_grad_next_tokens_to_reason = False\n    ):\n        super().__init__()\n        self.max_seq_len = max_seq_len\n\n        # embed\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        # reasoning tokens\n\n        self.max_reason_seq_len = max_reason_seq_len\n        self.reason_tokens = nn.Parameter(torch.randn(max_reason_seq_len, dim))\n        nn.init.normal_(self.reason_tokens, std = 0.02)\n\n        # transformer layers\n\n        self.layers = ModuleList([])\n        for _ in range(depth):\n\n            attn = CausalAttention(\n                dim = dim,\n                dim_head = dim_head,\n                heads = heads\n            )\n\n            ff = nn.Sequential(\n                RMSNorm(dim),\n                FeedForward(dim = dim)\n            )\n\n            self.layers.append(ModuleList([attn, ff]))\n\n        self.norm = RMSNorm(dim)\n        self.to_logits = nn.Linear(dim, num_tokens, bias = False)\n\n        # loss related\n\n        self.ignore_index = ignore_index\n\n        # stop gradient settings\n\n        self.stop_grad_next_tokens_to_reason = stop_grad_next_tokens_to_reason\n\n    def forward(\n        self,\n        x,\n        num_reason_tokens = 0,\n        num_steps_future_can_use_reason = 2,     # how many positions into the future until a reason token can be attended to\n        remove_reason_tokens_at_end = False,\n        return_loss = False\n    ):\n\n        if return_loss:\n            x, labels = x[:, :-1], x[:, 1:]\n\n        batch, seq, device = *x.shape, x.device\n\n        assert seq <= self.max_seq_len\n\n        x = self.token_emb(x)\n\n        seq_arange = torch.arange(seq, device = device)\n        pos = self.pos_emb(seq_arange)\n\n        attn_kwargs = dict()\n\n        # intersperse reasoning tokens if needed\n\n        has_reason_tokens = num_reason_tokens > 0\n\n        if has_reason_tokens:\n            assert num_reason_tokens <= self.max_reason_seq_len\n\n            x = rearrange(x, 'b n d -> b n 1 d')\n\n            reason_tokens = self.reason_tokens[:num_reason_tokens]\n            reason_tokens = repeat(reason_tokens, 'r d -> b n r d', b = batch, n = seq)\n\n            x = torch.cat((x, reason_tokens), dim = -2)\n            x = rearrange(x, 'b n r d -> b (n r) d')\n\n            # handle absolute positions\n            # applied axially to reasoning tokens and main token\n\n            num_tokens_per_timestep = num_reason_tokens + 1\n            pos = repeat(pos, 'n d -> (n r) d', r = num_tokens_per_timestep)\n\n            # handle masking for reasoning tokens\n            # each reason token can only be attended to by tokens (+ future reasoning tokens) that are {num_steps_future_can_use_reason}\n\n            seq_timesteps = repeat(seq_arange, 'n -> (n r)', r = num_tokens_per_timestep)\n\n            seq_with_reason_range = torch.arange(seq_timesteps.shape[-1], device = device)\n            is_reason_token_mask = ~(seq_with_reason_range % num_tokens_per_timestep == 0)\n\n            q_range = rearrange(seq_timesteps, 'n -> n 1')\n            k_range = rearrange(seq_timesteps, 'n -> 1 n')\n\n            attn_mask = ~(\n                is_reason_token_mask &\n                (q_range > k_range) &\n                ((q_range - num_steps_future_can_use_reason) <= k_range)\n            )\n\n            # whether to fully mask out or stop gradient on attention matrix\n\n            if self.stop_grad_next_tokens_to_reason:\n                attn_kwargs = dict(stop_grad_attn_mask = ~attn_mask)\n            else:\n                attn_kwargs = dict(attn_mask = attn_mask)\n\n        # attention and feedforward, passing in reason tokens mask from above\n\n        x = x + pos\n\n        for attn, ff in self.layers:\n            x = attn(x, **attn_kwargs) + x\n            x = ff(x) + x\n\n        embed = self.norm(x)\n\n        logits = self.to_logits(embed)\n\n        # whether to remove reason tokens at the very end\n\n        if has_reason_tokens and remove_reason_tokens_at_end:\n            logits = rearrange(logits, 'b (n r) c -> b n r c', r = num_tokens_per_timestep)\n            logits = logits[..., 0, :]\n\n        if not return_loss:\n            return logits\n\n        if has_reason_tokens and not remove_reason_tokens_at_end:\n            labels = repeat(labels, 'b n -> b (n r)', r = num_tokens_per_timestep)\n\n        loss = F.cross_entropy(\n            rearrange(logits, 'b n c -> b c n'),\n            labels,\n            ignore_index = self.ignore_index\n        )\n\n        return loss",
    "description": "The project \"Reasoning Tokens\" by Felipe Sens Bonetto aims to enhance the reasoning abilities of language models like GPT by teaching them to plan ahead in a self-supervised way. The core idea is to introduce \"reasoning tokens,\" where for each token predicted, an additional token is generated that duplicates the input and doesn't receive a gradient from the next token but from future tokens. This approach encourages the model to pre-cache information useful for future predictions. Initial experiments showed a significant reduction in loss, indicating improved performance. The project plans to explore this method further, especially in fine-tuned instruction-following models, potentially replacing the need for step-by-step explanations during training. The ultimate goal is to create models that can reason internally, improving their performance and reducing the need for manually crafted training data.",
    "url": "https://github.com/lucidrains/self-reasoning-tokens-pytorch"
}