{
    "acronym": "statefreeinf",
    "title": "State-Free Inference of State-Space Models: The Transfer Function Approach",
    "seed_ids": [
        "mamba",
        "flashfftconv",
        "hyenadistill",
        "h3",
        "s5",
        "s4d",
        "dssm",
        "s4",
        "gpt"
    ],
    "s2id": "101914972192c4fa0c7eab6ffc72f13d3ae1dde0",
    "abstract": "We approach designing a state-space model for deep learning applications through its dual representation, the transfer function, and uncover a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers -- parametrized in time-domain -- on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF.",
    "authors": [
        "Rom N. Parnichkun",
        "Stefano Massaroli",
        "Alessandro Moro",
        "Jimmy T.H. Smith",
        "Ramin M. Hasani",
        "Mathias Lechner",
        "Qi An",
        "Christopher R'e",
        "Hajime Asama",
        "Stefano Ermon",
        "Taiji Suzuki",
        "Atsushi Yamashita",
        "Michael Poli"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work uncovers a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size.",
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": "import torch\nimport torch.nn as nn\nfrom einops import rearrange, repeat\nimport math\nfrom torch.nn.functional import pad\nfrom torch.fft import rfft, irfft\nfrom functools import partial\ntry:\n    from flashfftconv import FlashFFTConv\n    imported_flash_fft_conv = True\nexcept:\n    imported_flash_fft_conv = False\n\nprint(\"Flash FFT Conv imported?\", imported_flash_fft_conv)\n\nclass RTF(nn.Module):\n    def  __init__(\n        self, \n        d_model: int,\n        state_size: int,\n        trunc_len: int,\n        num_a: int  = None,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n        flash_fft_conv: bool = False,\n        init: str = 'zeros',\n        constraint: str = 'no',\n    ):\n        \"\"\"\n        Args:\n            - d_model: Number of SISO channels.\n            - state_size: State size of SISO SSM.\n            - trunc_len: Truncation length (maximum length) for parallel inference.\n            - num_a: Number of unique set of denominator parameters (a). Must divide d_model, and if set to None, num_a => d_model.\n            - dropout: Dropout applied to the kernel.\n            - bidirectional: If set True, will process input signals with both a causal and an anti-causal SSM.\n            - flash_fft_conv: If set True, will use FlashFFTConv.\n            - init: Initialization function's name. (zeros, xavier, montel)\n            - constraint: Denominator constraint (to keep poles within unit circle). (no, l1_montel)\n        \"\"\"\n        super().__init__()\n\n        assert trunc_len > state_size, f\"Truncation length {trunc_len} must be larger than the state size {state_size}.\"\n\n        self.D = d_model\n        self.N = state_size \n        if(num_a == None):\n            self.num_a = d_model\n        else:\n            assert d_model%num_a == 0, \"num_a must divide d_model\"\n            self.num_a = num_a\n        self.L = trunc_len\n        self.bdir = bidirectional\n\n        init_fn = globals()[init+\"_init\"]\n        self.ab = nn.Parameter(init_fn((1+bidirectional)*(self.D + self.num_a), self.N)) # a, b parameters\n        self.h_0 = nn.Parameter(torch.randn((1+bidirectional)*self.D)) # h_0 parameter\n        self.a_channels = (1+bidirectional)*self.num_a\n\n        self.constraint_flag = False\n        if constraint is None:\n            constraint = \"no\"\n        a_constraint = globals()[constraint+\"_constraint\"]\n        if constraint in [\"l1_montel\"]:\n            self.scalar = torch.nn.Parameter(torch.rand(self.num_a))\n            self.a_constraint = partial(a_constraint, scalar=self.scalar)\n            self.constraint_flag = True\n        else:\n            self.a_constraint = a_constraint\n\n        self.dropout = torch.nn.Dropout(dropout)\n\n        if flash_fft_conv and imported_flash_fft_conv:\n            self.flash_fft_conv = FlashFFTConv(2*self.L, dtype=torch.bfloat16)\n        else:\n            self.flash_fft_conv = None\n        \n    def get_k(self, L=None):\n        \"\"\"\n        RTF kernel generation algorithm.\n        \"\"\"\n        if L is None:\n            L = self.L\n        assert L <= self.L\n        if self.constraint_flag:\n            ab = torch.cat((self.a, self.ab[self.a_channels:]), dim = 0)\n        else:\n            ab = self.ab\n        ab = pad(ab, (1, self.L-self.N-1+self.L%2))# zero padding params. +self.L%2 is rFFT specific\n        ab[:self.a_channels,0] = 1.0 # setting the monic term\n        AB = rfft(ab,dim=-1) # polynomial evaluation on points of unity\n        K = AB[self.a_channels:]/repeat(AB[:self.a_channels], \"D N -> (D R) N\", R=self.D//self.num_a) + self.h_0[:,None] # get kernel spectrum\n        k = irfft(K,dim=-1)[:,:L] # return time domain kernel\n        if self.bdir:\n            k = torch.cat((k[:self.D], k[self.D:].flip(-1)), dim=-1) # flip half of the kernels\n        return k\n        \n    def forward(self, u, **kwargs):\n        \"\"\"\n        u: (batch, length, channels)\n        \"\"\"\n        l = u.size(-2)\n        k = self.dropout(self.get_k(l))\n        self.k = k\n        # below this is functionally identical to s4/s4d\n        if self.flash_fft_conv is not None: \n            if self.bdir:\n                raise NotImplementedError(\"Strange behavior with FlashFFTConv, not allowing non-causal convolutions.\")\n            u = u.permute(0,2,1).to(torch.bfloat16).contiguous()\n            y = self.flash_fft_conv(u, k.to(torch.float32))\n            y = rearrange(y, \"B D L -> B L D\").to(u.dtype)\n        else:\n            if self.bdir:\n                u = rearrange(u, \"B L D -> (B D) L\")\n                u = pad(u, (0, l))\n                KU = rfft(torch.cat((k, u), dim=0), dim=-1)\n                Y = KU[:self.D].T*rearrange(KU[self.D:], \"(B D) L -> B L D\", D=self.D)\n                y = irfft(Y, dim=-2, n=2*l-l%2)[...,:l,:]\n            else:\n                u = rearrange(u, \"B L D -> L (B D)\")\n                KU = rfft(torch.cat((k.T,u),dim=1),n=2*l-l%2, dim=0)\n                U = rearrange(KU[:,self.D:], \"L (B D) -> B L D\", D=self.D)\n                Y = KU[:,:self.D]*U\n                y = irfft(Y, dim=-2, n=2*l-l%2)[:,:l]\n        return y\n        \n    def step(self, u, x_i, correct_C=True):\n        assert self.bdir == False\n        if correct_C:\n            c = self.get_C() # c can be cached \n        else:\n            c = self.ab[self.a_channels:]\n        a = repeat(self.a, \"D N -> (D R) N\", R=self.D//self.num_a).detach() # repeated a can be cached\n        y = torch.einsum(\"BNC,CN->BC\", x_i, c) + self.h_0*u\n        x_f = torch.roll(x_i, 1, 1)\n        x_f[:,0] = torch.einsum(\"CN,BNC->BC\",-a,x_i) + u\n        return y, x_f\n\n    @torch.no_grad()\n    def get_C(self):\n        \"\"\"\n        returns the corrected C matrix (AKA the numerator \"b\", for RTF)\n        \"\"\"\n        assert self.bdir == False\n        device = self.ab.device\n        N = self.N\n        A = torch.roll(torch.eye(self.N, device=device),1,0)\n        A = torch.clone(repeat(A, \"N M -> C N M\",C=self.num_a))\n        A[:,0] = -self.a # construct A matrix\n        I_AL = repeat(torch.eye(N, device=device) - torch.matrix_power(A, self.L), \"C N M -> (C R) N M\", R = self.D//self.num_a) # (I-A^L)\n        return torch.linalg.solve(I_AL, self.ab[self.a_channels:], left=True) # solves for C in, C_prime = C(I-A^L)\n        \n    def x_0(self, batch_shape, device=None):\n        return torch.zeros(batch_shape, self.N, self.D, device=device)\n    \n    def get_k_step(self, correct_C=True):\n        \"\"\"\n        Get the conv kernel recurrently. Used mainly for testing whether get_k() corresponds with get_k_step() or not.\n        \"\"\"\n        u = torch.zeros(1, self.L, self.D, device=self.ab.device)\n        u[0,0] = 1.0\n        x = self.x_0(1, device=self.ab.device)\n        k = []\n        for i in range(self.L):\n            k_, x = self.step(u[0:1, i], x, correct_C=correct_C)\n            k.append(k_)\n        return torch.cat(k, dim = -2).permute(1,0)\n    \n    @property\n    def a(self):\n        return self.a_constraint(self.ab[:self.a_channels])\n\ndef zeros_init(channels, order):\n    return torch.zeros(channels, order)\n\ndef xavier_init(channels, order): # xavier init can sometimes initialize an unstable system\n    stdv = 1. / math.sqrt(order)\n    return torch.FloatTensor(channels, order).uniform_(-stdv, stdv)\n\ndef montel_init(channels, order):\n    stdv = 1. / order\n    return torch.FloatTensor(channels, order).uniform_(-stdv, stdv)\n\ndef no_constraint(coefs, **kwargs):\n    return coefs\n\ndef l1_montel_constraint(coefs, scalar, **kwargs):\n    return coefs/(torch.sum(coefs.abs(), dim = -1) + scalar.abs() + 1e-6)[:,None]\n\nclass RTF2(nn.Module):\n    def  __init__(\n        self, \n        d_model: int,\n        state_size: int,\n        trunc_len: int,\n        num_a: int  = None,\n        dropout: float = 0.0,\n        bidirectional: bool = False,\n        flash_fft_conv: bool = False,\n        init: str = 'zeros',\n        constraint: str = 'no',\n    ):\n        \"\"\"RTF with numerator correction done on the convolutional mode. \n        Args:\n            - d_model: Number of SISO channels.\n            - state_size: State size of SISO SSM.\n            - trunc_len: Truncation length (maximum length) for parallel inference.\n            - num_a: Number of unique set of denominator parameters (a). Must divide d_model, and if set to None, num_a => d_model.\n            - dropout: Dropout applied to the kernel.\n            - bidirectional: If set True, will process input signals with both a causal and an anti-causal SSM.\n            - flash_fft_conv: If set True, will use FlashFFTConv.\n            - init: Initialization function's name. (zeros, xavier, montel)\n            - constraint: Denominator constraint (to keep poles within unit circle). (no, l1_montel)\n        \"\"\"\n        super().__init__()\n\n        assert trunc_len > state_size, f\"Truncation length {trunc_len} must be larger than the state size {state_size}.\"\n\n        self.D = d_model\n        self.N = state_size \n        if(num_a == None):\n            self.num_a = d_model\n        else:\n            assert d_model%num_a == 0, \"num_a must divide d_model\"\n            self.num_a = num_a\n        self.L = trunc_len\n        self.bdir = bidirectional\n\n        init_fn = globals()[init+\"_init\"]\n        self.ab = nn.Parameter(init_fn((1+bidirectional)*(self.D + self.num_a), self.N)) # a, b parameters\n        self.h_0 = nn.Parameter(torch.randn((1+bidirectional)*self.D)) # h_0 parameter\n        self.a_channels = (1+bidirectional)*self.num_a\n\n        self.constraint_flag = False\n        if constraint is None:\n            constraint = \"no\"\n        a_constraint = globals()[constraint+\"_constraint\"]\n        if constraint in [\"l1_montel\"]:\n            self.scalar = torch.nn.Parameter(torch.rand(self.num_a))\n            self.a_constraint = partial(a_constraint, scalar=self.scalar)\n            self.constraint_flag = True\n        else:\n            self.a_constraint = a_constraint\n\n        self.dropout = torch.nn.Dropout(dropout)\n\n        if flash_fft_conv and imported_flash_fft_conv:\n            self.flash_fft_conv = FlashFFTConv(2*self.L, dtype=torch.bfloat16)\n        else:\n            self.flash_fft_conv = None\n        \n    def get_k(self, L=None):\n        \"\"\"\n        RTF kernel generation algorithm.\n        \"\"\"\n        if L is None:\n            L = self.L\n        assert L <= self.L\n        a = pad(self.a, (1, self.L-self.N-1+self.L%2))\n        b = pad(self.get_C_prime(), (0, self.L-self.N+self.L%2))\n        ab = torch.cat((a, b), dim = 0)\n        ab[:self.a_channels,0] = 1.0 # setting the monic term\n        AB = rfft(ab,dim=-1) # polynomial evaluation on points of unity\n        K = AB[self.a_channels:]/repeat(AB[:self.a_channels], \"D N -> (D R) N\", R=self.D//self.num_a) + self.h_0[:,None] # get kernel spectrum\n        k = irfft(K,dim=-1)[:,:L] # return time domain kernel\n        if self.bdir:\n            k = torch.cat((k[:self.D], k[self.D:].flip(-1)), dim=-1) # flip half of the kernels\n        return k\n        \n    def forward(self, u, **kwargs):\n        \"\"\"\n        u: (batch, length, channels)\n        \"\"\"\n        l = u.size(-2)\n        k = self.dropout(self.get_k(l))\n        self.k = k\n        # below this is functionally identical to s4/s4d\n        if self.flash_fft_conv is not None: \n            if self.bdir:\n                raise NotImplementedError(\"Strange behavior with FlashFFTConv, not allowing non-causal convolutions.\")\n            u = u.permute(0,2,1).to(torch.bfloat16).contiguous()\n            y = self.flash_fft_conv(u, k.to(torch.float32))\n            y = rearrange(y, \"B D L -> B L D\").to(u.dtype)\n        else:\n            if self.bdir:\n                u = rearrange(u, \"B L D -> (B D) L\")\n                u = pad(u, (0, l))\n                KU = rfft(torch.cat((k, u), dim=0), dim=-1)\n                Y = KU[:self.D].T*rearrange(KU[self.D:], \"(B D) L -> B L D\", D=self.D)\n                y = irfft(Y, dim=-2, n=2*l-l%2)[...,:l,:]\n            else:\n                u = rearrange(u, \"B L D -> L (B D)\")\n                KU = rfft(torch.cat((k.T,u),dim=1),n=2*l-l%2, dim=0)\n                U = rearrange(KU[:,self.D:], \"L (B D) -> B L D\", D=self.D)\n                Y = KU[:,:self.D]*U\n                y = irfft(Y, dim=-2, n=2*l-l%2)[:,:l]\n        return y\n        \n    def step(self, u, x_i):\n        assert self.bdir == False\n        c = self.ab[self.a_channels:] # c can be cached \n        a = repeat(self.a, \"D N -> (D R) N\", R=self.D//self.num_a) # repeated a can be cached\n        x_f = torch.roll(x_i, 1, 1)\n        x_f[:,0] = torch.einsum(\"CN,BNC->BC\",-a,x_i) + u\n        y = torch.einsum(\"BNC,CN->BC\", x_f, c) + self.h_0*u\n        return y, x_f\n\n    def get_C_prime(self):\n        \"\"\"\n        returns the corrected C matrix (AKA the numerator \"b\", for RTF)\n        \"\"\"\n        device = self.ab.device\n        N = self.N\n        A = torch.roll(torch.eye(self.N, device=device),1,0)\n        A = torch.clone(repeat(A, \"N M -> C N M\",C=self.num_a*(1+self.bdir)))\n        A[:,0] = -self.a # construct A matrix\n        I_AL = repeat(torch.eye(N, device=device) - torch.matrix_power(A, self.L), \"C N M -> (C R) N M\", R = self.D//self.num_a) # (I-A^L)\n        return torch.einsum(\"CN,CNM->CM\", self.ab[self.a_channels:],I_AL)\n        \n    def x_0(self, batch_shape, device=None):\n        return torch.zeros(batch_shape, self.N, self.D, device=device)\n    \n    def get_k_step(self):\n        \"\"\"\n        Get the conv kernel recurrently. Used mainly for testing whether get_k() corresponds with get_k_step() or not.\n        \"\"\"\n        u = torch.zeros(1, self.L, self.D, device=self.ab.device)\n        u[0,0] = 1.0\n        x = self.x_0(1, device=self.ab.device)\n        k = []\n        for i in range(self.L):\n            k_, x = self.step(u[0:1, i], x)\n            k.append(k_)\n        return torch.cat(k, dim = -2).permute(1,0)\n    \n    @property\n    def a(self):\n        return self.a_constraint(self.ab[:self.a_channels])",
    "description": null,
    "url": null
}