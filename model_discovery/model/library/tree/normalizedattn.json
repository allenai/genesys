{
    "acronym": "normalizedattn",
    "title": "Normalized Attention Without Probability Cage",
    "seed_ids": [
        "transformer",
        "bert"
    ],
    "s2id": "71a72da632d55b4a9e12ca0b9e35bafe0466e318",
    "abstract": "Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.",
    "authors": [
        "Oliver Richter",
        "Roger Wattenhofer"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": "This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.",
    "citationCount": 18,
    "influentialCitationCount": 2,
    "code": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\n# helpers\n\ndef cum_mean(t):\n    device = t.device\n    running_num = torch.arange(t.shape[-1], device=t.device) + 1\n    return t.cumsum(dim=-1) / running_num\n\ndef normalize(t, eps=1e-8):\n    t -= t.mean(dim=-1, keepdim=True)\n    s = (t ** 2).mean(dim=-1, keepdim=True)\n    return t * torch.rsqrt(s + eps)\n\ndef causal_normalize(t, eps=1e-8):\n    t -= cum_mean(t).diagonal(dim1=-2, dim2=-1)[..., None]\n    s = cum_mean(t ** 2).diagonal(dim1=-2, dim2=-1)[..., None]\n    return t * torch.rsqrt(s + eps)\n\n# helper classes\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        return self.fn(x, *args, **kwargs) + x\n\nclass PostNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        x = self.fn(x)\n        return self.norm(x)\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        x = self.norm(x)\n        return self.fn(x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, causal = False, shared_kv = False):\n        super().__init__()\n        self.causal = causal\n        self.heads = heads\n        self.scale = dim ** -0.5\n        self.shared_kv = shared_kv\n        self.num_qkv = 3 if not shared_kv else 2\n\n        self.to_qkv = nn.Linear(dim, dim * self.num_qkv, bias = False)\n        self.to_out = nn.Linear(dim, dim)\n\n        self.norm_g = nn.Parameter(torch.ones(1, heads, 1, 1))\n        self.norm_b = nn.Parameter(torch.zeros(1, heads, 1, 1))\n\n    def forward(self, x):\n        b, n, _, h, device = *x.shape, self.heads, x.device\n        qkv = self.to_qkv(x)\n        qkv = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv = self.num_qkv, h = h)\n\n        if self.shared_kv:\n            q, k = qkv\n            v = k\n        else:\n            q, k, v = qkv\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n\n        if self.causal:\n            mask = torch.ones(n, n, device = device).triu_(1).bool()\n            dots.masked_fill_(mask, 0.)\n\n        normalize_fn = causal_normalize if self.causal else normalize\n        normed_attn = normalize_fn(dots)\n        attn = normed_attn * self.norm_g + self.norm_b\n\n        if self.causal:\n            attn.masked_fill_(mask, 0.)\n\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out =  self.to_out(out)\n        return out\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads = 8, causal = False, only_norm = False, shared_kv = False):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Residual(PostNorm(dim, Attention(dim, heads, causal = causal, shared_kv = shared_kv))),\n                Residual(PreNorm(dim, FeedForward(dim))) if not only_norm else nn.Identity(),\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x)\n            x = ff(x)\n        return x\n\nclass TransformerLM(nn.Module):\n    def __init__(self, *, num_tokens, dim, depth, max_seq_len, heads = 8, causal = False, only_norm = False, shared_kv = False):\n        super().__init__()\n        self.max_seq_len = max_seq_len\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n        self.transformer = Transformer(dim, depth, heads, causal = causal, only_norm = only_norm, shared_kv = shared_kv)\n        self.to_logits = nn.Linear(dim, num_tokens)\n\n    def forward(self, x, **kwargs):\n        _, n = x.shape\n        x = self.token_emb(x)\n        x += self.pos_emb(torch.arange(n, device=x.device))\n        x = self.transformer(x)\n        x = self.to_logits(x)\n        return x",
    "description": null,
    "url": null
}