{
    "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections",
    "acronym": "hippo",
    "s2id": "0964490205fdc38c2f0980c9d778069089ca92e3",
    "abstract": "A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "tldr": "This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.",
    "citationCount": 255,
    "influentialCitationCount": 36,
    "seed_ids": [
        "butterfly"
    ],
    "code": "\"\"\"Definitions of A and B matrices for various HiPPO operators.\n\n### HiPPO\n\nThe original HiPPO paper produced three main methods.\n1. LegT is the same as the prior method [LMU (Legendre Memory Unit)](https://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf). It defines specific formulas for A and B matrices in a time-invariant ODE $x'(t) = Ax(t) + Bu(t)$.\n2. LagT is another time-invariant ODE $x'(t) = Ax(t) + Bu(t)$ meant to memorize history according to a different weighting function.\n3. LegS was the main new method which produces a time-varying ODE $x'(t) = 1/t Ax(t) + 1/t Bu(t)$ meant to memorize history according to a uniform measure.\n\nThese methods were incorporated into a simple RNN called HiPPO-RNN where the measures $(A, B)$ were non-trainable.\n\n### HiPPO\nThe core HiPPO methods are just a set of equations and not end-to-end models.\nThe specific matrices are implemented in [[/src/models/hippo/hippo.py](/src/models/hippo/hippo.py)].\nThe connection between HiPPO/S4 matrices $(A, B)$ and convolution kernels is illustrated in [[/notebooks/ssm_kernels.ipynb](/notebooks/ssm_kernels.ipynb)].\nThe *online function reconstruction* theory is illustrated in [[/notebooks/hippo_function_approximation.ipynb](/notebooks/hippo_function_approximation.ipynb)].\nThe animation code can also be found in a [.py file](/src/models/hippo/visualizations.py) instead of notebook.\n\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom scipy import special as ss\nfrom einops import rearrange, repeat\n\ncontract = torch.einsum\n\n\ndef embed_c2r(A):\n    A = rearrange(A, '... m n -> ... m () n ()')\n    A = np.pad(A, ((0, 0), (0, 1), (0, 0), (0, 1))) + \\\n        np.pad(A, ((0, 0), (1, 0), (0, 0), (1,0)))\n    return rearrange(A, 'm x n y -> (m x) (n y)')\n\n# TODO take in 'torch' option to return torch instead of numpy, and converts the shape of B from (N, 1) to (N)\n# TODO remove tlagt\ndef transition(measure, N, **measure_args):\n    \"\"\"A, B transition matrices for different measures.\n\n    measure: the type of measure\n      legt - Legendre (translated)\n      legs - Legendre (scaled)\n      glagt - generalized Laguerre (translated)\n      lagt, tlagt - previous versions of (tilted) Laguerre with slightly different normalization\n    \"\"\"\n    # Laguerre (translated)\n    if measure == 'lagt':\n        b = measure_args.get('beta', 1.0)\n        A = np.eye(N) / 2 - np.tril(np.ones((N, N)))\n        B = b * np.ones((N, 1))\n    elif measure == 'tlagt':\n        # beta = 1 corresponds to no tilt\n        b = measure_args.get('beta', 1.0)\n        A = (1.-b)/2 * np.eye(N) - np.tril(np.ones((N, N)))\n        B = b * np.ones((N, 1))\n    # Generalized Laguerre\n    # alpha 0, beta small is most stable (limits to the 'lagt' measure)\n    # alpha 0, beta 1 has transition matrix A = [lower triangular 1]\n    elif measure == 'glagt':\n        alpha = measure_args.get('alpha', 0.0)\n        beta = measure_args.get('beta', 0.01)\n        A = -np.eye(N) * (1 + beta) / 2 - np.tril(np.ones((N, N)), -1)\n        B = ss.binom(alpha + np.arange(N), np.arange(N))[:, None]\n\n        L = np.exp(.5 * (ss.gammaln(np.arange(N)+alpha+1) - ss.gammaln(np.arange(N)+1)))\n        A = (1./L[:, None]) * A * L[None, :]\n        B = (1./L[:, None]) * B * np.exp(-.5 * ss.gammaln(1-alpha)) * beta**((1-alpha)/2)\n    # Legendre (translated)\n    elif measure == 'legt':\n        Q = np.arange(N, dtype=np.float64)\n        R = (2*Q + 1) ** .5\n        j, i = np.meshgrid(Q, Q)\n        A = R[:, None] * np.where(i < j, (-1.)**(i-j), 1) * R[None, :]\n        B = R[:, None]\n        A = -A\n\n        # Halve again for timescale correctness\n        A *= 0.5\n        B *= 0.5\n    # LMU: equivalent to LegT up to normalization\n    elif measure == 'lmu':\n        Q = np.arange(N, dtype=np.float64)\n        R = (2*Q + 1)[:, None] # / theta\n        j, i = np.meshgrid(Q, Q)\n        A = np.where(i < j, -1, (-1.)**(i-j+1)) * R\n        B = (-1.)**Q[:, None] * R\n    # Legendre (scaled)\n    elif measure == 'legs':\n        q = np.arange(N, dtype=np.float64)\n        col, row = np.meshgrid(q, q)\n        r = 2 * q + 1\n        M = -(np.where(row >= col, r, 0) - np.diag(q))\n        T = np.sqrt(np.diag(2 * q + 1))\n        A = T @ M @ np.linalg.inv(T)\n        B = np.diag(T)[:, None]\n        B = B.copy() # Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)\n    elif measure == 'legsd':\n        q = np.arange(N, dtype=np.float64)\n        col, row = np.meshgrid(q, q)\n        r = 2 * q + 1\n        M = -(np.where(row >= col, r, 0) - np.diag(q))\n        T = np.sqrt(np.diag(2 * q + 1))\n        A = T @ M @ np.linalg.inv(T)\n        B = np.diag(T)[:, None]\n        B = B.copy() # Otherwise \"UserWarning: given NumPY array is not writeable...\" after torch.as_tensor(B)\n        A += .5 * B*B[None, :, 0]\n        B = B / 2.0\n    elif measure in ['fourier_diag', 'foud']:\n        freqs = np.arange(N//2)\n        d = np.stack([freqs, np.zeros(N//2)], axis=-1).reshape(-1)[:-1]\n        A = 2*np.pi*(-np.diag(d, 1) + np.diag(d, -1))\n        A = A - .5 * np.eye(N)\n        B = np.zeros(N)\n        B[0::2] = 2**.5\n        B[0] = 1\n        B = B[:, None]\n    elif measure in ['fourier', 'fout']:\n        freqs = np.arange(N//2)\n        d = np.stack([np.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n        A = np.pi*(-np.diag(d, 1) + np.diag(d, -1))\n        B = np.zeros(N)\n        B[0::2] = 2**.5\n        B[0] = 1\n\n        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n        A = A - B[:, None] * B[None, :]\n        B = B[:, None]\n    elif measure == 'fourier_decay':\n        freqs = np.arange(N//2)\n        d = np.stack([np.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n        A = np.pi*(-np.diag(d, 1) + np.diag(d, -1))\n        B = np.zeros(N)\n        B[0::2] = 2**.5\n        B[0] = 1\n\n        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n        A = A - .5 * B[:, None] * B[None, :]\n        B = .5 * B[:, None]\n    elif measure == 'fourier2': # Double everything: orthonormal on [0, 1]\n        freqs = 2*np.arange(N//2)\n        d = np.stack([np.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]\n        A = np.pi*(-np.diag(d, 1) + np.diag(d, -1))\n        B = np.zeros(N)\n        B[0::2] = 2**.5\n        B[0] = 1\n\n        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case\n        A = A - B[:, None] * B[None, :] * 2\n        B = B[:, None] * 2\n    elif measure == 'random':\n        A = np.random.randn(N, N) / N\n        B = np.random.randn(N, 1)\n    elif measure == 'diagonal':\n        A = -np.diag(np.exp(np.random.randn(N)))\n        B = np.random.randn(N, 1)\n    else:\n        raise NotImplementedError\n\n    return A, B\n\ndef rank_correction(measure, N, rank=1, dtype=torch.float):\n    \"\"\"Return low-rank matrix L such that A + L is normal.\"\"\"\n\n    if measure == 'legs':\n        assert rank >= 1\n        P = torch.sqrt(.5+torch.arange(N, dtype=dtype)).unsqueeze(0) # (1 N)\n    elif measure == 'legt':\n        assert rank >= 2\n        P = torch.sqrt(1+2*torch.arange(N, dtype=dtype)) # (N)\n        P0 = P.clone()\n        P0[0::2] = 0.\n        P1 = P.clone()\n        P1[1::2] = 0.\n        P = torch.stack([P0, P1], dim=0) # (2 N)\n        P *= 2**(-0.5) # Halve the rank correct just like the original matrix was halved\n    elif measure == 'lagt':\n        assert rank >= 1\n        P = .5**.5 * torch.ones(1, N, dtype=dtype)\n    elif measure in ['fourier', 'fout']:\n        P = torch.zeros(N)\n        P[0::2] = 2**.5\n        P[0] = 1\n        P = P.unsqueeze(0)\n    elif measure == 'fourier_decay':\n        P = torch.zeros(N)\n        P[0::2] = 2**.5\n        P[0] = 1\n        P = P.unsqueeze(0)\n        P = P / 2**.5\n    elif measure == 'fourier2':\n        P = torch.zeros(N)\n        P[0::2] = 2**.5\n        P[0] = 1\n        P = 2**.5 * P.unsqueeze(0)\n    elif measure in ['fourier_diag', 'foud', 'legsd']:\n        P = torch.zeros(1, N, dtype=dtype)\n    else: raise NotImplementedError\n\n    d = P.size(0)\n    if rank > d:\n        P = torch.cat([P, torch.zeros(rank-d, N, dtype=dtype)], dim=0)  # (R N)\n    return P\n\ndef initial_C(measure, N, dtype=torch.float):\n    \"\"\"Return C that captures the other endpoint in the HiPPO approximation.\"\"\"\n\n    if measure == 'legt':\n        C = (torch.arange(N, dtype=dtype)*2+1)**.5 * (-1)**torch.arange(N)\n    elif measure == 'fourier':\n        C = torch.zeros(N)\n        C[0::2] = 2**.5\n        C[0] = 1\n    else:\n        C = torch.zeros(N, dtype=dtype) # (N)\n\n    return C\n\n\ndef nplr(measure, N, rank=1, dtype=torch.float, diagonalize_precision=True, B_clip=2.0):\n    \"\"\"Constructs NPLR form of HiPPO matrices.\n\n    Returns w, p, q, V, B such that\n    (w - p q^*, B) is unitarily equivalent to the original HiPPO A, B by the matrix V\n    i.e. A = V[w - p q^*]V^*, B = V B\n\n    measure: Name of HiPPO method.\n    N: Size of recurrent A matrix (also known as `d_state` elsewhere).\n    dtype: Single or double precision.\n    diagonalize_precision: Calculate diagonalization in double precision.\n    B_clip: Clip values of B, can help with stability. None for no clipping.\n    \"\"\"\n\n    assert dtype == torch.float or dtype == torch.double\n    cdtype = torch.cfloat if dtype == torch.float else torch.cdouble\n\n    A, B = transition(measure, N)\n    A = torch.as_tensor(A, dtype=dtype) # (N, N)\n    B = torch.as_tensor(B, dtype=dtype)[:, 0] # (N,)\n\n    P = rank_correction(measure, N, rank=rank, dtype=dtype) # (r N)\n    AP = A + torch.sum(P.unsqueeze(-2)*P.unsqueeze(-1), dim=-3)\n\n    # We require AP to be nearly skew-symmetric\n    _A = AP + AP.transpose(-1, -2)\n    if (err := torch.sum((_A - _A[0,0]*torch.eye(N))**2) / N) > 1e-5: # if not torch.allclose(_A - _A[0,0]*torch.eye(N), torch.zeros(N, N), atol=1e-5):\n        print(\"WARNING: HiPPO matrix not skew symmetric\", err)\n\n\n    # Take advantage of identity + skew-symmetric form to calculate real and imaginary parts separately\n    # Imaginary part can use eigh instead of eig\n    W_re = torch.mean(torch.diagonal(AP), -1, keepdim=True)\n\n    # Diagonalize in double precision\n    if diagonalize_precision: AP = AP.to(torch.double)\n    # w, V = torch.linalg.eig(AP) # (..., N) (..., N, N)\n    W_im, V = torch.linalg.eigh(AP*-1j) # (..., N) (..., N, N)\n    if diagonalize_precision: W_im, V = W_im.to(cdtype), V.to(cdtype)\n    W = W_re + 1j * W_im\n    # Check: V W V^{-1} = A\n    # print(\"check\", V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2))\n\n\n    # Only keep half of each conjugate pair\n    _, idx = torch.sort(W.imag)\n    W_sorted = W[idx]\n    V_sorted = V[:, idx]\n\n    # There is an edge case when eigenvalues can be 0, which requires some machinery to handle\n    # We use a huge hack here: Assume only one pair is 0, and that it is the first row/column of A (only happens in Fourier case)\n    V = V_sorted[:, :N//2]\n    W = W_sorted[:N//2]  # Only keep negative imaginary components\n    assert W[-2].abs() > 1e-4, \"Only 1 zero eigenvalue allowed in diagonal part of A\"\n    if W[-1].abs() < 1e-4:\n        V[:, -1] = 0.\n        V[0, -1] = 2**-0.5\n        V[1, -1] = 2**-0.5 * 1j\n\n    _AP = V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2)\n    if ((err := torch.sum((2*_AP.real-AP)**2)/N) > 1e-5):\n        print(\"Warning: Diagonalization of A matrix not numerically precise - error\", err)\n    # print(\"check\", V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2))\n\n    V_inv = V.conj().transpose(-1, -2)\n\n    # C = initial_C(measure, N, dtype=dtype)\n    B = contract('ij, j -> i', V_inv, B.to(V)) # V^* B\n    # C = contract('ij, j -> i', V_inv, C.to(V)) # V^* C\n    P = contract('ij, ...j -> ...i', V_inv, P.to(V)) # V^* P\n\n    if B_clip is not None:\n        B = B.real + 1j*torch.clamp(B.imag, min=-B_clip, max=B_clip)\n\n    # W represents the imaginary part of the DPLR form: A = W - PP^*\n    # Downstream classes just call this A for simplicity,\n    # which is also more consistent with the diagonal case\n    return W, P, B, V"
}