{
    "acronym": "funneltransformer",
    "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing",
    "seed_ids": [
        "transformerxl",
        "bert"
    ],
    "s2id": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09",
    "abstract": "With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at this https URL.",
    "authors": [
        "Zihang Dai",
        "Guokun Lai",
        "Yiming Yang",
        "Quoc V. Le"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "tldr": "This work proposes Funnel-Transformer, a model which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost and outperforms the standard Transformer on a wide variety of sequence-level prediction tasks.",
    "citationCount": 199,
    "influentialCitationCount": 28,
    "code": "\"\"\"Funnel-Transformer.\"\"\"\n\nimport json\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom ops import LayerNorm\nfrom ops import EmbeddindLookup\nfrom ops import PositionwiseFFN\nfrom ops import Dense,INF\n\n\n\ndef get_einsum_string(ndims, einsum_symbols=None):\n  if einsum_symbols is None:\n    einsum_symbols = [\"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n  assert ndims <= len(einsum_symbols)\n  einsum_prefix = \"\"\n  for i in range(ndims):\n    einsum_prefix += einsum_symbols[i]\n\n  return einsum_prefix\n\nclass RelMultiheadAttention(nn.Module):\n  \"\"\"Relative multi-head attention.\"\"\"\n\n  def __init__(self, net_config, args, d_model, n_head, d_head, dropout,\n               dropatt, bidx):\n    super(RelMultiheadAttention, self).__init__()\n\n    self.net_config = net_config\n    self.args = args\n    self.attn_type = args.attn_type\n    self.bidx = bidx\n\n    self.d_model = d_model\n    self.n_head = n_head\n    self.d_head = d_head\n\n    self.dropout = dropout\n    self.dropatt = dropatt\n\n    self.att_drop = nn.Dropout(self.dropatt)\n    self.hid_drop = nn.Dropout(self.dropout)\n\n    self.q_head = Dense(d_model, [n_head, d_head], bias=False,)\n    self.k_head = Dense(d_model, [n_head, d_head])\n    self.v_head = Dense(d_model, [n_head, d_head])\n\n    self.r_w_bias = nn.Parameter(torch.zeros([n_head, d_head]))\n    self.r_r_bias = nn.Parameter(torch.zeros([n_head, d_head]))\n    self.r_kernel = nn.Parameter(torch.zeros([d_model, n_head, d_head]))\n    self.r_s_bias = nn.Parameter(torch.zeros([n_head, d_head]))\n    self.seg_embed = nn.Parameter(torch.zeros([2, n_head, d_head]))\n\n    self.post_proj = Dense([n_head, d_head], d_model,)\n    self.layer_norm = LayerNorm(d_model)\n    self.scale = 1. / np.sqrt(d_head)\n    self.reset_parameters()\n\n  def reset_parameters(self):\n    nn.init.uniform_(self.r_w_bias, b=0.1)\n    nn.init.uniform_(self.r_r_bias, b=0.1)\n    nn.init.uniform_(self.r_kernel, b=0.1)\n    nn.init.uniform_(self.r_s_bias, b=0.1)\n    nn.init.uniform_(self.seg_embed, b=0.1)\n\n  def rel_shift(self, x, row_axis, key_len, shift=1):\n    \"\"\"Perform relative shift to form the relative attention score.\"\"\"\n    # Deal with negative indexing\n    row_axis = row_axis % x.ndim\n\n    # Assume `col_axis` = `row_axis + 1`\n    col_axis = row_axis + 1\n    assert col_axis < x.ndim\n\n    tgt_shape_1, tgt_shape_2 = [], []\n    for i in range(x.ndim):\n      if i == row_axis:\n        tgt_shape_1.append(x.shape[col_axis])\n        tgt_shape_2.append(x.shape[row_axis])\n      elif i == col_axis:\n        tgt_shape_1.append(x.shape[row_axis])\n        tgt_shape_2.append(x.shape[col_axis] - shift)\n      else:\n        tgt_shape_1.append(x.shape[i])\n        tgt_shape_2.append(x.shape[i])\n\n    y = torch.reshape(x, tgt_shape_1)\n    y = torch.narrow(y, row_axis, shift, x.shape[col_axis] - shift)\n    y = torch.reshape(y, tgt_shape_2)\n    y = torch.narrow(y, col_axis, 0, key_len)\n\n    return y\n\n  def rel_pos_bias(self, pos_enc, q_head, k_len, func_mask=None):\n    n_head = self.n_head\n    d_head = self.d_head\n    net_config = self.net_config\n    scale = self.scale\n    r_r_bias = self.r_r_bias\n    r_kernel = self.r_kernel\n    if self.attn_type == \"factorized\":\n      enc_q_1, enc_q_2, enc_k_1, enc_k_2 = pos_enc\n      q_head_r = torch.einsum(\"...inh,dnh->...ind\",\n                              q_head + r_r_bias * scale,\n                              r_kernel)\n      q_head_r_1 = q_head_r * torch.unsqueeze(enc_q_1, -2)\n      q_head_r_2 = q_head_r * torch.unsqueeze(enc_q_2, -2)\n      prefix_k = get_einsum_string(len(enc_k_1.shape) - 2)\n      einsum_str = \"...ind,{0}jd->...nij\".format(prefix_k)\n      bd = (torch.einsum(einsum_str, q_head_r_1, enc_k_1) +\n            torch.einsum(einsum_str, q_head_r_2, enc_k_2))\n    elif self.attn_type == \"rel_shift\":\n      if k_len != q_head.size(1):\n        # pooling case\n        shift = 2\n        pos_enc = pos_enc[self.bidx][1]\n      else:\n        shift = 1\n        pos_enc = pos_enc[self.bidx][0]\n      q_head = q_head + r_r_bias * scale\n      r_head = torch.einsum(\"td,dnh->tnh\", pos_enc, r_kernel)\n      bd = torch.einsum(\"bfnh,tnh->bnft\", q_head, r_head)\n      bd = self.rel_shift(bd, -2, k_len, shift)\n    else:\n      raise NotImplementedError\n    if func_mask is not None:\n      bd = bd * func_mask\n    return bd\n\n  def rel_seg_bias(self, seg_mat, q_head, func_mask=None):\n    # segment based attention score\n\n    if seg_mat is None:\n      seg_bias = 0\n    else:\n      r_s_bias = self.r_s_bias * self.scale\n      seg_embed = self.seg_embed\n\n      seg_bias = torch.einsum(\"...ind,snd->...nis\",\n                              q_head + r_s_bias, seg_embed)\n      tgt_shape = list(seg_mat.size())\n      tgt_shape.insert(-2, self.n_head)\n      seg_mat = torch.unsqueeze(seg_mat, -3).expand(tgt_shape)\n      _diff, _same = torch.split(seg_bias, 1, dim=-1)\n      _diff = _diff.expand(tgt_shape)\n      _same = _same.expand(tgt_shape)\n      seg_bias = torch.where(seg_mat, _same, _diff)\n      if func_mask is not None:\n        seg_bias *= func_mask\n    return seg_bias\n\n  def forward(self, q, k, v, attn_struct):\n    pos_enc, seg_mat, input_mask, attn_mask, func_mask = attn_struct\n    q_head = self.q_head(q)\n    k_head = self.k_head(k)\n    v_head = self.v_head(v)\n\n    q_head = q_head * self.scale\n    r_w_bias = self.r_w_bias * self.scale\n    # content based attention score\n    content_score = torch.einsum(\"...ind,...jnd->...nij\",\n                                 q_head + r_w_bias, k_head)\n    pos_bias = self.rel_pos_bias(pos_enc, q_head, k_head.size(1), func_mask)\n\n    seg_bias = self.rel_seg_bias(seg_mat, q_head, func_mask)\n    # merge attention scores\n    attn_score = content_score + pos_bias + seg_bias\n\n    # precision safe\n    dtype = attn_score.dtype\n    attn_score = attn_score.float()\n    # perform masking\n    if attn_mask is not None:\n      attn_score = attn_score - INF * attn_mask.float()\n    # attention probability\n    attn_prob = torch.softmax(attn_score, dim=-1)\n    attn_prob = attn_prob.type(dtype)\n\n    attn_prob = self.att_drop(attn_prob)\n    # attention output\n    attn_vec = torch.einsum(\"...nij,...jnd->...ind\", attn_prob, v_head)\n\n    attn_out = self.post_proj(attn_vec)\n    attn_out = self.hid_drop(attn_out)\n\n    output = self.layer_norm(q + attn_out)\n    return output\n\n\nclass AttentionStructure(nn.Module):\n  \"\"\"Relative multi-head attention.\"\"\"\n\n  def __init__(self, net_config, args, dtype=torch.float32, device=None):\n    super(AttentionStructure, self).__init__()\n\n    self.net_config = net_config\n    self.dtype = dtype\n    self.device = device\n    self.sin_drop = nn.Dropout(net_config.dropout)\n    self.cos_drop = nn.Dropout(net_config.dropout)\n    self.args = args\n    self.attn_type = args.attn_type\n    self.delta = None\n\n  def stride_pool_pos(self, pos_id, bidx):\n    net_config = self.net_config\n    if net_config.separate_cls:\n      # Under separate [cls], we treat the [cls] as the first token in\n      # the previous block of the 1st real block. Since the 1st real\n      # block always has position 1, the position of the previous block\n      # will 1 - 2**bidx, where `2 ** bidx` is the current stride.\n      cls_pos = pos_id.new_tensor([-2**bidx + 1])\n      if self.args.truncate_seq:\n        pooled_pos_id = pos_id[1:-1]\n      else:\n        pooled_pos_id = pos_id[1:]\n      pooled_pos_id = torch.cat([cls_pos, pooled_pos_id[::2]], 0)\n    else:\n      pooled_pos_id = pos_id[::2]\n\n    return pooled_pos_id\n\n  def construct_rel_pos_seq(self, q_pos, q_stride, k_pos, k_stride):\n    net_config = self.net_config\n    shift = q_stride // k_stride\n    pool_size = net_config.pooling_size\n\n    ref_point = q_pos[0] - k_pos[0]\n    num_remove = shift * len(q_pos)\n    max_dist = ref_point + num_remove * k_stride\n    min_dist = q_pos[0] - k_pos[-1]\n\n    rel_pos_id = torch.arange(max_dist,\n                              min_dist - 1,\n                              -k_stride,\n                              dtype=torch.long,\n                              device=q_pos.device)\n\n    return rel_pos_id\n\n  def get_pos_enc(self, seq_len, dtype, device):\n    \"\"\"Create inputs related to relative position encoding.\"\"\"\n    net_config = self.net_config\n    if self.attn_type == \"factorized\":\n      pos_seq = torch.arange(0, seq_len, 1.0, dtype=dtype, device=device)\n      pos_seq_q, pos_seq_k = pos_seq, pos_seq\n      d_model = self.net_config.d_model\n      d_model_half = d_model // 2\n      freq_seq = torch.arange(0, d_model_half, 1.0,\n                              dtype=dtype, device=device)\n      inv_freq = 1 / (10000 ** (freq_seq / d_model_half))\n      sinusoid_q = torch.einsum(\"...i,d->...id\", pos_seq_q, inv_freq)\n      sinusoid_k = torch.einsum(\"...i,d->...id\", pos_seq_k, inv_freq)\n      sin_enc_q = torch.sin(sinusoid_q)\n      cos_enc_q = torch.cos(sinusoid_q)\n      sin_enc_q = self.sin_drop(sin_enc_q)\n      cos_enc_q = self.cos_drop(cos_enc_q)\n      sin_enc_k = torch.sin(sinusoid_k)\n      cos_enc_k = torch.cos(sinusoid_k)\n      enc_q_1 = torch.cat([sin_enc_q, sin_enc_q], dim=-1)\n      enc_k_1 = torch.cat([cos_enc_k, sin_enc_k], dim=-1)\n      enc_q_2 = torch.cat([cos_enc_q, cos_enc_q], dim=-1)\n      enc_k_2 = torch.cat([-sin_enc_k, cos_enc_k], dim=-1)\n      return [enc_q_1, enc_q_2, enc_k_1, enc_k_2]\n    elif self.attn_type == \"rel_shift\":\n      d_model = self.net_config.d_model\n      d_model_half = d_model // 2\n      freq_seq = torch.arange(0, d_model_half, 1.0,\n                              dtype=dtype, device=device)\n      inv_freq = 1 / (10000 ** (freq_seq / d_model_half))\n\n      # initialize an extra long position sequnece\n      rel_pos_id = torch.arange(-seq_len * 2, seq_len * 2, 1.0,\n                                dtype=dtype, device=device)\n      zero_offset = seq_len * 2\n\n      sinusoid = torch.einsum(\"...i,d->...id\", rel_pos_id, inv_freq)\n      sin_enc = torch.sin(sinusoid)\n      cos_enc = torch.cos(sinusoid)\n      sin_enc = self.sin_drop(sin_enc)\n      cos_enc = self.cos_drop(cos_enc)\n      pos_enc = torch.cat([sin_enc, cos_enc], dim=-1)\n\n      # Pre-compute and cache the rel_pos_id for all blocks\n      pos_id = torch.arange(0, seq_len, dtype=dtype, device=device)\n      pooled_pos_id = pos_id\n      pos_enc_list = []\n\n      for bidx in range(0, self.net_config.n_block):\n        # For each block with bidx > 0, we need two types pos_encs:\n        #   - Attn(pooled-q, unpooled-kv)\n        #   - Attn(pooled-q, pooled-kv)\n\n        #### First type: Attn(pooled-q, unpooled-kv)\n        if bidx > 0:\n          pooled_pos_id = self.stride_pool_pos(pos_id, bidx)\n\n          # construct rel_pos_id\n          q_stride = self.net_config.pooling_size ** bidx\n          k_stride = self.net_config.pooling_size ** (bidx - 1)\n          rel_pos_id = self.construct_rel_pos_seq(\n              q_pos=pooled_pos_id, q_stride=q_stride,\n              k_pos=pos_id, k_stride=k_stride)\n\n          # gather relative positional encoding\n          rel_pos_id = rel_pos_id[:, None] + zero_offset\n          rel_pos_id = rel_pos_id.expand(rel_pos_id.size(0), d_model)\n          pos_enc_2 = torch.gather(pos_enc, 0, rel_pos_id)\n        else:\n          pos_enc_2 = None\n\n        #### Second type: Attn(pooled-q, pooled-kv)\n        # construct rel_pos_id\n        pos_id = pooled_pos_id\n        stride = self.net_config.pooling_size ** bidx\n        rel_pos_id = self.construct_rel_pos_seq(\n            q_pos=pos_id, q_stride=stride,\n            k_pos=pos_id, k_stride=stride)\n\n        # gather relative positional encoding\n        rel_pos_id = rel_pos_id[:, None] + zero_offset\n        rel_pos_id = rel_pos_id.expand(rel_pos_id.size(0), d_model)\n        pos_enc_1 = torch.gather(pos_enc, 0, rel_pos_id)\n\n        pos_enc_list.append([pos_enc_1, pos_enc_2])\n      return pos_enc_list\n    else:\n      raise NotImplementedError\n\n  def seg_id_to_mat(self, seg_q, seg_k):\n    \"\"\"Convert `seg_id` to `seg_mat`.\"\"\"\n    seg_mat = torch.eq(torch.unsqueeze(seg_q, -1), torch.unsqueeze(seg_k, -2))\n\n    # Treat [cls] as in the same segment as both A & B\n    cls_mat = torch.unsqueeze(torch.eq(seg_q, self.args.seg_id_cls), -1) | \\\n        torch.unsqueeze(torch.eq(seg_k, self.args.seg_id_cls), -2)\n    seg_mat = cls_mat | seg_mat\n\n    return seg_mat\n\n  def get_attn_mask(self, input_mask):\n    if input_mask is None:\n      attn_mask = None\n    else:\n      attn_mask = input_mask[:, None, None, :]\n    return attn_mask\n\n  def init_attn_structure(self, hidden, seg_id=None, input_mask=None):\n    net_config = self.net_config\n    self.delta = 1\n    seq_len = hidden.size(1)\n    self.seq_len = seq_len\n    pos_enc = self.get_pos_enc(seq_len, hidden.dtype, hidden.device)\n\n    if seg_id is None:\n      seg_mat = None\n    else:\n      seg_mat = self.seg_id_to_mat(seg_id, seg_id)\n\n    if net_config.separate_cls:\n      func_mask = F.pad(\n          torch.ones([seq_len - 1, seq_len - 1],\n                     dtype=hidden.dtype,\n                     device=hidden.device),\n          (1, 0, 1, 0))\n    else:\n      func_mask = None\n\n    attn_mask = self.get_attn_mask(input_mask)\n    attn_struct = (pos_enc, seg_mat, input_mask, attn_mask, func_mask)\n    return attn_struct\n\n  def stride_pool(self, tensor, axis):\n    \"\"\"Perform pooling by stride slicing the tensor along the given axis.\"\"\"\n    if tensor is None:\n      return None\n\n    net_config = self.net_config\n    if isinstance(tensor, (tuple, list)):\n      ndims = tensor[0].dim()\n    else:\n      ndims = tensor.dim()\n    axis = axis % ndims\n\n    enc_slice = []\n    for i in range(ndims):\n      if i == axis:\n        if net_config.separate_cls and self.args.truncate_seq:\n          enc_slice.append(slice(None, -1, 2))\n        else:\n          enc_slice.append(slice(None, None, 2))\n        break\n      else:\n        enc_slice.append(slice(None))\n\n    if net_config.separate_cls:\n      cls_slice = []\n      for i in range(ndims):\n        if i == axis:\n          cls_slice.append(slice(None, 1))\n          break\n        else:\n          cls_slice.append(slice(None))\n\n    def _pool_func(enc):\n      # separate_cls = True\n      #   trunc = False\n      #     [0 1 2 3 4 5 6 7] => [0] & [1 2 3 4 5 6 7] => [0] & [1 3 5 7]\n      #     [0 1 3 5 7] => [0] & [1 3 5 7] => [0] & [1 5]\n      #     [0 1 5] => [0] & [1 5] =>  [0] & [1]\n      #   trunc = True\n      #     [0 1 2 3 4 5 6 7] => [0] & [1 2 3 4 5 6] => [0] & [1 3 5]\n      #     [0 1 3 5] => [0] & [1 3] => [0] & [1]\n      #     [0 1] => [0] & [] => [0]\n      # separate_cls = False\n      #   [0 1 2 3 4 5 6 7] => [0 2 4 6]\n      #   [0 2 4 6] => [0 4]\n      #   [0 4] => [0]\n\n      if net_config.separate_cls:\n        enc = torch.cat([enc[cls_slice], enc], axis=axis)\n      return enc[enc_slice]\n\n    if isinstance(tensor, (tuple, list)):\n      return list(map(_pool_func, tensor))\n    else:\n      return _pool_func(tensor)\n\n  def pool_tensor(self, tensor, mode=\"mean\", stride=(2, 1)):\n    \"\"\"Apply 1D pooling to a tensor of size [B x T (x H)].\"\"\"\n    if tensor is None:\n      return None\n\n    net_config = self.net_config\n    ndims = tensor.dim()\n    if net_config.separate_cls:\n      if self.args.truncate_seq:\n        tensor = torch.cat([tensor[:, :1], tensor[:, :-1]], dim=1)\n      else:\n        tensor = torch.cat([tensor[:, :1], tensor], dim=1)\n\n    assert ndims == 2 or ndims == 3 or ndims == 4\n\n    if ndims == 2:\n      tensor = tensor[:, None, :, None]\n    elif ndims == 3:\n      tensor = tensor[:, None, :, :]\n\n    if mode == \"mean\":\n      tensor = F.avg_pool2d(\n          tensor, stride, stride=stride, ceil_mode=True)\n    elif mode == \"max\":\n      tensor = F.max_pool2d(\n          tensor, stride, stride=stride, ceil_mode=True)\n    elif mode == \"min\":\n      tensor = -F.max_pool2d(\n          -tensor, stride, stride=stride, ceil_mode=True)\n    else:\n      raise NotImplementedError\n    if ndims == 2:\n      tensor = tensor.squeeze(-1).squeeze(1)\n    elif ndims == 3:\n      tensor = tensor.squeeze(1)\n\n    return tensor\n\n  def pre_attn_pooling(self, output, attn_struct):\n    pos_enc, seg_mat, input_mask, attn_mask, func_mask = attn_struct\n    net_config = self.net_config\n    ret_dict = {}\n    if net_config.pool_q_only:\n      if self.args.attn_type == \"factorized\":\n        pos_enc = self.stride_pool(pos_enc[:2], 0) + pos_enc[2:]\n      seg_mat = self.stride_pool(seg_mat, 1)\n      func_mask = self.stride_pool(func_mask, 0)\n      output = self.pool_tensor(output, mode=net_config.pooling_type)\n    else:\n      self.delta *= 2\n      if self.args.attn_type == \"factorized\":\n        pos_enc = self.stride_pool(pos_enc, 0)\n      seg_mat = self.stride_pool(seg_mat, 1)\n      seg_mat = self.stride_pool(seg_mat, 2)\n      func_mask = self.stride_pool(func_mask, 1)\n      func_mask = self.stride_pool(func_mask, 2)\n      input_mask = self.pool_tensor(input_mask, mode=\"min\")\n      output = self.pool_tensor(output, mode=net_config.pooling_type)\n    attn_mask = self.get_attn_mask(input_mask)\n    attn_struct = (pos_enc, seg_mat, input_mask, attn_mask, func_mask)\n    return output, attn_struct, ret_dict\n\n  def post_attn_pooling(self, attn_struct):\n    net_config = self.net_config\n    pos_enc, seg_mat, input_mask, attn_mask, func_mask = attn_struct\n\n    if net_config.pool_q_only:\n      self.delta *= 2\n      if self.args.attn_type == \"factorized\":\n        pos_enc = pos_enc[:2] + self.stride_pool(pos_enc[2:], 0)\n      seg_mat = self.stride_pool(seg_mat, 2)\n      func_mask = self.stride_pool(func_mask, 1)\n      input_mask = self.pool_tensor(input_mask, mode=\"min\")\n    attn_mask = self.get_attn_mask(input_mask)\n    attn_struct = (pos_enc, seg_mat, input_mask, attn_mask, func_mask)\n    return attn_struct\n  \ndef parse_depth_string(depth_str):\n  depth_config = depth_str.split(\"x\")\n  if len(depth_config) == 1:\n    depth_config.append(1)\n  assert len(depth_config) == 2, \"Require two-element depth config.\"\n\n  return list(map(int, depth_config))\n\n\nclass ModelConfig(object):\n  \"\"\"ModelConfig contains fixed hyperparameters of a FunnelTFM model.\"\"\"\n\n  keys = [\"vocab_size\", \"d_embed\", \"d_model\", \"n_head\", \"d_head\",\n          \"d_inner\", \"dropout\", \"dropatt\", \"dropact\", \"block_size\",\n          \"pooling_type\", \"pooling_size\", \"separate_cls\", \"pool_q_only\"]\n\n  def __init__(self, vocab_size, d_embed, d_model, n_head, d_head,\n               d_inner, dropout, dropatt, dropact, block_size,\n               pooling_type, pooling_size,\n               separate_cls, pool_q_only):\n\n    self.vocab_size = vocab_size\n    self.d_embed = d_embed\n    self.d_model = d_model\n    self.n_head = n_head\n    self.d_head = d_head\n    self.d_inner = d_inner\n\n    self.dropout = dropout\n    self.dropatt = dropatt\n    self.dropact = dropact\n    self.block_size = block_size\n    block_size = block_size.split(\"_\")\n    self.n_block = len(block_size)\n    self.block_rep = []\n    self.block_param = []\n    for i, _ in enumerate(block_size):\n      block_size_i = parse_depth_string(block_size[i])\n      self.block_param.append(block_size_i[0])\n      self.block_rep.append(block_size_i[1])\n\n\n    self.pooling_type = pooling_type\n    self.pooling_size = pooling_size\n    self.separate_cls = separate_cls\n    self.pool_q_only = pool_q_only\n\n\n  @staticmethod\n  def init_from_text(file_path, args, sep_symbol=None):\n    \"\"\"Initialize ModelConfig from a text file.\"\"\"\n    print(\"Initialize ModelConfig from text file %s.\", file_path)\n    conf_args = {}\n    with open(file_path) as f:\n      for line in f:\n        k, v = line.strip().split(sep_symbol)\n        if k in ModelConfig.keys:\n          conf_args[k] = v\n        else:\n          print(\"Unused key %s\", k)\n\n    net_config = ModelConfig(**conf_args)\n\n    # Merge loaded config and args\n    for key in ModelConfig.keys:\n      overwrite_keys = set(args.overwrite_keys.split(\",\"))\n      if key in overwrite_keys:\n        setattr(net_config, key, getattr(args, key))\n      else:\n        setattr(args, key, getattr(net_config, key))\n\n    return net_config\n\n  @staticmethod\n  def init_from_json(file_path, args):\n    \"\"\"Initialize ModelConfig from a json file.\"\"\"\n    print(\"Initialize ModelConfig from json file %s.\", file_path)\n    with open(file_path) as f:\n      json_data = json.load(f)\n\n    net_config = ModelConfig(**json_data)\n\n    # Merge loaded config and args\n    for key in ModelConfig.keys:\n      overwrite_keys = set(args.overwrite_keys.split(\",\"))\n      if key in overwrite_keys:\n        setattr(net_config, key, getattr(args, key))\n      else:\n        setattr(args, key, getattr(net_config, key))\n\n    return net_config\n\n  @staticmethod\n  def init_from_args(args):\n    \"\"\"Initialize ModelConfig from args.\"\"\"\n    print(\"Initialize ModelConfig from args.\")\n    conf_args = {}\n    for key in ModelConfig.keys:\n      conf_args[key] = getattr(args, key)\n\n    return ModelConfig(**conf_args)\n\n  def to_json(self, json_path):\n    \"\"\"Save ModelConfig to a json file.\"\"\"\n    print(\"Save ModelConfig to json file {}.\".format(json_path))\n    json_data = {}\n    for key in ModelConfig.keys:\n      json_data[key] = getattr(self, key)\n\n    json_dir = os.path.dirname(json_path)\n    if not os.path.exists(json_dir):\n      os.makedirs(json_dir)\n    with open(json_path, \"w\") as f:\n      json.dump(json_data, f, indent=4, sort_keys=True)\n\n\nclass FunnelTFM(nn.Module):\n  \"\"\"FunnelTFM model.\"\"\"\n\n  def __init__(self, net_config, args, cls_target=True):\n    super(FunnelTFM, self).__init__()\n    self.net_config = net_config\n    self.args = args\n    self.input_layer = nn.Sequential(\n        EmbeddindLookup(net_config.vocab_size,\n                        net_config.d_embed),\n        LayerNorm(net_config.d_embed),\n        nn.Dropout(net_config.dropout))\n\n    self.pos_drop = nn.Dropout(net_config.dropout)\n\n    self.attn_info = AttentionStructure(net_config, args)\n    self.attn_layers = nn.ModuleList()\n    self.pffn_layers = nn.ModuleList()\n    for block_idx in range(net_config.n_block):\n      for _ in range(net_config.block_param[block_idx]):\n        self.attn_layers.append(\n            RelMultiheadAttention(\n                net_config,\n                args,\n                net_config.d_model,\n                net_config.n_head,\n                net_config.d_head,\n                net_config.dropout,\n                net_config.dropatt,\n                block_idx,\n            )\n        )\n        self.pffn_layers.append(\n            PositionwiseFFN(\n                net_config.d_model,\n                net_config.d_inner,\n                net_config.dropout,\n                net_config.dropact,\n            )\n        )\n\n  def tfmxl_layers(self, inputs, seg_id=None, input_mask=None):\n    net_config = self.net_config\n    output = inputs\n\n    ret_dict = {}\n    ##### TFM-XL layers\n    hiddens = []\n    layer_idx = 0\n    attn_struct = self.attn_info.init_attn_structure(output, seg_id, input_mask)\n    for block_idx in range(net_config.n_block):\n      if net_config.separate_cls:\n        pooling_flag = output.size(1) > 2\n      else:\n        pooling_flag = output.size(1) > 1\n\n      if block_idx > 0 and pooling_flag:\n        pooled_out, attn_struct, _ = self.attn_info.pre_attn_pooling(\n            output, attn_struct)\n      for param_idx in range(net_config.block_param[block_idx]):\n        for rep_idx in range(net_config.block_rep[block_idx]):\n          sub_idx = param_idx * net_config.block_rep[block_idx] + rep_idx\n          do_pooling = sub_idx == 0 and block_idx > 0 and pooling_flag\n\n          q = k = v = output\n          # update q, k, v for the first sub layer of a pooling block\n          if do_pooling:\n            if net_config.pool_q_only:\n              q = pooled_out\n              k = v = output\n            else:\n              q = k = v = pooled_out\n          output = self.attn_layers[layer_idx](q, k, v, attn_struct)\n          output = self.pffn_layers[layer_idx](output)\n          if do_pooling:\n            attn_struct = self.attn_info.post_attn_pooling(attn_struct)\n\n          hiddens.append(output)\n\n        layer_idx += 1\n    #print(torch.max(hiddens[-1][0][0]))\n    return hiddens, ret_dict\n\n  def extract_hiddens(self, inputs, seg_id=None, input_mask=None):\n    word_embed = self.input_layer(inputs)\n    hiddens, tfm_dict = self.tfmxl_layers(\n        word_embed,\n        seg_id=seg_id,\n        input_mask=input_mask)\n    return [word_embed] + hiddens, tfm_dict\n\n  def forward(self, inputs, input_mask=None, seg_id=None,\n              cls_target=None):\n\n    if input_mask is None and self.args.pad_id is not None:\n      input_mask = inputs == self.args.pad_id\n      input_mask = input_mask.float()\n\n    hiddens, tfm_dict = self.extract_hiddens(\n        inputs, seg_id=seg_id, input_mask=input_mask)\n\n    ret_dict = {}\n    if cls_target is not None:\n      last_hidden = hiddens[-1][:, 0]\n      cls_logits = self.cls_head(last_hidden)\n      prediction = torch.argmax(cls_logits, -1)\n      ret_dict[\"cls_pred\"] = prediction\n      cls_loss = self.cls_loss(cls_logits, cls_target)\n      ret_dict[\"cls_loss\"] = cls_loss\n      cls_correct = prediction == cls_target\n      cls_correct = cls_correct.type(torch.float32).sum()\n      ret_dict[\"cls_corr\"] = cls_correct\n    return hiddens, ret_dict\n\n",
    "description": null,
    "url": null
}