{
    "acronym": "hgrn",
    "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling",
    "seed_ids": [
        "transnormerllm",
        "rwkv4",
        "tnn",
        "resurrectrnn",
        "liquids4",
        "mega",
        "s5",
        "gssm",
        "s4d",
        "flash",
        "cosformer",
        "metaformer",
        "s4",
        "roformer",
        "performer",
        "transformer"
    ],
    "s2id": "434d751d355d7a7c20efa570e785c76286245e77",
    "abstract": "Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.",
    "authors": [
        "Zhen Qin",
        "Songlin Yang",
        "Yiran Zhong"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.",
    "citationCount": 38,
    "influentialCitationCount": 4,
    "code": "# -*- coding: utf-8 -*-\n\n# \"Hierarchically Gated Recurrent Neural Network for Sequence Modeling\" [https://arxiv.org/abs/2311.04823]\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nfrom fla.modules import FusedRMSNormSwishGate, ShortConvolution\nfrom fla.modules.activations import swiglu\nfrom fla.ops.hgrn import chunk_hgrn, fused_recurrent_hgrn\n\nif TYPE_CHECKING:\n    from fla.models.utils import Cache\n\n\nclass HGRNAttention(nn.Module):\n\n    def __init__(\n        self,\n        mode: str = 'chunk',\n        hidden_size: int = 1024,\n        num_heads: Optional[int] = None,\n        expand_ratio: Optional[int] = 1,\n        use_short_conv: bool = False,\n        conv_size: int = 4,\n        conv_bias: bool = False,\n        elementwise_affine: Optional[bool] = True,\n        norm_eps: float = 1e-5,\n        layer_idx: int = None\n    ) -> HGRNAttention:\n        super().__init__()\n\n        self.mode = mode\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.expand_ratio = expand_ratio\n        self.input_dim = int(hidden_size * expand_ratio)\n        self.head_dim = self.input_dim // self.num_heads\n\n        self.use_short_conv = use_short_conv\n        self.conv_size = conv_size\n        self.conv_bias = conv_bias\n\n        self.layer_idx = layer_idx\n\n        assert mode in ['chunk', 'fused_recurrent'], f\"Not suppoerted mode `{mode}`.\"\n        assert self.hidden_size % num_heads == 0, f\"hidden size must be divisible by num_heads of {num_heads}\"\n\n        self.i_proj = nn.Linear(hidden_size, self.input_dim, bias=False)\n        self.f_proj = nn.Linear(hidden_size, self.input_dim, bias=False)\n        self.g_proj = nn.Linear(hidden_size, self.input_dim, bias=False)\n\n        if use_short_conv:\n            self.conv_size = conv_size\n            self.q_conv1d = ShortConvolution(self.input_dim, conv_size, activation='silu')\n            self.f_conv1d = ShortConvolution(self.input_dim, conv_size)\n            self.i_conv1d = ShortConvolution(self.input_dim, conv_size)\n\n        self.g_norm = FusedRMSNormSwishGate(self.input_dim, elementwise_affine, norm_eps)\n        self.o_proj = nn.Linear(self.input_dim, hidden_size, bias=False)\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[Cache] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        lower_bound: Optional[torch.Tensor] = None,\n        **kwargs\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n        # launching the triton kernel for just one token will actually be slower\n        mode = 'fused_recurrent' if hidden_states.shape[1] == 1 else self.mode\n\n        last_state = past_key_values[self.layer_idx] if use_cache else None\n        if self.use_short_conv:\n            conv_state_i = last_state[0] if use_cache else None\n            conv_state_f = last_state[1] if use_cache else None\n            i = self.i_conv1d(self.i_proj(hidden_states), attention_mask, conv_state_i)\n            f = self.f_conv1d(self.f_proj(hidden_states), attention_mask, conv_state_f)\n        else:\n            i = self.i_proj(hidden_states)\n            f = self.f_proj(hidden_states)\n\n        # the lower bound for the first layer is zero\n        if lower_bound is None or self.layer_idx == 0:\n            i, f = swiglu(i, 1 - f.sigmoid()), F.logsigmoid(f)\n        else:\n            g = lower_bound + (1 - lower_bound) * f.sigmoid()\n            i, f = swiglu(i, 1 - g), g.log()\n\n        # dealing with left-padding\n        if attention_mask is not None:\n            i = i.mul_(attention_mask.unsqueeze(-1))\n        i, f = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=self.num_heads), (i, f))\n\n        recurrent_state = last_state[-1] if use_cache else None\n        if mode == 'chunk':\n            o, recurrent_state = chunk_hgrn(i, f, initial_state=recurrent_state, output_final_state=use_cache)\n        elif mode == 'fused_recurrent':\n            o, recurrent_state = fused_recurrent_hgrn(i, f, initial_state=recurrent_state, output_final_state=use_cache)\n        else:\n            raise NotImplementedError(f\"Not supported mode `{mode}`.\")\n\n        if past_key_values is not None:\n            if self.use_short_conv:\n                last_state = (conv_state_i, conv_state_f, recurrent_state)\n            else:\n                last_state = (recurrent_state,)\n            past_key_values.update(last_state, self.layer_idx, i.shape[2])\n\n        o = self.g_norm(rearrange(o, 'b h l d -> b l (h d)'), self.g_proj(hidden_states))\n        o = self.o_proj(o)\n\n        return o, None, past_key_values\n\n    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n        param = next(self.parameters())\n        state = tuple()\n        if self.use_short_conv:\n            state += (param.new_zeros(batch_size, self.hidden_size, self.conv_size),\n                      param.new_zeros(batch_size, self.hidden_size, self.conv_size),\n                      param.new_zeros(batch_size, self.hidden_size, self.conv_size))\n        state += (param.new_zeros(batch_size, self.num_heads, self.head_dim),)\n        return state\n\n    def state_size(self, **kwargs) -> int:\n        state_size = self.hidden_size\n        for module in self.children():\n            if isinstance(module, ShortConvolution):\n                state_size += module.state_size\n        return state_size",
    "description": null,
    "url": null
}