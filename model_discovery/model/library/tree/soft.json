{
    "acronym": "soft",
    "title": "SOFT: Softmax-free Transformer with Linear Complexity ",
    "seed_ids": [
        "rfa",
        "nystromformer",
        "performer",
        "lineartransformer",
        "linformer",
        "transformer"
    ],
    "s2id": "2e644c67a697073d561da4f4dad35e5ad5316cfd",
    "abstract": "Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.",
    "authors": [
        "Jiachen Lu",
        "Jinghan Yao",
        "Junge Zhang",
        "Xiatian Zhu",
        "Hang Xu",
        "Weiguo Gao",
        "Chunjing Xu",
        "T. Xiang",
        "Li Zhang"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": "For the first time, a softmax-free transformer or SOFT is proposed, which significantly improves the computational efficiency of existing ViT variants and allows much longer token sequences to be permitted in SOFT, resulting in superior trade-off between accuracy and complexity.",
    "citationCount": 118,
    "influentialCitationCount": 8,
    "code": "import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nfrom SOFT.kernel.subtraction import subtraction_gaussian_kernel\nfrom SOFT.kernel.inverse import newton_inverse_kernel\n\nclass Approx_GeLU(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.grad_checkpointing = True\n\n    def func(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n    def forward(self, x):\n        x = self.func(x)\n        return x\n\n\ndef subtraction_gaussian_kernel_torch(q, k):\n    # [B, H, H1*W1, C] @ [C, H2*W2] -> [B, H, H1*W1, H2*W2]\n    matA_square = q ** 2. @ torch.ones(k.shape[-2:]).cuda()\n    # [H1*W1, C] @ [B, H, C, H2*W2] -> [B, H, H1*W1, H2*W2]\n    matB_square = torch.ones(q.shape[-2:]).cuda() @ k ** 2.\n    return matA_square + matB_square - 2. * (q @ k)\n\n\nclass SoftmaxFreeAttentionKernel(nn.Module):\n    def __init__(self, dim, num_heads, ratio, use_conv, max_iter=20, kernel_method=\"cuda\"):\n        super().__init__()\n\n        self.head_dim = int(dim // num_heads)\n        self.num_head = num_heads\n        self.ratio = ratio\n        self.max_iter = max_iter\n\n        if kernel_method == \"torch\":\n            self.kernel_function = subtraction_gaussian_kernel_torch\n        elif kernel_method == \"cuda\":\n            self.kernel_function = subtraction_gaussian_kernel\n        else:\n            assert False, \"please choose kernel method from torch and cuda\"\n\n        if ratio == 1:\n            self.Qlandmark_op = nn.Linear(self.head_dim, self.head_dim, bias=False)\n            self.Qnorm_act = nn.Sequential(nn.LayerNorm(self.head_dim), nn.GELU())\n        else:\n            self.Qlandmark_op = nn.Conv2d(self.head_dim, self.head_dim, kernel_size=ratio, stride=ratio, bias=False)\n            self.Qnorm_act = nn.Sequential(nn.LayerNorm(self.head_dim), nn.GELU())\n\n        self.use_conv = use_conv\n        if self.use_conv:\n            self.conv = nn.Conv2d(\n                in_channels=self.num_head, out_channels=self.num_head,\n                kernel_size=(self.use_conv, self.use_conv), padding=(self.use_conv // 2, self.use_conv // 2),\n                bias=False,\n                groups=self.num_head)\n\n    def forward(self, Q, V, H, W):\n        b, nhead, num_sequence, headdim = Q.size()\n        # Q: [b, num_head, N, head_dim]\n        Q = Q / math.sqrt(math.sqrt(headdim))\n        K=Q\n        if self.ratio == 1:\n            Q_landmarks = Q.reshape(b * nhead, H * W + 1, headdim)\n            Q_landmarks = self.Qlandmark_op(Q_landmarks)\n            Q_landmarks = self.Qnorm_act(Q_landmarks).reshape(b, nhead, self.num_landmarks + 1, headdim)\n            K_landmarks = Q_landmarks\n            attn = self.kernel_function(Q_landmarks, K_landmarks.transpose(-1, -2).contiguous())\n            attn = torch.exp(-attn / 2)\n            X = torch.matmul(attn, V)\n\n            if self.use_conv:\n                V_ = V[:, :, 1:, :]\n                cls_token = V[:, :, 0, :].unsqueeze(2)\n                V_ = V_.reshape(b, nhead, H, W, headdim)\n                V_ = V_.permute(0, 4, 1, 2, 3).reshape(b * headdim, nhead, H, W)\n                out = self.conv(V_).reshape(b, headdim, nhead, H, W).flatten(3).permute(0, 2, 3, 1)\n                out = torch.cat([cls_token, out], dim=2)\n                X += out\n        else:\n            Q_landmarks = Q.reshape(b * nhead, H * W, \n                                    headdim).reshape(b * nhead, \n                                                           H, W, headdim).permute(0, 3, 1, 2)\n            Q_landmarks = self.Qlandmark_op(Q_landmarks)\n            Q_landmarks = Q_landmarks.flatten(2).transpose(1, 2).reshape(b, nhead, -1, headdim)\n            Q_landmarks = self.Qnorm_act(Q_landmarks)\n            K_landmarks = Q_landmarks\n\n            kernel_1_ = self.kernel_function(Q, K_landmarks.transpose(-1, -2).contiguous())\n            kernel_1_ = torch.exp(-kernel_1_/2)\n\n            kernel_2_ = self.kernel_function(Q_landmarks, K_landmarks.transpose(-1, -2).contiguous())\n            kernel_2_ = torch.exp(-kernel_2_/2)\n\n            kernel_3_ = kernel_1_.transpose(-1, -2)\n\n            X = torch.matmul(torch.matmul(kernel_1_, newton_inverse_kernel(kernel_2_, self.max_iter)), torch.matmul(kernel_3_, V))\n\n            if self.use_conv:\n                V = V.reshape(b, nhead, H, W, headdim)\n                V = V.permute(0, 4, 1, 2, 3).reshape(b*headdim, nhead, H, W)\n                X += self.conv(V).reshape(b, headdim, nhead, H, W).flatten(3).permute(0, 2, 3, 1)\n\n        return X\n\n\nclass SoftmaxFreeAttention(nn.Module):\n    def __init__(self, dim, num_heads, ratio, conv_size, max_iter=20, kernel_method=\"cuda\"):\n        super().__init__()\n\n        self.grad_checkpointing = True\n        self.dim = dim\n        self.head_dim = int(dim // num_heads)\n        self.num_head = num_heads\n\n        self.W_q = nn.Linear(self.dim, self.num_head * self.head_dim)\n        self.W_v = nn.Linear(self.dim, self.num_head * self.head_dim)\n\n        self.attn = SoftmaxFreeAttentionKernel(dim, num_heads, ratio, conv_size, max_iter, kernel_method)\n\n        self.ff = nn.Linear(self.num_head * self.head_dim, self.dim)\n\n    def forward(self, X, H, W):\n\n        Q = self.split_heads(self.W_q(X))\n        V = self.split_heads(self.W_v(X))\n        attn_out = self.attn(Q, V, H, W)\n        attn_out = self.combine_heads(attn_out)\n\n        out = self.ff(attn_out)\n        return out\n\n    def combine_heads(self, X):\n        X = X.transpose(1, 2)\n        X = X.reshape(X.size(0), X.size(1), self.num_head * self.head_dim)\n        return X\n\n    def split_heads(self, X):\n        X = X.reshape(X.size(0), X.size(1), self.num_head, self.head_dim)\n        X = X.transpose(1, 2)\n        return X\n\n\nclass SoftmaxFreeTransformer(nn.Module):\n    def __init__(self, dim, num_heads, ratio, conv_size, drop_path=0., max_iter=20, kernel_method=\"torch\"):\n        super().__init__()\n        self.dim = dim\n        self.hidden_dim = int(4*dim)\n\n        self.mha = SoftmaxFreeAttention(dim, num_heads, ratio, conv_size, max_iter, kernel_method)\n\n        self.dropout1 = torch.nn.Dropout(p=drop_path)\n        self.norm1 = nn.LayerNorm(self.dim)\n\n        self.ff1 = nn.Linear(self.dim, self.hidden_dim)\n        self.act = Approx_GeLU()\n        self.ff2 = nn.Linear(self.hidden_dim, self.dim)\n\n        self.dropout2 = torch.nn.Dropout(p=drop_path)\n        self.norm2 = nn.LayerNorm(self.dim)\n\n    def forward(self, X, H, W):\n        mha_out = self.mha(X, H, W)\n        mha_out = self.norm1(X + self.dropout1(mha_out))\n        ff_out = self.ff2(self.act(self.ff1(mha_out)))\n        mha_out = self.norm2(mha_out + self.dropout2(ff_out))\n        return mha_out\n\n\nclass SoftmaxFreeTransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, ratio, drop_path=0., conv_size=3, max_iter=20, kernel_method=\"cuda\"):\n        super().__init__()\n        self.att = SoftmaxFreeTransformer(dim, num_heads, ratio, conv_size, drop_path, max_iter, kernel_method)\n\n    def forward(self, x, H, W):\n        x = self.att(x, H, W)\n        return x\n\n\nclass SoftmaxFreeNormAttentionKernel(SoftmaxFreeAttentionKernel):\n    def __init__(self, dim, num_heads, ratio, use_conv, max_iter=20, kernel_method=\"cuda\"):\n        super(SoftmaxFreeSymNormAttentionKernel, self).__init__(dim, num_heads, ratio, use_conv, max_iter, kernel_method)\n\n    def forward(self, Q, V, H, W):\n        b, nhead, num_sequence, headdim = Q.size()\n        # Q: [b, num_head, N, head_dim]\n        Q = Q / math.sqrt(math.sqrt(headdim))\n        K=Q\n        if self.ratio == 1:\n            Q_landmarks = Q.reshape(b * nhead, H * W + 1, headdim)\n            Q_landmarks = self.Qlandmark_op(Q_landmarks)\n            Q_landmarks = self.Qnorm_act(Q_landmarks).reshape(b, nhead, self.num_landmarks + 1, headdim)\n            K_landmarks = Q_landmarks\n            attn = self.kernel_function(Q_landmarks, K_landmarks.transpose(-1, -2).contiguous())\n            attn = torch.exp(-attn / 2)\n            X = torch.matmul(attn, V)\n\n            if self.use_conv:\n                V_ = V[:, :, 1:, :]\n                cls_token = V[:, :, 0, :].unsqueeze(2)\n                V_ = V_.reshape(b, nhead, H, W, headdim)\n                V_ = V_.permute(0, 4, 1, 2, 3).reshape(b * headdim, nhead, H, W)\n                out = self.conv(V_).reshape(b, headdim, nhead, H, W).flatten(3).permute(0, 2, 3, 1)\n                out = torch.cat([cls_token, out], dim=2)\n                X += out\n        else:\n            Q_landmarks = Q.reshape(b * nhead, H * W, \n                                    headdim).reshape(b * nhead, \n                                                           H, W, headdim).permute(0, 3, 1, 2)\n            Q_landmarks = self.Qlandmark_op(Q_landmarks)\n            Q_landmarks = Q_landmarks.flatten(2).transpose(1, 2).reshape(b, nhead, -1, headdim)\n            Q_landmarks = self.Qnorm_act(Q_landmarks)\n            K_landmarks = Q_landmarks\n\n            kernel_1_ = self.kernel_function(Q, K_landmarks.transpose(-1, -2).contiguous())\n            kernel_1_ = torch.exp(-kernel_1_/2)\n\n            kernel_2_ = self.kernel_function(Q_landmarks, K_landmarks.transpose(-1, -2).contiguous())\n            kernel_2_ = torch.exp(-kernel_2_/2)\n            \n            D_sym = kernel_2_.sum(-1, keepdim=True)\n            D_sym = 1 / D_sym.sqrt()\n\n            kernel_2_inv = self.newton_inv(kernel_2_)\n            kernel_2_inv = D_sym * kernel_2_inv * D_sym.transpose(-1,-2)\n\n            kernel_3_ = kernel_1_.transpose(-1, -2)\n\n            X = torch.matmul(torch.matmul(kernel_1_, kernel_2_inv), torch.matmul(kernel_3_, V))\n\n            if self.use_conv:\n                V = V.reshape(b, nhead, H, W, headdim)\n                V = V.permute(0, 4, 1, 2, 3).reshape(b*headdim, nhead, H, W)\n                X += self.conv(V).reshape(b, headdim, nhead, H, W).flatten(3).permute(0, 2, 3, 1)\n\n        return X\n\n    def newton_inv(self, mat):\n        P = mat\n        I = torch.eye(mat.size(-1), device=mat.device)\n        alpha = 2 / (torch.max(torch.sum(mat, dim=-1)) ** 2)\n        beta = 0.5\n        V = alpha * P\n        pnorm = torch.max(torch.sum(torch.abs(I - torch.matmul(P, V)), dim=-2))\n        err_cnt = 0\n        while pnorm > 1.01 and err_cnt < 10:\n            alpha *= beta\n            V = alpha * P\n            pnorm = torch.max(torch.sum(torch.abs(I - torch.matmul(P, V)), dim=-2))\n            err_cnt += 1\n\n        for i in range(self.max_iter):\n            V = 2 * V - V @ P @ V\n        return V\n\n\nclass SoftmaxFreeNormAttention(SoftmaxFreeAttention):\n    def __init__(self, dim, num_heads, ratio, conv_size, max_iter=20, kernel_method=\"cuda\"):\n        super(SoftmaxFreeNormAttention, self).__init__(dim, num_heads, ratio, conv_size, max_iter, kernel_method)\n        self.attn = SoftmaxFreeNormAttentionKernel(dim, num_heads, ratio, conv_size, max_iter, kernel_method)\n\n\nclass SoftmaxFreeNormTransformer(SoftmaxFreeTransformer):\n    def __init__(self, dim, num_heads, ratio, conv_size, drop_path=0., max_iter=20, kernel_method=\"torch\"):\n        super(SoftmaxFreeNormTransformer, self).__init__(dim, num_heads, ratio, conv_size, drop_path, max_iter, kernel_method)\n        self.mha = SoftmaxFreeNormAttention(dim, num_heads, ratio, conv_size, max_iter, kernel_method)\n\n\nclass SoftmaxFreeNormTransformerBlock(SoftmaxFreeTransformerBlock):\n    def __init__(self, dim, num_heads, ratio, drop_path=0., conv_size=3, max_iter=20, kernel_method=\"cuda\"):\n        super(SoftmaxFreeNormTransformerBlock, self).__init__(dim, num_heads, ratio, drop_path, conv_size, max_iter, kernel_method)\n        self.att = SoftmaxFreeNormTransformer(dim, num_heads, ratio, conv_size, drop_path, max_iter, kernel_method)",
    "description": null,
    "url": null
}