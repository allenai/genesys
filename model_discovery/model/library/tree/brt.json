{
    "acronym": "brt",
    "title": "Block-Recurrent Transformers",
    "seed_ids": [
        "memorizingtrans",
        "longformer",
        "reformer",
        "compressivetransformer",
        "transformerxl"
    ],
    "s2id": "736eb449526fe7128917954ec5532b59e318ec78",
    "abstract": "We introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to sequence length. Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block in order to make efficient use of accelerator hardware. The cell itself is strikingly simple. It is merely a transformer layer: it uses self-attention and cross-attention to efficiently compute a recurrent function over a large set of state vectors and tokens. Our design was inspired in part by LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM cell up by several orders of magnitude. Our implementation of recurrence has the same cost in both computation time and parameter count as a conventional transformer layer, but offers dramatically improved perplexity in language modeling tasks over very long sequences. Our model out-performs a long-range Transformer XL baseline by a wide margin, while running twice as fast. We demonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source code. Our code has been released as open source.",
    "authors": [
        "DeLesley S. Hutchins",
        "Imanol Schlag",
        "Yuhuai Wu",
        "Ethan Dyer",
        "Behnam Neyshabur"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": "N/A",
    "citationCount": 66,
    "influentialCitationCount": 9,
    "code": "from collections import namedtuple\nfrom einops import rearrange, repeat\nimport torch\nfrom torch import einsum, nn\nimport torch.nn.functional as F\nfrom torchtyping import TensorType\nfrom typeguard import typechecked\nfrom typing import Optional, Tuple\nfrom x_transformers.x_transformers import (\n    apply_rotary_pos_emb, default, exists, FeedForward, RMSNorm\n)\n\n\"\"\"\nThis may change significantly as I work out how to implement this properly, but until large portions of this are copied from Phil Wang (@lucidrains)\n\"\"\"\n\n\nSeqTensor = TensorType['batch', 'seq_len', 'token_dim']\nStateTensor = TensorType['batch', 'state_len', 'state_dim']\n\n# constants\n\nDEFAULT_DIM_HEAD = 64\nMIN_DIM_HEAD = 32\n\nIntermediates = namedtuple('Intermediates', [\n    'pre_softmax_attn',\n    'post_softmax_attn'\n])\n\nLayerIntermediates = namedtuple('Intermediates', [\n    'hiddens',\n    'attn_intermediates'\n])\n\ndef cast_tuple(val, num = 1):\n    return val if isinstance(val, tuple) else ((val,) * num)\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def forward(self, max_seq_len, *, device, offset = 0):\n        seq = torch.arange(max_seq_len, device = device) + offset\n        freqs = einsum('i , j -> i j', seq.type_as(self.inv_freq), self.inv_freq)\n        emb = torch.cat((freqs, freqs), dim = -1)\n        return rearrange(emb, 'n d -> 1 1 n d')\n\ndef rotate_half(x):\n    x = rearrange(x, '... (j d) -> ... j d', j = 2)\n    x1, x2 = x.unbind(dim = -2)\n    return torch.cat((-x2, x1), dim = -1)\n\ndef apply_rotary_pos_emb(t, freqs):\n    seq_len, rot_dim = t.shape[-2], freqs.shape[-1]\n    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n    t = (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n    return torch.cat((t, t_pass), dim = -1)\n\n\n@typechecked\nclass RecurrentStateGate(nn.Module):\n    \"\"\"Poor man's LSTM\n    \"\"\"\n\n    def __init__(self, dim: int):\n        super().__init__()\n\n        self.main_proj = nn.Linear(dim, dim, bias = True)\n        self.input_proj = nn.Linear(dim, dim, bias = True)\n        self.forget_proj = nn.Linear(dim, dim, bias = True)\n    \n    def forward(self, x: SeqTensor, state: StateTensor) -> StateTensor:\n        z = torch.tanh(self.main_proj(x))\n        i = torch.sigmoid(self.input_proj(x) - 1)\n        f = torch.sigmoid(self.forget_proj(x) + 1)\n        return torch.mul(state, f) + torch.mul(z, i)\n\n\nclass Attention(nn.Module):\n    \"\"\"Shamelessly copied from github.com/lucidrains/RETRO-pytorch\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        *,\n        dim_head = 64,\n        heads = 8,\n        causal = False,\n        dropout = 0.,\n        null_kv = False\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.causal = causal\n        inner_dim = dim_head * heads\n\n        self.norm = RMSNorm(dim)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n        # allowing for attending to nothing (null function)\n        # and to save attention from breaking if all retrieved chunks are padded out\n        self.null_kv = nn.Parameter(torch.randn(2, inner_dim)) if null_kv else None\n\n    def forward(self, x, mask = None, context = None, pos_emb = None):\n        b, device, h, scale = x.shape[0], x.device, self.heads, self.scale\n\n        x = self.norm(x)\n        kv_input = default(context, x)\n\n        q = self.to_q(x)\n        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        # split heads\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        # scale\n        q = q * scale\n\n        # apply relative positional encoding (rotary embeddings)\n        if exists(pos_emb):\n            q_pos_emb, k_pos_emb = cast_tuple(pos_emb, num = 2)\n            q = apply_rotary_pos_emb(q, q_pos_emb)\n            k = apply_rotary_pos_emb(k, k_pos_emb)\n\n        # add null key / values\n        if exists(self.null_kv):\n            nk, nv = self.null_kv.unbind(dim = 0)\n            nk, nv = map(lambda t: repeat(t, '(h d) -> b h 1 d', b = b, h = h), (nk, nv))\n            k = torch.cat((nk, k), dim = -2)\n            v = torch.cat((nv, v), dim = -2)\n\n        # derive query key similarities\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        # masking\n        mask_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            if exists(self.null_kv):\n                mask = F.pad(mask, (1, 0), value = True)\n\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n            sim = sim.masked_fill(~mask, mask_value)\n\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones(i, j, device = device, dtype = torch.bool).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n        # attention\n        attn = sim.softmax(dim = -1)\n\n        attn = self.dropout(attn)\n\n        # aggregate\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n\n        # merge heads\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        \n        # combine heads linear out\n        return self.to_out(out)\n\n\n@typechecked\nclass BlockRecurrentAttention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        dim_state: int,\n        dim_head: int = DEFAULT_DIM_HEAD,\n        state_len: int = 512,\n        heads: int = 8,\n        **kwargs\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n\n        attn_kwargs = {}\n\n        self.dim = dim\n        self.dim_state = dim_state\n\n        self.heads = heads\n        self.causal = True\n        self.state_len = state_len\n        rotary_emb_dim = max(dim_head // 2, MIN_DIM_HEAD)\n        self.rotary_pos_emb = RotaryEmbedding(rotary_emb_dim)\n        \n        self.input_self_attn = Attention(dim, heads = heads, causal = True, **attn_kwargs)\n        self.state_self_attn = Attention(dim_state, heads = heads, causal = False, **attn_kwargs)\n\n        self.input_state_cross_attn = Attention(dim, heads = heads, causal = False, **attn_kwargs)\n        self.state_input_cross_attn = Attention(dim_state, heads = heads, causal = False, **attn_kwargs)\n\n        self.proj_gate = RecurrentStateGate(dim)\n        self.ff_gate = RecurrentStateGate(dim)\n\n        self.input_proj = nn.Linear(dim + dim_state, dim, bias = False)\n        self.state_proj = nn.Linear(dim + dim_state, dim, bias = False)\n\n        self.input_ff = FeedForward(dim)\n        self.state_ff = FeedForward(dim_state)\n\n    @typechecked\n    def forward(\n        self,\n        x: SeqTensor,\n        state: Optional[StateTensor] = None,\n        mask = None,\n        state_mask = None\n    ) -> Tuple[SeqTensor, StateTensor]:\n        batch, seq_len, device = x.shape[0], x.shape[-2], x.device\n        if not exists(state):\n            state = torch.zeros((batch, self.state_len, self.dim_state), device=device)\n        self_attn_pos_emb = self.rotary_pos_emb(seq_len, device = device)\n        state_pos_emb = self.rotary_pos_emb(self.state_len, device = device)\n        input_attn = self.input_self_attn(x, mask = mask, pos_emb = self_attn_pos_emb)\n        state_attn = self.state_self_attn(state, mask = state_mask, pos_emb = state_pos_emb)\n\n        # TODO: This is different from how it is implemented in the paper, because the Keys and Values aren't shared\n        # between the cross attention and self-attention. I'll implement that later, this is faster for now.\n        input_as_q_cross_attn = self.input_state_cross_attn(x, context = state, mask = mask)\n        state_as_q_cross_attn = self.state_input_cross_attn(state, context = x, mask = state_mask)\n\n        projected_input = self.input_proj(torch.concat((input_as_q_cross_attn, input_attn), dim=2))\n        projected_state = self.state_proj(torch.concat((state_as_q_cross_attn, state_attn), dim=2))\n\n        input_residual = projected_input + x\n        state_residual = self.proj_gate(projected_state, state)\n\n        output = self.input_ff(input_residual) + input_residual\n        next_state = self.ff_gate(self.state_ff(state_residual), state_residual)\n\n        return output, next_state",
    "description": null,
    "url": null
}