{
    "acronym": "transformerxl",
    "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
    "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
    "authors": [
        "Zihang Dai",
        "Zhilin Yang",
        "Yiming Yang",
        "J. Carbonell",
        "Quoc V. Le",
        "R. Salakhutdinov"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "tldr": "This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.",
    "citationCount": 3248,
    "influentialCitationCount": 394,
    "code": "import sys\nimport math\nimport functools\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nsys.path.append('utils')\nfrom proj_adaptive_softmax import ProjectedAdaptiveLogSoftmax\nfrom log_uniform_sampler import LogUniformSampler, sample_logits\n\nfrom modules import PositionwiseFF, PositionalEmbedding, AdaptiveEmbedding\n\n\nclass RelMultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False):\n        super(RelMultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n    def _parallelogram_mask(self, h, w, left=False):\n        mask = torch.ones((h, w)).byte()\n        m = min(h, w)\n        mask[:m,:m] = torch.triu(mask[:m,:m])\n        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n\n        if left:\n            return mask\n        else:\n            return mask.flip(0)\n\n    def _shift(self, x, qlen, klen, mask, left=False):\n        if qlen > 1:\n            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n                                    device=x.device, dtype=x.dtype)\n        else:\n            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n\n        if left:\n            mask = mask.flip(1)\n            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n        else:\n            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n\n        x = x_padded.masked_select(mask[:,:,None,None]) \\\n                    .view(qlen, klen, x.size(2), x.size(3))\n\n        return x\n\n    def _rel_shift(self, x, zero_triu=False):\n        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n                               device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n\n        x = x_padded[1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(0), x.size(1)))\n            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        raise NotImplementedError\n\nclass RelPartialLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelPartialLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n\n    def forward(self, w, r, r_w_bias, r_r_bias, attn_mask=None, mems=None):\n        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n\n        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + r_r_bias\n        BD = torch.einsum('ibnd,jnd->ijbn', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[None,:,:,None], -float('inf')).type_as(attn_score)\n            elif attn_mask.dim() == 3:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass RelLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n    def forward(self, w, r_emb, r_w_bias, r_bias, attn_mask=None, mems=None):\n        # r_emb: [klen, n_head, d_head], used for term B\n        # r_w_bias: [n_head, d_head], used for term C\n        # r_bias: [klen, n_head], used for term D\n\n        qlen, bsz = w.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)\n\n        if klen > r_emb.size(0):\n            r_emb_pad = r_emb[0:1].expand(klen-r_emb.size(0), -1, -1)\n            r_emb = torch.cat([r_emb_pad, r_emb], 0)\n            r_bias_pad = r_bias[0:1].expand(klen-r_bias.size(0), -1)\n            r_bias = torch.cat([r_bias_pad, r_bias], 0)\n        else:\n            r_emb = r_emb[-klen:]\n            r_bias = r_bias[-klen:]\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias[None]                                   # qlen x bsz x n_head x d_head\n\n        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n        B_ = torch.einsum('ibnd,jnd->ijbn', (w_head_q, r_emb))                  # qlen x klen x bsz x n_head\n        D_ = r_bias[None, :, None]                                              # 1    x klen x 1   x n_head\n        BD = self._rel_shift(B_ + D_)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float('inf'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float('inf'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass RelLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelLearnableMultiHeadAttn(n_head, d_model, d_head, dropout,\n                                         **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n\n    def forward(self, dec_inp, r_emb, r_w_bias, r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r_emb, r_w_bias, r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelPartialLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelPartialLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n                            d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n\n    def forward(self, dec_inp, r, r_w_bias, r_r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass MemTransformerLM(nn.Module):\n    def __init__(self, n_token, n_layer, n_head, d_model, d_head, d_inner,\n                 dropout, dropatt, tie_weight=True, d_embed=None, \n                 div_val=1, tie_projs=[False], pre_lnorm=False,\n                 tgt_len=None, ext_len=None, mem_len=None, \n                 cutoffs=[], adapt_inp=False,\n                 same_length=False, attn_type=0, clamp_len=-1, \n                 sample_softmax=-1):\n        super(MemTransformerLM, self).__init__()\n        self.n_token = n_token\n\n        d_embed = d_model if d_embed is None else d_embed\n        self.d_embed = d_embed\n        self.d_model = d_model\n        self.n_head = n_head\n        self.d_head = d_head\n\n        self.word_emb = AdaptiveEmbedding(n_token, d_embed, d_model, cutoffs, \n                                          div_val=div_val)\n\n        self.drop = nn.Dropout(dropout)\n\n        self.n_layer = n_layer\n\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n        self.max_klen = tgt_len + ext_len + mem_len\n\n        self.attn_type = attn_type\n\n        self.layers = nn.ModuleList()\n        if attn_type == 0: # the default attention\n            for i in range(n_layer):\n                self.layers.append(\n                    RelPartialLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n        elif attn_type == 1: # learnable embeddings\n            for i in range(n_layer):\n                self.layers.append(\n                    RelLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n\n        self.sample_softmax = sample_softmax\n        # use sampled softmax\n        if sample_softmax > 0:\n            self.out_layer = nn.Linear(d_model, n_token)\n            if tie_weight:\n                self.out_layer.weight = self.word_emb.weight\n            self.tie_weight = tie_weight\n            self.sampler = LogUniformSampler(n_token, sample_softmax)\n\n        # use adaptive softmax (including standard softmax)\n        else:\n            self.crit = ProjectedAdaptiveLogSoftmax(n_token, d_embed, d_model, \n                                                    cutoffs, div_val=div_val)\n\n            if tie_weight:\n                for i in range(len(self.crit.out_layers)):\n                    self.crit.out_layers[i].weight = self.word_emb.emb_layers[i].weight\n\n            if tie_projs:\n                for i, tie_proj in enumerate(tie_projs):\n                    if tie_proj and div_val == 1 and d_model != d_embed:\n                        self.crit.out_projs[i] = self.word_emb.emb_projs[0]\n                    elif tie_proj and div_val != 1:\n                        self.crit.out_projs[i] = self.word_emb.emb_projs[i]\n\n        self.same_length = same_length\n        self.clamp_len = clamp_len\n\n        self._create_params()\n\n    def backward_compatible(self):\n        self.sample_softmax = -1\n\n    def _create_params(self):\n        if self.attn_type == 0: # default attention\n            self.pos_emb = PositionalEmbedding(self.d_model)\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        elif self.attn_type == 1: # learnable\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.n_head, self.d_head))\n            self.r_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head))\n        elif self.attn_type == 2: # absolute standard\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        elif self.attn_type == 3: # absolute deeper SA\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n\n    def init_mems(self):\n        if self.mem_len > 0:\n            mems = []\n            param = next(self.parameters())\n            for i in range(self.n_layer+1):\n                empty = torch.empty(0, dtype=param.dtype, device=param.device)\n                mems.append(empty)\n\n            return mems\n        else:\n            return None\n\n    def _update_mems(self, hids, mems, qlen, mlen):\n        # does not deal with None\n        if mems is None: return None\n\n        # mems is not None\n        assert len(hids) == len(mems), 'len(hids) != len(mems)'\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n            beg_idx = max(0, end_idx - self.mem_len)\n            for i in range(len(hids)):\n\n                cat = torch.cat([mems[i], hids[i]], dim=0)\n                new_mems.append(cat[beg_idx:end_idx].detach())\n\n        return new_mems\n\n    def _forward(self, dec_inp, mems=None):\n        qlen, bsz = dec_inp.size()\n\n        word_emb = self.word_emb(dec_inp)\n\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n        if self.same_length:\n            all_ones = word_emb.new_ones(qlen, klen)\n            mask_len = klen - self.mem_len\n            if mask_len > 0:\n                mask_shift_len = qlen - mask_len\n            else:\n                mask_shift_len = qlen\n            dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n                    + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1\n        else:\n            dec_attn_mask = torch.triu(\n                word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]\n\n        hids = []\n        if self.attn_type == 0: # default\n            pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device, \n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb)\n            pos_emb = self.drop(pos_emb)\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, pos_emb, self.r_w_bias,\n                        self.r_r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n                hids.append(core_out)\n        elif self.attn_type == 1: # learnable\n            core_out = self.drop(word_emb)\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                if self.clamp_len > 0:\n                    r_emb = self.r_emb[i][-self.clamp_len :]\n                    r_bias = self.r_bias[i][-self.clamp_len :]\n                else:\n                    r_emb, r_bias = self.r_emb[i], self.r_bias[i]\n\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, r_emb, self.r_w_bias[i],\n                        r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n                hids.append(core_out)\n        elif self.attn_type == 2: # absolute\n            pos_seq = torch.arange(klen - 1, -1, -1.0, device=word_emb.device,\n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb + pos_emb[-qlen:])\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and i == 0:\n                    mems_i += pos_emb[:mlen]\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n                hids.append(core_out)\n        elif self.attn_type == 3:\n            core_out = self.drop(word_emb)\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and mlen > 0:\n                    cur_emb = self.r_emb[i][:-qlen]\n                    cur_size = cur_emb.size(0)\n                    if cur_size < mlen:\n                        cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n                        cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n                    else:\n                        cur_emb = cur_emb[-mlen:]\n                    mems_i += cur_emb.view(mlen, 1, -1)\n                core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n                hids.append(core_out)\n\n        core_out = self.drop(core_out)\n\n        new_mems = self._update_mems(hids, mems, mlen, qlen)\n\n        return core_out, new_mems\n\n    def forward(self, data, target, *mems):\n        # nn.DataParallel does not allow size(0) tensors to be broadcasted.\n        # So, have to initialize size(0) mems inside the model forward.\n        # Moreover, have to return new_mems to allow nn.DataParallel to piece\n        # them together.\n        if not mems: mems = self.init_mems()\n\n        tgt_len = target.size(0)\n        hidden, new_mems = self._forward(data, mems=mems)\n\n        pred_hid = hidden[-tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            assert self.tie_weight\n            logit = sample_logits(self.word_emb,\n                self.out_layer.bias, target, pred_hid, self.sampler)\n            loss = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            loss = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target.view(-1))\n            loss = loss.view(tgt_len, -1)\n\n        if new_mems is None:\n            return [loss]\n        else:\n            return [loss] + new_mems\n",
    "description": null,
    "url": null
}