{
    "acronym": "peer",
    "title": "Mixture of A Million Experts",
    "seed_ids": [
        "productkeymem"
    ],
    "s2id": "ebe6b817ba8a44d0481598575904231c8324932e",
    "abstract": "The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.",
    "authors": [
        "Xu Owen He"
    ],
    "venue": "",
    "year": 2024,
    "tldr": "PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import Module, ModuleList\n\nfrom functools import partial\nfrom torch.utils.checkpoint import checkpoint\n\nimport einx\nfrom einops.layers.torch import Rearrange\n\n\nfrom math import sqrt\nfrom einops import einsum, pack, unpack\n\n\n\n# helper functions\n\ndef exists(v):\n    return v is not None\n\ndef default(v, d):\n    return v if exists(v) else d\n\n# rmsnorm\n\nclass RMSNorm(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.scale = dim ** 0.5\n        self.gamma = nn.Parameter(torch.zeros(dim))\n\n    def forward(self, x):\n        return F.normalize(x, dim = -1) * self.scale * (self.gamma + 1)\n\n# main class\n\nclass PEER(Module):\n    \"\"\"\n    following Algorithm 1 in the paper\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        *,\n        heads = 8,                       # tested up to 32 - (hk = heads * num_experts_per_head (16))\n        num_experts = 1_000_000,         # he chose 1 million\n        num_experts_per_head = 16,       # he settled on 16, but was 32 in PKM paper\n        activation = nn.GELU,\n        dim_key = None,\n        product_key_topk = None,\n        separate_embed_per_head = False, # @smerky notes that heads may retrieve same redundant neurons. this setting would allow for separate embeds per head and prevent that\n        pre_rmsnorm = False,\n        dropout = 0.\n    ):\n        \"\"\"\n        einops notation\n        b - batch\n        n - sequence\n        d - dimension\n        h - heads\n        p - 2 for product key\n        k - number of keys\n        \"\"\"\n\n        super().__init__()\n\n        self.norm = RMSNorm(dim) if pre_rmsnorm else nn.Identity()\n\n        # whether to do separate embedding per head\n\n        num_expert_sets = 1 if not separate_embed_per_head else heads\n\n        self.heads = heads\n        self.separate_embed_per_head = separate_embed_per_head\n        self.num_experts = num_experts\n\n        # experts that will form the mlp project in / out weights\n\n        self.weight_down_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n        self.weight_up_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n\n        # activation function, defaults to gelu\n\n        self.activation = activation()\n\n        # queries and keys for product-key\n\n        assert sqrt(num_experts).is_integer(), '`num_experts` needs to be a square'\n        assert (dim % 2) == 0, 'feature dimension should be divisible by 2'\n\n        dim_key = default(dim_key, dim // 2)\n        self.num_keys = int(sqrt(num_experts))\n\n        self.to_queries = nn.Sequential(\n            nn.Linear(dim, dim_key * heads * 2, bias = False),\n            Rearrange('b n (p h d) -> p b n h d', p = 2, h = heads)\n        )\n\n        self.product_key_topk = default(product_key_topk, num_experts_per_head)\n        self.num_experts_per_head = num_experts_per_head\n\n        self.keys = nn.Parameter(torch.zeros(heads, self.num_keys, 2, dim_key))\n        nn.init.normal_(self.keys, std = 0.02)\n\n        # dropout\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        x\n    ):\n\n        x = self.norm(x)\n\n        # queries\n\n        queries = self.to_queries(x)\n\n        # first get similarity with keys\n\n        sim = einsum(queries, self.keys, 'p b n h d, h k p d -> p b n h k')\n\n        # product key logic\n\n        (scores_x, scores_y), (indices_x, indices_y) = sim.topk(self.product_key_topk, dim = -1)\n\n        all_scores = einx.add('... i, ... j -> ... (i j)', scores_x, scores_y)\n        all_indices = einx.add('... i, ... j -> ... (i j)', indices_x * self.num_keys, indices_y)\n\n        scores, pk_indices = all_scores.topk(self.num_experts_per_head, dim = -1)\n\n        indices = all_indices.gather(-1, pk_indices)\n\n        # if separate embeds per head, add appropriate offsets per head\n\n        if self.separate_embed_per_head:\n            head_expert_offsets = torch.arange(self.heads, device = x.device) * self.num_experts\n            indices = einx.add('b n h k, h -> b n h k', indices, head_expert_offsets)\n\n        # build the weight matrices for projecting in and out\n        # basically the experts are the gathered parameters for an MLP\n\n        weights_down = self.weight_down_embed(indices)\n        weights_up = self.weight_up_embed(indices)\n\n        # below is basically Algorithm 1 in paper\n\n        x = einsum(x, weights_down, 'b n d, b n h k d -> b n h k')\n\n        x = self.activation(x)\n        x = self.dropout(x)\n\n        x = x * scores.softmax(dim = -1)\n\n        x = einsum(x, weights_up, 'b n h k, b n h k d -> b n d')\n\n        return x\n    \n\nclass ChunkedPEER(Module):\n    def __init__(\n        self,\n        peer: PEER,\n        seq_chunk_size: int = 128\n    ):\n        super().__init__()\n        self.peer = peer\n        self.seq_chunk_size = seq_chunk_size\n\n    def forward(\n        self,\n        x\n    ):\n        peer = self.peer\n\n        if self.training and x.requires_grad:\n            peer = partial(checkpoint, peer)            \n\n        out = []\n        for chunk in x.split(self.seq_chunk_size, dim = 1):\n            chunk_out = peer(chunk)\n            out.append(chunk_out)\n\n        return torch.cat(out, dim = 1)\n\n\n\n# main class\n\nclass PK(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads = 8,\n        dim_key = None,\n        num_keys = 1_000,\n        product_keys = 2,\n        product_key_topk = None,\n        final_topk = 16,\n        num_experts_per_head = 16\n    ):\n        \"\"\"\n        einops notation\n        b - batch\n        n - sequence\n        d - dimension\n        h - heads\n        p - product keys\n        k - number of keys\n        \"\"\"\n\n        super().__init__()\n        assert (dim % 2) == 0\n        dim_key = default(dim_key, dim // 2)\n\n        self.to_queries = nn.Sequential(\n            nn.Linear(dim, dim_key * product_keys * heads, bias = False),\n            Rearrange('b n (p h d) -> p b n h d', h = heads, p = product_keys)\n        )\n\n        self.num_keys = num_keys\n        self.product_keys = product_keys\n\n        self.keys = nn.Parameter(torch.zeros(product_keys, num_keys, heads, dim_key))\n        nn.init.normal_(self.keys, std = 0.02)\n\n        product_key_topk = default(product_key_topk, final_topk)\n        assert final_topk <= (product_key_topk ** product_keys)\n\n        self.topk = product_key_topk\n        self.final_topk = final_topk\n\n        # the maximum index, or the total space being indexed into\n\n        self.max_index = int(num_keys ** product_keys)\n\n    def forward(\n        self,\n        x,\n        softmax_scores = False\n    ):\n\n        queries = self.to_queries(x)\n\n        sim = einsum(queries, self.keys, 'p b n h d, p k h d -> p b n h k')\n\n        scores, indices = sim.topk(self.topk, dim = -1)\n\n        # cartesian product indices\n\n        strides = self.num_keys ** torch.arange(self.product_keys, device = x.device)\n        indices = einx.multiply('p ..., p -> p ...', indices, strides)\n\n        index, *rest_indices = indices\n\n        for rest_index in rest_indices:\n            index = einx.add('... i, ... j -> ... (i j)', index, rest_index)\n\n        # cartesian product score\n\n        score, *rest_scores = scores\n\n        for rest_score in rest_scores:\n            score = einx.add('... i, ... j -> ... (i j)', score, rest_score)\n\n        final_scores, final_indices = score, index\n\n        # final topk\n\n        final_scores, pk_indices = final_scores.topk(self.final_topk, dim = -1)\n\n        final_indices = final_indices.gather(-1, pk_indices)\n\n        if softmax_scores:\n            final_scores = final_scores.softmax(dim = -1)\n\n        return final_scores, final_indices\n\n\n# helper functions\n\ndef exists(v):\n    return v is not None\n\ndef default(v, d):\n    return v if exists(v) else d\n\ndef pack_one(t, pattern):\n    return pack([t], pattern)\n\ndef unpack_one(t, ps, pattern):\n    return unpack(t, ps, pattern)[0]\n\n# rmsnorm\n\nclass RMSNorm(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.scale = dim ** 0.5\n        self.gamma = nn.Parameter(torch.zeros(dim))\n\n    def forward(self, x):\n        return F.normalize(x, dim = -1) * self.scale * (self.gamma + 1)\n\n# main class\n\nclass PKAttention(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        causal = True,\n        heads = 8,\n        num_key_values = 1_000_000,\n        key_value_pk_topk = 16,\n        dim_key = None,\n        product_keys = 2,\n        pre_rmsnorm = False,\n        dropout = 0.\n    ):\n        \"\"\"\n        einops notation\n        b - batch\n        n - sequence\n        d - dimension\n        h - heads\n        p - 2 for product key\n        k - number of keys\n        \"\"\"\n\n        super().__init__()\n        self.causal = causal\n        self.heads = heads\n        self.num_key_values = num_key_values\n\n        self.norm = RMSNorm(dim) if pre_rmsnorm else nn.Identity()\n\n        # experts that will form the mlp project in / out weights\n\n        self.to_queries = nn.Sequential(\n            nn.Linear(dim, dim * heads, bias = False),\n            Rearrange('b n (h d) -> b n h d', h = heads)\n        )\n\n        # keys and values selected using product-key\n\n        self.keys = nn.EmbeddingBag(num_key_values * heads, dim, mode = 'sum')\n        self.values = nn.EmbeddingBag(num_key_values * heads, dim, mode = 'sum')\n\n        assert sqrt(num_key_values).is_integer(), '`num_key_values` needs to be a square'\n        assert (dim % 2) == 0, 'feature dimension should be divisible by 2'\n\n        self.to_kv_pk_indices = PK(\n            dim = dim,\n            num_keys = int(sqrt(num_key_values)),\n            final_topk = key_value_pk_topk,\n            product_keys = product_keys\n        )\n\n        # dropout\n\n        self.dropout = nn.Dropout(dropout)\n\n        # output\n\n        self.to_out = nn.Sequential(\n            Rearrange('b h n d -> b n (h d)'),\n            nn.Linear(dim * heads, dim, bias = False)\n        )\n\n    def forward(\n        self,\n        x,\n        mask = None\n    ):\n        device = x.device\n\n        x = self.norm(x)\n\n        # queries\n\n        q = self.to_queries(x)\n\n        q = q * (q.shape[-1] ** -0.5)\n\n        # keys and values\n\n        kv_scores, indices = self.to_kv_pk_indices(x, softmax_scores = True)\n\n        offsets = torch.arange(self.heads, device = device) * self.num_key_values\n        indices = einx.add('b n h k, h -> b n h k', indices, offsets)\n\n        indices, packed_shape = pack_one(indices, '* k')\n        kv_scores, _ = pack_one(kv_scores, '* k')\n\n        k, v = self.keys(indices, per_sample_weights = kv_scores), self.values(indices, per_sample_weights = kv_scores)\n\n        k = unpack_one(k, packed_shape, '* d')\n        v = unpack_one(v, packed_shape, '* d')\n\n        # usual multihead self attention\n\n        sim = einsum(q, k, 'b i h d, b j h d -> b h i j')\n\n        # whether causal or not\n\n        if self.causal:\n            assert not exists(mask)\n            i, j, device = *sim.shape[-2:], x.device\n            causal_mask = torch.ones((i, j), device = device, dtype = torch.bool).triu(j - i + 1)\n            sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        elif exists(mask):\n            sim = einx.where('b j, b h i j, -> b h i j', mask, sim, -torch.finfo(sim.dtype).max)\n\n        # attention\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        # aggregate\n\n        out = einsum(attn, v, 'b h i j, b j h d -> b h i d')\n\n        # combine heads\n\n        return self.to_out(out)\n\n# main\n\nif __name__ == '__main__':\n    pk = PK(\n        dim = 512,\n        num_keys = 100,\n        final_topk = 10,\n        product_keys = 3\n    )\n\n    x = torch.randn(2, 1024, 512)\n    score, indices = pk(x)\n\n    assert score.shape == (2, 1024, 8, 10)\n    assert indices.shape == (2, 1024, 8, 10)\n\n    peer_attn = PKAttention(\n        dim = 256,\n        causal = True,\n        heads = 8,\n        num_key_values = int(1e4),\n        pre_rmsnorm = True\n    )\n\n    x = torch.randn(2, 512, 256)\n\n    out = peer_attn(x) + x\n\n    assert x.shape == out.shape",
    "description": null,
    "url": null
}