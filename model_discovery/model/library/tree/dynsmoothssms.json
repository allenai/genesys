{
    "acronym": "dynsmoothssms",
    "title": "State Space Models are Comparable to Transformers in Estimating Functions with Dynamic Smoothness",
    "seed_ids": [
        "theoryssms",
        "mamba",
        "hyena",
        "h3",
        "s4",
        "lineartransformer",
        "transformer"
    ],
    "s2id": "50457c4cc3b79b3332b66028e57c2d1247c8bcfa",
    "abstract": "Deep neural networks based on state space models (SSMs) are attracting much attention in sequence modeling since their computational cost is significantly smaller than that of Transformers. While the capabilities of SSMs have been primarily investigated through experimental comparisons, theoretical understanding of SSMs is still limited. In particular, there is a lack of statistical and quantitative evaluation of whether SSM can replace Transformers. In this paper, we theoretically explore in which tasks SSMs can be alternatives of Transformers from the perspective of estimating sequence-to-sequence functions. We consider the setting where the target function has direction-dependent smoothness and prove that SSMs can estimate such functions with the same convergence rate as Transformers. Additionally, we prove that SSMs can estimate the target function, even if the smoothness changes depending on the input sequence, as well as Transformers. Our results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice.",
    "authors": [
        "Naoki Nishikawa",
        "Taiji Suzuki"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "The results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice and prove that SSMs can estimate such functions with the same convergence rate as Transformers.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}