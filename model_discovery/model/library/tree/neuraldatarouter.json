{
    "acronym": "neuraldatarouter",
    "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization ",
    "seed_ids": [
        "nystromformer",
        "transformerxl",
        "universaltrans",
        "transformer"
    ],
    "s2id": "e528466e2aff981511d4ca6e063211297c0b4175",
    "abstract": "Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.",
    "authors": [
        "R. Csord\u00e1s",
        "Kazuki Irie",
        "J. Schmidhuber"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": "This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.",
    "citationCount": 42,
    "influentialCitationCount": 7,
    "code": "from typing import Optional, Dict, Any\nimport torch\nimport torch.nn\nimport torch.nn.functional as F\nfrom .multi_head_relative_pos_attention import FixedRelativeMultiheadAttention, AttentionMask\nfrom layers.layer_with_visualization import LayerWithVisualization\nfrom layers.regularized_layer import RegularizedLayer\nimport framework\nimport numpy as np\nimport math\n\n\nclass UniversalTransformerRandomLayerEncoder(torch.nn.Module):\n    def __init__(self, layer, n_layers: int, n_extra: int, n_test: Optional[int] = None, *args, **kwargs):\n        super().__init__()\n        self.layer = layer(*args, **kwargs)\n        self.n_extra = n_extra\n        self.n_layers = n_layers\n        self.n_test = n_test\n\n    def set_n_layers(self, n_layers: int):\n        self.layers = [self.layer] * n_layers\n\n    def forward(self, data: torch.Tensor, *args, **kwargs):\n        self.set_n_layers(np.random.randint(self.n_layers, self.n_extra + self.n_layers + 1) if self.training else \\\n                          (self.n_test or self.n_layers))\n        for l in self.layers:\n            data = l(data, *args, **kwargs)\n        return data\n\n\ndef UniversalTransformerRandomLayerEncoderWithLayer(layer):\n    return lambda *args, **kwargs: UniversalTransformerRandomLayerEncoder(layer, *args, **kwargs)\n\n\nclass NDRResidual(RegularizedLayer, LayerWithVisualization):\n    def __init__(self, d_model: int, nhead: int, dropout: float, scalar_gate: bool = False,\n                abs_gate:bool = True, attention_dropout=0, p_gate_drop=0, **kwargs):\n        super().__init__()\n        self.plot_cache = []\n\n        self.reg_loss = 0\n\n        self.att = FixedRelativeMultiheadAttention(d_model, nhead, dropout=attention_dropout, absolute_gate=abs_gate)\n\n        self.p1 = torch.nn.Linear(d_model, d_model*4)\n        self.p2 = torch.nn.Linear(d_model*4, d_model)\n\n\n        self.g1 = torch.nn.Linear(d_model, d_model)\n        self.g2 = torch.nn.Linear(d_model, 1 if scalar_gate else d_model)\n\n        self.n1 = torch.nn.LayerNorm(d_model)\n        self.nmerge = torch.nn.LayerNorm(d_model)\n\n        self.drop = torch.nn.Dropout(dropout)\n\n        self.g2.bias.data.fill_(-3)\n\n        self.p_gate_drop = p_gate_drop\n\n        self.reset_parameters()\n\n    def forward(self, src: torch.Tensor, mask: Optional[AttentionMask] = None) -> torch.Tensor:\n        \n        input = self.att(src, src, mask)\n\n        net = self.nmerge(src + self.drop(input))\n\n        mid = self.drop(torch.relu(self.p1(net)))\n        # proj = torch.relu(self.p2(mid))\n        proj = self.p2(mid)\n        # proj = self.n1(proj) #* self.scale\n        proj = torch.tanh(proj)\n\n        gate = self.g2(self.drop(torch.relu(self.g1(net))))\n        bgate = torch.sigmoid(gate)\n        # bgate = torch.softmax(gate, -2)\n\n        if self.training and self.p_gate_drop>0:\n            bgate = bgate.masked_fill(torch.rand(*bgate.shape[:-1], 1, device=bgate.device, dtype=bgate.dtype) < self.p_gate_drop, 0)\n\n        if self.visualization_enabled:\n            self.plot_cache.append(bgate[0])\n\n        src = src * (1-bgate) + proj * bgate\n\n\n        # self.add_reg(lambda: 0.0000001 * ((bgate+0.2).clamp(max=1)).sum() / bgate.shape[0])\n        # self.add_reg(lambda: 0.000001 * (bgate * (1-bgate)).sum() / bgate.shape[0])\n\n        return src\n\n    def plot(self, options: Dict[str, Any]) -> Dict[str, Any]:\n        r = {}\n        if self.visualization_enabled:\n            r[\"gate\"] = framework.visualize.plot.AnimatedHeatmap(\n                        torch.stack(self.plot_cache, 0).transpose(1,2),\n                        ylabel=\"dest\", xlabel=\"src\", textval=False, x_marks=options.get(\"steplabel\"))\n            self.plot_cache.clear()\n\n        return r\n\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self.p1.weight, gain=torch.nn.init.calculate_gain('relu'))\n        # torch.nn.init.xavier_uniform_(self.p2.weight)  \n\n        torch.nn.init.xavier_uniform_(self.g1.weight, gain=torch.nn.init.calculate_gain('relu'))\n        torch.nn.init.xavier_uniform_(self.g2.weight, gain=torch.nn.init.calculate_gain('sigmoid'))",
    "description": null,
    "url": null
}