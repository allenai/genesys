{
    "acronym": "hydra",
    "title": "Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers",
    "seed_ids": [],
    "s2id": "ea507df05bb5fe32cd8af80602708713c9bd2ba2",
    "abstract": "A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating sequence mixer and channel mixer layers. This paper studies a unifying matrix mixer view of sequence mixers that can be conceptualized as a linear map on the input sequence. This framework encompasses a broad range of well-known sequence models, including the self-attention of Transformers as well as recent strong alternatives such as structured state space models (SSMs), and allows understanding downstream characteristics such as efficiency and expressivity through properties of their structured matrix class. We identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent SSMs such as Mamba. Furthermore, the matrix mixer framework offers a systematic approach to developing sequence mixers with desired properties, allowing us to develop several new sub-quadratic sequence models. In particular, we propose a natural bidirectional extension of the Mamba model (Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks. As a drop-in replacement for attention layers, Hydra outperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1 accuracy on ImageNet.",
    "authors": [
        "Sukjun Hwang",
        "Aakash Lahoti",
        "Tri Dao",
        "Albert Gu"
    ],
    "venue": "",
    "year": 2024,
    "tldr": "This paper proposes a natural bidirectional extension of the Mamba model (Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "# Copyright (c) 2024, Sukjun Hwang, Aakash Lahoti, Ratish Puduppully, Tri Dao, Albert Gu.\n# Base code from https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\n\ntry:\n    from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated\nexcept ImportError:\n    RMSNormGated = None\n\nfrom mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined\n\n\nclass Hydra(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        d_state=64,\n        d_conv=7,\n        conv_init=None,\n        expand=2,\n        headdim=64,\n        ngroups=1,\n        dt_min=0.001,\n        dt_max=0.1,\n        dt_init_floor=1e-4,\n        dt_limit=(0.0, float(\"inf\")),\n        learnable_init_states=False,\n        activation=\"swish\",\n        bias=False,\n        conv_bias=True,\n        # Fused kernel and sharding options\n        chunk_size=256,\n        layer_idx=None,  # Absorb kwarg for general module\n        device=None,\n        dtype=None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.conv_init = conv_init\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.dt_limit = dt_limit\n        self.learnable_init_states = learnable_init_states\n        self.activation = activation\n        self.chunk_size = chunk_size\n        self.layer_idx = layer_idx\n\n        # Order: [z, x, B, C, dt]\n        d_in_proj = 2 * self.d_inner + 2 * (2 * self.ngroups * self.d_state) + 2 * self.nheads\n        self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n\n        conv_dim = self.d_inner + 2 * (2 * self.ngroups * self.d_state)\n        self.conv1d = nn.Conv1d(\n            in_channels=conv_dim,\n            out_channels=conv_dim,\n            bias=conv_bias,\n            kernel_size=d_conv,\n            groups=conv_dim,\n            padding=d_conv // 2,\n            **factory_kwargs,\n        )\n        if self.conv_init is not None:\n            nn.init.uniform_(self.conv1d.weight, -self.conv_init, self.conv_init)\n        # self.conv1d.weight._no_weight_decay = True\n\n        if self.learnable_init_states:\n            self.init_states = nn.Parameter(torch.zeros(self.nheads, self.headdim, self.d_state, **factory_kwargs))\n            self.init_states._no_weight_decay = True\n\n        self.act = nn.SiLU()\n\n        # Initialize log dt bias\n        dt = torch.exp(\n            torch.rand(self.nheads, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n            + math.log(dt_min)\n        )\n        dt = torch.clamp(dt, min=dt_init_floor)\n        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        self.dt_bias = nn.Parameter(inv_dt)\n        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n        # name.endswith(\"bias\") in param_grouping.py\n        self.dt_bias._no_weight_decay = True\n\n        # A parameter\n        A = torch.ones(self.nheads, dtype=torch.float32, device=device)\n        A_log = torch.log(A).to(dtype=dtype)\n        self.A_log = nn.Parameter(A_log)\n        # self.register_buffer(\"A_log\", torch.zeros(self.nheads, dtype=torch.float32, device=device), persistent=True)\n        self.A_log._no_weight_decay = True\n\n        # D \"skip\" parameter\n        self.D = nn.Parameter(torch.ones(self.nheads, device=device))\n        self.D._no_weight_decay = True\n        self.fc_D = nn.Linear(self.d_inner, self.nheads, bias=False, **factory_kwargs)\n\n        # Extra normalization layer right before output projection\n        assert RMSNormGated is not None\n        self.norm = RMSNormGated(self.d_inner, eps=1e-5, norm_before_gate=True, **factory_kwargs)\n\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n\n    def forward(self, u, seq_idx=None):\n        \"\"\"\n        u: (B, L, D)\n        Returns: same shape as u\n        \"\"\"\n        batch, seqlen, dim = u.shape\n\n        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj)\n        A = -torch.exp(self.A_log.float())  # (nheads) or (d_inner, d_state)\n        initial_states = repeat(self.init_states, \"... -> b ...\", b=2*batch) if self.learnable_init_states else None\n        dt_limit_kwargs = {} if self.dt_limit == (0.0, float(\"inf\")) else dict(dt_limit=self.dt_limit)\n\n        z, xBC, dt = torch.split(\n            zxbcdt,\n            [self.d_inner, self.d_inner + 2 * (2 * self.ngroups * self.d_state), 2 * self.nheads],\n            dim=-1\n        )\n\n        dt = torch.cat((dt[:, :, :self.nheads], torch.flip(dt[:, :, self.nheads:], (1,))), dim=0)\n        dt = F.softplus(dt + self.dt_bias)  # (2 * B, L, nheads)\n        assert self.activation in [\"silu\", \"swish\"]\n\n        # 1D Convolution\n        xBC = self.act(\n            self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)\n        )  # (B, L, self.d_inner + 2 * (2 * ngroups * d_state))\n\n        # Split into 3 main branches: X, B, C\n        # These correspond to V, K, Q respectively in the SSM/attention duality\n        x, BC = torch.split(xBC, [self.d_inner, 2 * (2 * self.ngroups * self.d_state)], dim=-1)\n        x_og = x\n        x = torch.cat((x, torch.flip(x, (1,))), dim=0)\n        BC = torch.cat(\n            (BC[:, :, :2 * self.ngroups * self.d_state],\n             torch.flip(BC[:, :, 2 * self.ngroups * self.d_state:], (1,))),\n            dim=0\n        )\n        B, C = torch.split(BC, [self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n\n        y = mamba_chunk_scan_combined(\n            rearrange(x, \"b l (h p) -> b l h p\", p=self.headdim),\n            dt,\n            A,\n            rearrange(B, \"b l (g n) -> b l g n\", g=self.ngroups),\n            rearrange(C, \"b l (g n) -> b l g n\", g=self.ngroups),\n            chunk_size=self.chunk_size,\n            D=None,\n            z=None,\n            seq_idx=seq_idx,\n            initial_states=initial_states,\n            **dt_limit_kwargs,\n        )\n        y = rearrange(y, \"b l h p -> b l (h p)\")\n        y = torch.roll(y, shifts=1, dims=1)\n        y[:, 0, :] = 0.0\n        y_fw, y_bw = y[:batch], torch.flip(y[batch:], (1,))\n        y = y_fw + y_bw + x_og * repeat(\n            F.linear(x_og, self.fc_D.weight, bias=self.D), \"b l h -> b l (h p)\", p=self.headdim\n        )\n\n        # Multiply \"gate\" branch and apply extra normalization layer\n        y = self.norm(y, z)\n        out = self.out_proj(y)\n\n        return out",
    "description": null,
    "url": null
}