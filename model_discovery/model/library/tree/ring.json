{
    "acronym": "ring",
    "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
    "seed_ids": [
        "bpt",
        "flashattn",
        "mea",
        "gpt3",
        "transformer"
    ],
    "s2id": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
    "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
    "authors": [
        "Hao Liu",
        "Matei Zaharia",
        "Pieter Abbeel"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.",
    "citationCount": 68,
    "influentialCitationCount": 7,
    "code": "from __future__ import annotations\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn, einsum, Tensor\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.nn import Module, ModuleList\n\nfrom einops import rearrange, repeat\n\nfrom beartype import beartype\n\nfrom ring_attention_pytorch.ring import (\n    is_distributed,\n    get_rank,\n    get_world_size\n)\n\nfrom ring_attention_pytorch.distributed import (\n    split_by_rank,\n    AllGather\n)\n\n# helper functions\n\ndef exists(v):\n    return v is not None\n\ndef default(v, d):\n    return v if exists(v) else d\n\ndef cast_tuple(t, length = 1):\n    return t if isinstance(t, tuple) else ((t,) * length)\n\ndef divisible_by(num, den):\n    return (num % den) == 0\n\ndef softclamp(t, value):\n    return (t / value).tanh() * value\n\n@beartype\ndef default_attention(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    mask: Tensor | None = None,\n    causal: bool = False,\n    softclamp_qk_sim: bool = False,\n    softclamp_value: float = 50.\n):\n    device = q.device\n    q = q * (q.shape[-1] ** -0.5)\n\n    mask_value = -torch.finfo(q.dtype).max\n\n    # account for grouped query attention\n\n    heads, kv_heads = q.shape[-2], k.shape[-2]\n    assert divisible_by(heads, kv_heads)\n    q_head_groups = heads // kv_heads\n\n    k, v = map(lambda t: repeat(t, '... h d -> ... (g h) d', g = q_head_groups), (k, v))\n\n    # similarity\n\n    sim = einsum('b i h d, b j h d -> b h i j', q, k)\n\n    # softclamp\n\n    if softclamp_qk_sim:\n        sim = softclamp(sim, softclamp_value)\n\n    # masking\n\n    if causal:\n        i, j = sim.shape[-2:]\n        causal_mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)\n        sim = torch.where(causal_mask, mask_value, sim)\n\n    elif exists(mask):\n        mask = rearrange(mask, 'b j -> b 1 1 j')\n        sim = sim.masked_fill(~mask, mask_value)\n\n    # attend\n\n    attn = sim.softmax(dim = -1)\n\n    # aggregate\n\n    out = einsum('b h i j, b j h d -> b i h d', attn, v)\n\n    return out\n\n# rotary embeddings with modifications to support striped attention\n\nclass RingRotaryEmbedding(Module):\n    @beartype\n    def __init__(\n        self,\n        dim,\n        ring: bool = False,\n        striped: bool = False,\n        buckets: int = 1,        # in striped attention with flash buckets > 1, one needs to specify the number of buckets per machine\n        theta = 10000\n    ):\n        super().__init__()\n        self.ring = ring\n        self.striped = striped\n        self.buckets = buckets\n\n        inv_freq = theta ** -(torch.arange(0, dim, 2).float() / dim)\n        self.register_buffer('inv_freq', inv_freq)\n\n    @property\n    def device(self):\n        return self.inv_freq.device\n\n    @property\n    def is_cuda(self):\n        return self.inv_freq.is_cuda\n\n    @autocast(enabled = False)\n    @beartype\n    def forward(\n        self,\n        seq_len: int\n    ):\n        device = self.device\n\n        pos = None\n\n        if self.ring:\n            if self.striped:\n                buckets = 1 if self.is_cuda else self.buckets\n                ring_stride = get_world_size() * buckets\n\n                pos = torch.arange(seq_len // buckets, device = device)\n                pos = repeat(pos, 'n -> n b', b = buckets)\n\n                pos = pos * ring_stride\n                pos += torch.arange(buckets, device = device) + (get_rank() * buckets)\n                pos = rearrange(pos, 'n b -> (b n)')\n\n            else:\n                pos = torch.arange(seq_len, device = device)\n                pos += seq_len * get_rank()\n        else:\n            pos = torch.arange(seq_len, device = device)\n\n        pos = pos.type_as(self.inv_freq)\n        freqs = einsum('i , j -> i j', pos, self.inv_freq)\n        return torch.cat((freqs, freqs), dim = -1)\n\ndef rotate_half(x):\n    x1, x2 = x.chunk(2, dim = -1)\n    return torch.cat((-x2, x1), dim=-1)\n\n@autocast(enabled = False)\ndef apply_rotary_pos_emb(pos, t):\n    pos = rearrange(pos, 'n d -> n 1 d')\n    return t * pos.cos() + rotate_half(t) * pos.sin()\n\n# batch to sequence sharding and back\n\ndef pad_at_dim(\n    t: Tensor,\n    pad: Tuple[int, int],\n    *,\n    dim = -1,\n    value = 0.\n):\n    dims_from_right = (- dim - 1) if dim < 0 else (t.ndim - dim - 1)\n    zeros = ((0, 0) * dims_from_right)\n    return F.pad(t, (*zeros, *pad), value = value)\n\ndef pad_to_multiple(\n    x: Tensor,\n    length: int,\n    pad_value = 0\n):\n    seq_len = x.shape[1]\n    remainder = seq_len % length\n\n    if remainder == 0:\n        return x, 0\n\n    pad_length = length - remainder\n    return pad_at_dim(x, (0, pad_length), value = pad_value, dim = 1), pad_length\n\ndef maybe_pad_seq_and_mask(\n    x: Tensor,\n    mask: Tensor | None,\n    seq_size: int\n):\n    orig_x, device, shape = x, x.device, x.shape[:2]\n    seq_len = shape[-1]\n\n    # auto pad sequence and mask, as ring passing makes assumption tensor is all same shape\n\n    x, pad_length = pad_to_multiple(x, seq_size)\n\n    if pad_length == 0:\n        return x, mask\n\n    if not exists(mask):\n        mask = torch.ones(shape, device = device).bool()\n\n    mask, _ = pad_to_multiple(mask, seq_size, pad_value = False)\n\n    return x, mask\n\ndef sharded_batch_to_sharded_seq(\n    x: Tensor,\n    mask: Tensor | None,\n    seq_size: int\n):\n    assert is_distributed()\n\n    # all gather across batch\n\n    all_gather = AllGather(dim = 0)\n\n    x, sizes = all_gather(x)\n\n    if exists(mask):\n        mask, _ = all_gather(mask)\n\n    # first make sure world size is divisible by the sequence size\n\n    world_size = get_world_size()\n\n    total_split_seq = x.shape[1] // seq_size\n\n    assert divisible_by(world_size, total_split_seq)\n\n    num_sharded_batches = world_size // total_split_seq\n\n    x = rearrange(x, '(b s) n ... -> b (s n) ...', s = num_sharded_batches)\n\n    # then split sequence across machines\n\n    x = x.split(seq_size, dim = 1)\n\n    x, _ = split_by_rank(x)\n\n    if exists(mask):\n        mask = rearrange(mask, '(b s) n -> b (s n)', s = num_sharded_batches)\n        mask = mask.split(seq_size, dim = -1)\n        mask, _ = split_by_rank(mask)\n\n    return (x, mask), sizes, num_sharded_batches\n\ndef sharded_seq_to_sharded_batch(\n    logits: Tensor,\n    sizes,\n    num_sharded_batches = 1\n):\n    all_gather = AllGather(dim = -2) # all gather across sequence\n\n    logits, _ = all_gather(logits)\n\n    logits = rearrange(logits, 'b (s n) c -> (b s) n c', s = num_sharded_batches)\n\n    logits = logits.split(sizes.tolist(), dim = 0)\n\n    logits, _ = split_by_rank(logits)\n\n    return logits\n\n# main class\n\nclass RingAttention(Module):\n    @beartype\n    def __init__(\n        self,\n        dim: int,\n        *,\n        dim_head: int = 64,\n        heads: int = 8,\n        num_grouped_query_heads: int = 1,\n        causal: bool = False,\n        eps: float = 1e-10,\n        bucket_size: int = 512,\n        ring_attn: bool = False,\n        ring_seq_size: int = 512,\n        max_lookback_seq_len: int | None = None,\n        striped_ring_attn: bool = False,\n        auto_shard_seq: bool = False,\n        prenorm: bool = True,\n        force_regular_attn: bool = False,\n        rotary_embed: bool = False,\n        rotary_embed_theta: int = 10000,\n        use_cuda_kernel: bool | None = None\n    ):\n        super().__init__()\n        # whether to use flash attention cuda kernel\n\n        use_cuda_kernel = default(use_cuda_kernel, torch.cuda.is_available())\n        assert not (use_cuda_kernel and not torch.cuda.is_available())\n        self.use_cuda_kernel = use_cuda_kernel\n\n        self.eps = eps\n        self.heads = heads\n        self.dim_head = dim_head\n\n        assert divisible_by(heads, num_grouped_query_heads), f'number of query heads ({heads}) must be divisible by the groups ({num_grouped_query_heads})'\n\n        kv_heads = heads // num_grouped_query_heads\n        self.num_grouped_query_heads = num_grouped_query_heads\n        self.qkv_head_breakdown = (heads, kv_heads, kv_heads)\n\n        self.scale = dim_head ** -0.5\n        self.causal = causal\n\n        assert (not ring_attn) or divisible_by(ring_seq_size, bucket_size), f'ring seq size {ring_seq_size} is not divisible by bucket size {bucket_size}'\n\n        self.ring_attn = ring_attn\n        self.max_lookback_seq_len = max_lookback_seq_len\n        self.striped_ring_attn = striped_ring_attn\n\n        self.using_striped_ring_cuda = striped_ring_attn and use_cuda_kernel\n\n        self.force_regular_attn = force_regular_attn\n        self.auto_shard_seq = default(auto_shard_seq, ring_attn) # this should be done at the transformer level on the token ids for efficiency, but for testing purposes\n\n        assert not (not self.ring_attn and self.auto_shard_seq)\n\n        self.ring_seq_size = ring_seq_size\n        self.bucket_size = bucket_size\n\n        # rotary\n\n        self.rotary_embed = None\n        if rotary_embed:\n            self.rotary_embed = RingRotaryEmbedding(\n                dim = dim_head,\n                ring = ring_attn,\n                striped = striped_ring_attn,\n                theta = rotary_embed_theta,\n                buckets = ring_seq_size // bucket_size\n            )\n\n        # projections\n\n        dim_inner = dim_head * heads\n        dim_kv_inner = dim_head * kv_heads\n\n        self.to_qkv_split = (dim_inner, dim_kv_inner, dim_kv_inner)\n\n        self.to_qkv = nn.Sequential(\n            RMSNorm(dim) if prenorm else nn.Identity(),\n            nn.Linear(dim, dim_inner + (dim_kv_inner * 2), bias = False)\n        )\n\n        self.to_out = nn.Linear(dim_inner, dim, bias = False)\n\n    def forward(\n        self,\n        x,\n        mask = None,\n        rotary_emb = None,\n        ring_size = None,\n    ):\n        \"\"\"\n        einstein notation\n\n        b - batch\n        h - heads\n        d - feature dimension\n        n, i, j - sequence\n        \"\"\"\n\n        ring_size = default(ring_size, get_world_size())\n        ring_attn = self.ring_attn & is_distributed()\n        auto_shard_seq = self.auto_shard_seq & is_distributed()\n\n        using_striped_ring_cuda = x.is_cuda and self.using_striped_ring_cuda\n        striped_bucket_size = self.bucket_size if not using_striped_ring_cuda else self.ring_seq_size\n\n        seq_len = x.shape[1]\n\n        if auto_shard_seq:\n            x, mask = maybe_pad_seq_and_mask(x, mask, self.ring_seq_size)\n\n            if self.striped_ring_attn:\n                x = rearrange(x, 'b (i j) d -> b (j i) d', i = striped_bucket_size)\n\n                if exists(mask):\n                    mask = rearrange(mask, 'b (i j) -> b (j i)', i = striped_bucket_size)\n\n            (x, mask), batch_sizes, num_sharded_batches = sharded_batch_to_sharded_seq(x, mask, self.ring_seq_size)\n\n        device = x.device\n\n        qkv = self.to_qkv(x)\n\n        q, k, v = rearrange(qkv, 'b n (h d) -> b n h d', d = self.dim_head).split(self.qkv_head_breakdown, dim = -2)\n\n        # rotary relative positions\n\n        if not exists(rotary_emb) and exists(self.rotary_embed):\n            rotary_emb = self.rotary_embed(q.shape[-3])\n\n        if exists(rotary_emb):\n            q = apply_rotary_pos_emb(rotary_emb, q)\n            k = apply_rotary_pos_emb(rotary_emb, k)\n\n        # regular attention vs flash w/ or w/o kv ring reduce\n\n        out = default_attention(q, k, v, mask = mask, causal = self.causal)\n\n\n        # combine heads\n\n        out = rearrange(out, 'b n h d -> b n (h d)')\n        out = self.to_out(out)\n\n        if auto_shard_seq:\n            out = sharded_seq_to_sharded_batch(out, batch_sizes, num_sharded_batches)\n\n            if self.striped_ring_attn:\n                out = rearrange(out, 'b (j i) d -> b (i j) d', i = striped_bucket_size)\n\n            out = out[:, :seq_len]\n\n        return out\n\n# simple transformer for end2end testing\n\nclass RMSNorm(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.scale = dim ** 0.5\n        self.gamma = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        return F.normalize(x, dim = -1) * self.scale * self.gamma\n",
    "description": null,
    "url": null
}