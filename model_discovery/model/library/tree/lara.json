{
    "title": "Linear complexity randomized self-attention mechanism",
    "acronym": "lara",
    "s2id": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
    "abstract": "Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": "A novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers is proposed and sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA).",
    "citationCount": 23,
    "influentialCitationCount": 1,
    "seed_ids": [
        "scatterbrain",
        "perceiverio",
        "nystromformer",
        "performer",
        "lineartransformer",
        "linformer",
        "sinkhorn",
        "reformer",
        "universaltrans",
        "transformer",
        "bert"
    ],
    "code": null
}