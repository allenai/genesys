{
    "acronym": "lara",
    "title": "Linear complexity randomized self-attention mechanism",
    "seed_ids": [
        "scatterbrain",
        "perceiverio",
        "nystromformer",
        "performer",
        "lineartransformer",
        "linformer",
        "sinkhorn",
        "reformer",
        "universaltrans",
        "transformer",
        "bert"
    ],
    "s2id": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
    "abstract": "Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.",
    "authors": [
        "Lin Zheng",
        "Chong Wang",
        "Lingpeng Kong"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": "A novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers is proposed and sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA).",
    "citationCount": 23,
    "influentialCitationCount": 1,
    "code": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom efficient_attention import MultiheadAttention, add_nested_argument\nfrom efficient_attention.attn_utils import (\n     FlattenTranspose\n)\n\ndef default(val, d):\n    return val if val is not None else d\n\ndef prm_projection(\n    data: torch.Tensor,\n    projection_matrix: torch.Tensor,\n    normalize: bool=True,\n    diagonal: bool=False,\n    return_exp: bool=False,\n    is_query: bool=False,\n    eps: float=1e-8):\n    \"\"\"\n    Constructs nonnegative kernel features for fast softmax attention.\n    Args:\n    data: input for which features are computes\n    projection_matrix: random matrix used to compute features\n    batch_dims_t: tuple of batch dimensions\n    is_query: predicate indicating whether input data corresponds to queries or\n        keys\n    eps: numerical stabilizer.\n    Returns:\n    Random features for fast softmax attention.\n    \"\"\"\n    # data : [n, b, h, lk, d]\n    # proj : [n, b, h, lc, d]\n    # We have e^{qk^T/sqrt{d}} = e^{q_norm k_norm^T}, where\n    # w_norm = w * data_normalizer for w in {q,k}.\n    # NOTE: scaler with 0.5 could considerably stablizes training.\n    # now test norm also uses scaled data: that is, multiply by data.shape[-1] ** -1.\n    # normalized_data = (data.shape[-1] ** -0.5) * data\n    # data_dash = torch.einsum('...nd,...md->...nm', \n    #                         projection_matrix,\n    #                         normalized_data,\n    #                         ) # [n, b, h, c, lq]\n    # norm = torch.sum(normalized_data ** 2, dim=-1).unsqueeze(-2) / 2.0# [n, b, h, 1, lk]\n    data_normalizer = (data.shape[-1] ** -0.5)\n    if diagonal:\n        data_dash = torch.einsum('...nd,...nd->...n', \n                            projection_matrix,\n                            (data_normalizer * data),\n                            ) # [n, b, h, lq, lk]\n        norm = data_normalizer * torch.sum(data ** 2, dim=-1) / 2.0# [n, b, h, 1, lk]\n    else:\n        data_dash = torch.einsum('...nd,...md->...nm', \n                                projection_matrix,\n                                (data_normalizer * data),\n                                ) # [n, b, h, lq, lk]\n        norm = data_normalizer * torch.sum(data ** 2, dim=-1).unsqueeze(-2) / 2.0# [n, b, h, 1, lk]\n    if normalize:\n        proj_data = F.softmax(data_dash - norm, dim=-1)  # [n, b, h, l_c, l_k]\n    elif return_exp:\n        if is_query:\n            proj_data = torch.exp(\n                data_dash - norm - torch.amax(data_dash, dim=-2, keepdim=True).detach()) + eps       \n        else:\n            proj_data = torch.exp(\n                data_dash - norm - torch.amax(data_dash, dim=(-1, -2, -3), keepdim=True).detach()) + eps           \n    else:\n        proj_data = data_dash - norm\n    return proj_data\n\n\nclass LinearRA(MultiheadAttention):\n    def __init__(self, \n                num_landmarks=49,\n                kernel_size=None,\n                proposal_gen='pool',\n                use_antithetics=False,\n                use_multisample=False,\n                pool_module_type='light',\n                mis_type='mis-opt',\n                alpha_coeff=1.0,\n                *args,\n                **kwargs):\n        super(LinearRA, self).__init__(*args, **kwargs)\n        self.num_landmarks = num_landmarks\n        self.proposal_gen = proposal_gen\n        self.use_antithetics = use_antithetics\n        self.use_multisample = use_multisample\n\n        self.pool_module_type = pool_module_type\n        self.mis_type = mis_type\n        self.alpha_coeff = alpha_coeff\n\n        if self.pool_module_type == 'dense':\n            num_channels = self.dim\n        elif self.pool_module_type == 'light':\n            num_channels = self.head_dim\n\n        if self.proposal_gen.startswith('pool'):\n            output_size = int(math.sqrt(self.num_landmarks))\n            self.q_bar_gen = nn.Sequential(\n                nn.AdaptiveAvgPool2d(output_size),\n                FlattenTranspose(),\n                nn.Linear(num_channels, num_channels),\n                nn.LayerNorm(num_channels), \n                )\n            self.k_bar_gen = nn.Sequential(\n                nn.AdaptiveAvgPool2d(output_size),\n                FlattenTranspose(),\n                nn.Linear(num_channels, num_channels),\n                nn.LayerNorm(num_channels), \n                )\n        elif self.proposal_gen.startswith('no-param-pool'):\n            # No additional parameters. \n            # This could be useful if you want to use \n            # LARA for inference with a pretrained softmax-attention model.\n            \n            # However, this option, when trained from scratch, \n            # might not be as effective as the others.\n            output_size = int(math.sqrt(self.num_landmarks))\n            self.q_bar_gen = nn.Sequential(\n                nn.AdaptiveAvgPool2d(output_size),\n                FlattenTranspose(),\n                )\n            self.k_bar_gen = nn.Sequential(\n                nn.AdaptiveAvgPool2d(output_size),\n                FlattenTranspose(),\n                )\n        elif self.proposal_gen.startswith('adaptive-1d'):\n            self.q_bar_gen = nn.Sequential(\n                nn.Linear(num_channels, num_channels),\n                nn.LayerNorm(num_channels),\n            )\n            self.k_bar_gen = nn.Sequential(\n                nn.Linear(num_channels, num_channels),\n                nn.LayerNorm(num_channels),\n            )\n        else:\n            raise NotImplementedError\n        self.apply(self._init_weights)\n\n    def _proposal_gen_1d(self, x, key_padding_mask=None):\n        landmarks = self.num_landmarks\n        q, k, v = self.proj_and_split_heads(x)\n        if key_padding_mask is not None:\n            mask = key_padding_mask.unsqueeze(1).unsqueeze(-1).type_as(v)\n            q = q * (1. - mask)\n            k = k * (1. - mask)\n            v = v * (1. - mask)\n        b, h, n, d = q.shape # [b, num_heads, N, D]\n        # if self.proposal_gen == 'ortho':\n        #     k_bar = k.mean(-2, keepdim=True)\n        #     with torch.no_grad():\n        #         q_bar = orthogonal_landmarks(q, k, landmarks)\n        #     # diag = False\n        segs = n // landmarks\n        if self.proposal_gen.startswith('adaptive-1d'):\n            q2 = self.q_bar_gen(q)\n            k2 = self.k_bar_gen(k)\n        else:\n            q2 = q\n            k2 = k\n        if n <= landmarks:\n            q_bar = q2\n            k_bar = k2\n        elif (n % landmarks == 0):\n            q_bar = q2.reshape(b, h, landmarks, n // landmarks, d).mean(dim = -2)\n            k_bar = k2.reshape(b, h, landmarks, n // landmarks, d).mean(dim = -2)\n        else:\n            num_k = (segs + 1) * landmarks - n\n\n            keys_landmarks_f = k2[:, :, :num_k * segs, :].reshape(\n                b, h, num_k, segs, d).mean(dim = -2)\n            keys_landmarks_l = k2[:, :, num_k * segs:, :].reshape(\n                b, h, landmarks - num_k, segs + 1, d).mean(dim = -2)\n            k_bar = torch.cat((keys_landmarks_f, keys_landmarks_l), dim = -2)\n\n            queries_landmarks_f = q2[:, :, :num_k * segs, :].reshape(\n                b, h, num_k, segs, d).mean(dim = -2)\n            queries_landmarks_l = q2[:, :, num_k * segs:, :].reshape(\n                b, h, landmarks - num_k, segs + 1, d).mean(dim = -2)\n            q_bar = torch.cat((queries_landmarks_f, queries_landmarks_l), dim = -2)\n\n\n        return q_bar, k_bar, q, k, v\n\n    def _proposal_gen_2d(self, x, key_padding_mask=None):\n        b, H, W, c = x.shape\n        if self.pool_module_type == 'dense':\n            qkv = self.qkv(x).reshape((b, H * W, 3, self.num_heads, self.head_dim)).permute(2, 0, 3, 1, 4)\n            q, k, v = qkv[0], qkv[1], qkv[2] # [b, h, H * W, d]\n\n            q_bar = self.q_bar_gen(q.transpose(-1,-2).reshape(b, c, H, W))\n            k_bar = self.k_bar_gen(k.transpose(-1,-2).reshape(b, c, H, W))\n\n            q_bar = q_bar.reshape(b, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n            k_bar = k_bar.reshape(b, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n\n        elif self.pool_module_type == 'light':\n            qkv = self.qkv(x).reshape((b, H * W, 3, self.num_heads, self.head_dim)).permute(2, 0, 3, 1, 4)\n            q, k, v = qkv[0], qkv[1], qkv[2] # [b, h, H * W, d]\n            assert q.dim() == 4 and k.dim() == 4\n            temp_q = q.reshape(b * self.num_heads, H, W, self.head_dim).permute(0, 3, 1, 2)\n            temp_k = k.reshape(b * self.num_heads, H, W, self.head_dim).permute(0, 3, 1, 2)\n\n            q_bar = self.q_bar_gen(temp_q)\n            k_bar = self.k_bar_gen(temp_k)\n            q_bar = q_bar.reshape(b, self.num_heads, -1, self.head_dim)\n            k_bar = k_bar.reshape(b, self.num_heads, -1, self.head_dim)\n\n        else:\n            raise NotImplementedError\n\n\n        if self.proposal_gen.endswith('mixed'):\n            k_logits = torch.einsum('...pd, ...cd-> ...pc', self.scale * k_bar, k_bar)\n            # To mix information over key landmarks, \n            # we could also incorporate the value information into the weights;\n            # This approximates the optimal proposal distribution for our importance\n            # sampling estimate. However, it brings marginal benefits for most cases.\n            if self.proposal_gen.endswith('-vmixed'):\n                v_bar = F.adaptive_avg_pool2d(\n                    v.reshape(b * self.num_heads, H, W, self.head_dim).permute(0, 3, 1, 2), \n                    int(math.sqrt(self.num_landmarks))\n                    ).reshape(b, self.num_heads, self.head_dim, -1).transpose(-2, -1) # [b, h, c, d]\n                log_v_norm = torch.log(torch.linalg.vector_norm(v_bar, ord=2, dim=-1) + 1e-4).unsqueeze(-2) # b h c 1\n                k_logits = k_logits + log_v_norm\n            k_bar = torch.einsum(\n                '...pc, ...cd->...pd',\n                torch.softmax(k_logits, dim=-1),\n                k_bar\n            )\n        return q_bar, k_bar, q, k, v\n\n    def forward(self, x, key_padding_mask=None):\n        ######################## Generate Proposal Parameters ###############################\n        B, *seq_shape, C = x.shape\n        if len(seq_shape) == 2:\n            q_bar, k_bar, q, k, v = self._proposal_gen_2d(x, key_padding_mask)\n            mu = q_bar + k_bar\n        elif len(seq_shape) == 1:\n            q_bar, k_bar, q, k, v = self._proposal_gen_1d(x, key_padding_mask)\n            mu = q_bar + k_bar\n\n        ######################## Sampling from proposal ###############################\n        if self.training:\n            if self.use_multisample:\n                noise = torch.randn(B, self.num_heads, mu.shape[-2] * 2, self.head_dim, dtype=mu.dtype, device=mu.device)\n                weights = mu.repeat(1, 1, 2, 1) + noise\n            elif self.use_antithetics:\n                noise = torch.randn_like(mu)\n                weights = torch.cat([mu + noise, mu - noise], dim=-2)\n            else:\n                weights = mu + torch.randn_like(mu)\n        else:\n            weights = mu\n        \n        ######################## Computing SNIS estimates ###############################\n        log_proj_q = prm_projection(q, weights, normalize=False) # [b, h, c, lq]\n        log_proj_k = prm_projection(k, weights, normalize=False) # [b, h, c, lk]\n\n        if key_padding_mask is not None:\n            log_proj_k = log_proj_k.masked_fill(\n                        key_padding_mask.unsqueeze(1).unsqueeze(-2).to(torch.bool),\n                        float(\"-inf\"),\n                    )\n\n        # computes f(\\omega), where no proj_q is involved since it is cancelled out at both numerator and denominator.\n        kv_stats = torch.einsum('...cm,...md->...cd', torch.softmax(log_proj_k, dim=-1), v) # [b, h, c, d]\n\n        ## Different methods differ in computing the proposal density & \\alpha's\n        if self.mis_type == 'mis-biased':\n            log_proj_mu = prm_projection(mu, weights, normalize=False) # [b, h, c, c_mu]\n            log_alpha = torch.einsum('...cd,...nd->...cn', self.scale * mu, q) # [b,h,c,l_q]\n            if self.training:\n                if self.use_multisample or self.use_antithetics:\n                    log_alpha = log_alpha.repeat(1, 1, 2, 1)\n            log_proposal = torch.logsumexp(log_proj_mu, dim=-1, keepdim=True) #[b,h,c,lq]\n        elif self.mis_type == 'mis-opt':\n            log_tnc = torch.einsum('...cd,...nd->...cn', self.scale * q_bar, q)\n            t_nc = torch.softmax(log_tnc, dim=-1)# [b,h,c,l_q]\n            if self.training:\n                if self.use_multisample or self.use_antithetics:\n                    mu = mu.repeat(1, 1, 2, 1)\n                    t_nc = t_nc.repeat(1, 1, 2, 1)\n            log_proj_mu = prm_projection(mu, weights, normalize=False) # [b, h, c, c_mu]\n            log_proposal = torch.diagonal(log_proj_mu, dim1=-1, dim2=-2).unsqueeze(-1)\n            balanced_heuristics = torch.exp(log_proposal - torch.logsumexp(log_proj_mu, dim=-1, keepdim=True))\n            alpha_prev = balanced_heuristics + self.alpha_coeff * (t_nc - t_nc.mean(-2, keepdim=True))\n            log_alpha = torch.log(alpha_prev.clamp(min=1e-8)) # [b, h, c, l_q]\n        elif self.mis_type == 'mis-bh':\n            log_proj_mu = prm_projection(mu, weights, normalize=False) # [b, h, c, c_mu]\n            log_alpha = 0.\n            log_proposal = torch.logsumexp(log_proj_mu, dim=-1, keepdim=True) # [b, h, c, 1]\n        else:\n            raise NotImplementedError(\"The attn_type {} is not supported yet.\".format(self.mis_type))\n        \n        # computes self-normalized importance weights.\n        log_true_prob = log_proj_q + torch.logsumexp(log_proj_k, dim=-1, keepdim=True)\n        log_iw_ratio = log_alpha + log_true_prob - log_proposal #[b,h,c,lq]\n        sniw = torch.softmax(log_iw_ratio, dim=-2)# [b, h, c, lq]\n\n        # the sum over all samples to compute the final estimator.\n        output = torch.einsum('...cn, ...cd->...nd', sniw, kv_stats)\n\n        x = output.transpose(1, 2).reshape((B,) + tuple(seq_shape) + (C,))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n",
    "description": null,
    "url": null
}