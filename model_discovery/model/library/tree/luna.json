{
    "acronym": "luna",
    "title": "Luna: Linear unified nested attention",
    "seed_ids": [
        "rfa",
        "performer",
        "linformer",
        "longformer",
        "routingtransformer",
        "sinkhorn",
        "reformer",
        "blockbert",
        "sparsetransformer",
        "transformerxl",
        "universaltrans",
        "transformer"
    ],
    "s2id": "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
    "abstract": "The quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety",
    "authors": [
        "Xuezhe Ma",
        "Xiang Kong",
        "Sinong Wang",
        "Chunting Zhou",
        "Jonathan May",
        "Hao Ma",
        "Luke Zettlemoyer"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": "Luna is proposed, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear time and space complexity.",
    "citationCount": 94,
    "influentialCitationCount": 17,
    "code": "# Author: Xuezhe Ma (Max)\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom fairseq import utils\nfrom fairseq.modules import LayerNorm, LunarMultiheadAttention\nfrom fairseq.modules.quant_noise import quant_noise\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\n\nimport math\nfrom typing import Dict, Optional, Tuple, Union\n\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\n@with_incremental_state\nclass LunarCausalAttention(nn.Module):\n    \"\"\"Lunar Causal attention.\n    See \"Linformer: Self-Attention with Linear Complexity\" for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        dropout=0.0,\n        bias=True,\n        tie_kv=True,\n        q_noise=0.0,\n        qn_block_size=8,\n        parallel=True,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.parallel = parallel\n        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)\n\n        self.head_dim = embed_dim // num_heads\n        assert (self.head_dim * num_heads == self.embed_dim), \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n\n        self.pq_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n        self.q_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n        self.pc_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n        if tie_kv:\n            self.c_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n            self.k_proj = self.v_proj = None\n        else:\n            self.k_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n            self.v_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n            self.c_proj = None\n\n        self.out_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n\n    def _compute_pattention(self, pq, context, key_padding_mask):\n        # N x B x D\n        len, bsz, dim = context.size()\n        # N x B x D\n        k = self.pc_proj(context)\n        # N x B*H x K -> B*H x N x K\n        k = k.view(len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n\n        # B x H x L x K -> B*H x L x K -> B*H x K x L\n        pq = pq.view(bsz * self.num_heads, -1, self.head_dim).transpose(1, 2)\n        # B*H x N x L\n        pattn = k.bmm(pq)\n        pattn = F.softplus(pattn, beta=math.log(2.0))\n        return pattn\n\n    def forward(\n        self,\n        query,\n        pquery,\n        key_padding_mask: Optional[Tensor] = None,\n        pkey_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = False,\n        need_head_weights: bool = False,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            pkey_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, proj_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        if need_head_weights:\n            need_weights = True\n\n        tgt_len, bsz, embed_dim = query.size()\n        assert embed_dim == self.embed_dim\n\n        pq = None\n        num_steps = None\n        saved_state = None\n        key_accum_mat = None\n        value_accum_mat = None\n\n        if pq is None:\n            plen = pquery.size(0)\n            # L x B x D -> L x B x H x K\n            pq = self.pq_proj(pquery).view(plen, bsz, self.num_heads, self.head_dim)\n            # L x B x H x K -> B x H x L x K\n            pq = pq.permute(1, 2, 0, 3) * self.scaling\n\n        plen = pq.size(2)\n        # B*H x N x L\n        pattn_weights = self._compute_pattention(pq, query, key_padding_mask)\n        pattn_weights = self.dropout_module(pattn_weights)\n\n        # N x B x D -> B*H x N x K\n        q = self.q_proj(query) * self.scaling\n        q = q.view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n        # N x B x D -> B*H x N x K\n        if self.c_proj is not None:\n            k = v = self.c_proj(query).view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n        else:\n            k = self.k_proj(query).view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n            v = self.v_proj(query).view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n\n        efficient_causal_attention = efficient_causal_attention_parallel if self.parallel else efficient_causal_attention_seq\n\n        if saved_state is not None:\n            # key accumulative matrix are store with shape (bsz, num_heads, head_dim, plen)\n            if \"prev_key_accum_mat\" in saved_state:\n                _prev_key_accum_mat = saved_state[\"prev_key_accum_mat\"]\n                key_accum_mat = _prev_key_accum_mat.view(bsz * self.num_heads, self.head_dim, plen)\n            # value accumulative matrix are store with shape (bsz, num_heads, plen, head_dim)\n            if \"prev_value_accum_mat\" in saved_state:\n                _prev_value_accum_mat = saved_state[\"prev_value_accum_mat\"]\n                value_accum_mat = _prev_value_accum_mat.view(bsz * self.num_heads, plen, self.head_dim)\n            if \"prev_num_steps\" in saved_state:\n                _prev_num_steps = saved_state[\"prev_num_steps\"]\n                num_steps = _prev_num_steps.view(bsz * self.num_heads) + 1.0\n\n        if num_steps is None:\n            num_steps = query.new_ones(bsz * self.num_heads)\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if pkey_padding_mask is not None and pkey_padding_mask.dim() == 0:\n            pkey_padding_mask = None\n\n        attn_weights = efficient_causal_attention(q, k, pattn_weights)\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, plen]\n\n        if pkey_padding_mask is not None:\n            # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, plen)\n            if not self.tpu:\n                attn_weights = attn_weights.masked_fill(pkey_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\"))\n            else:\n                attn_weights = attn_weights.transpose(0, 2)\n                attn_weights = attn_weights.masked_fill(pkey_padding_mask, float('-inf'))\n                attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, plen)\n\n        attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = self.dropout_module(attn_weights)\n\n        attn = efficient_causal_attention(attn_probs, pattn_weights, v)\n\n        if saved_state is not None:\n            saved_state[\"prev_pquery\"] = pq\n            saved_state[\"prev_key_accum_mat\"] = key_accum_mat.view(bsz, self.num_heads, self.head_dim, plen)\n            saved_state[\"prev_value_accum_mat\"] = value_accum_mat.view(bsz, self.num_heads, plen, self.head_dim)\n            saved_state[\"prev_num_steps\"] = num_steps.view(bsz, self.num_heads)\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n\n        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n        if self.onnx_trace and attn.size(1) == 1:\n            # when ONNX tracing a single decoder step (sequence length == 1)\n            # the transpose is a no-op copy before view, thus unnecessary\n            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n        else:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n        attn = self.out_proj(attn)\n        attn_weights: Optional[Tensor] = None\n        if need_weights:\n            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, plen).transpose(1, 0)\n            if not need_head_weights:\n                # average attention weights over heads\n                attn_weights = attn_weights.mean(dim=0)\n\n        return attn, attn_weights\n\n\ndef efficient_causal_attention_seq(x, y, z):\n    \"\"\"\n    efficient causal attention operation\n    Args:\n        x (Tensor): Tensor with shape `(batch, n, d1)`\n        y (Tensor): Tensor with shape `(batch, n, d1)`\n        z (Tensor): Tensor with shape '(batch, n, d2)`\n\n    return:\n    \"\"\"\n    n = x.size(1)\n    rets = []\n    accum_mat = 0\n    for i in range(n):\n        xx = x[:, i:i + 1] # B x 1 x d1\n        yy = y[:, i:i + 1] # B x 1 x d1\n        zz = z[:, i:i + 1] # B x 1 x d2\n\n        # B x d1 x d2\n        accum_mat = accum_mat + torch.bmm(yy.transpose(1, 2), zz)\n        # B x 1 x d2\n        rets.append(torch.bmm(xx, accum_mat).div(i + 1.))\n    # B x N x d2\n    return torch.cat(rets, dim=1)\n\n\ndef efficient_causal_attention_parallel(x, y, z):\n    \"\"\"\n    efficient causal attention operation\n    Args:\n        x (Tensor): Tensor with shape `(batch, n, d1)`\n        y (Tensor): Tensor with shape `(batch, n, d1)`\n        z (Tensor): Tensor with shape '(batch, n, d2)`\n    return:\n    \"\"\"\n    bsz, n, d1 = x.size()\n    # (bsz, n, d1, 1) x (bsz, n, 1, d2) -> (bsz, n, d1, d2)\n    sum_mat = torch.matmul(y.unsqueeze(3), z.unsqueeze(2))\n    accum_mat = torch.cumsum(sum_mat, dim=1)\n    # (bsz, n, 1, d1) x (bsz, n, d1, d2) -> (bsz, n, 1, d2) -> (bsz, n, d2)\n    res = torch.matmul(x.unsqueeze(2), accum_mat).squeeze(2)\n    # (1, n, 1)\n    length_div = torch.arange(1, n + 1, device=x.device).unsqueeze(0).unsqueeze(2)\n    res = res / length_div\n    return res\n\n\nclass LunaDecoderLayer(nn.Module):\n    \"\"\"Decoder layer block.\n\n    In the original paper each operation (multi-head attention, encoder\n    attention or FFN) is postprocessed with: `dropout -> add residual ->\n    layernorm`. In the tensor2tensor code they suggest that learning is more\n    robust when preprocessing each layer with layernorm and postprocessing with:\n    `dropout -> add residual`. We default to the approach in the paper, but the\n    tensor2tensor approach can be enabled by setting\n    *args.decoder_normalize_before* to ``True``.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    \"\"\"\n\n    def __init__(self, args, index):\n        super().__init__()\n        self.quant_noise = getattr(args, \"quant_noise_pq\", 0)\n        self.quant_noise_block_size = getattr(args, \"quant_noise_pq_block_size\", 8)\n\n        self.index = index\n        self.normalize_before = args.decoder_normalize_before\n        self.embed_dim = args.decoder_embed_dim\n\n        self.dropout_module = FairseqDropout(args.dropout, module_name=self.__class__.__name__)\n\n        self.self_attn = self.build_self_attention(self.embed_dim, args)\n\n        # use layerNorm rather than FusedLayerNorm for exporting.\n        # char_inputs can be used to determint this.\n        # TODO  remove this once we update apex with the fix\n        export = getattr(args, \"char_inputs\", False)\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.encoder_attn = self.build_encoder_attention(self.embed_dim, args)\n        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n        self.encoder_atten_proj_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.activation_fn = utils.get_activation_fn(activation=getattr(args, \"activation_fn\", \"relu\"))\n        activation_dropout_p = getattr(args, \"activation_dropout\", 0)\n        if activation_dropout_p == 0:\n            # for backwards compatibility with models that use args.relu_dropout\n            activation_dropout_p = getattr(args, \"relu_dropout\", 0)\n        self.activation_dropout_module = FairseqDropout(float(activation_dropout_p), module_name=self.__class__.__name__)\n\n        self.fc1 = self.build_fc1(self.embed_dim, args.decoder_ffn_embed_dim, self.quant_noise, self.quant_noise_block_size)\n        self.fc2 = self.build_fc2(args.decoder_ffn_embed_dim, self.embed_dim, self.quant_noise, self.quant_noise_block_size)\n        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.need_attn = True\n        self.onnx_trace = False\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_self_attention(self, embed_dim, args):\n        return LunarCausalAttention(\n            embed_dim,\n            args.decoder_attention_heads,\n            dropout=args.attention_dropout,\n            tie_kv=not args.untie_luna_kv,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n        )\n\n    def build_encoder_attention(self, embed_dim, args):\n        return LunarMultiheadAttention(\n            embed_dim,\n            args.decoder_attention_heads,\n            args.decoder_projected_attention_heads,\n            dropout=args.attention_dropout,\n            encoder_decoder_attention=True,\n            tie_kv=not args.untie_luna_kv,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n        )\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        x,\n        px,\n        encoder_out,\n        encoder_padding_mask: Optional[torch.Tensor] = None,\n        encoder_projected_padding_mask: Optional[torch.Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n        need_attn: bool = False,\n        need_head_weights: bool = False,\n    ):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            px (Tensor): projected input to the layer of shape `(proj_len, batch, embed_dim)`\n            encoder_out (Tensor): output from encoder of shape `(encoder_seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor, optional): binary\n                ByteTensor of shape `(batch, src_len)` where padding\n                elements are indicated by ``1``.\n            encoder_projected_padding_mask (ByteTensor, optional): binary\n                ByteTensor of shape `(batch, proj_len)` where padding\n                elements are indicated by ``1``.\n            need_attn (bool, optional): return attention weights\n            need_head_weights (bool, optional): return attention weights\n                for each head (default: return average over heads).\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n            projected output of shape `(proj_len, batch, embed_dim)`\n        \"\"\"\n        if need_head_weights:\n            need_attn = True\n\n        static_px = px is None\n\n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        x, attn = self.self_attn(query=x, pquery=px,\n                                 key_padding_mask=self_attn_padding_mask,\n                                 pkey_padding_mask=encoder_projected_padding_mask,\n                                 incremental_state=incremental_state,\n                                 need_weights=False)\n\n        x = self.dropout_module(x)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        residual = x\n        presidual = px\n        if self.normalize_before:\n            x = self.encoder_attn_layer_norm(x)\n            px = self.encoder_atten_proj_layer_norm(px) if not static_px else None\n\n        x, px, attn = self.encoder_attn(query=x, pquery=px, context=encoder_out,\n                                        context_padding_mask=encoder_padding_mask,\n                                        pcontext_padding_mask=encoder_projected_padding_mask,\n                                        incremental_state=incremental_state,\n                                        static_context=True,\n                                        need_weights=need_attn or (not self.training and self.need_attn),\n                                        need_head_weights=need_head_weights)\n        # apply dropout\n        x = self.dropout_module(x)\n        px = self.dropout_module(px) if not static_px else None\n\n        x = residual + x\n        px = presidual + px if not static_px else None\n        if not self.normalize_before:\n            x = self.encoder_attn_layer_norm(x)\n            px = self.encoder_atten_proj_layer_norm(px) if not static_px else None\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n\n        x = self.activation_fn(self.fc1(x))\n        x = self.activation_dropout_module(x)\n        x = self.fc2(x)\n        x = self.dropout_module(x)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n\n        return x, px, attn, None\n\n    def make_generation_fast_(self, need_attn: bool = False, **kwargs):\n        self.need_attn = need_attn",
    "description": null,
    "url": null
}