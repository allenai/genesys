{
    "acronym": "clex",
    "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
    "seed_ids": [
        "yarn",
        "pi",
        "lex",
        "rmt",
        "flashattn",
        "kerple",
        "alibi",
        "roformer",
        "compressivetransformer",
        "transformerxl"
    ],
    "s2id": "a54761081c2b001c057fb6e1ea9a48058d5aa5e0",
    "abstract": "Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4x or almost 8x training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.",
    "authors": [
        "Guanzheng Chen",
        "Xin Li",
        "Zaiqiao Meng",
        "Shangsong Liang",
        "Li Bing"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "Continuous Length EXtrapolation (CLEX) is proposed, which generalises the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths.",
    "citationCount": 13,
    "influentialCitationCount": 1,
    "code": "import torch\nfrom torch import nn\nfrom torchdiffeq import  odeint\n\nimport wandb\n\nimport math\n\n\n\n\nclass ODELinear(nn.Module):\n    def __init__(\n        self, \n        dim: int, \n        factor,\n        act,\n        base=10000,\n        **kwargs\n    ):\n        super().__init__()\n        self.ode_up_proj = nn.Parameter(torch.empty(dim//2, factor*dim))\n        self.ode_down_proj = nn.Parameter(torch.empty(factor*dim, dim//2))\n        self.dim = dim\n        self.base = base\n        if act == \"tanh\":\n            self.act = torch.nn.Tanh()\n        elif act == \"silu\":\n            self.act = torch.nn.SiLU()\n        else:\n            raise ValueError(f\"act must be one of ['tanh', 'silu'], got {act}\")\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.ode_up_proj, a=math.sqrt(5))\n        nn.init.zeros_(self.ode_down_proj)\n\n    def get_time_embedding(self, t, base=10000, device='cuda', dtype=torch.float32):\n        if t < 1:\n            alpha = 1\n        else:\n            alpha = 2*t-1\n        ntk_base = base * alpha ** (self.dim / (self.dim-2))\n        ntk_inv_freq = 1.0 / (ntk_base ** (torch.arange(0, self.dim, 2, dtype=torch.float32).to(device) / self.dim))\n        index = torch.arange(0, self.dim, 2, dtype=torch.float32).to(device)\n        delta_ntk_freq = -2*index/(self.dim-2) * 1 / (base ** (index/self.dim) * (alpha ** (index/(self.dim-2) + 1)))\n        return delta_ntk_freq.to(device, dtype=dtype), ntk_inv_freq.to(device, dtype=dtype)\n\n    def forward(self, t, x: torch.Tensor):\n\n        device = x.device\n        delta_time, time = self.get_time_embedding(t.to(device), device=device, dtype=x.dtype)\n        x = x + torch.log(time)\n        time_embed = delta_time / time\n        delta_inv_freq = self.act(x @ self.ode_up_proj.float()) @ self.ode_down_proj.float()\n        delta_inv_freq = delta_inv_freq + time_embed\n        return delta_inv_freq\n\n\n\n\n\nclass CLEXScalingRotaryEmbedding(nn.Module):\n\n    def __init__(self, dim, max_position_embeddings=2048, rope_scaling=None, base=10000, device=None) -> None:\n        super().__init__()\n\n        self.max_t = rope_scaling[\"max_factor\"]\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        self.proj_func = ODELinear(dim, rope_scaling[\"param_factor\"], rope_scaling[\"act\"], base)\n        self.rope_cached = None\n        self.max_t_cached = 0\n        self.freq_cached = None\n        self.time_dt = rope_scaling[\"time_dt\"]\n        self.ode_args = {\n            \"method\": \"rk4\",\n            \"options\": {\"step_size\": self.time_dt},\n        }\n\n    def sample_random_times(self, max_t, device):\n        return torch.randint(1, max_t, (1,), dtype = torch.long, device=device)\n\n    def get_random_position_ids(self, n=2048, max=8192):\n        positions = torch.randperm(max)[:n].sort().values\n        return positions\n    \n\n    def get_continuous_freq(self, time_grid, ex_positions, device):\n        solution = odeint(\n            self.proj_func, torch.log(self.inv_freq.to(device, dtype=torch.float32)), time_grid, **self.ode_args\n        )\n        if time_grid.size(0) == 2:\n            scale_inv_freq = torch.exp(solution[1])\n            freqs = torch.outer(ex_positions.float().squeeze(), scale_inv_freq)\n        else:\n            scale_inv_freq = torch.exp(solution)\n            return scale_inv_freq\n        embed = torch.cat((freqs,freqs), dim=-1)\n        return embed\n\n\n\n    def forward(self, input_embeds, seq_len, do_train=False):\n        device = self.proj_func.ode_up_proj.device\n        dtype = input_embeds.dtype\n        scale_factor = seq_len // self.max_position_embeddings\n        if do_train:\n            t_val = self.sample_random_times(self.max_t+1, device)[0]\n            if scale_factor < 1.0:\n                scale_factor = 1\n            sampled_position_ids = self.get_random_position_ids(n=seq_len-2, max=seq_len*t_val-2).float()\n            ex_positions = torch.cat([\n                torch.tensor([0]), \n                (sampled_position_ids + 1) / scale_factor,\n                torch.tensor([seq_len*t_val//scale_factor-1])]\n            ).to(device, dtype=torch.float32)\n        else:\n            t_val = scale_factor if seq_len%self.max_position_embeddings == 0.0 else scale_factor + 1\n            t_val = t_val if t_val <= self.max_t else self.max_t\n            ex_positions = torch.arange(0, self.max_position_embeddings * t_val, dtype=torch.float32).to(device)\n\n\n        \n        if t_val == 1.0:\n            scale_inv_freq = self.inv_freq.to(device)\n            freqs = torch.outer(ex_positions.float().squeeze(), scale_inv_freq)\n            embed = torch.cat((freqs,freqs), dim=-1)\n            cos, sin = embed.cos(), embed.sin()\n        elif do_train:\n            time_grid = torch.tensor([1.0, t_val]).float().to(device)\n            embed = self.get_continuous_freq(time_grid, ex_positions, device)\n            cos, sin = embed.cos(), embed.sin()\n        else:\n            if self.freq_cached is None:\n                time_grid = torch.arange(1.0, self.max_t+1.0, dtype=torch.float32).to(device)\n                self.freq_cached = self.get_continuous_freq(time_grid, ex_positions, device)\n            if t_val != self.max_t_cached:\n                scale_inv_freq = self.freq_cached[int(t_val-1.0)]\n                freqs = torch.outer(ex_positions.float().squeeze(), scale_inv_freq)\n                embed = torch.cat((freqs,freqs), dim=-1)\n                self.rope_cached = torch.cat((embed.cos()[None, :, :], embed.sin()[None, :, :]), dim=0)\n                self.max_t_cached = t_val\n            cos, sin = self.rope_cached\n        return torch.cat(\n            (cos[None, :seq_len].to(dtype=dtype),\n            sin[None, :seq_len].to(dtype=dtype)),\n            dim=0\n        )\n    ",
    "description": null,
    "url": null
}