{
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
    "acronym": "mamba2",
    "s2id": "ca9f5b3bf0f54ad97513e6175b30497873670fed",
    "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "citationCount": 25,
    "influentialCitationCount": 5,
    "seed_ids": [],
    "code": "# gab.py\n\nimport torch\nimport torch.nn as nn\n\nfrom model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n\n\n# YOU CAN IMPORT MORE MODULES HERE #\n\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n\n\n# Copyright (c) 2024, Tri Dao, Albert Gu.\n\nimport math\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\n\n# try:\n#     from causal_conv1d import causal_conv1d_fn\n# except ImportError:\n#     causal_conv1d_fn = None\n\n# try:\n#     from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated, LayerNorm\n# except ImportError:\n#     RMSNormGated, LayerNorm = None, None\n\n# from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn\n\n\n\ndef segsum_unstable(x):\n    \"\"\"Naive segment sum calculation.\"\"\"\n    T = x.size(-1)\n    x_cumsum = torch.cumsum(x, dim=-1)\n    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n    return x_segsum\n\ndef segsum(x):\n    \"\"\"More stable segment sum calculation.\"\"\"\n    T = x.size(-1)\n    x = repeat(x, \"... d -> ... d e\", e=T)\n    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=-1)\n    x = x.masked_fill(~mask, 0)\n    x_segsum = torch.cumsum(x, dim=-2)\n    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n    return x_segsum\n\n\ndef pad_to_block_length(X, block_len):\n    pad_len = (block_len - X.shape[1] % block_len) % block_len\n    if pad_len > 0:\n        padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=X.dtype, device=X.device)\n        X = torch.cat([X, padding], dim=1)\n    return X\n\ndef ssd_minimal_discrete(X, A, B, C, block_len, initial_states=None):\n    \"\"\"\n    Arguments:\n        X: (batch, length, n_heads, d_head)\n        A: (batch, length, n_heads)\n        B: (batch, length, n_heads, d_state)\n        C: (batch, length, n_heads, d_state)\n    Return:\n        Y: (batch, length, n_heads, d_head)\n    \"\"\"\n    assert X.dtype == A.dtype == B.dtype == C.dtype\n    # assert X.shape[1] % block_len == 0\n\n    # Rearrange into blocks/chunks\n    X, A, B, C = [rearrange(x, \"b (c l) ... -> b c l ...\", l=block_len) for x in (X, A, B, C)]\n\n    A = rearrange(A, \"b c l h -> b h c l\")\n    A_cumsum = torch.cumsum(A, dim=-1)\n\n    # 1. Compute the output for each intra-chunk (diagonal blocks)\n    L = torch.exp(segsum(A))\n    Y_diag  = torch.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", C, B, L, X)\n\n    # 2. Compute the state for each intra-chunk\n    # (right term of low-rank factorization of off-diagonal blocks; B terms)\n    decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n    states = torch.einsum(\"bclhn,bhcl,bclhp->bchpn\", B, decay_states, X)\n\n    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n    # (middle term of factorization of off-diag blocks; A terms)\n    if initial_states is None:\n        initial_states = torch.zeros_like(states[:, :1])\n    states = torch.cat([initial_states, states], dim=1)\n    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n    new_states = torch.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n    states, final_state = new_states[:, :-1], new_states[:, -1]\n\n    # 4. Compute state -> output conversion per chunk\n    # (left term of low-rank factorization of off-diagonal blocks; C terms)\n    state_decay_out = torch.exp(A_cumsum)\n    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)\n\n    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n    Y = rearrange(Y_diag+Y_off, \"b c l h p -> b (c l) h p\")\n    return Y, final_state\n\n\n\nclass Mamba2Simple(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        d_state=64,\n        d_conv=4,\n        expand=2,\n        headdim=128,\n        ngroups=1,\n        A_init_range=(1, 16),\n        dt_min=0.001,\n        dt_max=0.1,\n        dt_init_floor=1e-4,\n        # Fused kernel and sharding options\n        chunk_size=256,\n        device=None,\n        dtype=None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n\n        # Order: [z, x, B, C, dt]\n        d_in_proj = 2 * self.d_inner + 2 * self.ngroups * self.d_state + self.nheads\n        self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=True, **factory_kwargs)\n\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.Conv1d(\n            in_channels=conv_dim,\n            out_channels=conv_dim,\n            bias=True,\n            kernel_size=d_conv,\n            groups=conv_dim,\n            padding=d_conv - 1,\n            **factory_kwargs,\n        )\n        # self.conv1d.weight._no_weight_decay = True\n\n        self.act = nn.SiLU()\n\n        # Initialize log dt bias\n        dt = torch.exp(\n            torch.rand(self.nheads, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n            + math.log(dt_min)\n        )\n        dt = torch.clamp(dt, min=dt_init_floor)\n        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        self.dt_bias = nn.Parameter(inv_dt)\n        # Just to be explicit. Without this we already don't put wd on dt_bias because of the check\n        # name.endswith(\"bias\") in param_grouping.py\n        self.dt_bias._no_weight_decay = True\n\n        # A parameter\n        assert A_init_range[0] > 0 and A_init_range[1] >= A_init_range[0]\n        A = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(*A_init_range)\n        A_log = torch.log(A).to(dtype=dtype)\n        self.A_log = nn.Parameter(A_log)\n        # self.register_buffer(\"A_log\", torch.zeros(self.nheads, dtype=torch.float32, device=device), persistent=True)\n        self.A_log._no_weight_decay = True\n\n        # # D \"skip\" parameter\n        # self.D = nn.Parameter(torch.ones(self.nheads, device=device))\n        # self.D._no_weight_decay = True\n\n        # Extra normalization layer right before output projection\n        # assert RMSNormGated is not None\n        # self.norm = RMSNormGated(self.d_inner, eps=1e-5, norm_before_gate=False, **factory_kwargs)\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-5, **factory_kwargs)\n        self.silu = nn.SiLU()\n\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **factory_kwargs)\n\n    def forward(self, u):\n        \"\"\"\n        u: (B, L, D)\n        Returns: same shape as u\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u=pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n\n        zxbcdt = self.in_proj(u)  # (B, L, d_in_proj)\n        A = -torch.exp(self.A_log)  # (nheads) or (d_inner, d_state)\n\n        z, xBC, dt = torch.split(\n            zxbcdt, [self.d_inner, self.d_inner + 2 * self.ngroups * self.d_state, self.nheads], dim=-1\n        )\n        dt = F.softplus(dt + self.dt_bias)  # (B, L, nheads)\n\n        # 1D Convolution\n        # if causal_conv1d_fn is None:\n        xBC = self.act(\n            self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)\n        )  # (B, L, self.d_inner + 2 * ngroups * d_state)\n        xBC = xBC[:, :seqlen, :]\n        # else: # BUGGY, see https://github.com/state-spaces/mamba/pull/362 \n        #     xBC = causal_conv1d_fn(\n        #         x=xBC.transpose(1, 2),\n        #         weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n        #         bias=self.conv1d.bias,\n        #         activation='swish',\n        #     ).transpose(1, 2)\n\n        # Split into 3 main branches: X, B, C\n        # These correspond to V, K, Q respectively in the SSM/attention duality\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, \"b l (h p) -> b l h p\", p=self.headdim)\n        B = rearrange(B, \"b l (g n) -> b l g n\", g=self.ngroups)\n        C = rearrange(C, \"b l (g n) -> b l g n\", g=self.ngroups)\n        \n        y, _ = ssd_minimal_discrete(x*dt.unsqueeze(-1), A*dt, B, C, self.chunk_size)\n        y = rearrange(y, \"b l h p -> b l (h p)\")\n\n        # Multiply \"gate\" branch and apply extra normalization layer\n        y=self.norm(y * self.silu(z))\n        out = self.out_proj(y)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nclass GAB(GABBase):\n    \"\"\"Generalized Autoregressive Block\n        Input:        X: (batch, seqlen, embed_dim)\n        Output:       Y: (batch, seqlen, embed_dim)\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n    \"\"\"\n    def __init__(\n            self,\n            embed_dim: int, \n            device=None,\n            dtype=None,\n            d_state=64,\n            d_conv=4,\n            expand=2,\n            headdim=128,\n            ngroups=1,\n            A_init_range=(1, 16),\n            dt_min=0.001,\n            dt_max=0.1,\n            dt_init_floor=1e-4,\n            chunk_size=256,\n            **kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n        # argv: list of hyperparameters\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\n        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n        \n        # COMPLETING THE CODE HERE #\n        self.mamba1=Mamba2Simple(embed_dim, d_state, d_conv, expand, headdim, ngroups, A_init_range, dt_min, dt_max, dt_init_floor, chunk_size, **factory_kwargs)\n        self.mamba2=Mamba2Simple(embed_dim, d_state, d_conv, expand, headdim, ngroups, A_init_range, dt_min, dt_max, dt_init_floor, chunk_size, **factory_kwargs)\n        # self.norm1 = RMSNorm(embed_dim, eps=1e-5, **factory_kwargs)\n        # self.norm2 = RMSNorm(embed_dim, eps=1e-5, **factory_kwargs)\n        self.norm1 = nn.LayerNorm(embed_dim, **factory_kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, **factory_kwargs)\n\n\n    # YOU CAN ADD MORE FUNCTIONS HERE #\n\n\n    def _forward(self,X,**kwargs): # type hints are optional but recommended\n        # THE CODE HERE MUST BE COMPLETED #\n        hidden_states = self.norm1(X.to(dtype=self.norm1.weight.dtype))\n        X = self.mamba1(hidden_states) + X\n        hidden_states = self.norm2(X.to(dtype=self.norm2.weight.dtype))\n        X = self.mamba2(hidden_states) + X\n        return X\n        \n    \n    \n\"\"\" The dictionary of hyperparameters for constructing a GAB layer\n    embed_dim, device, dtype should NOT be included in gab_config\n\"\"\"\ngab_config = {\n    # THE HYPERPARAMETERS OF ADDITIONAL ARGUMENTS IN GAB CLASS #\n    'd_state': 64,\n    'd_conv': 4,\n    'expand': 2,\n    'headdim': 128,\n    'ngroups': 1,\n    'A_init_range': (1, 16),\n    'dt_min': 0.001,\n    'dt_max': 0.1,\n    'dt_init_floor': 1e-4,\n    'chunk_size': 256,\n}\n"
}