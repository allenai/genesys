{
    "acronym": "lineartransformer",
    "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
    "seed_ids": [
        "reformer",
        "sparsetransformer",
        "transformer"
    ],
    "s2id": "6f68e1bb253925d8431588555d3010419f322e04",
    "abstract": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
    "authors": [
        "Angelos Katharopoulos",
        "Apoorv Vyas",
        "Nikolaos Pappas",
        "Franccois Fleuret"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "tldr": "This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.",
    "citationCount": 1155,
    "influentialCitationCount": 164,
    "code": "\"\"\"Modules and functions for building attention models.\n\nReferences (used throughout the code):\n    [1]: https://arxiv.org/abs/1712.09763\n    [2]: https://arxiv.org/abs/2006.16236\n    [3]: https://arxiv.org/abs/1706.03762\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch import autograd, nn\nfrom torch.nn import functional as F\n\n\ndef _idx(i):\n    return (slice(None), slice(None), slice(i, i + 1, 1), slice(None))\n\n\nclass _UnnormalizedLinearCausalAttention(autograd.Function):\n    \"\"\"Computes unnormalized causal attention using only O(N*C) memory.\"\"\"\n\n    @staticmethod\n    def forward(ctx, Q, K, V):\n        ctx.save_for_backward(Q, K, V)\n\n        Vnew, S = torch.zeros_like(V), 0\n        for i in range(V.shape[2]):\n            S = S + K[_idx(i)].transpose(2, 3) @ V[_idx(i)]\n            Vnew[_idx(i)] = Q[_idx(i)] @ S\n        return Vnew\n\n    @staticmethod\n    def backward(ctx, G):\n        Q, K, V = ctx.saved_tensors\n\n        dQ, S = torch.zeros_like(Q), 0\n        for i in range(V.shape[2]):\n            S = S + K[_idx(i)].transpose(2, 3) @ V[_idx(i)]\n            dQ[_idx(i)] = G[_idx(i)] @ S.transpose(2, 3)\n\n        dK, dV, S = torch.zeros_like(K), torch.zeros_like(V), 0\n        for i in range(V.shape[2] - 1, -1, -1):\n            S = S + Q[_idx(i)].transpose(2, 3) @ G[_idx(i)]\n            dV[_idx(i)] = K[_idx(i)] @ S\n            dK[_idx(i)] = V[_idx(i)] @ S.transpose(2, 3)\n        return dQ, dK, dV\n\n\n# TODO(eugenhotaj): LinearCausalAttention currently does O(N) computations each\n# time forward is called. During sampling, forward is called N times to generate\n# N pixels. This means that during sampling  LinearCausalAttention unnecessarily\n# does O(N^2) computations, most of which are thrown away. Instead, we can do\n# O(N) work during sampling by storing previous activations as proposed in [2].\n# TODO(eugenhotaj): This API does not match the CausalAttention API. We need\n# to add support for mask_center and extra_input. There is also a lot of shared\n# code between the two which should be extracted. It's probably possible to\n# have base class which does the bookkeeping and the subclasses implement\n# the actual computations.\nclass LinearCausalAttention(nn.Module):\n    \"\"\"Memory efficient implementation of CausalAttention as introduced in [2].\n\n    NOTE: LinearCausalAttention is *much* slower than CausalAttention and should\n    only be used if your model cannot fit in memory.\n\n    This implementation only requires O(N) memory (instead of O(N^2)) for a\n    sequence of N elements (e.g. an image with N pixels). To achieve this memory\n    reduction, the implementation avoids storing the full attention matrix in\n    memory and instead computes the output directly as Q @ (K @ V). However, this\n    output cannot be vectorized and requires iterating over the sequence, which\n    drastically slows down the computation.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        feature_fn=lambda x: F.elu(x) + 1,\n        n_heads=1,\n        embed_channels=None,\n        out_channels=None,\n    ):\n        \"\"\"Initializes a new LinearCausalAttention instance.\n\n        Args:\n            in_channels: Number of input channels.\n            feature_fn: A kernel feature map applied to the Query and Key activations.\n                Defaults to lambda x: elu(x) + 1.\n            n_heads: Number of causal self-attention heads.\n            embed_channels: Number of embedding channels. Defaults to in_channels.\n            out_channels: Number of output channels. Defaults to in_channels.\n        \"\"\"\n        super().__init__()\n        self._feature_fn = feature_fn\n        self._n_heads = n_heads\n        self._embed_channels = embed_channels or in_channels\n        self._out_channels = out_channels or in_channels\n\n        self._query = nn.Conv2d(\n            in_channels=in_channels, out_channels=self._embed_channels, kernel_size=1\n        )\n        self._kv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=self._embed_channels + self._out_channels,\n            kernel_size=1,\n        )\n        self._numerator = _UnnormalizedLinearCausalAttention.apply\n\n    def forward(self, x):\n        def _to_multihead(t):\n            \"\"\"Reshapes an (N, C, H, W) tensor into (N, n_heads, H * W, head_size).\"\"\"\n            c = t.shape[1]\n            t = t.view(n, self._n_heads, c // self._n_heads, -1)\n            return t.transpose(2, 3)\n\n        n, _, h, w = x.shape\n\n        # Compute the Query, Key, and Value.\n        Q = _to_multihead(self._query(x))\n        K, V = self._kv(x).split([self._embed_channels, self._out_channels], dim=1)\n        K, V = _to_multihead(K), _to_multihead(V)\n\n        # Compute the causal attention weights.\n        Q, K = self._feature_fn(Q), self._feature_fn(K)\n        den = 1 / (torch.einsum(\"nlhi,nlhi->nlh\", Q, K.cumsum(1)) + 1e-10)\n        num = self._numerator(Q, K, V)\n        out = num * torch.unsqueeze(den, -1)\n        return out.transpose(2, 3).contiguous().view(n, -1, h, w)",
    "description": null,
    "url": null
}