{
    "acronym": "graphlm",
    "title": "Graph Language Models",
    "seed_ids": [],
    "s2id": "e0d303b5c1334e96dd04b9c94658fe0dc8f568ac",
    "abstract": "While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.",
    "authors": [
        "Moritz Plenz",
        "Anette Frank"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work introduces a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses, and designs the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}