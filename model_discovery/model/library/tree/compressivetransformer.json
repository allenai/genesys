{
    "acronym": "compressivetransformer",
    "title": "Compressive Transformers for Long-Range Sequence Modelling",
    "seed_ids": [
        "sparsetransformer",
        "transformerxl",
        "transformer",
        "bert"
    ],
    "s2id": "f51497f463566581874c941353dd9d80069c5b77",
    "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",
    "authors": [
        "Jack W. Rae",
        "Anna Potapenko",
        "Siddhant M. Jayakumar",
        "T. Lillicrap"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "tldr": "The Compressive Transformer is presented, an attentive sequence model which compresses past memories for long-range sequence learning and can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task.",
    "citationCount": 492,
    "influentialCitationCount": 64,
    "code": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom mogrifier import Mogrifier\n\nimport math\nfrom collections import namedtuple\nfrom functools import partial\nfrom inspect import isfunction\n\n# structs\n\nMemory = namedtuple('Memory', ['mem', 'compressed_mem'])\n\n# helper functions\n\ndef to(t):\n    return {'dtype': t.dtype, 'device': t.device}\n\ndef cast_tuple(el):\n    return el if isinstance(el, tuple) else (el,)\n\ndef default(x, val):\n    if x is not None:\n        return x\n    return val if not isfunction(val) else val()\n\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\n\ndef reshape_dim(t, dim, split_dims):\n    shape = list(t.shape)\n    num_dims = len(shape)\n    dim = (dim + num_dims) % num_dims\n    shape[dim:dim+1] = split_dims\n    return t.reshape(shape)\n\ndef split_at_index(dim, index, t):\n    pre_slices = (slice(None),) * dim\n    l = (*pre_slices, slice(None, index))\n    r = (*pre_slices, slice(index, None))\n    return t[l], t[r]\n\ndef queue_fifo(*args, length, dim=-2):\n    queue = torch.cat(args, dim=dim)\n    if length > 0:\n        return split_at_index(dim, -length, queue)\n\n    device = queue.device\n    shape = list(queue.shape)\n    shape[dim] = 0\n    return queue, torch.empty(shape, device = device)\n\ndef shift(x):\n    *_, i, j = x.shape\n    zero_pad = torch.zeros((*_, i, i), **to(x))\n    x = torch.cat([x, zero_pad], -1)\n    l = i + j - 1\n    x = x.view(*_, -1)\n    zero_pad = torch.zeros(*_, -x.size(-1) % l, **to(x))\n    shifted = torch.cat([x, zero_pad], -1).view(*_, -1, l)\n    return shifted[..., :i, i - 1:]\n\ndef iterate_tensor(t):\n    length = t.shape[0]\n    for ind in range(length):\n        yield t[ind]\n\n# full attention for calculating auxiliary reconstruction loss\n\ndef full_attn(q, k, v, dropout_fn = None):\n    *_, dim = q.shape\n    dots = torch.einsum('bhid,bhjd->bhij', q, k) * (dim ** -0.5)\n    attn = dots.softmax(dim=-1)\n    if dropout_fn is not None:\n        attn = dropout_fn(attn)\n    return torch.einsum('bhij,bhjd->bhid', attn, v)\n\n# helper classes\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        out = cast_tuple(out)\n        ret = (out[0] + x), *out[1:]\n        return ret\n\nclass GRUGating(nn.Module):\n    def __init__(self, dim, fn, mogrify = False):\n        super().__init__()\n        self.dim = dim\n        self.fn = fn\n        self.gru = nn.GRUCell(dim, dim)\n        self.mogrify = Mogrifier(dim, factorize_k = dim // 4) if mogrify else None\n\n    def forward(self, x, **kwargs):\n        batch, dim = x.shape[0], self.dim\n        out = self.fn(x, **kwargs)\n        (y, *rest) = cast_tuple(out)\n\n        if self.mogrify is not None:\n            y, x = self.mogrify(y, x)\n\n        gated_output = self.gru(\n            y.reshape(-1, dim),\n            x.reshape(-1, dim)\n        )\n\n        gated_output = gated_output.reshape(batch, -1, dim)\n        ret = gated_output, *rest\n        return ret\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\nclass ConvCompress(nn.Module):\n    def __init__(self, dim, ratio = 4):\n        super().__init__()\n        self.conv = nn.Conv1d(dim, dim, ratio, stride = ratio)\n\n    def forward(self, mem):\n        mem = mem.transpose(1, 2)\n        compressed_mem = self.conv(mem)\n        return compressed_mem.transpose(1, 2)\n\n# feedforward\n\nclass GELU_(nn.Module):\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\nGELU = nn.GELU if hasattr(nn, 'GELU') else GELU_\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4, dropout = 0., activation = None, glu = False):\n        super().__init__()\n        activation = default(activation, GELU)\n\n        self.glu = glu\n        self.w1 = nn.Linear(dim, dim * mult * (2 if glu else 1))\n        self.act = activation()\n        self.dropout = nn.Dropout(dropout)\n        self.w2 = nn.Linear(dim * mult, dim)\n\n    def forward(self, x, **kwargs):\n        if not self.glu:\n            x = self.w1(x)\n            x = self.act(x)\n        else:\n            x, v = self.w1(x).chunk(2, dim=-1)\n            x = self.act(x) * v\n\n        x = self.dropout(x)\n        x = self.w2(x)\n        return x\n\n# attention.\n\nclass SelfAttention(nn.Module):\n    def __init__(self, dim, seq_len, mem_len, cmem_len, cmem_ratio = 4, heads = 8, attn_dropout = 0., dropout = 0., reconstruction_attn_dropout = 0.):\n        super().__init__()\n        assert (dim % heads) == 0, 'dimension must be divisible by the number of heads'\n\n        self.heads = heads\n        self.dim_head = dim // heads\n        self.seq_len = seq_len\n        self.mem_len = mem_len\n        self.cmem_len = cmem_len\n        self.cmem_ratio = cmem_ratio\n        self.scale = self.dim_head ** (-0.5)\n\n        self.compress_mem_fn = ConvCompress(dim, cmem_ratio)\n\n        self.to_q = nn.Linear(dim, dim, bias = False)\n        self.to_kv = nn.Linear(dim, dim * 2, bias = False)\n        self.to_out = nn.Linear(dim, dim)\n\n        self.attn_dropout = nn.Dropout(attn_dropout)\n        self.dropout = nn.Dropout(dropout)\n\n        self.reconstruction_attn_dropout = nn.Dropout(reconstruction_attn_dropout)\n\n    def forward(self, x, memories = None, pos_emb = None, input_mask = None, calc_memory = True, **kwargs):\n        b, t, e, h, dim_h = *x.shape, self.heads, self.dim_head\n\n        memories = default(memories, (None, None))\n        mem, cmem = memories\n\n        init_empty_mem = lambda: torch.empty(b, 0, e, **to(x))\n        mem = default(mem, init_empty_mem)\n        cmem = default(cmem, init_empty_mem)\n\n        mem_len = mem.shape[1]\n        cmem_len = cmem.shape[1]\n\n        q = self.to_q(x)\n\n        kv_input = torch.cat((cmem, mem, x), dim=1)\n        kv_len = kv_input.shape[1]\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        merge_heads = lambda x: reshape_dim(x, -1, (-1, dim_h)).transpose(1, 2)\n        q, k, v = map(merge_heads, (q, k, v))\n\n        k, v = map(lambda x: x.expand(-1, h, -1, -1), (k, v))\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n        mask_value = max_neg_value(dots)\n\n        if pos_emb is not None:\n            pos_emb = pos_emb[:, -kv_len:].type(q.dtype)\n            pos_dots = torch.einsum('bhid,hjd->bhij', q, pos_emb) * self.scale\n            pos_dots = shift(pos_dots)\n            dots = dots + pos_dots\n\n        if input_mask is not None:\n            mask = input_mask[:, None, :, None] * input_mask[:, None, None, :]\n            mask = F.pad(mask, (mem_len + cmem_len, 0), value = True)\n            dots.masked_fill_(~mask, mask_value)\n\n        total_mem_len = mem_len + cmem_len\n        mask = torch.ones(t, t + total_mem_len, **to(x)).triu_(diagonal = 1 + total_mem_len).bool()\n        dots.masked_fill_(mask[None, None, ...], mask_value)\n\n        attn = dots.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = out.transpose(1, 2).reshape(b, t, -1)\n        logits = self.to_out(out)\n        logits = self.dropout(logits)\n\n        new_mem = mem\n        new_cmem = cmem\n        aux_loss = torch.zeros(1, requires_grad = True, **to(q))\n\n        if self.seq_len > t or not calc_memory:\n            return logits, Memory(new_mem, new_cmem), aux_loss\n\n        # calculate memory and compressed memory\n\n        old_mem, new_mem = queue_fifo(mem, x, length = self.mem_len, dim = 1)\n        old_mem_padding = old_mem.shape[1] % self.cmem_ratio\n\n        if old_mem_padding != 0:\n            old_mem = F.pad(old_mem, (0, 0, old_mem_padding, 0), value = 0.)\n\n        if old_mem.shape[1] == 0 or self.cmem_len <= 0:\n            return logits, Memory(new_mem, new_cmem), aux_loss\n\n        compressed_mem = self.compress_mem_fn(old_mem.detach())\n        old_cmem, new_cmem = split_at_index(1, -self.cmem_len, torch.cat((cmem, compressed_mem), dim=1))\n\n        if not self.training:\n            return logits, Memory(new_mem, new_cmem), aux_loss\n\n        # calculate compressed memory auxiliary loss if training\n\n        self.to_kv.weight.detach_()\n\n        cmem_k, cmem_v = self.to_kv(compressed_mem).chunk(2, dim=-1)\n        cmem_k, cmem_v = map(merge_heads, (cmem_k, cmem_v))\n        cmem_k, cmem_v = map(lambda x: x.expand(-1, h, -1, -1), (cmem_k, cmem_v))\n\n        old_mem_range = slice(- min(mem_len, self.mem_len) - self.seq_len, -self.seq_len)\n        old_mem_k, old_mem_v = map(lambda x: x[:, :, old_mem_range].clone(), (k, v))\n\n        q, old_mem_k, old_mem_v = map(torch.detach, (q, old_mem_k, old_mem_v))\n\n        attn_fn = partial(full_attn, dropout_fn = self.reconstruction_attn_dropout)\n\n        aux_loss = F.mse_loss(\n            attn_fn(q, old_mem_k, old_mem_v),\n            attn_fn(q, cmem_k, cmem_v)\n        )\n\n        return logits, Memory(new_mem, new_cmem), aux_loss\n\n# transformer\n\nclass CompressiveTransformer(nn.Module):\n    def __init__(\n        self,\n        num_tokens,\n        dim,\n        seq_len,\n        depth,\n        emb_dim = None,\n        memory_layers = None,\n        enhanced_recurrence = True,\n        mem_len = None,\n        cmem_len = None,\n        cmem_ratio = 4,\n        heads = 8,\n        gru_gated_residual = True,\n        mogrify_gru = False,\n        attn_dropout = 0.,\n        ff_glu = False,\n        ff_dropout = 0.,\n        attn_layer_dropout = 0.,\n        reconstruction_attn_dropout = 0.,\n        reconstruction_loss_weight = 1.\n    ):\n        super().__init__()\n        emb_dim = default(emb_dim, dim)\n        mem_len = default(mem_len, seq_len)\n        cmem_len = default(cmem_len, mem_len // cmem_ratio)\n        memory_layers = default(memory_layers, list(range(1, depth + 1)))\n\n        assert mem_len >= seq_len, 'length of memory should be at least the sequence length'\n        assert cmem_len >= (mem_len // cmem_ratio), f'length of compressed memory should be at least the memory length divided by the compression ratio {int(mem_len // cmem_ratio)}'\n        assert all([layer > 0 and layer <= depth for layer in memory_layers]), 'one of the indicated memory layers is invalid'\n\n        self.seq_len = seq_len\n\n        self.depth = depth\n        self.memory_layers = list(memory_layers)\n        self.enhanced_recurrence = enhanced_recurrence\n\n        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n        self.to_model_dim = nn.Identity() if emb_dim == dim else nn.Linear(emb_dim, dim)\n\n        seq_and_mem_len = seq_len + mem_len + cmem_len\n        self.pos_emb = nn.Parameter(torch.zeros(heads, seq_and_mem_len, dim // heads))\n        \n        self.to_logits = nn.Sequential(\n            nn.Identity() if emb_dim == dim else nn.Linear(dim, emb_dim),\n            nn.Linear(emb_dim, num_tokens)\n        )\n\n        wrapper = partial(GRUGating, dim, mogrify = mogrify_gru) if gru_gated_residual else Residual\n\n        self.attn_layers = nn.ModuleList([wrapper(PreNorm(dim, SelfAttention(dim, seq_len, mem_len, cmem_len, cmem_ratio, heads, dropout = attn_layer_dropout, attn_dropout = attn_dropout, reconstruction_attn_dropout = reconstruction_attn_dropout))) for _ in range(depth)])\n        self.ff_layers = nn.ModuleList([wrapper(PreNorm(dim, FeedForward(dim, dropout = ff_dropout, glu = ff_glu))) for _ in range(depth)])\n\n        self.reconstruction_loss_weight = reconstruction_loss_weight\n\n    def forward(self, x, memories = None, mask = None):\n        x = self.token_emb(x)\n        x = self.to_model_dim(x)\n        b, t, d = x.shape\n\n        assert t <= self.seq_len, f'input contains a sequence length {t} that is greater than the designated maximum sequence length {self.seq_len}'\n\n        memories = default(memories, (None, None))\n        mem, cmem = memories\n\n        num_memory_layers = len(self.memory_layers)\n        init_empty_mem = lambda: torch.empty(num_memory_layers, b, 0, d, **to(x))\n        mem = default(mem, init_empty_mem)\n        cmem = default(cmem, init_empty_mem)\n\n        total_len = mem.shape[2] + cmem.shape[2] + self.seq_len\n        pos_emb = self.pos_emb[:, (self.seq_len - t):total_len]\n\n        next_mem = []\n        next_cmem = []\n        aux_loss = torch.tensor(0., requires_grad = True, **to(x))\n\n        if self.enhanced_recurrence:\n            mem = torch.roll(mem, -1, 0)\n            cmem = torch.roll(cmem, -1, 0)\n\n        mem_iter, cmem_iter = map(iterate_tensor, (mem, cmem))\n\n        for ind, (attn, ff) in enumerate(zip(self.attn_layers, self.ff_layers)):\n            layer_num = ind + 1\n\n            use_memory = layer_num in self.memory_layers\n            memories = (next(mem_iter), next(cmem_iter)) if use_memory else None\n\n            x, (mem_out, cmem_out), layer_aux_loss = attn(x, memories = memories, calc_memory = use_memory, input_mask = mask, pos_emb = pos_emb)\n            x,  = ff(x)\n\n            aux_loss = aux_loss + layer_aux_loss\n\n            if not use_memory:\n                continue\n\n            next_mem.append(mem_out)\n            next_cmem.append(cmem_out)\n\n        out = self.to_logits(x)\n\n        next_mem, next_cmem = map(torch.stack, (next_mem, next_cmem))\n        next_mem, next_cmem = map(torch.detach, (next_mem, next_cmem))\n\n        aux_loss = aux_loss * self.reconstruction_loss_weight / num_memory_layers\n        return out, Memory(mem = next_mem, compressed_mem = next_cmem), aux_loss",
    "description": null,
    "url": null
}