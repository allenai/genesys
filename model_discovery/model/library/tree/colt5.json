{
    "acronym": "colt5",
    "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
    "seed_ids": [],
    "s2id": "27d391d65ab42c30dc35595213ba6585633afa5d",
    "abstract": "Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.",
    "authors": [
        "J. Ainslie",
        "Tao Lei",
        "Michiel de Jong",
        "Santiago Ontan'on",
        "Siddhartha Brahma",
        "Yury Zemlyanskiy",
        "David C. Uthus",
        "Mandy Guo",
        "J. Lee-Thorp",
        "Yi Tay",
        "Yun-Hsuan Sung",
        "Sumit K. Sanghai"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": "CoLT5 is proposed, a long-input Transformer model that builds on intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers, and achieves stronger performance than LongT5 with much faster training and inference.",
    "citationCount": 46,
    "influentialCitationCount": 5,
    "code": "import math\nfrom functools import partial\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn, einsum\n\nfrom typing import Tuple, Optional\n\nfrom local_attention import LocalMHA\nfrom einops import rearrange, repeat, pack, unpack\n\nfrom colt5_attention.attend import Attention,RotaryEmbedding\n\n\nfrom torch.cuda.amp import autocast\n\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef divisible_by(numer, denom):\n    return (numer % denom) == 0\n\ndef pack_one(t, pattern):\n    return pack([t], pattern)\n\ndef unpack_one(t, ps, pattern):\n    return unpack(t, ps, pattern)[0]\n\ndef pad_to_multiple(tensor, multiple, dim=-1, value=0):\n    seq_len = tensor.shape[dim]\n    m = seq_len / multiple\n    if m.is_integer():\n        return tensor, seq_len\n\n    remainder = math.ceil(m) * multiple - seq_len\n    pad_offset = (0,) * (-1 - dim) * 2\n    padded_tensor = F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n    return padded_tensor, seq_len\n\ndef batched_gather(x, indices):\n    batch_range = create_batch_range(indices, indices.ndim - 1)\n    return x[batch_range, indices]\n\ndef identity(t):\n    return t\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\n# tensor helpers\n\ndef create_batch_range(t, right_pad_dims = 1):\n    b, device = t.shape[0], t.device\n    batch_range = torch.arange(b, device = device)\n    pad_dims = ((1,) * right_pad_dims)\n    return batch_range.reshape(-1, *pad_dims)\n\n# modules\n\n\n# routing related logic\n\n@autocast(enabled = False)\ndef coor_descent(\n    s,\n    *,\n    n_iters,\n    k,\n    eps = 1e-1,\n    eps_init = None,\n    eps_decay = 1.,\n    mask = None\n):\n    \"\"\"\n    coordinate descent  - https://arxiv.org/abs/1502.04759, utilized in https://arxiv.org/abs/2303.09752\n    \u03b5-scaling           - https://arxiv.org/abs/1610.06519, utilized in https://arxiv.org/abs/2304.04947\n\n    in a follow up paper applying coordinate descent routing to efficient fine tuning\n    they were able to cut n_iters from 50 -> 20 by setting eps_init = 4 and eps_decay = 0.7\n    eps was dependent on the task, and ranged from 0.02 to 1\n    \"\"\"\n\n    assert n_iters > 0\n\n    mask_value = -torch.finfo(s.dtype).max\n\n    if not isinstance(k, torch.Tensor):\n        k = torch.Tensor([k]).to(s)\n    else:\n        k = rearrange(k, '... -> ... 1')\n\n    logk = log(k)\n\n    if exists(mask):\n        s = s.masked_fill(~mask, mask_value)\n\n    a = 0\n    b = -s\n\n    current_eps = max(default(eps_init, eps), eps)\n\n    for _ in range(n_iters):\n        sb = ((s + b) / current_eps)\n\n        if exists(mask):\n            sb = sb.masked_fill(~mask, mask_value)\n\n        a = current_eps * (logk - sb.logsumexp(dim = -1, keepdim = True))\n        b = -F.relu(s + a)\n\n        current_eps = max(current_eps * eps_decay, eps)\n\n    scores = ((s + a + b) / current_eps).exp()\n\n    if exists(mask):\n        scores = scores.masked_fill(~mask, 0.)\n\n    return scores\n\nRouterReturn = namedtuple('RouterReturn', ['indices', 'scores', 'routed_tokens', 'routed_mask'])\n\nclass CoordinateDescentRouter(nn.Module):\n    \"\"\"\n    from Wright et al. https://arxiv.org/abs/1502.04759\n    then adopted by https://arxiv.org/abs/2211.01267 for multi-vector document retrieval by Qian et al\n    finally, used successfully by this paper for routing to heavy branch attention / feedforward\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        straight_through = True,\n        n_iters = 20,                   # 20 iterations in a new paper, utilizing \u03b5-scaling\n        fetch_k_ratio = 9 / 8,          # in the paper, they do a bit slightly higher k (times this ratio) for better learning\n        eps = 0.03,                     # the epsilon for coordinate descent. in a recent paper, they used 0.03 for text and 1.0 for speech\n        eps_decay = 0.7,\n        eps_init = 4.,\n        num_routing_tokens = 1,\n        learned_routing_tokens = False,\n        use_triton = False,\n        cosine_sim_routing = False,\n        cosine_sim_scale = 8,\n        route_block_size = None,\n        triton_checkpoint_segments = None # whether to recompute the coordinate descent in segments, with 4 and 50 iterations, backwards is sped up 3x times at the expense of forwards and some memory for saving initial a and b\n    ):\n        super().__init__()\n        assert fetch_k_ratio >= 1.\n\n        self.n_iters = n_iters\n        self.fetch_k_ratio = fetch_k_ratio\n\n        self.coor_descent = coor_descent\n\n        # epsilon related hparams, for \u03b5-scaling\n\n        self.eps = eps\n        self.eps_decay = eps_decay\n        self.eps_init = eps_init\n\n        if use_triton:\n            from colt5_attention.triton_coor_descent import triton_coor_descent\n            triton_checkpoint_segments = default(triton_checkpoint_segments, n_iters // 5)\n            self.coor_descent = partial(triton_coor_descent, checkpoint_segments = triton_checkpoint_segments)\n\n        self.is_one_routing_token = num_routing_tokens == 1\n        self.num_routing_tokens = num_routing_tokens\n\n        self.route_block_size = route_block_size\n\n        self.routing_token = nn.Parameter(torch.randn(num_routing_tokens, dim)) if not learned_routing_tokens else None\n        self.straight_through = straight_through\n\n        # whether to use cosine sim for routing\n\n        self.cosine_sim_routing = cosine_sim_routing\n        self.cosine_sim_scale = cosine_sim_scale\n\n    def route_back(self, src, routed_tokens, indices):\n        batch_range = create_batch_range(routed_tokens)\n        src[batch_range, indices] = routed_tokens\n        return src\n\n    def forward(\n        self,\n        x,\n        *,\n        num_tokens,\n        mask = None,\n        random_route = False,\n        routing_tokens = None,\n        keep_one_route_dim = False  # if only one route, whether to keepdim\n    ):\n        n, device, eps, eps_init, eps_decay, num_routes, route_block_size = x.shape[-2], x.device, self.eps, self.eps_init, self.eps_decay, self.num_routing_tokens, self.route_block_size\n\n        # do not route if the sequence length is less than the number of tokens\n\n        has_route_dim = keep_one_route_dim or not self.is_one_routing_token\n\n        if n <= num_tokens:\n            b = x.shape[0]\n            r = self.num_routing_tokens\n\n            if has_route_dim:\n                scores_shape = (b, r, n)\n\n                x = repeat(x, 'b n d -> b r n d', r = r)\n\n                if exists(mask):\n                    mask = repeat(mask, 'b n -> b r n', r = r)\n            else:\n                scores_shape = (b, n)\n\n            scores = torch.ones(scores_shape, device = device, dtype = x.dtype)\n\n            return RouterReturn(None, scores, x, mask)\n\n        # whether to route even amounts from blocks of the sequence\n\n        if exists(route_block_size):\n            num_blocks = n // route_block_size\n            prev_seq_mult = num_blocks * route_block_size\n\n            # just curtail to last multiple of route block size\n\n            x = x[:, :prev_seq_mult]\n\n            # group sequence into blocks to route\n\n            x = rearrange(x, 'b (n w) d -> (b n) w d', w = route_block_size)\n\n            if exists(mask):\n                mask = mask[:, :prev_seq_mult]\n                mask = rearrange(mask, 'b (n w) -> (b n) w', w = route_block_size)\n\n            n = route_block_size\n            num_tokens = math.ceil(num_tokens / num_blocks)\n\n        # s stands for eventual normalized score\n\n        maybe_l2norm = l2norm if self.cosine_sim_routing else identity\n\n        if exists(self.routing_token):\n            s = einsum('b n d, r d -> b r n', maybe_l2norm(x), maybe_l2norm(self.routing_token))\n        else:\n            assert exists(routing_tokens)\n\n            if routing_tokens.ndim == 2:\n                routing_tokens = rearrange(routing_tokens, 'b d -> b 1 d')\n\n            s = einsum('b n d, b r d -> b r n', maybe_l2norm(x), maybe_l2norm(routing_tokens))\n\n        if self.cosine_sim_routing:\n            s = s * self.cosine_sim_scale\n\n        # merge routing dimension into batch\n\n        x = repeat(x, 'b ... -> (b r) ...', r = num_routes)\n        s, ps = pack_one(s, '* n')\n\n        if exists(mask):\n            mask = repeat(mask, 'b ... -> (b r) ...', r = num_routes)\n\n        # k, which controls the sparsity of the outputted scores from iterative coordinate descent\n\n        effective_k = min(num_tokens * self.fetch_k_ratio, n)\n\n        # coordinate descent\n\n        scores = self.coor_descent(\n            s,\n            n_iters = self.n_iters,\n            mask = mask,\n            k = effective_k,\n            eps = eps,\n            eps_init = eps_init,\n            eps_decay = eps_decay\n        )\n\n        # force random routing, if negative control\n\n        if random_route:\n            scores = torch.randn_like(scores)\n            scores = scores.masked_fill(~mask, -torch.finfo(scores.dtype).max)\n\n        # get the topk scores and indices from the sparse matrix\n\n        selected_scores, selected_indices = scores.topk(num_tokens, dim = -1)\n\n        if self.straight_through:\n            # this would make sure all normalized scores returned are 1., but still differentiable using straight-through trick\n            selected_scores = selected_scores + (1. - selected_scores).detach()\n\n            if exists(mask):                \n                selected_mask = batched_gather(mask, selected_indices)\n                selected_scores = selected_scores.masked_fill(~selected_mask, 0.)\n\n        # split out routing dimension again if need be\n\n        if has_route_dim:\n            selected_scores = unpack_one(selected_scores, ps, '* n')\n            selected_indices = unpack_one(selected_indices, ps, '* n')\n\n        # undo the windowing, if one were routing uniformly in blocks\n\n        if exists(route_block_size):\n            selected_scores = rearrange(selected_scores, '(b n) ... w -> b ... (n w)', n = num_blocks)\n            selected_indices = rearrange(selected_indices, '(b n) ... w -> b ... n w', n = num_blocks)\n\n            indices_offset = torch.arange(num_blocks, device = device) * route_block_size\n            selected_indices = selected_indices + rearrange(indices_offset, 'n -> n 1')\n            selected_indices = rearrange(selected_indices, 'b ... n w -> b ... (n w)')\n\n        # auto-gather the routed tokens and mask (if not None)\n\n        routed_tokens = batched_gather(x, selected_indices)\n\n        routed_mask = None\n        if exists(mask):\n            routed_mask = batched_gather(mask, selected_indices)\n\n        # return indices, scores, routed tokens and mask\n\n        return RouterReturn(selected_indices, selected_scores, routed_tokens, routed_mask)\n\n# main classes\n\n\nclass ConditionalRoutedAutoregressiveAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        num_heavy_tokens_q,\n        num_heavy_tokens_kv,\n        num_routed_kv = 1,\n        light_dim_head = 64,\n        light_heads = 8,\n        light_window_size = 128,        # each token would see ~ 64 tokens either way to left or right\n        heavy_window_size = None,\n        heavy_dim_head = 64,\n        heavy_heads = 8,\n        router_straight_through = True, # would make sure all normalized scores are 1., still differentiable\n        router_kwargs: dict = {},\n        multiply_keys_by_score = False,\n        multiply_queries_by_score = False,\n        use_triton = False,\n        use_null_q_tokens = True,\n        use_flash_attn = False,\n        rotary_emb = False\n    ):\n        super().__init__()\n\n        if use_triton:\n            router_kwargs = {**router_kwargs, 'use_triton': True}\n\n        self.num_heavy_tokens_q = num_heavy_tokens_q\n        self.num_heavy_tokens_kv = num_heavy_tokens_kv\n\n        self.multiply_queries_by_score = multiply_queries_by_score\n\n        self.heavy_window_size = default(heavy_window_size, light_window_size)\n\n        self.light_attn = LocalMHA(\n            dim = dim,\n            dim_head = light_dim_head,\n            heads = light_heads,\n            window_size = light_window_size,\n            prenorm = True,\n            causal = True,\n            exact_windowsize = False,\n            use_rotary_pos_emb = False\n        )\n\n        self.null_q_token = None\n        if use_null_q_tokens:\n            self.null_q_token = nn.Parameter(torch.randn(dim)) # for the query tokens not selected by the router, give it a learned output embed\n\n        self.q_router = CoordinateDescentRouter(\n            dim = dim,\n            straight_through = router_straight_through,\n            **router_kwargs\n        )\n\n        self.kv_router = CoordinateDescentRouter(\n            dim = dim,\n            num_routing_tokens = num_routed_kv,\n            straight_through = router_straight_through,\n            **router_kwargs\n        )\n\n        self.heavy_attn = Attention(\n            dim = dim,\n            dim_head = heavy_dim_head,\n            heads = heavy_heads,\n            multiply_keys_by_score = multiply_keys_by_score,\n            use_flash = use_flash_attn\n        )\n\n        # rotary embedding\n\n        self.rotary_emb = RotaryEmbedding(heavy_dim_head) if rotary_emb else None\n\n    def forward(\n        self,\n        x,\n        *,\n        num_heavy_tokens_q = None,\n        num_heavy_tokens_kv = None,\n        random_route = False\n    ):\n        batch, seq, device = *x.shape[:2], x.device\n\n        num_heavy_tokens_q = default(num_heavy_tokens_q, self.num_heavy_tokens_q)\n        num_heavy_tokens_kv = default(num_heavy_tokens_kv, self.num_heavy_tokens_kv)\n\n        # light local attention sees all tokens in a limited context\n\n        light_out = self.light_attn(x)\n\n        # pad sequence to multiple of the heavy window size\n        # routing will take place within each heavy window block size\n\n        window_size = self.heavy_window_size\n\n        x, seq_len = pad_to_multiple(x, window_size, dim = -2)\n\n        padded_seq_len = x.shape[-2]\n\n        # construct mask, and make sure not to attend to padding\n\n        q_mask = torch.ones((batch, seq_len), dtype = torch.bool, device = device)\n        q_mask = F.pad(q_mask, (0, padded_seq_len - seq_len), value = False)\n\n        # handy function\n\n        merge_to_batch = lambda t: rearrange(t, 'b n ... -> (b n) ...')\n\n        # block the sequence and mask into windows for the queries\n\n        q = rearrange(x, 'b (n w) d -> b n w d', w = window_size)\n        q_mask = rearrange(q_mask, 'b (n w) -> b n w', w = window_size)\n\n        q, q_mask = map(merge_to_batch, (q[:, 1:], q_mask[:, 1:]))\n\n        # each block of queries attend to sequences that are causally masked out appropriately\n\n        windows = padded_seq_len // window_size\n\n        kv = repeat(x, 'b n d -> b m n d', m = windows)\n\n        kv_mask = torch.ones((windows, windows), dtype = torch.bool, device = device).tril(-1)\n        kv_mask = repeat(kv_mask, 'm n -> b m (n w)', b = batch, w = window_size)\n\n        kv, kv_mask = map(merge_to_batch, (kv[:, 1:], kv_mask[:, 1:]))\n\n        # route tokens appropriately for heavy branch, if need be\n\n        should_route_q = q.shape[-2] > num_heavy_tokens_q\n        should_route_kv = kv.shape[-2] > num_heavy_tokens_kv\n\n        indices_q, normalized_scores_q, routed_tokens_q, _ = self.q_router(q, num_tokens = num_heavy_tokens_q, mask = q_mask, random_route = random_route)\n\n        indices_kv, normalized_scores_kv, routed_tokens_kv, routed_tokens_kv_mask = self.kv_router(kv, num_tokens = num_heavy_tokens_kv, mask = kv_mask, random_route = random_route)\n\n        # get rotary embeddings if specified\n\n        rotary_emb = None\n\n        if exists(self.rotary_emb):\n            seq_rotary_emb = self.rotary_emb(padded_seq_len)\n\n            windowed_rotary_emb = rearrange(seq_rotary_emb, '(n w) d -> n w d', w = window_size)\n            windowed_rotary_emb = windowed_rotary_emb[1:]\n            windowed_rotary_emb = repeat(windowed_rotary_emb, 'n w d -> (b n) w d', b = batch)\n\n            if exists(indices_q):\n                rotary_indices_q = repeat(indices_q, '... -> ... d', d = windowed_rotary_emb.shape[-1])\n                q_rotary_emb = windowed_rotary_emb.gather(1, rotary_indices_q)\n            else:\n                q_rotary_emb = windowed_rotary_emb\n\n            q_rotary_emb = rearrange(q_rotary_emb, 'b n d -> b 1 n d')\n            k_rotary_emb = rearrange(seq_rotary_emb[indices_kv], '... n d -> ... 1 n d') if exists(indices_kv) else seq_rotary_emb\n            rotary_emb = (q_rotary_emb, k_rotary_emb)\n\n        # do the heavier branch with only routed tokens\n\n        routed_tokens_out = self.heavy_attn(\n            routed_tokens_q,\n            mask = routed_tokens_kv_mask,\n            context = routed_tokens_kv,\n            rotary_emb = rotary_emb,\n            normalized_scores_kv = normalized_scores_kv,\n            normalized_scores_q = normalized_scores_q if self.multiply_queries_by_score else None\n        )\n\n        if exists(indices_q):\n            routed_tokens_out = routed_tokens_out * rearrange(normalized_scores_q, '... -> ... 1')\n\n            # scatter back the output of the heavy branch\n\n            if exists(self.null_q_token):\n                heavy_out = rearrange(self.null_q_token, 'd -> 1 1 d')\n                heavy_out = heavy_out.expand_as(q).clone()\n            else:\n                heavy_out = torch.zeros_like(q)\n\n            heavy_out = self.q_router.route_back(heavy_out, routed_tokens_out, indices_q)\n        else:\n            heavy_out = routed_tokens_out\n\n        # un-window and slice out original sequence\n\n        heavy_out = rearrange(heavy_out, '(b n) w d -> b (n w) d', b = batch)\n        heavy_out = heavy_out[:, :(seq_len - window_size)]\n\n        heavy_out = F.pad(heavy_out, (0, 0, window_size, 0), value = 0.)\n\n        # sum light and heavy branches\n\n        return light_out + heavy_out\n\n\n\n",
    "description": null,
    "url": null
}