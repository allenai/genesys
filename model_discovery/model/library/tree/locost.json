{
    "acronym": "locost",
    "title": "LOCOST: State-Space Models for Long Document Abstractive Summarization",
    "seed_ids": [
        "ptwoattn",
        "longt5",
        "fnet",
        "longformer"
    ],
    "s2id": "99621f3ce8caf5d99f2b350d53ec8e6c57695bc2",
    "abstract": "State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.",
    "authors": [
        "Florian Le Bronnec",
        "Song Duong",
        "Mathieu Ravaut",
        "Alexandre Allauzen",
        "Nancy F. Chen",
        "Vincent Guigue",
        "Alberto Lumbreras",
        "Laure Soulier",
        "Patrick Gallinari"
    ],
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "tldr": "This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.",
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": "import copy\nfrom typing import Any, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import PreTrainedModel\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\nfrom transformers.utils import DUMMY_INPUTS, DUMMY_MASK, is_torch_fx_proxy, logging\n\nfrom longt5_models import (\n    LongT5LayerCrossAttention,\n    LongT5LayerFF,\n    LongT5LayerNorm,\n    LongT5LayerSelfAttention,\n)\nfrom models_config import LOCOSTConfig\nfrom s4d_models import S4D\n\nlogger = logging.get_logger(__name__)\n\n\ndef clamp_inf(hidden_states):\n    if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n        clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n        hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n\nclass LOCOSTLayerSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.d_model = config.d_model\n\n        self.ssm_layer = S4D(config)\n\n        self.layer_prenorm = LongT5LayerNorm(\n            config.d_model, eps=config.layer_norm_epsilon\n        )\n\n        self.config = config\n\n    def forward(\n        self,\n        hidden_states,\n        original_attention_mask=None,\n        output_attentions=False,\n        **kwargs: Any,  # to accept past_key_value and use_cache kwargs\n    ):\n        normed_hidden_states = self.layer_prenorm(hidden_states)\n\n        ssm_outputs = self.ssm_layer(\n            normed_hidden_states,\n            attention_mask=original_attention_mask,\n            output_attentions=output_attentions,\n        )\n        ssm_hidden_states = ssm_outputs[0]\n        kernel_outputs = ssm_outputs[1:]\n\n        hidden_states = hidden_states + F.dropout(\n            ssm_hidden_states, p=self.config.dropout_rate, training=self.training\n        )\n\n        outputs = (hidden_states, None, None, None) + kernel_outputs\n        return outputs  # hidden_states, present_key_value, (self-attention position bias), (self-attention weights), kernel\n\n\nclass LOCOSTBlock(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n\n        self.layer = nn.ModuleList()\n        if self.is_decoder:\n            self_attn = LongT5LayerSelfAttention(\n                config, has_relative_attention_bias=has_relative_attention_bias\n            )\n        else:\n\n            self_attn = LOCOSTLayerSelfAttention(config)\n\n        self.layer.append(self_attn)\n        if self.is_decoder:\n            cross_attn_layer = LongT5LayerCrossAttention(config)\n            self.layer.append(cross_attn_layer)\n        ff_layer = LongT5LayerFF(config)\n        self.layer.append(ff_layer)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        original_attention_mask=None,\n        position_bias=None,\n        layer_head_mask=None,\n        cross_attn_layer_head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        encoder_decoder_position_bias=None,\n        use_cache=False,\n        output_attentions=False,\n        past_key_value=None,\n        return_dict=True,\n    ):\n        if past_key_value is not None:\n            if not self.is_decoder:\n                logger.warning(\n                    \"`past_key_values` is passed to the encoder. Please make sure this is intended.\"\n                )\n            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n\n            if len(past_key_value) != expected_num_past_key_values:\n                raise ValueError(\n                    f\"There should be {expected_num_past_key_values} past states. \"\n                    f\"{'2 (past / key) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n                    f\"Got {len(past_key_value)} past key / value states\"\n                )\n\n            self_attn_past_key_value = past_key_value[:2]\n            cross_attn_past_key_value = past_key_value[2:]\n        else:\n            self_attn_past_key_value, cross_attn_past_key_value = None, None\n\n        self_attn_output = self.layer[0](\n            hidden_states,\n            attention_mask=attention_mask,\n            original_attention_mask=original_attention_mask,\n            position_bias=position_bias,\n            layer_head_mask=layer_head_mask,\n            past_key_value=self_attn_past_key_value,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n        )\n\n        hidden_states, present_key_value_state = self_attn_output[:2]\n        attention_outputs = self_attn_output[2:]\n        do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n\n        clamp_inf(hidden_states)\n        if do_cross_attention:\n            if present_key_value_state is not None:\n                query_length = present_key_value_state[0].shape[2]\n            else:\n                query_length = None\n\n            cross_attention_outputs = self.layer[1](\n                hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                position_bias=encoder_decoder_position_bias,\n                layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n                query_length=query_length,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n            )\n            hidden_states = cross_attention_outputs[0]\n            clamp_inf(hidden_states)\n            if present_key_value_state is not None:\n                present_key_value_state = (\n                    present_key_value_state + cross_attention_outputs[1]\n                )\n\n            # Keep cross-attention outputs and relative position weights\n            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n\n        # FF\n        hidden_states = self.layer[-1](hidden_states)\n\n        clamp_inf(hidden_states)\n        outputs = (hidden_states, present_key_value_state) + attention_outputs\n        # hidde_states, present_key_value, (self-attention position bias), (self-attention weights), kernel, (cross-attention position bias), (cross-attention weights)\n        return outputs\n\n",
    "description": null,
    "url": null
}