{
    "title": "The Expressive Capacity of State Space Models: A Formal Language Perspective ",
    "acronym": "ssmformal",
    "s2id": "e7f47e8393c697696a3fccd9ff906dfdb49fe736",
    "abstract": "Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.",
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "seed_ids": [
        "griffin",
        "gla",
        "mamba",
        "s4"
    ],
    "code": null
}