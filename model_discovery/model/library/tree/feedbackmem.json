{
    "acronym": "feedbackmem",
    "title": "Addressing Some Limitations of Transformers with Feedback Memory",
    "seed_ids": [
        "transformer",
        "bert"
    ],
    "s2id": null,
    "abstract": "Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.",
    "authors": [
        "Angela Fan",
        "Thibaut Lavril",
        "Edouard Grave",
        "Armand Joulin",
        "Sainbayar Sukhbaatar"
    ],
    "venue": "arXiv",
    "year": 2020,
    "tldr": "Transformers have shortcomings - limited memory and limited state update - but Feedback Memory is a straightforward way to resolve these. ",
    "citationCount": null,
    "influentialCitationCount": null,
    "code": "import math\nfrom collections import namedtuple\n\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom einops import rearrange\n\n# constants\n\nMemory = namedtuple('Memory', ['keys', 'values'])\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef safe_cat(arr, el, dim = 1):\n    if not exists(arr):\n        return el\n    return torch.cat((arr, el), dim = dim)\n\n# positional embedding\n\nclass RelativePositionBias(nn.Module):\n    def __init__(\n        self,\n        causal = False,\n        num_buckets = 32,\n        max_distance = 128,\n        heads = 8\n    ):\n        super().__init__()\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, causal = True, num_buckets = 32, max_distance = 128):\n        ret = 0\n        n = -relative_position\n        if not causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n        ).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def forward(self, qk_dots):\n        i, j, device = *qk_dots.shape[-2:], qk_dots.device\n        q_pos = torch.arange(i, dtype = torch.long, device = device)\n        k_pos = torch.arange(j, dtype = torch.long, device = device)\n        rel_pos = k_pos[None, :] - q_pos[:, None]\n        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        bias = rearrange(values, 'i j h -> () h i j')\n        return bias\n\n# helper classes\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\nclass SkipIf(nn.Module):\n    def __init__(self, cond, fn):\n        super().__init__()\n        self.cond = cond\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        if self.cond(x, *args, **kwargs):\n            return x\n        return self.fn(x, *args, **kwargs)\n\n# feedforward\n\nclass GEGLU(nn.Module):\n    def forward(self, x):\n        x, gate = x.chunk(2, dim = -1)\n        return F.gelu(gate) * x\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        mult = 4,\n        dropout = 0.\n    ):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult * 2),\n            GEGLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * mult, dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        heads = 8,\n        dim_head = 64,\n        dropout = 0.\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        inner_dim = dim_head * heads\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, memory, pos_emb = None):\n        h, n, device = self.heads, x.shape[1], x.device\n\n        self_attend = n > 1 # only self attend if going at greater than 1 token at a time\n\n        q = self.to_q(x) * self.scale\n\n        k, v = memory if exists(memory) else (None, None)\n\n        if self_attend:\n            self_k, self_v = self.to_kv(x).chunk(2, dim = -1)\n            k = safe_cat(k, self_k, dim = 1)\n            v = safe_cat(v, self_v, dim = 1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        i, j = sim.shape[-2:]\n\n        if exists(pos_emb):\n            sim = sim + pos_emb(sim)\n\n        if self_attend:\n            causal_mask = torch.ones(i, j, device = device).triu_(j - i + 1).bool()\n            causal_mask = rearrange(causal_mask, 'i j -> () () i j')\n            mask_value = -torch.finfo(q.dtype).max\n            sim.masked_fill_(causal_mask, mask_value)\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\n# main class\n\nclass FeedbackTransformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        depth,\n        mem_len,\n        seq_len = 2,\n        heads = 8,\n        dim_head = 64,\n        attn_dropout = 0.,\n        ff_dropout = 0.,\n        keep_last_hidden = False\n    ):\n        super().__init__()\n        self.seq_len = seq_len\n        self.mem_len = mem_len\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = RelativePositionBias(causal = True, heads = heads)\n\n        # main layers\n\n        self.layers = nn.ModuleList([])\n        shared_kv_proj = None\n\n        for _ in range(depth):\n            attn = Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)\n            ff = FeedForward(dim = dim, dropout = ff_dropout)\n\n            shared_kv_proj = default(shared_kv_proj, attn.to_kv)\n            attn.to_kv = shared_kv_proj\n\n            attn, ff = map(lambda fn: Residual(PreNorm(dim, fn)), (attn, ff))\n\n            if seq_len == 1:\n                memory_is_empty = lambda *args, **kwargs: not exists(kwargs['memory'])\n                attn = SkipIf(memory_is_empty, attn)\n\n            self.layers.append(nn.ModuleList([\n                attn,\n                ff\n            ]))\n\n        # memory parameters\n\n        self.layer_weight = nn.Parameter(torch.ones(depth + 1))\n        self.shared_kv_proj = shared_kv_proj\n        self.keep_last_hidden = keep_last_hidden\n\n        # final projection to logits\n\n        self.to_logits = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_tokens)\n        )\n\n    def forward(self, x, memory = None, return_memory = False):\n        b, n, device = *x.shape, x.device\n\n        x = self.token_emb(x)\n\n        memory_keys = None\n        memory_values = None\n\n        if exists(memory):\n            memory_keys, memory_values = memory\n\n        outputs = []\n\n        # calculate weighting of layers for storing to memory\n\n        layer_weight = self.layer_weight.softmax(dim = -1)\n        layer_weight = rearrange(layer_weight, 'd -> d () () ()')\n\n        for x in x.split(self.seq_len, dim = 1):\n            hiddens = [x]\n\n            # prepare memory for attention, if it exists\n\n            memory = None\n            if exists(memory_keys):\n                memory = (memory_keys, memory_values)\n\n            for attn, ff in self.layers:\n\n                x = attn(x, memory = memory, pos_emb = self.pos_emb)\n                x = ff(x)\n\n                hiddens.append(x)\n\n            outputs.append(x)\n\n            # calculate new memory key / values and store to FIFO queue\n\n            if self.keep_last_hidden: # secret option for only keeping last hidden layer, as in paper\n                agg_hiddens = hiddens[-1]\n            else:\n                hiddens = torch.stack(hiddens)\n                agg_hiddens = (hiddens * layer_weight).sum(dim = 0)\n\n            # pre-calculate memory key / values and store to buffer\n\n            mem_k, mem_v = self.shared_kv_proj(agg_hiddens).chunk(2, dim = -1)\n            memory_keys = safe_cat(memory_keys, mem_k, dim = 1)\n            memory_values = safe_cat(memory_values, mem_v, dim = 1)\n\n            # enforce max length on memory buffer\n\n            memory_keys = memory_keys[:, -self.mem_len:]\n            memory_values = memory_values[:, -self.mem_len:]\n\n        x = torch.cat((outputs), dim = 1)\n        out = self.to_logits(x)\n\n        if not return_memory:\n            return out\n\n        return out, Memory(memory_keys, memory_values)",
    "description": null,
    "url": null
}