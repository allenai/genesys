{
    "acronym": "longformer",
    "title": "Longformer: The Long-Document Transformer",
    "seed_ids": [
        "reformer",
        "compressivetransformer",
        "bptrans",
        "blockbert",
        "sparsetransformer",
        "transformerxl",
        "transformer",
        "bert"
    ],
    "s2id": "925ad2897d1b5decbea320d07e99afa9110e09b2",
    "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
    "authors": [
        "Iz Beltagy",
        "Matthew E. Peters",
        "Arman Cohan"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": "Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks.",
    "citationCount": 3062,
    "influentialCitationCount": 566,
    "code": "from typing import List\nimport math\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers.modeling_roberta import RobertaConfig, RobertaModel, RobertaForMaskedLM\n\n\nfrom typing import Union\nfrom functools import lru_cache\n\nimport os.path\n\n\nclass DiagonaledMM(torch.autograd.Function):\n    '''Class to encapsulate tvm code for compiling a diagonal_mm function, in addition to calling\n    this function from PyTorch\n    '''\n\n    function_dict = {}  # save a list of functions, each has a different set of parameters\n\n    @staticmethod\n    def _compile_function(dtype: str, device: str, b0: int = 4, b1: int = 4, b2: int = 16):\n        '''Compiles a tvm function that computes diagonal_mm\n        args:\n        dtype: str in ['float64', 'float32', 'float16']\n        device: str in ['cpu' or 'cuda']\n        b0, b1, b2: size of tensor tiles. Very important for good performance\n\n        '''\n        import tvm  # import the full tvm library here for compilation. Don't import at the top of the file in case we don't need to compile\n        from tvm.contrib import nvcc\n        @tvm.register_func\n        def tvm_callback_cuda_compile(code):\n            \"\"\"Use nvcc compiler for better perf.\"\"\"\n            ptx = nvcc.compile_cuda(code, target=\"ptx\", arch='sm_52')  # use old arch for this to work on old GPUs\n            return ptx\n\n        assert dtype in ['float16', 'float32', 'float64']\n        assert device in ['cpu', 'cuda']\n        device = None if device == 'cpu' else device\n        tgt_host=\"llvm\"\n\n        b = tvm.var('b')  # batch size\n        n = tvm.var('n')  # sequence length\n        h = tvm.var('h')  # number of heads\n        m = tvm.var('m')  # hidden dimension\n        w = tvm.var('w')  # window size\n        w_upper = tvm.var('w_upper')  # window size to the right of the word. Should be `0` or `w`\n        padding = tvm.var('padding')  # padding\n        transpose_t1 = tvm.var('transpose_t1')  # t1 should be transposed\n        t1d3 = tvm.var('t1d3')  # last dimension of t1\n        t3d3 = tvm.var('t3d3')  # last dimension of t3 (the result tensor)\n        X = tvm.placeholder((b, n, h, t1d3), name='X', dtype=dtype)  # first tensor\n        Y = tvm.placeholder((b, n, h, m), name='Y', dtype=dtype)  # second tensor\n        k = tvm.reduce_axis((0, t1d3), name='k')  # dimension to sum over\n        D = tvm.placeholder((h), name='D', dtype='int')  # dilation per head\n        output_shape = (b, n, h, t3d3)  # shape of the result tensor\n        algorithm = lambda l, i, q, j: tvm.sum(\n            tvm.if_then_else(\n                t3d3 == m,  # if output dimension == m, then t1 is diagonaled (FIXME: This breaks if t3d3 == m == t1d3)\n                tvm.if_then_else(\n                    transpose_t1 == 0,\n                    tvm.if_then_else(\n                        tvm.all(\n                            i + D[q] * (k - w) >= 0,\n                            i + D[q] * (k - w) < n,\n                        ),\n                        X[l, i, q, k] * Y[l, i + D[q] * (k - w), q, j],  # t1 is diagonaled\n                        padding\n                    ),\n                    tvm.if_then_else(\n                        tvm.all(\n                            i + D[q] * (k - w_upper) >= 0,  # `w_upper` to handle the case `autoregressive=True`\n                            i + D[q] * (k - w_upper) < n,\n                        ),\n                        X[l, i + D[q] * (k - w_upper), q, (w_upper + w) - k] * Y[l, i + D[q] * (k - w_upper), q, j],  # # t1 is diagonaled and should be transposed\n                        padding\n                    ),\n                ),\n                tvm.if_then_else(\n                    tvm.all(\n                        i + D[q] * (j - w) >= 0,\n                        i + D[q] * (j - w) < n,\n                    ),\n                    X[l, i, q, k] * Y[l, i + D[q] * (j - w), q, k],  # t1 is not diagonaled, but the output tensor is going to be\n                    padding\n                )\n            ), axis=k)\n\n        Z = tvm.compute(output_shape, algorithm, name='Z')  # automatically generate cuda code\n        s = tvm.create_schedule(Z.op)\n\n        print('Lowering: \\n ===================== \\n{}'.format(tvm.lower(s, [X, Y, D], simple_mode=True)))\n\n        # split long axis into smaller chunks and assing each one to a separate GPU thread/block\n        ko, ki = s[Z].split(Z.op.reduce_axis[0], factor=b0)\n        ZF = s.rfactor(Z, ki)\n\n        j_outer, j_inner = s[Z].split(s[Z].op.axis[-1], factor=b1)\n        i_outer, i_inner = s[Z].split(s[Z].op.axis[1], factor=b2)\n\n        s[Z].bind(j_outer, tvm.thread_axis(\"blockIdx.x\"))\n        s[Z].bind(j_inner, tvm.thread_axis(\"threadIdx.y\"))\n\n        s[Z].bind(i_outer, tvm.thread_axis(\"blockIdx.y\"))\n        s[Z].bind(i_inner, tvm.thread_axis(\"threadIdx.z\"))\n\n        tx = tvm.thread_axis(\"threadIdx.x\")\n        s[Z].bind(s[Z].op.reduce_axis[0], tx)\n        s[ZF].compute_at(s[Z], s[Z].op.reduce_axis[0])\n        s[Z].set_store_predicate(tx.var.equal(0))\n\n        print('Lowering with GPU splits: \\n ===================== \\n{}'.format(tvm.lower(s, [X, Y, D], simple_mode=True)))\n\n        # compiling the automatically generated cuda code\n        diagonaled_mm = tvm.build(s, [X, Y, Z, D, w, w_upper, padding, transpose_t1, t3d3], target=device, target_host=tgt_host, name='diagonaled_mm')\n        return diagonaled_mm\n\n    @staticmethod\n    def _get_lib_filename(dtype: str, device: str):\n        base_filename = 'longformer/lib/lib_diagonaled_mm'\n        return '{}_{}_{}.so'.format(base_filename, dtype, device)\n\n    @staticmethod\n    def _save_compiled_function(f, dtype: str, device: str):\n        if not os.path.exists('longformer/lib/'):\n            os.makedirs('longformer/lib/')\n        f.export_library(DiagonaledMM._get_lib_filename(dtype, device))\n\n    @staticmethod\n    def _load_compiled_function(dtype: str, device: str):\n        from tvm.module import load  # this can be the small runtime python library, and doesn't need to be the whole thing\n        filename = DiagonaledMM._get_lib_filename(dtype, device)\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        potential_dirs = ['../../', '../', './', f'{current_dir}/', f'{current_dir}/../']\n        for potential_dir in  potential_dirs:\n            filepath = '{}{}'.format(potential_dir, filename)\n            if os.path.isfile(filepath):\n                print('Loading tvm binary from: {}'.format(filepath))\n                return load(filepath)\n        return None\n\n    @staticmethod\n    def _get_function(dtype: str, device: str):\n        '''Loads the function from the disk or compile it'''\n        # A list of arguments that define the function\n        args = (dtype, device)\n        if args not in DiagonaledMM.function_dict:\n            diagonaled_mm = DiagonaledMM._load_compiled_function(dtype, device)  # try to load from disk\n            if not diagonaled_mm:\n                print('Tvm binary not found. Compiling ...')\n                diagonaled_mm = DiagonaledMM._compile_function(dtype, device)  # compile\n                DiagonaledMM._save_compiled_function(diagonaled_mm, dtype, device)  # save to disk\n            # convert the tvm function into a pytorch function\n            from tvm.contrib import dlpack\n            diagonaled_mm_pytorch = dlpack.to_pytorch_func(diagonaled_mm)  # wrap it as a pytorch function\n            # save the function into a dictionary to be reused\n            DiagonaledMM.function_dict[args] = diagonaled_mm_pytorch  # save it in a dictionary for next time\n        return DiagonaledMM.function_dict[args]\n\n    @staticmethod\n    def _diagonaled_mm(t1: torch.Tensor, t2: torch.Tensor, w: int, d: Union[torch.Tensor,int],\n                       is_t1_diagonaled: bool = False, transpose_t1: bool = False, padding: int = 0,\n                       autoregressive: bool = False):\n        '''Calls the compiled function after checking the input format. This function is called in three different modes.\n        t1 x t2 = r ==> t1 and t2 are not diagonaled, but r is. Useful for query x key = attention_scores\n        t1 x t2 = r ==> t1 is diagonaled, but t2 and r are not. Useful to compuate attantion_scores x value = context\n        t1 x t2 = r ==> t1 is diagonaled and it should be transposed, but t2 and r are not diagonaled. Useful in some of\n                            the calculations in the backward pass.\n        '''\n        dtype = str(t1.dtype).split('.')[1]\n        device = t1.device.type\n        assert len(t1.shape) == 4\n        assert len(t1.shape) == len(t2.shape)\n        assert t1.shape[:3] == t2.shape[:3]\n        if isinstance(d, int):  # if d is an integer, replace it with a tensor of the same length\n                                # as number of heads, and it is filled with the same dilation value\n            d = t1.new_full(size=(t1.shape[2],), fill_value=d, dtype=torch.int, requires_grad=False)\n\n        assert len(d.shape) == 1\n        assert d.shape[0] == t1.shape[2]  # number of dilation scores should match number of heads\n        b = t1.shape[0]  # batch size\n        n = t1.shape[1]  # sequence length\n        h = t1.shape[2]  # number of heads\n        m = t2.shape[3]  # hidden dimension\n        w_upper = 0 if autoregressive else w\n        c = w_upper + w + 1  # number of diagonals\n        if is_t1_diagonaled:\n            assert t1.shape[3] == c\n            r = t1.new_empty(b, n, h, m)  # allocate spase for the result tensor\n        else:\n            assert not transpose_t1\n            assert t1.shape[3] == m\n            r = t1.new_empty(b, n, h, c)  # allocate spase for the result tensor\n\n        # gets function from memory, from disk or compiles it from scratch\n        _diagonaled_mm_function = DiagonaledMM._get_function(dtype=dtype, device=device)\n\n        # The last argument to this function is a little hacky. It is the size of the last dimension of the result tensor\n        # We use it as a proxy to tell if t1_is_diagonaled or not (if t1 is diagonaled, result is not, and vice versa).\n        # The second reason is that the lambda expression in `_compile_function` is easier to express when the shape\n        # of the output is known\n        # This functions computes diagonal_mm then saves the result in `r`\n        if m == c:\n            # FIXME\n            print('Error: the hidden dimension {m} shouldn\\'t match number of diagonals {c}')\n            assert False\n        _diagonaled_mm_function(t1, t2, r, d, w, w_upper, padding, transpose_t1, m if is_t1_diagonaled else c)\n        return r\n\n    @staticmethod\n    def _prepare_tensors(t):\n        '''Fix `stride()` information of input tensor. This addresses some inconsistency in stride information in PyTorch.\n        For a tensor t, if t.size(0) == 1, then the value of t.stride()[0] doesn't matter.\n        TVM expects this value to be the `product(t.size()[1:])` but PyTorch some times sets it to `t.stride()[1]`.\n        Here's an example to reporduce this issue:\n            import torch\n            print(torch.randn(1, 10).stride())\n            > (10, 1)\n            print(torch.randn(10, 1).t().contiguous().stride())\n            > (1, 1)  # expected it to be (10, 1) as above\n            print(torch.randn(10, 2).t().contiguous().stride())\n            > (10, 1) # but gets the expected stride if the first dimension is > 1\n        '''\n        assert t.is_contiguous()\n        t_stride = list(t.stride())\n        t_size = list(t.size())\n        # Fix wrong stride information for the first dimension. This occures when batch_size=1\n        if t_size[0] == 1 and t_stride[0] == t_stride[1]:\n            # In this case, the stride of the first dimension should be the product\n            # of the sizes  of all other dimensions\n            t_stride[0] = t_size[1] * t_size[2] * t_size[3]\n            t = t.as_strided(size=t_size, stride=t_stride)\n        return t\n\n    min_seq_len = 16  # unexpected output if seq_len < 16\n\n    @staticmethod\n    def forward(ctx, t1: torch.Tensor, t2: torch.Tensor, w: int, d: Union[torch.Tensor,int], is_t1_diagonaled: bool = False, padding: int = 0, autoregressive: bool = False) -> torch.Tensor:\n        '''Compuates diagonal_mm of t1 and t2.\n        args: \n        t1: torch.Tensor = (batch_size, seq_len, num_attention_heads, hidden_size|number_of_diagonals).\n            t1 can be a regular tensor (e.g. `query_layer`) or a diagonaled one (e.g. `attention_scores`)\n        t2: torch.Tensor = (batch_size, seq_len, num_attention_heads, hidden_size). This is always a non-diagonaled\n            tensor, e.g. `key_layer` or `value_layer`\n        w: int = window size; number of attentions on each side of the word\n        d: torch.Tensor or int = dilation of attentions per attention head. If int, the same dilation value will be used for all\n            heads. If torch.Tensor, it should be 1D of lenth=number of attention heads\n        is_t1_diagonaled: is t1 a diagonaled or a regular tensor\n        padding: the padding value to use when accessing invalid locations. This is mainly useful when the padding\n            needs to be a very large negative value (to compute softmax of attentions). For other usecases,\n            please use zero padding.\n        autoregressive: if true, return only the lower triangle\n        returns: torch.Tensor = (batch_size, seq_len, num_attention_heads, hidden_size|number_of_diagonals)\n            if t1 is diagonaed, result is non-diagonaled, and vice versa\n        '''\n        batch_size, seq_len, num_attention_heads, hidden_size = t1.size()\n        assert seq_len >= DiagonaledMM.min_seq_len, 'avoid splitting errors by using seq_len >= {}'.format(DiagonaledMM.min_seq_len)  # FIXME\n        ctx.save_for_backward(t1, t2)\n        ctx.w = w\n        ctx.d = d\n        ctx.is_t1_diagonaled = is_t1_diagonaled\n        ctx.autoregressive = autoregressive\n        t1 = DiagonaledMM._prepare_tensors(t1)\n        t2 = DiagonaledMM._prepare_tensors(t2)\n        # output = t1.mm(t2)  # what would have been called if this was a regular matmul\n        output = DiagonaledMM._diagonaled_mm(t1, t2, w, d, is_t1_diagonaled=is_t1_diagonaled, padding=padding, autoregressive=autoregressive)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        t1, t2 = ctx.saved_tensors\n        w = ctx.w\n        d = ctx.d\n        is_t1_diagonaled = ctx.is_t1_diagonaled\n        autoregressive = ctx.autoregressive\n        if not grad_output.is_contiguous():\n            grad_output = grad_output.contiguous()  # tvm requires all input tensors to be contiguous\n        grad_output = DiagonaledMM._prepare_tensors(grad_output)\n        t1 = DiagonaledMM._prepare_tensors(t1)\n        t2 = DiagonaledMM._prepare_tensors(t2)\n        # http://cs231n.github.io/optimization-2/\n        # https://pytorch.org/docs/master/notes/extending.html\n        # grad_t1 = grad_output.mm(t2)  # what would have been called if this was a regular matmul\n        grad_t1 = DiagonaledMM._diagonaled_mm(grad_output, t2, w, d, is_t1_diagonaled=not is_t1_diagonaled, autoregressive=autoregressive)\n        # grad_t2 = grad_output.t().mm(t1)  # or `grad_t2 = t1.t().mm(grad_output).t()` because `(AB)^T = B^TA^T`\n        if is_t1_diagonaled:\n            grad_t2 = DiagonaledMM._diagonaled_mm(t1, grad_output, w, d, is_t1_diagonaled=True, transpose_t1=True, autoregressive=autoregressive)\n        else:\n            grad_t2 = DiagonaledMM._diagonaled_mm(grad_output, t1, w, d, is_t1_diagonaled=True, transpose_t1=True, autoregressive=autoregressive)\n        return grad_t1, grad_t2, None, None, None, None, None\n\n\ndef _get_invalid_locations_mask_fixed_dilation(seq_len: int, w: int, d: int):\n    diagonals_list = []\n    for j in range(-d * w, d, d):\n        diagonal_mask = torch.zeros(seq_len, device='cpu', dtype=torch.uint8)\n        diagonal_mask[:-j] = 1\n        diagonals_list.append(diagonal_mask)\n    return torch.stack(diagonals_list, dim=-1)\n\n@lru_cache()\ndef _get_invalid_locations_mask(w: int, d: Union[torch.Tensor,int], autoregressive: bool, device: str):\n    if isinstance(d, int):\n        affected_seq_len = w * d\n        mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n        mask = mask[None, :, None, :]\n    else:\n        affected_seq_len = w * d.max()\n        head_masks = []\n        d_list = d.cpu().numpy().tolist()\n        for d in d_list:\n            one_head_mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n            head_masks.append(one_head_mask)\n        mask = torch.stack(head_masks, dim=-2)\n        mask = mask[None, :, :, :]\n\n    ending_mask = None if autoregressive else mask.flip(dims=(1, 3)).bool().to(device)\n    return affected_seq_len, mask.bool().to(device), ending_mask\n\ndef mask_invalid_locations(input_tensor: torch.Tensor, w: int, d: Union[torch.Tensor, int], autoregressive: bool) -> torch.Tensor:\n    affected_seq_len, beginning_mask, ending_mask = _get_invalid_locations_mask(w, d, autoregressive, input_tensor.device)\n    seq_len = input_tensor.size(1)\n    beginning_input = input_tensor[:, :affected_seq_len, :, :w+1]\n    beginning_mask = beginning_mask[:, :seq_len].expand(beginning_input.size())\n    beginning_input.masked_fill_(beginning_mask, -float('inf'))\n    if not autoregressive:\n        ending_input = input_tensor[:, -affected_seq_len:, :, -(w+1):]\n        ending_mask = ending_mask[:, -seq_len:].expand(ending_input.size())\n        ending_input.masked_fill_(ending_mask, -float('inf'))\n\n\ndiagonaled_mm = DiagonaledMM.apply\n\ndiagonaled_mm_tvm = diagonaled_mm\n# The non-tvm implementation is the default, we don't need to load the kernel at loading time.\n# DiagonaledMM._get_function('float32', 'cuda')\n\n\ndef _skew(x, direction, padding_value):\n    '''Convert diagonals into columns (or columns into diagonals depending on `direction`'''\n    x_padded = F.pad(x, direction, value=padding_value)\n    x_padded = x_padded.view(*x_padded.size()[:-2], x_padded.size(-1), x_padded.size(-2))\n    return x_padded\n\n\ndef _skew2(x, padding_value):\n    '''shift every row 1 step to right converting columns into diagonals'''\n    # X = B x C x M x L\n    B, C, M, L = x.size()\n    x = F.pad(x, (0, M + 1), value=padding_value)  # B x C x M x (L+M+1)\n    x = x.view(B, C, -1)  # B x C x ML+MM+M\n    x = x[:, :, :-M]  # B x C x ML+MM\n    x = x.view(B, C, M, M + L)  # B x C, M x L+M\n    x = x[:, :, :, :-1]\n    return x\n\n\ndef _chunk(x, w):\n    '''convert into overlapping chunkings. Chunk size = 2w, overlap size = w'''\n\n    # non-overlapping chunks of size = 2w\n    x = x.view(x.size(0), x.size(1) // (w * 2), w * 2, x.size(2))\n\n    # use `as_strided` to make the chunks overlap with an overlap size = w\n    chunk_size = list(x.size())\n    chunk_size[1] = chunk_size[1] * 2 - 1\n\n    chunk_stride = list(x.stride())\n    chunk_stride[1] = chunk_stride[1] // 2\n    return x.as_strided(size=chunk_size, stride=chunk_stride)\n\n\ndef sliding_chunks_matmul_qk(q: torch.Tensor, k: torch.Tensor, w: int, padding_value: float):\n    '''Matrix multiplicatio of query x key tensors using with a sliding window attention pattern.\n    This implementation splits the input into overlapping chunks of size 2w (e.g. 512 for pretrained Longformer)\n    with an overlap of size w'''\n    bsz, seqlen, num_heads, head_dim = q.size()\n    assert seqlen % (w * 2) == 0\n    assert q.size() == k.size()\n\n    chunks_count = seqlen // w - 1\n\n    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size w * 2\n    q = q.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n    k = k.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n\n    chunk_q = _chunk(q, w)\n    chunk_k = _chunk(k, w)\n\n    # matrix multipication\n    # bcxd: bsz*num_heads x chunks x 2w x head_dim\n    # bcyd: bsz*num_heads x chunks x 2w x head_dim\n    # bcxy: bsz*num_heads x chunks x 2w x 2w\n    chunk_attn = torch.einsum('bcxd,bcyd->bcxy', (chunk_q, chunk_k))  # multiply\n\n    # convert diagonals into columns\n    diagonal_chunk_attn = _skew(chunk_attn, direction=(0, 0, 0, 1), padding_value=padding_value)\n\n    # allocate space for the overall attention matrix where the chunks are compined. The last dimension\n    # has (w * 2 + 1) columns. The first (w) columns are the w lower triangles (attention from a word to\n    # w previous words). The following column is attention score from each word to itself, then\n    # followed by w columns for the upper triangle.\n\n    diagonal_attn = diagonal_chunk_attn.new_empty((bsz * num_heads, chunks_count + 1, w, w * 2 + 1))\n\n    # copy parts from diagonal_chunk_attn into the compined matrix of attentions\n    # - copying the main diagonal and the upper triangle\n    diagonal_attn[:, :-1, :, w:] = diagonal_chunk_attn[:, :, :w, :w + 1]\n    diagonal_attn[:, -1, :, w:] = diagonal_chunk_attn[:, -1, w:, :w + 1]\n    # - copying the lower triangle\n    diagonal_attn[:, 1:, :, :w] = diagonal_chunk_attn[:, :, - (w + 1):-1, w + 1:]\n    diagonal_attn[:, 0, 1:w, 1:w] = diagonal_chunk_attn[:, 0, :w - 1, 1 - w:]\n\n    # separate bsz and num_heads dimensions again\n    diagonal_attn = diagonal_attn.view(bsz, num_heads, seqlen, 2 * w + 1).transpose(2, 1)\n\n    mask_invalid_locations(diagonal_attn, w, 1, False)\n    return diagonal_attn\n\n\ndef sliding_chunks_matmul_pv(prob: torch.Tensor, v: torch.Tensor, w: int):\n    '''Same as sliding_chunks_matmul_qk but for prob and value tensors. It is expecting the same output\n    format from sliding_chunks_matmul_qk'''\n    bsz, seqlen, num_heads, head_dim = v.size()\n    assert seqlen % (w * 2) == 0\n    assert prob.size()[:3] == v.size()[:3]\n    assert prob.size(3) == 2 * w + 1\n    chunks_count = seqlen // w - 1\n    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size 2w\n    chunk_prob = prob.transpose(1, 2).reshape(bsz * num_heads, seqlen // w, w, 2 * w + 1)\n\n    # group bsz and num_heads dimensions into one\n    v = v.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n\n    # pad seqlen with w at the beginning of the sequence and another w at the end\n    padded_v = F.pad(v, (0, 0, w, w), value=-1)\n\n    # chunk padded_v into chunks of size 3w and an overlap of size w\n    chunk_v_size = (bsz * num_heads, chunks_count + 1, 3 * w, head_dim)\n    chunk_v_stride = padded_v.stride()\n    chunk_v_stride = chunk_v_stride[0], w * chunk_v_stride[1], chunk_v_stride[1], chunk_v_stride[2]\n    chunk_v = padded_v.as_strided(size=chunk_v_size, stride=chunk_v_stride)\n\n    skewed_prob = _skew2(chunk_prob, padding_value=0)\n\n    context = torch.einsum('bcwd,bcdh->bcwh', (skewed_prob, chunk_v))\n    return context.view(bsz, num_heads, seqlen, head_dim).transpose(1, 2)\n\n\n\n# ========= \"sliding_chunks_no_overlap\": alternative implemenation of the sliding window attention =========\n# This implementation uses non-overlapping chunks (or blocks) of size `w` with number of local attention = 3xw\n# To make this implemenation comparable to \"sliding_chunks\" set w such that\n#       w_of_sliding_chunks_no_overlap = w_of_sliding_chunks * 2 / 3\n# For example,\n#    w_of_sliding_chunks = 256 (this is one sided. Total attention size = 512)\n#    w_of_sliding_chunks_no_overlap = 170 (Total attention size = 510)\n# Performance:\n# - Speed: 30% faster than \"sliding_chunks\"\n# - Memory: 95% of the memory usage of \"sliding_chunks\"\n# The windows are asymmetric where number of attention on each side of a token ranges between w to 2w\n# while \"sliding_chunks\" has a symmetric window around each token.\n\n\ndef sliding_chunks_no_overlap_matmul_qk(q: torch.Tensor, k: torch.Tensor, w: int, padding_value: float):\n    bsz, seqlen, num_heads, head_dim = q.size()\n    assert seqlen % w == 0\n    assert q.size() == k.size()\n    # chunk seqlen into non-overlapping chunks of size w\n    chunk_q = q.view(bsz, seqlen // w, w, num_heads, head_dim)\n    chunk_k = k.view(bsz, seqlen // w, w, num_heads, head_dim)\n    chunk_k_expanded = torch.stack((\n        F.pad(chunk_k[:, :-1], (0, 0, 0, 0, 0, 0, 1, 0), value=0.0),\n        chunk_k,\n        F.pad(chunk_k[:, 1:], (0, 0, 0, 0, 0, 0, 0, 1), value=0.0),\n    ), dim=-1)\n    diagonal_attn = torch.einsum('bcxhd,bcyhde->bcxhey', (chunk_q, chunk_k_expanded))  # multiply\n    return diagonal_attn.reshape(bsz, seqlen, num_heads, 3 * w)\n\n\ndef sliding_chunks_no_overlap_matmul_pv(prob: torch.Tensor, v: torch.Tensor, w: int):\n    bsz, seqlen, num_heads, head_dim = v.size()\n    chunk_prob = prob.view(bsz, seqlen // w, w, num_heads, 3, w)\n    chunk_v = v.view(bsz, seqlen // w, w, num_heads, head_dim)\n    chunk_v_extended = torch.stack((\n        F.pad(chunk_v[:, :-1], (0, 0, 0, 0, 0, 0, 1, 0), value=0.0),\n        chunk_v,\n        F.pad(chunk_v[:, 1:], (0, 0, 0, 0, 0, 0, 0, 1), value=0.0),\n    ), dim=-1)\n    context = torch.einsum('bcwhpd,bcdhep->bcwhe', (chunk_prob, chunk_v_extended))\n    return context.reshape(bsz, seqlen, num_heads, head_dim)\n\n\nclass Longformer(RobertaModel):\n    def __init__(self, config):\n        super(Longformer, self).__init__(config)\n        if config.attention_mode == 'n2':\n            pass  # do nothing, use BertSelfAttention instead\n        else:\n            for i, layer in enumerate(self.encoder.layer):\n                layer.attention.self = LongformerSelfAttention(config, layer_id=i)\n\n\nclass LongformerForMaskedLM(RobertaForMaskedLM):\n    def __init__(self, config):\n        super(LongformerForMaskedLM, self).__init__(config)\n        if config.attention_mode == 'n2':\n            pass  # do nothing, use BertSelfAttention instead\n        else:\n            for i, layer in enumerate(self.roberta.encoder.layer):\n                layer.attention.self = LongformerSelfAttention(config, layer_id=i)\n\n\nclass LongformerSelfAttention(nn.Module):\n    def __init__(self, config, layer_id):\n        super(LongformerSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n        self.num_heads = config.num_attention_heads\n        self.head_dim = int(config.hidden_size / config.num_attention_heads)\n        self.embed_dim = config.hidden_size\n\n        self.query = nn.Linear(config.hidden_size, self.embed_dim)\n        self.key = nn.Linear(config.hidden_size, self.embed_dim)\n        self.value = nn.Linear(config.hidden_size, self.embed_dim)\n\n        self.query_global = nn.Linear(config.hidden_size, self.embed_dim)\n        self.key_global = nn.Linear(config.hidden_size, self.embed_dim)\n        self.value_global = nn.Linear(config.hidden_size, self.embed_dim)\n\n        self.dropout = config.attention_probs_dropout_prob\n\n        self.layer_id = layer_id\n        self.attention_window = config.attention_window[self.layer_id]\n        self.attention_dilation = config.attention_dilation[self.layer_id]\n        self.attention_mode = config.attention_mode\n        self.autoregressive = config.autoregressive\n        assert self.attention_window > 0\n        assert self.attention_dilation > 0\n        assert self.attention_mode in ['tvm', 'sliding_chunks', 'sliding_chunks_no_overlap']\n        if self.attention_mode in ['sliding_chunks', 'sliding_chunks_no_overlap']:\n            assert not self.autoregressive  # not supported\n            assert self.attention_dilation == 1  # dilation is not supported\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        output_attentions=False,\n    ):\n        '''\n        The `attention_mask` is changed in `BertModel.forward` from 0, 1, 2 to\n            -ve: no attention\n              0: local attention\n            +ve: global attention\n        '''\n        assert encoder_hidden_states is None, \"`encoder_hidden_states` is not supported and should be None\"\n        assert encoder_attention_mask is None, \"`encoder_attention_mask` is not supported and shiould be None\"\n\n        if attention_mask is not None:\n            attention_mask = attention_mask.squeeze(dim=2).squeeze(dim=1)\n            key_padding_mask = attention_mask < 0\n            extra_attention_mask = attention_mask > 0\n            remove_from_windowed_attention_mask = attention_mask != 0\n\n            num_extra_indices_per_batch = extra_attention_mask.long().sum(dim=1)\n            max_num_extra_indices_per_batch = num_extra_indices_per_batch.max()\n            if max_num_extra_indices_per_batch <= 0:\n                extra_attention_mask = None\n            else:\n                # To support the case of variable number of global attention in the rows of a batch,\n                # we use the following three selection masks to select global attention embeddings\n                # in a 3d tensor and pad it to `max_num_extra_indices_per_batch`\n                # 1) selecting embeddings that correspond to global attention\n                extra_attention_mask_nonzeros = extra_attention_mask.nonzero(as_tuple=True)\n                zero_to_max_range = torch.arange(0, max_num_extra_indices_per_batch,\n                                                 device=num_extra_indices_per_batch.device)\n                # mask indicating which values are actually going to be padding\n                selection_padding_mask = zero_to_max_range < num_extra_indices_per_batch.unsqueeze(dim=-1)\n                # 2) location of the non-padding values in the selected global attention\n                selection_padding_mask_nonzeros = selection_padding_mask.nonzero(as_tuple=True)\n                # 3) location of the padding values in the selected global attention\n                selection_padding_mask_zeros = (selection_padding_mask == 0).nonzero(as_tuple=True)\n        else:\n            remove_from_windowed_attention_mask = None\n            extra_attention_mask = None\n            key_padding_mask = None\n\n        hidden_states = hidden_states.transpose(0, 1)\n        seq_len, bsz, embed_dim = hidden_states.size()\n        assert embed_dim == self.embed_dim\n        q = self.query(hidden_states)\n        k = self.key(hidden_states)\n        v = self.value(hidden_states)\n        q /= math.sqrt(self.head_dim)\n\n        q = q.view(seq_len, bsz, self.num_heads, self.head_dim).transpose(0, 1)\n        k = k.view(seq_len, bsz, self.num_heads, self.head_dim).transpose(0, 1)\n        # attn_weights = (bsz, seq_len, num_heads, window*2+1)\n        if self.attention_mode == 'tvm':\n            q = q.float().contiguous()\n            k = k.float().contiguous()\n            attn_weights = diagonaled_mm_tvm(q, k, self.attention_window, self.attention_dilation, False, 0, False)\n        elif self.attention_mode == \"sliding_chunks\":\n            attn_weights = sliding_chunks_matmul_qk(q, k, self.attention_window, padding_value=0)\n        elif self.attention_mode == \"sliding_chunks_no_overlap\":\n            attn_weights = sliding_chunks_no_overlap_matmul_qk(q, k, self.attention_window, padding_value=0)\n        else:\n            raise False\n        mask_invalid_locations(attn_weights, self.attention_window, self.attention_dilation, False)\n        if remove_from_windowed_attention_mask is not None:\n            # This implementation is fast and takes very little memory because num_heads x hidden_size = 1\n            # from (bsz x seq_len) to (bsz x seq_len x num_heads x hidden_size)\n            remove_from_windowed_attention_mask = remove_from_windowed_attention_mask.unsqueeze(dim=-1).unsqueeze(dim=-1)\n            # cast to float/half then replace 1's with -inf\n            float_mask = remove_from_windowed_attention_mask.type_as(q).masked_fill(remove_from_windowed_attention_mask, -10000.0)\n            repeat_size = 1 if isinstance(self.attention_dilation, int) else len(self.attention_dilation)\n            float_mask = float_mask.repeat(1, 1, repeat_size, 1)\n            ones = float_mask.new_ones(size=float_mask.size())  # tensor of ones\n            # diagonal mask with zeros everywhere and -inf inplace of padding\n            if self.attention_mode == 'tvm':\n                d_mask = diagonaled_mm_tvm(ones, float_mask, self.attention_window, self.attention_dilation, False, 0, False)\n            elif self.attention_mode == \"sliding_chunks\":\n                d_mask = sliding_chunks_matmul_qk(ones, float_mask, self.attention_window, padding_value=0)\n            elif self.attention_mode == \"sliding_chunks_no_overlap\":\n                d_mask = sliding_chunks_no_overlap_matmul_qk(ones, float_mask, self.attention_window, padding_value=0)\n\n            attn_weights += d_mask\n        assert list(attn_weights.size())[:3] == [bsz, seq_len, self.num_heads]\n        assert attn_weights.size(dim=3) in [self.attention_window * 2 + 1, self.attention_window * 3]\n\n        # the extra attention\n        if extra_attention_mask is not None:\n            selected_k = k.new_zeros(bsz, max_num_extra_indices_per_batch, self.num_heads, self.head_dim)\n            selected_k[selection_padding_mask_nonzeros] = k[extra_attention_mask_nonzeros]\n            # (bsz, seq_len, num_heads, max_num_extra_indices_per_batch)\n            selected_attn_weights = torch.einsum('blhd,bshd->blhs', (q, selected_k))\n            selected_attn_weights[selection_padding_mask_zeros[0], :, :, selection_padding_mask_zeros[1]] = -10000\n            # concat to attn_weights\n            # (bsz, seq_len, num_heads, extra attention count + 2*window+1)\n            attn_weights = torch.cat((selected_attn_weights, attn_weights), dim=-1)\n        attn_weights_float = F.softmax(attn_weights, dim=-1, dtype=torch.float32)  # use fp32 for numerical stability\n        if key_padding_mask is not None:\n            # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n            attn_weights_float = torch.masked_fill(attn_weights_float, key_padding_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = F.dropout(attn_weights_float.type_as(attn_weights), p=self.dropout, training=self.training)\n        v = v.view(seq_len, bsz, self.num_heads, self.head_dim).transpose(0, 1)\n        attn = 0\n        if extra_attention_mask is not None:\n            selected_attn_probs = attn_probs.narrow(-1, 0, max_num_extra_indices_per_batch)\n            selected_v = v.new_zeros(bsz, max_num_extra_indices_per_batch, self.num_heads, self.head_dim)\n            selected_v[selection_padding_mask_nonzeros] = v[extra_attention_mask_nonzeros]\n            # use `matmul` because `einsum` crashes sometimes with fp16\n            # attn = torch.einsum('blhs,bshd->blhd', (selected_attn_probs, selected_v))\n            attn = torch.matmul(selected_attn_probs.transpose(1, 2), selected_v.transpose(1, 2).type_as(selected_attn_probs)).transpose(1, 2)\n            attn_probs = attn_probs.narrow(-1, max_num_extra_indices_per_batch, attn_probs.size(-1) - max_num_extra_indices_per_batch).contiguous()\n\n        if self.attention_mode == 'tvm':\n            v = v.float().contiguous()\n            attn += diagonaled_mm_tvm(attn_probs, v, self.attention_window, self.attention_dilation, True, 0, False)\n        elif self.attention_mode == \"sliding_chunks\":\n            attn += sliding_chunks_matmul_pv(attn_probs, v, self.attention_window)\n        elif self.attention_mode == \"sliding_chunks_no_overlap\":\n            attn += sliding_chunks_no_overlap_matmul_pv(attn_probs, v, self.attention_window)\n        else:\n            raise False\n\n        attn = attn.type_as(hidden_states)\n        assert list(attn.size()) == [bsz, seq_len, self.num_heads, self.head_dim]\n        attn = attn.transpose(0, 1).reshape(seq_len, bsz, embed_dim).contiguous()\n\n        # For this case, we'll just recompute the attention for these indices\n        # and overwrite the attn tensor. TODO: remove the redundant computation\n        if extra_attention_mask is not None:\n            selected_hidden_states = hidden_states.new_zeros(max_num_extra_indices_per_batch, bsz, embed_dim)\n            selected_hidden_states[selection_padding_mask_nonzeros[::-1]] = hidden_states[extra_attention_mask_nonzeros[::-1]]\n\n            q = self.query_global(selected_hidden_states)\n            k = self.key_global(hidden_states)\n            v = self.value_global(hidden_states)\n            q /= math.sqrt(self.head_dim)\n\n            q = q.contiguous().view(max_num_extra_indices_per_batch, bsz * self.num_heads, self.head_dim).transpose(0, 1)  # (bsz*self.num_heads, max_num_extra_indices_per_batch, head_dim)\n            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)  # bsz * self.num_heads, seq_len, head_dim)\n            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)  # bsz * self.num_heads, seq_len, head_dim)\n            attn_weights = torch.bmm(q, k.transpose(1, 2))\n            assert list(attn_weights.size()) == [bsz * self.num_heads, max_num_extra_indices_per_batch, seq_len]\n\n            attn_weights = attn_weights.view(bsz, self.num_heads, max_num_extra_indices_per_batch, seq_len)\n            attn_weights[selection_padding_mask_zeros[0], :, selection_padding_mask_zeros[1], :] = -10000.0\n            if key_padding_mask is not None:\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2),\n                    -10000.0,\n                )\n            attn_weights = attn_weights.view(bsz * self.num_heads, max_num_extra_indices_per_batch, seq_len)\n            attn_weights_float = F.softmax(attn_weights, dim=-1, dtype=torch.float32)  # use fp32 for numerical stability\n            attn_probs = F.dropout(attn_weights_float.type_as(attn_weights), p=self.dropout, training=self.training)\n            selected_attn = torch.bmm(attn_probs, v)\n            assert list(selected_attn.size()) == [bsz * self.num_heads, max_num_extra_indices_per_batch, self.head_dim]\n\n            selected_attn_4d = selected_attn.view(bsz, self.num_heads, max_num_extra_indices_per_batch, self.head_dim)\n            nonzero_selected_attn = selected_attn_4d[selection_padding_mask_nonzeros[0], :, selection_padding_mask_nonzeros[1]]\n            attn[extra_attention_mask_nonzeros[::-1]] = nonzero_selected_attn.view(len(selection_padding_mask_nonzeros[0]), -1).type_as(hidden_states)\n\n        context_layer = attn.transpose(0, 1)\n        if output_attentions:\n            if extra_attention_mask is not None:\n                # With global attention, return global attention probabilities only\n                # batch_size x num_heads x max_num_global_attention_tokens x sequence_length\n                # which is the attention weights from tokens with global attention to all tokens\n                # It doesn't not return local attention\n                # In case of variable number of global attantion in the rows of a batch,\n                # attn_weights are padded with -10000.0 attention scores\n                attn_weights = attn_weights.view(bsz, self.num_heads, max_num_extra_indices_per_batch, seq_len)\n            else:\n                # without global attention, return local attention probabilities\n                # batch_size x num_heads x sequence_length x window_size\n                # which is the attention weights of every token attending to its neighbours\n                attn_weights = attn_weights.permute(0, 2, 1, 3)\n        outputs = (context_layer, attn_weights) if output_attentions else (context_layer,)\n        return outputs",
    "description": null,
    "url": null
}