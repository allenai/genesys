{
    "acronym": "gla",
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "seed_ids": [
        "xlstm",
        "based",
        "mamba",
        "hyenadistill",
        "vqtransformer",
        "retnet",
        "rwkv4",
        "tnn",
        "transnormer",
        "flashattn",
        "flash",
        "roformer",
        "rfa",
        "deltanet",
        "lineartransformer",
        "lighdynconv"
    ],
    "s2id": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
    "abstract": "Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.",
    "authors": [
        "Songlin Yang",
        "Bailin Wang",
        "Yikang Shen",
        "Rameswar Panda",
        "Yoon Kim"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.",
    "citationCount": 43,
    "influentialCitationCount": 9,
    "code": "# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n# -*- coding: utf-8 -*-\n\n# \"Gated Linear Attention Transformers with Hardware-Efficient Training\"[https://arxiv.org/abs/2312.06635]\n\nfrom __future__ import annotations\n\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache\n\nfrom fla.modules import FusedRMSNormSwishGate, RMSNorm, ShortConvolution\nfrom fla.ops.gla import chunk_gla, fused_chunk_gla, fused_recurrent_gla\n\n\nclass GatedLinearAttention(nn.Module):\n\n    def __init__(\n        self,\n        mode: str = 'fused_chunk',\n        hidden_size: int = 1024,\n        expand_k: float = 1.0,\n        expand_v: float = 2.0,\n        num_heads: int = 4,\n        use_short_conv: bool = False,\n        conv_size: int = 4,\n        conv_bias: bool = False,\n        share_conv_kernel: bool = False,\n        gate_fn: str = 'swish',\n        layernorm_eps: float = 1e-5,\n        gate_logit_normalizer: int = 16,\n        gate_low_rank_dim: int = 16,\n        clamp_min: Optional[float] = None,\n        fuse_norm: bool = True,\n        layer_idx: int = None,\n        **kwargs\n    ) -> GatedLinearAttention:\n        super().__init__()\n\n        self.mode = mode\n        self.hidden_size = hidden_size\n        self.expand_k = expand_k\n        self.expand_v = expand_v\n        self.num_heads = num_heads\n\n        self.use_short_conv = use_short_conv\n        self.conv_size = conv_size\n        self.conv_bias = conv_bias\n        self.share_conv_kernel = share_conv_kernel\n\n        self.key_dim = int(hidden_size * expand_k)\n        self.value_dim = int(hidden_size * expand_v)\n        self.clamp_min = clamp_min\n        self.layer_idx = layer_idx\n\n        assert mode in ['chunk', 'fused_recurrent', 'fused_chunk'], f\"Not suppoerted mode `{mode}`.\"\n        assert self.key_dim % num_heads == 0, f\"key dim must be divisible by num_heads of {num_heads}\"\n        assert self.value_dim % num_heads == 0, f\"value dim must be divisible by num_heads of {num_heads}\"\n\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n\n        if use_short_conv:\n            self.conv_size = conv_size\n            if share_conv_kernel:\n                self.h_conv1d = ShortConvolution(hidden_size, conv_size, activation='silu')\n            else:\n                self.q_conv1d = ShortConvolution(self.key_dim, conv_size, activation='silu')\n                self.k_conv1d = ShortConvolution(self.key_dim, conv_size, activation='silu')\n                self.v_conv1d = ShortConvolution(self.value_dim, conv_size, activation='silu')\n\n        self.gk_proj = nn.Sequential(nn.Linear(hidden_size, gate_low_rank_dim, bias=False),\n                                     nn.Linear(gate_low_rank_dim, self.key_dim, bias=True))\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False)\n\n        if gate_fn == 'swish' and fuse_norm:\n            self.g_norm_swish_gate = FusedRMSNormSwishGate(self.head_v_dim, eps=layernorm_eps)\n            self.fuse_norm_and_gate = True\n        else:\n            self.fuse_norm_and_gate = False\n            self.g_norm = RMSNorm(self.head_v_dim, eps=layernorm_eps)\n            self.gate_fn = ACT2FN[gate_fn]\n\n        self.gate_logit_normalizer = gate_logit_normalizer\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[Cache] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        **kwargs\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n        # launching the triton kernel for just one token will actually be slower\n        mode = 'fused_recurrent' if hidden_states.shape[1] == 1 else self.mode\n\n        last_state = past_key_values[self.layer_idx] if use_cache else None\n        if self.use_short_conv:\n            conv_state = last_state[0] if use_cache else None\n            if self.share_conv_kernel:\n                # conv state is updated inplace\n                hidden_states = self.h_conv1d(hidden_states, attention_mask, conv_state)\n                q = self.q_proj(hidden_states)\n                k = self.k_proj(hidden_states)\n                v = self.v_proj(hidden_states)\n            else:\n                conv_state_q = last_state[0] if use_cache else None\n                conv_state_k = last_state[1] if use_cache else None\n                conv_state_v = last_state[2] if use_cache else None\n                q = self.q_proj(hidden_states)\n                k = self.k_proj(hidden_states)\n                v = self.v_proj(hidden_states)\n                q = self.q_conv1d(q, attention_mask, conv_state_q)\n                k = self.k_conv1d(k, attention_mask, conv_state_k)\n                v = self.v_conv1d(v, attention_mask, conv_state_v)\n        else:\n            q = self.q_proj(hidden_states)\n            k = self.k_proj(hidden_states)\n            v = self.v_proj(hidden_states)\n\n        # dealing with left-padding\n        if attention_mask is not None:\n            v = v.mul_(attention_mask.unsqueeze(-1))\n        q, k, v = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=self.num_heads), (q, k, v))\n        gk = rearrange(self.gk_proj(hidden_states), 'b n (h d) -> b h n d', h=self.num_heads)\n        gk = F.logsigmoid(gk) / self.gate_logit_normalizer\n\n        if self.clamp_min is not None:\n            gk = torch.clamp_min(gk, self.clamp_min)\n\n        recurrent_state = last_state[-1] if use_cache else None\n        if mode == 'fused_recurrent':\n            o, recurrent_state = fused_recurrent_gla(q, k, v, gk, initial_state=recurrent_state, output_final_state=use_cache)\n        elif mode == 'fused_chunk':\n            o, recurrent_state = fused_chunk_gla(q, k, v, gk, initial_state=recurrent_state, output_final_state=use_cache)\n        elif mode == 'chunk':\n            o, recurrent_state = chunk_gla(q, k, v, gk, initial_state=recurrent_state, output_final_state=use_cache)\n        else:\n            raise NotImplementedError(f\"Not supported mode `{mode}`.\")\n\n        if past_key_values is not None:\n            if self.use_short_conv:\n                if self.share_conv_kernel:\n                    last_state = (conv_state, recurrent_state)\n                else:\n                    last_state = (conv_state_q, conv_state_k, conv_state_v, recurrent_state)\n            else:\n                last_state = (recurrent_state,)\n            past_key_values.update(last_state, self.layer_idx, q.shape[2])\n\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(hidden_states)\n        if self.fuse_norm_and_gate:\n            g = rearrange(g, 'b l (h d) -> b l h d', h=self.num_heads)\n            o = self.g_norm_swish_gate(o, g)\n            o = rearrange(o, 'b l h d -> b l (h d)')\n        else:\n            o = self.g_norm(o)\n            o = rearrange(o, 'b l h d -> b l (h d)')\n            o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n\n        return o, None, past_key_values\n\n    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n        param = next(self.parameters())\n        state = tuple()\n        if self.use_short_conv:\n            if self.share_conv_kernel:\n                state += (param.new_zeros(batch_size, self.hidden_size, self.conv_size),)\n            else:\n                state += (param.new_zeros(batch_size, self.key_dim, self.conv_size),\n                          param.new_zeros(batch_size, self.key_dim, self.conv_size),\n                          param.new_zeros(batch_size, self.value_dim, self.conv_size))\n        state += (param.new_zeros(batch_size, self.num_heads, self.head_qk_dim, self.head_v_dim),)\n        return state\n\n    def state_size(self, **kwargs) -> int:\n        state_size = self.key_dim * self.head_v_dim\n        for module in self.children():\n            if isinstance(module, ShortConvolution):\n                state_size += module.state_size\n        return state_size",
    "description": null,
    "url": null
}