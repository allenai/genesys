{
    "acronym": "mentaltimetravel",
    "title": "Towards mental time travel: a hierarchical memory for reinforcement learning agents",
    "seed_ids": [
        "transformerxl",
        "transformer"
    ],
    "s2id": "008cfd24dfdfb28fc6a89c32772c7ffe5cb0cf8a",
    "abstract": "Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore\"mentally time-travel\"-- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.",
    "authors": [
        "Andrew Kyle Lampinen",
        "Stephanie C. Y. Chan",
        "Andrea Banino",
        "Felix Hill"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": "Hierarchical Chunk Attention Memory improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures), and is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.",
    "citationCount": 38,
    "influentialCitationCount": 4,
    "code": "from math import ceil\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef pad_to_multiple(t, multiple, dim = -2, value = 0.):\n    seq_len = t.shape[dim]\n    pad_to_len = ceil(seq_len / multiple) * multiple\n    remainder = pad_to_len - seq_len\n\n    if remainder == 0:\n        return t\n\n    zeroes = (0, 0) * (-dim - 1)\n    padded_t = F.pad(t, (*zeroes, remainder, 0), value = value)\n    return padded_t\n\n# positional encoding\n\nclass SinusoidalPosition(nn.Module):\n    def __init__(\n        self,\n        dim,\n        min_timescale = 2.,\n        max_timescale = 1e4\n    ):\n        super().__init__()\n        freqs = torch.arange(0, dim, min_timescale)\n        inv_freqs = max_timescale ** (-freqs / dim)\n        self.register_buffer('inv_freqs', inv_freqs)\n\n    def forward(self, x):\n        seq_len = x.shape[-2]\n        seq = torch.arange(seq_len - 1, -1, -1.)\n        sinusoidal_inp = rearrange(seq, 'n -> n ()') * rearrange(self.inv_freqs, 'd -> () d')\n        pos_emb = torch.cat((sinusoidal_inp.sin(), sinusoidal_inp.cos()), dim = -1)\n        return pos_emb\n\n# multi-head attention\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 64,\n        heads = 8,\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n    def forward(\n        self,\n        x,\n        mems,\n        mask = None\n    ):\n        h = self.heads\n        q, k, v = self.to_q(x), *self.to_kv(mems).chunk(2, dim = -1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b ... (h d) -> (b h) ... d', h = h), (q, k, v))\n        q = q * self.scale\n\n        sim = einsum('b m i d, b m i j d -> b m i j', q, k)\n\n        if exists(mask):\n            mask = repeat(mask, 'b ... -> (b h) ...', h = h)\n            mask_value = -torch.finfo(sim.dtype).max\n            sim = sim.masked_fill(~mask, mask_value)\n\n        attn = sim.softmax(dim = -1)\n\n        out = einsum('... i j, ... i j d -> ... i d', attn, v)\n        out = rearrange(out, '(b h) ... d -> b ... (h d)', h = h)\n        return self.to_out(out)\n\n# main class\n\nclass HTMAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        heads,\n        topk_mems = 2,\n        mem_chunk_size = 32,\n        dim_head = 64,\n        add_pos_enc = True,\n        eps = 1e-5\n    ):\n        super().__init__()\n        self.dim = dim\n        self.eps = eps\n        self.scale = dim ** -0.5\n\n        self.to_summary_queries = nn.Linear(dim, dim)\n        self.to_summary_keys = nn.Linear(dim, dim)\n\n        self.attn = Attention(dim = dim, heads = heads, dim_head = dim_head)\n\n        self.topk_mems = topk_mems\n        self.mem_chunk_size = mem_chunk_size\n        self.pos_emb = SinusoidalPosition(dim = dim) if add_pos_enc else None\n\n    def forward(\n        self,\n        queries,\n        memories,\n        mask = None,\n        chunk_attn_mask = None\n    ):\n        dim, query_len, mem_chunk_size, topk_mems, scale, eps = self.dim, queries.shape[1], self.mem_chunk_size, self.topk_mems, self.scale, self.eps\n\n        # pad memories, and the memory mask, if needed\n        # and then divide into chunks\n\n        memories = pad_to_multiple(memories, mem_chunk_size, dim = -2, value = 0.)\n        memories = rearrange(memories, 'b (n c) d -> b n c d', c = mem_chunk_size)\n\n        if exists(mask):\n            mask = pad_to_multiple(mask, mem_chunk_size, dim = -1, value = False)\n            mask = rearrange(mask, 'b (n c) -> b n c', c = mem_chunk_size)\n\n        # summarize memories through mean-pool, accounting for mask\n\n        if exists(mask):\n            mean_mask = rearrange(mask, '... -> ... ()')\n            memories = memories.masked_fill(~mean_mask, 0.)\n            numer = memories.sum(dim = 2)\n            denom = mean_mask.sum(dim = 2)\n            summarized_memories = numer / (denom + eps)\n        else:\n            summarized_memories = memories.mean(dim = 2)\n\n        # derive queries and summarized memory keys\n\n        summary_queries = self.to_summary_queries(queries)\n        summary_keys = self.to_summary_keys(summarized_memories.detach())\n\n        # do a single head attention over summary keys\n\n        sim = einsum('b i d, b j d -> b i j', summary_queries, summary_keys) * scale\n        mask_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            chunk_mask = mask.any(dim = 2)\n            chunk_mask = rearrange(chunk_mask, 'b j -> b () j')\n            sim = sim.masked_fill(~chunk_mask, mask_value)\n\n        if exists(chunk_attn_mask):\n            sim = sim.masked_fill(~chunk_attn_mask, mask_value)\n\n        topk_logits, topk_indices = sim.topk(k = topk_mems, dim = -1)\n        weights = topk_logits.softmax(dim = -1)\n\n        # ready queries for in-memory attention\n\n        queries = repeat(queries, 'b n d -> b k n d', k = topk_mems)\n\n        # select the topk memories\n\n        memories = repeat(memories, 'b m j d -> b m i j d', i = query_len)\n        mem_topk_indices = repeat(topk_indices, 'b i m -> b m i j d', j = mem_chunk_size, d = dim)\n        selected_memories = memories.gather(1, mem_topk_indices)\n\n        # positional encoding\n\n        if exists(self.pos_emb):\n            pos_emb = self.pos_emb(memories)\n            selected_memories = selected_memories + rearrange(pos_emb, 'n d -> () () () n d')\n\n        # select the mask\n\n        selected_mask = None\n        if exists(mask):\n            mask = repeat(mask, 'b m j -> b m i j', i = query_len)\n            mask_topk_indices = repeat(topk_indices, 'b i m -> b m i j', j = mem_chunk_size)\n            selected_mask = mask.gather(1, mask_topk_indices)\n\n        # now do in-memory attention\n\n        within_mem_output = self.attn(\n            queries,\n            selected_memories.detach(),\n            mask = selected_mask\n        )\n\n        # weight the in-memory attention outputs\n\n        weighted_output = within_mem_output * rearrange(weights, 'b i m -> b m i ()')\n        output = weighted_output.sum(dim = 1)\n        return output\n\n# HTM Block\n\nclass HTMBlock(nn.Module):\n    def __init__(self, dim, **kwargs):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.attn = HTMAttention(dim = dim, **kwargs)\n    def forward(\n        self,\n        queries,\n        memories,\n        **kwargs\n    ):\n        queries = self.norm(queries)\n        out = self.attn(queries, memories, **kwargs) + queries\n        return out",
    "description": null,
    "url": null
}