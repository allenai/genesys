{
    "acronym": "aft",
    "title": "An Attention Free Transformer",
    "seed_ids": [
        "performer",
        "lineartransformer",
        "synthesizer",
        "routingtransformer",
        "sinkhorn",
        "reformer",
        "sparsetransformer",
        "lighdynconv",
        "transformerxl",
        "transformer"
    ],
    "s2id": "d5e999aae76d5270ef272076979c809817458212",
    "abstract": "We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.",
    "authors": [
        "Shuangfei Zhai",
        "Walter A. Talbott",
        "Nitish Srivastava",
        "Chen Huang",
        "Hanlin Goh",
        "Ruixiang Zhang",
        "J. Susskind"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": "Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention, is introduced and demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.",
    "citationCount": 90,
    "influentialCitationCount": 10,
    "code": "import torch, math\nfrom torch import nn, einsum\nimport torch.nn.functional as F    \n\nclass AFTFull(nn.Module):\n    def __init__(self, max_seqlen, dim, hidden_dim=64):\n        super().__init__()\n        '''\n        max_seqlen: the maximum number of timesteps (sequence length) to be fed in\n        dim: the embedding dimension of the tokens\n        hidden_dim: the hidden dimension used inside AFT Full\n\n        Number of heads is 1 as done in the paper\n        '''\n        self.dim = dim\n        self.hidden_dim = hidden_dim\n        self.to_q = nn.Linear(dim, hidden_dim)\n        self.to_k = nn.Linear(dim, hidden_dim)\n        self.to_v = nn.Linear(dim, hidden_dim)\n        self.project = nn.Linear(hidden_dim, dim)\n        self.wbias = nn.Parameter(torch.Tensor(max_seqlen, max_seqlen))\n        nn.init.xavier_uniform_(self.wbias)\n\n    def forward(self, x):\n        B, T, _ = x.shape\n        Q = self.to_q(x).view(B, T, self.hidden_dim)\n        K = self.to_k(x).view(B, T, self.hidden_dim)\n        V = self.to_v(x).view(B, T, self.hidden_dim)\n        temp_wbias = self.wbias[:T, :T].unsqueeze(0) # sequences can still be variable length\n\n        '''\n        From the paper\n        '''\n        Q_sig = torch.sigmoid(Q)\n        temp = torch.exp(temp_wbias) @ torch.mul(torch.exp(K), V)\n        weighted = temp / (torch.exp(temp_wbias) @ torch.exp(K))\n        Yt = torch.mul(Q_sig, weighted)\n\n        Yt = Yt.view(B, T, self.hidden_dim)\n        Yt = self.project(Yt)\n\n        return Yt\n\nclass AFTSimple(nn.Module):\n    def __init__(self, max_seqlen, dim, hidden_dim=64):\n        super().__init__()\n        '''\n        max_seqlen: the maximum number of timesteps (sequence length) to be fed in\n        dim: the embedding dimension of the tokens\n        hidden_dim: the hidden dimension used inside AFT Full\n        \n        Number of Heads is 1 as done in the paper.\n        '''\n        self.dim = dim\n        self.hidden_dim = hidden_dim\n        self.to_q = nn.Linear(dim, hidden_dim)\n        self.to_k = nn.Linear(dim, hidden_dim)\n        self.to_v = nn.Linear(dim, hidden_dim)\n        self.project = nn.Linear(hidden_dim, dim)\n\n    def forward(self, x):\n        B, T, _ = x.shape\n        Q = self.to_q(x).view(B, T, self.hidden_dim)\n        K = self.to_k(x).view(B, T, self.hidden_dim)\n        V = self.to_v(x).view(B, T, self.hidden_dim)\n\n        '''\n        From the paper\n        '''\n        weights = torch.mul(torch.softmax(K, 1), V).sum(dim=1, keepdim=True)\n        Q_sig = torch.sigmoid(Q)\n        Yt = torch.mul(Q_sig, weights)\n\n        Yt = Yt.view(B, T, self.hidden_dim)\n        Yt = self.project(Yt)\n\n        return Yt\n\nclass AFTLocal(nn.Module):\n    def __init__(self, max_seqlen, dim, hidden_dim=64, s=256):\n        super().__init__()\n        '''\n        max_seqlen: the maximum number of timesteps (sequence length) to be fed in\n        dim: the embedding dimension of the tokens\n        hidden_dim: the hidden dimension used inside AFT Full\n        s: the window size used for AFT-Local in the paper\n\n        Number of heads is 1 as done in the paper\n        '''\n        self.dim = dim\n        self.hidden_dim = hidden_dim\n        self.to_q = nn.Linear(dim, hidden_dim)\n        self.to_k = nn.Linear(dim, hidden_dim)\n        self.to_v = nn.Linear(dim, hidden_dim)\n        self.project = nn.Linear(hidden_dim, dim)\n        self.wbias = nn.Parameter(torch.Tensor(max_seqlen, max_seqlen))\n        self.max_seqlen = max_seqlen\n        self.s = s\n        nn.init.xavier_uniform_(self.wbias)\n\n\n    def forward(self, x):\n        B, T, _ = x.shape\n        Q = self.to_q(x).view(B, T, self.hidden_dim)\n        K = self.to_k(x).view(B, T, self.hidden_dim)\n        V = self.to_v(x).view(B, T, self.hidden_dim)\n        self.wbias = nn.Parameter(torch.Tensor([\n            [self.wbias[i][j] if math.fabs(i-j) < self.s else 0 for j in range(self.max_seqlen)] \n            for i in range(self.max_seqlen)\n            ]))\n        temp_wbias = self.wbias[:T, :T].unsqueeze(0) # sequences can still be variable length\n\n        '''\n        From the paper\n        '''\n        Q_sig = torch.sigmoid(Q)\n        temp = torch.exp(temp_wbias) @ torch.mul(torch.exp(K), V)\n        weighted = temp / (torch.exp(temp_wbias) @ torch.exp(K))\n        Yt = torch.mul(Q_sig, weighted)\n\n        Yt = Yt.view(B, T, self.hidden_dim)\n        Yt = self.project(Yt)\n\n        return Yt\n\nclass AFTConv(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        raise NotImplementedError\n\n'''\nTaken from the PyTorch docs for Positional Embeddings only\n'''\n# class PositionalEncoding(nn.Module):\n#     def __init__(self, dim, p=0.1, max_len=5000):\n#         super(PositionalEncoding, self).__init__()\n#         self.dropout = nn.Dropout(p=p)\n\n#         pe = torch.zeros(max_len, dim)\n#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n#         div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n#         pe[:, 0::2] = torch.sin(position * div_term)\n#         pe[:, 1::2] = torch.cos(position * div_term)\n#         pe = pe.unsqueeze(0).transpose(0, 1)\n#         self.register_buffer('pe', pe)\n\n#     def forward(self, x):\n#         x = x + self.pe[:x.size(0), :]\n#         return self.dropout(x)\n\n# class MLP(nn.Module):\n#     def __init__(self, dim, hidden_dim, dp=0.1):\n#         super().__init__()\n#         self.l1 = nn.Linear(dim, hidden_dim)\n#         self.g1 = nn.GELU()\n#         self.l2 = nn.Linear(hidden_dim, dim)\n#         self.d1 = nn.Dropout(dp)\n\n#     def forward(self, x):\n#         x = self.l1(x)\n#         x = self.g1(x)\n#         x = self.d1(x)\n#         out = self.l2(x)\n\n#         return out        \n\n# class AFTEncoderBlock(nn.Module):\n#     def __init__(self, dim, hidden_dim, p=0.1):\n#         super().__init__()\n#         self.ln = nn.LayerNorm(dim)\n#         self.attn = AFTFull(dim, hidden_dim)\n#         self.mlp = MLP(dim, hidden_dim)\n#         self.d1 = nn.Dropout(p)\n#         self.d2 = nn.Dropout(p)\n\n#     def forward(self, x):\n#         x = self.ln(x)\n#         x = self.attn(x) + x\n#         x = self.d1(x)\n#         x = self.ln(x)\n#         x = self.mlp(x)\n#         out = self.d2(x) + x\n\n#         return out\n\n# class AFTDecoderBlock(nn.Module):\n#     def __init__(self, dim, hidden_dim, p=0.1):\n#         super().__init__()\n#         self.ln = nn.LayerNorm(dim)\n#         self.attn1 = AFTFull(dim, hidden_dim)\n#         self.attn2 = AFTFull(dim, hidden_dim)\n#         self.mlp = MLP(dim, hidden_dim, p=p)\n#         self.d1 = nn.Dropout(p)\n#         self.d2 = nn.Dropout(p)\n#         self.d3 = nn.Dropout(p)\n\n#     def forward(self, x):\n#         x = self.ln(x)\n#         x = self.attn1(x)\n#         x = self.d1(x) + x\n#         x = self.ln(x)\n#         x = self.attn2(x)\n#         x = self.dropout_2(x) + x\n#         x = self.ln(x)\n#         x = self.mlp(x)\n#         out = self.d3(x) + x\n\n#         return out\n\n# class AFT(nn.Module):\n#     def __init__(self, vocab_size, dim, hidden_dim, enc=None, dec=None, depth=6, p=0.1):\n#         super().__init__()\n#         self.layers = nn.ModuleList()\n#         self.pos_embed = PositionalEncoding(dim, p=p)\n#         self.embed = nn.Embedding(vocab_size, dim)\n#         self.enc = enc if enc else nn.ModuleList([AFTEncoderBlock(dim, hidden_dim) for _ in range(depth)])\n#         # self.dec = nn.ModuleList([AFTDecoderBlock(dim, hidden_dim) for _ in range(depth)])\n#         self.dec = dec if dec else nn.Linear(dim, vocab_size)\n#         self.dim = dim\n\n#     def forward(self, x):\n#         x = self.embed(x) * math.sqrt(self.dim)\n#         print (x.shape)\n#         x = self.pos_embed(x)\n#         x = self.enc(x)\n#         out = self.dec(x)\n\n#         return out",
    "description": null,
    "url": null
}