{
    "acronym": "lengenewope",
    "title": "Length Generalization of Causal Transformers without Position Encoding",
    "seed_ids": [
        "pi",
        "alibi",
        "roformer",
        "compressivetransformer"
    ],
    "s2id": "37b1ce339678f63315c82841c6824dd739269636",
    "abstract": "Generalizing to longer sentences is important for recent Transformer-based language models. Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge. In this paper, we study the length generalization property of NoPE. We find that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length. We identify a connection between the failure of NoPE's generalization and the distraction of attention distributions. We propose a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE's context size. Experiments on long sequence language modeling, the synthetic passkey retrieval task and real-world long context tasks show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms. The source code is publicly accessible",
    "authors": [
        "Jie Wang",
        "Tao Ji",
        "Yuanbin Wu",
        "Hang Yan",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Xiaoling Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This paper finds that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length, and proposes a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE's context size.",
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": "import logging\nimport math\nimport warnings\nfrom functools import partialmethod\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import PreTrainedModel\nfrom transformers.models.llama import modeling_llama\n\nimport utils\nfrom args.model_args import FreezeType, ModelArguments, SoftMaxScaleType\n\nlogger = logging.getLogger(__name__)\n\norigin_apply_rotary_pos_emb = modeling_llama.apply_rotary_pos_emb\nOriginLlamaAttention = modeling_llama.LlamaAttention\nOriginLlamaFlashAttention2 = modeling_llama.LlamaFlashAttention2\n\n\ndef monkey_patch_before(args: ModelArguments):\n    utils.log_once(logger, f\"monkey patch before {args}\")\n    if args.nope:\n        modeling_llama.apply_rotary_pos_emb = nope_monkey_patch\n    else:\n        # logger.warning(\"Skipping monkey patch for RoPE model\")\n        # return\n        pass\n    if args.use_flash_attention:\n        modeling_llama.LlamaAttention.forward = utils.forbidden_func  # must use flash attention\n        LlamaFlashAttention2MonkeyPatch.__init__ = partialmethod(\n            LlamaFlashAttention2MonkeyPatch.__init__,\n            softmax_scale=args.softmax_scale,\n            softmax_scale_type=args.softmax_scale_type,\n        )  # type: ignore\n        modeling_llama.LlamaFlashAttention2 = LlamaFlashAttention2MonkeyPatch\n    else:  # output_attentions=True and entropy=True\n        modeling_llama.LlamaFlashAttention2.forward = utils.forbidden_func\n        LlamaAttentionMonkeyPatch.__init__ = partialmethod(\n            LlamaAttentionMonkeyPatch.__init__,\n            softmax_scale=args.softmax_scale,\n            softmax_scale_type=args.softmax_scale_type,\n        )  # type: ignore\n        modeling_llama.LlamaAttention = LlamaAttentionMonkeyPatch\n\n\ndef monkey_patch_after(model: PreTrainedModel, args: ModelArguments):\n    if args.yarn is not None:\n        assert isinstance(model, modeling_llama.LlamaForCausalLM)\n        from .LlamaYaRNScaledRotaryEmbedding import (\n            patch_llama_for_yarn_scaled_rotary_embeddings,\n        )\n\n        patch_llama_for_yarn_scaled_rotary_embeddings(model, args.yarn, 2048)\n\n\ndef prepare_for_training(model: PreTrainedModel, args: ModelArguments):\n    assert isinstance(model, modeling_llama.LlamaForCausalLM)\n    assert isinstance(model.config, modeling_llama.LlamaConfig)\n\n    if not args.yarn:\n        # yarn patches rope, cannot assert yarn\n        _assert_rope_type(type(model.model.layers[0].self_attn.rotary_emb), args.scale_type)  # type: ignore\n    if args.freeze_type == FreezeType.BASE:\n        if hasattr(model.model.layers[0].self_attn, \"scale_param\"):\n            # freeze model except scale_param\n            model.requires_grad_(False)  # freeze everything\n            for i in range(model.config.num_hidden_layers):\n                model.model.layers[i].self_attn.scale_param.requires_grad_(True)  # type: ignore\n\n\ndef _assert_rope_type(rope_type: type, scale_type: Optional[str]):\n    logger.info(f\"RoPE module: {rope_type}\")  # check rotary type\n    if scale_type is None:\n        assert rope_type is modeling_llama.LlamaRotaryEmbedding\n    elif scale_type == \"linear\":\n        assert rope_type is modeling_llama.LlamaLinearScalingRotaryEmbedding\n    elif scale_type == \"dynamic\":\n        assert rope_type is modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding\n    else:\n        assert False, f\"unknown scale type {scale_type}\"\n\n\ndef nope_monkey_patch(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n    utils.log_once(logger, f\"monkey patching rope to nope\")\n    return q, k\n\n\nclass ScaleMixin(OriginLlamaAttention):\n    def __init__(\n        self,\n        config,\n        softmax_scale: float,\n        softmax_scale_b: float,\n        softmax_scale_type: SoftMaxScaleType,\n        QNA: bool,\n        KNA: bool,\n        window_attn: Optional[int],\n    ):\n        super().__init__(config)\n        self.softmax_scale = softmax_scale\n        self.softmax_scale_type = softmax_scale_type\n        self.QNA = QNA\n        self.KNA = KNA\n        self.window_attn = window_attn\n        if self.softmax_scale_type == SoftMaxScaleType.HS:\n            init_value = softmax_scale\n            self.scale_param = torch.nn.Parameter(torch.full((self.num_heads,), init_value, dtype=torch.bfloat16))\n\n    def query_scale(\n        self,\n        query_states: torch.Tensor,\n        position_ids: Optional[torch.Tensor],\n    ):\n        utils.log_once(\n            logger,\n            f\"monkey patching llama attention: {self.softmax_scale=} {self.softmax_scale_type=}\",\n        )\n        bsz, num_heads, q_len, head_dim = query_states.shape\n        assert num_heads == self.num_heads and head_dim == self.head_dim\n        assert position_ids is not None\n        assert position_ids.shape == (bsz, q_len) or position_ids.shape == (1, q_len), position_ids.shape\n        # scale_factor = scale / math.sqrt(head_dim)\n        # return query_states * scale_factor.unsqueeze(-1).to(query_states.dtype).to(query_states.device)\n        if self.softmax_scale_type == SoftMaxScaleType.CONST:\n            scale_factor = self.softmax_scale / math.sqrt(head_dim)\n        elif self.softmax_scale_type == SoftMaxScaleType.HS:\n            assert head_dim == 64\n            origin_scale = 1.0 / math.sqrt(head_dim)\n            scale_factor = self.scale_param * origin_scale\n            scale_factor = scale_factor.unsqueeze(-1).unsqueeze(-1).to(query_states.dtype)\n        else:\n            assert False, f\"unknown softmax scale type {self.softmax_scale_type}\"\n        query_states = query_states * scale_factor\n        assert query_states.shape == (bsz, num_heads, q_len, head_dim), query_states.shape\n        return query_states\n\n\nclass LlamaAttentionMonkeyPatch(ScaleMixin):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        utils.log_once(logger, \"Using normal attention\")\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = modeling_llama.apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)  # type: ignore\n\n        past_key_value = (key_states, value_states) if use_cache else None  # type: ignore\n\n        ##########\n        query_states = self.query_scale(query_states, position_ids)\n        ##########\n\n        key_states = modeling_llama.repeat_kv(key_states, self.num_key_value_groups)\n        value_states = modeling_llama.repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3))  # scaled in query scale\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        ##########\n        attn_weights = _calc_entropy(attn_weights)  # overwrite to entropy\n        ##########\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value  # type: ignore\n\n\nclass LlamaFlashAttention2MonkeyPatch(OriginLlamaFlashAttention2, ScaleMixin):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.count = 0\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        utils.log_once(logger, \"Using flash attention\")\n        # LlamaFlashAttention2 attention does not support output_attentions\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n\n            # overwrite attention_mask with padding_mask\n            attention_mask = kwargs.pop(\"padding_mask\")\n\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\n        query_states, key_states = modeling_llama.apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)  # type: ignore\n\n        past_key_value = (key_states, value_states) if use_cache else None  # type: ignore\n\n        ##########\n        query_states = self.query_scale(query_states, position_ids)\n        ##########\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        # TODO: llama does not have dropout in the config??\n        # It is recommended to use dropout with FA according to the docs\n        # when training.\n        dropout_rate = 0.0  # if not self.training else self.attn_dropout\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            # Handle the case where the model is quantized\n            if hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(  # type: ignore\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            q_len,\n            dropout=dropout_rate,\n            ##########\n            softmax_scale=1,  # scaled in query scale\n            ##########\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n@torch.no_grad()\ndef _calc_entropy(attn: torch.Tensor):\n    # tensor: [bs, n_heads, seq_len, seq_len]\n    # assert torch.sum(attn, dim=-1, keepdim=True).allclose(torch.ones_like(attn, dtype=torch.float32), atol=5e-4)\n    info = attn * torch.log2(attn)\n    info = torch.where(torch.isnan(info), torch.zeros_like(info), info)  # nan comes from 0 * log(0), which should be 0\n    entropy = -torch.sum(info, dim=-1)\n    return entropy  # [bs, n_heads, seq_len]",
    "description": null,
    "url": null
}