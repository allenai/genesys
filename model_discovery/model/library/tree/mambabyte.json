{
    "acronym": "mambabyte",
    "title": "MambaByte: Token-free Selective State Space Model",
    "seed_ids": [
        "mamba",
        "megabyte",
        "ptwoattn",
        "s5",
        "gssm",
        "dssm",
        "hierarchitrans",
        "roformer",
        "funneltransformer",
        "compressivetransformer",
        "transformer"
    ],
    "s2id": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
    "abstract": "Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.",
    "authors": [
        "Junxiong Wang",
        "Tushaar Gangavarapu",
        "Jing Nathan Yan",
        "Alexander M. Rush"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work proposes MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences, and develops an adaptation of speculative decoding with tokenized drafting and byte-level verification, establishing the viability of SSMs in enabling token-free language modeling.",
    "citationCount": 18,
    "influentialCitationCount": 1,
    "code": "import math\nfrom dataclasses import dataclass\nfrom typing import Union\n\nimport torch\nfrom torch import nn \nimport torch.nn.functional as F\n\n\"\"\"\n\nAn implementation of the parallel scan operation in PyTorch (Blelloch version).\nThis code follows the skeleton proposed by Francois Fleuret in his pscan. However, the keys differences are :\n-it has been written in an iterative way (rather than recursive)\n-the backward pass has been rewritten\n\nPlease see docs/pscan.ipynb for a detailed explanation of what happens here.\n\n\"\"\"\n\n#\u00a0taken straight from https://github.com/johnma2006/mamba-minimal/blob/master/model.py\nclass RMSNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-5):\n        super().__init__()\n\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n\n    def forward(self, x):\n        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n\n        return output\n\n\nclass PScan(torch.autograd.Function):\n    @staticmethod\n    def pscan(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n\n        # modifies X in place by doing a parallel scan.\n        # more formally, X will be populated by these values :\n        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n\n        B, D, L, _ = A.size()\n        num_steps = int(math.log2(L))\n\n        # up sweep or reduction step\n        Aa = A\n        Xa = X\n        for k in range(num_steps):\n            T = 2 * (Xa.size(2) // 2)\n\n            Aa = Aa[:, :, :T].view(B, D, T // 2, 2, -1)\n            Xa = Xa[:, :, :T].view(B, D, T // 2, 2, -1)\n\n            Xa[:, :, :, 1].add_(Aa[:, :, :, 1].mul(Xa[:, :, :, 0]))\n            Aa[:, :, :, 1].mul_(Aa[:, :, :, 0])\n\n            Aa = Aa[:, :, :, 1]\n            Xa = Xa[:, :, :, 1]\n\n        # down sweep\n        for k in range(num_steps - 1, -1, -1):\n            Aa = A[:, :, 2**k - 1 : L : 2**k]\n            Xa = X[:, :, 2**k - 1 : L : 2**k]\n\n            T = 2 * (Xa.size(2) // 2)\n\n            if T < Xa.size(2):\n                Xa[:, :, -1].add_(Aa[:, :, -1].mul(Xa[:, :, -2]))\n                Aa[:, :, -1].mul_(Aa[:, :, -2])\n\n            Aa = Aa[:, :, :T].view(B, D, T // 2, 2, -1)\n            Xa = Xa[:, :, :T].view(B, D, T // 2, 2, -1)\n\n            Xa[:, :, 1:, 0].add_(\n                Aa[:, :, 1:, 0].mul(Xa[:, :, :-1, 1])\n            )\n            Aa[:, :, 1:, 0].mul_(Aa[:, :, :-1, 1])\n\n    @staticmethod\n    def forward(ctx, A_in, X_in):\n        \"\"\"\n        Applies the parallel scan operation, as defined above. Returns a new tensor.\n\n        Args:\n            A_in : (B, L, D, N)\n            X_in : (B, L, D, N)\n\n        Returns:\n            H : (B, L, D, N)\n        \"\"\"\n\n        # clone tensor (in-place ops)\n        A = A_in.clone()  # (B, L, D, N)\n        X = X_in.clone()  # (B, L, D, N)\n\n        # prepare tensors\n        A = A.transpose(2, 1)  # (B, D, L, N)\n        X = X.transpose(2, 1)  # (B, D, L, N)\n\n        # parallel scan\n        PScan.pscan(A, X)\n\n        ctx.save_for_backward(A_in, X)\n\n        return X.transpose(2, 1)\n\n    @staticmethod\n    def backward(ctx, grad_output_in):\n        \"\"\"\n        Flows the gradient from the output to the input. Returns two new tensors.\n\n        Args:\n            ctx : A_in : (B, L, D, N), X : (B, D, L, N)\n            grad_output_in : (B, L, D, N)\n\n        Returns:\n            gradA : (B, L, D, N), gradX : (B, L, D, N)\n        \"\"\"\n\n        A_in, X = ctx.saved_tensors\n\n        # clone tensors\n        A = A_in.clone()\n        # grad_output_in will be cloned with flip()\n\n        # prepare tensors\n        A = A.transpose(2, 1)  # (B, D, L, N)\n        A = torch.cat((A[:, :, :1], A[:, :, 1:].flip(2)), dim=2)\n        grad_output_b = grad_output_in.transpose(2, 1)\n\n        # reverse parallel scan\n        grad_output_b = grad_output_b.flip(2)\n        PScan.pscan(A, grad_output_b)\n        grad_output_b = grad_output_b.flip(2)\n\n        Q = torch.zeros_like(X)\n        Q[:, :, 1:].add_(X[:, :, :-1] * grad_output_b[:, :, 1:])\n\n        return Q.transpose(2, 1), grad_output_b.transpose(2, 1)\n\n\npscan = PScan.apply\n\n\n@dataclass\nclass MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n\n    dt_min: float = 0.001\n    dt_max: float = 0.1\n    dt_init: str = \"random\"  # \"random\" or \"constant\"\n    dt_scale: float = 1.0\n    dt_init_floor = 1e-4\n\n    bias: bool = False\n    conv_bias: bool = True\n\n    pscan: bool = True  # use parallel scan mode or sequential mode when training\n\n    def __post_init__(self):\n        self.d_inner = (\n            self.expand_factor * self.dim\n        )  # E*D = ED in comments\n\n        if self.dt_rank == \"auto\":\n            self.dt_rank = math.ceil(self.dim / 16)\n            \n            \n\nclass ResidualBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n\n        self.mixer = MambaBlock(config)\n        self.norm = RMSNorm(config.dim)\n\n    def forward(self, x):\n        # x : (B, L, D)\n\n        # output : (B, L, D)\n\n        output = self.mixer(self.norm(x)) + x\n        return output\n\n    def step(self, x, cache):\n        # x : (B, D)\n        # cache : (h, inputs)\n        # h : (B, ED, N)\n        # inputs: (B, ED, d_conv-1)\n\n        # output : (B, D)\n        # cache : (h, inputs)\n\n        output, cache = self.mixer.step(self.norm(x), cache)\n        output = output + x\n        return output, cache\n\n\n\nclass MambaBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n\n        self.config = config\n\n        # projects block input from D to 2*ED (two branches)\n        self.in_proj = nn.Linear(\n            config.dim, 2 * config.d_inner, bias=config.bias\n        )\n\n        self.conv1d = nn.Conv1d(\n            in_channels=config.d_inner,\n            out_channels=config.d_inner,\n            kernel_size=config.d_conv,\n            bias=config.conv_bias,\n            groups=config.d_inner,\n            padding=config.d_conv - 1,\n        )\n\n        # projects x to input-dependent \u0394, B, C\n        self.x_proj = nn.Linear(\n            config.d_inner,\n            config.dt_rank + 2 * config.d_state,\n            bias=False,\n        )\n\n        # projects \u0394 from dt_rank to d_inner\n        self.dt_proj = nn.Linear(\n            config.dt_rank, config.d_inner, bias=True\n        )\n\n        # dt initialization\n        # dt weights\n        dt_init_std = config.dt_rank**-0.5 * config.dt_scale\n        if config.dt_init == \"constant\":\n            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n        elif config.dt_init == \"random\":\n            nn.init.uniform_(\n                self.dt_proj.weight, -dt_init_std, dt_init_std\n            )\n        else:\n            raise NotImplementedError\n\n        # dt bias\n        dt = torch.exp(\n            torch.rand(config.d_inner)\n            * (math.log(config.dt_max) - math.log(config.dt_min))\n            + math.log(config.dt_min)\n        ).clamp(min=config.dt_init_floor)\n        inv_dt = dt + torch.log(\n            -torch.expm1(-dt)\n        )  # inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n        with torch.no_grad():\n            self.dt_proj.bias.copy_(inv_dt)\n        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n        # todo : explain why removed\n\n        # S4D real initialization\n        A = torch.arange(\n            1, config.d_state + 1, dtype=torch.float32\n        ).repeat(config.d_inner, 1)\n        self.A_log = nn.Parameter(\n            torch.log(A)\n        )  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n        self.D = nn.Parameter(torch.ones(config.d_inner))\n\n        # projects block output from ED back to D\n        self.out_proj = nn.Linear(\n            config.d_inner, config.dim, bias=config.bias\n        )\n\n    def forward(self, x):\n        # x : (B, L, D)\n\n        # y : (B, L, D)\n\n        _, L, _ = x.shape\n\n        xz = self.in_proj(x)  # (B, L, 2*ED)\n        x, z = xz.chunk(2, dim=-1)  # (B, L, ED), (B, L, ED)\n\n        # x branch\n        x = x.transpose(1, 2)  # (B, ED, L)\n        x = self.conv1d(x)[\n            :, :, :L\n        ]  # depthwise convolution over time, with a short filter\n        x = x.transpose(1, 2)  # (B, L, ED)\n\n        x = F.silu(x)\n        y = self.ssm(x)\n\n        # z branch\n        z = F.silu(z)\n\n        output = y * z\n        output = self.out_proj(output)  # (B, L, D)\n\n        return output\n\n    def ssm(self, x):\n        # x : (B, L, ED)\n\n        # y : (B, L, ED)\n\n        A = -torch.exp(self.A_log.float())  # (ED, N)\n        D = self.D.float()\n        # TODO remove .float()\n\n        deltaBC = self.x_proj(x)  # (B, L, dt_rank+2*N)\n\n        delta, B, C = torch.split(\n            deltaBC,\n            [\n                self.config.dt_rank,\n                self.config.d_state,\n                self.config.d_state,\n            ],\n            dim=-1,\n        )  # (B, L, dt_rank), (B, L, N), (B, L, N)\n        delta = F.softplus(self.dt_proj(delta))  # (B, L, ED)\n\n        if self.config.pscan:\n            y = self.selective_scan(x, delta, A, B, C, D)\n        else:\n            y = self.selective_scan_seq(x, delta, A, B, C, D)\n\n        return y\n\n    def selective_scan(self, x, delta, A, B, C, D):\n        # x : (B, L, ED)\n        # \u0394 : (B, L, ED)\n        # A : (ED, N)\n        # B : (B, L, N)\n        # C : (B, L, N)\n        # D : (ED)\n\n        # y : (B, L, ED)\n\n        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n\n        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n\n        hs = pscan(deltaA, BX)\n\n        y = (hs @ C.unsqueeze(-1)).squeeze(\n            3\n        )  # (B, L, ED, N) @\u00a0(B, L, N, 1) -> (B, L, ED, 1)\n\n        y = y + D * x\n\n        return y\n\n    def selective_scan_seq(self, x, delta, A, B, C, D):\n        # x : (B, L, ED)\n        # \u0394 : (B, L, ED)\n        # A : (ED, N)\n        # B : (B, L, N)\n        # C : (B, L, N)\n        # D : (ED)\n\n        # y : (B, L, ED)\n\n        _, L, _ = x.shape\n\n        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n\n        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n\n        h = torch.zeros(\n            x.size(0),\n            self.config.d_inner,\n            self.config.d_state,\n            device=deltaA.device,\n        )  # (B, ED, N)\n        hs = []\n\n        for t in range(0, L):\n            h = deltaA[:, t] * h + BX[:, t]\n            hs.append(h)\n\n        hs = torch.stack(hs, dim=1)  # (B, L, ED, N)\n\n        y = (hs @ C.unsqueeze(-1)).squeeze(\n            3\n        )  # (B, L, ED, N) @\u00a0(B, L, N, 1) -> (B, L, ED, 1)\n\n        y = y + D * x\n\n        return y\n\n    # -------------------------- inference -------------------------- #\n    \"\"\"\n    Concerning auto-regressive inference\n\n    The cool part of using Mamba : inference is constant wrt to sequence length\n    We just have to keep in cache, for each layer, two things :\n    - the hidden state h (which is (B, ED, N)), as you typically would when doing inference with a RNN\n    - the last d_conv-1 inputs of the layer, to be able to compute the 1D conv which is a convolution over the time dimension\n      (d_conv is fixed so this doesn't incur a growing cache as we progress on generating the sequence)\n      (and d_conv is usually very small, like 4, so we just have to \"remember\" the last 3 inputs)\n\n    Concretely, these two quantities are put inside a cache tuple, and are named h and inputs respectively.\n    h is (B, ED, N), and inputs is (B, ED, d_conv-1)\n    The MambaBlock.step() receives this cache, and, along with outputing the output, alos outputs the updated cache for the next call.\n\n    The cache object is initialized as follows : (None, torch.zeros()).\n    When h is None, the selective scan function detects it and start with h=0.\n    The torch.zeros() isn't a problem (it's same as just feeding the input, because the conv1d is padded)\n\n    As we need one such cache variable per layer, we store a caches object, which is simply a list of cache object. (See mamba_lm.py)\n    \"\"\"\n\n    def step(self, x, cache):\n        # x : (B, D)\n        # cache : (h, inputs)\n        # h : (B, ED, N)\n        # inputs : (B, ED, d_conv-1)\n\n        # y : (B, D)\n        # cache : (h, inputs)\n\n        h, inputs = cache\n\n        xz = self.in_proj(x)  # (B, 2*ED)\n        x, z = xz.chunk(2, dim=1)  # (B, ED), (B, ED)\n\n        # x branch\n        x_cache = x.unsqueeze(2)\n        x = self.conv1d(torch.cat([inputs, x_cache], dim=2))[\n            :, :, self.config.d_conv - 1\n        ]  # (B, ED)\n\n        x = F.silu(x)\n        y, h = self.ssm_step(x, h)\n\n        # z branch\n        z = F.silu(z)\n\n        output = y * z\n        output = self.out_proj(output)  # (B, D)\n\n        # prepare cache for next call\n        inputs = torch.cat(\n            [inputs[:, :, 1:], x_cache], dim=2\n        )  # (B, ED, d_conv-1)\n        cache = (h, inputs)\n\n        return output, cache\n\n    def ssm_step(self, x, h):\n        # x : (B, ED)\n        # h : (B, ED, N)\n\n        # y : (B, ED)\n        # h : (B, ED, N)\n\n        A = -torch.exp(\n            self.A_log.float()\n        )  # (ED, N) #\u00a0todo : ne pas le faire tout le temps, puisque c'est ind\u00e9pendant de la timestep\n        D = self.D.float()\n        # TODO remove .float()\n\n        deltaBC = self.x_proj(x)  # (B, dt_rank+2*N)\n\n        delta, B, C = torch.split(\n            deltaBC,\n            [\n                self.config.dt_rank,\n                self.config.d_state,\n                self.config.d_state,\n            ],\n            dim=-1,\n        )  # (B, dt_rank), (B, N), (B, N)\n        delta = F.softplus(self.dt_proj(delta))  # (B, ED)\n\n        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, ED, N)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(1)  # (B, ED, N)\n\n        BX = deltaB * (x.unsqueeze(-1))  # (B, ED, N)\n\n        if h is None:\n            h = torch.zeros(\n                x.size(0),\n                self.config.d_inner,\n                self.config.d_state,\n                device=deltaA.device,\n            )  # (B, ED, N)\n\n        h = deltaA * h + BX  # (B, ED, N)\n\n        y = (h @ C.unsqueeze(-1)).squeeze(\n            2\n        )  # (B, ED, N) @\u00a0(B, N, 1) -> (B, ED, 1)\n\n        y = y + D * x\n\n        # todo : pq h.squeeze(1) ??\n        return y, h.squeeze(1)\n\n\nclass Mamba(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.layers = nn.ModuleList(\n            [ResidualBlock(config) for _ in range(config.depth)]\n        )\n        # self.norm_f = RMSNorm(config.dim)\n\n    def forward(self, x):\n        # x : (B, L, D)\n\n        # y : (B, L, D)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        # x = self.norm_f(x)\n\n        return x\n\n    def step(self, x, caches):\n        # x : (B, L, D)\n        # caches : [cache(layer) for all layers], cache : (h, inputs)\n\n        # y : (B, L, D)\n        # caches : [cache(layer) for all layers], cache : (h, inputs)\n\n        for i, layer in enumerate(self.layers):\n            x, caches[i] = layer.step(x, caches[i])\n\n        return x, caches\n    \n    ",
    "description": null,
    "url": null
}