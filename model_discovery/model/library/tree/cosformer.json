{
    "acronym": "cosformer",
    "title": "cosFormer: Rethinking Softmax in Attention",
    "seed_ids": [
        "roformer",
        "rfa",
        "nystromformer",
        "performer",
        "bigbird",
        "lineartransformer",
        "linformer",
        "longformer",
        "sinkhorn",
        "reformer",
        "transformer",
        "bert"
    ],
    "s2id": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
    "abstract": "Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called cosFormer that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. cosFormer is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, cosFormer fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at https://github.com/OpenNLPLab/cosFormer.",
    "authors": [
        "Zhen Qin",
        "Weixuan Sun",
        "Huicai Deng",
        "Dongxu Li",
        "Yunshen Wei",
        "Baohong Lv",
        "Junjie Yan",
        "Lingpeng Kong",
        "Yiran Zhong"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": "A linear transformer called cosFormer is proposed that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions and is based on two key properties of softmax attention: non-negativeness of the attention matrix and a non-linear re-weighting scheme that can concentrate the distribution of the Attention matrix.",
    "citationCount": 152,
    "influentialCitationCount": 23,
    "code": "import torch\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom torch import Tensor\nfrom typing import Optional\nfrom torch import nn\n\nclass CosformerAttention(nn.Module):\n    \"\"\"\n    cosformer attention in \"cosFormer: Rethinking Softmax In Attention\"\n    https://arxiv.org/abs/2202.08791\n    \"\"\"\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout_rate=0.0,\n        causal=False,\n        has_outproj=True,\n        act_fun=\"relu\",\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if kdim is not None else embed_dim\n        self.num_heads = num_heads\n        self.has_outproj = has_outproj\n        self.act_fun = self.get_act_fun(act_fun)\n        # q, k, v projection\n        self.k_proj = nn.Linear(self.kdim, embed_dim)\n        self.v_proj = nn.Linear(self.vdim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        # outprojection\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        # dropout rate\n        self.dropout_rate = dropout_rate\n        # causal\n        self.causal = causal\n\n        assert (self.embed_dim % self.num_heads == 0), \"embed_dim must be divisible by num_heads\"\n\n    def get_index(self, seq_len):\n        index = np.pi / 2 * torch.arange(1, seq_len + 1).reshape(1, -1, 1)\n\n        return nn.Parameter(index, requires_grad=False)\n\n    def get_act_fun(self, act_fun):\n        if act_fun == \"relu\":\n            return F.relu\n        elif act_fun == \"elu\":\n            return 1 + F.elu\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Optional[Tensor] = None,\n        value: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        eps: Optional[float] = 1e-6,\n    ):\n        \"\"\"Input shape: Sequence x Batch x Embedding\n        Args:\n            query (Tensor): `(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension.\n            key (Tensor): `(S, N, E)` where S is the source sequence length, N is the batch size,\n            E is the embedding dimension.\n            value (Tensor): `(S, N, E)` where S is the source sequence length, N is the batch size,\n            E is the embedding dimension.\n            attn_mask (Optional[Tensor], optional): typically used to implement causal attention, \n            where the mask prevents the attention from looking forward in time (default: None).\n        \"\"\"\n        if key == None:\n            key = query\n        if value == None:\n            value = query\n        \n        num_heads = self.num_heads\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = key.size(0)\n        head_dim = embed_dim // num_heads\n\n        # get q, k, v\n        # (L, N, E)\n        q = self.q_proj(query)\n        # (S, N, E)\n        k = self.k_proj(key)\n        # (S, N, E)\n        v = self.v_proj(value)\n\n        # activation\n        q = self.act_fun(q)\n        k = self.act_fun(k)\n\n        # multihead reshape\n        # (N * h, L, d)\n        q = q.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n        # (N * h, S, d)\n        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n        # (N * h, S, d)\n        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n        \n        # cos transform\n        m = max(src_len, tgt_len)\n        # get index and send to cuda\n        weight_index = self.get_index(m).to(q)\n        # (N * h, L, 2 * d)\n        q_ = torch.cat([q * torch.sin(weight_index[:, :tgt_len, :] / m), q * torch.cos(weight_index[:, :tgt_len, :] / m)], dim=-1)\n        # (N * h, S, 2 * d)\n        k_ = torch.cat([k * torch.sin(weight_index[:, :src_len, :] / m), k * torch.cos(weight_index[:, :src_len, :] / m)], dim=-1)\n\n        if self.causal:\n            ## Need to improve speed!\n            # (N * h, L, 2 * d) (N * h, L, d) -> (N * h, L, h, 2 * d, d)\n            kv_ = torch.einsum(\"nld,nlm->nldm\", k_, v)\n            # (N * h, L, 2 * d, d) -> (N * h, L, 2 * d, d)\n            kv_cum = torch.cumsum(kv_, dim=1)\n            # (N * h, L, 2 * d) (N * h, L, 2 * d, d) -> (N * h, L, d)\n            qkv = torch.einsum(\"nld,nldm->nlm\", q_, kv_cum)\n            # (N * h, L, 2 * d) -> (N * h, L, 2 * d)\n            k_cum = torch.cumsum(k_, dim=1)\n            # (N * h, L, 2 * d) (N * h, L, 2 * d) -> (N * h, L)\n            denom = torch.clamp_min(torch.einsum(\"nlm,nlm->nl\", q_, k_cum), eps)\n            # (N * h, L, d) (N * h, L, 1) -> (N * h, L, d)\n            attn_output = qkv / denom.unsqueeze(-1)\n            # (N * h, L, d) -> (L, N * h, d) -> (L, N, E)\n            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, -1)\n        else:\n            # (N * h, L, 2 * d) (N * h, L, d) -> (N * h, 2 * d, d)\n            kv_ = torch.einsum('nld,nlm->ndm', k_, v)\n            # (N * h, L, 2 * d) (N * h, 2 * d) -> (N * h, L)\n            z_ = 1 / torch.clamp_min(torch.einsum('nld,nd->nl', q_, torch.sum(k_, axis=1)), eps)\n            # (N * h, L, 2 * d) (N * h, d, 2 * d) (N * h, L) -> (N * h, L, d)\n            attn_output = torch.einsum('nld,ndm,nl->nlm', q_, kv_, z_)\n            # (N * h, L, d) -> (L, N * h, d) -> (L, N, E)\n            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, -1)\n        # L, N, E\n        if self.has_outproj:\n            attn_output = self.out_proj(attn_output)\n\n        return attn_output\n\n    def left_product(\n        self,\n        query: Tensor,\n        key: Optional[Tensor] = None,\n        value: Optional[Tensor] = None,\n        attn_mask: Optional[Tensor] = None,\n        eps: Optional[float] = 1e-6,\n    ):\n        \"\"\"Input shape: Sequence x Batch x Embedding\n        Args:\n            query (Tensor): `(L, N, E)` where L is the target sequence length, N is the batch size,\n            E is the embedding dimension.\n            key (Tensor): `(S, N, E)` where S is the source sequence length, N is the batch size,\n            E is the embedding dimension.\n            value (Tensor): `(S, N, E)` where S is the source sequence length, N is the batch size,\n            E is the embedding dimension.\n            attn_mask (Optional[Tensor], optional): typically used to implement causal attention, \n            where the mask prevents the attention from looking forward in time (default: None).\n        \"\"\"\n        # test for the correctness of the program\n        if key == None:\n            key = query\n        if value == None:\n            value = query\n        \n        num_heads = self.num_heads\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = key.size(0)\n        head_dim = embed_dim // num_heads\n\n        # get q, k, v\n        # (L, N, E)\n        q = self.q_proj(query)\n        # (S, N, E)\n        k = self.k_proj(key)\n        # (S, N, E)\n        v = self.v_proj(value)\n\n        # activation\n        q = self.act_fun(q)\n        k = self.act_fun(k)\n\n        # multihead reshape\n        # (N * h, L, d)\n        q = q.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n        # (N * h, S, d)\n        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n        # (N * h, S, d)\n        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n        \n        # cos transform\n        m = max(src_len, tgt_len)\n        # get index and send to cuda\n        weight_index = self.get_index(m).to(q)\n        # (N * h, L, 2 * d)\n        q_ = torch.cat([q * torch.sin(weight_index[:, :tgt_len, :] / m), q * torch.cos(weight_index[:, :tgt_len, :] / m)], dim=-1)\n        # (N * h, S, 2 * d)\n        k_ = torch.cat([k * torch.sin(weight_index[:, :src_len, :] / m), k * torch.cos(weight_index[:, :src_len, :] / m)], dim=-1)\n\n        # (N * h, L, d) (N * h, d, S) -> (N * h, L, S)\n        weights = torch.bmm(q_, k_.transpose(1, 2))\n        # mask\n        if self.causal:\n            weights = weights.masked_fill(attn_mask==float(\"-inf\"), 0)\n        # (N * h, L, S) -> (N * h, L, S)\n        denom = torch.clamp_min(weights.sum(dim=-1, keepdim=True), eps)\n        # (N * h, L, S) (N * h, L, S) -> (N * h, L, S)\n        attn_weights = weights / denom\n        # (N * h, L, S) (N * h, S, d) -> (N * h, L, d)\n        attn_output = torch.bmm(attn_weights, v)\n        # (N * h, L, d) -> (L, N * h, d) -> (L, N, E)\n        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, -1)\n        # L, N, E\n        if self.has_outproj:\n            attn_output = self.out_proj(attn_output)\n\n        return attn_output\n\ndef test(batch=2, tgt_len=10, src_len=20, embed_dim=128, num_heads=8, N=100, causal=False):\n    model = CosformerAttention(embed_dim=embed_dim, num_heads=num_heads, causal=causal)\n    diff = 0\n    if causal:\n        mask = (torch.triu(torch.ones(tgt_len, tgt_len)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf'))\n    else:\n        mask = None\n    for i in range(N):\n        query = torch.rand(tgt_len, batch, embed_dim)\n        key = torch.rand(src_len, batch, embed_dim)\n        value = torch.rand(src_len, batch, embed_dim)\n        left_res = model.left_product(query, key, value, mask)\n        right_res = model(query, key, value)\n        diff += torch.norm(left_res - right_res)\n    diff /= N\n\n    if causal:\n        print(\"Test result for causal model:\")\n    else:\n        print(\"Test result for bidirectional model:\")\n    print(f\"The error of left multiplication and right multiplication is {diff}\")\n\ndef main():\n    test(tgt_len=10, src_len=20, causal=False)\n    test(tgt_len=10, src_len=10, causal=True)\n\nif __name__ == \"__main__\":\n    main()",
    "description": null,
    "url": null
}