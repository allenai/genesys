{
    "acronym": "cape",
    "title": "CAPE: Context-Adaptive Positional Encoding for Length Extrapolation",
    "seed_ids": [
        "longrope",
        "bipe",
        "clex",
        "streamingllm",
        "yarn",
        "receptivefieldana",
        "kerple",
        "alibi",
        "roformer",
        "performer",
        "gpt3",
        "sinkhorn",
        "fire"
    ],
    "s2id": "24313a18e1ba789fc72ef22489b9b9a630244aed",
    "abstract": "Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be context-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Context-Adaptive Positional Encoding (CAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that CAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.",
    "authors": [
        "Chuanyang Zheng",
        "Yihang Gao",
        "Han Shi",
        "Minbin Huang",
        "Jingyao Li",
        "Jing Xiong",
        "Xiaozhe Ren",
        "Michael Ng",
        "Xin Jiang",
        "Zhenguo Li",
        "Yu Li"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "A Context-Adaptive Positional Encoding (CAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors is proposed, which enhances model performances in terms of trained length and length generalization.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "import torch\nimport torch.nn as nn\nimport math\n\nclass AliBi(torch.nn.Module):\n    def __init__(self, num_heads, mp_size=1, mp_rank=1):\n        super().__init__()\n        # megatron splits across heads, so we need to make sure each\n        # head receives the correct matrix\n        assert mp_size <= num_heads and mp_rank <= mp_size\n        self.mp_size = mp_size\n        self.mp_rank = mp_rank\n        self.num_heads = num_heads\n        self.slice_size = num_heads // mp_size\n        self.cached_matrix = None\n        self.cached_seq_len = None\n        slopes = torch.Tensor(self._get_slopes(num_heads))[\n            mp_rank * self.slice_size : (mp_rank + 1) * self.slice_size\n        ]\n        self.register_buffer(\"slopes\", slopes)\n        print(slopes.dtype)\n\n\n\n    def _get_slopes(self, n):\n        \"\"\"\n        Get slopes for Alibi positional embedding\n        n : int = number of heads.\n        For best performance, restrict n to a power of 2.\n        \"\"\"\n\n        def get_slopes_power_of_2(n):\n            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n            ratio = start\n            return [start * ratio**i for i in range(n)]\n\n        if math.log2(n).is_integer():\n            return get_slopes_power_of_2(n)\n        else:\n            closest_power_of_2 = 2 ** math.floor(math.log2(n))\n            return (\n                get_slopes_power_of_2(closest_power_of_2)\n                + self._get_slopes(2 * closest_power_of_2)[0::2][\n                    : n - closest_power_of_2\n                ]\n            )\n\n\n    def forward(self, x):\n        # [b, np, sq, sk]\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n\n        # Initialize the AliBi matrix to match the first provided key length; grow it exponentially\n        # afterwards if longer inputs are provided. This is important for inference, where we will\n        # encounter progressively longer samples; it should have no effect at training time.\n        if self.cached_seq_len is not None and self.cached_seq_len >= seq_len_k:\n            a = self.cached_matrix\n        else:\n            target_seq_len = (\n                seq_len_k\n            )\n            a = -torch.tril(\n                torch.arange(target_seq_len)\n                .view(target_seq_len, 1)\n                .repeat(1, target_seq_len)\n                + torch.arange(0, -target_seq_len, -1)\n            )\n            a = a.to(x.device).to(x.dtype)\n            # print(a)\n            slopes = self.slopes.to(a.device).to(a.dtype)\n            a = a * slopes.view(self.slopes.shape[0], 1, 1)\n            self.cached_seq_len = target_seq_len\n            self.cached_matrix = a\n\n        # If the AliBi matrix is larger than the key length, clip it.\n        if self.cached_seq_len > seq_len_k:\n            a = self.cached_matrix[:, :seq_len_k, :seq_len_k]\n\n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n            a = a[:, seq_len_k - 1, :].view(\n                a.shape[0], 1, a.shape[2]\n            )  # seq_len_k - 1 points to the last token index in the current inference batch.\n\n        return x + a\n\n\nclass AliBi_CAPE(torch.nn.Module):\n    def __init__(self, num_heads, mp_size=1, mp_rank=1,mlp_width=32):\n        super().__init__()\n        # megatron splits across heads, so we need to make sure each\n        # head receives the correct matrix\n        assert mp_size <= num_heads and mp_rank <= mp_size\n        self.mp_size = mp_size\n        self.mp_rank = mp_rank\n        self.num_heads = num_heads\n        self.slice_size = num_heads // mp_size\n        self.cached_matrix = None\n        self.cached_seq_len = None\n        slopes = torch.Tensor(self._get_slopes(num_heads))[\n            mp_rank * self.slice_size : (mp_rank + 1) * self.slice_size\n        ]\n        self.register_buffer(\"slopes\", slopes)\n\n        self.mlp2 = nn.Sequential(\n            nn.Linear(num_heads * 2, mlp_width),\n            nn.LeakyReLU(),\n            nn.Linear(mlp_width, num_heads))\n\n\n\n    def _get_slopes(self, n):\n        \"\"\"\n        Get slopes for Alibi positional embedding\n        n : int = number of heads.\n        For best performance, restrict n to a power of 2.\n        \"\"\"\n\n        def get_slopes_power_of_2(n):\n            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n            ratio = start\n            return [start * ratio**i for i in range(n)]\n\n        if math.log2(n).is_integer():\n            return get_slopes_power_of_2(n)\n        else:\n            closest_power_of_2 = 2 ** math.floor(math.log2(n))\n            return (\n                get_slopes_power_of_2(closest_power_of_2)\n                + self._get_slopes(2 * closest_power_of_2)[0::2][\n                    : n - closest_power_of_2\n                ]\n            )\n\n\n    def forward(self, x):\n        # [b, np, sq, sk]\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n\n        # Initialize the AliBi matrix to match the first provided key length; grow it exponentially\n        # afterwards if longer inputs are provided. This is important for inference, where we will\n        # encounter progressively longer samples; it should have no effect at training time.\n        if self.cached_seq_len is not None and self.cached_seq_len >= seq_len_k:\n            a = self.cached_matrix\n        else:\n            target_seq_len = (\n                seq_len_k\n            )\n            a = -torch.tril(\n                torch.arange(target_seq_len)\n                .view(target_seq_len, 1)\n                .repeat(1, target_seq_len)\n                + torch.arange(0, -target_seq_len, -1)\n            )\n            a = a.to(x.device).to(x.dtype)\n            # print(a)\n            slopes = self.slopes.to(a.device).to(a.dtype)\n            a = a * slopes.view(self.slopes.shape[0], 1, 1)\n            self.cached_seq_len = target_seq_len\n            self.cached_matrix = a\n\n        # If the AliBi matrix is larger than the key length, clip it.\n        if self.cached_seq_len > seq_len_k:\n            a = self.cached_matrix[:, :seq_len_k, :seq_len_k]\n\n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n            a = a[:, seq_len_k - 1, :].view(\n                a.shape[0], 1, a.shape[2]\n            )  # seq_len_k - 1 points to the last token index in the current inference batch.\n\n\n        x_a_bias=torch.cat((x,torch.tile(a,(x.shape[0],1,1,1))),dim=1)\n        # print(x_a_bias)\n        x_a_bias=torch.permute(x_a_bias,(0,2,3,1))\n        x_a_bias=self.mlp2(x_a_bias)\n        x_a_bias=torch.permute(x_a_bias,(0,3,1,2))\n\n        return x + a+ x_a_bias\n\n\n\nclass ParallelKerpleLog(torch.nn.Module):\n    \"\"\"Kernelized T5 Relative Position Bias parallelized in the heads dimension\"\"\"\n\n    def __init__(\n            self,\n            neox_args,\n    ):\n        super().__init__()\n        self.heads = neox_args.num_attention_heads\n        # self.model_parallel_size = get_model_parallel_world_size()\n        # self.model_parallel_rank = get_model_parallel_rank()\n        self.num_heads_per_partition = self.heads\n        self.pos_emb = neox_args.pos_emb\n        self.eps = 1e-2\n\n\n        # megatron splits across heads, so we need to make sure each head receives the correct matrix\n        # assert self.model_parallel_size <= self.heads and self.model_parallel_rank <= self.model_parallel_size\n\n        # Allocate weights and initialize.\n        # The kernel has the form -p*log(1+a*|m-n|)\n        def get_parameter(scale, init_method):\n            if init_method == 'ones':\n                return nn.Parameter(torch.ones(\n                    self.num_heads_per_partition,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )[:, None, None] * scale)\n            elif init_method == 'uniform':\n                return nn.Parameter(torch.rand(\n                    self.num_heads_per_partition,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )[:, None, None] * scale)\n\n        self.bias_p = get_parameter(2, 'uniform')\n        self.bias_a = get_parameter(1, 'uniform')\n\n        self.cached_matrix = None\n        self.cached_seq_len = None\n\n\n\n\n    def stats(self):\n        def get_stats(name, obj):\n            return {name + '_mean': obj.mean().detach().cpu(),\n                    name + '_std': obj.std().detach().cpu(),\n                    name + '_max': obj.max().detach().cpu(),\n                    name + '_min': obj.min().detach().cpu()}\n\n        dd = {}\n        self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n        dd.update(get_stats('bias_a', self.bias_a))\n        self.bias_p.data = self.bias_p.data.clamp(min=self.eps)\n        dd.update(get_stats('bias_p', self.bias_p))\n        return dd\n\n    def forward(self, x):\n        # [b, np, sq, sk]\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n        if self.cached_seq_len != seq_len_k:\n            diff = torch.tril(\n                torch.arange(seq_len_k, device=x.device).view(seq_len_k, 1).repeat(1, seq_len_k)\n                + torch.arange(0, -seq_len_k, -1, device=x.device)\n            )\n            diff = diff.to(x.dtype)\n            self.cached_seq_len = seq_len_k\n            self.cached_matrix = diff\n        else:\n            diff = self.cached_matrix\n\n        self.bias_p.data = self.bias_p.data.clamp(min=self.eps)\n        self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n        bias = -self.bias_p * torch.log(1 + self.bias_a * diff)  # log kernel\n\n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                    seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n\n            if type(bias) != float:\n                # seq_len_k - 1 points to the last token index in the current inference batch.\n                bias = bias[:, seq_len_k - 1, :].view(bias.shape[0], 1, bias.shape[2])\n\n        return x + bias\n\n\n\n\nclass ParallelKerpleLog_CAPE(torch.nn.Module):\n    \"\"\"Kernelized T5 Relative Position Bias parallelized in the heads dimension\"\"\"\n\n    def __init__(\n            self,\n            neox_args,\n            layer_number\n    ):\n        super().__init__()\n        self.heads = neox_args.num_attention_heads\n        # self.model_parallel_size = get_model_parallel_world_size()\n        # self.model_parallel_rank = get_model_parallel_rank()\n        self.num_heads_per_partition = self.heads\n        self.pos_emb = neox_args.pos_emb\n        self.eps = 1e-2\n\n        self.layer_number_visualization = layer_number\n\n        # megatron splits across heads, so we need to make sure each head receives the correct matrix\n        # assert self.model_parallel_size <= self.heads and self.model_parallel_rank <= self.model_parallel_size\n\n        # Allocate weights and initialize.\n        # The kernel has the form -p*log(1+a*|m-n|)\n        def get_parameter(scale, init_method):\n            if init_method == 'ones':\n                return nn.Parameter(torch.ones(\n                    self.num_heads_per_partition,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )[:, None, None] * scale)\n            elif init_method == 'uniform':\n                return nn.Parameter(torch.rand(\n                    self.num_heads_per_partition,\n                    device=torch.cuda.current_device(),\n                    dtype=neox_args.params_dtype,\n                )[:, None, None] * scale)\n\n        self.bias_p = get_parameter(2, 'uniform')\n        self.bias_a = get_parameter(1, 'uniform')\n\n        self.cached_matrix = None\n        self.cached_seq_len = None\n        self.mlp2 = nn.Sequential(\n            nn.Linear(self.num_heads_per_partition * 2, neox_args.mlp_width),\n            nn.LeakyReLU(),\n            nn.Linear(neox_args.mlp_width, self.num_heads_per_partition))\n\n\n\n\n    def stats(self):\n        def get_stats(name, obj):\n            return {name + '_mean': obj.mean().detach().cpu(),\n                    name + '_std': obj.std().detach().cpu(),\n                    name + '_max': obj.max().detach().cpu(),\n                    name + '_min': obj.min().detach().cpu()}\n\n        dd = {}\n        self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n        dd.update(get_stats('bias_a', self.bias_a))\n        self.bias_p.data = self.bias_p.data.clamp(min=self.eps)\n        dd.update(get_stats('bias_p', self.bias_p))\n        return dd\n\n    def forward(self, x):\n        # [b, np, sq, sk]\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n        if self.cached_seq_len != seq_len_k:\n            diff = torch.tril(\n                torch.arange(seq_len_k, device=x.device).view(seq_len_k, 1).repeat(1, seq_len_k)\n                + torch.arange(0, -seq_len_k, -1, device=x.device)\n            )\n            diff = diff.to(x.dtype)\n            self.cached_seq_len = seq_len_k\n            self.cached_matrix = diff\n        else:\n            diff = self.cached_matrix\n\n        self.bias_p.data = self.bias_p.data.clamp(min=self.eps)\n        self.bias_a.data = self.bias_a.data.clamp(min=self.eps)\n        bias = -self.bias_p * torch.log(1 + self.bias_a * diff)  # log kernel\n\n        if seq_len_q != seq_len_k:\n            # In the train case x has dimensionality [b, np, sq, sk] with sq == sk\n            # The number of query tokens is equal to the number of key tokens\n            # At inference time with cache in layer_past sq is not equal to sk. sq only contains one token (the last one in the full sequence)\n            # In this case we use the appropriate token index of the cache matrix.\n            # As the cache matrix could already be bigger from a past inference, not the last token index in the sq sequence is used\n            assert (\n                    seq_len_q == 1\n            ), \"assumption sq == sk unless at inference time with cache in layer_past with sq == 1\"\n\n            if type(bias) != float:\n                # seq_len_k - 1 points to the last token index in the current inference batch.\n                bias = bias[:, seq_len_k - 1, :].view(bias.shape[0], 1, bias.shape[2])\n\n\n\n        x_a_bias=torch.cat((x,torch.tile(bias,(x.shape[0],1,1,1))),dim=1)\n        # print(x_a_bias)\n        x_a_bias=torch.permute(x_a_bias,(0,2,3,1))\n        x_a_bias=self.mlp2(x_a_bias)\n        x_a_bias=torch.permute(x_a_bias,(0,3,1,2))\n\n\n        return x + bias + x_a_bias\n\n\n\n\nclass FIRE(nn.Module):\n    def __init__(self, num_heads=12, mlp_width=32, init_c=0.1, init_L=512., eps=1e-6):\n        super(FIRE, self).__init__()\n\n        # Define the MLP layers\n        self.mlp = nn.Sequential(\n            nn.Linear(1, mlp_width),\n            nn.ReLU(),\n            nn.Linear(mlp_width, num_heads))\n        # Initialize c (log transformation parameter)\n        self.c = nn.Parameter(torch.tensor(init_c))\n\n\n        # Initialize L (threshold)\n\n        self.init_L = nn.Parameter(torch.tensor(init_L), requires_grad=False)\n        # Learn a multiplier to L\n\n        self.L_multiplier = nn.Parameter(torch.tensor(1.0))\n\n        self.eps = eps\n\n        self.cached_matrix = None\n        self.cached_seq_len = None\n\n\n    def forward(self, x: torch.Tensor):\n        seq_length = x.size(2)\n\n        # positions = torch.arange(seq_length, dtype=torch.float, device=x.device)\n\n\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n        if self.cached_seq_len != seq_len_k:\n            rel_distance = torch.tril(\n                torch.arange(seq_len_k, device=x.device).view(seq_len_k, 1).repeat(1, seq_len_k)\n                + torch.arange(0, -seq_len_k, -1, device=x.device)\n            )\n            rel_distance = rel_distance.to(torch.float32)\n            self.cached_seq_len = seq_len_k\n            self.cached_matrix = rel_distance\n        else:\n            rel_distance = self.cached_matrix\n        # print(rel_distance)\n\n        # rel_distance = positions[:, None] - positions[None, :]\n\n        threshold = torch.abs(self.L_multiplier * self.init_L)\n        rel_distance_max = torch.max(torch.tril(rel_distance), dim=-1)[0]\n        # print(rel_distance_max)\n\n        pos_normalizer = torch.max(rel_distance_max, threshold)\n\n        pos_normalizer = pos_normalizer[:, None]\n\n\n        # self.c=self.c.to(rel_distance.device)\n        rel_distance = torch.log(torch.abs(self.c * rel_distance) + 1\n                                 )\n\n        pos_normalizer = torch.log(torch.abs(self.c * pos_normalizer) + 1) + self.eps\n\n        # Progressive interpolation\n\n        normalized_distance = rel_distance / pos_normalizer\n        normalized_distance=normalized_distance.to(x.dtype)\n        # print(normalized_distance)\n\n        fire_bias = self.mlp(normalized_distance.unsqueeze(-1))\n\n        fire_bias = fire_bias.unsqueeze(0).permute(0, 3, 1, 2)\n        # print(x)\n        # print(fire_bias)\n\n        return x + fire_bias\n\n\nclass FIRE_CAPE(nn.Module):\n    def __init__(self, num_heads=12, mlp_width=32, init_c=0.1, init_L=512., eps=1e-6):\n        super(FIRE_CAPE, self).__init__()\n\n        # Define the MLP layers\n        self.mlp = nn.Sequential(\n            nn.Linear(1, mlp_width),\n            nn.LeakyReLU(),\n            nn.Linear(mlp_width, num_heads))\n        # Initialize c (log transformation parameter)\n\n        self.mlp2 = nn.Sequential(\n            nn.Linear(num_heads*2, mlp_width),\n            nn.LeakyReLU(),\n            nn.Linear(mlp_width, num_heads))\n        self.c = nn.Parameter(torch.tensor(init_c))\n\n        # Initialize L (threshold)\n\n        self.init_L = nn.Parameter(torch.tensor(init_L), requires_grad=False)\n        # Learn a multiplier to L\n\n        self.L_multiplier = nn.Parameter(torch.tensor(1.0))\n\n        self.eps = eps\n\n        self.cached_matrix = None\n        self.cached_seq_len = None\n\n    def forward(self, x: torch.Tensor):\n        seq_length = x.size(2)\n\n        # positions = torch.arange(seq_length, dtype=torch.float, device=x.device)\n\n        seq_len_q = x.shape[-2]\n        seq_len_k = x.shape[-1]\n        if self.cached_seq_len != seq_len_k:\n            rel_distance = torch.tril(\n                torch.arange(seq_len_k, device=x.device).view(seq_len_k, 1).repeat(1, seq_len_k)\n                + torch.arange(0, -seq_len_k, -1, device=x.device)\n            )\n            rel_distance = rel_distance.to(torch.float32)\n            self.cached_seq_len = seq_len_k\n            self.cached_matrix = rel_distance\n        else:\n            rel_distance = self.cached_matrix\n\n\n        rel_distance=rel_distance.to(x.device)\n        # print(rel_distance)\n\n        # rel_distance = positions[:, None] - positions[None, :]\n\n        threshold = torch.abs(self.L_multiplier * self.init_L)\n        rel_distance_max=torch.max(torch.tril(rel_distance),dim=-1)[0]\n        # print(rel_distance_max)\n\n        pos_normalizer = torch.max(rel_distance_max, threshold)\n        # print(threshold)\n\n        pos_normalizer = pos_normalizer[:, None]\n\n\n        # self.c=self.c.to(rel_distance.device)\n        rel_distance = torch.log(torch.abs(self.c * rel_distance) + 1\n                                 )\n\n        pos_normalizer = torch.log(torch.abs(self.c * pos_normalizer) + 1) + self.eps\n\n        # Progressive interpolation\n\n        normalized_distance = rel_distance / pos_normalizer\n        normalized_distance=normalized_distance.to(x.dtype)\n        # print(normalized_distance)\n\n        fire_bias = self.mlp(normalized_distance.unsqueeze(-1))\n\n        fire_bias = fire_bias.unsqueeze(0).permute(0, 3, 1, 2)\n\n        x_fire_bias=torch.cat((x,torch.tile(fire_bias,(x.shape[0],1,1,1))),dim=1)\n        x_fire_bias=x_fire_bias.permute(0,2,3,1)\n        x_fire_bias=self.mlp2(x_fire_bias)\n        x_fire_bias = x_fire_bias.permute(0, 3, 1, 2)\n\n        return x + fire_bias+x_fire_bias",
    "description": null,
    "url": null
}