{
    "acronym": "megabyte",
    "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
    "seed_ids": [
        "templatent",
        "roformer",
        "compressivetransformer",
        "sparsetransformer",
        "gpt2"
    ],
    "s2id": "412e266cddfd87c79087a88ba1e4d11b89a45a13",
    "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
    "authors": [
        "L. Yu",
        "Daniel Simig",
        "Colin Flaherty",
        "Armen Aghajanyan",
        "Luke Zettlemoyer",
        "M. Lewis"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes, is proposed, establishing the viability of tokenization-free autoregressive sequence modeling at scale.",
    "citationCount": 58,
    "influentialCitationCount": 5,
    "code": "import math\nimport functools\nfrom itertools import zip_longest\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, reduce, repeat, pack, unpack\nfrom einops.layers.torch import Rearrange\n\nfrom beartype import beartype\nfrom beartype.typing import Tuple, Union\n\nfrom MEGABYTE_pytorch.attend import Attend\n\nfrom tqdm import tqdm\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef pack_one(t, pattern):\n    return pack([t], pattern)\n\ndef unpack_one(t, ps, pattern):\n    return unpack(t, ps, pattern)[0]\n\ndef remainder_to_mult(num, mult):\n    return (mult - num % mult) % mult\n\ndef cast_tuple(t, length = 1):\n    return t if isinstance(t, tuple) else ((t,) * length)\n\ndef reduce_mult(nums):\n    return functools.reduce(lambda x, y: x * y, nums, 1)\n\n# tensor helpers\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\ndef gumbel_noise(t):\n    noise = torch.zeros_like(t).uniform_(0, 1)\n    return -log(-log(noise))\n\ndef gumbel_sample(t, temperature = 1., dim = -1):\n    return ((t / temperature) + gumbel_noise(t)).argmax(dim = dim)\n\ndef top_k(logits, thres = 0.5):\n    num_logits = logits.shape[-1]\n    k = max(int((1 - thres) * num_logits), 1)\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float('-inf'))\n    probs.scatter_(1, ind, val)\n    return probs\n\n# token shift, from Peng et al of RWKV\n\ndef token_shift(t):\n    t, t_shift = t.chunk(2, dim = -1)\n    t_shift = F.pad(t_shift, (0, 0, 1, -1))\n    return torch.cat((t, t_shift), dim = -1)\n\n# rotary positional embedding\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim, theta = 10000):\n        super().__init__()\n        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    @property\n    def device(self):\n        return next(self.buffers()).device\n\n    def forward(self, seq_len):\n        t = torch.arange(seq_len, device = self.device).type_as(self.inv_freq)\n        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)\n        freqs = torch.cat((freqs, freqs), dim = -1)\n        return freqs\n\ndef rotate_half(x):\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(pos, t):\n    return t * pos.cos() + rotate_half(t) * pos.sin()\n\n# norm\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-8):\n        super().__init__()\n        self.scale = dim ** -0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        norm = torch.norm(x, dim = -1, keepdim = True) * self.scale\n        return x / norm.clamp(min = self.eps) * self.g\n\n# helper classes\n\ndef FeedForward(*, dim, mult = 4, dropout = 0.):\n    return nn.Sequential(\n        RMSNorm(dim),\n        nn.Linear(dim, dim * mult),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        nn.Linear(dim * mult, dim)\n    )\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.,\n        flash = False\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.attend = Attend(\n            causal = True,\n            flash = flash,\n            dropout = dropout\n        )\n\n        self.dropout = nn.Dropout(dropout)\n        self.norm = RMSNorm(dim)\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n\n    def forward(self, x, rotary_emb = None):\n        h, device = self.heads, x.device\n\n        x = self.norm(x)\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n\n        if exists(rotary_emb):\n            q, k = map(lambda t: apply_rotary_pos_emb(rotary_emb, t), (q, k))\n\n        out = self.attend(q, k, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        layers,\n        dim_head = 64,\n        heads = 8,\n        attn_dropout = 0.,\n        ff_dropout = 0.,\n        ff_mult = 4,\n        rel_pos = True,\n        flash_attn = False\n    ):\n        super().__init__()\n        self.rotary_emb = RotaryEmbedding(dim_head) if rel_pos else None\n        self.layers = nn.ModuleList([])\n\n        for _ in range(layers):\n            self.layers.append(nn.ModuleList([\n                Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, flash = flash_attn),\n                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)\n            ]))\n\n        self.norm = RMSNorm(dim)\n\n    def forward(self, x):\n        n = x.shape[-2]\n        rotary_emb = self.rotary_emb(n) if exists(self.rotary_emb) else None\n\n        for attn, ff in self.layers:\n            x = attn(token_shift(x), rotary_emb = rotary_emb) + x\n            x = ff(token_shift(x)) + x\n\n        return self.norm(x)\n\n# main class\n\nclass MEGABYTE(nn.Module):\n\n    @beartype\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim: Union[Tuple, int],\n        depth: Tuple,\n        max_seq_len: Tuple,\n        dim_head = 64,\n        heads = 8,\n        attn_dropout = 0.,\n        ff_mult = 4,\n        ff_dropout = 0.,\n        pad_id = 0,\n        rel_pos = False,\n        pos_emb = False,\n        flash_attn = False\n    ):\n        super().__init__()\n\n        # simplified configuration for each stage of the hierarchy\n        # depth = (2, 2, 4) would translate to depth 2 at first stage, depth 2 second stage, depth 4 third\n        # max_seq_len = (16, 8, 4) would translate to max sequence length of 16 at first stage, length of 8 at second stage, length of 4 for last\n\n        assert isinstance(depth, tuple) and isinstance(max_seq_len, tuple)\n        assert len(depth) == len(max_seq_len)\n\n        self.stages = len(depth)\n        dim = cast_tuple(dim, self.stages)\n\n        assert len(dim) == self.stages\n\n        coarsest_dim, *_, fine_dim = dim\n\n        self.max_seq_len = max_seq_len\n\n        self.start_tokens = nn.ParameterList([nn.Parameter(torch.randn(h_dim)) for h_dim, seq_len in zip(dim, max_seq_len)])\n        self.pos_embs = nn.ModuleList([nn.Embedding(seq_len, h_dim) for h_dim, seq_len in zip(dim, max_seq_len)]) if pos_emb else None\n\n        self.token_embs = nn.ModuleList([])\n\n        patch_size = 1\n        self.token_embs.append(nn.Embedding(num_tokens, fine_dim))\n\n        for dim_out, seq_len in zip(reversed(dim[:-1]), reversed(max_seq_len[1:])):\n            patch_size *= seq_len\n\n            self.token_embs.append(nn.Sequential(\n                nn.Embedding(num_tokens, fine_dim),\n                Rearrange('... r d -> ... (r d)'),\n                nn.LayerNorm(patch_size * fine_dim),\n                nn.Linear(patch_size * fine_dim, dim_out),\n                nn.LayerNorm(dim_out)\n            ))\n\n        self.transformers = nn.ModuleList([])\n        self.to_next_transformer_projections = nn.ModuleList([])\n\n        for h_dim, next_h_dim, stage_depth, next_seq_len in zip_longest(dim, dim[1:], depth, max_seq_len[1:]):\n            self.transformers.append(Transformer(\n                dim = h_dim,\n                layers = stage_depth,\n                dim_head = dim_head,\n                heads = heads,\n                attn_dropout = attn_dropout,\n                ff_dropout = ff_dropout,\n                ff_mult = ff_mult,\n                rel_pos = rel_pos,\n                flash_attn = flash_attn\n            ))\n\n            proj = nn.Identity()\n\n            if exists(next_h_dim) and next_h_dim != dim:\n                proj = nn.Sequential(\n                    Rearrange('b ... d -> b (...) d'),\n                    nn.Linear(h_dim, next_h_dim * next_seq_len),\n                    Rearrange('b m (n d) -> (b m) n d', n = next_seq_len)\n                )\n\n            self.to_next_transformer_projections.append(proj)\n\n        self.to_logits = nn.Linear(fine_dim, num_tokens)\n        self.pad_id = pad_id\n\n    def generate(self, prime = None, filter_thres = 0.9, temperature = 1., default_batch_size = 1):\n        total_seq_len = reduce_mult(self.max_seq_len)\n        device = next(self.parameters()).device\n\n        if not exists(prime):\n            prime = torch.empty((default_batch_size, 0), dtype = torch.long, device = device)\n\n        seq = prime\n        batch = seq.shape[0]\n\n        for _ in tqdm(range(total_seq_len - seq.shape[-1])):\n            logits = self.forward(seq)[:, -1]\n            logits = top_k(logits, thres = filter_thres)\n            sampled = gumbel_sample(logits, dim = -1, temperature = temperature)\n            seq = torch.cat((seq, rearrange(sampled, 'b -> b 1')), dim = -1)\n\n        return seq.reshape(batch, *self.max_seq_len)\n\n    def forward_empty(self, batch_size):\n        # take care of special case\n        # where you sample from input of 0 (start token only)\n\n        prev_stage_tokens_repr = None\n\n        for stage_start_tokens, transformer, proj in zip(self.start_tokens, self.transformers, self.to_next_transformer_projections):\n            tokens = repeat(stage_start_tokens, 'd -> b 1 d', b = batch_size)\n\n            if exists(prev_stage_tokens_repr):\n                tokens = tokens + prev_stage_tokens_repr[..., :tokens.shape[-2], :]\n\n            tokens = transformer(tokens)\n            prev_stage_tokens_repr = proj(tokens)\n\n        return self.to_logits(tokens)\n\n    def forward(self, ids, return_loss = False):\n        batch = ids.shape[0]\n\n        assert ids.ndim in {2, self.stages + 1}\n        flattened_dims = ids.ndim == 2\n        ids_orig_ndim = ids.ndim\n\n        if ids.numel() == 0:\n            return self.forward_empty(ids.shape[0])\n\n        if flattened_dims:\n            # allow for ids to be given in the shape of (batch, seq)\n            # in which case it will be auto-padded to the next nearest multiple of depth seq len\n            seq_len = ids.shape[-1]\n            multiple_of = reduce_mult(self.max_seq_len[1:])\n            padding = remainder_to_mult(seq_len, multiple_of)\n            ids = F.pad(ids, (0, padding), value = self.pad_id)\n            ids = ids.reshape(batch, -1, *self.max_seq_len[1:])\n\n        b, *prec_dims, device = *ids.shape, ids.device\n\n        # check some dimensions\n\n        assert prec_dims[0] <= self.max_seq_len[0], 'the first dimension of your axial autoregressive transformer must be less than the first tuple element of max_seq_len (like any autoregressive transformer)'\n        assert tuple(prec_dims[1:]) == tuple(self.max_seq_len[1:]), 'all subsequent dimensions must match exactly'\n\n        # get tokens for all hierarchical stages, reducing by appropriate dimensions\n        # and adding the absolute positional embeddings\n\n        tokens_at_stages = []\n        pos_embs = default(self.pos_embs, (None,))\n\n        for ind, pos_emb, token_emb in zip_longest(range(len(prec_dims)), pos_embs, self.token_embs):\n            is_first = ind == 0\n\n            tokens = token_emb(ids)\n\n            if exists(pos_emb):\n                positions = pos_emb(torch.arange(tokens.shape[-2], device = device))\n                tokens = tokens + positions\n\n            tokens_at_stages.insert(0, tokens)\n\n            if is_first:\n                continue\n\n            ids = rearrange(ids, '... m n -> ... (m n)')\n\n        # the un-pixelshuffled representations of the previous hierarchy, starts with None\n\n        prev_stage_tokens_repr = None\n\n        # spatial tokens is tokens with depth pos reduced along depth dimension + spatial positions        \n\n        for stage_start_tokens, stage_tokens, transformer, proj in zip(self.start_tokens, tokens_at_stages, self.transformers, self.to_next_transformer_projections):\n            stage_tokens, ps = pack_one(stage_tokens, '* n d')\n            stage_start_tokens = repeat(stage_start_tokens, 'f -> b 1 f', b = stage_tokens.shape[0])\n\n            # concat start token\n\n            stage_tokens = torch.cat((\n                stage_start_tokens,\n                stage_tokens,\n            ), dim = -2)\n\n            # sum the previous hierarchy's representation\n\n            if exists(prev_stage_tokens_repr):\n                prev_stage_tokens_repr = F.pad(prev_stage_tokens_repr, (0, 0, 1, 0), value = 0.)\n                stage_tokens = stage_tokens + prev_stage_tokens_repr\n\n            attended = transformer(stage_tokens)\n\n            attended = unpack_one(attended, ps, '* n d')\n\n            # project for next stage in the hierarchy\n\n            prev_stage_tokens_repr = proj(attended[..., :-1, :])\n\n        # project to logits\n\n        logits = self.to_logits(attended)\n\n        start_tokens = logits[(slice(None), *((0,) * (logits.ndim - 2)), slice(None))]\n        start_tokens = rearrange(start_tokens, 'b d -> b 1 d')\n\n        logits = logits[..., 1:, :]\n\n        if not return_loss:\n\n            if flattened_dims:\n                logits = rearrange(logits, 'b ... c -> b (...) c')\n                logits = logits[:, :seq_len]\n\n            return logits\n\n        logits = rearrange(logits, 'b ... c -> b (...) c')\n        logits = torch.cat((start_tokens, logits), dim = -2)\n\n        preds = rearrange(logits, 'b n c -> b c n')\n        labels = rearrange(ids, 'b ... -> b (...)')\n\n        loss = F.cross_entropy(\n            preds[..., :-1],\n            labels,\n            ignore_index = self.pad_id\n        )\n\n        return loss",
    "description": null,
    "url": null
}