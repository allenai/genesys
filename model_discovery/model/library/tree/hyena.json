{
    "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
    "acronym": "hyena",
    "s2id": "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
    "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": "This work proposes Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating, and sets a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets.",
    "citationCount": 163,
    "influentialCitationCount": 28,
    "seed_ids": [
        "flashconv",
        "sgconv",
        "gssm",
        "flashattn",
        "s4",
        "aft",
        "gpt3",
        "compressivetransformer",
        "butterfly"
    ],
    "code": "import math\n\nfrom re import U\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n\nfrom einops import rearrange, repeat\n\ntry:\n    from src.ops.fftconv import fftconv_ref, fftconv_func \nexcept ImportError:\n    fftconv_func = None\n\ntry:\n    from flash_attn.ops.fused_dense import FusedDense\nexcept ImportError:\n    FusedDense = None\n\nimport src.utils.registry as registry\nfrom src.utils.train import OptimModule\nfrom src.utils.config import instantiate, auto_assign_attrs\nfrom src.models.nn import Activation\n\n\n# reference convolution with residual connection\ndef fftconv_ref(u, k, D, dropout_mask, gelu=True, k_rev=None):\n    seqlen = u.shape[-1]\n    fft_size = 2 * seqlen\n    k_f = torch.fft.rfft(k, n=fft_size) / fft_size\n    if k_rev is not None:\n        k_rev_f = torch.fft.rfft(k_rev, n=fft_size) / fft_size\n        k_f = k_f + k_rev_f.conj()\n    u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size)\n    \n    if len(u.shape) > 3: k_f = k_f.unsqueeze(1)\n\n    y = torch.fft.irfft(u_f * k_f, n=fft_size, norm='forward')[..., :seqlen]\n\n    out = y + u * D.unsqueeze(-1)\n    if gelu:\n        out = F.gelu(out)\n    if dropout_mask is not None:\n        return (out * rearrange(dropout_mask, 'b H -> b H 1')).to(dtype=u.dtype)\n    else:\n        return out.to(dtype=u.dtype)\n\n\n@torch.jit.script \ndef mul_sum(q, y):\n    return (q * y).sum(dim=1)\n\n\nclass Sin(nn.Module):\n    def __init__(self, dim, w=10, train_freq=True):\n        super().__init__()\n        self.freq = nn.Parameter(w * torch.ones(1, dim)) if train_freq else w * torch.ones(1, dim)\n\n    def forward(self, x):\n        return torch.sin(self.freq * x)\n    \n    \nclass PositionalEmbedding(OptimModule):\n    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float=1e-5, **kwargs): \n        \"\"\"Complex exponential positional embeddings for Hyena filters.\"\"\"  \n        super().__init__()\n        \n        self.seq_len = seq_len\n        # The time embedding fed to the filteres is normalized so that t_f = 1\n        t = torch.linspace(0, 1, self.seq_len)[None, :, None] # 1, L, 1\n        \n        if emb_dim > 1:\n            bands = (emb_dim - 1) // 2            \n        # To compute the right embeddings we use the \"proper\" linspace \n        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None]\n        w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1 \n        \n        f = torch.linspace(1e-4, bands - 1, bands)[None, None] \n        z = torch.exp(-1j * f * w)\n        z = torch.cat([t, z.real, z.imag], dim=-1)\n        self.register(\"z\", z, lr=lr_pos_emb) \n        self.register(\"t\", t, lr=0.0)\n        \n    def forward(self, L):\n        return self.z[:, :L], self.t[:, :L]\n    \n\nclass ExponentialModulation(OptimModule):\n    def __init__(\n        self,\n        d_model,\n        fast_decay_pct=0.3,\n        slow_decay_pct=1.5,\n        target=1e-2,\n        modulation_lr=0.0,\n        modulate: bool=True,\n        shift: float = 0.0,\n        **kwargs\n    ):\n        super().__init__()\n        self.modulate = modulate\n        self.shift = shift\n        max_decay = math.log(target) / fast_decay_pct\n        min_decay = math.log(target) / slow_decay_pct\n        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None]\n        self.register(\"deltas\", deltas, lr=modulation_lr)\n        \n    def forward(self, t, x):\n        if self.modulate:\n            decay = torch.exp(-t * self.deltas.abs()) \n            x = x * (decay + self.shift)\n        return x                  \n\n\nclass HyenaFilter(OptimModule):\n    def __init__(\n            self, \n            d_model,\n            emb_dim=3, # dim of input to MLP, augments with positional encoding\n            order=16, # width of the implicit MLP \n            fused_fft_conv=False,\n            seq_len=1024, \n            lr=1e-3, \n            lr_pos_emb=1e-5,\n            dropout=0.0, \n            w=1, # frequency of periodic activations \n            wd=0, # weight decay of kernel parameters \n            bias=True,\n            num_inner_mlps=2,\n            normalized=False,\n            **kwargs\n        ):\n        \"\"\"\n        Implicit long filter with modulation.\n        \n        Args:\n            d_model: number of channels in the input\n            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands\n            order: width of the FFN\n            num_inner_mlps: number of inner linear layers inside filter MLP\n        \n        Note:\n            filter_dropout is not implemented\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.use_bias = bias\n        self.fused_fft_conv = fused_fft_conv\n        self.bias = nn.Parameter(torch.randn(self.d_model))\n        self.dropout = nn.Dropout(dropout)\n        \n        act = Sin(dim=order, w=w)\n        self.emb_dim = emb_dim\n        assert emb_dim % 2 != 0 and emb_dim >= 3, \"emb_dim must be odd and greater or equal to 3 (time, sine and cosine)\"\n        self.seq_len = seq_len\n  \n        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)\n\n        # uses a variable number of inner linear layers\n        self.implicit_filter = nn.Sequential(\n            nn.Linear(emb_dim, order),\n            act,\n        )\n        for i in range(num_inner_mlps):\n            self.implicit_filter.append(nn.Linear(order, order))\n            self.implicit_filter.append(act)\n        # final linear layer\n        self.implicit_filter.append(nn.Linear(order, d_model, bias=False))\n            \n        self.modulation = ExponentialModulation(d_model, **kwargs)\n        \n        self.normalized = normalized\n        for c in self.implicit_filter.children():\n            for name, v in c.state_dict().items():        \n                optim = {\"weight_decay\": wd, \"lr\": lr}\n                setattr(getattr(c, name), \"_optim\", optim)\n\n    def filter(self, L, *args, **kwargs):\n        z, t = self.pos_emb(L)\n        h = self.implicit_filter(z)\n\n        h = self.modulation(t, h)\n\n        if self.normalized: h = h / torch.norm(h, dim=-1, p=1, keepdim=True)\n\n        return h\n\n    def forward(self, x, L, k=None, bias=None, *args, **kwargs):\n        if k is None: k = self.filter(L)\n        \n        # Ensure compatibility with filters that return a tuple \n        k = k[0] if type(k) is tuple else k \n        if bias is None: bias = self.bias\n        bias = bias if self.use_bias else 0 * bias\n\n        if self.fused_fft_conv: \n            bias = bias.to(dtype=torch.float32)\n            y = fftconv_func(\n                x, k, bias, dropout_mask=None, gelu=False, \n                force_fp16_output=torch.is_autocast_enabled()\n            )\n        else:\n            y = fftconv_ref(x, k, bias, dropout_mask=None, gelu=False)\n\n        return y\n    \n    \nclass HyenaOperator(nn.Module):\n    def __init__(\n            self,\n            d_model,\n            l_max,\n            order=2, \n            filter_order=64,\n            num_heads=1, \n            inner_factor=1,\n            num_blocks=1, \n            fused_bias_fc=False,\n            outer_mixing=False,\n            dropout=0.0,  \n            filter_dropout=0.0, \n            filter_cls='hyena-filter',\n            post_order_ffn=False,\n            jit_filter=False, \n            short_filter_order=3, \n            activation=\"id\",\n            return_state=False,\n            **filter_args,\n        ):\n        r\"\"\"\n        Hyena operator described in the paper https://arxiv.org/pdf/2302.10866.pdf\n        \n        Args:\n            d_model (int): Dimension of the input and output embeddings (width of the layer)\n            l_max: (int): Maximum input sequence length. Defaults to None\n            order: (int): Depth of the Hyena recurrence. Defaults to 2\n            filter_order: (int): Width of the FFN parametrizing the implicit filter. Defaults to 64\n            num_heads: (int): Number of heads. Defaults to 1\n            inner_factor: (int): Width multiplier. Defaults to 1\n            num_blocks: (int): Number of blocks in sequence length. Defaults to 1\n            fused_bias_fc: (bool): Whether to use fused bias FC. Defaults to False\n            dropout: (float): Dropout probability. Defaults to 0.0\n            filter_dropout: (float): Dropout probability for the filter. Defaults to 0.0\n            post_order_ffn: (bool): Apply a dense layer between steps of the recurrence. Defaults to False\n            jit_filter: (bool): Whether JIT the implicit filter function. Defaults to False\n            short_filter_order: (int): Length of the explicit input convolutional filter. Defaults to 3\n            activation: (str): type of act between kernel output and FF (default identity)\n            return_state: (bool): whether to return a state\n        \"\"\"\n        super().__init__()\n        assert d_model % num_heads == 0, f'Model dimension {d_model} must be divisible by num heads {num_heads}'\n        assert l_max % num_blocks == 0, f'Maximum signal length {l_max} must be divisible by block dimension {num_blocks}'\n        block_dim = l_max // num_blocks\n        head_dim = d_model // num_heads\n        \n        auto_assign_attrs(\n            self, d_model=d_model, order=order, l_max=l_max, num_heads=num_heads, inner_factor=inner_factor, \n            block_dim=block_dim, head_dim=head_dim, filter_order=filter_order, post_order_ffn=post_order_ffn,\n            short_filter_order=short_filter_order, num_blocks = num_blocks, filter_dropout=filter_dropout,\n            jit_filter=jit_filter, outer_mixing=outer_mixing, activation=activation, return_state=return_state,\n        )\n        self.activation = Activation(activation)\n        self.dropout = nn.Dropout(dropout)\n        self.setup_projections(fused_bias_fc, inner_factor)\n        self.setup_filters(filter_cls, filter_args)\n\n\n    def setup_projections(self, fused_bias_fc, inner_factor):\n        \"Initializes input and output projections (over the width dimension)\"\n        if fused_bias_fc and FusedDense is None:\n            raise ImportError('fused_dense is not installed')\n        linear_cls = nn.Linear if not fused_bias_fc else FusedDense\n        self.out_proj = linear_cls(self.d_model * inner_factor, self.d_model)\n        self.in_proj = linear_cls(self.d_model, (self.order + 1) * self.d_model) \n        if self.post_order_ffn:   \n            self.ord_proj_w = nn.Parameter(torch.randn(self.order, self.num_heads, self.num_heads) / math.sqrt(self.head_dim))\n            \n            \n    def setup_filters(self, filter_cls, filter_args):   \n        \"Initializes the explicit and implicit filters\"\n        assert self.order >= 2, f'Order must be at least 2, (got {self.order})'\n        total_width = self.d_model * self.inner_factor * (self.order + 1)\n        \n        self.short_filter = nn.Conv1d(\n            in_channels=total_width, \n            out_channels=total_width, \n            kernel_size=self.short_filter_order, \n            groups=total_width, \n            padding=self.short_filter_order - 1\n        )\n        \n        filter_cls = instantiate(registry.layer, filter_cls, partial=True)\n                    \n        self.filter_fn = filter_cls(\n            self.head_dim * self.inner_factor * (self.order - 1), \n            order=self.filter_order, \n            seq_len=self.l_max,\n            channels=1, \n            dropout=self.filter_dropout, \n            **filter_args\n        ) \n        if self.jit_filter: self.filter_fn = torch.jit.script(self.filter_fn, self.L)\n\n    def recurrence(self, u , state):\n        \"Fast inference mode via distilled recurrence\"\n        raise NotImplementedError(\"Working on it!\")\n    \n    def forward(self, u, *args, **kwargs):\n        l = u.size(-2)\n        l_filter = min(l, self.l_max)\n        u = self.in_proj(u)\n        u = rearrange(u, 'b l d -> b d l')\n        \n        uc = self.short_filter(u)[...,:l_filter] \n        \n        uc = rearrange(uc, 'b (ho v) (z l) -> b ho v z l', \n            z=self.num_blocks, \n            ho=self.num_heads, \n            v=self.head_dim * (self.order + 1)\n        )\n\n        *x, v = uc.split(self.d_model, dim=2)\n        k = self.filter_fn.filter(l_filter)\n        \n        # `c` is always 1 by default\n        k = rearrange(k, 'c l (v o) -> c o v l', v=self.head_dim, o=self.order - 1)[0]\n        \n        bias = rearrange(self.filter_fn.bias, '(v o) -> o v', v=self.head_dim, o=self.order - 1)\n\n        for o, x_i in enumerate(reversed(x[1:])):\n            if self.outer_mixing:\n                v = rearrange(v, 'b h v z l -> b h 1 v z l') \n                v = self.dropout(\n                    v * rearrange(x_i, 'b h v z l -> b h v 1 z l') \n                )\n                v = v.sum(dim=2)\n            else:\n                v = self.dropout(v * x_i)\n\n            # the bias term is broadcasted. Last dimension (l) is handled by fftconv\n            v = self.filter_fn(v, l_filter, k=k[o], bias=bias[o, None, :, None])\n            \n            if self.post_order_ffn: \n                w = self.ord_proj_w[o]\n                v = mul_sum(\n                    rearrange(w, 'h1 h2 -> 1 h1 h2 1 1 1'), rearrange(v, 'b h v z l -> b h 1 v z l')\n                )\n\n        y = self.activation(rearrange(v * x[0], 'b h v z l -> b (z l) (h v)', z=self.num_blocks, h=self.num_heads))\n        y = self.out_proj(y)\n        \n        if self.return_state:\n            return y, None\n        return y\n\n    @property\n    def d_output(self):\n        return self.d_model"
}