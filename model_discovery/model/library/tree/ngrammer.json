{
    "acronym": "ngrammer",
    "title": "N-grammer: Augmenting Transformers with latent n-grams",
    "seed_ids": [
        "gpt3",
        "productkeymem",
        "transformer"
    ],
    "s2id": "6ac1fccf1e04487d439ee598f51c03ddac5144ca",
    "abstract": "Transformer models have recently emerged as one of the foundational models in natural language processing, and as a byproduct, there is significant recent interest and investment in scaling these models. However, the training and inference costs of these large Transformer language models are prohibitive, thus necessitating more research in identifying more efficient variants. In this work, we propose a simple yet effective modification to the Transformer architecture inspired by the literature in statistical language modeling, by augmenting the model with n-grams that are constructed from a discrete latent representation of the text sequence. We evaluate our model, the N-Grammer on language modeling on the C4 data-set as well as text classification on the SuperGLUE data-set, and find that it outperforms several strong baselines such as the Transformer and the Primer. We open-source our model for reproducibility purposes in Jax.",
    "authors": [
        "Aurko Roy",
        "Rohan Anil",
        "Guangda Lai",
        "Benjamin Lee",
        "Jeffrey Zhao",
        "Shuyuan Zhang",
        "Shibo Wang",
        "Ye Zhang",
        "Shen Wu",
        "Rigel Swavely",
        "Tao Yu",
        "Phuong Dao",
        "Christopher Fifty",
        "Z. Chen",
        "Yonghui Wu"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": "The N-Grammer is proposed, a simple yet effective modification to the Transformer architecture inspired by the literature in statistical language modeling, by augmenting the model with n-grams that are constructed from a discrete latent representation of the text sequence.",
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": "import flax.linen as nn\nimport jax.numpy as jnp\nimport jax\nfrom jax.numpy import einsum\nfrom einops import rearrange\nfrom typing import Callable\nimport sympy\n\n\n\ndefault_init = nn.initializers.normal(1.0)\n\n\ndef get_bigram_ids(ids, vocab_size, segment_pos = None):\n    \"\"\"Generate bi-gram ids from uni-gram ids.\n    \n    Args:\n      ids: uni-gram cluster ids, dtype int32, shape [B, L, H].\n      vocab_size: Vocabulary size of `ids`, must be > 0.\n      segment_pos: If not None (meaning `ids` is packed, i.e. each example\n      containing multiple segments), a tensor of shape [B, L], containing\n      the position of each id in `ids` in a segment.\n    \n    Returns:\n      ngram_ids: dtype int64, shape [B, L, H].\n    \n    \"\"\"\n    assert vocab_size > 0, \"vocab_size has to be greater than 0\"\n    batch_size = ids.shape[0]\n    num_heads = ids.shape[-1]\n    # Cast to int64 to avoid overflow (only if it is enabled)\n    ids = jnp.array(ids, dtype=jnp.int64)  # [batch, seq, heads]\n    pad = jnp.zeros([batch_size, 1, num_heads], dtype=ids.dtype)  # [batch, 1, heads]\n    \n    #   bigram_id = original_id + shifted_id * vocab_size.\n    ids_0 = jnp.concatenate([ids, pad], 1)  # [batch, seq+1, heads]\n    ids_1 = jnp.concatenate([pad, ids], 1)  # [batch, seq+1, heads]\n    if segment_pos is not None:\n        mask = jnp.array(jnp.equal(segment_pos, 0), dtype=ids.dtype) #[batch, seq]\n        mask = 1 - mask\n        mask = mask[:,:,jnp.newaxis] # [batch, seq, 1]\n        mask = jnp.concatenate([mask, jnp.zeros([batch_size, 1, 1], dtype=ids.dtype)],1)\n        ids_1 *= mask \n\n\n    ngram_ids = ids_0 + ids_1 * vocab_size  # Bigram ids.\n    ngram_ids = ngram_ids[:, 0:-1]\n    return ngram_ids\n\n\n\n\nclass ProductQuantization(nn.Module):\n    \"\"\"Implements the Product Quantization layer\n\n  use the following capital letters to denote shape parameters:\n    B = batch size\n    L = length of the input sequence \n    H = number of attention heads\n    D = dimensions of each attention head\n    K = number of clusters\n  \"\"\"\n\n    num_clusters: int\n    num_heads: int\n    dim_per_head: int\n    decay : float = 0.999\n    epsilon: float = 1e-6\n    mean_init: Callable = default_init\n\n    @nn.compact\n    def __call__(self, x, train: bool = True):\n        \n        initializing = self.is_mutable_collection('params')\n        # means is the k-mean of the mini-batch\n        if initializing:\n          key = self.make_rng('batch_stats')\n          means = self.variable(\n              'batch_stats', 'means', self.mean_init, \n              key,(self.num_heads, self.num_clusters, self.dim_per_head))\n        else:\n          means = self.variable('batch_stats','means', None)\n        \n        # compute distances of the input to all centroids \n        # [B,L,H,K]\n        dists = -2 * jnp.einsum('B L H D, H K D -> B L H K', x, means.value)\n        # [B,L,H,1]\n        x_norm_sq = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n        # [H,K]\n        means_norm_sq = jnp.sum(jnp.square(means.value), axis=-1, keepdims=False)\n        # [1, 1, H, K]\n        means_norm_sq = means_norm_sq[jnp.newaxis, jnp.newaxis, :, :]\n        # [B,L,H,K]\n        dists += x_norm_sq + means_norm_sq\n\n        #  find the nearest centroids id for each input vector\n        # [B,L,H]\n        cluster_ids = jnp.argmin(dists, axis=-1)\n      \n        \n        if train and not initializing:\n          # [B,L,H,K]\n          nearest_one_hot = jax.nn.one_hot(cluster_ids, self.num_clusters)\n          per_cluster_count = jnp.sum(nearest_one_hot, axis=[0, 1])\n          sum_x = jnp.einsum('B L H K, B L H D -> H K D', nearest_one_hot,x)\n          # means_x is the average over all the input vectors closest to this centroid.\n          means_x = sum_x / (self.epsilon + jnp.expand_dims(per_cluster_count, axis=-1))\n          # exponential moving average\n          new_means = (1 - self.decay) * means_x + self.decay * means.value\n          # update the means \n          means.value = new_means\n\n        return(cluster_ids)\n\nclass Ngrammer(nn.Module):\n    \"\"\"Augments the input embeddings with VQ n-gram layer embeddings.\n\n    Attributes:\n      unigram_vocab_size: Size of the unigram vocabulary, i.e. number of unique centeroids \n      dim_per_head: The dimension per each head of the input\n      num_heads: Number of attention heads\n      ngram_emb_dim: Size of the ngram dimension per head\n      ngram_vocab_size : Size of the ngram vocabulary\n      concat_ngrams:If True, then concat ngrams and unigram, otherwise add\n      embed_init: initializer function for ngram embedding layer \n    \"\"\"\n    unigram_vocab_size: int\n    dim_per_head: int\n    num_heads:int  = 1\n    ngram_emb_dim: int = 8\n    ngram_vocab_size :int = 768 * 256\n    concat_ngrams: bool = True\n    embed_init: Callable = default_init\n\n\n    @nn.compact\n    def __call__(self, ids, x, mask = None, segment_pos = None):\n        \"\"\"\n        Args:\n          ids: Input unigram id tensor of shape  [B, L, H].\n          x: Input unigram embedding tensor of shape [B, L, H, D] to which to add the ngram embedding.\n      \n        Returns:\n          out: output with ngram embedding added of shape [B, L, H * D]\n\n        \"\"\"\n        if self.concat_ngrams:\n          # The ngram_emb_dim must be smaller than dim_per_head.\n           assert self.ngram_emb_dim <= self.dim_per_head\n        else:\n         # If not concatenating ngram embeddings, check the dims are compatible.\n           assert self.ngram_emb_dim == self.dim_per_head\n\n        ngram_cluster_ids = get_bigram_ids(ids, self.unigram_vocab_size, segment_pos)\n        \n        #  multi-way hash ids \n        primes = list(sympy.primerange(self.ngram_vocab_size + 1,2 * self.ngram_vocab_size))[0:self.num_heads]\n        primes = jnp.array(primes)[jnp.newaxis,jnp.newaxis,:]\n        \n        head_range = jnp.arange(self.num_heads)[jnp.newaxis,jnp.newaxis,:]\n\n        def _multi_way_hash_ids(x, a, b, prime, buckets):\n            return ((x * a + b) % prime) % buckets\n\n        ngram_ids = _multi_way_hash_ids(ngram_cluster_ids, head_range+1, head_range+1, primes, self.ngram_vocab_size)\n        \n        # shift vocab range for each head appropriately by the head number\n        ngram_ids = ngram_ids + (self.ngram_vocab_size * head_range)\n\n        ngram_embed = nn.Embed(self.ngram_vocab_size * self.num_heads, self.ngram_emb_dim, embedding_init = self.embed_init)\n        \n        y = ngram_embed(ngram_ids)\n        \n        normed_x = nn.LayerNorm(epsilon = 1e-5, reduction_axes=-1, feature_axes=(-2,-1))(x)\n        normed_y = nn.LayerNorm(epsilon = 1e-5, reduction_axes=-1, feature_axes=(-2,-1))(y)\n        \n        input_sliced_dim = normed_x.shape[-1] - normed_y.shape[-1]\n        out = jnp.concatenate((\n                normed_x[..., :input_sliced_dim],\n                normed_y), axis = -1)\n        \n        out = rearrange(out, 'b n ... -> b n (...)')\n\n        # mask if needed\n\n        if mask is not None:\n            out = out * mask[:,:,jnp.newaxis]\n        \n\n        return(out)\n\n    \n\nclass PQNgrammer(nn.Module):\n    \"\"\"Implements a PQ based ngrammer layer which looks up latent ngram id.\n\n    We use the following capital letters to denote shape parameters:\n    B = batch size\n    L = length of the input sequence (referred to as S or T elsewhere)\n    H = number of attention heads\n    D = dimensions of each attention head\n    K = number of clusters\n    \"\"\"\n    num_clusters: int \n    num_heads: int \n    dim_per_head: int \n    ngram_vocab_size :int = 768 * 256\n    ngram_emb_dim: int = 8\n    decay : float = 0.999\n    epsilon : float = 1e-6\n    concat_ngrams: bool = True\n    mean_init: Callable = default_init\n    embed_init: Callable = default_init\n    \n    @nn.compact\n    def __call__(self, x, train: bool = True,  mask = None, segment_pos = None):\n\n        assert x.shape[-1] == (self.num_heads * self.dim_per_head), f'input embedding feature dimension must be {self.num_heads * self.dim_per_head}'\n        \n        x = rearrange(x, 'B L (H D) -> B L H D', H = self.num_heads)\n        \n        cluster_ids = ProductQuantization(\n            num_clusters = self.num_clusters,\n            num_heads = self.num_heads,\n            dim_per_head = self.dim_per_head,\n            decay = self.decay,\n            epsilon = self.epsilon,\n            mean_init = self.mean_init)(x, train)\n \n        output_embs = Ngrammer(\n            unigram_vocab_size = self.num_clusters,\n            dim_per_head = self.dim_per_head,\n            num_heads = self.num_heads,\n            ngram_emb_dim = self.ngram_emb_dim,\n            ngram_vocab_size  = self.ngram_vocab_size,\n            concat_ngrams = self.concat_ngrams,\n            embed_init = self.embed_init)(cluster_ids,x, mask, segment_pos)\n\n\n        return(output_embs)\n        \n        ",
    "description": null,
    "url": null
}