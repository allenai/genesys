{
    "acronym": "neurocache",
    "title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling",
    "seed_ids": [
        "unlimiformer",
        "gpt3",
        "sparsetransformer",
        "transformerxl",
        "transformer"
    ],
    "s2id": "0ea22780c42536f488c097431c037aa45aac0b2d",
    "abstract": "This paper introduces Neurocache, an approach to extend the effective context size of large language models (LLMs) using an external vector cache to store its past states. Like recent vector retrieval approaches, Neurocache uses an efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states and incorporate them into the attention process. Neurocache improves upon previous methods by (1) storing compressed states, which reduces cache size; (2) performing a single retrieval operation per token which increases inference speed; and (3) extending the retrieval window to neighboring states, which improves both language modeling and downstream task accuracy. Our experiments show the effectiveness of Neurocache both for models trained from scratch and for pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the cache mechanism. We also compare Neurocache with text retrieval methods and show improvements in single-document question-answering and few-shot learning tasks. We made the source code available under: https://github.com/alisafaya/neurocache",
    "authors": [
        "Ali Safaya",
        "Deniz Yuret"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "tldr": "Neurocache is an approach to extend the effective context size of large language models (LLMs) using an external vector cache to store its past states using an efficient k-nearest-neighbor algorithm to retrieve relevant past states and incorporate them into the attention process.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "from __future__ import annotations\n\nimport functools\n\nimport torch\nfrom torch import nn\nfrom transformers import PretrainedConfig, PreTrainedModel\n\nfrom neurocache.utils import COMMON_MODULE_NAMES, NeurocacheType\n\nfrom .cache import BatchedCache, OnDeviceCache, OnDeviceCacheConfig\nfrom .config import NeurocacheConfig\n\n\nArray = torch.Tensor\n\n\ndef get_attribute(object, attr_name):\n    for option in COMMON_MODULE_NAMES[attr_name]:\n        if hasattr(object, option):\n            return getattr(object, option)\n    raise AttributeError(\"Cannot infer attribute. Options: {}\".format(attr_name))\n\n\ndef repeat_kv(kv, n_groups):\n    bsz, seq_len, num_kv_heads, n_neighbors, head_dim = kv.shape\n    if n_groups == 1:\n        return kv\n    kv = kv[..., None, :, :].expand(bsz, seq_len, num_kv_heads, n_groups, n_neighbors, head_dim)\n    return kv.reshape(bsz, seq_len, -1, n_neighbors, head_dim)\n\n\nclass CacheAttention(nn.Module):\n    \"\"\"\n    Wrapper that adds cache attention to a layer.\n\n    Decoder Layer: (\n        ...\n        self_attn\n        cache_attn\n        layer_norm\n        ...\n    )\n\n    Args:\n        config: Configuration object containing the parameters of model.\n        base_layer: Layer to be wrapped.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_decoder_layer: nn.Module,\n        config: NeurocacheConfig,\n        base_config: PretrainedConfig,\n    ):\n        super().__init__()\n        self.config = config\n\n        try:\n            # check if the model supports grouped query attention\n            self.num_kv_heads = get_attribute(base_config, \"num_kv_heads\")\n        except AttributeError:\n            self.num_kv_heads = get_attribute(base_config, \"num_heads\")\n\n        self.num_heads = get_attribute(base_config, \"num_heads\")\n        self.num_key_value_groups = self.num_heads // self.num_kv_heads\n        self.hidden_size = get_attribute(base_config, \"hidden_size\")\n\n        self.head_dim = self.hidden_size // self.num_heads\n        self.scaler = self.head_dim**-0.5\n        self.r_hidden_size = self.hidden_size // self.config.compression_factor\n\n        self.k_proj = nn.Linear(self.r_hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.r_hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.apply(self._weight_init)\n\n    @torch.no_grad()\n    def _weight_init(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.trunc_normal_(module.weight, mean=0.0, std=module.in_features**-0.5)\n\n    @torch.no_grad()\n    def _init_proj_weights(self, self_attn_module):\n        \"\"\"Initialize projection weights with self attention weights.\"\"\"\n        self.q_proj.weight.data = get_attribute(self_attn_module, \"q_proj\").weight.data.clone()\n        self.o_proj.weight.data = get_attribute(self_attn_module, \"o_proj\").weight.data.clone()\n\n        if self.config.compression_factor == 1:\n            # copy the weights\n            self.k_proj.weight.data = get_attribute(self_attn_module, \"k_proj\").weight.data.clone()\n            self.v_proj.weight.data = get_attribute(self_attn_module, \"v_proj\").weight.data.clone()\n        else:\n            # slice the weights\n            self.k_proj.weight.data = (\n                get_attribute(self_attn_module, \"k_proj\").weight[:, : self.r_hidden_size].clone()\n            )\n            self.v_proj.weight.data = (\n                get_attribute(self_attn_module, \"v_proj\").weight[:, : self.r_hidden_size].clone()\n            )\n\n    def ext_attention(\n        self,\n        external_keys: Array,\n        external_values: Array,\n        queries: Array,\n    ) -> Array:\n        \"\"\"Attention over (keys, values) retrieved from cache.\n\n        Args:\n            external_keys: per-query keys from cache, of shape\n                [batch_size, num_queries, num_heads, num_neighbors, head_size]\n            external_values: per-query values from cache, of shape\n                [batch_size, num_queries, num_heads, num_neighbors, head_size]\n            queries: current queries, of shape:\n                [batch_size, num_queries, num_heads, head_size]\n\n        Returns:\n            Attention outputs of shape [batch_size, num_queries, num_heads, head_size]\n        \"\"\"\n        assert external_keys.ndim == 5\n        assert queries.ndim == 4\n        assert external_values.shape == external_keys.shape\n\n        # Compute attention weights.\n        ext_attn = torch.einsum(\"...qhd,...qhid->...hqi\", queries, external_keys)\n        if ext_attn.dtype == torch.float16:\n            ext_attn = nn.functional.softmax(ext_attn, dim=-1, dtype=torch.float32).to(\n                torch.float16\n            )\n        else:\n            ext_attn = nn.functional.softmax(ext_attn, dim=-1)\n\n        # Compute weighted sum of values.\n        attn_output = torch.einsum(\"...hqi,...qhid->...qhd\", ext_attn, external_values)\n        return attn_output\n\n    def forward(self, hidden_states, retrieved_hidden_states, cached_key_values=None):\n        \"\"\"Attention over retrieved cached states.\n            1. Project hidden states into queries.\n            2. Project cached states into keys and values.\n            3. Repeat neighbors for each query if context size > 1.\n            4. Compute attention over keys and values.\n            5. Project attention outputs back to hidden states.\n\n        Args:\n            hidden_states: output of the previous layer.\n                [batch_size, seq_len, hidden_size]\n            retrieved_hidden_states: hidden states retrieved from cache.\n                [batch_size, seq_len, num_neighbors, hidden_size]\n            cached_key_values: cached kv from previous passes for generation.\n                [batch_size, context_size - 1, num_kv_heads, num_neighbors, head_dim]\n            use_cache: whether to use key values for generation.\n        \"\"\"\n        assert retrieved_hidden_states.shape[-1] == self.r_hidden_size\n        assert hidden_states.shape[-1] == self.hidden_size\n        batch_size, seq_len, hidden_size = hidden_states.shape\n        _, _, num_neighbors, r_hidden_size = retrieved_hidden_states.shape\n\n        # Project hidden states into queries.\n        queries = self.q_proj(hidden_states) * self.scaler\n        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # Project external states into keys and values.\n        keys = self.k_proj(retrieved_hidden_states)\n        keys = keys.view(batch_size, seq_len, self.num_kv_heads, num_neighbors, self.head_dim)\n        values = self.v_proj(retrieved_hidden_states)\n        values = values.view(batch_size, seq_len, self.num_kv_heads, num_neighbors, self.head_dim)\n\n        def repeat_neighbors(neighbors: Array, ctx_size: int):\n            assert neighbors.ndim == 5\n            # Allows access to previous tokens neighbors\n            # i-th token will have the neighbors of all tokens from (i - c) to i.\n            # We clip so the first tokens do not have negative indices\n            num_kv = neighbors.shape[1]\n            n_heads = neighbors.shape[2]\n            context = (\n                torch.arange(num_kv)[:, None] + torch.arange(-ctx_size + 1, 1)[None, :]\n            ).clamp_(0, num_kv - 1)\n\n            neighbors = neighbors.moveaxis(1, 2)\n            neighbors = neighbors[:, :, context]\n            neighbors = neighbors.reshape(batch_size, n_heads, num_kv, -1, self.head_dim)\n            neighbors = neighbors.moveaxis(1, 2)\n            return neighbors\n\n        # Repeat neighbors for each query.\n        if self.config.context_size > 1:\n            if cached_key_values is not None:\n                keys = torch.cat([cached_key_values[2], keys], dim=1)\n                values = torch.cat([cached_key_values[3], values], dim=1)\n\n            cached_key_values = (\n                keys[:, -self.config.context_size + 1 :],\n                values[:, -self.config.context_size + 1 :],\n            )\n            keys = repeat_neighbors(keys, self.config.context_size)\n            values = repeat_neighbors(values, self.config.context_size)\n\n            if keys.shape[1] > seq_len:\n                keys = keys[:, -seq_len:]\n                values = values[:, -seq_len:]\n        else:\n            cached_key_values = (None, None)\n\n        keys = repeat_kv(keys, self.num_key_value_groups)\n        values = repeat_kv(values, self.num_key_value_groups)\n\n        # Compute attention over keys and values.\n        attn_output = self.ext_attention(keys, values, queries)\n\n        # Project attention outputs back to hidden states.\n        attn_output = attn_output.view(batch_size, seq_len, hidden_size)\n        attn_output = self.o_proj(attn_output)\n        return attn_output, cached_key_values\n\n\nclass Neurocache(nn.Module):\n    \"\"\"\n    Wrapper that handles cache retrieval, attention, and cache updates.\n\n    Args:\n        base_model: model to be wrapped with cache.\n        config: Configuration object containing the parameters of model.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_model: PreTrainedModel,\n        config: OnDeviceCacheConfig,\n    ):\n        super().__init__()\n        self.config = config\n        self.base_model_config = base_model.config\n        self.hidden_size = get_attribute(base_model.config, \"hidden_size\")\n\n        if self.config.compression_factor > 1:\n            # project hidden states to lower dimension and normalize\n            def _weight_init(m):\n                if isinstance(m, nn.Linear):\n                    nn.init.trunc_normal_(m.weight, mean=0.0, std=m.in_features**-0.5)\n\n            self.cache_dim = self.hidden_size // self.config.compression_factor\n            self.h_proj = nn.Linear(self.hidden_size, self.cache_dim, bias=False)\n            self.h_proj.apply(_weight_init)\n            self.h_norm = nn.LayerNorm(self.cache_dim)\n        else:\n            # no-op\n            self.h_proj = nn.Identity()\n            self.h_norm = nn.Identity()\n            self.cache_dim = self.hidden_size\n\n        try:\n            self.cache_dtype = getattr(torch, config.cache_dtype)\n        except AttributeError:\n            raise ValueError(\n                \"\"\"Invalid cache dtype: {}, available options are float16, bfloat16, and float32\"\"\".format(\n                    config.cache_dtype\n                )\n            )\n\n        self.retrieval_state = {}\n        self.cache_attns = nn.ModuleList([])\n        self.caches = nn.ModuleList([])\n        self.hooks = self._register_hooks(base_model)\n        self.enabled = True\n        self.update_cache = True\n\n    def enable(self):\n        \"\"\"Enable cache.\"\"\"\n        self.enabled = True\n\n    def disable(self):\n        \"\"\"Disable cache.\"\"\"\n        self.enabled = False\n\n    def _remove_hooks(self):\n        for hook in self.hooks:\n            hook.remove()\n        self.caches = nn.ModuleList([])\n        self.cache_attns = nn.ModuleList([])\n        self.hooks = []\n\n    def should_reencode(self):\n        \"\"\"Check if retrieved cache should be re-encoded.\"\"\"\n        return (\n            not self.config.inference_mode\n            and self.training\n            and self.config.neurocache_type == NeurocacheType.ONDEVICE\n            and self.config.compression_factor > 1\n        )\n\n    def disable_update(self):\n        \"\"\"Disable cache update.\"\"\"\n        self.update_cache = False\n\n    def enable_update(self):\n        \"\"\"Enable cache update.\"\"\"\n        self.update_cache = True\n\n    def reinitialize_cache(self):\n        \"\"\"Reinitialize cache.\n        This can be used to change the batch size of the cache.\n        \"\"\"\n        self.retrieval_state = {}\n        for cache in self.caches:\n            cache.wrapped = None\n\n    def infer_layers(self, base_model: PreTrainedModel):\n        \"\"\"Infer the layers list of the base model.\"\"\"\n        if hasattr(base_model, \"base_model\"):\n            base_model = base_model.model\n\n        if hasattr(base_model, \"model\"):\n            base_model = base_model.model\n\n        if hasattr(base_model, \"decoder\"):\n            base_model = base_model.decoder\n\n        try:\n            layers = get_attribute(base_model, \"layers\")\n            num_layers = len(layers)\n        except AttributeError:\n            raise AttributeError(\"Cannot infer decoder layers.\")\n\n        return layers, num_layers\n\n    def get_layer_indices(self, num_layers: int):\n        \"\"\"Get the cache, retrieval, and attention layers indices.\"\"\"\n\n        if self.config.retrieval_map is None or len(self.config.retrieval_map) == 0:\n            cache_idx = num_layers * 3 // 4\n            retrieval_map = {cache_idx: cache_idx}\n        else:\n            retrieval_map = self.config.retrieval_map\n\n        if self.config.attention_layers is None or len(self.config.attention_layers) == 0:\n            attention_layers = list(range(min(retrieval_map.keys()), num_layers))\n            self.config.attention_layers = attention_layers\n        else:\n            attention_layers = self.config.attention_layers\n\n        # Check that cache layers are before attention layers\n        assert min(attention_layers) >= min(\n            retrieval_map.keys()\n        ), \"Attention layers must be greater than or equal to retrieval layers\"\n        return retrieval_map, sorted(set(attention_layers))\n\n    def _register_hooks(self, base_model: PreTrainedModel):\n        \"\"\"Register hooks for cache retrieval, attention, and cache updates.\"\"\"\n        layers, num_layers = self.infer_layers(base_model)\n        retrieval_map, attention_layers = self.get_layer_indices(num_layers)\n\n        # Initialize cache attention layers\n        for idx in attention_layers:\n            self.cache_attns.append(\n                CacheAttention(layers[idx], self.config, self.base_model_config)\n            )\n\n        layer_idx2cache_idx = {v: k for k, v in enumerate(sorted(set(retrieval_map.values())))}\n\n        # Register retrieval hooks\n        hooks = []\n        for retrieval_idx, cachelayer_idx in retrieval_map.items():\n            hook = get_attribute(layers[retrieval_idx], \"self_attn\").register_forward_hook(\n                functools.partial(self.retrieve_hook, idx=layer_idx2cache_idx[cachelayer_idx]),\n                prepend=True,\n                with_kwargs=True,\n            )\n            hooks.append(hook)\n\n        # Register cache updates hooks\n        split_dims = () if self.config.global_cache else (0,)\n        for layer_idx, cache_idx in layer_idx2cache_idx.items():\n            # Create batched caches as many as the number of unique cache indices\n            # We initialize the cache in the first forward pass to infer batch size\n            # and start retrieval in the next passes.\n            self.caches.append(BatchedCache(None, split_dims))\n\n            hook = get_attribute(layers[layer_idx], \"self_attn\").register_forward_hook(\n                functools.partial(self.update_hook, idx=cache_idx), with_kwargs=True\n            )\n            hooks.append(hook)\n\n        # Register cache attention hooks\n        for i, idx in enumerate(attention_layers):\n            hook = get_attribute(layers[idx], \"self_attn\").register_forward_hook(\n                functools.partial(self.attention_hook, idx=i), with_kwargs=True\n            )\n            hooks.append(hook)\n        return hooks\n\n    def reset_cache(self, start_of_sequence: Array):\n        \"\"\"Reset cache at the beginning of a sequence.\"\"\"\n        if not self.config.global_cache:\n            for cache in self.caches:\n                if cache.wrapped is not None:\n                    cache.reset(start_of_sequence)\n\n    @torch.no_grad()\n    def retrieve_topk(self, phs: Array, input_mask: Array, idx: int):\n        \"\"\"Retrieve topk neighbors from cache.\"\"\"\n        return self.caches[idx].topk_retrieval(phs, input_mask, self.config.topk)\n\n    def attention_hook(self, module, args, kwargs, outputs, idx):\n        \"\"\"Compute attention over retrieved cache, and combine with self attention.\"\"\"\n        if not self.enabled:\n            return\n\n        cache_attn_output, cached_key_values = self.cache_attns[idx](\n            kwargs[\"hidden_states\"],\n            self.retrieval_state[\"retrieved_hidden_states\"],\n            cached_key_values=kwargs[\"past_key_value\"],\n        )\n\n        # Residual connection with self attention output\n        residual = outputs[0] + cache_attn_output\n        past_key_value = outputs[-1] + cached_key_values if outputs[-1] is not None else None\n        outputs = (residual,) + outputs[1:-1] + (past_key_value,)\n        return outputs\n\n    def retrieve_hook(self, module, args, kwargs, outputs, idx):\n        \"\"\"Retrieve topk neighbors from cache using the hidden states\"\"\"\n        if not self.enabled:\n            return\n\n        hs = kwargs[\"hidden_states\"].detach()\n        batch_size, seq_len, hidden_size = hs.shape\n\n        # Infer padding mask from attention mask\n        if kwargs.get(\"attention_mask\") is not None and False:\n            input_mask = kwargs[\"attention_mask\"].detach().to(torch.bool)\n            input_mask = input_mask.all(axis=-1).reshape(batch_size, seq_len).logical_not()\n        else:\n            input_mask = None\n\n        # Project hidden states to lower dimension and normalize these\n        # will be used as queries for the cache and as values to be\n        # stored in the cache. We detach since we do not backpropagate\n        # through the cache. No gradients are computed during retrieval.\n        with torch.no_grad():\n            projected_hs = self.h_norm(self.h_proj(hs)).to(self.cache_dtype)\n\n        # Since we need to infer the batch size before initializing\n        # the cache we initialize the cache in the first forward pass\n        # and start retrieval in the next passes.\n        if (\n            self.caches[idx].wrapped is None\n            and self.config.neurocache_type == NeurocacheType.ONDEVICE\n        ):\n            value_dim = hidden_size if self.should_reencode() else 0\n\n            # TODO: Just pass OnDeviceCacheConfig instead of individual args\n            self.caches[idx].wrapped = OnDeviceCache(\n                batch_size,\n                self.config.cache_size,\n                self.cache_dim,\n                value_dim,\n                self.config.neighborhood_size,\n                self.config.similarity_fn,\n                dtype=self.cache_dtype,\n                ordering=self.config.cache_type,\n            ).to(projected_hs.device)\n\n            retrieved_hs_dim = value_dim if self.should_reencode() else self.cache_dim\n            retrieved_hs = torch.zeros(\n                batch_size,\n                seq_len,\n                self.config.topk * self.config.neighborhood_size,\n                retrieved_hs_dim,\n                dtype=hs.dtype,\n            ).to(projected_hs.device)\n\n        # Else if the cache is already initialized retrieve from cache\n        else:\n            # retrieve topk neighbors from cache\n            assert self.caches[idx].wrapped is not None\n            assert self.caches[idx].wrapped.num_caches == batch_size or self.config.global_cache, (\n                \"Cache batch size does not match hidden states batch size. \"\n                \"Please re-initalize Neurocache.\"\n            )\n            keys, values = self.retrieve_topk(projected_hs, input_mask, idx)\n            retrieved_hs = values if self.should_reencode() else keys\n            assert retrieved_hs is not None, \"Retrieved cache is None\"\n            # This is no-op if dtype is the same\n            retrieved_hs = retrieved_hs.to(hs.dtype)\n\n        if self.should_reencode():\n            # re-project retrieved values (full size hidden states)\n            # to train h_proj and h_norm layers\n            retrieved_hs = self.h_norm(self.h_proj(retrieved_hs))\n\n        # Store retrieved states for attention in following layers\n        self.retrieval_state[\"retrieved_hidden_states\"] = retrieved_hs\n\n    def update_hook(self, module, args, kwargs, outputs, idx):\n        \"\"\"Update the cache with the new hidden states.\"\"\"\n        if not self.enabled or not self.update_cache:\n            return\n\n        hs = kwargs[\"hidden_states\"].detach()\n        batch_size, seq_len, _ = hs.shape\n\n        # Infer padding mask from attention mask\n        if kwargs.get(\"attention_mask\") is not None and False:\n            input_mask = kwargs[\"attention_mask\"].detach().to(torch.bool)\n            input_mask = input_mask.all(axis=-1).reshape(batch_size, seq_len).logical_not()\n        else:\n            input_mask = None\n\n        # Project hidden states to lower dimension and normalize these\n        # will be used as queries for the cache and as values to be\n        # stored in the cache. This is always done without gradients,\n        # even during training since we do not backpropagate through the cache.\n        with torch.no_grad():\n            # TODO: if the same layer is used for retrieval and update, we can\n            # avoid encoding the hidden states twice.\n            projected_hs = self.h_norm(self.h_proj(hs)).to(self.cache_dtype)\n\n        if self.should_reencode():\n            # Store both projected and original hidden states\n            self.caches[idx].update(projected_hs, hs.to(self.cache_dtype), input_mask)\n        else:\n            # Store only projected hidden states\n            self.caches[idx].update(projected_hs, None, input_mask)",
    "description": null,
    "url": null
}