{
    "acronym": "compositionalattn",
    "title": "Compositional Attention: Disentangling Search and Retrieval",
    "seed_ids": [
        "universaltrans",
        "transformer"
    ],
    "s2id": "b8b813111c411ae61881ab9cd25707d9de6444ec",
    "abstract": "Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.",
    "authors": [
        "Sarthak Mittal",
        "S. Raparthy",
        "I. Rish",
        "Yoshua Bengio",
        "Guillaume Lajoie"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": "This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.",
    "citationCount": 13,
    "influentialCitationCount": 1,
    "code": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom fairseq.modules.group_linear_layer import GroupLinearLayer\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nfrom fairseq.modules.quant_noise import quant_noise\nfrom torch import Tensor, nn\nfrom torch.nn import Parameter\n\n@with_incremental_state\nclass CompositionalAttention(nn.Module):\n    \"\"\"Multi-headed attention.\n    See \"Attention Is All You Need\" for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        num_rules,\n        gumbel=False,\n        qk_rule=False,\n        selection_dim=None,\n        nonlinear=False,\n        q_compose=False,\n        attn_dim=None,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        self_attention=False,\n        encoder_decoder_attention=False,\n        q_noise=0.0,\n        qn_block_size=8,\n    ):\n        super().__init__()\n\n        if attn_dim is None:\n            attn_dim = embed_dim\n\n        if selection_dim is None:\n            selection_dim = embed_dim // num_heads\n\n        self.embed_dim = embed_dim\n        self.attn_dim = attn_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.num_rules = num_rules\n        self.gumbel = gumbel\n        self.qk_rule = qk_rule\n        self.selection_dim = selection_dim\n        self.nonlinear = nonlinear\n        self.q_compose = q_compose\n\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n\n        self.head_dim = embed_dim // num_heads\n        self.value_dim = attn_dim // num_rules\n\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        assert (\n            self.value_dim * num_rules == self.attn_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n        self.scaling_values = self.selection_dim ** -0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            \"Self-attention requires query, key and \" \"value to be of the same size\"\n        )\n\n        self.k_proj = quant_noise(\n            nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.v_proj = quant_noise(\n            nn.Linear(self.vdim, attn_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.q_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        self.out_proj = quant_noise(\n            nn.Linear(self.num_heads * self.value_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        if self.qk_rule:\n            self.value_k = quant_noise(\n                nn.Linear(self.value_dim, self.selection_dim, bias=bias), q_noise, qn_block_size\n            )\n            if self.q_compose:\n                self.value_q = quant_noise(\n                    nn.Linear(self.head_dim, self.selection_dim, bias=bias), q_noise, qn_block_size\n                )\n            else:\n                self.value_q = quant_noise(\n                    nn.Linear(embed_dim, self.selection_dim * self.num_heads, bias=bias), q_noise, qn_block_size\n                )\n        else:\n            if self.q_compose:\n                self.value_q = quant_noise(\n                    nn.Linear(self.head_dim, self.selection_dim, bias=bias), q_noise, qn_block_size\n                )\n            else:\n                self.value_q = quant_noise(\n                    nn.Linear(embed_dim, self.selection_dim * self.num_heads, bias=bias), q_noise, qn_block_size\n                )\n            if self.nonlinear:\n                # Can change the capacity of the score_network MLP here\n                self.score_network1 = quant_noise(\n                    nn.Linear(self.selection_dim + self.value_dim, self.selection_dim, bias=bias), q_noise, qn_block_size\n                )\n                self.score_network2 = quant_noise(\n                    nn.Linear(self.selection_dim, 1, bias=bias), q_noise, qn_block_size\n                )\n            else:\n                self.score_network = quant_noise(\n                    nn.Linear(self.selection_dim + self.value_dim, 1, bias=bias), q_noise, qn_block_size\n                )\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self.reset_parameters()\n\n        self.onnx_trace = False\n\n    def reset_parameters(self):\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_proj.weight)\n            nn.init.xavier_uniform_(self.v_proj.weight)\n            nn.init.xavier_uniform_(self.q_proj.weight)\n\n        if self.qk_rule:\n            nn.init.xavier_uniform_(self.value_k.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.value_q.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.value_q.weight)\n            if self.nonlinear:\n                nn.init.xavier_uniform_(self.score_network1.weight)\n                nn.init.xavier_uniform_(self.score_network2.weight)\n            else:\n                nn.init.xavier_uniform_(self.score_network.weight)\n\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = True,\n        static_kv: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        before_softmax: bool = False,\n        need_head_weights: bool = False,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        if need_head_weights:\n            need_weights = True\n\n        is_tpu = query.device.type == \"xla\"\n\n        tgt_len, bsz, embed_dim = query.size()\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n\n        if (False\n            and not self.onnx_trace\n            and not is_tpu  # don't use PyTorch version on TPUs\n            and incremental_state is None\n            and not static_kv\n            # A workaround for quantization to work. Otherwise JIT compilation\n            # treats bias in linear module as method.\n            and not torch.jit.is_scripting()\n        ):\n            assert key is not None and value is not None\n            return F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                torch.empty([0]),\n                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout_module.p,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                self.training or self.dropout_module.apply_during_inference,\n                key_padding_mask,\n                need_weights,\n                attn_mask,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj.weight,\n                k_proj_weight=self.k_proj.weight,\n                v_proj_weight=self.v_proj.weight,\n            )\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and \"prev_key\" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q *= self.scaling\n\n        if self.bias_k is not None:\n            assert self.bias_v is not None\n            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n                    ],\n                    dim=1,\n                )\n\n        q = (\n            q.contiguous()\n            .view(tgt_len, bsz * self.num_heads, self.head_dim)\n            .transpose(0, 1)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                .view(-1, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                .view(-1, bsz * self.num_rules, self.value_dim)\n                .transpose(0, 1)\n            )\n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n            if \"prev_key\" in saved_state:\n                _prev_key = saved_state[\"prev_key\"]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n            if \"prev_value\" in saved_state:\n                _prev_value = saved_state[\"prev_value\"]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_rules, -1, self.value_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if \"prev_key_padding_mask\" in saved_state:\n                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n            assert k is not None and v is not None\n            key_padding_mask = CompositionalAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_value\"] = v.view(bsz, self.num_rules, -1, self.value_dim)\n            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        src_len = k.size(1)\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        if self.add_zero_attn:\n            assert v is not None\n            src_len += 1\n            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n                            key_padding_mask\n                        ),\n                    ],\n                    dim=1,\n                )\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(0)\n            if self.onnx_trace:\n                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n            attn_weights += attn_mask\n\n        if key_padding_mask is not None:\n            # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            if not is_tpu:\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                    float(\"-inf\"),\n                )\n            else:\n                attn_weights = attn_weights.transpose(0, 2)\n                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n                attn_weights = attn_weights.transpose(0, 2)\n\n        attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n\n        if before_softmax:\n            print(\"Before Softmax\")\n            exit()\n            return attn_weights, v\n\n        attn_weights_float = utils.softmax(\n            attn_weights, dim=-1, onnx_trace=self.onnx_trace\n        )\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = self.dropout_module(attn_weights)\n\n        assert v is not None\n        v = v.view(bsz, 1, self.num_rules, src_len, self.value_dim)\n        attn_probs = attn_probs.unsqueeze(2)\n        attn = torch.matmul(attn_probs, v).view(bsz * self.num_heads * self.num_rules, tgt_len, self.value_dim)\n        assert list(attn.size()) == [bsz * self.num_heads * self.num_rules, tgt_len, self.value_dim]\n        if self.onnx_trace and attn.size(1) == 1:\n            # when ONNX tracing a single decoder step (sequence length == 1)\n            # the transpose is a no-op copy before view, thus unnecessary\n            attn = attn.contiguous().view(tgt_len, bsz, self.num_heads, self.num_rules, self.value_dim)\n        else:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, self.num_heads, self.num_rules, self.value_dim)\n\n        if self.q_compose:\n            v_q = self.value_q(q.transpose(0,1)).contiguous().view(tgt_len, bsz, self.num_heads, 1, self.selection_dim)\n        else:\n            v_q = self.value_q(query).contiguous().view(tgt_len, bsz, self.num_heads, 1, self.selection_dim)\n\n        if self.qk_rule:\n            v_q *= self.scaling_values\n            v_k = self.value_k(attn).view(tgt_len, bsz, self.num_heads, self.num_rules, self.selection_dim).transpose(4,3).contiguous()\n            v_score = torch.matmul(v_q, v_k).view(tgt_len, bsz, self.num_heads, self.num_rules, 1)\n        else:\n            v_q = v_q.repeat(1, 1, 1, self.num_rules, 1)\n            v_in = torch.cat([attn, v_q], dim=-1)\n            if self.nonlinear:\n                v_score = self.score_network1(v_in).view(tgt_len, bsz, self.num_heads, self.num_rules, self.selection_dim)\n                v_score = self.score_network2(F.relu(v_score))\n            else:\n                v_score = self.score_network(v_in).view(tgt_len, bsz, self.num_heads, self.num_rules, 1)\n\n        if self.gumbel:\n            v_score = F.gumbel_softmax(v_score, dim=3)\n        else:\n            v_score = F.softmax(v_score, dim=3)\n\n        # v_score = torch.zeros_like(v_score)\n        # v_score[:,:,0,0,:] = 1.\n        # v_score[:,:,1,1,:] = 1.\n        # v_score[:,:,2,2,:] = 1.\n        # v_score[:,:,3,3,:] = 1.\n        # v_score[:,:,4,4,:] = 1.\n        # v_score[:,:,5,5,:] = 1.\n        # v_score[:,:,6,6,:] = 1.\n        # v_score[:,:,7,7,:] = 1.\n\n        attn = (attn * v_score).sum(dim=3).view(tgt_len, bsz, self.num_heads * self.value_dim)\n        attn = self.out_proj(attn)\n\n        attn_weights: Optional[Tensor] = None\n        if need_weights:\n            attn_weights = attn_weights_float.view(\n                bsz, self.num_heads, tgt_len, src_len\n            ).transpose(1, 0)\n            if not need_head_weights:\n                # average attention weights over heads\n                attn_weights = attn_weights.mean(dim=0)\n\n        return attn, attn_weights\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n        key_padding_mask: Optional[Tensor],\n        prev_key_padding_mask: Optional[Tensor],\n        batch_size: int,\n        src_len: int,\n        static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            filler = torch.zeros(\n                (batch_size, src_len - prev_key_padding_mask.size(1)),\n                device=prev_key_padding_mask.device,\n            )\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), filler.float()], dim=1\n            )\n        elif key_padding_mask is not None:\n            filler = torch.zeros(\n                (batch_size, src_len - key_padding_mask.size(1)),\n                device=key_padding_mask.device,\n            )\n            new_key_padding_mask = torch.cat(\n                [filler.float(), key_padding_mask.float()], dim=1\n            )\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights\n",
    "description": null,
    "url": null
}