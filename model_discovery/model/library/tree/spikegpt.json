{
    "title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks",
    "acronym": "spikegpt",
    "s2id": "9f52317ea9c5a6804b978987ff2a6557f98b5b2c",
    "abstract": "As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.",
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.",
    "citationCount": 54,
    "influentialCitationCount": 2,
    "seed_ids": [
        "aft",
        "performer",
        "gpt3",
        "synthesizer",
        "reformer",
        "transformer",
        "bert",
        "gpt2"
    ],
    "code": "from src.spikingjelly.clock_driven.surrogate import ATan as atan\nfrom src.spikingjelly.clock_driven import functional, surrogate, lava_exchange\nfrom src.spikingjelly.clock_driven.neuron import LIFNode, check_backend\n\nfrom typing import Callable\n\ntry:\n    from src.spikingjelly.clock_driven import neuron_kernel\nexcept BaseException as e:\n    logging.info(f'spikingjelly.clock_driven.neuron: {e}')\n    neuron_kernel = None\n\n\n# from src.spikingjelly_n.activation_based import neuron,functional,surrogate\nimport math, os\nimport numpy as np\nimport logging\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n#import pdb\n\nfrom src.rwkv import RWKV_Init, RWKV_ChannelMix, RWKV_TimeMix\n\ntry:\n    from deepspeed.ops.adam import FusedAdam\nexcept:\n    pass  # some poor windows users cant install deepspeed\n\nlogger = logging.getLogger(__name__)\n\nRWKV_HEAD_QK_DIM = 0\nprint(f'\\nRWKV_HEAD_QK_DIM {RWKV_HEAD_QK_DIM}\\n')\n\n\nclass L2Wrap(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, loss, y):\n        ctx.save_for_backward(y)\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        y = ctx.saved_tensors[0]\n        # to encourage the logits to be close to 0\n        factor = 1e-4 / (y.shape[0] * y.shape[1])\n        maxx, ids = torch.max(y, -1, keepdim=True)\n        gy = torch.zeros_like(y)\n        gy.scatter_(-1, ids, maxx * factor)\n        return (grad_output, gy)\n\n\n\n\nclass MultiStepLIFNode(LIFNode):\n    def __init__(self, tau: float = 2., decay_input: bool = True, v_threshold: float = 1.,\n                 v_reset: float = 0., surrogate_function: Callable = surrogate.Sigmoid(),\n                 detach_reset: bool = False, backend='torch', lava_s_cale=1 << 6):\n        \"\"\"\n        * :ref:`API in English <MultiStepLIFNode.__init__-en>`\n\n        .. _MultiStepLIFNode.__init__-cn:\n\n        :param tau: \u819c\u7535\u4f4d\u65f6\u95f4\u5e38\u6570\n        :type tau: float\n\n        :param decay_input: \u8f93\u5165\u662f\u5426\u4f1a\u8870\u51cf\n        :type decay_input: bool\n\n        :param v_threshold: \u795e\u7ecf\u5143\u7684\u9608\u503c\u7535\u538b\n        :type v_threshold: float\n\n        :param v_reset: \u795e\u7ecf\u5143\u7684\u91cd\u7f6e\u7535\u538b\u3002\u5982\u679c\u4e0d\u4e3a ``None``\uff0c\u5f53\u795e\u7ecf\u5143\u91ca\u653e\u8109\u51b2\u540e\uff0c\u7535\u538b\u4f1a\u88ab\u91cd\u7f6e\u4e3a ``v_reset``\uff1b\n            \u5982\u679c\u8bbe\u7f6e\u4e3a ``None``\uff0c\u5219\u7535\u538b\u4f1a\u88ab\u51cf\u53bb ``v_threshold``\n        :type v_reset: float\n\n        :param surrogate_function: \u53cd\u5411\u4f20\u64ad\u65f6\u7528\u6765\u8ba1\u7b97\u8109\u51b2\u51fd\u6570\u68af\u5ea6\u7684\u66ff\u4ee3\u51fd\u6570\n        :type surrogate_function: Callable\n\n        :param detach_reset: \u662f\u5426\u5c06reset\u8fc7\u7a0b\u7684\u8ba1\u7b97\u56fe\u5206\u79bb\n        :type detach_reset: bool\n\n        :param backend: \u4f7f\u7528\u54ea\u79cd\u8ba1\u7b97\u540e\u7aef\uff0c\u53ef\u4ee5\u4e3a ``'torch'`` \u6216 ``'cupy'``\u3002``'cupy'`` \u901f\u5ea6\u66f4\u5feb\uff0c\u4f46\u4ec5\u652f\u6301GPU\u3002\n        :type backend: str\n\n        \u591a\u6b65\u7248\u672c\u7684 :class:`spikingjelly.clock_driven.neuron.LIFNode`\u3002\n\n        .. tip::\n\n            \u5bf9\u4e8e\u591a\u6b65\u795e\u7ecf\u5143\uff0c\u8f93\u5165 ``x_seq.shape = [T, *]``\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u4f7f\u7528 ``.v`` \u548c ``.spike`` \u83b7\u53d6 ``t = T - 1`` \u65f6\u523b\u7684\u7535\u538b\u548c\u8109\u51b2\uff0c\u8fd8\u80fd\u591f\n            \u4f7f\u7528 ``.v_seq`` \u548c ``.spike_seq`` \u83b7\u53d6\u5b8c\u6574\u7684 ``T`` \u4e2a\u65f6\u523b\u7684\u7535\u538b\u548c\u8109\u51b2\u3002\n\n        .. tip::\n\n            \u9605\u8bfb :doc:`\u4f20\u64ad\u6a21\u5f0f <./clock_driven/10_propagation_pattern>` \u4ee5\u83b7\u53d6\u66f4\u591a\u5173\u4e8e\u5355\u6b65\u548c\u591a\u6b65\u4f20\u64ad\u7684\u4fe1\u606f\u3002\n\n        * :ref:`\u4e2d\u6587API <MultiStepLIFNode.__init__-cn>`\n\n        .. _MultiStepLIFNode.__init__-en:\n\n        :param tau: membrane time constant\n        :type tau: float\n\n        :param decay_input: whether the input will decay\n        :type decay_input: bool\n\n        :param v_threshold: threshold voltage of neurons\n        :type v_threshold: float\n\n        :param v_reset: reset voltage of neurons. If not ``None``, voltage of neurons that just fired spikes will be set to\n            ``v_reset``. If ``None``, voltage of neurons that just fired spikes will subtract ``v_threshold``\n        :type v_reset: float\n\n        :param surrogate_function: surrogate function for replacing gradient of spiking functions during back-propagation\n        :type surrogate_function: Callable\n\n        :param detach_reset: whether detach the computation graph of reset\n        :type detach_reset: bool\n\n        :param backend: use which backend, ``'torch'`` or ``'cupy'``. ``'cupy'`` is faster but only supports GPU\n        :type backend: str\n\n        The multi-step version of :class:`spikingjelly.clock_driven.neuron.LIFNode`.\n\n        .. admonition:: Tip\n            :class: tip\n\n            The input for multi-step neurons are ``x_seq.shape = [T, *]``. We can get membrane potential and spike at\n            time-step ``t = T - 1`` by ``.v`` and ``.spike``. We can also get membrane potential and spike at all ``T``\n            time-steps by ``.v_seq`` and ``.spike_seq``.\n\n        .. admonition:: Tip\n            :class: tip\n\n            Read :doc:`Propagation Pattern <./clock_driven_en/10_propagation_pattern>` for more details about single-step\n            and multi-step propagation.\n\n        \"\"\"\n        super().__init__(tau, decay_input, v_threshold, v_reset, surrogate_function, detach_reset)\n        self.register_memory('v_seq', None)\n\n        check_backend(backend)\n\n        self.backend = backend\n\n        self.lava_s_cale = lava_s_cale\n\n        if backend == 'lava':\n            self.lava_neuron = self.to_lava()\n        else:\n            self.lava_neuron = None\n\n    def forward(self, x_seq: torch.Tensor):\n        assert x_seq.dim() > 1\n        # x_seq.shape = [T, *]\n\n        if self.backend == 'torch':\n            spike_seq = []\n            self.v_seq = []\n            for t in range(x_seq.shape[0]):\n                spike_seq.append(super().forward(x_seq[t]).unsqueeze(0))\n                self.v_seq.append(self.v.unsqueeze(0))\n            spike_seq = torch.cat(spike_seq, 0)\n            self.v_seq = torch.cat(self.v_seq, 0)\n            return spike_seq\n\n        elif self.backend == 'cupy':\n            if isinstance(self.v, float):\n                v_init = self.v\n                self.v = torch.zeros_like(x_seq[0].data)\n                if v_init != 0.:\n                    torch.fill_(self.v, v_init)\n\n            spike_seq, self.v_seq = neuron_kernel.MultiStepLIFNodePTT.apply(\n                x_seq.flatten(1), self.v.flatten(0), self.decay_input, self.tau, self.v_threshold, self.v_reset, self.detach_reset, self.surrogate_function.cuda_code)\n\n            spike_seq = spike_seq.reshape(x_seq.shape)\n            self.v_seq = self.v_seq.reshape(x_seq.shape)\n\n            self.v = self.v_seq[-1].clone()\n\n            return spike_seq\n\n        elif self.backend == 'lava':\n            if self.lava_neuron is None:\n                self.lava_neuron = self.to_lava()\n\n            spike, self.v = lava_exchange.lava_neuron_forward(self.lava_neuron, x_seq, self.v)\n\n            return spike\n\n        else:\n            raise NotImplementedError(self.backend)\n\n    def extra_repr(self):\n        return super().extra_repr() + f', backend={self.backend}'\n\n    def to_lava(self):\n        return lava_exchange.to_lava_neuron(self)\n\n    def reset(self):\n        super().reset()\n        if self.lava_neuron is not None:\n            self.lava_neuron.current_state.zero_()\n            self.lava_neuron.voltage_state.zero_()\n\n\n########################################################################################################\n# The GPT Model with our blocks\n########################################################################################################\n\n\nclass Block(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.config = config\n        self.layer_id = layer_id\n\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        self.lif1 = MultiStepLIFNode(tau=2., surrogate_function=surrogate.ATan(alpha=2.0), backend='cupy',\n                                            v_threshold=1.)\n        self.lif2 = MultiStepLIFNode(tau=2., surrogate_function=surrogate.ATan(alpha=2.0), backend='cupy',\n                                            v_threshold=1.)\n        self.dropout = nn.Dropout(0.03)\n\n        if self.layer_id == 0:\n            self.ln0 = nn.LayerNorm(config.n_embd)\n\n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            self.ffnPre = RWKV_ChannelMix(config, 0)\n        else:\n            self.att = RWKV_TimeMix(config, layer_id)\n\n        self.ffn = RWKV_ChannelMix(config, layer_id)\n\n    def forward(self, x):\n        if self.layer_id == 0:\n            x = self.ln0(x)\n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            x = x + self.lif1(self.ffnPre(self.ln1(x)).permute(1, 0, 2)).permute(1, 0, 2)  # better in some cases\n        else:\n            x = x + self.lif1(self.att(self.ln1(x)).permute(1, 0, 2)).permute(1, 0, 2)\n        x = x + self.lif2(self.ffn(self.ln2(x)).permute(1, 0, 2)).permute(1, 0, 2)\n        x = self.dropout(x)\n\n        return x\n\n"
}