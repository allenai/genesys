{
    "acronym": "rwkv4",
    "title": "Reinventing RNNs for the Transformer Era",
    "seed_ids": [
        "resurrectrnn",
        "h3",
        "rmt",
        "flashattn",
        "mea",
        "metaformer",
        "alibi",
        "luna",
        "aft",
        "gmlp",
        "gpt3",
        "reformer",
        "transformer"
    ],
    "s2id": "026b3396a63ed5772329708b7580d633bb86bec9",
    "abstract": "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
    "authors": [
        "Bo Peng",
        "Eric Alcaide",
        "Quentin G. Anthony",
        "Alon Albalak",
        "Samuel Arcadinho",
        "Stella Biderman",
        "Huanqi Cao",
        "Xin Cheng",
        "Michael Chung",
        "Matteo Grella",
        "G. Kranthikiran",
        "Xuming He",
        "Haowen Hou",
        "Przemyslaw Kazienko",
        "Jan Koco\u0144",
        "Jiaming Kong",
        "Bartlomiej Koptyra",
        "Hayden Lau",
        "Krishna Sri Ipsit Mantri",
        "Ferdinand Mom",
        "Atsushi Saito",
        "Xiangru Tang",
        "Bolun Wang",
        "J. S. Wind",
        "Stansilaw Wozniak",
        "Ruichong Zhang",
        "Zhenyuan Zhang",
        "Qihang Zhao",
        "P. Zhou",
        "Jian Zhu",
        "Rui Zhu"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": "This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
    "citationCount": 251,
    "influentialCitationCount": 27,
    "code": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\n\nimport math, os\nimport numpy as np\nimport logging\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntry:\n    from deepspeed.ops.adam import FusedAdam\nexcept:\n    pass # some poor windows users cant install deepspeed\n\nlogger = logging.getLogger(__name__)\n\nRWKV_HEAD_QK_DIM = 0\nprint(f'\\nRWKV_HEAD_QK_DIM {RWKV_HEAD_QK_DIM}\\n')\n\nclass L2Wrap(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, loss, y):\n        ctx.save_for_backward(y)\n        return loss\n    @staticmethod\n    def backward(ctx, grad_output):\n        y = ctx.saved_tensors[0]\n        # to encourage the logits to be close to 0\n        factor = 1e-4 / (y.shape[0] * y.shape[1])\n        maxx, ids = torch.max(y, -1, keepdim=True)\n        gy = torch.zeros_like(y)\n        gy.scatter_(-1, ids, maxx * factor)\n        return (grad_output, gy)\n\n########################################################################################################\n# CUDA Kernel\n########################################################################################################\n\nT_MAX = 1024 # increase this if your ctx_len is long [NOTE: TAKES LOTS OF VRAM!]\n# it's possible to go beyond CUDA limitations if you slice the ctx and pass the hidden state in each slice\n\nfrom torch.utils.cpp_extension import load\nwkv_cuda = load(name=\"wkv\", sources=[\"cuda/wkv_op.cpp\", \"cuda/wkv_cuda.cu\"],\n                verbose=True, extra_cuda_cflags=['-res-usage', '--maxrregcount 60', '--use_fast_math', '-O3', '-Xptxas -O3', f'-DTmax={T_MAX}'])\n\nclass WKV(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, B, T, C, w, u, k, v):\n        ctx.B = B\n        ctx.T = T\n        ctx.C = C\n        assert T <= T_MAX\n        assert B * C % min(C, 1024) == 0\n        if '32' in os.environ['RWKV_FLOAT_MODE']:\n            w = -torch.exp(w.contiguous())\n            u = u.contiguous()\n            k = k.contiguous()\n            v = v.contiguous()\n        else:\n            w = -torch.exp(w.float().contiguous())\n            u = u.float().contiguous()\n            k = k.float().contiguous()\n            v = v.float().contiguous()\n        ctx.save_for_backward(w, u, k, v)\n        y = torch.empty((B, T, C), device='cuda', memory_format=torch.contiguous_format)\n        wkv_cuda.forward(B, T, C, w, u, k, v, y)\n        if '32' in os.environ['RWKV_FLOAT_MODE']:\n            return y\n        elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n            return y.half()\n        elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n            return y.bfloat16()\n\n    @staticmethod\n    def backward(ctx, gy):\n        B = ctx.B\n        T = ctx.T\n        C = ctx.C\n        assert T <= T_MAX\n        assert B * C % min(C, 1024) == 0\n        w, u, k, v = ctx.saved_tensors\n        gw = torch.zeros((B, C), device='cuda').contiguous()\n        gu = torch.zeros((B, C), device='cuda').contiguous()\n        gk = torch.zeros((B, T, C), device='cuda').contiguous()\n        gv = torch.zeros((B, T, C), device='cuda').contiguous()\n        if '32' in os.environ['RWKV_FLOAT_MODE']:\n            wkv_cuda.backward(B, T, C, w, u, k, v, gy.contiguous(), gw, gu, gk, gv)\n        else:\n            wkv_cuda.backward(B, T, C, w, u, k, v, gy.float().contiguous(), gw, gu, gk, gv)\n        gw = torch.sum(gw, dim=0)\n        gu = torch.sum(gu, dim=0)\n        if '32' in os.environ['RWKV_FLOAT_MODE']:\n            return (None, None, None, gw, gu, gk, gv)\n        elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n            return (None, None, None, gw.half(), gu.half(), gk.half(), gv.half())\n        elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n            return (None, None, None, gw.bfloat16(), gu.bfloat16(), gk.bfloat16(), gv.bfloat16())\n\ndef RUN_CUDA(B, T, C, w, u, k, v):\n    return WKV.apply(B, T, C, w.cuda(), u.cuda(), k.cuda(), v.cuda())\n\n########################################################################################################\n# RWKV: RWKV Time-mix + RWKV Channel-mix\n########################################################################################################\n\ndef RWKV_Init(model, args):  # fancy initialization of all lin & emb layer in the model\n    print(\"\\n[--> first run, init model params (very slow for large models) <--]\")\n    print(\"[so you shall only do it for 1 single GPU and save the checkpt and load it when using multiple GPU]\\n\")\n\n    for mm in model.modules():\n        if \"RecursiveScriptModule\" in str(type(mm)):\n            if mm.original_name not in [\"Linear\"]:\n                continue\n            ww = None\n            for name, param in mm.named_parameters():\n                if name == \"weight\":\n                    ww = param\n        else:\n            m = mm\n            if not isinstance(m, (nn.Linear, nn.Embedding)):\n                continue\n            ww = m.weight\n        with torch.no_grad():\n            name = \"[unknown weight]\"\n            for name, parameter in model.named_parameters():  # find the name of the weight\n                if id(ww) == id(parameter):\n                    break\n\n            shape = ww.shape\n            gain = 1.0\n            scale = 1.0  # extra scale for gain\n\n            if isinstance(m, nn.Embedding):\n                gain = math.sqrt(max(shape[0], shape[1]))\n                if shape[0] == args.vocab_size and shape[1] == args.n_embd:  # token emb?\n                    scale = 1e-4\n                else:\n                    scale = 0\n\n            if isinstance(m, nn.Linear):\n                if shape[0] > shape[1]:\n                    gain = math.sqrt(shape[0] / shape[1])\n                if shape[0] == args.vocab_size and shape[1] == args.n_embd:  # final projection?\n                    scale = 0.5\n\n            if hasattr(m, \"scale_init\"):\n                scale = m.scale_init\n\n            # print(f\"{str(shape[0]).ljust(5)} {str(shape[1]).ljust(5)} {str(scale).ljust(4)} {name}\")\n\n            gain *= scale\n            if scale == -999:\n                nn.init.eye_(ww)\n            elif gain == 0:\n                # zero init is great for some RWKV matrices\n                nn.init.zeros_(ww)\n            elif gain > 0:\n                nn.init.orthogonal_(ww, gain=gain)\n            else:\n                nn.init.normal_(ww, mean=0.0, std=-scale)\n\n\nclass RWKV_TimeMix(torch.jit.ScriptModule):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.ctx_len = config.ctx_len\n        self.n_embd = config.n_embd\n\n        attn_sz = config.n_embd\n\n        with torch.no_grad(): # fancy init\n            ratio_0_to_1 = (layer_id / (config.n_layer - 1)) # 0 to 1\n            ratio_1_to_almost0 = (1.0 - (layer_id / config.n_layer)) # 1 to ~0\n            \n            # fancy time_decay\n            decay_speed = torch.ones(attn_sz)\n            for h in range(attn_sz):\n                decay_speed[h] = -5 + 8 * (h / (attn_sz-1)) ** (0.7 + 1.3 * ratio_0_to_1)\n            self.time_decay = nn.Parameter(decay_speed)\n            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n\n            # fancy time_first\n            zigzag = (torch.tensor([(i+1)%3 - 1 for i in range(attn_sz)]) * 0.5)\n            self.time_first = nn.Parameter(torch.ones(attn_sz) * math.log(0.3) + zigzag)\n            \n            # fancy time_mix\n            x = torch.ones(1, 1, config.n_embd)\n            for i in range(config.n_embd):\n                x[0, 0, i] = i / config.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.time_mix_v = nn.Parameter(torch.pow(x, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n            self.time_mix_r = nn.Parameter(torch.pow(x, 0.5 * ratio_1_to_almost0))\n            \n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n\n        self.key = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.value = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, attn_sz, bias=False)\n\n        self.output = nn.Linear(attn_sz, config.n_embd, bias=False)\n\n        self.key.scale_init = 0\n        self.receptance.scale_init = 0\n        self.output.scale_init = 0\n\n    @torch.jit.script_method\n    def jit_func(self, x):\n\n        # Mix x with the previous timestep to produce xk, xv, xr\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n\n        # Use xk, xv, xr to produce k, v, r\n        k = self.key(xk)\n        v = self.value(xv)\n        r = self.receptance(xr)\n        sr = torch.sigmoid(r)\n\n        return sr, k, v\n\n    def forward(self, x):\n        B, T, C = x.size() # x = (Batch,Time,Channel)\n\n        sr, k, v = self.jit_func(x)\n\n        rwkv = sr * RUN_CUDA(B, T, C, self.time_decay, self.time_first, k, v)\n        rwkv = self.output(rwkv)\n        return rwkv\n\n\nclass RWKV_ChannelMix(torch.jit.ScriptModule):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n\n        with torch.no_grad(): # fancy init of time_mix\n            ratio_1_to_almost0 = (1.0 - (layer_id / config.n_layer)) # 1 to ~0\n\n            x = torch.ones(1, 1, config.n_embd)\n            for i in range(config.n_embd):\n                x[0, 0, i] = i / config.n_embd\n\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.time_mix_r = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n\n        hidden_sz = 4 * config.n_embd\n        self.key = nn.Linear(config.n_embd, hidden_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.value = nn.Linear(hidden_sz, config.n_embd, bias=False)\n\n        self.value.scale_init = 0\n        self.receptance.scale_init = 0\n\n    @torch.jit.script_method\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n\n        k = self.key(xk)\n        k = torch.square(torch.relu(k))\n        kv = self.value(k)\n\n        rkv = torch.sigmoid(self.receptance(xr)) * kv\n        return rkv\n\n########################################################################################################\n# The GPT Model with our blocks\n########################################################################################################\n\n\nclass GPTConfig:\n    def __init__(self, vocab_size, ctx_len, **kwargs):\n        self.vocab_size = vocab_size\n        self.ctx_len = ctx_len\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n\nclass Block(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.config = config\n        self.layer_id = layer_id\n\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n\n        if self.layer_id == 0:\n            self.ln0 = nn.LayerNorm(config.n_embd)\n\n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            self.ffnPre = RWKV_ChannelMix(config, 0)\n        else:\n            self.att = RWKV_TimeMix(config, layer_id)\n\n        self.ffn = RWKV_ChannelMix(config, layer_id)\n\n    def forward(self, x):\n        if self.layer_id == 0:\n            x = self.ln0(x)        \n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            x = x + self.ffnPre(self.ln1(x))  # better in some cases\n        else:\n            x = x + self.att(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))\n        return x\n\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.step = 0\n        self.config = config\n\n        self.emb = nn.Embedding(config.vocab_size, config.n_embd)\n\n        self.blocks = nn.Sequential(*[Block(config, i)\n                                    for i in range(config.n_layer)])\n\n        self.ln_out = nn.LayerNorm(config.n_embd)\n        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        if RWKV_HEAD_QK_DIM > 0:\n            self.head_q = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n            self.head_q.scale_init = 0\n            self.head_k = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n            self.head_k.scale_init = 0.1\n            self.register_buffer(\"copy_mask\", torch.tril(\n                torch.ones(config.ctx_len, config.ctx_len)))\n\n        self.ctx_len = config.ctx_len\n\n        try:\n            if os.environ['RWKV_LOAD_MODEL'] == str(False):\n                RWKV_Init(self, config) \n        except:\n            pass\n\n        logger.info(\"number of parameters: %e\", sum(p.numel()\n                    for p in self.parameters()))\n\n    def get_ctx_len(self):\n        return self.ctx_len\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear)):\n            module.weight.data.normal_(mean=0.0, std=0.01)\n        if isinstance(module, (nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=1e-5)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def configure_optimizers(self, train_config):\n        no_decay = set()\n\n        for mn, m in self.named_modules():  # here we disable weight_decay\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n                no_decay.add(fpn)\n\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        optim_groups = [\n            {\"params\": [param_dict[pn]\n                        for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n\n        try:\n            optimizer = FusedAdam(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps, bias_correction=True, adam_w_mode=False, weight_decay=0, amsgrad=False)\n        except:\n            print('\\n\\nDeepSpeed not found. Using torch optimizer instead (probably slower)\\n\\n')\n            optimizer = torch.optim.Adam(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps)\n\n        return optimizer\n\n    def forward(self, idx, targets=None):\n        idx = idx.to(self.emb.weight.device)\n\n        self.step += 1\n        B, T = idx.size()\n        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\"\n\n        x = self.emb(idx)\n        x = self.blocks(x)\n        x = self.ln_out(x)\n\n        if RWKV_HEAD_QK_DIM > 0:\n            q = self.head_q(x)[:, :T, :]\n            k = self.head_k(x)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (1.0 / RWKV_HEAD_QK_DIM)\n            c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n            \n            if '32' in os.environ['RWKV_FLOAT_MODE']:\n                c = c @ F.one_hot(idx, num_classes=self.config.vocab_size)\n            elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n                c = c @ F.one_hot(idx, num_classes=self.config.vocab_size).half()\n            elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n                c = c @ F.one_hot(idx, num_classes=self.config.vocab_size).bfloat16()\n\n            x = self.head(x) + c\n        else:\n            x = self.head(x)\n\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.to(x.device).view(-1))\n\n        return L2Wrap.apply(loss, x)",
    "description": null,
    "url": null
}