{
    "acronym": "assorecurmemtrans",
    "title": "Associative Recurrent Memory Transformer",
    "seed_ids": [
        "flagembedding",
        "yarn",
        "rwkv4",
        "deltanet",
        "gpt3",
        "transformer"
    ],
    "s2id": "1085ddc5028be0a6f517bde6c44029abc208c63f",
    "abstract": "This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.",
    "authors": [
        "Ivan Rodkin",
        "Yuri Kuratov",
        "Aydar Bulatov",
        "Mikhail Burtsev"
    ],
    "venue": "",
    "year": 2024,
    "tldr": "It is demonstrated that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}