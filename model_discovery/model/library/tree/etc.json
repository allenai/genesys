{
    "acronym": "etc",
    "title": "ETC: Encoding Long and Structured Inputs in Transformers",
    "seed_ids": [
        "longformer",
        "routingtransformer",
        "reformer",
        "compressivetransformer",
        "bptrans",
        "sparsetransformer",
        "transformerxl",
        "transformer",
        "bert"
    ],
    "s2id": "d27669c82faf78ea08cceaa0a171b540cccc304d",
    "abstract": "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",
    "authors": [
        "J. Ainslie",
        "Santiago Onta\u00f1\u00f3n",
        "Chris Alberti",
        "V. Cvicek",
        "Zachary Kenneth Fisher",
        "Philip Pham",
        "Anirudh Ravula",
        "Sumit K. Sanghai",
        "Qifan Wang",
        "Li Yang"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "tldr": "A new Transformer architecture, Extended Transformer Construction (ETC), is presented that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs.",
    "citationCount": 309,
    "influentialCitationCount": 28,
    "code": "# coding=utf-8\n# Copyright 2024 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Attention layers for ETC.\"\"\"\n\nfrom typing import List, Optional, Text, Tuple\n\nimport tensorflow as tf\n\nfrom etcmodel import feature_utils\nfrom etcmodel import tensor_utils\nfrom etcmodel.layers import recomputing_dropout\n\n\n\nclass FusedGlobalLocalAttention(tf.keras.layers.Layer):\n  \"\"\"Global-local attention used in `GlobalLocalTransformerLayers`.\n\n  We call this layer \"fused\" since the l2l and l2g attention operations are\n  fused together under 1 attention softmax, as are the g2g and g2l attention\n  operations.  This formulation makes standard Transformer attention a\n  special case of fused attention given the following conditions:\n    1. The local_radius for local self-attention covers the entire \"long\" input.\n    2. The query, key, value, and output projections are shared between l2l,\n      g2g, l2g, and g2l.\n    3. The global memory tokens would be concatenated with the long input tokens\n      to form the input to standard Transformer.\n\n  The connection with standard Transformer raises the possibility of directly\n  lifting the weights of a standard Transformer model into a fused attention\n  `GlobalLocalTransformerLayers` model to fine-tune on larger inputs.\n\n  See `GlobalLocalTransformerLayers` for more details about the long and global\n  inputs expected.\n  \"\"\"\n\n  def __init__(self,\n               long_hidden_size: int,\n               global_hidden_size: int,\n               num_heads: int,\n               local_radius: int,\n               long_total_att_size: Optional[int] = None,\n               global_total_att_size: Optional[int] = None,\n               relative_vocab_size: Optional[int] = None,\n               att_dropout_prob: float = 0.0,\n               initializer=None,\n               share_kv_projections: bool = False,\n               share_qkv_projections: bool = False,\n               share_att_output_projection: bool = False,\n               use_one_hot_lookup: bool = False,\n               name: Text = 'fused_global_local_att',\n               **kwargs):\n    \"\"\"Init.\n\n    Args:\n      long_hidden_size: Size of the long input hidden dimension. This will also\n        be the size of the long output and intermediate queries/keys/values.\n      global_hidden_size: Size of the global input hidden dimension. This will\n        also be the size of the global output and intermediate\n        queries/keys/values.\n      num_heads: Number of attention heads.\n      local_radius: How many tokens to the left/right to locally attend to for\n        long-to-long attention. For example, a value of 1 would allow each token\n        to only attend to 1 token to the left and 1 token to the right of it.\n      long_total_att_size: Total size of the long attention query/key/value\n        vectors after concatenating all heads. Defaults to `long_hidden_size`.\n        Must be a multiple of `num_heads`.\n      global_total_att_size: Total size of the global attention query/key/value\n        vectors after concatenating all heads. Defaults to `global_hidden_size`.\n        Must be a multiple of `num_heads`.\n      relative_vocab_size: Size of relative position vocabulary. If left\n        unspecified, relative positions will be ignored for attention.\n      att_dropout_prob: Dropout probability for attention probabilities. Must be\n        between 0.0 and 1.0. The default of 0.0 skips dropout.\n      initializer: Initializer to use for non-bias variables other than the\n        relative embedding table. Bias variables will be initialized to 0,\n        and the relative embedding table has its own default initialization\n        scale.\n      share_kv_projections: If True, key and value projections will be shared\n        between long-to-long and long-to-global components, as well as between\n        global-to-global and global-to-long components. This results in 2 key\n        projections per layer instead of 4 (and similarly for value\n        projections). Note that if `share_qkv_projections` is True, then\n        `share_kv_projections` is completely ignored since the former results\n        in even more sharing.\n      share_qkv_projections: If True, all attention components (long-to-long,\n        global-to-global, long-to-global, and global-to-long) will share the\n        same query, key, and value projections.\n      share_att_output_projection: If True, both long and global attention\n        results will share the same output projection per layer.\n      use_one_hot_lookup: Whether to use tf.one_hot for embedding lookup instead\n        of tf.gather. Default is False, but setting to True may be more\n        efficient on TPUs for vocab sizes that aren't too large. Currently this\n        is only used during lookup of relative position embeddings.\n      name: Name of the layer.\n      **kwargs: Forwarded to super.\n    \"\"\"\n    super(FusedGlobalLocalAttention, self).__init__(name=name, **kwargs)\n\n    if long_total_att_size is None:\n      long_total_att_size = long_hidden_size\n    if global_total_att_size is None:\n      global_total_att_size = global_hidden_size\n\n    self.long_hidden_size = long_hidden_size\n    self.global_hidden_size = global_hidden_size\n    self.num_heads = num_heads\n    self.local_radius = local_radius\n    self.long_total_att_size = long_total_att_size\n    self.global_total_att_size = global_total_att_size\n    self.relative_vocab_size = relative_vocab_size\n    self.att_dropout_prob = att_dropout_prob\n    self.initializer = initializer\n    self.share_kv_projections = share_kv_projections\n    self.share_qkv_projections = share_qkv_projections\n    self.share_att_output_projection = share_att_output_projection\n    self.use_one_hot_lookup = use_one_hot_lookup\n\n    self._validate_init_parameters()\n\n    def make_att_head_projection(total_att_size, name):\n      return ProjectAttentionHeads(\n          num_heads=num_heads,\n          size_per_head=total_att_size // num_heads,\n          use_bias=True,\n          initializer=initializer,\n          name=name)\n\n    # Long attention layers\n\n    self.long_query_projection = make_att_head_projection(\n        long_total_att_size, 'long_query_projection')\n\n    self.l2l_key_projection = make_att_head_projection(long_total_att_size,\n                                                       'l2l_key_projection')\n    if share_qkv_projections or share_kv_projections:\n      self.l2g_key_projection = self.l2l_key_projection\n    else:\n      self.l2g_key_projection = make_att_head_projection(\n          long_total_att_size, 'l2g_key_projection')\n\n    self.l2l_value_projection = make_att_head_projection(\n        long_total_att_size, 'l2l_value_projection')\n    if share_qkv_projections or share_kv_projections:\n      self.l2g_value_projection = self.l2l_value_projection\n    else:\n      self.l2g_value_projection = make_att_head_projection(\n          long_total_att_size, 'l2g_value_projection')\n\n    self.long_qkv_attention = QkvRelativeLocalAttention(\n        local_radius=local_radius,\n        relative_vocab_size=relative_vocab_size,\n        att_dropout_prob=att_dropout_prob,\n        use_one_hot_lookup=use_one_hot_lookup,\n        name='long_qkv_attention')\n\n    self.long_output_projection = _make_output_projection(\n        output_size=long_hidden_size,\n        name='long_output_projection',\n        kernel_initializer=initializer)\n\n    # Global attention layers\n\n    if share_qkv_projections:\n      self.global_query_projection = self.long_query_projection\n      self.g2g_key_projection = self.l2l_key_projection\n      self.g2l_key_projection = self.l2l_key_projection\n      self.g2g_value_projection = self.l2l_value_projection\n      self.g2l_value_projection = self.l2l_value_projection\n    else:\n      self.global_query_projection = make_att_head_projection(\n          global_total_att_size, 'global_query_projection')\n      self.g2g_key_projection = make_att_head_projection(\n          global_total_att_size, 'g2g_key_projection')\n      self.g2g_value_projection = make_att_head_projection(\n          global_total_att_size, 'g2g_value_projection')\n      if share_kv_projections:\n        self.g2l_key_projection = self.g2g_key_projection\n        self.g2l_value_projection = self.g2g_value_projection\n      else:\n        self.g2l_key_projection = make_att_head_projection(\n            global_total_att_size, 'g2l_key_projection')\n        self.g2l_value_projection = make_att_head_projection(\n            global_total_att_size, 'g2l_value_projection')\n\n    self.global_qkv_attention = QkvRelativeAttention(\n        relative_vocab_size=relative_vocab_size,\n        att_dropout_prob=att_dropout_prob,\n        use_one_hot_lookup=use_one_hot_lookup,\n        name='global_qkv_attention')\n\n    if share_att_output_projection:\n      self.global_output_projection = self.long_output_projection\n    else:\n      self.global_output_projection = _make_output_projection(\n          output_size=global_hidden_size,\n          name='global_output_projection',\n          kernel_initializer=initializer)\n\n  def call(self,\n           long_input: tf.Tensor,\n           global_input: tf.Tensor,\n           l2l_att_mask: Optional[tf.Tensor] = None,\n           g2g_att_mask: Optional[tf.Tensor] = None,\n           l2g_att_mask: Optional[tf.Tensor] = None,\n           g2l_att_mask: Optional[tf.Tensor] = None,\n           l2l_relative_att_ids: Optional[tf.Tensor] = None,\n           g2g_relative_att_ids: Optional[tf.Tensor] = None,\n           l2g_relative_att_ids: Optional[tf.Tensor] = None,\n           g2l_relative_att_ids: Optional[tf.Tensor] = None,\n           att_implementation: Text = 'auto',\n           training=None) -> List[tf.Tensor]:\n    \"\"\"Calls the layer.\n\n    We use abbreviations like \"l2g\" to mean \"long-to-global\".\n\n    Args:\n      long_input: <float32>[batch_size, long_seq_len, long_hidden_size].\n      global_input: <float32>[batch_size, global_seq_len, global_hidden_size].\n      l2l_att_mask: <int32>[batch_size, long_seq_len,  2*local_radius + 1]\n        long-to-long attention mask for local attention. Should have only 0 and\n        1 values, with 0 for entries that should be masked and 1 otherwise.\n        Leave as None to allow all long elements to attend to all other long\n        elements within the local radius.\n      g2g_att_mask: <int32>[batch_size, global_seq_len, global_seq_len]\n        global-to-global attention mask. Should have only 0 and 1 values, with 0\n        for entries that should be masked and 1 otherwise. Leave as None to\n        allow all global elements to attend to all other global elements within\n        each example.\n      l2g_att_mask: <int32>[batch_size, long_seq_len, global_seq_len]\n        long-to-global attention mask. Should have only 0 and 1 values, with 0\n        for entries that should be masked and 1 otherwise. Leave as None to\n        allow all long elements to attend to all global elements within each\n        example.\n      g2l_att_mask: <int32>[batch_size, global_seq_len, long_seq_len]\n        global-to-long attention mask. Should have only 0 and 1 values, with 0\n        for entries that should be masked and 1 otherwise. Leave as None to\n        allow all global elements to attend to all long elements within each\n        example.\n      l2l_relative_att_ids: <int32>[batch_size, long_seq_len, 2*local_radius+1]\n        long-to-long relative local self-attention ids. Leave as None to skip\n        the relative portion of l2l attention.\n      g2g_relative_att_ids: <int32>[batch_size, global_seq_len, global_seq_len]\n        global-to-global relative attention ids. Leave as None to skip the\n        relative portion of g2g attention.\n      l2g_relative_att_ids: <int32>[batch_size, long_seq_len, global_seq_len]\n        long-to-global relative attention ids. Leave as None to skip the\n        relative portion of l2g attention.\n      g2l_relative_att_ids: <int32>[batch_size, global_seq_len, long_seq_len]\n        global-to-long relative attention ids. Leave as None to skip the\n        relative portion of g2l attention.\n      att_implementation: String representing which internal attention\n        implementation to use. Valid values include 'auto' (the default),\n        'sparse', and 'full'. 'sparse' is preferred for sequences longer than\n        about 1k tokens, but 'full' may be faster for sequences shorter than\n        this. 'auto' attempts to automatically decide when to use full\n        attention. See `QkvRelativeLocalAttention` for more details.\n      training: For Keras, optional boolean scalar tensor or Python boolean\n        indicating whether the call is meant for training or inference.\n\n    Returns:\n      A list of Tensors, [long_output, global_output]:\n        long_output: <float32>[batch_size, long_seq_len, long_hidden_size]\n        global_output: <float32>[batch_size, global_seq_len, global_hidden_size]\n    \"\"\"\n    if (g2g_relative_att_ids is None) != (g2l_relative_att_ids is None):\n      raise ValueError(\n          '`g2g_relative_att_ids` and `g2l_relative_att_ids` must be either '\n          'both present or both absent.')\n\n    batch_size = tf.shape(long_input)[0]\n    long_seq_len = tf.shape(long_input)[1]\n    global_seq_len = tf.shape(global_input)[1]\n\n    # Make sure the global attention masks are not None so we can concatenate\n    # these masks together.  The local attention masks are handled separately\n    # within `QkvRelativeLocalAttention`.\n    if g2g_att_mask is None:\n      g2g_att_mask = tf.ones([batch_size, global_seq_len, global_seq_len],\n                             dtype=tf.int32)\n    if g2l_att_mask is None:\n      g2l_att_mask = tf.ones([batch_size, global_seq_len, long_seq_len],\n                             dtype=tf.int32)\n\n    # Long attention\n\n    long_queries = self.long_query_projection(long_input, training=training)\n    l2l_keys = self.l2l_key_projection(long_input, training=training)\n    l2g_keys = self.l2g_key_projection(global_input, training=training)\n    l2l_values = self.l2l_value_projection(long_input, training=training)\n    l2g_values = self.l2g_value_projection(global_input, training=training)\n\n    # [batch_size, long_seq_len, num_heads, long_hidden_size // num_heads]\n    long_att_output = self.long_qkv_attention(\n        queries=long_queries,\n        keys=l2l_keys,\n        values=l2l_values,\n        att_mask=l2l_att_mask,\n        relative_att_ids=l2l_relative_att_ids,\n        side_keys=l2g_keys,\n        side_values=l2g_values,\n        side_att_mask=l2g_att_mask,\n        side_relative_att_ids=l2g_relative_att_ids,\n        att_implementation=att_implementation,\n        training=training)\n\n    # [batch_size, long_seq_len, long_hidden_size]\n    flat_long_att_output = tensor_utils.flatten_dims(\n        long_att_output, first_dim=-2)\n\n    # [batch_size, long_seq_len, long_hidden_size]\n    long_output = self.long_output_projection(\n        flat_long_att_output, training=training)\n\n    # Global attention\n\n    global_queries = self.global_query_projection(\n        global_input, training=training)\n    # TODO(jainslie): Consider using the long attention key/value projection\n    # results above when `share_qkv_projections` is True, although graph\n    # optimization may already dedup these computations.\n    g2g_keys = self.g2g_key_projection(global_input, training=training)\n    g2l_keys = self.g2l_key_projection(long_input, training=training)\n    g2g_values = self.g2g_value_projection(global_input, training=training)\n    g2l_values = self.g2l_value_projection(long_input, training=training)\n\n    # [batch_size, global_seq_len + long_seq_len, num_heads,\n    #   global_hidden_size // num_heads]\n    concat_keys = tf.concat([g2g_keys, g2l_keys], axis=1)\n    concat_values = tf.concat([g2g_values, g2l_values], axis=1)\n\n    # [batch_size, global_seq_len, global_seq_len + long_seq_len]\n    concat_att_mask = tf.concat([g2g_att_mask, g2l_att_mask], axis=-1)\n    if g2g_relative_att_ids is None:\n      concat_relative_att_ids = None\n    else:\n      concat_relative_att_ids = tf.concat(\n          [g2g_relative_att_ids, g2l_relative_att_ids], axis=-1)\n\n    # [batch_size, global_seq_len, num_heads, global_hidden_size // num_heads]\n    global_att_output = self.global_qkv_attention(\n        queries=global_queries,\n        keys=concat_keys,\n        values=concat_values,\n        att_mask=concat_att_mask,\n        relative_att_ids=concat_relative_att_ids,\n        training=training)\n\n    # [batch_size, global_seq_len, global_hidden_size]\n    flat_global_att_output = tensor_utils.flatten_dims(\n        global_att_output, first_dim=-2)\n\n    # [batch_size, global_seq_len, global_hidden_size]\n    global_output = self.global_output_projection(\n        flat_global_att_output, training=training)\n\n    return [long_output, global_output]\n\n  def _validate_init_parameters(self) -> None:\n    if self.long_total_att_size % self.num_heads != 0:\n      raise ValueError(\n          '`long_total_att_size` must be a multiple of `num_heads`.')\n    if self.global_total_att_size % self.num_heads != 0:\n      raise ValueError(\n          '`global_total_att_size` must be a multiple of `num_heads`.')\n\n    share_qkv_or_output_projections = (\n        self.share_qkv_projections or self.share_att_output_projection)\n    if (share_qkv_or_output_projections and\n        self.long_hidden_size != self.global_hidden_size):\n      raise ValueError(\n          '`long_hidden_size` must equal `global_hidden_size` when '\n          '`share_qkv_projections` or `share_att_output_projection` is True.')\n    if (share_qkv_or_output_projections and\n        self.long_total_att_size != self.global_total_att_size):\n      raise ValueError(\n          '`long_total_att_size` must equal `global_total_att_size` when '\n          '`share_qkv_projections` or `share_att_output_projection` is True.')\n    if (self.share_kv_projections and\n        self.long_hidden_size != self.global_hidden_size):\n      raise ValueError(\n          '`long_hidden_size` must equal `global_hidden_size` when '\n          '`share_kv_projections` is True.')\n\n\nclass ProjectAttentionHeads(tf.keras.layers.Layer):\n  \"\"\"Layer for projecting a sequence to multi-head queries/keys/values.\"\"\"\n\n  def __init__(self,\n               num_heads: int,\n               size_per_head: int,\n               use_bias: bool = True,\n               initializer=None,\n               name: Text = 'attention_head_projection',\n               **kwargs):\n    \"\"\"Init.\n\n    Args:\n      num_heads: Number of attention heads.\n      size_per_head: Output size of each head.\n      use_bias: Whether to add a bias to the result. Default True.\n      initializer: Initializer to use for the kernel. The bias will be\n        initialized to 0.\n      name: Name of the layer.\n      **kwargs: Forwarded to super.\n    \"\"\"\n    super(ProjectAttentionHeads, self).__init__(name=name, **kwargs)\n\n    if num_heads < 1:\n      raise ValueError('`num_heads` must be positive.')\n    if size_per_head < 1:\n      raise ValueError('`size_per_head` must be positive.')\n\n    self.num_heads = num_heads\n    self.size_per_head = size_per_head\n    self.use_bias = use_bias\n    self.initializer = initializer\n\n    self.linear = tf.keras.layers.Dense(\n        units=num_heads * size_per_head,\n        activation=None,\n        use_bias=use_bias,\n        kernel_initializer=initializer,\n        bias_initializer='zeros',\n        name='linear')\n\n  def call(self, inputs: tf.Tensor) -> tf.Tensor:\n    \"\"\"Calls the layer.\n\n    Args:\n      inputs: <float32>[batch_size, ..., hidden_size].\n\n    Returns:\n      <float32>[batch_size, ..., num_heads, size_per_head].\n    \"\"\"\n    # [batch_size, ..., num_heads * size_per_head]\n    x = self.linear(inputs)\n\n    output_shape = tf.concat(\n        [tf.shape(inputs)[:-1], [self.num_heads, self.size_per_head]], 0)\n    return tf.reshape(x, output_shape)\n\n\nclass QkvRelativeAttention(tf.keras.layers.Layer):\n  \"\"\"Relative attention layer over queries, keys, and values (\"qkv\").\n\n  This implements scaled dot-product attention with (optional) relative\n  position representations. We allow the user to supply arbitrary\n  relative edges in `relative_att_ids` rather than forcing an edge pattern\n  based on relative distance.\n\n  For the original scaled dot-product formulation without relative position,\n  see https://arxiv.org/abs/1706.03762 .\n  For the original formulation of relative position representations, see\n  https://arxiv.org/abs/1803.02155 .\n  For the relative position formulation with bias terms included, see\n  https://arxiv.org/abs/1901.02860 . Note we don't include a global content\n  bias term in this implementation since it may already be included in\n  the projection that creates `queries`.\n  \"\"\"\n\n  def __init__(\n      self,\n      relative_vocab_size: Optional[int] = None,\n      att_dropout_prob: float = 0.0,\n      initializer=tf.keras.initializers.TruncatedNormal(stddev=0.5),\n      use_one_hot_lookup: bool = False,\n      name: Text = 'qkv_relative_attention',\n      **kwargs):\n    \"\"\"Init.\n\n    Args:\n      relative_vocab_size:  Size of relative position vocabulary. If left\n        unspecified, relative positions will be ignored for attention.\n      att_dropout_prob: Dropout probability for attention probabilities. Must be\n        between 0.0 and 1.0. The default of 0.0 skips dropout.\n      initializer: Initializer to use for relative embedding table. Ideally, the\n        initialization scale should probably be comparable to the expected scale\n        of the `keys`.\n      use_one_hot_lookup: Whether to use tf.one_hot for relative embedding\n        lookup instead of tf.gather. Default is False, but setting to True may\n        be more efficient on TPUs for vocab sizes that aren't too large.\n      name: Name of the layer.\n      **kwargs: Forwarded to super.\n    \"\"\"\n    super(QkvRelativeAttention, self).__init__(name=name, **kwargs)\n\n    if relative_vocab_size is not None and relative_vocab_size < 1:\n      raise ValueError('`relative_vocab_size` must be positive.')\n\n    self.relative_vocab_size = relative_vocab_size\n    self.att_dropout_prob = att_dropout_prob\n    self.initializer = initializer\n    self.use_one_hot_lookup = use_one_hot_lookup\n\n    if att_dropout_prob != 0.0:\n      self.att_dropout = recomputing_dropout.RecomputingDropout(\n          rate=att_dropout_prob)\n\n    self._is_custom_built = False\n\n  # The Keras `build` method only receives the shape for `queries`, but we need\n  # the shape of `values` also, so we created this custom build method.\n  def _custom_build(self, queries_shape: tf.TensorShape,\n                    values_shape: tf.TensorShape) -> None:\n    \"\"\"Build function with custom shape arguments.\n\n    Args:\n      queries_shape: [batch_size, query_len, num_heads, key_size_per_head] shape\n        of queries Tensor.\n      values_shape: [batch_size, key_len, num_heads, value_size_per_head] shape\n        of values Tensor.\n    \"\"\"\n    num_heads = queries_shape.as_list()[-2]\n    key_size_per_head = queries_shape.as_list()[-1]\n\n    with tf.init_scope():  # Make sure this happens in eager.\n      if self.relative_vocab_size is not None:\n        self.relative_emb_table = self.add_weight(\n            name='relative_emb_table',\n            shape=[self.relative_vocab_size, num_heads, key_size_per_head],\n            initializer=self.initializer,\n            trainable=True)\n        self.relative_bias_table = self.add_weight(\n            name='relative_bias_table',\n            shape=[self.relative_vocab_size, num_heads],\n            initializer='zeros',\n            trainable=True)\n\n    self._is_custom_built = True\n\n  def call(self,\n           queries: tf.Tensor,\n           keys: tf.Tensor,\n           values: tf.Tensor,\n           att_mask: Optional[tf.Tensor] = None,\n           relative_att_ids: Optional[tf.Tensor] = None,\n           training=None) -> tf.Tensor:\n    \"\"\"Calls the layer.\n\n    Args:\n      queries: <float32>[batch_size, query_len, num_heads, key_size_per_head].\n      keys: <float32>[batch_size, key_len, num_heads, key_size_per_head].\n      values: <float32>[batch_size, key_len, num_heads, value_size_per_head].\n      att_mask: <int32>[batch_size, query_len, key_len]. Should have only 0 and\n        1 values, with 0 for keys that should be masked and 1 otherwise. Leave\n        as None to allow all elements to attend to all other elements within\n        each example.\n      relative_att_ids: <int32>[batch_size, query_len, key_len]. Leave as None\n        to skip the relative portion of attention.\n      training: For Keras, optional boolean scalar tensor or Python boolean\n        indicating whether the call is meant for training or inference.\n\n    Returns:\n      <float32>[batch_size, query_len, num_heads, value_size_per_head].\n    \"\"\"\n    if not self._is_custom_built:\n      self._custom_build(queries_shape=queries.shape, values_shape=values.shape)\n\n    if relative_att_ids is not None and self.relative_vocab_size is None:\n      raise ValueError('Cannot use `relative_att_ids` without specifying '\n                       '`relative_vocab_size`.')\n\n    # `queries` shape: [batch_size, query_len, num_heads, key_size_per_head]\n    # `keys` shape: [batch_size, key_len, num_heads, key_size_per_head]\n\n    # [batch_size, query_len, key_len, num_heads]\n    content_att_scores = tf.einsum('bqhd,bkhd->bqkh', queries, keys)\n\n    # TODO(jainslie): Add option to overlay sinusoid encoding matrix in\n    # addition to (or instead of) explicit relative position embeddings.\n    if relative_att_ids is None:\n      att_scores = content_att_scores\n    else:\n      # `self.relative_emb_table` shape:\n      # [relative_vocab_size, num_heads, key_size_per_head]\n\n      # [batch_size, query_len, relative_vocab_size, num_heads]\n      all_relative_scores = tf.einsum('bqhd,rhd->bqrh', queries,\n                                      self.relative_emb_table)\n      all_relative_scores += self.relative_bias_table\n\n      # `relative_att_ids` shape: [batch_size, query_len, key_len]\n\n      # `relative_att_scores` shape: [batch_size, query_len, key_len, num_heads]\n      if self.use_one_hot_lookup:\n        relative_att_scores = tensor_utils.batch_gather_by_one_hot(\n            all_relative_scores, relative_att_ids, batch_dims=2)\n      else:\n        relative_att_scores = tf.gather(\n            all_relative_scores, relative_att_ids, batch_dims=2)\n\n      att_scores = content_att_scores + relative_att_scores\n\n    key_size_per_head = tf.shape(keys)[-1]\n\n    # `att_scores` shape: [batch_size, query_len, key_len, num_heads]\n    att_scores /= tf.sqrt(tf.cast(key_size_per_head, att_scores.dtype))\n\n    if att_mask is not None:\n      # `att_mask` shape: [batch_size, query_len, key_len]\n\n      # [batch_size, query_len, key_len, 1]\n      mask_adder = -10000.0 * (\n          1.0 - tf.cast(att_mask[:, :, :, tf.newaxis], att_scores.dtype))\n\n      att_scores += mask_adder\n\n    # [batch_size, query_len, key_len, num_heads]\n    att_probs = tf.nn.softmax(att_scores, axis=-2)\n\n    if self.att_dropout_prob != 0.0:\n      # Transpose `att_probs` for a memory layout with no padding on TPUs.\n      # [batch_size, num_heads, query_len, key_len]\n      att_probs = self.att_dropout(\n          tf.transpose(att_probs, [0, 3, 1, 2]), training=training)\n      return tf.einsum('bhqk,bkhd->bqhd', att_probs, values)\n\n    # `values` shape: [batch_size, key_len, num_heads, value_size_per_head]\n\n    # [batch_size, query_len, num_heads, value_size_per_head]\n    return tf.einsum('bqkh,bkhd->bqhd', att_probs, values)\n\n\nclass QkvRelativeLocalAttention(tf.keras.layers.Layer):\n  \"\"\"Relative local attention layer over queries, keys, and values (\"qkv\").\n\n  This layer is similar to the `QkvRelativeAttention` layer except it's\n  specialized for efficient self-attention over a long input sequence via\n  a locality constraint. The layer assumes the long input is already projected\n  to queries, keys, and values, and it accepts optional side keys and values\n  for every query to attend to also. Efficiency is maintained for long inputs\n  by only allowing tokens to attend to other tokens within a `local_radius`,\n  resulting in complexity that scales linearly in the long input length\n  (for a fixed `local_radius`) rather than quadratically in the input length.\n\n  If the input sequence isn't actually that long (e.g. ~1k tokens or less),\n  it may be faster to use the full-attention implementation internally,\n  which is available via the `att_implementation` argument to `call`.\n\n  Just like `QkvRelativeAttention`, attention masking and relative attention ids\n  can further constrain or customize how attention behaves.\n  \"\"\"\n\n  def __init__(\n      self,\n      local_radius: int,\n      relative_vocab_size: Optional[int] = None,\n      att_dropout_prob: float = 0.0,\n      initializer=tf.keras.initializers.TruncatedNormal(stddev=0.5),\n      use_one_hot_lookup: bool = False,\n      name: Text = 'qkv_relative_local_att',\n      **kwargs):\n    \"\"\"Init.\n\n    Args:\n      local_radius: How many tokens to the left/right to locally attend to. For\n        example, a value of 1 would allow each token to only attend to 1 token\n        to the left and 1 token to the right of it.\n      relative_vocab_size:  Size of relative position vocabulary. If left\n        unspecified, relative positions will be ignored for attention.\n      att_dropout_prob: Dropout probability for attention probabilities. Must be\n        between 0.0 and 1.0. The default of 0.0 skips dropout.\n      initializer: Initializer to use for relative embedding table. Ideally, the\n        initialization scale should probably be comparable to the expected scale\n        of the `keys`.\n      use_one_hot_lookup: Whether to use tf.one_hot for relative embedding\n        lookup instead of tf.gather. Default is False, but setting to True may\n        be more efficient on TPUs for vocab sizes that aren't too large.\n      name: Name of the layer.\n      **kwargs: Forwarded to super.\n    \"\"\"\n    super(QkvRelativeLocalAttention, self).__init__(name=name, **kwargs)\n\n    if local_radius < 1:\n      raise ValueError('`local_radius` must be positive.')\n\n    self.local_radius = local_radius\n    self.relative_vocab_size = relative_vocab_size\n    self.att_dropout_prob = att_dropout_prob\n    self.initializer = initializer\n    self.use_one_hot_lookup = use_one_hot_lookup\n\n    self.qkv_relative_attention = QkvRelativeAttention(\n        relative_vocab_size=relative_vocab_size,\n        att_dropout_prob=att_dropout_prob,\n        initializer=initializer,\n        use_one_hot_lookup=use_one_hot_lookup)\n\n  def call(self,\n           queries: tf.Tensor,\n           keys: tf.Tensor,\n           values: tf.Tensor,\n           att_mask: Optional[tf.Tensor] = None,\n           relative_att_ids: Optional[tf.Tensor] = None,\n           side_keys: Optional[tf.Tensor] = None,\n           side_values: Optional[tf.Tensor] = None,\n           side_att_mask: Optional[tf.Tensor] = None,\n           side_relative_att_ids: Optional[tf.Tensor] = None,\n           att_implementation: Text = 'auto',\n           training=None) -> tf.Tensor:\n    \"\"\"Calls the layer.\n\n    Args:\n      queries: <float32>[batch_size, long_len, num_heads, key_size_per_head].\n      keys: <float32>[batch_size, long_len, num_heads, key_size_per_head].\n      values: <float32>[batch_size, long_len, num_heads, value_size_per_head].\n      att_mask: <int32>[batch_size, long_len, 2*local_radius + 1]. For the i-th\n        example and j-th long token (with 0-based indexing), `att_mask[i, j, :]`\n          is the local attention mask centered around j. It should have only 0\n          and 1 values, with 0 for keys that should be masked and 1 otherwise.\n          Leave as None to allow all tokens to attend to all other local tokens.\n      relative_att_ids: <int32>[batch_size, long_len, 2*local_radius + 1]. Leave\n        as None to skip the relative portion of attention.\n      side_keys: <float32>[batch_size, side_len, num_heads, key_size_per_head].\n        Keys of the optional side inputs for all queries to attend to.\n      side_values: <float32>[batch_size, side_len, num_heads,\n        value_size_per_head]. Values of the optional side inputs for all queries\n        to attend to.\n      side_att_mask: <int32>[batch_size, long_len, side_len]. Analogous 0/1 mask\n        for side inputs with 0 for side keys that should be masked and 1\n        otherwise. Leave as None to allow attention to all side inputs.\n      side_relative_att_ids: <int32>[batch_size, long_len, side_len]. Relative\n        attention for side inputs. Must be None if and only if\n        `relative_att_ids` is None.\n      att_implementation: String representing which internal attention\n        implementation to use. Valid values include 'auto' (the default),\n        'sparse', and 'full'. 'sparse' is preferred for sequences longer than\n        about 1k tokens, but 'full' may be faster for sequences shorter than\n        this. 'auto' defaults to 'sparse' but chooses 'full' if `long_len` is\n        statically known and is no more than 1024 (a number subject to change\n        in the future). The 'full' implementation has quadratic time and\n        memory complexity in the `long_len`, whereas 'sparse' is roughly\n        linear (for fixed `side_len`).\n      training: For Keras, optional boolean scalar tensor or Python boolean\n        indicating whether the call is meant for training or inference.\n\n    Returns:\n      <float32>[batch_size, long_len, num_heads, value_size_per_head].\n    \"\"\"\n    if (side_keys is None) != (side_values is None):\n      raise ValueError(\n          '`side_keys` and `side_values` must be either both present or both '\n          'absent.')\n    if side_att_mask is not None and side_keys is None:\n      raise ValueError(\n          '`side_keys` must be present when specifying `side_att_mask`.')\n    if side_relative_att_ids is not None and side_keys is None:\n      raise ValueError('`side_keys` must be present when specifying '\n                       '`side_relative_att_ids`.')\n    if (side_keys is not None and (relative_att_ids is None) !=\n        (side_relative_att_ids is None)):\n      raise ValueError(\n          '`relative_att_ids` and `side_relative_att_ids` must be either both '\n          'present or both absent when using side inputs.')\n    if att_implementation not in ['auto', 'sparse', 'full']:\n      raise ValueError(\n          '`att_implementation` must be one of [\"auto\", \"sparse\", \"full\"], '\n          'but got: \"{}\"'.format(att_implementation))\n\n    (batch_size, long_len, num_heads,\n     value_size_per_head) = tensor_utils.get_shape_list(values)\n\n    static_long_len = values.shape.as_list()[1]\n    if att_implementation == 'full' or (att_implementation == 'auto' and\n                                        static_long_len is not None and\n                                        static_long_len <= 1024):\n      return self._call_full_att_implementation(\n          queries=queries,\n          keys=keys,\n          values=values,\n          att_mask=att_mask,\n          relative_att_ids=relative_att_ids,\n          side_keys=side_keys,\n          side_values=side_values,\n          side_att_mask=side_att_mask,\n          side_relative_att_ids=side_relative_att_ids,\n          training=training)\n\n    # We add 1 since the token itself is part of the block.\n    block_len = self.local_radius + 1\n\n    # [batch_size, num_blocks, block_len, num_heads, key_size_per_head]\n    # where num_blocks = ceiling(long_len / block_len)\n    blocked_queries = tensor_utils.split_into_blocks(\n        queries, block_len=block_len, axis=1)\n\n    num_blocks = tensor_utils.get_shape_list(blocked_queries)[1]\n\n    # [batch_size, num_blocks, 3*block_len, num_heads, key_size_per_head]\n    blocked_keys = tensor_utils.concat_3_blocks(\n        tensor_utils.split_into_blocks(keys, block_len=block_len, axis=1))\n\n    # [batch_size, num_blocks, 3*block_len, num_heads, value_size_per_head]\n    blocked_values = tensor_utils.concat_3_blocks(\n        tensor_utils.split_into_blocks(values, block_len=block_len, axis=1))\n\n    if att_mask is None:\n      # Create a locality mask so tokens can't attend beyond `local_radius`.\n      att_mask = tf.ones([batch_size, long_len, 2 * self.local_radius + 1],\n                         dtype=tf.int32)\n\n    # [batch_size, num_blocks, block_len, 3*block_len]\n    blocked_att_mask = _expand_local_ids_to_blocks(att_mask)\n\n    # [batch_size, num_blocks, block_len, 3*block_len]\n    blocked_relative_att_ids = (None if relative_att_ids is None else\n                                _expand_local_ids_to_blocks(relative_att_ids))\n\n    if side_keys is not None:\n      if side_att_mask is None:\n        side_len = tf.shape(side_keys)[1]\n        side_att_mask = tf.ones([batch_size, long_len, side_len],\n                                dtype=tf.int32)\n      (\n          # [batch_size, num_blocks, 3*block_len + side_len, num_heads,\n          #   key_size_per_head]\n          blocked_keys,\n          # [batch_size, num_blocks, 3*block_len + side_len, num_heads,\n          #   value_size_per_head]\n          blocked_values,\n          # [batch_size, num_blocks, block_len, 3*block_len + side_len]\n          blocked_att_mask,\n          # [batch_size, num_blocks, block_len, 3*block_len + side_len] or None\n          blocked_relative_att_ids  #\n      ) = self._concat_side_inputs(blocked_keys, blocked_values,\n                                   blocked_att_mask, blocked_relative_att_ids,\n                                   side_keys, side_values, side_att_mask,\n                                   side_relative_att_ids)\n\n    # [batch_size * num_blocks, block_len, num_heads, key_size_per_head]\n    flat_blocked_queries = tensor_utils.flatten_dims(\n        blocked_queries, last_dim=1)\n\n    # [batch_size * num_blocks, 3*block_len + side_len, num_heads,\n    #   key_size_per_head]\n    flat_blocked_keys = tensor_utils.flatten_dims(blocked_keys, last_dim=1)\n\n    # [batch_size * num_blocks, 3*block_len + side_len, num_heads,\n    #   value_size_per_head]\n    flat_blocked_values = tensor_utils.flatten_dims(blocked_values, last_dim=1)\n\n    # [batch_size * num_blocks, block_len, 3*block_len + side_len]\n    flat_blocked_att_mask = tensor_utils.flatten_dims(\n        blocked_att_mask, last_dim=1)\n\n    # [batch_size * num_blocks, block_len, 3*block_len + side_len]\n    flat_blocked_relative_att_ids = (None if blocked_relative_att_ids is None\n                                     else tensor_utils.flatten_dims(\n                                         blocked_relative_att_ids, last_dim=1))\n\n    # [batch_size * num_blocks, block_len, num_heads, value_size_per_head]\n    flat_blocked_att_result = self.qkv_relative_attention(\n        queries=flat_blocked_queries,\n        keys=flat_blocked_keys,\n        values=flat_blocked_values,\n        att_mask=flat_blocked_att_mask,\n        relative_att_ids=flat_blocked_relative_att_ids,\n        training=training)\n\n    # [batch_size, num_blocks * block_len, num_heads, value_size_per_head]\n    padded_att_result = tf.reshape(\n        flat_blocked_att_result,\n        [batch_size, num_blocks * block_len, num_heads, value_size_per_head])\n\n    # [batch_size, long_len, num_heads, value_size_per_head]\n    return padded_att_result[:, :long_len, :, :]\n\n  def _concat_side_inputs(\n      self, blocked_keys: tf.Tensor, blocked_values: tf.Tensor,\n      blocked_att_mask: tf.Tensor,\n      blocked_relative_att_ids: Optional[tf.Tensor], side_keys: tf.Tensor,\n      side_values: tf.Tensor, side_att_mask: tf.Tensor,\n      side_relative_att_ids: Optional[tf.Tensor]\n  ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, Optional[tf.Tensor]]:\n    \"\"\"Concatenates side inputs to blocked long inputs, returning the result.\"\"\"\n\n    def concat_side_vectors(blocked_vectors, side_vectors):\n      \"\"\"Concatenates side keys or values to blocked keys or values.\n\n      Args:\n        blocked_vectors: <float>[batch_size, num_blocks, 3*block_len, num_heads,\n          size_per_head]\n        side_vectors: <float>[batch_size, side_len, num_heads, size_per_head]\n\n      Returns:\n        <float>[batch_size, num_blocks, 3*block_len + side_len, num_heads,\n          size_per_head]\n      \"\"\"\n      num_blocks = tf.shape(blocked_vectors)[1]\n\n      # [batch_size, num_blocks, side_len, num_heads, size_per_head]\n      expanded_side_vectors = tf.tile(side_vectors[:, tf.newaxis, :, :, :],\n                                      [1, num_blocks, 1, 1, 1])\n\n      return tf.concat([blocked_vectors, expanded_side_vectors], axis=2)\n\n    # [batch_size, num_blocks, 3*block_len + side_len, num_heads,\n    #   key_size_per_head]\n    blocked_keys = concat_side_vectors(blocked_keys, side_keys)\n\n    # [batch_size, num_blocks, 3*block_len + side_len, num_heads,\n    #   value_size_per_head]\n    blocked_values = concat_side_vectors(blocked_values, side_values)\n\n    def concat_side_ids(blocked_ids, side_ids):\n      \"\"\"Concatenates side mask or relative attention ids to blocked ones.\n\n      Args:\n        blocked_ids: <int32>[batch_size, num_blocks, block_len, 3*block_len]\n        side_ids: <int32>[batch_size, long_len, side_len]\n\n      Returns:\n        <int32>[batch_size, num_blocks, block_len, 3*block_len + side_len]\n      \"\"\"\n      block_len = blocked_ids.shape.as_list()[2]\n\n      # [batch_size, num_blocks, block_len, side_len]\n      blocked_side_ids = tensor_utils.split_into_blocks(\n          side_ids, block_len=block_len, axis=1)\n\n      return tf.concat([blocked_ids, blocked_side_ids], axis=-1)\n\n    # [batch_size, num_blocks, block_len, 3*block_len + side_len]\n    blocked_att_mask = concat_side_ids(blocked_att_mask, side_att_mask)\n\n    if blocked_relative_att_ids is not None:\n      # [batch_size, num_blocks, block_len, 3*block_len + side_len]\n      blocked_relative_att_ids = concat_side_ids(blocked_relative_att_ids,\n                                                 side_relative_att_ids)\n\n    return (blocked_keys, blocked_values, blocked_att_mask,\n            blocked_relative_att_ids)\n\n  def _call_full_att_implementation(\n      self,\n      queries: tf.Tensor,\n      keys: tf.Tensor,\n      values: tf.Tensor,\n      att_mask: Optional[tf.Tensor] = None,\n      relative_att_ids: Optional[tf.Tensor] = None,\n      side_keys: Optional[tf.Tensor] = None,\n      side_values: Optional[tf.Tensor] = None,\n      side_att_mask: Optional[tf.Tensor] = None,\n      side_relative_att_ids: Optional[tf.Tensor] = None,\n      training=None) -> tf.Tensor:\n    \"\"\"Calls the full-attention implementation.\n\n    Args:\n      queries: <float32>[batch_size, long_len, num_heads, key_size_per_head].\n      keys: <float32>[batch_size, long_len, num_heads, key_size_per_head].\n      values: <float32>[batch_size, long_len, num_heads, value_size_per_head].\n      att_mask: <int32>[batch_size, long_len, 2*local_radius + 1].\n      relative_att_ids: <int32>[batch_size, long_len, 2*local_radius + 1].\n      side_keys: <float32>[batch_size, side_len, num_heads, key_size_per_head].\n      side_values: <float32>[batch_size, side_len, num_heads,\n        value_size_per_head].\n      side_att_mask: <int32>[batch_size, long_len, side_len].\n      side_relative_att_ids: <int32>[batch_size, long_len, side_len].\n      training: For Keras, optional boolean scalar tensor or Python boolean\n        indicating whether the call is meant for training or inference.\n\n    Returns:\n      <float32>[batch_size, long_len, num_heads, value_size_per_head].\n    \"\"\"\n    batch_size = tensor_utils.get_shape_list(queries)[0]\n    long_len = tensor_utils.get_shape_list(queries)[1]\n\n    if att_mask is None:\n      att_mask = tf.ones([batch_size, long_len, 2 * self.local_radius + 1],\n                         dtype=tf.int32)\n\n    # [batch_size, long_len, long_len]\n    skewed_att_mask = tensor_utils.skew_elements_right(\n        att_mask, axis=-1)[..., self.local_radius:-self.local_radius]\n\n    if relative_att_ids is None:\n      skewed_relative_att_ids = None\n    else:\n      # [batch_size, long_len, long_len]\n      skewed_relative_att_ids = tensor_utils.skew_elements_right(\n          relative_att_ids, axis=-1)[..., self.local_radius:-self.local_radius]\n\n    concat_keys = keys\n    concat_values = values\n    concat_att_mask = skewed_att_mask\n    concat_relative_att_ids = skewed_relative_att_ids\n\n    if side_keys is not None:\n      concat_keys = tf.concat([concat_keys, side_keys], axis=1)\n      concat_values = tf.concat([concat_values, side_values], axis=1)\n\n      if side_att_mask is None:\n        side_len = tf.shape(side_keys)[1]\n        side_att_mask = tf.ones([batch_size, long_len, side_len],\n                                dtype=tf.int32)\n      concat_att_mask = tf.concat([concat_att_mask, side_att_mask], axis=-1)\n\n      if concat_relative_att_ids is not None:\n        concat_relative_att_ids = tf.concat(\n            [concat_relative_att_ids, side_relative_att_ids], axis=-1)\n\n    # [batch_size, long_len, num_heads, value_size_per_head]\n    return self.qkv_relative_attention(\n        queries=queries,\n        keys=concat_keys,\n        values=concat_values,\n        att_mask=concat_att_mask,\n        relative_att_ids=concat_relative_att_ids,\n        training=training)\n\n\ndef _make_output_projection(output_size: int, name: Text, kernel_initializer):\n  \"\"\"Helper for output projection.\"\"\"\n  return tf.keras.layers.Dense(\n      units=output_size,\n      activation=None,\n      use_bias=True,\n      kernel_initializer=kernel_initializer,\n      bias_initializer='zeros',\n      name=name)\n\n\ndef _expand_local_ids_to_blocks(local_ids: tf.Tensor,\n                                mask_padding_ids: bool = True) -> tf.Tensor:\n  \"\"\"Helper to expand local ids to blocked format.\n\n  Args:\n    local_ids: [batch_size, seq_len, 2*local_radius + 1] shaped Tensor. This is\n      the shape of the `att_mask` and `relative_att_id` Tensors in the\n      `QkvRelativeLocalAttention` layer.\n    mask_padding_ids: If True (the default), zero out ids representing attention\n      from the first tokens to padding tokens before the start of the sequence\n      and from the last tokens to padding tokens after the end of the sequence.\n      When the ids are actually attention masks (such that 0 means no attention)\n      this removes attention to any padding tokens beyond the boundaries of the\n      sequence.\n\n  Returns:\n    A Tensor of shape [batch_size, num_blocks, block_len, 3*block_len],\n    where block_len = local_radius + 1, and\n    num_blocks = ceiling(seq_len / block_len).\n  \"\"\"\n  batch_size = tf.shape(local_ids)[0]\n  local_window_size = local_ids.shape.as_list()[-1]\n  if local_window_size is None:\n    raise ValueError('`local_ids.shape[-1]` must be known statically.')\n  block_len = local_window_size // 2 + 1\n\n  if mask_padding_ids:\n    padding_mask = feature_utils.make_local_segmented_att_mask(\n        tf.ones(tf.shape(local_ids)[:-1]), local_radius=block_len - 1)\n    local_ids *= tf.cast(padding_mask, local_ids.dtype)\n\n  # [batch_size, num_blocks, block_len, 2*local_radius + 1]\n  blocked_ids = tensor_utils.split_into_blocks(\n      local_ids, block_len=block_len, axis=1)\n\n  # [batch_size, num_blocks, block_len, 3*block_len - 2]\n  skewed_ids = tensor_utils.skew_elements_right(blocked_ids, axis=-1)\n\n  # [batch_size, num_blocks, block_len, 3*block_len]\n  result = tf.pad(skewed_ids, [[0, 0], [0, 0], [0, 0], [1, 1]])\n\n  if not mask_padding_ids:\n    return result\n\n  ones = tf.ones_like(result, dtype=result.dtype)\n\n  # [batch_size, 1, block_len, 3*block_len]\n  leftmost_mask = tf.concat([\n      tf.zeros([batch_size, 1, block_len, block_len], dtype=ones.dtype),\n      tf.ones([batch_size, 1, block_len, 2 * block_len], dtype=ones.dtype)\n  ], -1)\n\n  # [batch_size, 1, block_len, 3*block_len]\n  rightmost_mask = tf.concat([\n      tf.ones([batch_size, 1, block_len, 2 * block_len], dtype=ones.dtype),\n      tf.zeros([batch_size, 1, block_len, block_len], dtype=ones.dtype)\n  ], -1)\n\n  result *= tf.concat([leftmost_mask, ones[:, 1:, :, :]], 1)\n  result *= tf.concat([ones[:, :-1, :, :], rightmost_mask], 1)\n  return result",
    "description": null,
    "url": null
}