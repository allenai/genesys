{
    "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "acronym": "roformer",
    "s2id": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
    "abstract": "N/A",
    "venue": "Neurocomputing",
    "year": 2021,
    "tldr": "A novel method named Rotary Position Embedding(RoPE) is proposed to effectively leverage the positional information in transformer-based language models and enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding.",
    "citationCount": 978,
    "influentialCitationCount": 100,
    "seed_ids": [
        "performer",
        "lineartransformer",
        "transformer",
        "gpt2",
        "gpt"
    ],
    "code": null
}