{
    "acronym": "roformer",
    "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "seed_ids": [
        "performer",
        "lineartransformer",
        "transformer",
        "gpt2",
        "gpt"
    ],
    "s2id": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
    "abstract": "N/A",
    "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Bo Wen",
        "Yunfeng Liu"
    ],
    "venue": "Neurocomputing",
    "year": 2021,
    "tldr": "A novel method named Rotary Position Embedding(RoPE) is proposed to effectively leverage the positional information in transformer-based language models and enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding.",
    "citationCount": 978,
    "influentialCitationCount": 100,
    "code": "from __future__ import annotations\nfrom math import pi, log\n\nimport torch\nfrom torch.nn import Module, ModuleList\nfrom torch.cuda.amp import autocast\nfrom torch import nn, einsum, broadcast_tensors, Tensor\n\nfrom einops import rearrange, repeat\n\nfrom typing import Literal\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\n# broadcat, as tortoise-tts was using it\n\ndef broadcat(tensors, dim = -1):\n    broadcasted_tensors = broadcast_tensors(*tensors)\n    return torch.cat(broadcasted_tensors, dim = dim)\n\n# rotary embedding helper functions\n\ndef rotate_half(x):\n    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return rearrange(x, '... d r -> ... (d r)')\n\n@autocast(enabled = False)\ndef apply_rotary_emb(freqs, t, start_index = 0, scale = 1., seq_dim = -2):\n    dtype = t.dtype\n\n    if t.ndim == 3:\n        seq_len = t.shape[seq_dim]\n        freqs = freqs[-seq_len:]\n\n    rot_dim = freqs.shape[-1]\n    end_index = start_index + rot_dim\n\n    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'\n\n    t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n    out = torch.cat((t_left, t, t_right), dim = -1)\n\n    return out.type(dtype)\n\n# learned rotation helpers\n\ndef apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n    if exists(freq_ranges):\n        rotations = einsum('..., f -> ... f', rotations, freq_ranges)\n        rotations = rearrange(rotations, '... r f -> ... (r f)')\n\n    rotations = repeat(rotations, '... n -> ... (n r)', r = 2)\n    return apply_rotary_emb(rotations, t, start_index = start_index)\n\n# classes\n\nclass RotaryEmbedding(Module):\n    def __init__(\n        self,\n        dim,\n        custom_freqs: Tensor | None = None,\n        freqs_for:  Literal['lang', 'pixel', 'constant'] = 'lang',\n        theta = 10000,\n        max_freq = 10,\n        num_freqs = 1,\n        learned_freq = False,\n        use_xpos = False,\n        xpos_scale_base = 512,\n        interpolate_factor = 1.,\n        theta_rescale_factor = 1.,\n        seq_before_head_dim = False,\n        cache_if_possible = True\n    ):\n        super().__init__()\n        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n        # has some connection to NTK literature\n        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n\n        theta *= theta_rescale_factor ** (dim / (dim - 2))\n\n        self.freqs_for = freqs_for\n\n        if exists(custom_freqs):\n            freqs = custom_freqs\n        elif freqs_for == 'lang':\n            freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n        elif freqs_for == 'pixel':\n            freqs = torch.linspace(1., max_freq / 2, dim // 2) * pi\n        elif freqs_for == 'constant':\n            freqs = torch.ones(num_freqs).float()\n\n        self.cache_if_possible = cache_if_possible\n\n        self.tmp_store('cached_freqs', None)\n        self.tmp_store('cached_scales', None)\n\n        self.freqs = nn.Parameter(freqs, requires_grad = learned_freq)\n\n        self.learned_freq = learned_freq\n\n        # dummy for device\n\n        self.tmp_store('dummy', torch.tensor(0))\n\n        # default sequence dimension\n\n        self.seq_before_head_dim = seq_before_head_dim\n        self.default_seq_dim = -3 if seq_before_head_dim else -2\n\n        # interpolation factors\n\n        assert interpolate_factor >= 1.\n        self.interpolate_factor = interpolate_factor\n\n        # xpos\n\n        self.use_xpos = use_xpos\n        if not use_xpos:\n            self.tmp_store('scale', None)\n            return\n\n        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n        self.scale_base = xpos_scale_base\n        self.tmp_store('scale', scale)\n\n        # add apply_rotary_emb as static method\n\n        self.apply_rotary_emb = staticmethod(apply_rotary_emb)\n\n    @property\n    def device(self):\n        return self.dummy.device\n\n    def tmp_store(self, key, value):\n        self.register_buffer(key, value, persistent = False)\n\n    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n\n    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, scale = None):\n        seq_dim = default(seq_dim, self.default_seq_dim)\n\n        assert not self.use_xpos or exists(scale), 'you must use `.rotate_queries_and_keys` method instead and pass in both queries and keys, for length extrapolatable rotary embeddings'\n\n        device, dtype, seq_len = t.device, t.dtype, t.shape[seq_dim]\n\n        seq = self.get_seq_pos(seq_len, device = device, dtype = dtype, offset = offset)\n\n        freqs = self.forward(seq, seq_len = seq_len, offset = offset)\n\n        if seq_dim == -3:\n            freqs = rearrange(freqs, 'n d -> n 1 d')\n\n        return apply_rotary_emb(freqs, t, scale = default(scale, 1.), seq_dim = seq_dim)\n\n    def rotate_queries_with_cached_keys(self, q, k, seq_dim = None, offset = 0):\n        dtype, device, seq_dim = q.dtype, q.device, default(seq_dim, self.default_seq_dim)\n\n        q_len, k_len = q.shape[seq_dim], k.shape[seq_dim]\n        assert q_len <= k_len\n\n        q_scale = k_scale = 1.\n\n        if self.use_xpos:\n            seq = self.get_seq_pos(k_len, dtype = dtype, device = device)\n\n            q_scale = self.get_scale(seq[-q_len:]).type(dtype)\n            k_scale = self.get_scale(seq).type(dtype)\n\n        rotated_q = self.rotate_queries_or_keys(q, seq_dim = seq_dim, scale = q_scale, offset = k_len - q_len + offset)\n        rotated_k = self.rotate_queries_or_keys(k, seq_dim = seq_dim, scale = k_scale ** -1)\n\n        rotated_q = rotated_q.type(q.dtype)\n        rotated_k = rotated_k.type(k.dtype)\n\n        return rotated_q, rotated_k\n\n    def rotate_queries_and_keys(self, q, k, seq_dim = None):\n        seq_dim = default(seq_dim, self.default_seq_dim)\n\n        assert self.use_xpos\n        device, dtype, seq_len = q.device, q.dtype, q.shape[seq_dim]\n\n        seq = self.get_seq_pos(seq_len, dtype = dtype, device = device)\n\n        freqs = self.forward(seq, seq_len = seq_len)\n        scale = self.get_scale(seq, seq_len = seq_len).to(dtype)\n\n        if seq_dim == -3:\n            freqs = rearrange(freqs, 'n d -> n 1 d')\n            scale = rearrange(scale, 'n d -> n 1 d')\n\n        rotated_q = apply_rotary_emb(freqs, q, scale = scale, seq_dim = seq_dim)\n        rotated_k = apply_rotary_emb(freqs, k, scale = scale ** -1, seq_dim = seq_dim)\n\n        rotated_q = rotated_q.type(q.dtype)\n        rotated_k = rotated_k.type(k.dtype)\n\n        return rotated_q, rotated_k\n\n    def get_scale(\n        self,\n        t: Tensor,\n        seq_len: int | None = None,\n        offset = 0\n    ):\n        assert self.use_xpos\n\n        should_cache = (\n            self.cache_if_possible and\n            exists(seq_len)\n        )\n\n        if (\n            should_cache and \\\n            exists(self.cached_scales) and \\\n            (seq_len + offset) <= self.cached_scales.shape[0]\n        ):\n            return self.cached_scales[offset:(offset + seq_len)]\n\n        scale = 1.\n        if self.use_xpos:\n            power = (t - len(t) // 2) / self.scale_base\n            scale = self.scale ** rearrange(power, 'n -> n 1')\n            scale = torch.cat((scale, scale), dim = -1)\n\n        if should_cache:\n            self.tmp_store('cached_scales', scale)\n\n        return scale\n\n    def get_axial_freqs(self, *dims):\n        Colon = slice(None)\n        all_freqs = []\n\n        for ind, dim in enumerate(dims):\n            if self.freqs_for == 'pixel':\n                pos = torch.linspace(-1, 1, steps = dim, device = self.device)\n            else:\n                pos = torch.arange(dim, device = self.device)\n\n            freqs = self.forward(pos, seq_len = dim)\n\n            all_axis = [None] * len(dims)\n            all_axis[ind] = Colon\n\n            new_axis_slice = (Ellipsis, *all_axis, Colon)\n            all_freqs.append(freqs[new_axis_slice])\n\n        all_freqs = broadcast_tensors(*all_freqs)\n        return torch.cat(all_freqs, dim = -1)\n\n    @autocast(enabled = False)\n    def forward(\n        self,\n        t: Tensor,\n        seq_len = None,\n        offset = 0\n    ):\n        should_cache = (\n            self.cache_if_possible and \\\n            not self.learned_freq and \\\n            exists(seq_len) and \\\n            self.freqs_for != 'pixel'\n        )\n\n        if (\n            should_cache and \\\n            exists(self.cached_freqs) and \\\n            (offset + seq_len) <= self.cached_freqs.shape[0]\n        ):\n            return self.cached_freqs[offset:(offset + seq_len)].detach()\n\n        freqs = self.freqs\n\n        freqs = einsum('..., f -> ... f', t.type(freqs.dtype), freqs)\n        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n\n        if should_cache:\n            self.tmp_store('cached_freqs', freqs.detach())\n\n        return freqs",
    "description": null,
    "url": null
}