{
    "acronym": "bigbird",
    "title": "Big Bird: Transformers for Longer Sequences",
    "seed_ids": [
        "reformer",
        "bptrans",
        "blockbert",
        "sparsetransformer",
        "transformer"
    ],
    "s2id": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
    "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
    "authors": [
        "M. Zaheer",
        "Guru Guruganesh",
        "Kumar Avinava Dubey",
        "J. Ainslie",
        "Chris Alberti",
        "Santiago Onta\u00f1\u00f3n",
        "Philip Pham",
        "Anirudh Ravula",
        "Qifan Wang",
        "Li Yang",
        "Amr Ahmed"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "tldr": "It is shown that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model.",
    "citationCount": 1631,
    "influentialCitationCount": 238,
    "code": "# Copyright 2021 The BigBird Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"BigBird Attention Layers.\"\"\"\n\nfrom absl import logging\nfrom bigbird.core import recompute_grad\nfrom bigbird.core import utils\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\nMAX_SEQ_LEN = 4096\n\n\ndef get_single_block_row_attention(block_id,\n                                   to_start_block_id,\n                                   to_end_block_id,\n                                   num_rand_blocks,\n                                   window_block_left=1,\n                                   window_block_right=1,\n                                   global_block_left=1,\n                                   global_block_right=1):\n  \"\"\"For a single row block get random row attention.\n\n  Args:\n    block_id: int. block id of row.\n    to_start_block_id: int. random attention coloum start id.\n    to_end_block_id: int. random attention coloum end id.\n    num_rand_blocks: int. number of random blocks to be selected.\n    window_block_left: int. number of blocks of window to left of a block.\n    window_block_right: int. number of blocks of window to right of a block.\n    global_block_left: int. Number of blocks globally used to the left.\n    global_block_right: int. Number of blocks globally used to the right.\n\n  Returns:\n    row containing the random attention vector of size num_rand_blocks.\n  \"\"\"\n\n  # list of to_blocks from which to choose random attention\n  to_block_list = np.arange(to_start_block_id, to_end_block_id,\n                            dtype=np.int32)\n  # permute the blocks\n  perm_block = np.random.permutation(to_block_list)\n  # print(perm_block)\n\n  # illegal blocks for the current block id, using window\n  illegal_blocks = list(\n      range(block_id - window_block_left, block_id + window_block_right + 1))\n\n  # Add blocks at the start and at the end\n  illegal_blocks.extend(list(range(global_block_left)))\n  illegal_blocks.extend(\n      list(range(to_end_block_id - global_block_right, to_end_block_id)))\n\n  # The second from_block cannot choose random attention on second last to_block\n  if block_id == 1:\n    illegal_blocks.append(to_end_block_id-2)\n\n  # The second last from_block cannot choose random attention on second to_block\n  if block_id == to_end_block_id - 2:\n    illegal_blocks.append(1)\n\n  selected_random_blokcs = []\n\n  for i in range(to_end_block_id - to_start_block_id):\n    if perm_block[i] not in illegal_blocks:\n      selected_random_blokcs.append(perm_block[i])\n    if len(selected_random_blokcs) == num_rand_blocks:\n      break\n  return np.array(selected_random_blokcs, dtype=np.int32)\n\n\ndef bigbird_block_rand_mask_with_head(seq_length,\n                                      block_size,\n                                      num_heads,\n                                      plan_from_length,\n                                      plan_num_rand_blocks,\n                                      window_block_left=1,\n                                      window_block_right=1,\n                                      global_block_top=1,\n                                      global_block_bottom=1,\n                                      global_block_left=1,\n                                      global_block_right=1):\n  \"\"\"Create adjacency list of random attention.\n\n  Args:\n    seq_length: int. length of sequence.\n    block_size: int. size of block in sequence.\n    num_heads: int. total number of heads.\n    plan_from_length: list. plan from lenght where num_rand are choosen from.\n    plan_num_rand_blocks: list. number of rand blocks within the plan.\n    window_block_left: int. number of blocks of window to left of a block.\n    window_block_right: int. number of blocks of window to right of a block.\n    global_block_top: int. number of blocks at the top.\n    global_block_bottom: int. number of blocks at the bottom.\n    global_block_left: int. Number of blocks globally used to the left.\n    global_block_right: int. Number of blocks globally used to the right.\n\n  Returns:\n    adjacency list of size num_head where each element is of size\n    from_seq_length//from_block_size-2 by num_rand_blocks\n  \"\"\"\n  # Total number of blocks in the mmask\n  num_blocks = seq_length//block_size\n  # Number of blocks per plan\n  plan_block_length = np.array(plan_from_length) // block_size\n  # till when to follow plan\n  max_plan_idx = plan_from_length.index(seq_length)\n  # Random Attention adjajency list\n  rand_attn = [np.zeros((num_blocks,\n                         np.sum(plan_num_rand_blocks[:max_plan_idx+1])),\n                        dtype=np.int32) for i in range(num_heads)]\n\n  # We will go iteratively over the plan blocks and pick random number of\n  # Attention blocks from the legally allowed blocks\n  for plan_idx in range(max_plan_idx+1):\n    rnd_r_cnt = 0\n    if plan_idx > 0:\n      # set the row for all from_blocks starting from 0 to\n      # plan_block_length[plan_idx-1]\n      # column indx start fromm plan_block_length[plan_idx-1] and ends at\n      # plan_block_length[plan_idx]\n      if plan_num_rand_blocks[plan_idx] > 0:\n        rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))\n        curr_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx+1]))\n        for blk_rw_idx in range(global_block_top,\n                                plan_block_length[plan_idx-1]):\n          for h in range(num_heads):\n            # print(\"head\", h, \"blk_rw_idx\", blk_rw_idx)\n            rand_attn[h][blk_rw_idx,\n                         rnd_r_cnt:curr_r_cnt] = get_single_block_row_attention(\n                             block_id=blk_rw_idx,\n                             to_start_block_id=plan_block_length[plan_idx - 1],\n                             to_end_block_id=plan_block_length[plan_idx],\n                             num_rand_blocks=plan_num_rand_blocks[plan_idx],\n                             window_block_left=window_block_left,\n                             window_block_right=window_block_right,\n                             global_block_left=global_block_left,\n                             global_block_right=global_block_right)\n\n      for pl_id in range(plan_idx):\n        if plan_num_rand_blocks[pl_id] == 0:\n          continue\n        for blk_rw_idx in range(plan_block_length[plan_idx-1],\n                                plan_block_length[plan_idx]):\n          rnd_r_cnt = 0\n          to_start_block_id = 0\n          if pl_id > 0:\n            rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:pl_id]))\n            to_start_block_id = plan_block_length[pl_id-1]\n          curr_r_cnt = int(np.sum(plan_num_rand_blocks[:pl_id+1]))\n          for h in range(num_heads):\n            # print(\"head\", h, \"blk_rw_idx\", blk_rw_idx)\n            rand_attn[h][blk_rw_idx,\n                         rnd_r_cnt:curr_r_cnt] = get_single_block_row_attention(\n                             block_id=blk_rw_idx,\n                             to_start_block_id=to_start_block_id,\n                             to_end_block_id=plan_block_length[pl_id],\n                             num_rand_blocks=plan_num_rand_blocks[pl_id],\n                             window_block_left=window_block_left,\n                             window_block_right=window_block_right,\n                             global_block_left=global_block_left,\n                             global_block_right=global_block_right)\n\n    if plan_num_rand_blocks[plan_idx] == 0:\n      continue\n    # print(\"Start from here\")\n    curr_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx+1]))\n    from_start_block_id = global_block_top\n    to_start_block_id = 0\n    if plan_idx > 0:\n      rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))\n      from_start_block_id = plan_block_length[plan_idx-1]\n      to_start_block_id = plan_block_length[plan_idx-1]\n\n    for blk_rw_idx in range(from_start_block_id, plan_block_length[plan_idx]):\n      for h in range(num_heads):\n        # print(\"head\", h, \"blk_rw_idx\", blk_rw_idx)\n        rand_attn[h][blk_rw_idx,\n                     rnd_r_cnt:curr_r_cnt] = get_single_block_row_attention(\n                         block_id=blk_rw_idx,\n                         to_start_block_id=to_start_block_id,\n                         to_end_block_id=plan_block_length[plan_idx],\n                         num_rand_blocks=plan_num_rand_blocks[plan_idx],\n                         window_block_left=window_block_left,\n                         window_block_right=window_block_right,\n                         global_block_left=global_block_left,\n                         global_block_right=global_block_right)\n\n  for nh in range(num_heads):\n    rand_attn[nh] = rand_attn[nh][global_block_top:num_blocks -\n                                  global_block_bottom, :]\n  return rand_attn\n\n\ndef get_rand_attn_plan(from_seq_length, from_block_size, num_rand_blocks):\n  \"\"\"Gives the plan of where to put random attention.\n\n  Args:\n    from_seq_length: int. length of from sequence.\n    from_block_size: int. size of block in from sequence.\n    num_rand_blocks: int. Number of random chunks per row.\n\n  Returns:\n    plan_from_length: ending location of from block\n    plan_num_rand_blocks: number of random ending location for each block\n  \"\"\"\n  # general plan\n  plan_from_length = []\n  plan_num_rand_blocks = []\n  if (2*num_rand_blocks + 5) < (from_seq_length // from_block_size):\n    plan_from_length.append(int((2*num_rand_blocks + 5)*from_block_size))\n    plan_num_rand_blocks.append(num_rand_blocks)\n    plan_from_length.append(from_seq_length)\n    plan_num_rand_blocks.append(0)\n  elif (num_rand_blocks + 5) < (from_seq_length // from_block_size):\n    plan_from_length.append(int((num_rand_blocks + 5)*from_block_size))\n    plan_num_rand_blocks.append(num_rand_blocks//2)\n    plan_from_length.append(from_seq_length)\n    plan_num_rand_blocks.append(num_rand_blocks - (num_rand_blocks//2))\n  else:\n    plan_from_length.append(from_seq_length)\n    plan_num_rand_blocks.append(num_rand_blocks)\n\n  return plan_from_length, plan_num_rand_blocks\n\n\ndef bigbird_block_rand_mask(from_seq_length,\n                            to_seq_length,\n                            from_block_size,\n                            to_block_size,\n                            num_rand_blocks,\n                            last_idx=-1):\n  \"\"\"Create adjacency list of random attention.\n\n  Args:\n    from_seq_length: int. length of from sequence.\n    to_seq_length: int. length of to sequence.\n    from_block_size: int. size of block in from sequence.\n    to_block_size: int. size of block in to sequence.\n    num_rand_blocks: int. Number of random chunks per row.\n    last_idx: if -1 then num_rand_blocks blocks chosen anywhere in to sequence,\n      if positive then num_rand_blocks blocks choosen only upto last_idx.\n\n  Returns:\n    adjacency list of size from_seq_length//from_block_size-2 by num_rand_blocks\n  \"\"\"\n  rand_attn = np.zeros(\n      (from_seq_length // from_block_size - 2, num_rand_blocks), dtype=np.int32)\n  middle_seq = np.arange(1, to_seq_length // to_block_size - 1, dtype=np.int32)\n  last = to_seq_length // to_block_size - 1\n  if last_idx > (2 * to_block_size):\n    last = (last_idx // to_block_size) - 1\n\n  r = num_rand_blocks  # shorthand\n  for i in range(1, from_seq_length // from_block_size-1):\n    start = i-2\n    end = i\n    if i == 1:\n      rand_attn[i-1, :] = np.random.permutation(middle_seq[2:last])[:r]\n    elif i == 2:\n      rand_attn[i-1, :] = np.random.permutation(middle_seq[3:last])[:r]\n    elif i == from_seq_length // from_block_size - 3:\n      rand_attn[i-1, :] = np.random.permutation(middle_seq[:last])[:r]\n      # Missing -3: should have been sliced till last-3\n    elif i == from_seq_length // from_block_size - 2:\n      rand_attn[i-1, :] = np.random.permutation(middle_seq[:last])[:r]\n      # Missing -4: should have been sliced till last-4\n    else:\n      if start > last:\n        start = last\n        rand_attn[i-1, :] = np.random.permutation(middle_seq[:start])[:r]\n      elif (end+1) == last:\n        rand_attn[i-1, :] = np.random.permutation(middle_seq[:start])[:r]\n      else:\n        rand_attn[i-1, :] = np.random.permutation(\n            np.concatenate((middle_seq[:start], middle_seq[end+1:last])))[:r]\n  return rand_attn\n\n\n\ndef create_rand_mask_from_inputs(from_blocked_mask,\n                                 to_blocked_mask,\n                                 rand_attn,\n                                 num_attention_heads,\n                                 num_rand_blocks,\n                                 from_seq_length,\n                                 from_block_size):\n  \"\"\"Create 4D attention mask from a 3D tensor mask.\n\n  Args:\n    from_blocked_mask: 2D Tensor of shape [batch_size,\n      from_seq_length//from_block_size, from_block_size].\n    to_blocked_mask: int32 Tensor of shape [batch_size,\n      to_seq_length//to_block_size, to_block_size].\n    rand_attn: [batch_size, num_attention_heads,\n      from_seq_length//from_block_size-2, num_rand_blocks]\n    num_attention_heads: int. Number of attention heads.\n    num_rand_blocks: int. Number of random chunks per row.\n    from_seq_length: int. length of from sequence.\n    from_block_size: int. size of block in from sequence.\n\n  Returns:\n    float Tensor of shape [batch_size, num_attention_heads,\n                           from_seq_length//from_block_size-2,\n                           from_block_size, num_rand_blocks*to_block_size].\n  \"\"\"\n  num_windows = from_seq_length // from_block_size - 2\n  rand_mask = tf.reshape(\n      tf.gather(to_blocked_mask, rand_attn, batch_dims=1), [\n          -1, num_attention_heads, num_windows,\n          num_rand_blocks * from_block_size\n      ])\n  rand_mask = tf.einsum(\"BLQ,BHLK->BHLQK\", from_blocked_mask[:, 1:-1],\n                        rand_mask)\n  return rand_mask\n\n\n\ndef bigbird_block_sparse_attention(query_layer,\n                                   key_layer,\n                                   value_layer,\n                                   band_mask,\n                                   from_mask,\n                                   to_mask,\n                                   from_blocked_mask,\n                                   to_blocked_mask,\n                                   rand_attn,\n                                   num_attention_heads,\n                                   size_per_head,\n                                   num_rand_blocks,\n                                   from_seq_length,\n                                   to_seq_length,\n                                   from_block_size,\n                                   to_block_size):\n  \"\"\"BigBird attention sparse calculation using blocks in linear time.\n\n  Assumes from_seq_length//from_block_size == to_seq_length//to_block_size.\n  A pure function with a long argument list to allow easy use outside our\n  framework.\n\n  Args:\n    query_layer: float Tensor of shape [batch_size, num_attention_heads,\n      from_seq_length, size_per_head]\n    key_layer: float Tensor of shape [batch_size, num_attention_heads,\n      to_seq_length, size_per_head]\n    value_layer: float Tensor of shape [batch_size, num_attention_heads,\n      to_seq_length, size_per_head]\n    band_mask: float32 Tensor of shape [batch_size, 1,\n      from_seq_length//from_block_size-4, from_block_size, 3*to_block_size].\n      The values should be 1 or 0. The attention scores will effectively be\n      set to -infinity for any positions in the mask that are 0, and will be\n      unchanged for positions that are 1.\n    from_mask: float32 Tensor of shape [batch_size, 1, from_seq_length, 1].\n      The values should be 1 or 0. The attention scores will effectively be set\n      to -infinity for any positions in the mask that are 0, and will be\n      unchanged for positions that are 1.\n    to_mask: float32 Tensor of shape [batch_size, 1, 1, to_seq_length].\n      The values should be 1 or 0. The attention scores will effectively be set\n      to -infinity for any positions in the mask that are 0, and will be\n      unchanged for positions that are 1.\n    from_blocked_mask: float32 Tensor of shape [batch_size,\n      from_seq_length//from_block_size, from_block_size].\n      Same as from_mask, just reshaped.\n    to_blocked_mask: float32 Tensor of shape [batch_size,\n      to_seq_length//to_block_size, to_block_size].\n      Same as to_mask, just reshaped.\n    rand_attn: int32 Tensor of shape [num_attention_heads,\n      from_seq_length//from_block_size-2, num_rand_blocks] specifying which\n      blocks to attend to for each from sequence block (except 2 global ones).\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    num_rand_blocks: int. Number of random chunks per row.\n    from_seq_length: int. length of from sequence.\n    to_seq_length: int. length of to sequence.\n    from_block_size: int. size of block in from sequence.\n    to_block_size: int. size of block in to sequence.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, num_attention_heads,\n      size_per_head].\n  \"\"\"\n  assert from_seq_length//from_block_size == to_seq_length//to_block_size\n\n  # repeat for batch size\n  batch_size = utils.get_shape_list(query_layer)[0]\n  rand_attn = tf.expand_dims(rand_attn, 0)\n  rand_attn = tf.repeat(rand_attn, batch_size, 0)\n\n  rand_mask = create_rand_mask_from_inputs(\n      from_blocked_mask, to_blocked_mask, rand_attn,\n      num_attention_heads, num_rand_blocks,\n      from_seq_length, from_block_size)\n\n  # Define shorthands\n  # b = batch_size\n  h = num_attention_heads\n  r = num_rand_blocks\n  d = size_per_head\n  m = from_seq_length\n  n = to_seq_length\n  wm = from_block_size\n  wn = to_block_size\n\n  blocked_query_matrix = tf.reshape(query_layer, (-1, h, m // wm, wm, d))\n  blocked_key_matrix = tf.reshape(key_layer, (-1, h, n // wn, wn, d))\n  blocked_value_matrix = tf.reshape(value_layer, (-1, h, n // wn, wn, d))\n  gathered_key = tf.reshape(\n      tf.gather(blocked_key_matrix, rand_attn, batch_dims=2, name=\"gather_key\"),\n      (-1, h, m // wm - 2, r * wn, d))  # [b, h, n//wn-2, r, wn, -1]\n  gathered_value = tf.reshape(\n      tf.gather(\n          blocked_value_matrix, rand_attn, batch_dims=2, name=\"gather_value\"),\n      (-1, h, m // wm - 2, r * wn, d))  # [b, h, n//wn-2, r, wn, -1]\n\n  first_product = tf.einsum(\n      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, 0],\n      key_layer)  # [b, h, wm, -1] x [b, h, n, -1] ==> [b, h, wm, n]\n  first_product = tf.multiply(first_product, 1.0 / np.sqrt(d))\n  first_product += (1.0 - to_mask) * -10000.0\n  first_attn_weights = tf.nn.softmax(first_product)  # [b, h, wm, n]\n  first_context_layer = tf.einsum(\n      \"BHQK,BHKD->BHQD\", first_attn_weights,\n      value_layer)  # [b, h, wm, n] x [b, h, n, -1] ==> [b, h, wm, -1]\n  first_context_layer = tf.expand_dims(first_context_layer, 2)\n\n  second_key_mat = tf.concat([\n      blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, 1],\n      blocked_key_matrix[:, :, 2], blocked_key_matrix[:, :, -1],\n      gathered_key[:, :, 0]], 2)  # [b, h, (4+r)*wn, -1]\n  second_value_mat = tf.concat([\n      blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, 1],\n      blocked_value_matrix[:, :, 2], blocked_value_matrix[:, :, -1],\n      gathered_value[:, :, 0]], 2)  # [b, h, (4+r)*wn, -1]\n  second_product = tf.einsum(\n      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, 1], second_key_mat\n  )  # [b, h, wm, -1] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, (4+r)*wn]\n  second_seq_pad = tf.concat([\n      to_mask[:, :, :, :3 * wn], to_mask[:, :, :, -wn:],\n      tf.ones_like(rand_mask[:, :1, 0, :1])], 3)\n  second_rand_pad = tf.concat(\n      [tf.ones_like(second_product[:, :, :, :4 * wn]), rand_mask[:, :, 0]], 3)\n  second_product = tf.multiply(second_product, 1.0 / np.sqrt(d))\n  second_product += (1.0 -\n                     tf.minimum(second_seq_pad, second_rand_pad)) * -10000.0\n  second_attn_weights = tf.nn.softmax(second_product)  # [b , h, wm, (4+r)*wn]\n  second_context_layer = tf.einsum(\n      \"BHQK,BHKD->BHQD\", second_attn_weights, second_value_mat\n  )  # [b, h, wm, (4+r)*wn] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, -1]\n  second_context_layer = tf.expand_dims(second_context_layer, 2)\n\n  exp_blocked_key_matrix = tf.concat([\n      blocked_key_matrix[:, :, 1:-3], blocked_key_matrix[:, :, 2:-2],\n      blocked_key_matrix[:, :, 3:-1]], 3)  # [b, h, m//wm-4, 3*wn, -1]\n  exp_blocked_value_matrix = tf.concat([\n      blocked_value_matrix[:, :, 1:-3], blocked_value_matrix[:, :, 2:-2],\n      blocked_value_matrix[:, :, 3:-1]], 3)  # [b, h, m//wm-4, 3*wn, -1]\n  middle_query_matrix = blocked_query_matrix[:, :, 2:-2]\n  inner_band_product = tf.einsum(\n      \"BHLQD,BHLKD->BHLQK\", middle_query_matrix, exp_blocked_key_matrix\n  )  # [b, h, m//wm-4, wm, -1] x [b, h, m//wm-4, 3*wn, -1]\n  #     ==> [b, h, m//wm-4, wm, 3*wn]\n  inner_band_product = tf.multiply(inner_band_product, 1.0 / np.sqrt(d))\n  rand_band_product = tf.einsum(\n      \"BHLQD,BHLKD->BHLQK\", middle_query_matrix, gathered_key[:, :, 1:-1]\n  )  # [b, h, m//wm-4, wm, -1] x [b, h, m//wm-4, r*wn, -1]\n  #     ==> [b, h, m//wm-4, wm, r*wn]\n  rand_band_product = tf.multiply(rand_band_product, 1.0 / np.sqrt(d))\n  first_band_product = tf.einsum(\n      \"BHLQD,BHKD->BHLQK\", middle_query_matrix, blocked_key_matrix[:, :, 0]\n  )  # [b, h, m//wm-4, wm, -1] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, wn]\n  first_band_product = tf.multiply(first_band_product, 1.0 / np.sqrt(d))\n  last_band_product = tf.einsum(\n      \"BHLQD,BHKD->BHLQK\", middle_query_matrix, blocked_key_matrix[:, :, -1]\n  )  # [b, h, m//wm-4, wm, -1] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, wn]\n  last_band_product = tf.multiply(last_band_product, 1.0 / np.sqrt(d))\n  inner_band_product += (1.0 - band_mask) * -10000.0\n  first_band_product += (\n      1.0 - tf.expand_dims(to_mask[:, :, :, :wn], 3)) * -10000.0\n  last_band_product += (\n      1.0 - tf.expand_dims(to_mask[:, :, :, -wn:], 3)) * -10000.0\n  rand_band_product += (1.0 - rand_mask[:, :, 1:-1]) * -10000.0\n  band_product = tf.concat([\n      first_band_product, inner_band_product, rand_band_product,\n      last_band_product], -1)  # [b, h, m//wm-4, wm, (5+r)*wn]\n  attn_weights = tf.nn.softmax(band_product)  # [b, h, m//wm-4, wm, (5+r)*wn]\n  context_layer = tf.einsum(\n      \"BHLQK,BHLKD->BHLQD\", attn_weights[:, :, :, :, wn:4 * wn],\n      exp_blocked_value_matrix\n  )  # [b, h, m//wm-4, wm, 3*wn] x [b, h, m//wm-4, 3*wn, -1]\n  #     ==> [b, h, m//wm-4, wm, -1]\n  context_layer += tf.einsum(\n      \"BHLQK,BHLKD->BHLQD\", attn_weights[:, :, :, :, 4 * wn:-wn],\n      gathered_value[:, :, 1:-1]\n  )  # [b, h, m//wm-4, wm, r*wn] x [b, h, m//wm-4, r*wn, -1]\n  #     ==> [b, h, m//wm-4, wm, -1]\n  context_layer += tf.einsum(\n      \"BHLQK,BHKD->BHLQD\", attn_weights[:, :, :, :, :wn],\n      blocked_value_matrix[:, :, 0]\n  )  # [b, h, m//wm-4, wm, wn] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, -1]\n  context_layer += tf.einsum(\n      \"BHLQK,BHKD->BHLQD\", attn_weights[:, :, :, :, -wn:],\n      blocked_value_matrix[:, :, -1]\n  )  # [b, h, m//wm-4, wm, wn] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, -1]\n\n  second_last_key_mat = tf.concat([\n      blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, -3],\n      blocked_key_matrix[:, :, -2], blocked_key_matrix[:, :, -1],\n      gathered_key[:, :, -1]], 2)  # [b, h, (4+r)*wn, -1]\n  second_last_value_mat = tf.concat([\n      blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, -3],\n      blocked_value_matrix[:, :, -2], blocked_value_matrix[:, :, -1],\n      gathered_value[:, :, -1]], 2)  # [b, h, (4+r)*wn, -1]\n  second_last_product = tf.einsum(\n      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, -2], second_last_key_mat\n  )  # [b, h, wm, -1] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, (4+r)*wn]\n  second_last_seq_pad = tf.concat([\n      to_mask[:, :, :, :wn], to_mask[:, :, :, -3 * wn:],\n      tf.ones_like(rand_mask[:, :1, 0, :1])], 3)\n  second_last_rand_pad = tf.concat(\n      [tf.ones_like(second_last_product[:, :, :, :4 * wn]),\n       rand_mask[:, :, -1]], 3)\n  second_last_product = tf.multiply(second_last_product, 1.0 / np.sqrt(d))\n  second_last_product += (\n      1.0 - tf.minimum(second_last_seq_pad, second_last_rand_pad)) * -10000.0\n  second_last_attn_weights = tf.nn.softmax(\n      second_last_product)  # [b, h, wm, (4+r)*wn]\n  second_last_context_layer = tf.einsum(\n      \"BHQK,BHKD->BHQD\", second_last_attn_weights, second_last_value_mat\n  )  # [b, h, wm, (4+r)*wn] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, -1]\n  second_last_context_layer = tf.expand_dims(second_last_context_layer, 2)\n\n  last_product = tf.einsum(\n      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, -1],\n      key_layer)  # [b, h, wm, -1] x [b, h, n, -1] ==> [b, h, wm, n]\n  last_product = tf.multiply(last_product, 1.0 / np.sqrt(d))\n  last_product += (1.0 - to_mask) * -10000.0\n  last_attn_weights = tf.nn.softmax(last_product)  # [b, h, wm, n]\n  last_context_layer = tf.einsum(\n      \"BHQK,BHKD->BHQD\", last_attn_weights,\n      value_layer)  # [b, h, wm, n] x [b, h, n, -1] ==> [b, h, wm, -1]\n  last_context_layer = tf.expand_dims(last_context_layer, 2)\n\n  context_layer = tf.concat([\n      first_context_layer, second_context_layer, context_layer,\n      second_last_context_layer, last_context_layer\n  ], 2)\n  context_layer = tf.reshape(context_layer, (-1, h, m, d)) * from_mask\n  context_layer = tf.transpose(context_layer, (0, 2, 1, 3))\n  return context_layer\n\n\nclass MultiHeadedAttentionLayer(tf.keras.layers.Layer):\n  \"\"\"A multi-headed attention layer.\n\n  It implements following types of multi-headed attention:\n  - original_full attention from \"Attention is all you Need\".\n  - simulated_sparse attention from BigBird with full quadratic implemention.\n  - block_sparse attention from BigBird with memory efficient linear impl.\n  \"\"\"\n\n  def __init__(self,\n               attention_type,\n               num_attention_heads=1,\n               size_per_head=512,\n               num_rand_blocks=3,\n               from_seq_length=1024,\n               to_seq_length=1024,\n               from_block_size=64,\n               to_block_size=64,\n               attention_probs_dropout_prob=0.0,\n               initializer_range=0.02,\n               use_bias=True,\n               seed=None,\n               query_act=None,\n               key_act=None,\n               value_act=None,\n               name=None):\n    \"\"\"Constructor for a multi-headed attention layer.\n\n    Args:\n      attention_type: Type of attention, needs to be one of ['original_full',\n        'simulated_sparse', 'block_sparse'].\n      num_attention_heads: (optional) int. Number of attention heads.\n      size_per_head: (optional) int. Size of each attention head.\n      num_rand_blocks: (optional) int. Number of random chunks per row.\n      from_seq_length: int. (optional) length of from sequence.\n      to_seq_length: int. (optional) length of to sequence.\n      from_block_size: (optional) int. size of block in from sequence.\n      to_block_size: (optional) int. size of block in to sequence.\n      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n        attention probabilities.\n      initializer_range: (optional) float. Range of the weight initializer.\n      use_bias: Whether the layer uses a bias vector.\n      seed: (Optional) int. Reandom seed for generating random mask.\n      query_act: (optional) Activation function for the query transform.\n      key_act: (optional) Activation function for the key transform.\n      value_act: (optional) Activation function for the value transform.\n      name: The name scope of this layer.\n    \"\"\"\n    super(MultiHeadedAttentionLayer, self).__init__(name=name)\n    self.num_attention_heads = num_attention_heads\n    self.size_per_head = size_per_head\n    self.num_rand_blocks = num_rand_blocks\n    self.from_seq_length = from_seq_length\n    self.to_seq_length = to_seq_length\n    self.from_block_size = from_block_size\n    self.to_block_size = to_block_size\n    self.seed = seed\n\n    with tf.compat.v1.variable_scope(name):\n      self.query_layer = utils.Dense3dLayer(\n          num_attention_heads, size_per_head,\n          utils.create_initializer(initializer_range), query_act,\n          \"query\", head_first=True, use_bias=use_bias)\n\n      self.key_layer = utils.Dense3dLayer(\n          num_attention_heads, size_per_head,\n          utils.create_initializer(initializer_range), key_act,\n          \"key\", head_first=True, use_bias=use_bias)\n\n      self.value_layer = utils.Dense3dLayer(\n          num_attention_heads, size_per_head,\n          utils.create_initializer(initializer_range), value_act,\n          \"value\", head_first=True, use_bias=use_bias)\n\n    logging.info(\"**** Using block sparse attention ****\")\n    assert from_seq_length//from_block_size == to_seq_length//to_block_size, (\n        \"Error the number of blocks needs to be same!\")\n    self.attention_dropout = None\n    self.rand_attn = self.generate_rand_attn_list()\n    self.attn_impl = self.bigbird_block_sparse_attention\n\n  def generate_rand_attn_list(self):\n    # generate random attention and corresponding masks\n    if self.seed is not None:\n      np.random.seed(self.seed)\n    # old plans used in paper\n    if self.from_seq_length in [1024, 2048, 3072, 4096]:\n      rand_attn = [\n          bigbird_block_rand_mask(  # pylint: disable=g-complex-comprehension\n              MAX_SEQ_LEN, MAX_SEQ_LEN,\n              self.from_block_size, self.to_block_size, self.num_rand_blocks,\n              last_idx=1024\n          )[:(self.from_seq_length // self.from_block_size - 2)]\n          for _ in range(self.num_attention_heads)\n      ]\n    else:\n      plan_from_length, plan_num_rand_blocks = get_rand_attn_plan(\n          self.from_seq_length, self.from_block_size, self.num_rand_blocks)\n      rand_attn = bigbird_block_rand_mask_with_head(\n          seq_length=self.from_seq_length,\n          block_size=self.from_block_size,\n          num_heads=self.num_attention_heads,\n          plan_from_length=plan_from_length,\n          plan_num_rand_blocks=plan_num_rand_blocks)\n    rand_attn = np.stack(rand_attn, axis=0)\n    return tf.constant(rand_attn, dtype=tf.int32)\n\n  def bigbird_block_sparse_attention(self,\n                                     query_layer,\n                                     key_layer,\n                                     value_layer,\n                                     masks,\n                                     training=None):\n\n    (_, band_mask, from_mask, to_mask,\n     from_blocked_mask, to_blocked_mask) = masks\n\n    return bigbird_block_sparse_attention(\n        query_layer, key_layer, value_layer,\n        band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask,\n        self.rand_attn, self.num_attention_heads, self.size_per_head,\n        self.num_rand_blocks, self.from_seq_length, self.to_seq_length,\n        self.from_block_size, self.to_block_size)\n\n  def call(self,\n           from_tensor,\n           to_tensor,\n           masks,\n           cache=None,\n           decode_i=None,\n           training=None):\n    # Scalar dimensions referenced here:\n    #   b = batch size (number of sequences)\n    #   m = `from_tensor` sequence length\n    #   n = `to_tensor` sequence length\n    #   h = `num_attention_heads`\n    #   d = `size_per_head`\n\n    # `query` = [b, h, m, d]\n    query = self.query_layer(from_tensor)\n\n    # `key` = [b, h, n, d]\n    key = self.key_layer(to_tensor)\n\n    # `value_layer` = [b, h, n, d]\n    value = self.value_layer(to_tensor)\n\n    if cache is not None and decode_i is not None:\n      max_len = utils.get_shape_list(cache[\"k\"])[2]\n      indices_select = tf.reshape(\n          tf.one_hot(decode_i, max_len, dtype=to_tensor.dtype),\n          [1, 1, max_len, 1])\n      key = cache[\"k\"] + key * indices_select\n      value = cache[\"v\"] + value * indices_select\n      cache[\"k\"] = key\n      cache[\"v\"] = value\n\n    contextual_output = self.attn_impl(\n        query, key, value, masks, training=training)\n\n    return contextual_output",
    "description": null,
    "url": null
}