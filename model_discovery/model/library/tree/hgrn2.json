{
    "acronym": "hgrn2",
    "title": "HGRN2: Gated Linear RNNs with State Expansion",
    "seed_ids": [
        "rwkv6",
        "griffin",
        "based",
        "mamba",
        "hgrn",
        "tnn",
        "resurrectrnn",
        "transnormer",
        "flash",
        "s4",
        "rfa",
        "deltanet",
        "performer",
        "lineartransformer"
    ],
    "s2id": "46732358e98ce6be0c564ae11f71d556a64b4c35",
    "abstract": "Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.",
    "authors": [
        "Zhen Qin",
        "Songlin Yang",
        "Weixuan Sun",
        "Xuyang Shen",
        "Dong Li",
        "Weigao Sun",
        "Yiran Zhong"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.",
    "citationCount": 11,
    "influentialCitationCount": 2,
    "code": "# -*- coding: utf-8 -*-\n\n# \"HGRN2: Gated Linear RNNs with State Expansion\"[https://arxiv.org/abs/2404.07904]\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\nfrom fla.modules import RMSNorm, ShortConvolution\nfrom fla.modules.activations import swish\nfrom fla.ops.gla import chunk_gla, fused_chunk_gla, fused_recurrent_gla\n\nif TYPE_CHECKING:\n    from fla.models.utils import Cache\n\n\nclass HGRN2Attention(nn.Module):\n\n    def __init__(\n        self,\n        mode: str = 'chunk',\n        hidden_size: int = 1024,\n        num_heads: Optional[int] = None,\n        expand_ratio: Optional[int] = 128,\n        use_short_conv: bool = False,\n        conv_size: int = 4,\n        conv_bias: bool = False,\n        elementwise_affine: Optional[bool] = True,\n        norm_eps: float = 1e-5,\n        layer_idx: int = None\n    ) -> HGRN2Attention:\n        super().__init__()\n\n        self.mode = mode\n        self.hidden_size = hidden_size\n\n        if expand_ratio is None and num_heads is not None:\n            expand_ratio = hidden_size // num_heads\n        elif expand_ratio is not None and num_heads is None:\n            num_heads = hidden_size // expand_ratio\n        elif expand_ratio is None and num_heads is None:\n            raise RuntimeError(\"One of `expand_ratio` or `num_heads` should be provided.\")\n        self.num_heads = num_heads\n        self.expand_ratio = expand_ratio\n\n        self.use_short_conv = use_short_conv\n        self.conv_size = conv_size\n        self.conv_bias = conv_bias\n\n        self.forget_dim = int(self.num_heads * self.expand_ratio)\n        self.input_dim = hidden_size\n        self.layer_idx = layer_idx\n\n        assert mode in ['chunk', 'fused_recurrent', 'fused_chunk'], f\"Not suppoerted mode `{mode}`.\"\n        assert self.forget_dim % num_heads == 0, f\"forget dim must be divisible by num_heads of {num_heads}\"\n        assert self.input_dim % num_heads == 0, f\"input dim must be divisible by num_heads of {num_heads}\"\n\n        self.head_f_dim = self.expand_ratio\n        self.head_i_dim = self.hidden_size // num_heads\n\n        self.q_proj = nn.Linear(hidden_size, self.forget_dim, bias=False)\n        self.f_proj = nn.Linear(hidden_size, self.forget_dim, bias=False)\n        self.i_proj = nn.Linear(hidden_size, self.input_dim, bias=False)\n\n        if use_short_conv:\n            self.conv_size = conv_size\n            self.q_conv1d = ShortConvolution(self.forget_dim, conv_size)\n            self.f_conv1d = ShortConvolution(self.forget_dim, conv_size)\n            self.i_conv1d = ShortConvolution(self.input_dim, conv_size)\n\n        self.g_norm = RMSNorm(self.hidden_size, elementwise_affine, norm_eps)\n        self.o_proj = nn.Linear(self.input_dim, hidden_size, bias=False)\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[Cache] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        lower_bound: Optional[torch.Tensor] = None,\n        **kwargs\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n        # launching the triton kernel for just one token will actually be slower\n        mode = 'fused_recurrent' if hidden_states.shape[1] == 1 else self.mode\n\n        last_state = past_key_values[self.layer_idx] if use_cache else None\n        if self.use_short_conv:\n            conv_state_q = last_state[0] if use_cache else None\n            conv_state_f = last_state[1] if use_cache else None\n            conv_state_i = last_state[2] if use_cache else None\n            q = self.q_proj(hidden_states)\n            f = self.f_proj(hidden_states)\n            i = self.i_proj(hidden_states)\n            q = self.q_conv1d(q, attention_mask, conv_state_q)\n            f = self.f_conv1d(f, attention_mask, conv_state_f)\n            i = self.i_conv1d(i, attention_mask, conv_state_i)\n        else:\n            q = self.q_proj(hidden_states)\n            f = self.f_proj(hidden_states)\n            i = self.i_proj(hidden_states)\n\n        # dealing with left-padding\n        if attention_mask is not None:\n            i = i.mul_(attention_mask.unsqueeze(-1))\n\n        q = swish(q)\n\n        # improve precision\n        f = f.float()\n\n        # the lower bound for the first layer is zero\n        if lower_bound is None or self.layer_idx == 0:\n            k, g = 1 - f.sigmoid(), F.logsigmoid(f)\n        else:\n            g = lower_bound + (1 - lower_bound) * f.sigmoid()\n            k, g = 1 - g, g.log()\n        \n        q, k, i, g = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=self.num_heads), (q, k.to(i), i, g))\n\n        recurrent_state = last_state[-1] if use_cache else None\n        if mode == 'fused_recurrent':\n            o, recurrent_state = fused_recurrent_gla(q, k, i, g, initial_state=recurrent_state, output_final_state=use_cache)\n        elif mode == 'fused_chunk':\n            o, recurrent_state = fused_chunk_gla(q, k, i, g, initial_state=recurrent_state, output_final_state=use_cache)\n        elif mode == 'chunk':\n            o, recurrent_state = chunk_gla(q, k, i, g, initial_state=recurrent_state, output_final_state=use_cache)\n        else:\n            raise NotImplementedError(f\"Not supported mode `{mode}`.\")\n\n        if past_key_values is not None:\n            if self.use_short_conv:\n                last_state = (conv_state_q, conv_state_f, conv_state_i, recurrent_state)\n            else:\n                last_state = (recurrent_state,)\n            past_key_values.update(last_state, self.layer_idx, q.shape[2])\n\n        o = self.g_norm(rearrange(o, 'b h l d -> b l (h d)'))\n        o = self.o_proj(o)\n\n        return o, None, past_key_values\n\n    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n        param = next(self.parameters())\n        state = tuple()\n        if self.use_short_conv:\n            state += (param.new_zeros(batch_size, self.forget_dim, self.conv_size),\n                        param.new_zeros(batch_size, self.forget_dim, self.conv_size),\n                        param.new_zeros(batch_size, self.input_dim, self.conv_size))\n        state += (param.new_zeros(batch_size, self.num_heads, self.head_f_dim, self.head_i_dim),)\n        return state\n\n    def state_size(self, **kwargs) -> int:\n        state_size = self.forget_dim * self.head_i_dim\n        for module in self.children():\n            if isinstance(module, ShortConvolution):\n                state_size += module.state_size\n        return state_size",
    "description": null,
    "url": null
}