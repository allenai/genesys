{
    "acronym": "abc",
    "title": "Attention with Bounded-memory Control",
    "seed_ids": [
        "luna",
        "rfa",
        "deltanet",
        "performer",
        "bigbird",
        "lineartransformer",
        "linformer",
        "etc",
        "longformer",
        "compressivetransformer",
        "transformer"
    ],
    "s2id": "e0cbbca02b332f398c6639b3bea0613f79166220",
    "abstract": "Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.",
    "authors": [
        "Hao Peng",
        "Jungo Kasai",
        "Nikolaos Pappas",
        "Dani Yogatama",
        "Zhaofeng Wu",
        "Lingpeng Kong",
        "Roy Schwartz",
        "Noah A. Smith"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": "This work shows that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and it outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.",
    "citationCount": 21,
    "influentialCitationCount": 2,
    "code": "# -*- coding: utf-8 -*-\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\n\nfrom fla.modules import (FusedRMSNormSwishGate, RMSNorm, RotaryEmbedding,\n                         ShortConvolution)\nfrom fla.modules.activations import swiglu, swish\nfrom fla.modules.convolution import proj_then_conv1d\nfrom fla.ops.abc.chunk import chunk_abc\n\nif TYPE_CHECKING:\n    from fla.models.utils import Cache\n\n\nclass ABCAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int = 1024,\n        expand_k: float = 0.5,\n        expand_v: float = 1.0,\n        num_heads: int = 4,\n        use_short_conv: bool = False,\n        conv_size: int = 4,\n        conv_bias: bool = False,\n        num_slots: Optional[int] = None,\n        elementwise_affine: Optional[bool] = True,\n        norm_eps: float = 1e-5,\n        gate_low_rank_dim: int = 16,\n        gate_logit_normalizer: int = 16,\n        use_input_gate: bool = False,\n        use_output_gate: bool = True,\n        use_norm: bool = True,\n        clamp_min: Optional[float] = -32,\n        clamp_max: Optional[float] = 32,\n        layer_idx: Optional[int] = None,\n        **kwargs\n    ) -> ABCAttention:\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.expand_k = expand_k\n        self.expand_v = expand_v\n        self.num_heads = num_heads\n        self.key_dim = int(self.hidden_size * self.expand_k)\n        self.value_dim = int(self.hidden_size * self.expand_v)\n        self.head_k_dim = self.key_dim // self.num_heads\n        self.head_v_dim = self.value_dim // self.num_heads\n\n        self.use_short_conv = use_short_conv\n        self.conv_size = conv_size\n        self.conv_bias = conv_bias\n\n        self.gate_low_rank_dim = gate_low_rank_dim\n        self.gate_logit_normalizer = gate_logit_normalizer\n\n        self.use_input_gate = use_input_gate\n        self.use_output_gate = use_output_gate\n        self.use_norm = use_norm\n\n        if num_slots is None:\n            num_slots = self.head_k_dim\n        self.num_slots = num_slots\n\n        self.norm_eps = norm_eps\n\n        self.clamp_min = clamp_min\n        self.clamp_max = clamp_max\n        self.layer_idx = layer_idx\n\n        if layer_idx is None:\n            warnings.warn(\n                f\"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will \"\n                \"to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.key_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.key_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.value_dim, bias=False)\n\n        if use_output_gate:\n            self.g_proj = nn.Linear(self.hidden_size, self.value_dim, bias=False)\n        self.s_proj = nn.Linear(self.hidden_size, self.num_heads * self.num_slots, bias=False)\n        self.o_proj = nn.Linear(self.value_dim, self.hidden_size, bias=False)\n\n        if use_short_conv:\n            self.conv_size = conv_size\n            self.q_conv1d = ShortConvolution(self.key_dim, conv_size, activation='silu')\n            self.k_conv1d = ShortConvolution(self.key_dim, conv_size, activation='silu')\n            self.v_conv1d = ShortConvolution(self.value_dim, conv_size, activation='silu')\n\n        if self.use_norm:\n            if self.use_output_gate:\n                self.g_norm = FusedRMSNormSwishGate(self.head_v_dim, elementwise_affine, norm_eps)\n            else:\n                self.g_norm = RMSNorm(hidden_size=self.head_v_dim, elementwise_affine=elementwise_affine, eps=norm_eps)\n\n        if self.use_rope:\n            self.rotary = RotaryEmbedding(self.head_k_dim)\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[Cache] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        **kwargs\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n\n        if self.use_short_conv:\n            q = proj_then_conv1d(hidden_states, self.q_proj.weight, self.q_conv1d.weight, self.q_conv1d.bias)\n            k = proj_then_conv1d(hidden_states, self.k_proj.weight, self.k_conv1d.weight, self.k_conv1d.bias)\n            v = proj_then_conv1d(hidden_states, self.v_proj.weight, self.v_conv1d.weight, self.v_conv1d.bias)\n        else:\n            q = self.q_proj(hidden_states)\n            k = self.k_proj(hidden_states)\n            v = self.v_proj(hidden_states)\n\n        if self.use_input_gate:\n            q, k, v = map(lambda x: swish(x), (q, k, v))\n\n        if self.use_rope:\n            q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n            k = rearrange(k, '... (h d) -> ... h d', h=self.num_heads)\n            seqlen_offset = 0\n            if past_key_values is not None:\n                seqlen_offset = past_key_values.get_seq_length(self.layer_idx)\n            q, k = self.rotary(q, k, seqlen_offset)\n            q = rearrange(q, 'b n h d -> b h n d', h=self.num_heads)\n            k = rearrange(k, 'b n h d -> b h n d', h=self.num_heads)\n        else:\n            q = rearrange(q, 'b n (h d) -> b h n d', h=self.num_heads)\n            k = rearrange(k, 'b n (h d) -> b h n d', h=self.num_heads)\n        v = rearrange(v, 'b n (h d) -> b h n d', h=self.num_heads)\n\n        # [batch_size, n_heads, seq_len, num_slots]\n        s = rearrange(self.s_proj(hidden_states), 'b t (h m) -> b h t m', h=self.num_heads)\n        s = s.clamp_(self.clamp_min, self.clamp_max)\n\n        last_state = past_key_values[self.layer_idx] if use_cache else None\n        o, last_state = chunk_abc(q, k, v, s, initial_state=last_state, output_final_state=use_cache)\n        if past_key_values is not None and last_state is not None:\n            past_key_values.update(last_state, self.layer_idx, q.shape[2])\n\n        o = rearrange(o, 'b h t d -> b t h d')\n        if self.use_norm and not self.use_output_gate:\n            o = self.g_norm(o)\n        elif self.use_output_gate:\n            g = rearrange(self.g_proj(hidden_states), 'b t (h d) -> b t h d', h=self.num_heads)\n            o = self.g_norm(o, g) if self.use_norm else swiglu(g, o)\n        o = rearrange(o, 'b t h d -> b t (h d)')\n        o = self.o_proj(o)\n\n        return o, None, past_key_values\n\n    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n        param = next(self.parameters())\n        state = tuple()\n        if self.use_short_conv:\n            state += (param.new_zeros(batch_size, self.hidden_size, self.conv_size),)\n        state += (param.new_zeros(batch_size, self.num_heads, self.head_k_dim, self.num_slots),\n                  param.new_zeros(batch_size, self.num_heads, self.num_slots, self.head_v_dim))\n        return state\n\n    def state_size(self, sequence_length: int = 2048):\n        return self.num_heads * self.key_dim * self.head_v_dim",
    "description": null,
    "url": null
}