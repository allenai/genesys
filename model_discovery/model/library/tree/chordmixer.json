{
    "acronym": "chordmixer",
    "title": "ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths",
    "seed_ids": [
        "metaformer",
        "s4",
        "luna",
        "nystromformer",
        "linformer",
        "reformer",
        "transformer"
    ],
    "s2id": "2fc384087f55111a196c96d88029aab324aa543e",
    "abstract": "Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.",
    "authors": [
        "Ruslan Khalitov",
        "Tong Yu",
        "Lei Cheng",
        "Zhirong Yang"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": "A simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths, and substantially outperforms other neural attention models.",
    "citationCount": 11,
    "influentialCitationCount": 0,
    "code": "\"\"\"\nThe main difference between `RotateChord` and `RotateChordVarLen` is how they handle sequences in a batch:\n\n1. `RotateChord` operates on fixed-length sequences in a batch.\n    It rolls all sequences in the batch jointly along the time dimension.\n    This means that the same roll operation (with the same offsets) is applied to all sequences in the batch.\n\n2. `RotateChordVarLen` operates on variable-length sequences in a batch.\n    It rolls each sequence separately along the time dimension, considering its specific length.\n    This is useful when you have a batch containing sequences of extremely different lengths,\n    and you want to apply the roll operation independently to each sequence according to its length.\n\nIn summary, `RotateChord` is designed for batches with fixed-length sequences, while `RotateChordVarLen` is designed for batches with variable-length sequences.\n\"\"\"\n\nimport math\nimport torch\nimport numpy as np\nfrom torch import nn\n\nclass Mlp(nn.Module):\n    \"\"\"\n    A simple Multi-Layer Perceptron (MLP) module in PyTorch.\n\n    Args:\n    in_features (int): Number of input features.\n    hidden_features (int, optional): Hidden layer size. Defaults to in_features.\n    out_features (int, optional): Output size. Defaults to in_features.\n    act_layer (nn.Module, optional): Activation function. Defaults to nn.GELU.\n    drop (float, optional): Dropout probability. Defaults to 0.\n    \"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n   \n    \nclass BatchNorm(nn.Module):\n    \"\"\"\n    A PyTorch module implementing 1D Batch Normalization for token embeddings.\n\n    Args:\n        embedding_size (int): The size of the token embeddings.\n    \"\"\"\n    def __init__(self, embedding_size):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(embedding_size)\n\n    def forward(self, x):\n        x = torch.permute(x, (0, 2, 1))\n        x = self.bn(x)\n        x = torch.permute(x, (0, 2, 1))\n        return x\n\n\nclass GroupNorm(nn.Module):\n    \"\"\"\n    A PyTorch module implementing Group Normalization for token embeddings.\n\n    Args:\n        embedding_size (int): The size of the token embeddings.\n        n_groups (int): The number of groups to divide the channels into.\n    \"\"\"\n    def __init__(self, embedding_size, n_groups):\n        super().__init__()\n        self.gn = nn.GroupNorm(n_groups, embedding_size)\n\n    def forward(self, x):\n        x = torch.permute(x, (0, 2, 1))\n        x = self.gn(x)\n        x = torch.permute(x, (0, 2, 1))\n        return x\n\n\ndef map_norm(norm_type, embedding_size, track_size=None):\n    \"\"\"\n    Maps the given normalization type to the corresponding PyTorch module.\n\n    Args:\n        norm_type (str): The normalization type ('LN', 'BN', 'GN', or None).\n        embedding_size (int): The size of the token embeddings.\n        track_size (int, optional): The number of groups for Group Normalization.\n\n    Returns:\n        nn.Module: The corresponding normalization module.\n    \"\"\"\n    if norm_type == 'LN':\n        norm = nn.LayerNorm(embedding_size)\n    elif norm_type == 'BN':\n        norm = BatchNorm(embedding_size)\n    elif norm_type == 'GN':\n        norm = GroupNorm(embedding_size, track_size)\n    elif norm_type == 'None':\n        norm = nn.Identity()\n    return norm\n\n\nclass RotateChord(nn.Module):\n    \"\"\"\n    A PyTorch module that performs a parameter-free rotation of tracks within token embeddings.\n\n    This module can be used to augment or modify the input data in a data-driven manner. The rotation is\n    performed jointly for all sequences in a batch and is based on powers of 2 (Chord protocol).\n\n    Args:\n        track_size (int): The size of tracks to be rotated.\n    \"\"\"\n    def __init__(self, track_size):\n        super().__init__()\n        self.track_size = track_size\n\n    def forward(self, x, lengths=None):\n        y = torch.split(\n            tensor=x,\n            split_size_or_sections=self.track_size,\n            dim=-1\n        )\n\n        # Roll sequences in a batch jointly\n        # The first track remains unchanged\n        z = [y[0]]\n        for i in range(1, len(y)):\n            offset = - 2 ** (i - 1)\n            z.append(torch.roll(y[i], shifts=offset, dims=1))\n\n        z = torch.cat(z, -1)\n        return z\n\n    \nclass RotateChordVarLen(nn.Module):\n    \"\"\"\n    A PyTorch module that performs a parameter-free rotation of tracks within variable-length token embeddings.\n\n    This module can be used to augment or modify the input data in a data-driven manner. The rotation is\n    performed separately for all sequences in a batch and is based on powers of 2. This version is designed to\n    handle variable-length input sequences of extremely diverse range.\n    \n    No padding is applied.\n\n    Args:\n        track_size (int): The size of tracks to be rotated.\n    \"\"\"\n    def __init__(self, track_size):\n        super().__init__()\n        self.track_size = track_size\n\n    def forward(self, x, lengths):\n        ys = torch.split(\n            tensor=x,\n            split_size_or_sections=lengths.tolist(),\n            dim=0\n        )\n\n        zs = []\n\n        # Roll sequences separately\n        for y in ys:\n            y = torch.split(\n                tensor=y,\n                split_size_or_sections=self.track_size,\n                dim=-1\n            )\n            z = [y[0]]\n            for i in range(1, len(y)):\n                offset = -2 ** (i - 1)\n                z.append(torch.roll(y[i], shifts=offset, dims=0))\n            z = torch.cat(z, -1)\n            zs.append(z)\n\n        z = torch.cat(zs, 0)\n        return z\n    \n\nclass ChordMixerBlock(nn.Module):\n    \"\"\"\n    A PyTorch module implementing the ChordMixerBlock.\n\n    This module combines two main steps in the ChordMixer layer: Rotate and Mix.\n    The dropout between too is added.\n\n    Args:\n        embedding_size (int): The size of the token embeddings.\n        track_size (int): The size of tracks to be rotated.\n        hidden_size (int): The hidden layer size for the MLP.\n        mlp_dropout (float): The dropout probability for the MLP.\n        layer_dropout (float): The dropout probability for the ChordMixerBlock.\n        prenorm (str): The type of normalization for the pre-normalization step.\n        norm (str): The type of normalization for the post-normalization step.\n        var_len (bool): Whether to use variable-length input sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_size,\n        track_size,\n        hidden_size,\n        mlp_dropout,\n        layer_dropout,\n        prenorm,\n        norm,\n        var_len=False\n    ):\n        super().__init__()\n        self.prenorm = map_norm(prenorm, embedding_size, track_size)\n        self.norm = map_norm(norm, embedding_size, track_size)\n         \n        self.mix = Mlp(\n            embedding_size,\n            hidden_size,\n            embedding_size,\n            act_layer=nn.GELU,\n            drop=mlp_dropout\n        )\n\n        self.dropout = nn.Dropout(layer_dropout)\n        if var_len:\n            self.rotate = RotateChordVarLen(track_size)\n        else:\n            self.rotate = RotateChord(track_size)\n    \n    def forward(self, x, lengths=None):\n        res_con = x\n        x = self.prenorm(x)\n        x = self.mix(x)\n        x = self.dropout(x)\n        x = self.rotate(x, lengths)\n        x = x + res_con\n        x = self.norm(x)\n        return x\n    \n\nclass ChordMixerEncoder(nn.Module):\n    \"\"\"\n    A PyTorch module implementing a ChordMixer Encoder as a stack of ChordMixer layers. \n    The number of layers in the stack is determined by the maximum sequence length in the batch.\n    The number of layers is fixed for the equal lengths mode.\n\n    Args:\n        max_seq_len (int): The maximum sequence length of the input tensor.\n        track_size (int): The size of tracks to be rotated.\n        hidden_size (int): The hidden layer size for the MLP.\n        mlp_dropout (float): The dropout probability for the MLP.\n        layer_dropout (float): The dropout probability for the ChordMixerBlock.\n        prenorm (str): The type of normalization for the pre-normalization step.\n        norm (str): The type of normalization for the post-normalization step.\n        var_len (bool): Whether to use variable-length input sequences.\n    \"\"\"\n    def __init__(\n        self,\n        max_seq_len,\n        track_size,\n        hidden_size,\n        mlp_dropout,\n        layer_dropout,\n        prenorm,\n        norm,\n        var_len=False\n    ):\n        super().__init__()\n        self.max_seq_len = max_seq_len\n        self.max_n_layers = math.ceil(np.log2(max_seq_len))\n        embedding_size = int((self.max_n_layers + 1) * track_size)\n        self.var_len = var_len\n        self.chordmixer_blocks = nn.ModuleList(\n            [\n                ChordMixerBlock(\n                    embedding_size,\n                    track_size,\n                    hidden_size,\n                    mlp_dropout,\n                    layer_dropout,\n                    prenorm,\n                    norm,\n                    var_len\n                )\n                for _ in range(self.max_n_layers)\n            ]\n        )\n\n    def forward(self, x, lengths=None):\n        # If var_len, use a variable number of layers\n        if self.var_len:\n            n_layers = torch.ceil(torch.log2(lengths[0])).detach().cpu().int()\n        else:\n            n_layers = self.max_n_layers\n\n        for layer in range(n_layers):\n            x = self.chordmixer_blocks[layer](x, lengths)\n        return x\n\n\nclass LinearDecoderVarLen(nn.Module):\n    \"\"\"\n    Linear decoder for variable-length input sequences.\n\n    This module computes the mean of the input tensor along the sequence length dimension for each\n    input sequence and applies a linear transformation to the resulting tensor to produce the output.\n\n    Args:\n        embedding_size (int): The size of the token embeddings.\n        output_size (int): The size of the output tensor.\n    \"\"\"\n    def __init__(self, embedding_size, output_size):\n        super().__init__()\n        self.decoder = nn.Linear(embedding_size, output_size)\n\n    def forward(self, x, lengths=None):\n        x = [torch.mean(t, dim=0) for t in torch.split(x, lengths.tolist(), 0)]\n        x = torch.stack(x)\n        return self.decoder(x)\n\n\nclass LinearDecoder(nn.Module):\n    \"\"\"\n    Linear decoder for fixed-length input sequences.\n\n    This module computes the mean of the input tensor along the sequence length dimension and\n    applies a linear transformation to the resulting tensor to produce the output.\n\n    Args:\n        embedding_size (int): The size of the token embeddings.\n        output_size (int): The size of the output tensor.\n    \"\"\"\n    def __init__(self, embedding_size, output_size):\n        super().__init__()\n        self.decoder = nn.Linear(embedding_size, output_size)\n\n    def forward(self, x, lengths=None):\n        x = torch.mean(x, dim=1)\n        return self.decoder(x)\n    \n    \nclass ChordMixer(nn.Module):\n    \"\"\"\n    The ChordMixer model. Encoder is a stack of ChordMixer blocks. Decoder a global average pooling, followed by a linear layer.\n\n    Args:\n        input_size (int): The input size of the embedding layer.\n        output_size (int): The output size of the decoder layer.\n        embedding_type (str): The type of embedding layer ('sparse' or 'linear').\n        decoder (str): The type of decoder layer. We use 'linear'.\n        max_seq_len (int): The maximum sequence length in the data.\n        track_size (int): The size of tracks to be rotated.\n        hidden_size (int): The hidden layer size for the MLPs.\n        mlp_dropout (float): The dropout probability for the MLPs.\n        layer_dropout (float): The dropout probability for the ChordMixerBlock.\n        prenorm (str): The type of normalization for the pre-normalization step.\n        norm (str): The type of normalization for the post-normalization step.\n        var_len (bool, optional): Whether to use variable-length mode.\n    \"\"\"\n    def __init__(self,\n        input_size,\n        output_size,\n        embedding_type,\n        decoder,\n        max_seq_len,\n        track_size,\n        hidden_size,\n        mlp_dropout,\n        layer_dropout,\n        prenorm,\n        norm,\n        var_len=False\n        ):\n            super().__init__()\n            self.max_n_layers = math.ceil(np.log2(max_seq_len))\n            embedding_size = int((self.max_n_layers + 1) * track_size)\n            if embedding_type == 'sparse':\n                self.embedding = nn.Embedding(\n                    input_size,\n                    embedding_size,\n                    padding_idx=0\n                ).apply(self._init_weights)\n            elif embedding_type == 'linear':\n                self.embedding = nn.Linear(\n                    input_size,\n                    embedding_size\n                ).apply(self._init_weights)\n                \n            self.encoder = ChordMixerEncoder(\n                max_seq_len,\n                track_size,\n                hidden_size,\n                mlp_dropout,\n                layer_dropout,\n                prenorm,\n                norm,\n                var_len\n            ).apply(self._init_weights)\n            \n            if decoder == 'linear':\n                if var_len:\n                    self.decoder = LinearDecoderVarLen(embedding_size, output_size).apply(self._init_weights)\n                else:\n                    self.decoder = LinearDecoder(embedding_size, output_size).apply(self._init_weights)\n            \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=1.0)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            \n    def forward(self, x, lengths=None):\n        x = self.embedding(x)\n        x = self.encoder(x, lengths)\n        x = self.decoder(x, lengths)\n        return x\n\n\n\n# for retrieval\nclass ChordMixerNoDec(nn.Module):\n    \"\"\"\n    The ChordMixer model for the Retrieval task. Has no Decoder\n    \"\"\"\n    def __init__(self,\n        input_size,\n        output_size,\n        embedding_type,\n        decoder,\n        max_seq_len,\n        track_size,\n        hidden_size,\n        mlp_dropout,\n        layer_dropout,\n        prenorm,\n        norm,\n        var_len=False\n        ):\n            super().__init__()\n            self.max_n_layers = math.ceil(np.log2(max_seq_len))\n            embedding_size = int((self.max_n_layers + 1) * track_size)\n            if embedding_type == 'sparse':\n                self.embedding = nn.Embedding(\n                    input_size,\n                    embedding_size,\n                    padding_idx=0\n                ).apply(self._init_weights)\n            elif embedding_type == 'linear':\n                self.embedding = nn.Linear(\n                    input_size,\n                    embedding_size\n                ).apply(self._init_weights)\n                \n            self.encoder = ChordMixerEncoder(\n                max_seq_len,\n                track_size,\n                hidden_size,\n                mlp_dropout,\n                layer_dropout,\n                prenorm,\n                norm,\n                var_len\n            ).apply(self._init_weights)\n            \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=1.0)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            \n    def forward(self, x, lengths=None):\n        x = self.embedding(x)\n        x = self.encoder(x, lengths)\n        x = torch.mean(x, dim=1)\n        return x",
    "description": null,
    "url": null
}