{
    "acronym": "unlimiformer",
    "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
    "seed_ids": [
        "memorizingtrans",
        "longformer",
        "memcompress"
    ],
    "s2id": "dbc368bc8b49347dd27679894524fa62f88492c9",
    "abstract": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .",
    "authors": [
        "Amanda Bertsch",
        "Uri Alon",
        "Graham Neubig",
        "Matthew R. Gormley"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.",
    "citationCount": 74,
    "influentialCitationCount": 7,
    "code": "import logging\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom typing import TypeVar, Generic\n\nfrom index_building import Datastore, DatastoreBatch\nfrom unlimiformer_utils import ActivationCapturer\n\nlogger = logging.getLogger('Unlimiformer')\nlogger.setLevel(20)\n\nModelType = TypeVar('ModelType')\nclass Unlimiformer(Generic[ModelType]):\n    def __init__(self, model: ModelType, \n            layer_begin=-1, layer_end=None,\n            unlimiformer_head_num=None, \n            exclude_attention=False, \n            model_encoder_max_len=None,\n            chunk_overlap=0,\n            verbose=False, save_heatmap=False, \n            tokenizer=None, unlimiformer_training=False,\n            use_datastore=False, \n            flat_index=False,\n            test_datastore=False, reconstruct_embeddings=False, \n            gpu_datastore=False, gpu_index=False,\n            index_devices=(0,), datastore_device=0,\n            ):\n        super().__init__()\n        self.model = model\n        model.unlimiformer = self\n        self.layer_begin = layer_begin\n        self.layer_end = layer_end\n        self.specific_head = unlimiformer_head_num\n        self.exclude_attention = exclude_attention\n        self.actual_model_window_size = None\n        self.model_encoder_max_len = model_encoder_max_len\n        self.chunk_overlap = chunk_overlap\n        self.verbose = verbose\n        self.save_heatmap = save_heatmap\n        self.tokenizer = tokenizer\n        self.unlimiformer_training = unlimiformer_training\n\n        self.use_datastore = use_datastore\n        self.flat_index = flat_index\n        self.reconstruct_embeddings = reconstruct_embeddings\n        self.gpu_datastore = gpu_datastore\n        self.gpu_index = gpu_index\n        if torch.cuda.is_available() and gpu_index:\n            self.index_devices = [torch.device(f'cuda:{i}') for i in index_devices]\n        else:\n            self.index_devices = [torch.device('cpu')]\n        self.datastore_device = torch.device(f'cuda:{datastore_device}' if torch.cuda.is_available() and gpu_datastore else 'cpu')\n        self.test_datastore = test_datastore # flag for debugging\n\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.activation_capturer = None\n        self.is_encoder_decoder = model.config.is_encoder_decoder\n        self.hook_handles = []\n        self.is_input_encoding_pass = False\n        self.is_first_test_decoding_step = False\n        self.prev_tokens = None\n        self.last_beam_idx = None\n        self.heatmap = None\n        self.cur_decoder_layer_index = None\n        self.datastore = None\n\n        self.input_ids_size = 0  # must be initialized here to run forward() without generate\n\n        self.break_into(model)\n\n    def break_into(self, model):\n        self.actual_model_window_size = self.window_size()\n        if self.model_encoder_max_len is None:\n            self.model_encoder_max_len = self.actual_model_window_size\n        self.window_margin = int(self.model_encoder_max_len * self.chunk_overlap / 2)\n        self.num_heads = model.config.num_attention_heads\n        if self.specific_head is None:\n            self.head_nums = Ellipsis # torch.arange(0, self.num_heads, device=self.device)\n        else:\n            self.head_nums = self.specific_head\n        self.hooks_injected = False\n        self.training_hooks_injected = False\n        self.original_forward_func = model.forward\n\n        # Activate Unlimiformer when calling model.eval(), deactivate for model.train()\n        self.original_model_eval_func = model.eval\n        model.eval = self.pre_eval_hook\n        self.original_model_train_func = model.train\n        model.train = self.pre_train_hook\n\n    def pre_eval_hook(self):\n        self.remove_training_hooks(self.model)\n        self.inject_hooks(self.model)\n        self.original_model_eval_func()\n\n    def pre_train_hook(self, mode=True):\n        # mode=True means model.train() is called\n        # mode=False means model.eval() is called\n        torch.cuda.empty_cache()\n        if mode is True:\n            self.break_out(self.model)\n            if self.unlimiformer_training:\n                self.inject_training_hooks(self.model)\n        self.original_model_train_func(mode)\n        \n    def inject_hooks(self, model):\n        if self.hooks_injected:\n            return\n        # Inject our activation_capturer to capture the activations at every forward pass\n        attention_layers_to_capture = self.activation_to_capture(self.layer_begin, self.layer_end)\n        self.activation_capturer = []\n        for layer in attention_layers_to_capture:\n            if type(layer) is list:\n                layer_capturers = []\n                for k_or_v in layer:\n                    capturer = ActivationCapturer(k_or_v, capture_input=False)\n                    layer_capturers.append(capturer)\n                    self.register_hook(k_or_v, capturer)\n                self.activation_capturer.append(layer_capturers)\n            else:\n                capturer = ActivationCapturer(layer, capture_input=False)\n                self.register_hook(layer, capturer)\n                self.activation_capturer.append(capturer)\n\n        # Inject our main function after the main attention function\n        attention_layers_to_run = self.attention_op_to_run(self.layer_begin, self.layer_end)\n        for layer in attention_layers_to_run:\n            self.register_hook(layer, self.attention_forward_hook)\n\n        decoder_layers_to_run = self.attention_layer_to_run(self.layer_begin, self.layer_end)\n        self.original_decoder_layer_cross_attn_forward_funcs = []\n        for i, decoder_layer in enumerate(decoder_layers_to_run):\n            decoder_layer_cross_attention = self.cross_attention(decoder_layer)\n            self.original_decoder_layer_cross_attn_forward_funcs.append(decoder_layer_cross_attention.forward)\n            decoder_layer_cross_attention.forward = self.create_cross_attn_pre_forward_hook(decoder_layer_cross_attention.forward, i)\n\n        # Inject our hook function in the beginning of generation.\n        # When the \"model.generate()\" will be called, it will first call our \"reset_generation()\" function, \n        # and only then call \"model.generate()\"\n        self.original_generate_func = model.generate\n        model.generate = self.pre_generate_hook\n\n        model.forward = self.pre_forward_hook\n        \n        self.original_reorder_cache_func = model._reorder_cache\n        model._reorder_cache = self.reorder_cache_hook\n        self.hooks_injected = True\n\n    def inject_training_hooks(self, model):\n        if self.training_hooks_injected:\n            return\n        # self.original_forward_func = model.forward\n        model.forward = self.pre_forward_hook\n\n        decoder_layers_to_run = self.attention_layer_to_run(self.layer_begin, self.layer_end)\n        \n        self.original_decoder_layer_self_attn_forward_funcs = []\n        for decoder_layer in decoder_layers_to_run:\n            attention = self.self_attention(decoder_layer)\n            self.original_decoder_layer_self_attn_forward_funcs.append(attention.forward)\n            attention.forward = self.create_self_attn_pre_forward_hook(attention.forward)\n\n        self.original_decoder_layer_cross_attn_forward_funcs = []\n        for i, decoder_layer in enumerate(decoder_layers_to_run):\n            decoder_layer_cross_attention = self.cross_attention(decoder_layer)\n            self.original_decoder_layer_cross_attn_forward_funcs.append(decoder_layer_cross_attention.forward)\n            decoder_layer_cross_attention.forward = self.create_cross_attn_pre_forward_hook(decoder_layer_cross_attention.forward, i)\n\n        self.original_decoder_layer_forward_funcs = []\n        for decoder_layer in decoder_layers_to_run:\n            self.original_decoder_layer_forward_funcs.append(decoder_layer.forward)\n            decoder_layer.forward = self.create_decoder_layer_func(decoder_layer.forward, decoder_layer)\n\n        self.inject_hooks_for_unaffected_layers(model, decoder_layers_to_run)\n\n        attention_layers_to_run = self.attention_op_to_run(self.layer_begin, self.layer_end)\n        for layer in attention_layers_to_run:\n            self.register_hook(layer, self.train_attention_forward_hook)\n\n        self.training_hooks_injected = True\n\n    def inject_hooks_for_unaffected_layers(self, model, decoder_layers_to_run):\n        self.original_non_injected_decoder_layer_forward_funcs = []\n        non_injected_decoder_layers = [l for l in self.attention_layer_to_run(0, None) \n            if l not in decoder_layers_to_run]\n        for decoder_layer in non_injected_decoder_layers:\n            self.original_non_injected_decoder_layer_forward_funcs.append(decoder_layer.forward)\n            decoder_layer.forward = self.create_noninjected_decoder_layer_func(decoder_layer.forward, decoder_layer)\n\n    def create_self_attn_pre_forward_hook(self, original_self_attn_forward_func):\n        def self_attention_pre_forward_hook(*args, **kwargs):\n            kwargs['past_key_value'] = None\n            return original_self_attn_forward_func(*args, **kwargs)\n        \n        return self_attention_pre_forward_hook\n\n    def create_decoder_layer_func(self, decoder_layer_original_forward_func, decoder_layer):\n        def checkpointed_decoder_layer(\n                hidden_states: torch.Tensor,\n                attention_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                layer_head_mask=None,\n                cross_attn_layer_head_mask=None,\n                past_key_value=None,\n                output_attentions=False,\n                position_bias=None,\n                encoder_decoder_position_bias=None,\n                use_cache=True):\n\n            def forward_with_all_keys(hidden_states, attention_mask, \n                    encoder_hidden_states, encoder_attention_mask, layer_head_mask, \n                    cross_attn_layer_head_mask, past_key_value, \n                    output_attentions, use_cache, long_inputs, long_inputs_mask,\n                    position_bias, encoder_decoder_position_bias):\n                \n                key, value = self.create_key_value(long_inputs, decoder_layer)\n                decoder_layer_args = self.create_decoder_layer_args(\n                    hidden_states=hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    layer_head_mask=layer_head_mask,\n                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    position_bias=position_bias,\n                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n                    use_cache=use_cache,\n                    key=key,value=value)\n                return decoder_layer_original_forward_func(**decoder_layer_args)\n\n            return torch.utils.checkpoint.checkpoint(\n                forward_with_all_keys, hidden_states, attention_mask, \n                encoder_hidden_states, encoder_attention_mask, layer_head_mask, \n                cross_attn_layer_head_mask, None, \n                output_attentions, use_cache, self.long_inputs_encoded, self.long_inputs_mask,\n                position_bias, encoder_decoder_position_bias)\n\n        return checkpointed_decoder_layer\n\n    def create_noninjected_decoder_layer_func(self, decoder_layer_original_forward_func, decoder_layer):\n        def checkpointed_decoder_layer(\n                hidden_states: torch.Tensor,\n                attention_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                layer_head_mask=None,\n                cross_attn_layer_head_mask=None,\n                past_key_value=None,\n                output_attentions=False,\n                position_bias=None,\n                encoder_decoder_position_bias=None,\n                use_cache=True):\n\n           \n            def forward_with_all_keys(hidden_states, attention_mask, \n                    encoder_hidden_states, encoder_attention_mask, layer_head_mask, \n                    cross_attn_layer_head_mask, past_key_value, \n                    output_attentions, use_cache, long_inputs, long_inputs_mask,\n                    position_bias, encoder_decoder_position_bias):\n                \n                decoder_layer_args = self.create_decoder_layer_args(\n                    hidden_states=hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    layer_head_mask=layer_head_mask,\n                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    position_bias=position_bias,\n                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n                    use_cache=use_cache, key=None, value=None)\n                return decoder_layer_original_forward_func(**decoder_layer_args)\n\n            return torch.utils.checkpoint.checkpoint(\n                forward_with_all_keys, hidden_states, attention_mask, \n                encoder_hidden_states, encoder_attention_mask, layer_head_mask, \n                cross_attn_layer_head_mask, None, \n                output_attentions, use_cache, self.long_inputs_encoded, self.long_inputs_mask,\n                position_bias, encoder_decoder_position_bias)\n\n        return checkpointed_decoder_layer\n\n    def register_hook(self, layer, func, pre=False):\n        handle = layer.register_forward_pre_hook(func) if pre else layer.register_forward_hook(func)\n        self.hook_handles.append(handle)\n\n    def break_out(self, model):\n        self.prompt_keys = []\n        self.prompt_values = []\n        self.prompt_attention_mask = []\n        self.generated_input_ids = []\n        torch.cuda.empty_cache()\n        if not self.hooks_injected:\n            return\n\n        for h in self.hook_handles:\n            h.remove()\n        model.generate = self.original_generate_func\n        model.forward = self.original_forward_func\n        model._reorder_cache = self.original_reorder_cache_func\n\n        decoder_layers_to_run = self.attention_layer_to_run(self.layer_begin, self.layer_end)\n        for decoder_layer, original_func in zip(decoder_layers_to_run, self.original_decoder_layer_cross_attn_forward_funcs):\n            self.cross_attention(decoder_layer).forward = original_func\n        self.hooks_injected = False\n\n    def remove_training_hooks(self, model):\n        self.long_inputs_encoded, self.long_inputs_mask = None, None\n        if not self.training_hooks_injected:\n            return\n        for h in self.hook_handles:\n            h.remove()\n        model.forward = self.original_forward_func\n\n        decoder_layers_to_run = self.attention_layer_to_run(self.layer_begin, self.layer_end)\n        for decoder_layer, original_func in zip(decoder_layers_to_run, self.original_decoder_layer_self_attn_forward_funcs):\n            self.self_attention(decoder_layer).forward = original_func\n        for decoder_layer, original_func in zip(decoder_layers_to_run, self.original_decoder_layer_cross_attn_forward_funcs):\n            self.cross_attention(decoder_layer).forward = original_func\n        for decoder_layer, original_func in zip(decoder_layers_to_run, self.original_decoder_layer_forward_funcs):\n            decoder_layer.forward = original_func\n\n        non_injected_decoder_layers = [l for l in self.attention_layer_to_run(0, None) \n            if l not in decoder_layers_to_run]\n        for decoder_layer, original_func in zip(non_injected_decoder_layers, self.original_non_injected_decoder_layer_forward_funcs):\n            decoder_layer.forward = original_func\n\n        self.training_hooks_injected = False\n\n    def reset_memory(self, input_ids, attention_mask):\n        if self.use_datastore:\n            if self.is_encoder_decoder:\n                self.datastore = [DatastoreBatch(dim=self.model.config.hidden_size, batch_size=input_ids.shape[0], flat_index=self.flat_index, \n                    gpu_index=self.gpu_index, index_device=self.index_devices[0])]\n                self.hidden_states = [[]]\n            else:\n                self.datastore = [DatastoreBatch(dim=self.model.config.hidden_size, batch_size=input_ids.shape[0], flat_index=self.flat_index, \n                    gpu_index=self.gpu_index, index_device=self.index_devices[i % len(self.index_devices)]) \n                    for i in range(self.model.config.num_hidden_layers)[self.layer_begin:self.layer_end]]\n                self.hidden_states = [[] for _ in range(self.model.config.num_hidden_layers)[self.layer_begin:self.layer_end]]\n            torch.cuda.empty_cache()\n        self.prompt_input_ids = input_ids\n        self.input_ids_size = input_ids.shape[-1]\n        self.prompt_keys, self.prompt_values = None, None\n        self.prev_tokens = [None for _ in range(len(self.original_decoder_layer_cross_attn_forward_funcs))]\n        self.last_beam_idx = None\n        self.cur_layer_key_value_placeholder = None\n        self.is_input_encoding_pass = True\n        if self.is_encoder_decoder:\n            dummy_labels = torch.zeros((input_ids.shape[0], 1), dtype=torch.long, device=input_ids.device)\n        else:\n            dummy_labels = None\n        if self.save_heatmap:\n            if self.heatmap is not None:\n                print(f'Generated: {self.tokenizer.decode(self.generated_input_ids[0])}')\n                self.plot_heatmap(self.heatmap[0].detach().cpu().numpy())\n            self.heatmap = torch.tensor([], dtype=torch.float, device=input_ids.device)\n        self.generated_input_ids = torch.tensor([], dtype=torch.long, device=input_ids.device)\n\n        self.prompt_keys = [[] for _ in range(self.model.config.num_hidden_layers)[self.layer_begin:self.layer_end]]\n        self.prompt_values = [[] for _ in range(self.model.config.num_hidden_layers)[self.layer_begin:self.layer_end]]\n        self.prompt_attention_mask = []\n        window_indices = self.window_indices(input_ids.shape[-1])\n\n        for context_start_ind, context_end_ind, update_start_ind, update_end_ind in window_indices:\n            logger.info(f'Encoding {context_start_ind} to {context_end_ind} out of {input_ids.shape[-1]}')\n            chunk = input_ids[:, context_start_ind:context_end_ind].to(self.device)\n            chunk_attention_mask = attention_mask[:, context_start_ind:context_end_ind].to(self.device)\n            with torch.inference_mode():\n                _ = self.model(chunk, attention_mask=chunk_attention_mask, labels=dummy_labels) # , return_dict=True, output_hidden_states=True)\n            if self.use_datastore:\n                # TODO: verify with BART as well\n                # hidden_states_to_index = [hidden_states.encoder_last_hidden_state] # list of length 1 of (batch, chunked_source_len, dim)\n                hidden_states_to_index = [\n                    layer_capturer.captured for layer_capturer in self.activation_capturer\n                ] \n                # hidden_states_to_index = list(hidden_states.hidden_states)[:-1][self.layer_begin:self.layer_end]\n                to_add = [state[:, update_start_ind:update_end_ind].detach() for state in hidden_states_to_index]\n                to_apply_mask = chunk_attention_mask[:, update_start_ind:update_end_ind]\n                # to_apply_mask = to_apply_mask.log().to(to_add[0].dtype)\n                to_apply_mask = to_apply_mask.to(to_add[0].dtype)\n                if not self.reconstruct_embeddings:\n                    to_add_embeddings = to_add\n                    if not self.gpu_datastore:\n                        to_add_embeddings = [states.cpu() for states in to_add_embeddings]\n                        to_apply_mask = to_apply_mask.cpu()\n                    for i, layer_states in enumerate(to_add_embeddings):\n                        layer_states = layer_states * to_apply_mask.unsqueeze(-1)\n                        self.hidden_states[i].append(layer_states.to(self.datastore_device))\n                # list of len layers, inside it there is a list of len batch, each item is (masked_time, dim)\n                # for i, to_add_layer in enumerate(to_add):\n                #     keys = [key[mask.bool()] for key, mask in zip(to_add_layer, to_apply_mask)]\n                #     self.datastore[i].add_keys(keys)\n            if (not self.use_datastore) or self.test_datastore:\n                layers_kv = [\n                    self.process_key_value(layer_capturer) # (batch, head, time, dim)\n                    for layer_capturer in self.activation_capturer\n                ] # list of pairs of (batch, head, time, dim)\n\n                # list of (batch, head, chunked_time, dim)\n                key = [layer[0][:, :, update_start_ind:update_end_ind] for layer in layers_kv]\n                value = [layer[1][:, :, update_start_ind:update_end_ind] for layer in layers_kv]\n                chunk_attention_mask = chunk_attention_mask[:, update_start_ind:update_end_ind] # (batch, chunked_time)\n\n                # key = torch.stack(key, dim=0) # (num_layers, batch, head, time, dim)\n                # value = torch.stack(value, dim=0) # (num_layers, batch, head, time, dim)\n\n                for i, (layer_key, layer_value) in enumerate(zip(key, value)):\n                    self.prompt_keys[i].append(layer_key) # (num_layers, batch, head, chunked_source_len, dim)\n                    self.prompt_values[i].append(layer_value) # (num_layers, batch, head, chunked_source_len, dim)\n                self.prompt_attention_mask.append(chunk_attention_mask) # (batch, chunked_source_len)\n        \n        if self.use_datastore:\n            # keys are all in datastore already!\n            if not self.reconstruct_embeddings:\n                # self.hidden_states = [torch.cat(layer_hidden_states, axis=1) for layer_hidden_states in self.hidden_states]\n                concat_hidden_states = []\n                for i in range(len(self.hidden_states)):\n                    concat_hidden_states.append(torch.cat(self.hidden_states[i], axis=1))\n                    self.hidden_states[i] = None\n                self.hidden_states = concat_hidden_states\n            for datastore, layer_hidden_states in zip(self.datastore, self.hidden_states):\n                datastore.train_index(layer_hidden_states)\n        if (not self.use_datastore) or self.test_datastore:\n            for i, (layer_keys, layer_values) in enumerate(zip(self.prompt_keys, self.prompt_values)):\n                self.prompt_keys[i] = torch.cat(layer_keys, dim=-2)\n                self.prompt_values[i] = torch.cat(layer_values, dim=-2)\n            # self.prompt_keys = torch.cat(self.prompt_keys, dim=-2) # (num_layers, batch, head, source_len, dim)\n            # self.prompt_values = torch.cat(self.prompt_values, dim=-2) # (num_layers, batch, head, source_len, dim)\n            self.prompt_attention_mask = torch.cat(self.prompt_attention_mask, dim=-1) # (batch, source_len)\n\n        self.is_input_encoding_pass = False\n        if self.verbose:\n            print(f'Input: '\n                f'{self.tokenizer.decode(input_ids[0][:self.actual_model_window_size], skip_special_tokens=True)} ||| '\n                f'{self.tokenizer.decode(input_ids[0][self.actual_model_window_size:], skip_special_tokens=True)}')\n            print()\n\n    def chunked_encode_input(self, input_ids, attention_mask):\n        long_inputs_encoded = []\n        long_inputs_mask = []\n        window_indices = self.window_indices(input_ids.shape[-1])\n\n        self.is_input_encoding_pass = True\n        for context_start_ind, context_end_ind, update_start_ind, update_end_ind in window_indices:\n            chunk = input_ids[:, context_start_ind:context_end_ind]\n            chunk_attention_mask = attention_mask[:, context_start_ind:context_end_ind]\n            output = self.model.base_model.encoder(chunk, attention_mask=chunk_attention_mask, return_dict=True, output_hidden_states=True)\n            encoder_last_hidden_state = output.last_hidden_state # (batch, time, dim)\n            \n            # list of (batch, head, chunked_time, dim)\n            encoder_last_hidden_state = encoder_last_hidden_state[:, update_start_ind:update_end_ind] # (batch, chunked_time, dim)\n            chunk_attention_mask = chunk_attention_mask[:, update_start_ind:update_end_ind] # (batch, chunked_time)\n\n            long_inputs_encoded.append(encoder_last_hidden_state) # (batch, chunked_source_len, dim)\n            long_inputs_mask.append(chunk_attention_mask) # (batch, chunked_source_len)\n        \n        long_inputs_encoded = torch.cat(long_inputs_encoded, dim=1) # (batch, source_len, dim)\n        long_inputs_mask = torch.cat(long_inputs_mask, dim=1) # (batch, source_len)\n\n        self.is_input_encoding_pass = False\n        if self.verbose:\n            print(f'Input: '\n                f'{self.tokenizer.decode(input_ids[0][:self.actual_model_window_size], skip_special_tokens=True)} ||| '\n                f'{self.tokenizer.decode(input_ids[0][self.actual_model_window_size:], skip_special_tokens=True)}')\n            print()\n        return long_inputs_encoded, long_inputs_mask\n\n    def window_indices(self, total_seq_len):\n        # Copied from SLED (Ivgy et al., 2022)\n        # https://github.com/Mivg/SLED/blob/main/sled/modeling_sled.py#L467\n        if total_seq_len <= self.model_encoder_max_len:\n            return [(0, total_seq_len, 0, total_seq_len)]\n        else:\n            results = []\n            # if self.chunk_overlap == 0:\n            #     stride = self.model_encoder_max_len\n            stride = self.model_encoder_max_len - 2 * self.window_margin\n            context_start = update_start_ind = 0\n            context_end = self.model_encoder_max_len\n            if self.is_encoder_decoder:\n                update_end_ind = context_end - self.window_margin\n            else:\n                update_end_ind = context_end\n            # first window always should update from the beginning\n            results.append((context_start, context_end, update_start_ind, update_end_ind))  \n\n            while context_end < total_seq_len:\n                context_end = min(total_seq_len, context_end + stride)\n                context_start = (\n                    context_start + stride if context_end < total_seq_len else total_seq_len - self.model_encoder_max_len\n                )\n                update_start_ind = max(update_start_ind + stride, update_end_ind)\n                # last window always should update until the end\n                update_end_ind = (\n                    min(total_seq_len, update_end_ind + stride) if context_end < total_seq_len else total_seq_len\n                )\n\n                cs, ce, us, ue = context_start, context_end, update_start_ind - context_start, \\\n                                 update_end_ind - context_start\n\n                results.append((cs, ce, us, ue))\n            return results\n\n    def pre_generate_hook(self, input_ids, **kwargs):\n        if 'attention_mask' not in kwargs:\n            kwargs['attention_mask'] = torch.ones_like(input_ids)\n        self.reset_memory(input_ids, kwargs['attention_mask'])\n        new_kwargs = kwargs\n        if 'attention_mask' in kwargs:\n            new_kwargs = {k: v for k, v in kwargs.items() if k != 'attention_mask'}\n            new_kwargs['attention_mask'] = kwargs['attention_mask'][:, :self.actual_model_window_size].to(self.device)\n        new_kwargs['use_cache'] = True\n        if self.is_encoder_decoder:\n            input_ids_prefix = input_ids[:, :self.actual_model_window_size]\n        else:\n            input_ids_prefix = input_ids[:, -self.actual_model_window_size:]\n        input_ids_prefix = input_ids_prefix.to(self.device)\n        return self.original_generate_func(input_ids_prefix, **new_kwargs)\n\n    def pre_forward_hook(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        self.set_gradient_checkpointing(False)\n        if not self.is_input_encoding_pass:\n            if self.model.training:\n                # self.reset_memory(input_ids, attention_mask)\n                self.long_inputs_encoded, self.long_inputs_mask = self.chunked_encode_input(input_ids=input_ids, attention_mask=attention_mask)\n                input_ids = input_ids[:, :self.actual_model_window_size]\n                attention_mask = attention_mask[:, :self.actual_model_window_size] if attention_mask is not None else None\n                # input_ids = input_ids[:, :self.model_encoder_max_len]\n                # labels = labels[:, :self.model_encoder_max_len] if labels is not None else None\n            else:\n                if kwargs.get('past_key_values') is None:\n                    self.is_first_test_decoding_step = True\n\n                if input_ids is not None:\n                    # self.input_ids_size += input_ids.shape[-1]\n                    self.input_ids_size += 1\n                if kwargs.get('decoder_input_ids') is not None:\n                    self.generated_input_ids = torch.cat([self.generated_input_ids, kwargs['decoder_input_ids']], axis=-1)\n            \n        result = self.original_forward_func(input_ids=input_ids, labels=labels, attention_mask=attention_mask, **kwargs)\n        self.is_first_test_decoding_step = False\n        return result\n\n    def create_cross_attn_pre_forward_hook(self, original_cross_attn_forward_func, cur_layer_num):\n        def attention_pre_forward_hook(hidden_states, attention_mask=None, *args, **kwargs):\n            self.cur_decoder_layer_index = cur_layer_num\n            if kwargs.get('past_key_value') is not None:\n                # it's a tuple, and we convert it to a list to be able to perform assignment \n                # and modify its items from our attention_forward_hook\n                self.cur_layer_key_value_placeholder = \\\n                    kwargs['past_key_value'] = list(kwargs['past_key_value']) # (batch, head, time, attn_dim)\n\n            batch_size, tgt_len, dim = hidden_states.shape\n            if self.model.training:\n                # from: (batch, tgt_len, dim) to: (batch * tgt_len, 1, dim)\n                hidden_states = hidden_states.reshape(-1, 1, hidden_states.shape[-1])\n                # from: (batch, 1, tgt_len, dim) to: (batch * tgt_len, 1, 1, dim)\n                attention_mask = attention_mask.reshape(-1, 1, 1, attention_mask.shape[-1])\n                \n                attn_output, attn_weights_reshaped, past_key_value = original_cross_attn_forward_func(hidden_states=hidden_states, attention_mask=attention_mask, *args, **kwargs)\n                attn_output = attn_output.reshape(batch_size, tgt_len, dim)\n                result = (attn_output, attn_weights_reshaped, past_key_value)\n            else:\n                result = original_cross_attn_forward_func(hidden_states=hidden_states, attention_mask=attention_mask, *args, **kwargs)\n                # Uri: this part adds the generated tokens to the prompt. \n                # However it was commented out because currently we always keep the generated tokens in the attention window\n                # if not self.is_encoder_decoder and not self.is_input_encoding_pass and \\\n                #         past_key_value[0].shape[2] > self.prompt_keys[self.cur_decoder_layer_index].shape[2]:\n                #     self.prompt_keys[self.cur_decoder_layer_index] = torch.cat([self.prompt_keys[self.cur_decoder_layer_index], past_key_value[0][:,:,-1:]], dim=-2)\n                #     self.prompt_values[self.cur_decoder_layer_index] = torch.cat([self.prompt_values[self.cur_decoder_layer_index], past_key_value[1][:,:,-1:]], dim=-2)\n                #     if self.cur_decoder_layer_index == self.model.config.num_hidden_layers - 1:\n                #         self.prompt_attention_mask = torch.cat([\n                #             self.prompt_attention_mask, \n                #             torch.ones([self.prompt_attention_mask.shape[0], 1], dtype=self.prompt_attention_mask.dtype).to(self.device)], dim=-1)\n        \n            return result\n        return attention_pre_forward_hook\n\n\n    def attention_forward_hook(self, module, input, output):\n        # output: (batch, time, 3 * heads * attention_dim)\n        if self.is_input_encoding_pass or self.is_first_test_decoding_step:\n            return\n        with torch.no_grad():\n            prompt_size = self.prompt_input_ids.shape[1]\n            generated_size = self.input_ids_size - prompt_size\n            window_size = self.cur_layer_key_value_placeholder[0].shape[-2]\n            # topk = min(self.actual_model_window_size, attn_weights.shape[-1])\n            topk = min(prompt_size, window_size)\n            if not self.is_encoder_decoder:\n                topk = min(topk, window_size - generated_size + 1)\n            if self.gpu_index:\n                topk = min(topk, 2048)\n\n            query = self.process_query(output)[:,-1] # (batch * beam, head, dim)\n            query = query[:, self.head_nums] # (batch * beam, head, dim)\n\n            if self.use_datastore:\n                # query: (batch, beam, head, dim)\n                # need to multiply by key vector\n                # query.view(query.shape[0], query.shape[1] * query.shape[2])\n                # k_proj in attention? \n                datastore_index = 0 if self.is_encoder_decoder else self.cur_decoder_layer_index\n                attention_layer_list = self.get_kv_projections(self.layer_begin, self.layer_end)\n                k_proj_layer = [layers[0] for layers in attention_layer_list][self.cur_decoder_layer_index]\n                v_proj_layer = [layers[1] for layers in attention_layer_list][self.cur_decoder_layer_index]\n                \n                # modify query by k_projs \n                k_proj = k_proj_layer.weight\n                datastore_query = self.preprocess_query(query, k_proj) # (batch * beam, num_heads, embed_dim)\n                batch_size = self.datastore[datastore_index].batch_size\n                datastore_query = datastore_query.view((batch_size, -1, datastore_query.shape[2])) # (batch, beam * num_heads, embed_dim)\n                # then search\n                if self.reconstruct_embeddings:\n                    # embeddings: (batch, beam * head, actual_model_window_size, dim)\n                    _, top_search_key_indices, embeddings = self.datastore[datastore_index].search_and_reconstruct(datastore_query, k=topk) \n                else:\n                    _, top_search_key_indices = self.datastore[datastore_index].search(datastore_query, k=topk)\n                    # self.embeddings: (batch,              src_len, dim)\n                    # indices:         (batch, beam * head, actual_model_window_size)\n                    # embeddings: (batch, beam * head, actual_model_window_size, dim)\n                    embeddings = torch.take_along_dim(input=self.hidden_states[datastore_index].unsqueeze(1), \n                        indices=top_search_key_indices.unsqueeze(-1).to(self.hidden_states[datastore_index].device), dim=-2)\n                    embeddings = embeddings.to(self.device)\n                # (batch, beam, head, actual_model_window_size)\n                # top_search_key_scores = top_search_key_scores.reshape(batch_size, -1, *top_search_key_scores.shape[1:])\n                top_search_key_indices = top_search_key_indices.reshape(batch_size, -1, *top_search_key_indices.shape[1:])\n                # embeddings: (batch, beam, head, actual_model_window_size, dim)\n                embeddings = embeddings.reshape(batch_size, -1, self.num_heads, *embeddings.shape[2:])\n                                    \n            # raw_values are actually token indices; need to look them up\n            if (not self.use_datastore) or self.test_datastore:\n                this_layer_prompt_keys = self.prompt_keys[self.cur_decoder_layer_index]\n                this_layer_prompt_values = self.prompt_values[self.cur_decoder_layer_index]\n                # query: (batch * beam, head, dim)\n                batch_size = self.prompt_input_ids.shape[0]\n                beam_size = query.shape[0] // batch_size\n                # query: (batch, beam, head, dim)\n                query = query.reshape(batch_size, beam_size, *query.shape[1:])\n                # this_layer_prompt_keys: (batch, head, source_len, dim)\n                # this_layer_prompt_keys.unsqueeze(1):  (batch, 1, head, source_len, dim)\n                # query.unsqueeze(-1):             (batch, beam, head, dim, 1)\n                # attn_weights:  (batch, beam, head, source_len)\n                attn_weights = torch.matmul(this_layer_prompt_keys.unsqueeze(1)[:, :, self.head_nums], query.unsqueeze(-1)).squeeze(-1) \n                # attn_weights = torch.matmul(query.unsqueeze(-2), this_layer_prompt_keys.unsqueeze(1)[:, :, self.head_nums]).squeeze(-2) \n                prompt_attention_mask_to_add = (1 - self.prompt_attention_mask) * -1e9 # (batch, source_len)\n                prompt_attention_mask_to_add = prompt_attention_mask_to_add.unsqueeze(1).unsqueeze(1)\n                attn_weights += prompt_attention_mask_to_add # (batch, beam, head, source_len)\n                if self.exclude_attention and attn_weights.shape[-1] > self.actual_model_window_size:\n                    attn_weights[..., :self.actual_model_window_size] -= 1e9\n\n                # target_keys, target_values, topk = self.get_target_slices(output)\n                top_key_scores, top_key_indices = torch.topk(attn_weights, k=topk, dim=-1, sorted=True) # (batch, beam, head, trunc_source)\n                if self.save_heatmap:\n                    # heatrow: (beam, heads, source_len)\n                    heatrow = torch.zeros([top_key_indices.shape[1], top_key_indices.shape[2], this_layer_prompt_keys.shape[-2]], dtype=torch.float)\n                    heatrow = heatrow.scatter(index=top_key_indices[0], src=torch.ones_like(top_key_scores[0]), dim=-1)\n                    # heatrow = torch.nn.functional.softmax(heatrow, dim=-1)\n                    # self.heatmap: (beam, heads, targets, source_len)\n                    self.heatmap = torch.cat([self.heatmap, heatrow.unsqueeze(-2)], axis=-2)\n\n            if self.test_datastore:\n                assert top_key_indices.shape == top_search_key_indices.shape\n                assert torch.mean((top_key_indices == top_search_key_indices).float()) > 0.99\n\n            if self.verbose:\n                if self.is_encoder_decoder:\n                    for i, beam in enumerate(self.generated_input_ids):\n                        print(f'({i}) Generated: {self.tokenizer.decode(beam)}')\n                # else:\n                #     print(f'Generated: {self.tokenizer.decode(self.input_ids)}')\n                print()\n        \n        if self.use_datastore:\n            # k_proj_layer.weight, v_proj_layer.weight: (embed_dim, embed_dim)\n            # embeddings: (batch, beam, head, encoder_len, embed_dim)\n            retrieved_keys, retrieved_values = self.post_process_retrieved(embeddings, k_proj_layer, v_proj_layer, top_search_key_indices)\n        else:\n            # this_layer_prompt_keys:   (batch,       head, source_len, dim)\n            # top_key_indices:          (batch, beam, head, trunc_source)\n            retrieved_keys = torch.take_along_dim(this_layer_prompt_keys.unsqueeze(1), indices=top_key_indices.unsqueeze(-1), \n                dim=-2) # (batch, head, trunc_source, attn_dim)\n            retrieved_values = torch.take_along_dim(this_layer_prompt_values.unsqueeze(1), indices=top_key_indices.unsqueeze(-1), \n                dim=-2) # (batch, head, trunc_source, attn_dim)\n\n        if self.test_datastore:\n            correct_keys = torch.take_along_dim(this_layer_prompt_keys.unsqueeze(1), indices=top_key_indices.unsqueeze(-1), \n                dim=-2) # (batch, head, trunc_source, attn_dim)\n            correct_values = torch.take_along_dim(this_layer_prompt_values.unsqueeze(1), indices=top_key_indices.unsqueeze(-1), \n                dim=-2) # (batch, head, trunc_source, attn_dim)\n            assert correct_keys.shape == retrieved_keys.shape\n            assert correct_values.shape == retrieved_values.shape\n            assert torch.mean(torch.isclose(correct_keys, retrieved_keys, rtol=1e-3, atol=1e-3).float()) > 0.99\n            assert torch.mean(torch.isclose(correct_values, retrieved_values, rtol=1e-3, atol=1e-3).float()) > 0.99\n\n        # retrieved_keys, retrieved_values: (batch * beam, head, encoder_len, attn_dim)\n        retrieved_keys = retrieved_keys.flatten(0, 1)[:,:,:topk]\n        retrieved_values = retrieved_values.flatten(0, 1)[:,:,:topk]\n        self.cur_layer_key_value_placeholder[0] = torch.cat([retrieved_keys, self.cur_layer_key_value_placeholder[0][:,:,topk:]], dim=-2)\n        self.cur_layer_key_value_placeholder[1] = torch.cat([retrieved_values, self.cur_layer_key_value_placeholder[1][:,:,topk:]], dim=-2)\n        return\n\n    def train_attention_forward_hook(self, module, input, output):\n        # output: (batch, time, 3 * heads * attention_dim)\n        if self.is_input_encoding_pass or self.is_first_test_decoding_step:\n            return\n        this_layer_prompt_keys = self.cur_layer_key_value_placeholder[0]\n        this_layer_prompt_values = self.cur_layer_key_value_placeholder[1]\n        with torch.no_grad():\n            query = self.process_query(output) # (batch * beam, tgt_len, head, dim)\n            # query = query[:, :, self.head_nums] # (batch * beam, head, dim)\n            \n            # query: (batch * beam, tgt_len, head, dim)\n            batch_size = this_layer_prompt_keys.shape[0]\n            tgt_len = query.shape[0] // batch_size\n            # query: (batch, tgt, head, dim)\n            query = query.reshape(batch_size, tgt_len, *query.shape[2:])\n            # this_layer_prompt_keys: (batch, head, source_len, dim)\n            # this_layer_prompt_keys.unsqueeze(1):  (batch, 1, head, source_len, dim)\n            # attn_weights:  (batch, tgt_len, head, 1, source_len)\n            # attn_weights = torch.matmul(query.unsqueeze(-2), this_layer_prompt_keys.unsqueeze(1).permute(0,1,2,4,3))\n            attn_weights = torch.matmul(this_layer_prompt_keys.unsqueeze(1), query.unsqueeze(-1)) \\\n                .reshape(batch_size, tgt_len, query.shape[-2], 1, this_layer_prompt_keys.shape[-2])\n            # attn_weights = torch.matmul(query.unsqueeze(-2), this_layer_prompt_keys.unsqueeze(1)[:, :, self.head_nums]).squeeze(-2) \n            prompt_attention_mask_to_add = (1 - self.long_inputs_mask) * -1e9 # (batch, source_len)\n            prompt_attention_mask_to_add = prompt_attention_mask_to_add.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n            attn_weights += prompt_attention_mask_to_add # (batch, beam, head, source_len)\n\n            # target_keys, target_values, topk = self.get_target_slices(output)\n            topk = min(self.actual_model_window_size, attn_weights.shape[-1])\n            top_key_scores, top_key_indices = torch.topk(attn_weights, k=min(topk, attn_weights.shape[-1]), dim=-1, sorted=True) # (batch, beam, head, tgt, trunc_source)\n\n                   \n        # this_layer_prompt_keys:   (batch,          head,    source_len, dim)\n        # top_key_indices:          (batch, tgt_len, head, 1, trunc_source)\n        new_keys = torch.take_along_dim(this_layer_prompt_keys.unsqueeze(2).unsqueeze(1), indices=top_key_indices.unsqueeze(-1), \n            dim=-2) # (batch, tgt_len, head, 1, trunc_source, attn_dim)\n        new_values = torch.take_along_dim(this_layer_prompt_values.unsqueeze(2).unsqueeze(1), indices=top_key_indices.unsqueeze(-1), \n            dim=-2) # (batch, tgt_len, head, 1, trunc_source, attn_dim)\n        \n        # (batch * beam, head, tgt_len, trunc_source, attn_dim)\n        self.cur_layer_key_value_placeholder[0] = new_keys.flatten(0, 1).squeeze(2)\n        self.cur_layer_key_value_placeholder[1] = new_values.flatten(0, 1).squeeze(2)\n        return\n\n    \n    def preprocess_query(self, query, k_proj_weight):\n        k_proj = k_proj_weight.view(1, self.num_heads, query.shape[-1], k_proj_weight.shape[0]) # (1, num_heads, attn_dim, embed_dim)\n        datastore_query = query.unsqueeze(-2) # (batch * beam, num_heads, 1, attn_dim)\n        datastore_query = torch.matmul(datastore_query, k_proj) # (batch * beam, num_heads, 1, embed_dim)\n        datastore_query = datastore_query.squeeze(-2)  # (batch * beam, num_heads, embed_dim)\n        return datastore_query\n\n    def post_process_retrieved(self, embeddings, k_proj_layer, v_proj_layer, top_search_key_indices):\n        embed_dim = embeddings.shape[-1]\n        k_weight = k_proj_layer.weight.view(1, 1, self.num_heads, embed_dim // self.num_heads, embed_dim).transpose(-2,-1) # (1, 1, heads, embed_dim, attn_dim)\n        k_bias = 0\n        if k_proj_layer.bias is not None:\n            k_bias = k_proj_layer.bias.view(1, self.num_heads, embed_dim // self.num_heads).unsqueeze(-2).unsqueeze(0)\n        v_weight = v_proj_layer.weight.view(1, 1, self.num_heads, embed_dim // self.num_heads, embed_dim).transpose(-2,-1)  # (1, heads, embed_dim, attn_dim)\n        v_bias = 0\n        if v_proj_layer.bias is not None:\n            v_bias = v_proj_layer.bias.view(1, self.num_heads, embed_dim // self.num_heads).unsqueeze(-2).unsqueeze(0)\n        # new_keys, new_values: (batch, beam, head, encoder_len, attn_dim)\n        retrieved_keys = torch.matmul(embeddings, k_weight) + k_bias # (beam, head, encoder_len, embed_dim)\n        retrieved_values = torch.matmul(embeddings, v_weight) + v_bias # (beam, head, encoder_len, embed_dim)\n        return retrieved_keys, retrieved_values\n\n    def set_gradient_checkpointing(self, value):\n        self.model.base_model.decoder.gradient_checkpointing = value\n\n    def reorder_cache_hook(self, past, beam_idx):\n        self.last_beam_idx = beam_idx\n        self.generated_input_ids = self.generated_input_ids[beam_idx]\n        for i, layer_prev_tokens in enumerate(self.prev_tokens):\n            if layer_prev_tokens is not None:\n                self.prev_tokens[i] = layer_prev_tokens.flatten(0, 1)[beam_idx].reshape(layer_prev_tokens.shape)\n        if self.save_heatmap and self.heatmap.numel() > 0:\n            self.heatmap = self.heatmap[beam_idx]\n        return self.original_reorder_cache_func(past, beam_idx)\n    ",
    "description": null,
    "url": null
}