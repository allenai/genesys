{
    "acronym": "sparsetokentrans",
    "title": "Sparse Token Transformer with Attention Back Tracking",
    "seed_ids": [],
    "s2id": "8b16dc5b4c0728147eef1647a6ab7f786333b76c",
    "abstract": "N/A",
    "authors": [
        "Heejun Lee",
        "Minki Kang",
        "Youngwan Lee",
        "Sung Ju Hwang"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "tldr": "N/A",
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": "\"\"\"\nSparse Token Transformer with Attetion Back Tracking\n***\n2022\n\"\"\"\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom transformers.modeling_utils import apply_chunking_to_forward\nfrom transformers.utils import logging\n\nfrom .modules import BertAttention, BertIntermediate, BertOutput\n\nlogger = logging.get_logger(__name__)\n\nEPS = 1e-7\nUSE_LTP_ON_CONCRETE = False\n\n\n\nclass LTPPruneToken(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.soft_pruning = True\n        self.threshold = None # nn.Parameter(torch.randn((1,), dtype=torch.float32))\n        self.last_mask = None\n        self.new_attention_mask = None\n        self.temperature = 5e-4\n    \n    def init_threshold(self, l, L):\n        self.threshold = nn.Parameter(torch.tensor([0.01 * l / L], dtype=torch.float32))\n\n    def forward(self, x, attention_score, attention_mask):\n        # x: (N, T, H)\n        # attention_score: (N, HEAD, T, T)\n        N, T0, H = x.shape\n        _N, HEAD, T1, T2 = attention_score.shape\n        assert T1 == T2\n        assert T0 == T1\n        T = T1\n        assert N == _N\n\n        if self.soft_pruning:\n            #score (N, T)\n            score = torch.mean(torch.mean(attention_score, dim=1), dim=1)\n            self.last_mask = torch.sigmoid((score - self.threshold) / self.temperature)\n        else:\n            score = torch.mean(torch.mean(attention_score, dim=1), dim=1)\n            self.last_mask = (score > self.threshold) * 1.0\n            # this is replace the attention mask for next layer. so equivalent to drop the token.\n            # have to update attention mask when hard pruning, according to LTP implementation.\n            new_attention_mask = (1-self.last_mask) * (-10000)\n            attention_mask = new_attention_mask.view(*attention_mask.shape)\n        self.last_mask = self.last_mask.unsqueeze(-1) # masking layer output\n        self.new_attention_mask = attention_mask\n        \n        return x * self.last_mask\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config, arch='bert'):\n        super().__init__()\n        self.arch = arch\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config, arch=arch)\n        self.is_decoder = config.is_decoder\n        self.add_cross_attention = config.add_cross_attention\n        if self.add_cross_attention:\n            if not self.is_decoder:\n                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\", arch=arch)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config, arch=arch)\n        \n        if arch == 'vit':\n            self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n            self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        \n        #ltp\n        self.ltp_prune_token = False\n        self.ltp_prune_token_module = LTPPruneToken()\n\n        #concrete dropout\n        self.concrete_weight_regularizer = 1e-6\n        self.concrete_dropout_regularizer = 1e-5\n        self.concrete_calc_loss = False\n        if USE_LTP_ON_CONCRETE:\n            self.concrete_init_min = 0.001\n            self.concrete_init_max = 0.1\n        else:\n            self.concrete_init_min = 0.0\n            self.concrete_init_max = self.concrete_init_min\n        self.concrete_prop_p_logit = nn.Parameter(torch.tensor(0.0, dtype=torch.float32))\n        self.p_logit = nn.Parameter(torch.empty(1).uniform_(self.concrete_init_min, self.concrete_init_max))\n        self.temperature = 0.1\n        self.input_dimensionality = 0\n\n        self.concrete_loss_factor = 1e-3\n\n    def init_p_logits(self):\n        torch.nn.init.uniform_(self.p_logit, self.concrete_init_min, self.concrete_init_max)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        self.input_dimensionality = hidden_states[0].numel() # Number of elements of first item in batch\n\n        if self.arch == 'bert':\n            # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n            self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n            self_attention_outputs = self.attention(\n                hidden_states,\n                attention_mask,\n                head_mask,\n                output_attentions=output_attentions,\n                past_key_value=self_attn_past_key_value,\n            )\n            self.self_attention_outputs = self_attention_outputs\n            attention_output = self_attention_outputs[0]\n\n            # if decoder, the last output is tuple of self-attn cache\n            if self.is_decoder: raise Exception()\n            else: outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n            cross_attn_present_key_value = None\n            if self.is_decoder and encoder_hidden_states is not None: raise Exception()\n\n            layer_output = apply_chunking_to_forward(\n                self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n            )\n            if self.ltp_prune_token:\n                layer_output = self.ltp_prune_token_module(layer_output, self_attention_outputs[-1], attention_mask)\n            self.layer_output = layer_output\n            outputs = (layer_output,) + outputs\n\n            # if decoder, return the attn key/values as the last output\n            if self.is_decoder: raise Exception()\n\n            return outputs\n        elif self.arch == 'vit':\n            self_attention_outputs = self.attention(\n                hidden_states=self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n            )\n            attention_output = self_attention_outputs[0]\n            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n            # first residual connection\n            hidden_states = attention_output + hidden_states\n\n            # in ViT, layernorm is also applied after self-attention\n            layer_output = self.layernorm_after(hidden_states)\n            layer_output = self.intermediate(layer_output)\n\n            # second residual connection is done here\n            layer_output = self.output(layer_output, hidden_states)\n\n            outputs = (layer_output,) + outputs\n\n            return outputs\n        else:\n            raise Exception()\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n    \n    def loss_concrete(self, input_dict):\n        if self.concrete_calc_loss:\n            if USE_LTP_ON_CONCRETE:\n                loss = torch.mean(torch.mean(self.output.dense.concrete_mask.squeeze(-1), dim=-1) / torch.mean(input_dict['attention_mask'].squeeze(-1) * 1.0, dim = -1)) * 1e-1\n            else:\n                # p = torch.sigmoid(self.p_logit)\n\n                # sum_of_square = 0\n                # for param in self.parameters():\n                #     sum_of_square += torch.sum(torch.pow(param, 2))\n                \n                # weights_regularizer = self.concrete_weight_regularizer * sum_of_square / (1 - p + EPS)\n                \n                # dropout_regularizer = p * torch.log(p + EPS) + (1. - p) * torch.log(1. - p + EPS)\n                # dropout_regularizer *= self.concrete_dropout_regularizer * self.input_dimensionality\n                \n                # loss = weights_regularizer + dropout_regularizer\n                loss = ((self.p_logit - self.concrete_init_min) ** 2) * self.concrete_loss_factor\n                #loss = (torch.sigmoid(self.p_logit) ** 2) * 1e-6\n                #raise_if_nan(loss)\n                #loss = 0\n            return loss\n        else:\n            return 0\n        \n\n",
    "description": null,
    "url": null
}