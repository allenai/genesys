{
    "acronym": "nbce",
    "title": "Naive Bayes-based Context Extension for Large Language Models",
    "seed_ids": [
        "gpt2"
    ],
    "s2id": "c79fe572b44b0ad904bd30bdfd78d5d3c591e342",
    "abstract": "Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM\u2019s maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes\u2019 theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master",
    "authors": [
        "Jianlin Su",
        "Murtadha Ahmed",
        "Wenbo Luo",
        "Mingren Ao",
        "Zhun Liu",
        "Sumithra Bhakthavatsalam",
        "Daniel Khashabi",
        "Tushar Khot",
        "Bhavana Dalvi",
        "Kyle Mishra",
        "Richardson Ashish",
        "Carissa Sabharwal",
        "Oyvind Schoenick",
        "Tafjord Peter Clark",
        "Tom B. Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "T. Henighan",
        "R. Child",
        "Aditya Ramesh",
        "Daniel M. Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Ma-teusz Litwin",
        "S. Gray",
        "B. Chess",
        "J. Clark",
        "Christopher Berner",
        "Sam McCandlish",
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "J. Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "A. Roberts",
        "Hyung Paul Barham",
        "Won Chung",
        "Charles Sutton",
        "Sebastian Gehrmann",
        "Parker Schuh",
        "Kensen Shi",
        "Sasha Tsvyashchenko",
        "Joshua Maynez",
        "Abhishek Rao",
        "Parker Barnes",
        "Yi Tay",
        "Noam M. Shazeer",
        "Vinodkumar Prabhakaran",
        "Emily Reif",
        "Nan Du",
        "Ben Hutchinson",
        "Reiner Pope",
        "James Bradbury",
        "Jacob Austin",
        "M. Isard",
        "Guy Gur-Ari",
        "Pengcheng Yin",
        "Toju Duke",
        "Anselm Levskaya",
        "Sanjay Ghemawat",
        "Sunipa Dev",
        "H. Michalewski",
        "Xavier Garcia",
        "Vedant Misra",
        "Kevin Robinson",
        "L. Fedus",
        "Denny Zhou",
        "Daphne Ippolito",
        "D. Luan",
        "Hyeontaek Lim",
        "Barret Zoph",
        "A. Spiridonov",
        "Ryan Sepassi",
        "David Dohan",
        "Shivani Agrawal",
        "Mark Omernick",
        "Andrew M. Dai",
        "Thanumalayan Sankaranarayana",
        "Marie Pellat",
        "Aitor Lewkowycz",
        "Erica Moreira",
        "Oleksandr Polozov",
        "Katherine Lee",
        "Zongwei Zhou",
        "Xuezhi Wang",
        "Brennan Saeta",
        "Mark D\u00edaz",
        "Orhan Firat",
        "M. Catasta",
        "Jason Wei",
        "K. Meier-Hellstern"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "tldr": "A novel framework, called Naive Bayes-based Context Extension (NBCE), is introduced to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size, all the while preserving linear efficiency.",
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": "import logging\nfrom typing import List, Tuple, Optional, Dict\n\nimport numpy as np\nimport torch\nfrom transformers import PreTrainedTokenizerBase, PreTrainedModel\n\nfrom utils import n_tokens_in_prompt\nfrom transformers import TopPLogitsWarper, LogitsProcessorList\nfrom numpy import typing as npt\nfrom transformers import LogitsProcessor\n\nLOGIT_BIAS = 100\n\n\nclass RestrictiveTokensLogitsProcessor(LogitsProcessor):\n    \"\"\" Restrictive decoding is done by adding logits_bias to the relevant tokens. Based on:\n    https://help.openai.com/en/articles/5247780-using-logit-bias-to-define-token-probability\n    \"\"\"\n\n    def __init__(self,\n                 restrictive_token_ids: npt.NDArray[int],\n                 eos_token_id: int,\n                 prompt_length_to_skip: int = 0,\n                 logits_bias: int = LOGIT_BIAS):\n        self.restrictive_token_ids = restrictive_token_ids\n        self.eos_token_id = eos_token_id\n        self.logits_bias = logits_bias\n        self.prompt_length_to_skip = prompt_length_to_skip\n        self.mask = np.ones(restrictive_token_ids.shape[0], dtype=bool)\n\n        self._preprocess_restrictive_array()\n\n    def _preprocess_restrictive_array(self):\n        # extend restrictive_token_ids to include eos as last token for each sequence\n        if not (self.restrictive_token_ids[:, -1] == self.eos_token_id).all():\n            self.restrictive_token_ids = np.column_stack(\n                (self.restrictive_token_ids, np.ones(self.restrictive_token_ids.shape[0]) * self.eos_token_id)). \\\n                astype(int)\n\n    def update_new_prompt_length_to_skip(self, prompt_length_to_skip: int):\n        self.prompt_length_to_skip = prompt_length_to_skip\n        self.mask = np.ones(self.restrictive_token_ids.shape[0], dtype=bool)\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        # assert input_ids.shape[0] == 1, \"This implementation doesn't support batching\"\n        new_tokens_length = input_ids.shape[-1] - self.prompt_length_to_skip\n        if new_tokens_length > 0:\n            self.mask = self.mask & (self.restrictive_token_ids[:, new_tokens_length - 1] == input_ids[\n                0, -1].item())\n\n\n        # scores[:, self.restrictive_token_ids[self.mask, new_tokens_length]] += self.logits_bias\n        scores[:,:, self.restrictive_token_ids[self.mask, new_tokens_length]] += self.logits_bias\n        # print(scores.shape, self.restrictive_token_ids[self.mask, new_tokens_length].shape)\n        # scores[:,self.restrictive_token_ids[self.mask, new_tokens_length]:,:] += self.logits_bias\n\n        return scores\n\n\nprocessors = LogitsProcessorList()\nprocessors.append(TopPLogitsWarper(0.95))\n\n\n\ndef combine_past_key_values(past_key_value):\n    present = ()\n    for layer_past in zip(*past_key_value):\n        key, value = tuple(zip(*layer_past))\n        key = torch.cat(key, dim=0)\n        value = torch.cat(value, dim=0)\n        present += ((key, value), )\n    return present\n\ndef generate_NBCE_position_ids(attention_mask: torch.Tensor, max_window_size: int,\n                              past_key_values: Tuple[Tuple[torch.Tensor]],\n                              sum_windows_size: int, windows_key_values: Tuple[Tuple[torch.Tensor]]) -> torch.Tensor:\n    position_ids = attention_mask.long().cumsum(-1) - 1\n    n_task_tokens = position_ids.shape[1] - sum_windows_size\n    position_ids[0, -n_task_tokens:] = torch.arange(max_window_size, max_window_size + n_task_tokens, 1)\n    position_ids.masked_fill_(attention_mask == 0, 1)\n    if past_key_values:  # i.e., first token is already generated\n        position_ids = position_ids[:, -1].unsqueeze(-1)\n    elif windows_key_values:  # i.e., we are in the first token generation\n        position_ids = position_ids[:, sum_windows_size:]\n    return position_ids\n\n\nclass NBCEModelWrapper:\n    def __init__(self,\n                 model: PreTrainedModel,\n                 tokenizer: PreTrainedTokenizerBase,\n                 task: str,\n                 device: str,\n                 context_window_size: int,\n                 right_indentation: bool = False,\n                 beta: float = None\n                 ):\n        self.model = model\n        self.task = task\n        self.beta = beta\n        self.tokenizer = tokenizer\n        self.context_window_size = context_window_size\n        self.device = device\n        self.right_indentation = right_indentation\n    def get_chunks(self, texts, max_length):\n\n        demo_encoding = []\n\n        max_length = self.context_window_size -(max_length*2)\n        demo_encoding_batch = [[self.tokenizer.pad_token_id] * max_length]\n        attention_mask_batch = [[0] * max_length]\n        for text in texts:\n            demo_input_ids = self.tokenizer(text, add_special_tokens=False).input_ids\n            # if len(demo_input_ids) < 30: continue\n            if len(demo_encoding) + len(demo_input_ids) <= max_length:\n                demo_encoding += demo_input_ids\n            else:\n                demo_encoding_batch.append((demo_encoding + demo_input_ids)[-max_length:])\n                attention_mask_batch.append([1] * max_length)\n                demo_encoding = []\n            if len(demo_encoding_batch) >= len(texts)-1:break\n\n\n        if len(demo_encoding_batch) == 0:  # doesn't need chunk!\n            demo_encoding_batch.append(demo_encoding)\n            attention_mask_batch.append([1] * len(demo_encoding))\n\n        return demo_encoding_batch, attention_mask_batch\n\n\n    def _get_windows(self, texts: List[str], max_length:int) -> List[Dict]:\n        windows = []\n        if self.right_indentation:\n            max_window_size = max(n_tokens_in_prompt(self.tokenizer, t, add_special_tokens=True) for t in texts)\n\n\n        demo_encoding_batch , attention_mask_batch = self.get_chunks(texts, max_length=max_length)\n\n        demo_encoding_batch = torch.LongTensor(demo_encoding_batch).to(self.device)\n        attention_mask_batch = torch.LongTensor(attention_mask_batch).to(self.device)\n        for demo_encoding, attention_mask in zip(demo_encoding_batch, attention_mask_batch):\n            window_size = len(demo_encoding)\n            with torch.no_grad():\n\n                output = self.model(\n                    input_ids = demo_encoding.unsqueeze(0),\n                    attention_mask = attention_mask.unsqueeze(0),\n                    use_cache = True\n                )\n\n            windows.append({'text': self.tokenizer.decode(demo_encoding, skip_special_tokens=True),\n                            'encoded_input': demo_encoding.unsqueeze(0),\n                            'attention_mask': attention_mask.unsqueeze(0),\n                            'window_size': window_size,\n                            'output': output,\n                            'past': output['past_key_values']})\n        return windows\n\n    def get_contexts_cache(self, contexts: List[str], max_length:int) -> Dict:\n        windows = self._get_windows(contexts, max_length=max_length)\n        windows_sizes = [window['window_size'] for window in windows]\n        j = np.argmax(windows_sizes)\n       \n        past_attention_mask=torch.cat([window['attention_mask'] for window in windows],  dim=0)\n\n        return {'past_key_values': combine_past_key_values([window['past'] for window in windows]),\n                'max_window_size': max(windows_sizes),\n                'past_attention_mask':past_attention_mask,\n\n                'sum_windows_size': sum(windows_sizes) - (len(windows) - 1)}\n\n\n    def nbce_generate(self,\n                     contexts: Optional[List[str]] = None,\n                     task_text: Optional[str] = None,\n                     answer: Optional[str] = None,\n                     contexts_cache: Optional[Dict] = None,\n                     restrictive_logit_preprocessor: Optional[RestrictiveTokensLogitsProcessor] = None,\n                     **kwargs\n                     ) -> str:\n        assert (contexts is None) != (contexts_cache is None), \"nbce_generate should work with contexts or cache, not with both!\"\n        cache = contexts_cache or self.get_contexts_cache(contexts)\n        encoded_task_text = self.tokenizer(task_text, add_special_tokens=False, return_tensors='pt').to(self.device)\n        if restrictive_logit_preprocessor:\n            restrictive_logit_preprocessor.update_new_prompt_length_to_skip(encoded_task_text['input_ids'].shape[1])\n            kwargs['logits_processor'] = [restrictive_logit_preprocessor]\n        n=cache['past_attention_mask'].shape[0]\n        logits_processor = LogitsProcessorList([\n            restrictive_logit_preprocessor,\n        ])\n\n        attention_mask = torch.cat([cache['past_attention_mask'], encoded_task_text['attention_mask'].tile(n, 1)],\n                                   dim=-1).to(self.device)\n\n        input_ids = encoded_task_text['input_ids'].tile(n, 1)\n        past_key_values=cache['past_key_values']\n       \n        ids=input_ids[0].unsqueeze(0)\n        if self.task in ['banking77', 'clinic150', 'nlu', 'nluscenario', 'trecfine']:\n            res=''\n            for j in range(kwargs['max_new_tokens']) :\n                with torch.no_grad():\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        past_key_values=past_key_values,\n                    )\n                    past_key_values= outputs.past_key_values\n                    logits = logits_processor(ids, outputs.logits)[:, -1]\n\n\n                    beta, eta = self.beta, 0.1\n                    logits = logits - logits.logsumexp(dim=-1, keepdims=True)\n                   \n                    entropy = -(logits.exp() * logits.clip(-100, 0)).sum(dim=-1)\n                    k = entropy[1:].argmin() + 1\n                    logits_max = logits[k]\n                    # logits_max = logits[1:].mean(0).clip(-100, 0) #mean Eq.7\n\n                    logits_uncond = logits[0]\n                    logits_merged = (1 + beta) * logits_max - beta * logits_uncond\n                    logits = torch.where(logits_uncond > -100, logits_merged, logits_max)\n                    \n                    tau = 0.01\n                    probas = torch.nn.functional.softmax(logits[None] / tau, dim=-1)\n                    next_tokens = torch.multinomial(probas, num_samples=1).squeeze(1)\n\n                    if next_tokens[0]== self.tokenizer.eos_token_id:\n                        break\n\n                    ret = self.tokenizer.batch_decode(next_tokens)\n                    \n                    input_ids = next_tokens.unsqueeze(-1).tile(n, 1)\n                    ids = torch.cat([ids, input_ids[0].unsqueeze(0)], dim=-1)\n                    attention_mask = torch.cat([attention_mask, torch.ones(n, 1, dtype=torch.long, device=self.device)],\n                                               dim=-1)\n                    res += '' + ret[0]\n        elif self.task in ['piqa', 'hellaswag' ,'obqa', 'copa', 'arce']:\n\n            answer_encoding = self.tokenizer(\n                answer,\n                padding=True,\n                return_tensors='pt',\n                add_special_tokens=False\n            ).to(self.device)\n            res = torch.empty(0).to(self.device)\n            for candidate_encoding, candidate_mask in zip(answer_encoding.input_ids,\n                                                          answer_encoding.attention_mask):\n                candidate_encoding = candidate_encoding[torch.where(candidate_mask)].unsqueeze(0)\n                multi_encoding = torch.cat((input_ids, candidate_encoding.tile(n, 1)), dim=-1)\n                multi_attention_mask = torch.cat((cache['past_attention_mask'], torch.ones(multi_encoding.shape, device=self.device)),\n                                                 dim=-1)\n              \n                with torch.no_grad():\n                        outputs = self.model(\n                            input_ids=multi_encoding,\n                            attention_mask=multi_attention_mask,\n                            past_key_values=past_key_values\n                    ).logits\n\n\n                beta, eta = self.beta, 0.1\n                logits = outputs[:, (input_ids.shape[1] - 1): -1]\n                logits = logits - logits.logsumexp(dim=-1, keepdims=True)\n                entropy = -(logits.exp() * logits.clip(-100, 0)).sum(dim=-1)\n                k = entropy[:, 0].argmin()\n                logits_max = logits[k]\n                # logits_max = logits[1:].mean(0).clip(-100, 0) #mean Eq.7\n                logits_uncond = logits[0]\n                logits_merged = (1 + beta) * logits_max - beta * logits_uncond\n                logits = torch.where(logits_uncond > -100, logits_merged, logits_max)\n\n                logits = logits[torch.arange(logits.shape[0]).to(self.device), candidate_encoding.flatten()].mean()\n                res = torch.cat((res, logits.unsqueeze(0)), dim=0)\n            res= res.argmax(dim=-1).item()\n        else:\n            attention_mask = torch.cat([cache['past_attention_mask'], encoded_task_text['attention_mask'].tile(n, 1)],\n                                       dim=-1).to(self.device)\n\n        \n\n            with torch.no_grad():\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    past_key_values=past_key_values\n                ).logits\n\n            beta, eta = self.beta, 0.1\n            \n            logits = logits_processor(ids, outputs)[:, -1]\n\n            logits = logits - logits.logsumexp(dim=-1, keepdims=True)\n            entropy = -(logits.exp() * logits.clip(-100, 0)).sum(dim=-1)\n           \n            k = entropy[1:].argmin() + 1\n            logits_max = logits[k]\n            # logits_max = logits[1:].mean(0).clip(-100, 0) #mean Eq.7\n\n            logits_uncond = logits[0]\n            logits_merged = (1 + beta) * logits_max - beta * logits_uncond\n\n            logits = torch.where(logits_uncond > -100, logits_merged, logits_max)\n           \n            tau = 0.01\n            probas = torch.nn.functional.softmax(logits[None] / tau, dim=-1)\n\n            next_tokens = torch.multinomial(probas, num_samples=1).squeeze(1)\n            res = self.tokenizer.batch_decode(next_tokens)[0]\n\n        return res",
    "description": null,
    "url": null
}