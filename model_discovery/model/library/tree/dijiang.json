{
    "acronym": "dijiang",
    "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
    "seed_ids": [
        "streamingllm",
        "retnet",
        "eva",
        "cosformer",
        "soft",
        "fnet",
        "rfa",
        "performer",
        "longformer",
        "reformer",
        "transformer"
    ],
    "s2id": "ef8846c0b9eb9e915d44e18bf06fda51f9b5e2fa",
    "abstract": "In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various benchmark while requires only about 1/50 training cost. Code is available at https://github.com/YuchuanTian/DiJiang.",
    "authors": [
        "Hanting Chen",
        "Zhicheng Liu",
        "Xutao Wang",
        "Yuchuan Tian",
        "Yunhe Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "DiJiang is presented, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs by employing a weighted Quasi-Monte Carlo method for sampling.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "# coding=utf-8\n# Copyright 2022 EleutherAI The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Tuple, Union\nimport math\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\n\nfrom transformers.activations import ACT2FN\nfrom transformers.utils import logging\n\nimport numpy as np\nfrom scipy.fft import dct\n\nlogger = logging.get_logger(__name__)\n\n\nclass GPTNeoXAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.num_attention_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        if self.hidden_size % self.num_attention_heads != 0:\n            raise ValueError(\n                \"The hidden size is not divisble by the number of attention heads! Make sure to update them\"\n            )\n        self.head_size = self.hidden_size // self.num_attention_heads\n        self.rotary_ndims = int(self.head_size * config.rotary_pct)\n        self._init_bias(config.max_position_embeddings)\n\n        self.register_buffer(\"masked_bias\", torch.tensor(-1e9), persistent=False)\n        self._init_rope()\n\n        self.norm_factor = self.head_size**-0.5\n        self.query_key_value = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.attention_dropout = nn.Dropout(config.attention_dropout)\n        self.proj_matrix = self._build_projection()\n        self.v_dim = config.hidden_size\n        self.W_G = nn.Parameter(torch.randn(self.hidden_size, self.v_dim) / self.hidden_size)\n        self.swish = nn.SiLU()\n        self.group_norm = nn.GroupNorm(self.head_size, self.v_dim)\n        nn.init.xavier_uniform_(self.W_G.data, gain=2 ** -2.5)\n        self.D1 = self._get_D1(self.config.max_position_embeddings)\n        self.D2 = self._get_D2(self.config.max_position_embeddings)\n        self.mask = self._get_mask(self.config.max_position_embeddings).unsqueeze(0)\n\n\n    def _init_bias(self, max_positions, device=None):\n        self.register_buffer(\n            \"bias\",\n            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n                1, 1, max_positions, max_positions\n            ),\n            persistent=False,\n        )\n        if device is not None:\n            self.bias = self.bias.to(device)\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = GPTNeoXRotaryEmbedding(\n                self.rotary_ndims, self.config.max_position_embeddings, base=self.config.rotary_emb_base\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = GPTNeoXLinearScalingRotaryEmbedding(\n                    self.rotary_ndims,\n                    self.config.max_position_embeddings,\n                    base=self.config.rotary_emb_base,\n                    scaling_factor=scaling_factor,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = GPTNeoXDynamicNTKScalingRotaryEmbedding(\n                    self.rotary_ndims,\n                    self.config.max_position_embeddings,\n                    base=self.config.rotary_emb_base,\n                    scaling_factor=scaling_factor,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n                \n    def _get_D1(self, sequence_length):\n        D = ((1 - torch.exp(torch.linspace(math.log(1/32), math.log(1/512), self.num_attention_heads))).view(self.num_attention_heads, 1, 1) ** (torch.arange(sequence_length).unsqueeze(1))).float().unsqueeze(0)\n\n        return nn.Parameter(D)\n    \n    def _get_D2(self, sequence_length):\n        D = 1/((1 - torch.exp(torch.linspace(math.log(1/32), math.log(1/512), self.num_attention_heads))).view(self.num_attention_heads, 1, 1) ** (torch.arange(sequence_length).unsqueeze(1))).float().unsqueeze(0)\n\n        return nn.Parameter(D)\n\n    def _get_mask(self, sequence_length):\n        n = torch.arange(sequence_length).unsqueeze(1)\n        m = torch.arange(sequence_length).unsqueeze(0)\n\n        M = torch.ones(self.num_attention_heads).view(self.num_attention_heads, 1, 1)*(n >= m).float() \n\n        return M\n\n    def _build_projection(self):\n        icdf_w = torch.distributions.Normal(0, 1).icdf(torch.diag_embed(torch.diag(torch.rand(self.head_size, self.head_size))))\n        icdf_w = torch.where(torch.isinf(icdf_w), torch.full_like(icdf_w, 0), icdf_w)\n        C = dct(np.eye(self.head_size, self.head_size), axis=0,norm='ortho')\n        C = torch.from_numpy(C).type(torch.FloatTensor)\n        return nn.Parameter((C @ icdf_w).contiguous(), requires_grad=False)\n\n    def forward(\n        self,\n        hidden_states: torch.FloatTensor,\n        attention_mask: torch.FloatTensor,\n        position_ids: torch.LongTensor,\n        head_mask: Optional[torch.FloatTensor] = None,\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ):\n        has_layer_past = layer_past is not None\n\n        # Compute QKV\n        # Attention heads [batch, seq_len, hidden_size]\n        #   --> [batch, seq_len, (np * 3 * head_size)]\n        #print(self.query_key_value.weight.dtype)\n        #assert False\n        qkv = self.query_key_value(hidden_states)\n\n        # [batch, seq_len, (num_heads * 3 * head_size)]\n        #   --> [batch, seq_len, num_heads, 3 * head_size]\n        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n        qkv = qkv.view(*new_qkv_shape)\n\n        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n\n        # Compute rotary embeddings on rotary_ndims\n        query_rot = query[..., : self.rotary_ndims]\n        query_pass = query[..., self.rotary_ndims :]\n        key_rot = key[..., : self.rotary_ndims]\n        key_pass = key[..., self.rotary_ndims :]\n\n        # Compute token offset for rotary embeddings (when decoding)\n        seq_len = key.shape[-2]\n        if has_layer_past:\n            seq_len += layer_past[0].shape[-2]\n        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n        query = torch.cat((query, query_pass), dim=-1)\n        key = torch.cat((key, key_pass), dim=-1)\n\n        # Cache QKV values\n        if has_layer_past:\n            past_key = layer_past[0]\n            past_value = layer_past[1]\n            key = torch.cat((past_key, key), dim=-2)\n            value = torch.cat((past_value, value), dim=-2)\n        present = (key, value) if use_cache else None\n\n        # Compute attention\n        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        # Reshape outputs\n        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n        attn_output = self.group_norm(attn_output.reshape(-1, self.v_dim)).reshape(attn_output.shape)\n        attn_output = self.dense(self.swish(hidden_states @ self.W_G) * attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n    @classmethod\n    def _split_heads(cls, tensor, num_attention_heads, attn_head_size):\n        \"\"\"\n        Splits hidden dim into attn_head_size and num_attention_heads\n        \"\"\"\n        # tensor: [bs, seq_len, hidden_size]\n        new_shape = tensor.size()[:-1] + (num_attention_heads, attn_head_size)\n        # -> [bs, seq_len, num_attention_heads, attn_head_size]\n        tensor = tensor.view(new_shape)\n        # -> [bs, num_attention_heads, seq_len, attn_head_size]\n        tensor = tensor.permute(0, 2, 1, 3)\n        return tensor\n\n    @classmethod\n    def _merge_heads(cls, tensor, num_attention_heads, attn_head_size):\n        \"\"\"\n        Merges attn_head_size dim and num_attn_heads dim into hidden dim\n        \"\"\"\n        # tensor [bs, num_attention_heads, seq_len, attn_head_size]\n        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n        # -> [bs, seq_len, num_attention_heads, attn_head_size]\n        tensor = tensor.view(tensor.size(0), tensor.size(1), num_attention_heads * attn_head_size)\n        # -> [bs, seq_len, hidden_size]\n        return tensor\n\n    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n        # compute causal mask from causal mask buffer\n        batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n        key_length = key.size(-2)\n\n        # dynamically increase the causal mask with the key length, if needed.\n        if key_length > self.bias.shape[-1]:\n            self._init_bias(key_length, device=key.device)\n        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n        \n        query = nn.functional.softmax(query@self.proj_matrix, dim=-1)\n        key =  nn.functional.softmax(key@self.proj_matrix, dim=-1)\n        query = query*self.D1[:,:,:query_length,:]\n        key = key*self.D2[:,:,:key_length,:]\n\n        attn_scores = torch.matmul(query, key.transpose(2, 3))\n\n        attn_scores = attn_scores * self.mask[:,:,:query_length,:key_length].to(attn_scores.device,dtype=attn_scores.dtype)\n\n        attn_weights = attn_scores.to(value.dtype)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attn_weights = attn_weights * head_mask\n\n        attn_weights = self.attention_dropout(attn_weights)\n\n        attn_output = torch.matmul(attn_weights, value)\n        \n        return attn_output, attn_weights\n\n\ndef attention_mask_func(attention_scores, ltor_mask):\n    attention_scores.masked_fill_(~ltor_mask, torch.finfo(attention_scores.dtype).min)\n    return attention_scores\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with LlamaRotary->GPTNeoXRotary\nclass GPTNeoXRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        # Build here to make `torch.jit.trace` work.\n        self._set_cos_sin_cache(\n            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n        )\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        if seq_len > self.max_seq_len_cached:\n            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n\n        return (\n            self.cos_cached[:seq_len].to(dtype=x.dtype),\n            self.sin_cached[:seq_len].to(dtype=x.dtype),\n        )\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->GPTNeoX\nclass GPTNeoXLinearScalingRotaryEmbedding(GPTNeoXRotaryEmbedding):\n    \"\"\"GPTNeoXRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n        t = t / self.scaling_factor\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->GPTNeoX\nclass GPTNeoXDynamicNTKScalingRotaryEmbedding(GPTNeoXRotaryEmbedding):\n    \"\"\"GPTNeoXRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\n# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`):\n            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n            used to pass offsetted position ids when working with a KV-cache.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass GPTNeoXMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense_h_to_4h = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.dense_4h_to_h = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.act = ACT2FN[config.hidden_act]\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense_h_to_4h(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.dense_4h_to_h(hidden_states)\n        return hidden_states\n\n\nclass GPTNeoXLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.use_parallel_residual = config.use_parallel_residual\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_attention_dropout = nn.Dropout(config.hidden_dropout)\n        self.post_mlp_dropout = nn.Dropout(config.hidden_dropout)\n        self.attention = GPTNeoXAttention(config)\n        self.mlp = GPTNeoXMLP(config)\n\n    def forward(\n        self,\n        hidden_states: Optional[torch.FloatTensor],\n        attention_mask: Optional[torch.FloatTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = False,\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n    ):\n        attention_layer_outputs = self.attention(\n            self.input_layernorm(hidden_states),\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            layer_past=layer_past,\n            head_mask=head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n        )\n        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n        attn_output = self.post_attention_dropout(attn_output)\n        outputs = attention_layer_outputs[1:]\n\n        if self.use_parallel_residual:\n            # pseudocode:\n            # x = x + attn(ln1(x)) + mlp(ln2(x))\n            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\n            mlp_output = self.post_mlp_dropout(mlp_output)\n            hidden_states = mlp_output + attn_output + hidden_states\n        else:\n            # pseudocode:\n            # x = x + attn(ln1(x))\n            # x = x + mlp(ln2(x))\n            attn_output = attn_output + hidden_states\n            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\n            mlp_output = self.post_mlp_dropout(mlp_output)\n            hidden_states = mlp_output + attn_output\n\n        if use_cache:\n            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\n        else:\n            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\n\n        return outputs\n\n",
    "description": null,
    "url": null
}