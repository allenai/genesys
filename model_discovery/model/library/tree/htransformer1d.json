{
    "acronym": "htransformer1d",
    "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences",
    "seed_ids": [
        "bigbird",
        "linformer",
        "synthesizer",
        "sinkhorn",
        "transformerxl",
        "transformer",
        "bert"
    ],
    "s2id": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
    "abstract": "We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.",
    "authors": [
        "Zhenhai Zhu",
        "Radu Soricut"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": "This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.",
    "citationCount": 32,
    "influentialCitationCount": 7,
    "code": "from math import log2, ceil\nfrom functools import wraps\n\nimport torch\nfrom torch import nn, einsum, diagonal\nimport torch.nn.functional as F\n\nfrom h_transformer_1d.reversible import ReversibleSequence, SequentialSequence\nfrom rotary_embedding_torch import apply_rotary_emb, RotaryEmbedding\nfrom einops import rearrange, reduce, repeat\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef masked_aggregate(tensor, mask = None, dim = -1, average = True):\n    if not exists(mask):\n        fn = torch.sum if not average else torch.mean\n        return fn(tensor, dim = dim)\n\n    diff_len = len(tensor.shape) - len(mask.shape)\n    mask = mask[(..., *((None,) * diff_len))]\n    tensor = tensor.masked_fill(~mask, 0.)\n\n    total_el = mask.sum(dim = dim)\n    agg = tensor.sum(dim = dim)\n\n    if average:\n        agg = agg / total_el.clamp(min = 1.)\n\n    agg.masked_fill_(total_el == 0, 0.)\n    return agg\n\ndef shift(t, amount, mask = None):\n    if amount == 0:\n        return t\n\n    if exists(mask):\n        t = t.masked_fill(~mask[..., None], 0.)\n\n    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n\n# helper classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        mult = 4\n    ):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult),\n            nn.GELU(),\n            nn.Linear(dim * mult, dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# token shifting\n\nclass PreShiftTokens(nn.Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n\n    def forward(self, x, **kwargs):\n        mask = kwargs.get('mask', None)\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments\n        splitted = x.split(feats_per_shift, dim = -1)\n        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n        segments_to_shift = list(map(lambda args: shift(*args, mask = mask), zip(segments_to_shift, shifts)))\n        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n        return self.fn(x, **kwargs)\n\n# hierarchical attention helper functions\n\ndef cast_for_op(cast_type, fn):\n    @wraps(fn)\n    def inner(t, *args, **kwargs):\n        orig_type = t.dtype\n        t = t.type(cast_type)\n        out = fn(t, *args, **kwargs)\n        out = out.type(orig_type)\n        return out\n    return inner\n\ndef flip_every_two(t):\n    t = rearrange(t, 'b (n r) ... -> b n r ...', r = 2)\n    t = torch.flip(t, dims = (2,))                          # so we pay attention to the off-diagonal blocks in the attention matrix\n    t = rearrange(t, 'b n r ... -> b (n r) ...')\n    return t\n\n# attention\n\nclass HAttention1D(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        heads = 8,\n        dim_head = 64,\n        block_size = 16,\n        pos_emb = None,\n        eps = 1e-8,\n        **kwargs\n    ):\n        super().__init__()\n        self.eps = eps\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.block_size = block_size\n        inner_dim = heads * dim_head\n\n        self.pos_emb = pos_emb\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n    def forward(self, x, mask = None):\n        b, n, h, device, bsz, eps = *x.shape[:2], self.heads, x.device, self.block_size, self.eps\n\n        # pad sequence length to power of 2\n\n        pad_to_len = 2 ** ceil(log2(n))\n        padding = pad_to_len - n\n\n        if padding != 0:\n            x = F.pad(x, (0, 0, 0, padding), value = 0.)\n            if exists(mask):\n                mask = F.pad(mask, (0, padding), value = False)\n\n        # derive queries, keys, values\n\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n\n        # split out heads, and also divide sequence into blocks\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n\n        if exists(mask):\n            mask = repeat(mask, 'b n -> (b h) n', h = h)\n\n        # scale\n\n        q = q * self.scale\n\n        # rotary pos emb\n\n        if exists(self.pos_emb):\n            freqs = self.pos_emb(torch.arange(pad_to_len, device = device))\n            freqs = rearrange(freqs, 'n d -> () n d')\n            q, k, v = map(lambda t: apply_rotary_emb(freqs, t), (q, k, v))\n\n        # calculate number of levels until 2 x 2\n\n        num_levels = int(log2(pad_to_len // bsz)) - 2\n        assert num_levels >= 0, 'number of levels must be at least greater than 0'\n\n        # coarsening\n\n        qkvs = [(q, k, v, mask)]\n\n        for level in range(num_levels):\n            q, k, v = map(lambda t: rearrange(t, 'b (n r) d -> b n r d', r = 2), (q, k, v))\n\n            if exists(mask):\n                mask = repeat(mask, 'b (n r) -> b n r', r = 2)\n\n            # masked mean for queries and keys, but not values\n\n            q = masked_aggregate(q, mask, dim = 2)\n            k = masked_aggregate(k, mask, dim = 2)\n            v = masked_aggregate(v, mask, dim = 2, average = False)\n\n            if exists(mask):\n                mask = torch.any(mask, dim = 2)\n\n            coarsened_qkvs = (q, k, v, mask)\n            qkvs.append(coarsened_qkvs)\n\n        qkvs = [qkvs[0], *qkvs]  # duplicate the finest resolution an extra time, for the base diagonal\n\n        # half-attention function\n\n        def calculate_Y_and_A(q, k, v, mask = None):\n            S = einsum('... i d, ... j d -> ... i j', q, k)\n\n            if exists(mask):\n                mask_value = -torch.finfo(S.dtype).max\n                S = S.masked_fill(~mask, mask_value)\n\n            S = S - torch.max(S, dim = -1, keepdim = True).values\n            A = S.exp()\n\n            y = einsum('... i j, ... j d -> ... i d', A, v)\n\n            A = A.sum(dim = -1)\n\n            y = rearrange(y, 'b ... n d -> b (... n) d')\n            A = rearrange(A, 'b ... i -> b (... i)')\n            return y, A\n\n        to_blocks = lambda t: rearrange(t, 'b (n z) ... -> b n z ...', z = bsz)\n\n        # calculate Ys, as in the paper\n\n        Ys = []\n\n        for ind, (q, k, v, mask) in enumerate(reversed(qkvs)):\n            is_last = ind == (len(qkvs) - 1)\n\n            q, k, v = map(to_blocks, (q, k, v))\n\n            # generate the mask for S\n\n            S_mask = None\n            if exists(mask):\n                mask = to_blocks(mask)\n                q_mask = mask\n                k_mask = cast_for_op(torch.int, flip_every_two)(mask) if not is_last else mask\n                S_mask = rearrange(q_mask, '... n -> ... n ()') * rearrange(k_mask, '... n -> ... () n')\n\n            # flip keys and values to capture the off-diagonals\n\n            if not is_last:\n                k, v = map(flip_every_two, (k, v))\n\n            Y_level = calculate_Y_and_A(q, k, v, mask = S_mask)\n            Ys.append(Y_level)\n\n        # interpolate\n\n        Y = 0\n        A = 0\n\n        for ind, (Y_level, A_level) in enumerate(Ys):\n            is_last = ind == (len(Ys) - 1)\n\n            if not is_last and torch.is_tensor(Y):\n                Y = repeat(Y, 'b n d -> b (n r) d', r = 2)\n\n            if not is_last and torch.is_tensor(A):\n                A = repeat(A, 'b n -> b (n r)', r = 2)\n\n            Y = Y_level + Y\n            A = A_level + A\n\n        out = Y / rearrange(A + eps, 'b n -> b n ()')\n\n        # merge heads\n\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n\n        # combine out\n\n        return self.to_out(out[:, :n])\n\n# causal attention\n\nclass CausalHAttention1D(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        max_seq_len,\n        heads = 8,\n        dim_head = 64,\n        block_size = 16,\n        eps = 1e-8,\n        pos_emb = None\n    ):\n        super().__init__()\n        self.eps = eps\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.block_size = block_size\n        inner_dim = heads * dim_head\n\n        self.pos_emb = pos_emb\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n        # derive mask\n\n        num_levels = int(log2(max_seq_len // block_size)) - 1\n        root_seq = torch.arange(max_seq_len)\n        seqs = [root_seq]\n        seq = root_seq\n\n        for ind in range(num_levels):\n            seq = rearrange(seq, '(n r) -> n r', r = 2)\n            seq = seq.max(dim = -1).values\n            expanded_mask_seq = repeat(seq, 'n -> (n r)', r = (2 ** (ind + 1)))\n            seqs.append(expanded_mask_seq)\n\n        seq_keys = torch.stack(seqs, dim = 0)\n        mask = seq_keys > rearrange(root_seq, 'n -> () n')\n        self.register_buffer('mask', mask)\n\n    def forward(self, x, **kwargs):\n        b, n, h, device, bsz, eps = *x.shape[:2], self.heads, x.device, self.block_size, self.eps\n\n        # pad sequence length to power of 2\n\n        pad_to_len = 2 ** ceil(log2(n))\n        padding = pad_to_len - n\n\n        if padding != 0:\n            x = F.pad(x, (0, 0, 0, padding), value = 0.)\n\n        # derive queries, keys, values\n\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n\n        # split out heads, and also divide sequence into blocks\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n\n        # scale\n\n        q = q * self.scale\n\n        # rotary embedding\n\n        if exists(self.pos_emb):\n            freqs = self.pos_emb(torch.arange(pad_to_len, device = device))\n            freqs = rearrange(freqs, 'n d -> () n d')\n            q, k, v = map(lambda t: apply_rotary_emb(freqs, t), (q, k, v))\n\n        # calculate number of levels until 2 x 2\n\n        num_levels = int(log2(pad_to_len // bsz)) - 1\n\n        # coarsening\n\n        qkvs = [(q, k, v)]\n\n        for level in range(num_levels):\n            q, k, v = map(lambda t: rearrange(t, 'b (n r) d -> b n r d', r = 2), (q, k, v))\n\n            # masked mean for queries and keys, but not values\n\n            q = q.mean(dim = 2)\n            k = k.mean(dim = 2)\n            v = v.sum(dim = 2)\n\n            coarsened_qkvs = (q, k, v)\n            qkvs.append(coarsened_qkvs)\n\n        # half-attention function\n\n        def calculate_Y_and_A(q, k, v, mask_right_off_diagonals = False, causal_mask_diagonal = False):\n            if mask_right_off_diagonals:\n                q, k, v = map(lambda t: rearrange(t, 'b (n r) ... -> b n r ...', r = 2), (q, k, v))\n                q, k, v = map(lambda t: t[:, :, 1], (q, k, v))\n\n            S = einsum('... i d, ... j d -> ... i j', q, k)\n\n            if causal_mask_diagonal:\n                causal_mask = torch.ones(*S.shape[-2:], device = S.device).triu(1).bool()\n                mask_value = -torch.finfo(S.dtype).max\n                causal_mask = rearrange(causal_mask, 'i j -> () () i j')\n                S = S.masked_fill(causal_mask, mask_value)\n\n            S = S - torch.amax(S, dim = -1, keepdim = True)\n            A = S.exp()\n\n            y = einsum('... i j, ... j d -> ... i d', A, v)\n\n            A = A.sum(dim = -1)\n\n            if mask_right_off_diagonals:\n                y, A = map(lambda t: rearrange(t, 'b n ... -> b n () ...'), (y, A))\n                y = F.pad(y, (0, 0, 0, 0, 1, 0), value = 0.)\n                A = F.pad(A, (0, 0, 1, 0), value = 0.)\n\n            y = rearrange(y, 'b ... d -> b (...) d')\n            A = rearrange(A, 'b ... -> b (...)')\n            return y, A\n\n        to_blocks = lambda t: rearrange(t, 'b (n z) ... -> b n z ...', z = bsz)\n\n        # calculate Ys, as in the paper\n\n        Ys = []\n\n        for ind, (q, k, v) in enumerate(reversed(qkvs)):\n            is_last = ind == (len(qkvs) - 1)\n\n            q, k, v = map(to_blocks, (q, k, v))\n\n            # flip keys and values to capture the off-diagonals\n\n            if not is_last:\n                k, v = map(flip_every_two, (k, v))\n\n            Y_level = calculate_Y_and_A(q, k, v, mask_right_off_diagonals = not is_last, causal_mask_diagonal = is_last)\n            Ys.append(Y_level)\n\n        # interpolate\n\n        def safe_cat(acc, el, dim = 0):\n            if not exists(acc):\n                return el\n            return torch.cat((el, acc), dim = dim)\n\n        Y = None\n        A = None\n\n        for Y_level, A_level in Ys:\n            Y_level, A_level = map(lambda t: rearrange(t, '... -> () ...'), (Y_level, A_level))\n\n            if torch.is_tensor(Y):\n                Y = repeat(Y, '... n d -> ... (n r) d', r = 2)\n\n            if torch.is_tensor(A):\n                A = repeat(A, '... n -> ... (n r)', r = 2)\n\n            Y = safe_cat(Y, Y_level)\n            A = safe_cat(A, A_level)\n\n        # create causal mask for Y and A\n\n        causal_mask = self.mask[:(num_levels + 1), :pad_to_len]\n\n        # mask and sum\n\n        Y_causal_mask = rearrange(causal_mask, 'h n -> h () n ()')\n        A_causal_mask = rearrange(causal_mask, 'h n -> h () n')\n\n        Y = Y.masked_fill(Y_causal_mask, 0.)\n        A = A.masked_fill(A_causal_mask, 0.)\n\n        Y = Y.sum(dim = 0)\n        A = A.sum(dim = 0)\n\n        # normalize\n\n        out = Y / rearrange(A + eps, 'b n -> b n ()')\n\n        # merge heads\n\n        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n\n        # combine out\n\n        return self.to_out(out[:, :n])\n\n# main class\n\nclass HTransformer1D(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        depth,\n        max_seq_len,\n        causal = False,\n        heads = 8,\n        dim_head = 64,\n        ff_mult = 4,\n        block_size = 128,     # this is the Nr in the paper - Nb = (max_seq_len / tokens_per_block)\n        pos_emb = None,\n        reversible = False,\n        shift_tokens = False\n    ):\n        super().__init__()\n        assert (max_seq_len % block_size) == 0, 'maximum sequence length must be divisible by the block size'\n        num_blocks = max_seq_len // block_size\n        assert log2(max_seq_len // block_size).is_integer(), f'number of blocks {num_blocks} must be a power of 2'\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = RotaryEmbedding(dim = dim_head)\n        self.max_seq_len = max_seq_len\n\n        layers = nn.ModuleList([])\n\n        attn_class = CausalHAttention1D if causal else HAttention1D\n        attn_kwargs = dict(max_seq_len = max_seq_len) if causal else dict()\n\n        shift_token_ranges = (0, 1) if shift_tokens else (-1, 0, 1)\n\n        for ind in range(depth):\n            attn = attn_class(dim, dim_head = dim_head, heads = heads, block_size = block_size, pos_emb = self.pos_emb, **attn_kwargs)\n            ff = FeedForward(dim, mult = ff_mult)\n\n            if shift_tokens:\n                attn, ff = map(lambda t: PreShiftTokens(shift_token_ranges, t), (attn, ff))\n\n            attn, ff = map(lambda t: PreNorm(dim, t), (attn, ff))\n            layers.append(nn.ModuleList([attn ,ff]))\n\n        execute_type = ReversibleSequence if reversible else SequentialSequence\n        route_attn = ((True, False),) * depth\n        attn_route_map = {'mask': route_attn}\n\n        self.layers = execute_type(layers, args_route = {**attn_route_map})\n\n        self.to_logits = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_tokens)\n        )\n\n    def forward(self, x, mask = None):\n        b, n, device = *x.shape, x.device\n        assert n <= self.max_seq_len, 'sequence length must be less than the maximum sequence length'\n        x = self.token_emb(x)\n        x = self.layers(x, mask = mask)\n        return self.to_logits(x)",
    "description": null,
    "url": null
}