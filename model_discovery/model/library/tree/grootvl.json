{
    "acronym": "grootvl",
    "title": "GrootVL: Tree Topology is All You Need in State Space Model",
    "seed_ids": [
        "mamba",
        "lssl",
        "hippo",
        "gpt2"
    ],
    "s2id": "8bcff5a73c2191084564e7d50ee954358ce12de4",
    "abstract": "The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost.",
    "authors": [
        "Yicheng Xiao",
        "Lin Song",
        "Shaoli Huang",
        "Jiangshan Wang",
        "Siyu Song",
        "Yixiao Ge",
        "Xiu Li",
        "Ying Shan"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "The GrootVL network is proposed, which first dynamically generates a tree topology based on spatial relationships and input features, and then feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "from torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom tree_scan_lan import _C\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange,repeat\n\nclass _MST(Function):\n    @staticmethod\n    def forward(ctx, edge_index, edge_weight, vertex_index):\n        edge_out = _C.mst_forward(edge_index, edge_weight, vertex_index)\n        return edge_out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        return None, None, None\n\nclass _BFS(Function):\n    @staticmethod\n    def forward(ctx, edge_index, max_adj_per_vertex):\n        sorted_index, sorted_parent, sorted_child,_ =\\\n                _C.bfs_forward(edge_index, max_adj_per_vertex)\n        return sorted_index, sorted_parent, sorted_child\n\nclass _Refine(Function):\n    @staticmethod\n    def forward(ctx, feature_in, edge_weight, sorted_index, sorted_parent, sorted_child):\n        feature_out =\\\n            _C.tree_scan_refine_forward(feature_in, edge_weight, sorted_index, sorted_parent, sorted_child)\n            \n        ctx.save_for_backward(feature_out, edge_weight, sorted_index, sorted_parent,\n                sorted_child)\n        return feature_out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        feature_out, edge_weight, sorted_index, sorted_parent,\\\n        sorted_child = ctx.saved_tensors\n\n        grad_feature, grad_edge = _C.tree_scan_refine_backward_feature(feature_out, edge_weight\n                ,sorted_index, sorted_parent, sorted_child,\n                grad_output)\n        return grad_feature, grad_edge, None, None, None\n\ndef norm2_distance(fm_ref, fm_tar):\n    diff = fm_ref - fm_tar\n    weight = (diff * diff).sum(dim=-2)\n    return torch.exp(weight)     # with - is for max tree\n\ndef cosine_distance(fm_ref, fm_tar):\n    weight = -torch.cosine_similarity(fm_ref, fm_tar,dim=1)\n    return torch.exp(weight)    #with - is for min tree\n\ndef batch_index_opr(data, index):\n    with torch.no_grad():\n        channel = data.shape[1]\n        index = index.unsqueeze(1).expand(-1, channel, -1).long()\n    data = torch.gather(data, 2, index)\n    return data\n\ndef tree_scanning_algorithm(self, input_states, contex_len, cache_params):\n    batch_size, seq_len, _ = input_states.shape\n    dtype = input_states.dtype\n    device = input_states.device\n    # 1. Gated MLP's linear projection\n    projected_states = self.in_proj(input_states).transpose(1, 2)                   # [batch, 2 * intermediate_size, seq_len]\n    hidden_states, gate = projected_states.chunk(2, dim=1)\n\n    hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])         # [batch, intermediate_size, seq_len]\n    # 3. State Space Model sequence transformation\n    # 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]\n    ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n    time_step, B, C = torch.split(\n        ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n    )\n    discrete_time_step = self.dt_proj(time_step)                                    # [batch, seq_len, intermediate_size]\n    discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2) # [batch, intermediate_size, seq_len]\n    # 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)\n    A = -torch.exp(self.A_log.float())                                              # [intermediate_size, ssm_state_size]\n    discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None]) # [batch, intermediate_size, seq_len, ssm_state_size]\n    discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()       # [batch, intermediade_size, seq_len, ssm_state_size]\n    deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n    ### tree scan\n    weight = rearrange(discrete_A,'b d l n -> b (d n) l').contiguous()\n    feature_in = rearrange(deltaB_u,'b d l n -> b (d n) l').contiguous()\n    feature_in = torch.flip(feature_in,dims=[-1]).contiguous()\n    weight = torch.roll(torch.flip(weight,dims=[-1]),1,-1).contiguous()\n\n    mst = _MST.apply\n    bfs = _BFS.apply\n    refine = _Refine.apply\n\n    ### hand-build tree\n    tree_ = []\n    for i in range(seq_len-1):\n        tree_.append([i, i + 1])\n    tree_ = torch.tensor(tree_,dtype=torch.int32).to(device)\n    tree = tree_.repeat(batch_size,1,1)\n    sorted_index1 ,sorted_parent1,sorted_child1 = bfs(tree,4)\n    \n    ### build tree by feature\n    try:\n        contex_len = min(contex_len)\n    except:\n        contex_len = contex_len\n    with torch.no_grad():\n        def generate_pairs(L,prompt_len):\n            pairs = []\n            for i in range(0, L-prompt_len):\n                pairs.append([i,i+1])\n            for i in range(L-prompt_len,L-3):\n                pairs.append([i, i+1])\n                pairs.append([i, i+2])\n                pairs.append([i, i+3])\n            pairs.append([L-3, L-2])\n            pairs.append([L-3, L-1])\n            pairs.append([L-2, L-1])\n            return pairs\n        # import pdb;pdb.set_trace()\n        if contex_len > 2:\n            pairs = torch.tensor(generate_pairs(seq_len,contex_len),dtype=torch.int32,device=feature_in.device)\n            data1 = torch.index_select(feature_in,2,pairs[:,0])\n            data2 = torch.index_select(feature_in,2,pairs[:,1])\n            # import pdb;pdb.set_trace()\n            tree_weight = cosine_distance(data1,data2)\n\n            tree = mst(pairs.repeat(batch_size,1,1),tree_weight,seq_len)\n            sorted_index2, sorted_parent2, sorted_child2 = bfs(tree,contex_len)\n        else:\n            sorted_index2 ,sorted_parent2, sorted_child2 = sorted_index1 ,sorted_parent1,sorted_child1\n\n        # import pdb;pdb.set_trace()\n    # import pdb;pdb.set_trace()\n    feature_out1 = refine(feature_in, weight, sorted_index1, sorted_parent1, sorted_child1)\n    # import pdb;pdb.set_trace()\n    edge_weight = batch_index_opr(weight, sorted_index2)\n    feature_out2 = refine(feature_in, edge_weight, sorted_index2, sorted_parent2, sorted_child2)\n    feature_out  = feature_out2 * 0.3 + feature_out1  # 0.3 is scaling factor (hyperparameter)\n\n    feature_out = rearrange(torch.flip(feature_out.to(dtype),dims=[-1]),'b (d n) l -> b l d n',b=batch_size,n=discrete_A.shape[-1]).contiguous()\n    scan_output_ = (feature_out @ C.unsqueeze(-1)).squeeze(-1).transpose(-1,-2) # (B, L, D, N) @ (B, L, N, 1) -> (B, L, D, 1)\n    \n    # [batch, seq_len, intermediade_size]\n    scan_output = scan_output_ + (hidden_states * self.D[None, :, None])\n    scan_output = (scan_output * self.act(gate))\n    # 4. Final linear projection\n    contextualized_states = self.out_proj(scan_output.transpose(1, 2))             # [batch, seq_len, hidden_size]\n    return contextualized_states\n\n\ndef slow_forward(self,input_states, cache_params):\n    batch_size, seq_len, _ = input_states.shape\n    dtype = input_states.dtype\n    # 1. Gated MLP's linear projection\n    projected_states = self.in_proj(input_states).transpose(1, 2)                   # [batch, 2 * intermediate_size, seq_len]\n    hidden_states, gate = projected_states.chunk(2, dim=1)\n    # 2. Convolution sequence transformation\n    if cache_params is not None:\n        ssm_state = cache_params.ssm_states[self.layer_idx]\n        if cache_params.seqlen_offset > 0:\n            conv_state = cache_params.conv_states[self.layer_idx]                   # [batch, intermediate_size, conv_kernel_size]\n            conv_state = torch.roll(conv_state, shifts=-1, dims=-1)\n            conv_state[:, :, -1] = hidden_states[:, :, 0]\n            cache_params.conv_states[self.layer_idx].copy_(conv_state)\n            hidden_states = torch.sum(conv_state * self.conv1d.weight[:, 0, :], dim=-1)\n            if self.use_conv_bias:\n                hidden_states += self.conv1d.bias\n            hidden_states = self.act(hidden_states).to(dtype).unsqueeze(-1)         # [batch, intermediate_size, 1] : decoding\n        else:\n            conv_state = nn.functional.pad(\n                hidden_states,\n                (self.conv_kernel_size - hidden_states.shape[-1], 0)\n            )\n            cache_params.conv_states[self.layer_idx].copy_(conv_state)\n            hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])     # [batch, intermediate_size, seq_len]\n    else:\n        ssm_state = torch.zeros(\n            (batch_size, self.intermediate_size, self.ssm_state_size),\n            device=hidden_states.device, dtype=dtype\n        )\n        hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])         # [batch, intermediate_size, seq_len]\n    # 3. State Space Model sequence transformation\n    # 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]\n    ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n    time_step, B, C = torch.split(\n        ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n    )\n    discrete_time_step = self.dt_proj(time_step)                                    # [batch, seq_len, intermediate_size]\n    discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2) # [batch, intermediate_size, seq_len]\n    # 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)\n    A = -torch.exp(self.A_log.float())                                              # [intermediate_size, ssm_state_size]\n    discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None]) # [batch, intermediate_size, seq_len, ssm_state_size]\n    discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()       # [batch, intermediade_size, seq_len, ssm_state_size]\n    deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n    # 3.c perform the recurrence y \u2190 SSM(A, B, C)(x)\n    scan_outputs = []\n    for i in range(seq_len):\n        ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]      # [batch, intermediade_size, ssm_state]\n        scan_output = torch.matmul(ssm_state.to(dtype), C[:, i, :].unsqueeze(-1))  # [batch, intermediade_size, 1]\n        scan_outputs.append(scan_output[:, :, 0])\n    scan_output = torch.stack(scan_outputs, dim=-1)   \n    # return scan_output                    # [batch, seq_len, intermediade_size]\n    scan_output = scan_output + (hidden_states * self.D[None, :, None])\n    scan_output = (scan_output * self.act(gate))\n    if cache_params is not None:\n        cache_params.ssm_states[self.layer_idx].copy_(ssm_state)\n    # 4. Final linear projection\n    contextualized_states = self.out_proj(scan_output.transpose(1, 2))             # [batch, seq_len, hidden_size]\n    return contextualized_states\n",
    "description": null,
    "url": null
}