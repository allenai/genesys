{
    "acronym": "zebra",
    "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
    "seed_ids": [
        "receptivefieldana",
        "flashattn",
        "longt5",
        "rfa",
        "bigbird",
        "linformer",
        "etc",
        "longformer",
        "compressivetransformer",
        "mqa",
        "transformer",
        "bert",
        "gpt"
    ],
    "s2id": "36697944858ab17ca23b23ae2043aa6c0b2e3d5d",
    "abstract": "This paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency.",
    "authors": [
        "Kaiqiang Song",
        "Xiaoyang Wang",
        "Sangwoo Cho",
        "Xiaoman Pan",
        "Dong Yu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "This paper proposes a new model architecture, referred to as Zebra, that efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers.",
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": "import math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nfrom transformers.utils import (\n    logging,\n)\n\nfrom transformers.models.llama.modeling_llama import (\n    LlamaConfig,\n    LlamaRotaryEmbedding,\n    LlamaLinearScalingRotaryEmbedding,\n    LlamaDynamicNTKScalingRotaryEmbedding,\n    repeat_kv,\n    apply_rotary_pos_emb,\n)\n\n\nlogger = logging.get_logger(__name__)\n\n\ndef _pad_to_multiple(x: torch.Tensor, block_len: int, dim: int, pad_value: int = 0) -> torch.Tensor:\n    \"\"\"Pad a tensor so that a sequence length will be a multiple of `block_len`\"\"\"\n    if x.shape[dim] % block_len == 0:\n        return x\n    pad_len = -x.shape[dim] % block_len\n    # Handle cases when an empty input sequence is given\n    if not all(x.shape):\n        new_shape = list(x.shape)\n        new_shape[dim] += pad_len\n        return torch.zeros(new_shape, dtype=x.dtype)\n\n    pad = [(0, 0)] * x.ndim\n    pad[dim] = (0, pad_len)\n    pad = sum(pad[::-1], ())\n    x = torch.nn.functional.pad(x, pad=pad, mode=\"constant\", value=pad_value)\n    return x\n\n\ndef _pad_to_multiple_2d(x: torch.Tensor, block_len: int, dim_1: int, dim_2: int, pad_value: int = 0) -> torch.Tensor:\n    pad_len_1 = -x.shape[dim_1] % block_len\n    pad_len_2 = -x.shape[dim_2] % block_len\n    # Handle cases when an empty input sequence is given\n    if not all(x.shape):\n        new_shape = list(x.shape)\n        new_shape[dim_1] += pad_len_1\n        new_shape[dim_2] += pad_len_2\n        return torch.zeros(new_shape, dtype=x.dtype)\n\n    pad = [(0, 0)] * x.ndim\n    pad[dim_1] = (0, pad_len_1)\n    pad[dim_2] = (0, pad_len_2)\n    pad = sum(pad[::-1], ())\n    x = torch.nn.functional.pad(x, pad=pad, mode=\"constant\", value=pad_value)\n    return x\n\n\ndef _split_into_blocks(x: torch.Tensor, block_len: int, dim: int) -> torch.Tensor:\n    \"\"\"Split an input tensor into blocks of a given `block_len` along the given `dim`. If the dimension length\n    is not a multiple of `block_len`, it will be padded first with selected `pad_value`.\n    \"\"\"\n    assert x.shape[\n               dim] % block_len == 0, f\"sequence length({x.shape[dim]}) should be multiple of block length({block_len})\"\n    num_blocks = x.shape[dim] // block_len\n    output_shape = x.shape[:dim] + (num_blocks, block_len) + x.shape[(dim + 1):]\n    # If 0 is in output_shape, we cannot apply reshape because of incompatibility with ONNX conversion\n    if 0 in output_shape:\n        return torch.empty(output_shape, dtype=x.dtype, device=x.device)\n    return x.reshape(output_shape)\n\n\ndef _concatenate_2_blocks(x: torch.Tensor, block_dim: int, sequence_dim: int, pad_value: int = 0) -> torch.Tensor:\n    \"\"\"Concatenate three consecutive blocks for each input block for local attentiont.\n    For more information, see: https://arxiv.org/pdf/2112.07916.pdf.\n    \"\"\"\n    num_blocks = x.shape[block_dim]\n\n    pad = [(0, 0)] * x.ndim\n    pad[block_dim] = (1, 0)\n    pad = sum(pad[::-1], ())\n    # [batch_size, num_blocks, block_len] -> [batch_size, 1 + num_blocks , block_len]\n    x = torch.nn.functional.pad(x, pad=pad, mode=\"constant\", value=pad_value)\n\n    blocks_list = []\n    for i in range(2):\n        # We use indexing approach here:\n        # https://numpy.org/doc/stable/user/basics.indexing.html#dealing-with-variable-numbers-of-indices-within-programs\n        indices = [slice(0, None)] * x.ndim\n        indices[block_dim] = slice(i, i + num_blocks)\n        indices = tuple(indices)\n        blocks_list.append(x[indices])\n    # [batch_size, num_blocks, 2 * block_len, ...]\n    return torch.cat(blocks_list, dim=sequence_dim)\n\n\ndef _get_local_casual_attention_mask(block_len: int, device=None) -> torch.Tensor:\n    m = torch.cat([torch.zeros((block_len, block_len + 1)), torch.ones((block_len, block_len))], dim=-1).to(device)\n    m = m.reshape(-1)[: block_len * block_len * 2]\n    return m.reshape(block_len, block_len * 2).unsqueeze(0).unsqueeze(0) > 0.5\n\n\ndef _get_local_attention_mask(m: torch.Tensor, block_len: int) -> torch.Tensor:\n    \"\"\" Construct the local attention mask from the original attention mask.\n        The Input shape is: [batch_size, 1, seq_len, seq_len]\n        The Output shape is: [batch_size * num_blocks, 1, block_len, 2 * block_len]\n    \"\"\"\n    # First Padding to Multiple of block_len\n    if m.shape[-2] % block_len != 0 or m.shape[-1] % block_len != 0:\n        m = _pad_to_multiple_2d(m, block_len, dim_1=-2, dim_2=-1, pad_value=1)\n\n    # Reshape to [batch_size, 1, num_blocks, block_len, num_blocks, block_len]\n    num_blocks = m.shape[-2] // block_len\n    output_shape = m.shape[:-2] + (num_blocks, block_len) + (num_blocks, block_len)\n    blocked_m = m.reshape(output_shape)\n\n    # Padding One Block at dim -2\n    pad = [(0, 0)] * blocked_m.ndim\n    pad[-2] = (1, 0)\n    pad = sum(pad[::-1], ())\n    # [batch_size, 1, num_blocks, block_len, 1 + num_blocks, block_len]\n    padded_m = torch.nn.functional.pad(blocked_m, pad=pad, mode=\"constant\", value=1)\n    mask_block_list = []\n    for i in range(2):\n        indices = [slice(0, None)] * padded_m.ndim\n        indices[-2] = slice(i, i + num_blocks)\n        indices = tuple(indices)\n        mask_block_list.append(padded_m[indices])\n    # shape of [batch_size, 1, num_blocks, block_len, num_block, 2 * block_len]\n    cat_m = torch.cat(mask_block_list, dim=-1)\n    # shape of [num_blocks, batch_size, 1, block_len, 2 * block_len]\n    ret_m = cat_m[:, :, torch.arange(num_blocks), :, torch.arange(num_blocks), :].transpose(0, 1).transpose(1, 2)\n    return ret_m\n\n\ndef attention_mask_func(attn_score, attn_mask):\n    dtype = attn_score.dtype\n    attn_score = attn_score.mask_fill(attn_mask, torch.finfo(dtype).min)\n    # attn_score = attn_score.mask_fill(attn_mask, -10000.0)\n    return attn_score\n\n\nclass MaskedSoftmax(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mask_func = attention_mask_func\n\n    def forward(self, input, mask):\n        dtype = input.dtype\n        input = input.to(dtype=torch.float32)\n        mask_output = self.mask_func(input, mask) if mask is not None else input\n        probs = torch.nn.Softmax(dim=-1)(mask_output).to(dtype)\n        return probs\n\n\nclass ZebraMixAttention(nn.Module):\n    \"\"\"Sparse attention implementation by Kaiqiang\"\"\"\n\n    def __init__(self, layer_id: int, config: LlamaConfig):\n        super().__init__()\n        self.layer_id = layer_id\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.pretraining_tp = config.pretraining_tp\n        self.max_position_embeddings = config.max_position_embeddings\n        # Added for Mix Attention by Kaiqiang\n        self.self_attn_type = config.self_attn_type\n        self.block_len = config.window_size\n        self.layer_group_size = config.layer_group_size\n        self.softmax_func = MaskedSoftmax()\n\n        # Addef for Mix Attention by Kaiqiang\n        if self.self_attn_type == \"mix\":\n            if self.layer_id % self.layer_group_size == 0:\n                self.self_attn_type = \"full\"\n            else:\n                self.self_attn_type = \"sparse\"\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            rope_theta = self.config.rope_theta\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    base=rope_theta,\n                    scaling_factor=scaling_factor\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    base=rope_theta,\n                    scaling_factor=scaling_factor\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def full_attention(self, query_states, key_states, value_states, attention_mask, help_args):\n        bsz = help_args[\"bsz\"]\n        q_len = help_args[\"q_len\"]\n        kv_seq_len = help_args[\"kv_seq_len\"]\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if self.self_attn_type == \"full\" and attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Full Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if self.self_attn_type == \"full\" and attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Full Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        return attn_output, attn_weights\n\n    def sparse_attention(self, query_states, key_states, value_states, attention_mask, help_args):\n        \"\"\" states: bsz, self.num_heads, q/k/v_len, self.head_dim\n        \"\"\"\n        bsz = help_args[\"bsz\"]\n        q_len = help_args[\"q_len\"]\n        kv_seq_len = help_args[\"kv_seq_len\"]\n\n        assert q_len == kv_seq_len, \\\n            f\"sparse attention only used for training when q_len({q_len} == kv_seq_len({kv_seq_len}))\"\n\n        # Convert Attention to 0 (valid) and 1 (invalid)\n        if attention_mask is not None:\n            attention_mask = (attention_mask < 0)\n\n        # Transpose to shape: bsz, seq_len, num_heads, head_dim\n        query_layer = query_states.transpose(1, 2)\n        key_layer = key_states.transpose(1, 2)\n        value_layer = value_states.transpose(1, 2)\n\n        # Padded to multiple\n        query_layer = _pad_to_multiple(query_layer, self.block_len, dim=1, pad_value=0)\n        key_layer = _pad_to_multiple(key_layer, self.block_len, dim=1, pad_value=0)\n        value_layer = _pad_to_multiple(value_layer, self.block_len, dim=1, pad_value=0)\n\n        padded_seq_len = query_layer.shape[1]\n        num_blocks = padded_seq_len // self.block_len\n\n        ###############################################\n        # Processing Q,K,V for local attention\n        ###############################################\n\n        # split into blocks -> (batch_size, num_blocks, block_len, num_heads_per_partition, dim_per_head)\n        query_layer_local = _split_into_blocks(query_layer, self.block_len, dim=1)\n        key_layer_local = _split_into_blocks(key_layer, self.block_len, dim=1)\n        value_layer_local = _split_into_blocks(value_layer, self.block_len, dim=1)\n\n        # Concatenate 2 blocks for keys and values\n        # -> (batch_size, num_blocks, 2 * block_len, num_heads_per_partition, dim_per_head)\n        key_layer_local = _concatenate_2_blocks(key_layer_local, block_dim=1, sequence_dim=2)\n        value_layer_local = _concatenate_2_blocks(value_layer_local, block_dim=1, sequence_dim=2)\n\n        ###############################################\n        # Calculate Local Attention Score\n        ###############################################\n\n        # Compute Local Attention Scores\n        # -> (batch_size, num_heads_per_partition, num_blocks, block_len, 2 * block_len)\n        attn_score_local = torch.einsum(\n            \"...qhd,...khd->...hqk\", query_layer_local, key_layer_local\n        ).transpose(1, 2)\n\n        alpha = 1.0 / self.norm_factor\n        attn_score_local = alpha * attn_score_local\n\n        # Convert Shape to [b, np, sq, sk] Style\n        # -> (batch_size, num_heads_per_partition, padded_seq_len, 2 * block_len)\n        new_shape = (bsz, self.num_heads, padded_seq_len, 2 * self.block_len)\n        attn_score_local = attn_score_local.reshape(new_shape)\n\n        ###############################################\n        # Building Local Attention Masks\n        ###############################################\n\n        # Get local attention mask\n        # -> (batch_size * num_blocks, 1, block_len, 2 * block_len)\n        attn_mask_local = _get_local_attention_mask(attention_mask, self.block_len)\n        attn_mask_local_ = _get_local_casual_attention_mask(self.block_len, device=attn_mask_local.device)\n        attn_mask_local = torch.logical_or(attn_mask_local, attn_mask_local_)\n\n        # Convert Shape to [b, np, sq, sk] Style\n        # -> (batch_size, 1, padded_seq_len, 2 * block_len)\n        new_shape = (bsz, 1, padded_seq_len, 2 * self.block_len)\n        attn_mask_local = attn_mask_local.reshape(new_shape)\n\n        ###############################################\n        # Calculating attention probabilities\n        ###############################################\n\n        # using softmax to calculate the attention probabilities\n        attn_probs = self.softmax_func(attn_score_local, attn_mask_local)\n\n        # Convert attn_probs\n        # -> (batch_size, num_heads_per_partition, num_blocks, block_len, 2 * block_len)\n        shape = (bsz, self.num_heads, num_blocks, self.block_len, 2 * self.block_len)\n        attn_probs = attn_probs.reshape(shape)\n        # Convert attn_probs\n        # -> (batch_size, num_blocks, num_heads_per_partition, block_len, 2 * block_len)\n        attn_probs = attn_probs.transpose(1, 2)\n\n        # shape: (batch_Size, num_blocks, block_len, n_head, dim_per_head)\n        attn_outputs = torch.einsum(\n            \"...hqk,...khd->...qhd\", attn_probs, value_layer_local\n        )\n\n        # convert attn_output\n        # -> (batch_size, num_blocks * block_len, n_head * dim_per_head)\n        attn_outputs = attn_outputs.reshape(\n            bsz,\n            padded_seq_len,\n            self.num_heads * self.head_dim\n        )\n\n        # Removing the padded length and transpose\n        # -> (batch_size, seq_len, dim_per_partition)\n        attn_outputs = attn_outputs.narrow(1, 0, q_len)\n\n        if attn_outputs.size() != (bsz, q_len, self.num_heads * self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, q_len, self.num_heads * self.head_dim)}, but is\"\n                f\" {attn_outputs.size()}\"\n            )\n\n        return attn_outputs, attn_probs\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if position_ids is not None:\n            position_ids = position_ids.to(hidden_states.device)\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.pretraining_tp\n            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.pretraining_tp, dim=0)\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        query_states = query_states.to(hidden_states.device)\n        key_states = key_states.to(hidden_states.device)\n        value_states = value_states.to(hidden_states)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        # make on same device\n        cos = cos.to(hidden_states.device)\n        sin = sin.to(hidden_states.device)\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        # repeat k/v heads if n_kv_heads < n_heads\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        help_args = {\n            \"bsz\": bsz,\n            \"q_len\": q_len,\n            \"kv_seq_len\": kv_seq_len\n        }\n\n        attention_mask = attention_mask.to(hidden_states.device)\n\n        if self.self_attn_type == \"full\":\n            # use fully attention for full attention\n            attn_output, attn_weights = self.full_attention(\n                query_states, key_states, value_states, attention_mask, help_args\n            )\n        elif use_cache:\n            # use full attention with truncated key/value cache for sparse attention\n            truncate_attention_mask = attention_mask\n            if attention_mask.shape[3] > self.block_len + 1:\n                truncate_attention_mask[:, :, :, :-(self.block_len + 1)].fill_(torch.finfo(attention_mask.dtype).min)\n            attn_output, attn_weights = self.full_attention(\n                query_states, key_states, value_states, truncate_attention_mask, help_args\n            )\n        else:\n            # use sparse attention only for training\n            # print(\"In cache\", position_ids, self.layer_id, \"use sparse attn\")\n            attn_output, attn_weights = self.sparse_attention(\n                query_states, key_states, value_states, attention_mask, help_args\n            )\n\n        if self.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n",
    "description": null,
    "url": null
}