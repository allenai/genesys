{
    "acronym": "flashfftconv",
    "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores",
    "seed_ids": [
        "m2",
        "hyena",
        "h3",
        "ptwoattn",
        "liquids4",
        "mega",
        "s5",
        "gssm",
        "httyh",
        "dssm",
        "s4"
    ],
    "s2id": "5c104f905fcacf390270f619f232a2ba4eb873f2",
    "abstract": "Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.",
    "authors": [
        "Daniel Y. Fu",
        "Hermann Kumbong",
        "Eric N. D. Nguyen",
        "Christopher R'e"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.",
    "citationCount": 14,
    "influentialCitationCount": 1,
    "code": "# Copyright (c) Together\n# This software is distributed under the terms of the Apache License, Version 2.0\n# Author: Michael Poli\n# Note: MP and PP utilities are removed for ease of use and editing.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom stripedhyena.cache import InferenceParams, RecurrentInferenceParams\nfrom stripedhyena.engine import HyenaInferenceEngine\nfrom stripedhyena.layers import ParallelGatedMLP, RMSNorm, VocabParallelEmbedding\nfrom stripedhyena.utils import column_split, print_rank_0\n\ntry:\n    from flash_attn.modules.mha import MHA\nexcept ImportError:\n    \"flash_attn not installed\"\n\ntry:\n    from stripedhyena.positional_embeddings import swap_mha_rope\nexcept ImportError:\n    \"could not import swap_mha_rope from src.positional_embeddings\"\n\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, config, layer_idx) -> None:\n        super().__init__()\n        self.config = config\n        self.pre_norm, self.post_norm = RMSNorm(config), RMSNorm(config)\n        self.layer_idx = layer_idx\n        self.proj_groups = config.get(\"proj_groups\", 1)\n        dtype = config.get(\"attn_block_dtype\", torch.bfloat16)\n        mlp_dtype = config.get(\"mlp_dtype\", torch.bfloat16)\n        self.num_attention_heads = config.num_attention_heads\n        self.hidden_size_per_attention_head = config.hidden_size // config.num_attention_heads\n\n        self.counter = 0\n        self.inner_mha_cls = MHA(\n            embed_dim=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_heads_kv=config.num_attention_heads // self.proj_groups,\n            rotary_emb_dim=config.hidden_size // config.num_attention_heads,\n            qkv_proj_bias=config.get(\"qkv_proj_bias\", True),\n            rotary_emb_base=config.get(\"rotary_emb_base\", 10000),\n            causal=True,\n            layer_idx=layer_idx,\n            out_proj_bias=config.get(\"mha_out_proj_bias\", True),\n            use_flash_attn=self.config.use_flash_attn,\n        ).to(dtype=dtype)\n\n        # check if using interpolated rotary pos emb from config, and swap the rope emb\n        if config.get(\"use_interpolated_rotary_pos_emb\", False):\n            swap_mha_rope(\n                mha=self.inner_mha_cls,\n                kwargs_new_rope={\"scaling_factor\": config.get(\"rotary_emb_scaling_factor\", 1.0)},\n            )\n\n        if self.config.get(\"smeared_gqa\", False):\n            self.inner_mha_cls.num_heads_kv = self.inner_mha_cls.num_heads\n        self.inner_mha_cls.rotary_emb.register_buffer(\"inv_freq\", self.inner_mha_cls.rotary_emb.inv_freq)\n\n        self.mlp = ParallelGatedMLP(config).to(dtype=mlp_dtype)\n\n    def forward(self, u, inference_params=None, padding_mask=None, *args, **kwargs):\n        if (\n            type(padding_mask) == torch.Tensor\n        ):  # workaround for masking bug in FA. This works because Wqkv does not have bias\n            # and attention scores will be also automatically zeroed.\n            u = u * padding_mask[..., None]\n\n        u = (\n            self.inner_mha_cls(\n                self.pre_norm(u),\n                inference_params=inference_params,\n            )\n            + u\n        )\n        if type(padding_mask) == torch.Tensor:  # guard against bias\n            u = u * padding_mask[..., None]\n        u = self.mlp(self.post_norm(u)) + u\n        return u, None\n\n\nclass ParallelHyenaFilter(nn.Module):\n    def __init__(self, config, layer_idx) -> None:\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.hyena_filter_groups = config.get(\"hyena_filter_groups\", self.config.hidden_size)\n\n        self.use_flashfft = config.get(\"use_flashfft\", False)\n        self.state_size = config.state_size\n        self.hidden_size = config.hidden_size\n        self.num_filters = config.num_filters\n        self.inference_mode = config.get(\"inference_mode\", True)\n        self.counter = 0\n        self.column_split_hyena = config.get(\"column_split_hyena\", True)\n\n        assert self.hidden_size % self.num_filters == 0 and self.num_filters <= self.hidden_size\n\n        self.D = nn.Parameter(torch.zeros(self.hidden_size))\n\n        # attention heads are not used except to split post short_filter\n        # projections in the same way as the checkpoint\n        self.num_attention_heads = config.num_attention_heads\n        self.hidden_size_per_attention_head = self.hidden_size // self.num_attention_heads\n\n        # after preprocessing here we can save the new checkpoint\n        self.short_filter_length = config.short_filter_length\n        self.short_filter_weight = nn.Parameter(torch.randn(3 * config.hidden_size, 1, config.short_filter_length))\n        self.short_filter_bias = (\n            nn.Parameter(torch.randn(3 * config.hidden_size)) if config.short_filter_bias else None\n        )\n\n        self.engine = HyenaInferenceEngine(layer_idx=layer_idx)\n        self.use_flash_depthwise = config.get(\"use_flash_depthwise\", False)\n        self.data_dtype = None\n\n        if self.use_flash_depthwise:\n            try:\n                from flashfftconv import FlashDepthwiseConv1d\n\n                self.fir_fn = FlashDepthwiseConv1d(\n                    channels=3 * self.hidden_size,\n                    kernel_size=self.short_filter_length,\n                    padding=self.short_filter_length - 1,\n                    weights=self.short_filter_weight,\n                    bias=self.short_filter_bias,\n                    device=None,\n                    dtype=self.config.get(\"depthwise_dtype\", torch.bfloat16),\n                )\n            except ImportError:\n                \"flashfftconv not installed\"\n        else:\n            self.fir_fn = F.conv1d\n\n        self.fftconv_fn = None\n        self.long_fir_threshold = config.get(\"long_fir_threshold\", None)\n        if self.long_fir_threshold is not None:\n            assert self.use_flashfft is False, \"long_fir_threshold not compatible with fused flashfft\"\n\n        self.num_systems = self.hidden_size // self.hyena_filter_groups\n\n        poles = torch.randn(self.num_systems, self.state_size, 1, 2)\n\n        # TODO: bring over init from internals\n        poles[..., 0] = 1e-2 * torch.randn(self.num_systems, self.state_size, 1)\n        poles[..., 1] = 1e-3 * torch.randn(self.num_systems, self.state_size, 1)\n\n        self.poles = nn.Parameter(poles)\n\n        self.residues = nn.Parameter(torch.randn(self.num_systems, self.state_size, 1, 2))\n        self.h = None\n\n    def forward(self, u, inference_params=None, padding_mask=None, *args, **kwargs):\n        if inference_params is not None and self.layer_idx in inference_params.fir_state_dict.keys():\n            return self.sequential_forward(u, inference_params)\n\n        else:\n            return self.parallel_forward(u, inference_params, padding_mask)\n\n    def parallel_forward(self, u, inference_params=None, padding_mask=None):\n        L = u.shape[1]\n        z_pre, fir_state = self.engine.parallel_fir(\n            self.fir_fn,\n            u,\n            self.short_filter_weight,\n            self.short_filter_bias,\n            L,\n            fir_length=self.short_filter_length,\n            inference_params=inference_params,\n            padding_mask=padding_mask,\n        )\n        if inference_params:\n            inference_params.fir_state_dict[self.layer_idx] = fir_state\n\n        if self.h is None:\n            h, filter_dtype, poles, residues = self.compute_filter(L, u.device)\n        else:\n            h = self.h\n            filter_dtype = self.h.dtype\n\n        if self.hyena_filter_groups > 1:\n            h = h.repeat_interleave(self.hidden_size // self.hyena_filter_groups, 1)\n\n        # if inference_params is not None, we plan to perform generation:\n        # prefilling is handled by the engine.\n        dims = (\n            self.hidden_size,\n            self.num_attention_heads,\n            self.hidden_size_per_attention_head,\n            self.state_size,\n            self.hyena_filter_groups,\n        )\n        y = self.engine.parallel_iir(\n            z_pre,\n            h,\n            self.D,\n            L,\n            t=self.t,\n            poles=self.poles,\n            residues=self.residues,\n            dims=dims,\n            inference_params=inference_params,\n            layer_idx=self.layer_idx,\n            prefill_style=self.config.get(\"prefill_style\", \"fft\"),\n            use_flashfft=self.use_flashfft,\n            fftconv_fn=self.fftconv_fn,\n            column_split_hyena=self.column_split_hyena,\n            long_fir_threshold=self.long_fir_threshold,\n            padding_mask=padding_mask,\n        )\n\n        return y, inference_params\n\n    def sequential_forward(self, u, inference_params):\n        if self.data_dtype is None:\n            self.data_dtype = u.dtype\n        if len(u.shape) > 2:\n            u = u[:, -1]\n\n        fir_state, iir_state = (\n            inference_params.fir_state_dict[self.layer_idx],\n            inference_params.state_dict[self.layer_idx],\n        )\n\n        z_pre, fir_state = self.engine.step_fir(\n            u, fir_state, weight=self.short_filter_weight, bias=self.short_filter_bias\n        )\n        x2, x1, v = (\n            column_split(z_pre, self.num_attention_heads, self.hidden_size_per_attention_head)\n            if self.column_split_hyena\n            else z_pre.split([self.hidden_size, self.hidden_size, self.hidden_size], dim=1)\n        )\n\n        y, iir_state = self.engine.step_iir(\n            x2,\n            x1,\n            v,\n            self.D,\n            self.residues,\n            self.poles,\n            iir_state,\n            iir_groups=self.hyena_filter_groups,\n        )\n\n        inference_params.fir_state_dict[self.layer_idx] = fir_state\n        inference_params.state_dict[self.layer_idx] = iir_state\n        y = y.to(dtype=self.data_dtype)\n        return y[:, None], inference_params\n\n    def update_time(self, L, device):\n        \"\"\"\n        Set [0, 1, ..., L-1] where L is the length of the current batch of inputs.\n        If L is greater than the length of the previous batch, then the time vector is\n        reinitialized. Otherwise, the time vector is truncated from cache.\n        \"\"\"\n        if not hasattr(self, \"t\"):\n            self.t = torch.arange(L, device=device)[None, None]\n        elif self.t.shape[-1] < L:\n            self.t = torch.arange(L, device=device)[None, None]\n        else:\n            self.t = self.t[..., :L]\n\n    def compute_filter(self, L, device):\n        self.update_time(L, device)\n        filter_dtype = torch.float32\n        residues, log_poles = (\n            torch.view_as_complex(self.residues.to(filter_dtype)),\n            torch.view_as_complex(self.poles.to(filter_dtype)).log(),\n        )\n        h = (residues * (log_poles * self.t).exp()).real.sum(1)[None]\n        return h, filter_dtype, log_poles, residues\n\n\nclass ParallelGatedConvBlock(nn.Module):\n    def __init__(self, config, layer_idx) -> None:\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.low_mem_mode = config.get(\"low_mem_mode\", False)\n        dtype = config.get(\"hyena_block_dtype\", torch.float32)\n        mlp_dtype = config.get(\"mlp_dtype\", torch.bfloat16)\n        self.pre_norm, self.post_norm = RMSNorm(config).to(dtype=dtype), RMSNorm(config).to(dtype=dtype)\n        self.filter = ParallelHyenaFilter(config, layer_idx).to(dtype=dtype)\n        self.projections = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n        self.out_filter_dense = nn.Linear(config.hidden_size, config.hidden_size).to(dtype)\n        self.mlp = ParallelGatedMLP(config).to(dtype=mlp_dtype)\n\n        self.proj_norm_fn = self.proj_norm\n        self.res_mlp_norm_fn = self.res_mlp_norm\n\n        if self.config.get(\"compile\", False):\n            self.proj_norm_fn = torch.compile(self.proj_norm, fullgraph=True, dynamic=False, mode=\"reduce-overhead\")\n            self.res_mlp_norm_fn = torch.compile(\n                self.res_mlp_norm, fullgraph=True, dynamic=False, mode=\"reduce-overhead\"\n            )\n\n    def proj_norm(self, x):\n        return self.projections(self.pre_norm(x))\n\n    def res_mlp_norm(self, x):\n        return self.mlp(self.post_norm(x)) + x\n\n    def forward(self, u, inference_params=None, padding_mask=None, *args, **kwargs):\n        z = self.proj_norm_fn(u)\n\n        if type(padding_mask) == torch.Tensor:  # guard against bias\n            z = z * padding_mask[..., None]\n\n        z, inference_params = self.filter(z, inference_params=inference_params, padding_mask=padding_mask)\n\n        z_in = self.out_filter_dense(z) + u\n\n        if type(padding_mask) == torch.Tensor:  # guard against bias\n            z_in = z_in * padding_mask[..., None]\n\n        y = self.res_mlp_norm_fn(z_in)\n\n        return y, inference_params\n\n\ndef get_block(config, layer_idx, flash_fft=None):\n    if layer_idx in config.attn_layer_idxs:\n        return AttentionBlock(config, layer_idx)\n    elif layer_idx in config.hyena_layer_idxs:\n        block = ParallelGatedConvBlock(config, layer_idx)\n        if config.get(\"use_flashfft\", \"False\"):\n            block.filter.fftconv_fn = flash_fft\n        return block\n    else:\n        raise NotImplementedError\n\n\nclass StripedHyena(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embedding_layer = VocabParallelEmbedding(config)\n        self.norm = RMSNorm(config) if config.get(\"final_norm\", True) else None\n        self.unembed = self.embedding_layer if config.tie_embeddings else VocabParallelEmbedding(config)\n\n        if config.get(\"use_flashfft\", \"True\"):\n            try:\n                from flashfftconv import FlashFFTConv\n\n                self.flash_fft = FlashFFTConv(config.seqlen, dtype=torch.bfloat16)\n            except ImportError:\n                \"flashfftconv not installed\"\n        else:\n            self.flash_fft = None\n\n        self.blocks = nn.ModuleList(\n            get_block(config, layer_idx, flash_fft=self.flash_fft) for layer_idx in range(config.num_layers)\n        )\n\n    def forward(self, x, inference_params_dict=None, padding_mask=None):\n        L = x.shape[1]\n        x = self.embedding_layer.embed(x)\n        if inference_params_dict is not None:\n            x, inference_params_dict_out = self.stateful_forward(\n                x,\n                inference_params_dict=inference_params_dict,\n            )\n        else:\n            x, inference_params_dict_out = self.stateless_forward(x, padding_mask=padding_mask)\n\n        x = self.norm(x)\n        x = self.unembed.unembed(x)\n        return x, inference_params_dict_out\n\n    def stateful_forward(self, x, inference_params_dict=None):\n        for block_idx, block in enumerate(self.blocks):\n            block_name = \"mha\" if block_idx in self.config.attn_layer_idxs else \"hyena\"\n            inference_params = inference_params_dict[block_name]\n            x, _ = block(x, inference_params=inference_params)\n\n        return x, inference_params_dict\n\n    def stateless_forward(self, x, padding_mask=None):\n        if type(padding_mask) == torch.Tensor:\n            x = x * padding_mask[..., None]\n\n        for _, block in enumerate(self.blocks):\n            x, _ = block(x, inference_params=None, padding_mask=padding_mask)\n        return x, None\n\n    def initialize_inference_params(self):\n        inference_params_dict = {\n            \"mha\": InferenceParams(\n                max_seqlen=self.config.get(\"max_seqlen\", 8192),\n                max_batch_size=self.config.get(\"max_batch_size\", 1),\n                seqlen_offset=0,\n            ),\n            \"hyena\": RecurrentInferenceParams(\n                fir_filter_length=self.config.short_filter_length,\n                state_dim=self.config.state_size,\n                seqlen_offset=0,\n            ),\n        }\n        return inference_params_dict\n\n    def precompute_filters(self, L, device):\n        for block_idx, block in enumerate(self.blocks):\n            if type(block) == ParallelGatedConvBlock:\n                if type(block.filter) == ParallelHyenaFilter:\n                    L = block.filter.long_fir_threshold or L\n                    print_rank_0(f\"Precomputing filters, L={L}...\")\n\n                    filter_dtype = torch.float16 if L >= 2048 else torch.float32\n\n                    block.filter._set_time(L, device)\n                    residues, poles = (\n                        torch.view_as_complex(block.filter.residues.to(torch.float16)),\n                        torch.view_as_complex(block.filter.poles.to(torch.float16)),\n                    )\n\n                    block.filter.h = (residues * poles**block.filter.t).real.sum(1)[None]\n                    block.filter.h = block.filter.h.to(dtype=filter_dtype)\n\n    def load_poles_residues(self, path):\n        \"Load different poles and residues for each layer.\"\n        for block_idx, block in enumerate(self.blocks):\n            if type(block) == ParallelGatedConvBlock:\n                if type(block.filter) == ParallelHyenaFilter:\n                    print(f\"Loading poles and residues for block {block_idx}\")\n                    poles = torch.load(path + f\"/approx_poles_{block_idx+1}.pt\", map_location=\"cpu\")\n                    poles = torch.view_as_real(poles)\n                    residues = torch.load(path + f\"/approx_residues_{block_idx+1}.pt\", map_location=\"cpu\")\n                    residues = torch.view_as_real(residues)\n                    poles = poles.permute(1, 0, 2).unsqueeze(-2)\n                    residues = residues.permute(1, 0, 2).unsqueeze(-2)\n\n                    block.filter.poles = nn.Parameter(poles)\n                    block.filter.residues = nn.Parameter(residues)\n\n    def to_bfloat16_except_poles_residues(self):\n        \"\"\"Convert all parameters to bfloat16 except for the poles and residues.\n\n        Particularly important for longer prompts.\n        \"\"\"\n        for k, p in self.named_parameters():\n            if \"poles\" not in k and \"residues\" not in k:\n                p.data = p.data.to(torch.bfloat16)",
    "description": null,
    "url": null
}