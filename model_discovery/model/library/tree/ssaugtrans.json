{
    "acronym": "ssaugtrans",
    "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer",
    "seed_ids": [
        "mega",
        "flash",
        "longt5",
        "s4",
        "poolingformer",
        "rfa",
        "nystromformer",
        "performer",
        "clusteredattn",
        "lineartransformer",
        "linformer",
        "etc",
        "longformer",
        "routingtransformer",
        "sinkhorn",
        "reformer",
        "blockbert",
        "sparsetransformer",
        "bert"
    ],
    "s2id": "661e8d555c4424b5953f17434f2ba910bfcf3afe",
    "abstract": "Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.",
    "authors": [
        "Simiao Zuo",
        "Xiaodong Liu",
        "Jian Jiao",
        "Denis Xavier Charles",
        "Eren Manavoglu",
        "Tuo Zhao",
        "Jianfeng Gao"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": "The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.",
    "citationCount": 29,
    "influentialCitationCount": 3,
    "code": "import json\nimport torch\nimport torch.nn as nn\n\nfrom torch import Tensor\nfrom typing import Dict, List, Optional\n\nfrom fairseq import utils\nfrom fairseq.modules import LayerNorm\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nfrom fairseq.modules.quant_noise import quant_noise\n\nfrom .local_attention import LocalAttention\nfrom .s4 import S4Module\n\n\nclass SpadeDecoderLayerBase(nn.Module):\n    \"\"\"Decoder layer block.\n\n    In the original paper each operation (multi-head attention, encoder\n    attention or FFN) is postprocessed with: `dropout -> add residual ->\n    layernorm`. In the tensor2tensor code they suggest that learning is more\n    robust when preprocessing each layer with layernorm and postprocessing with:\n    `dropout -> add residual`. We default to the approach in the paper, but the\n    tensor2tensor approach can be enabled by setting\n    *cfg.decoder.normalize_before* to ``True``.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg,\n        layer_idx,\n        no_encoder_attn=False,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        has_relative_attention_bias=False,\n    ):\n        super().__init__()\n        assert not cfg.cross_self_attention and no_encoder_attn\n\n        self.embed_dim = cfg.decoder.embed_dim\n        self.dropout_module = FairseqDropout(\n            cfg.dropout, module_name=self.__class__.__name__\n        )\n        self.quant_noise = cfg.quant_noise.pq\n        self.quant_noise_block_size = cfg.quant_noise.pq_block_size\n\n        self.cross_self_attention = cfg.cross_self_attention\n\n        self.self_attn = self.build_self_attention(\n            self.embed_dim,\n            cfg,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            has_relative_attention_bias=has_relative_attention_bias,\n        )\n        self.attn_ln = (\n            LayerNorm(self.embed_dim)\n            if utils.safe_getattr(cfg, \"scale_attn\", False)\n            else None\n        )\n        self.nh = self.self_attn.num_heads\n        self.head_dim = self.self_attn.head_dim\n        scale_heads = utils.safe_getattr(cfg, \"scale_heads\", False)\n        self.c_attn = (\n            nn.Parameter(torch.ones((self.nh,)), requires_grad=True)\n            if scale_heads\n            else None\n        )\n\n        if layer_idx % cfg.s4_every_n_layers == 0:\n            assert self.c_attn is None  # handled via self.norm_local\n            if isinstance(cfg.s4_lr, str):\n                cfg.s4_lr = json.loads(cfg.s4_lr)\n            self.s4_module = S4Module(cfg, is_decoder=True)\n            self.s4_local_combine = cfg.s4_local_combine\n            if self.s4_local_combine == \"concat\":\n                self.aggregate = quant_noise(\n                    nn.Linear(2 * self.embed_dim, self.embed_dim),\n                    self.quant_noise, self.quant_noise_block_size\n                )\n                self.norm_global = LayerNorm(self.embed_dim, export=cfg.export)\n                self.norm_local = LayerNorm(self.embed_dim, export=cfg.export)\n            elif self.s4_local_combine == \"add\":\n                self.s4_weight = cfg.s4_weight\n                self.norm_global = LayerNorm(self.embed_dim, export=cfg.export)\n                self.norm_local = LayerNorm(self.embed_dim, export=cfg.export)\n            elif self.s4_local_combine == \"stack\":\n                pass\n            else:\n                raise KeyError(\"concat/add/stack S4 and local attention\")\n        else:\n            self.s4_module = None\n\n        self.activation_fn = utils.get_activation_fn(activation=cfg.activation_fn)\n        activation_dropout_p = cfg.activation_dropout\n        if activation_dropout_p == 0:\n            # for backwards compatibility with models that use cfg.relu_dropout\n            activation_dropout_p = cfg.relu_dropout or 0\n        self.activation_dropout_module = FairseqDropout(\n            float(activation_dropout_p), module_name=self.__class__.__name__\n        )\n        self.normalize_before = cfg.decoder.normalize_before\n\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)\n\n        if no_encoder_attn:\n            self.encoder_attn = None\n            self.encoder_attn_layer_norm = None\n        else:\n            self.encoder_attn = self.build_encoder_attention(self.embed_dim, cfg, has_relative_attention_bias)\n            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)\n\n        self.ffn_layernorm = (\n            LayerNorm(cfg.decoder.ffn_embed_dim)\n            if utils.safe_getattr(cfg, \"scale_fc\", False)\n            else None\n        )\n        self.w_resid = (\n            nn.Parameter(\n                torch.ones(\n                    self.embed_dim,\n                ),\n                requires_grad=True,\n            )\n            if utils.safe_getattr(cfg, \"scale_resids\", False)\n            else None\n        )\n\n        self.fc1 = self.build_fc1(\n            self.embed_dim,\n            cfg.decoder.ffn_embed_dim,\n            self.quant_noise,\n            self.quant_noise_block_size,\n        )\n        self.fc2 = self.build_fc2(\n            cfg.decoder.ffn_embed_dim,\n            self.embed_dim,\n            self.quant_noise,\n            self.quant_noise_block_size,\n        )\n\n        self.final_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)\n        self.need_attn = True\n\n        self.onnx_trace = False\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_self_attention(\n        self, embed_dim, cfg, add_bias_kv=False, add_zero_attn=False, has_relative_attention_bias=False,\n    ):\n        kwargs = {\n            \"local_radius\": cfg.local_radius,\n            \"has_relative_attention_bias\": has_relative_attention_bias,\n            \"relative_attention_num_buckets\": cfg.relative_attention_num_buckets,\n            \"relative_attention_max_distance\": cfg.relative_attention_max_distance,\n        }\n        return LocalAttention(\n            embed_dim,\n            cfg.decoder.attention_heads,\n            is_decoder=True,\n            dropout=cfg.attention_dropout,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n            kwargs=kwargs,\n        )\n\n    def build_encoder_attention(self, embed_dim, cfg, has_relative_attention_bias=False):\n        kwargs = {\n            \"local_radius\": cfg.local_radius,\n            \"has_relative_attention_bias\": has_relative_attention_bias,\n            \"relative_attention_num_buckets\": cfg.relative_attention_num_buckets,\n            \"relative_attention_max_distance\": cfg.relative_attention_max_distance,\n        }\n        return LocalAttention(\n            embed_dim,\n            cfg.decoder.attention_heads,\n            is_decoder=True,\n            dropout=cfg.attention_dropout,\n            encoder_decoder_attention=True,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n            kwargs=kwargs,\n        )\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def residual_connection(self, x, residual):\n        return residual + x\n\n    def forward(\n        self,\n        x,\n        encoder_out: Optional[torch.Tensor] = None,\n        encoder_padding_mask: Optional[torch.Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        prev_self_attn_state: Optional[List[torch.Tensor]] = None,\n        prev_attn_state: Optional[List[torch.Tensor]] = None,\n        self_attn_mask: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n        need_attn: bool = False,\n        need_head_weights: bool = False,\n        local_attn_mask: Optional[torch.Tensor] = None,\n        position_bias: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor, optional): binary\n                ByteTensor of shape `(batch, src_len)` where padding\n                elements are indicated by ``1``.\n            need_attn (bool, optional): return attention weights\n            need_head_weights (bool, optional): return attention weights\n                for each head (default: return average over heads).\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"\n        assert encoder_out is None and encoder_padding_mask is None \\\n               and incremental_state is None and prev_self_attn_state is None \\\n               and prev_attn_state is None, \"local attention only supports language modeling\"\n\n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        \"\"\"compute global information using S4\"\"\"\n        s4_x = None\n        if self.s4_module is not None:\n            s4_x = self.s4_module(x, encoder_padding_mask)\n            if self.s4_local_combine == \"stack\":\n                x = s4_x  # directly feed s4_x to local attention\n\n        \"\"\"compute local attention\"\"\"\n        local_x, _, position_bias = self.self_attn(\n            hidden_states=x,\n            local_mask=local_attn_mask,\n            position_bias=position_bias,\n        )\n        if self.c_attn is not None:\n            tgt_len, bsz = local_x.size(0), local_x.size(1)\n            local_x = local_x.view(tgt_len, bsz, self.nh, self.head_dim)\n            local_x = torch.einsum(\"tbhd,h->tbhd\", local_x, self.c_attn)\n            local_x = local_x.reshape(tgt_len, bsz, self.embed_dim)\n        if self.attn_ln is not None:\n            local_x = self.attn_ln(local_x)\n\n        \"\"\"aggregate local and global information\"\"\"\n        if self.s4_module is not None:\n            if self.s4_local_combine == \"stack\":\n                x = local_x\n            else:\n                local_x = self.norm_local(local_x)\n                s4_x = self.norm_global(s4_x)\n                if self.s4_local_combine == \"concat\":\n                    x = torch.concat((s4_x, local_x), dim=-1)\n                    x = self.aggregate(x)\n                elif self.s4_local_combine == \"add\":\n                    x = self.s4_weight * s4_x + (1.0 - self.s4_weight) * local_x\n        else:\n            x = local_x\n\n        x = self.dropout_module(x)\n        x = self.residual_connection(x, residual)\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n\n        x = self.activation_fn(self.fc1(x))\n        x = self.activation_dropout_module(x)\n        if self.ffn_layernorm is not None:\n            x = self.ffn_layernorm(x)\n        x = self.fc2(x)\n        x = self.dropout_module(x)\n        if self.w_resid is not None:\n            residual = torch.mul(self.w_resid, residual)\n        x = self.residual_connection(x, residual)\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        return x, None, None, position_bias",
    "description": null,
    "url": null
}