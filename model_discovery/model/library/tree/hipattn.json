{
    "acronym": "hipattn",
    "title": "HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning",
    "seed_ids": [
        "hyperattention",
        "streamingllm",
        "gqa",
        "flashattn",
        "longformer",
        "sinkhorn",
        "reformer"
    ],
    "s2id": "45294f7227d0228f3d26b2796824f46524ad7a76",
    "abstract": "In modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering. However, handling long context sequences with LLMs is prohibitively costly due to the conventional attention mechanism's quadratic time and space complexity, and the context window size is limited by the GPU memory. Although recent works have proposed linear and sparse attention mechanisms to address this issue, their real-world applicability is often limited by the need to re-train pre-trained models. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which simultaneously reduces the training and inference time complexity from $O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To this end, we devise a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly. HiP is training-free as it only utilizes the pre-trained attention scores to spot the positions of the top-$k$ most significant elements for each query. Moreover, it ensures that no token is overlooked, unlike the sliding window-based sub-quadratic attention methods, such as StreamingLLM. Extensive experiments on diverse real-world benchmarks demonstrate that HiP significantly reduces prompt (i.e., prefill) and decoding latency and memory usage while maintaining high generation performance with little or no degradation. As HiP allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no additional engineering due to its easy plug-and-play deployment, we believe that our work will have a large practical impact, opening up the possibility to many long-context LLM applications previously infeasible.",
    "authors": [
        "Heejun Lee",
        "Geon Park",
        "Youngwan Lee",
        "Jina Kim",
        "Wonyoung Jeong",
        "Myeongjae Jeon",
        "Sung Ju Hwang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "Hierarchically Pruned Attention (HiP) is proposed, a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly, opening up the possibility to many long-context LLM applications previously infeasible.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "\"\"\"\n- Need to stop expansion when reach #patch\n> multiple = 4, #patch:p = 16, k = 64, w = 8192\n| w    | z    | z'   | k'   | keep?|\n|------|------|------|------|------|\n| 64   | 64   | 1    | 16   | True |\n| 256  | 64   | 2    | 16   | True |\n| 1024 | 64   | 8    | 16   | True |\n| 4096 | 64   | 32   | 32   | done |\n| 8192 | done | done | done | done |\n\n- When approximator interation stops?\nw / T * k >= p\n\nif p and k is constant\nw = (p/k)T\napproximator is logN, but sparse attention is linear\n\nif p=T/C\nw = T^2/(kC) -- log w = 2log T - log kC\napproximator is quadratic, but sparse attention is linear\n\nif k=T/C\nw = pC\napproximator is linear, but sparse attention is quadratic\n\nif p=T/C and k=T/C\nw = T\napproximator is log N, but sparse attention is quadratic\n\"\"\"\n\nimport math\nimport numba\nimport numpy as np\nimport torch\nfrom numpy import ndarray\nfrom torch import Tensor\nimport matplotlib.pyplot as plt\nimport tqdm\nimport skimage.measure\n\ndef mask(\n    queries: ndarray, \n    keys: ndarray, \n    \n    w_start: int = 32,\n    n_patches: int = 16,\n    mask_k: int = 128,\n    scale_up: int = 4,\n    \n    # w_start: int = 4,\n    # n_patches: int = 8,\n    # mask_k: int = 16,\n    # scale_up: int = 2,\n) -> ndarray:\n    # NOTE: uncomment for cumsum\n    # queries_cs = queries.cumsum(1)\n    # keys_cs = keys.cumsum(1)\n    \n    dtype = np.float32\n    N, T_DST, HID = queries.shape\n    _, T_SRC, _ = keys.shape\n    assert T_DST <= T_SRC\n    \n    # NOTE: width of last query\n    w_curr = round(w_start / scale_up)\n    t_srcs = np.arange(T_SRC-T_DST+1, T_SRC+1, 1, dtype=np.int32).reshape((1, T_DST, 1)).repeat(N, axis=0)\n    ws = t_srcs.clip(0, w_curr)\n    ks = ws.copy()\n    mask = (np.arange(mask_k, dtype=np.float32).reshape((1, 1, mask_k)) / ks)\n    t_mask = np.zeros((N, T_DST, mask_k*math.ceil(scale_up)), dtype=np.float32)\n    scores = np.zeros((N, T_DST, mask_k*math.ceil(scale_up)), dtype=dtype)\n    \n    def to_dense(mask, ks, ws):\n        dense = np.zeros((N, T_DST, T_SRC))\n        for i in range(N):\n            for j in range(T_DST):\n                nonzero_k = ks[i, j, 0]\n                for k in range(nonzero_k):\n                    dense[i, j, int(mask[i, j, k] * ws[i, j, 0])] = 1\n        return dense\n    \n    while w_curr < T_SRC:\n        # scale up, and top k masking\n        for i in range(N):\n            for j in tqdm.tqdm(range(T_DST)):\n                # for each query\n                w_old = ws[i, j, 0]\n                t_src = t_srcs[i, j, 0]\n                w_new = min(round(w_old * scale_up), t_src)\n                # print(i, j, w_old, w_new)\n                if w_old != w_new:\n                    k_old = ks[i, j, 0]\n                    k_new = max(n_patches, int(min(mask_k / t_src, 1.0) * w_new))\n                    \n                    def resize_query(i: int, j: int, mask: ndarray, t_mask: ndarray, k_old: int, w_old: int, w_new: int) -> int:\n                        num_pixels = 0\n                        for k in range(k_old):\n                            loc = mask[i, j, k]\n                            loc_idx_start = int(loc * w_old)\n                            loc_idx_end = loc_idx_start + 1\n                            loc_idx_start = int(loc_idx_start / w_old * w_new)\n                            loc_idx_end = int(loc_idx_end / w_old * w_new)\n                            dup_pixels = loc_idx_end - loc_idx_start\n                            for l in range(dup_pixels):\n                                t_mask[i, j, num_pixels + l] = (loc_idx_start + l) / w_new\n                            num_pixels += dup_pixels\n                        return num_pixels\n                    \n                    k_new = min(t_src, max(n_patches, k_new))\n                    \n                    # mask -> t_mask    \n                    num_pixels = resize_query(i, j, mask, t_mask, k_old, w_old, w_new)\n                    \n                    # t_mask -> mask (using scores)\n                    if k_new < num_pixels:\n                        # need top_k, so compute scores\n                        for k in range(num_pixels):\n                            vec_q = queries[i, j, :]\n                            \n                            # NOTE: nearest\n                            loc = t_mask[i, j, k]\n                            vec_k = keys[i, int(loc * t_src), :]\n                            \n                            # NOTE: cumsum\n                            # loc_start = int(int(loc * w_new) * (t_src / w_new))\n                            # loc_end = int((int(loc * w_new) + 1) * (t_src / w_new))\n                            # loc_end = max(loc_end, loc_start + 1)\n                            # vec_k = keys_cs[i, loc_end - 1, :]# - keys_cs[i, loc_start, :]\n                            # if loc_start > 0:\n                            #     vec_k -= keys_cs[i, loc_start - 1, :]\n                            \n                            score = np.dot(vec_q, vec_k)\n                            scores[i, j, k] = -score # NOTE: store negative store\n                        \n                        topk_indices = np.argpartition(scores[i, j, :num_pixels], kth=k_new)\n                        topk_indices = np.sort(topk_indices[:k_new])\n                        for k in range(k_new):\n                            mask[i, j, k] = t_mask[i, j, topk_indices[k]]\n                    else:\n                        mask[i, j, :num_pixels] = t_mask[i, j, :num_pixels]\n                    \n                    ws[i, j, 0] = w_new\n                    ks[i, j, 0] = min(k_new, num_pixels)\n                # end if w_old != w_new\n            # end for j\n        # end for i\n        \n        # print(t_mask[0, 1000:1016, :5])\n        # print(ws[0, 1000:1016])\n        \n        # NOTE: debug image output\n        x = to_dense(mask, ks, ws)[0]\n        x = skimage.measure.block_reduce(x, (4, 4), np.max)\n        plt.imshow(x)\n        plt.savefig('hello.png', dpi=200)\n        input('>>> ')\n        \n        w_curr = round(w_curr * scale_up)\n    # end while\n    \n    # NOTE: for debug image output\n    # print mask\n    mask = to_dense(mask, ks, ws)[0]\n    x = skimage.measure.block_reduce(mask, (4, 4), np.max)\n    plt.imshow(x)\n    plt.savefig('hello.png', dpi=200)\n    \n    # print probabilites\n    x = np.matmul(queries[0], keys[0].transpose((-1, -2)))\n    x = x + (1 - np.tri(*x.shape)) * (-32000)\n    x = np.exp(x - x.max(-1, keepdims=True))\n    x = x / x.sum(-1, keepdims=True)\n    x = skimage.measure.block_reduce(x, (8, 8), np.max) ** 0.2\n    plt.imshow(x)\n    plt.savefig('hello_2.png', dpi=200)\n    # NOTE: end of debug output\n    \n    print(ks)\n    \n    return\n\ndef sparse_attention(q: ndarray, k: ndarray, v: ndarray, csr_mask: ndarray):\n    pass\n\ndef attention(q: Tensor, k: Tensor, v: Tensor):\n    assert q.ndim == 3\n    assert k.ndim == 3\n    assert v.ndim == 3\n    N, T_DST, HID = q.shape\n    _N, T_SRC, _HID = k.shape\n    assert k.shape[:-1] == v.shape[:-1]\n    assert N == _N\n    assert HID == _HID\n    \n    q = q.numpy()\n    k = k.numpy()\n    v = v.numpy()\n    csr_scores = mask(q, k)\n    # out = sparse_attention(csr_scores, v)\n    \n    # return out\n\nif __name__ == '__main__':\n    data_source = 'llama'\n    if data_source == 'llama':\n        state = torch.load('./cache/llama/qkvout.pth', map_location='cpu')\n        q = state['q']\n        k = state['k']\n        v = state['v']\n        N, H, T_DST, HID = q.shape\n        N, H, T_SRC, HID = k.shape\n        idx = 7\n        q = q.view(N*H, T_DST, HID)[idx:idx+1].contiguous()\n        k = k.view(N*H, T_SRC, HID)[idx:idx+1].contiguous()\n        v = v.view(N*H, T_SRC, HID)[idx:idx+1].contiguous()\n    else:\n        q = torch.randn((1, 64, 4))\n        k = torch.randn((1, 64, 4))\n        v = k.clone()\n    \n    print(q.shape, k.shape, v.shape)\n    out = attention(q, k, v)",
    "description": null,
    "url": null
}