{
    "acronym": "productkeymem",
    "title": "Large Memory Layers with Product Keys",
    "seed_ids": [],
    "s2id": "bf442ab269074665a68e4dbbe19e4efc97862541",
    "abstract": "This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.",
    "authors": [
        "Guillaume Lample",
        "Alexandre Sablayrolles",
        "Marc'Aurelio Ranzato",
        "Ludovic Denoyer",
        "H. J\u00e9gou"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "tldr": "A structured memory which can be easily integrated into a neural network and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead is introduced.",
    "citationCount": 113,
    "influentialCitationCount": 17,
    "code": "import math\nimport torch\nfrom torch import nn, einsum\n\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange, Reduce\n\nfrom torch.cuda.amp import autocast\n\nfrom collections import namedtuple\nimport torch.nn.functional as F\n\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\n@autocast(enabled = False)\ndef coor_descent(\n    s,\n    *,\n    n_iters,\n    k,\n    eps = 1e-1,\n    eps_init = None,\n    eps_decay = 1.,\n    mask = None\n):\n    \"\"\"\n    coordinate descent  - https://arxiv.org/abs/1502.04759, utilized in https://arxiv.org/abs/2303.09752\n    \u03b5-scaling           - https://arxiv.org/abs/1610.06519, utilized in https://arxiv.org/abs/2304.04947\n\n    in a follow up paper applying coordinate descent routing to efficient fine tuning\n    they were able to cut n_iters from 50 -> 20 by setting eps_init = 4 and eps_decay = 0.7\n    eps was dependent on the task, and ranged from 0.02 to 1\n    \"\"\"\n\n    assert n_iters > 0\n\n    mask_value = -torch.finfo(s.dtype).max\n\n    if not isinstance(k, torch.Tensor):\n        k = torch.Tensor([k]).to(s)\n    else:\n        k = rearrange(k, '... -> ... 1')\n\n    logk = log(k)\n\n    if exists(mask):\n        s = s.masked_fill(~mask, mask_value)\n\n    a = 0\n    b = -s\n\n    current_eps = max(default(eps_init, eps), eps)\n\n    for _ in range(n_iters):\n        sb = ((s + b) / current_eps)\n\n        if exists(mask):\n            sb = sb.masked_fill(~mask, mask_value)\n\n        a = current_eps * (logk - sb.logsumexp(dim = -1, keepdim = True))\n        b = -F.relu(s + a)\n\n        current_eps = max(current_eps * eps_decay, eps)\n\n    scores = ((s + a + b) / current_eps).exp()\n\n    if exists(mask):\n        scores = scores.masked_fill(~mask, 0.)\n\n    return scores\n\nTopkReturn = namedtuple('TopkReturn', ['values', 'indices', 'coor_descent_values', 'gates'])\n\n@autocast(enabled = False)\ndef topk(\n    x,\n    k,\n    coor_descent_k_ratio = 9 / 8,\n    n_iters = 20,\n    eps = 1e-1,\n    eps_init = None,\n    eps_decay = 1.,\n    mask = None,\n    fused = False,\n    non_differentiable = False\n):\n    \"\"\"\n    differentiable top-k on last dimension\n    \"\"\"\n\n    if non_differentiable:\n        values, indices = torch.topk(x, k = k, dim = -1)\n        return TopkReturn(values, indices, None, None)\n\n    assert coor_descent_k_ratio >= 1.\n    assert k > 0\n\n    # whether to used fused kernel or not\n\n    fn = coor_descent\n\n    if fused and x.is_cuda:\n        from colt5_attention.triton_coor_descent import triton_coor_descent\n        fn = triton_coor_descent\n\n    # do coordinate descent for gradients\n\n    coor_descent_out = fn(\n        x,\n        k = min(k * coor_descent_k_ratio, x.shape[-1]),   # fetch a bit more for better learning, as in CoLT5 paper (they fetched 9 / 8 times more)\n        mask = mask,\n        n_iters = n_iters,\n        eps = eps,\n        eps_init = eps_init,\n        eps_decay = eps_decay\n    )\n\n    # do straight through\n\n    gates = coor_descent_out + (1 - coor_descent_out).detach()\n\n    x = x * gates\n\n    # hard topk\n\n    values, indices = torch.topk(x, k, dim = -1)\n\n    # return something that looks like a usual topk, but now differentiable\n\n    coor_descent_values = coor_descent_out.gather(-1, indices)\n    gates = gates.gather(-1, indices)\n\n    return TopkReturn(values, indices, coor_descent_values, gates)\n\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\ndef gumbel_noise(t):\n    noise = torch.zeros_like(t).uniform_(0, 1)\n    return -log(-log(noise))\n\n# init\n\ndef init_(t, dim = None):\n    dim = default(dim, t.shape[-1])\n    std = 1. / math.sqrt(dim)\n    return nn.init.normal_(t, mean=0, std=std)\n\n# optimizer\n\ndef list_subtract(l, r):\n    return [el for el in l if el not in set(r)]\n\ndef fetch_pkm_value_parameters(module):\n    params = []\n    for m in module.modules():\n        if isinstance(m, PKM):\n            params.append(m.values.weight)\n    rest = list_subtract(module.parameters(), params)\n    return params, rest\n\ndef fetch_optimizer_parameters(module, pkm_learning_rate = 1e-2):\n    pkm_params, rest = fetch_pkm_value_parameters(module)\n    return [{'params': rest}, {'params': pkm_params, 'lr': pkm_learning_rate}]\n\n# norm\n\nclass MaskedBatchNorm1D(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(\n        self,\n        x,\n        mask = None\n    ):\n        if exists(mask):\n            initial_x = x\n            x = x[mask]\n\n        x = self.fn(x)\n\n        if exists(mask):\n            initial_x[mask] = x\n            x = initial_x\n\n        return x\n\nclass PKM(nn.Module):\n    def __init__(\n        self,\n        dim,\n        heads = 4,\n        num_keys = 128,\n        topk = 32,\n        dim_head = 128,\n        input_dropout = 0.,\n        query_dropout = 0.,\n        value_dropout = 0.,\n        attn_dropout = 0.,\n        use_layernorm = True,\n        pre_layernorm = False,\n        differentiable_topk = False,\n        concat_values_and_combine = False,\n        norm_output = False\n    ):\n        super().__init__()\n        self.topk = topk\n        self.heads = heads\n        self.num_keys = num_keys\n\n        dim_query = dim_head * heads * 2\n        self.to_queries = nn.Linear(dim, dim_query, bias = False)\n\n        # pre-layernorm pattern\n\n        self.pre_layernorm = nn.LayerNorm(dim) if pre_layernorm else nn.Identity()\n\n        # batchnorm would break causality\n\n        self.use_layernorm = use_layernorm\n\n        if use_layernorm:\n            self.norm = nn.LayerNorm(dim_head)\n        else:\n            self.norm = MaskedBatchNorm1D(nn.BatchNorm1d(dim_head))\n\n        # keys\n\n        self.keys = nn.Parameter(torch.zeros(heads, num_keys, 2, dim_head))\n        init_(self.keys)\n\n        # values\n\n        self.concat_values_and_combine = concat_values_and_combine\n\n        if concat_values_and_combine:\n            values = nn.Embedding(num_keys ** 2, dim_head)\n\n            self.values = nn.Sequential(\n                values,\n                Reduce('b (h k) d -> b h d', 'sum', h = heads),\n                Rearrange('b n d -> b (n d)'),\n                nn.Linear(dim_head * heads, dim, bias = False)\n            )\n        else:\n            values = nn.EmbeddingBag(num_keys ** 2, dim, mode = 'sum')\n            self.values = values\n\n\n        init_(values.weight)\n\n        # dropouts\n\n        self.input_dropout = nn.Dropout(input_dropout)\n        self.query_dropout = nn.Dropout(query_dropout)\n        self.value_dropout = nn.Dropout(value_dropout)\n        self.attn_dropout = nn.Dropout(attn_dropout)\n\n        # use a differentiable topk, based on coordinate descent\n\n        self.differentiable_topk = differentiable_topk\n\n        # https://arxiv.org/abs/2302.06461\n        # claims to boost performance of softmax key / value networks by simply layernorming the output\n\n        self.output_norm = nn.LayerNorm(dim) if norm_output else nn.Identity()\n\n    def forward(\n        self,\n        x,\n        input_mask = None,\n        gumbel_noise_scale = 0.,\n        **kwargs\n    ):\n        b, t, h = *x.shape[:2], self.heads\n\n        x = self.pre_layernorm(x)\n        x = self.input_dropout(x)\n\n        queries = self.to_queries(x)\n\n        # split out query heads\n\n        queries = rearrange(queries, 'b t (p h d) -> (b p h) t d', p = 2, h = h)\n\n        # norm and dropout queries\n\n        norm_kwargs = dict(mask = input_mask) if not self.use_layernorm else dict()\n        queries = self.norm(queries, **norm_kwargs)\n        queries = self.query_dropout(queries)\n\n        # ready queries\n\n        queries = rearrange(queries, '(b p h) t d -> p b t h d', p = 2, h = h)\n\n        # similarity to keys\n\n        dots = einsum('p b t h d, h n p d -> b t h p n', queries, self.keys)\n\n        # gumbel noise\n\n        if gumbel_noise_scale > 0.:\n            dots = dots + gumbel_noise(dots) * gumbel_noise_scale\n\n        # topk scores\n\n        if self.differentiable_topk:\n            scores, indices, *_ = topk(dots, k = self.topk, fused = True)\n        else:\n            scores, indices = dots.topk(k = self.topk, dim = -1)\n\n        # scores are factorized\n\n        (scores_x, scores_y), (indices_x, indices_y) = map(lambda t: t.chunk(2, dim = 3), (scores, indices))\n\n        all_topk = self.topk ** 2\n\n        all_scores = rearrange((\n            rearrange(scores_x, '... k -> ... k 1') +\n            rearrange(scores_y, '... k -> ... 1 k')\n        ), 'b t h ... -> b t h (...)')\n\n        all_indices = rearrange((\n            rearrange(indices_x, '... k -> ... k 1') * self.num_keys +\n            rearrange(indices_y, '... k -> ... 1 k')\n        ), 'b t h ... -> b t h (...)')\n\n        final_topk, final_indices = all_scores.topk(self.topk, dim=-1)\n        value_indices = all_indices.gather(-1, final_indices)\n\n        # attention\n\n        attn = final_topk.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n\n        value_indices, attn = map(lambda t: rearrange(t, 'b t h k -> (b t) (h k)'), (value_indices, attn))\n\n        # aggregate\n\n        if self.concat_values_and_combine:\n            out = self.values(value_indices)\n        else:\n            out = self.values(value_indices, per_sample_weights = attn)\n\n        out = self.value_dropout(out)\n\n        # maybe layernorm the output\n\n        out = self.output_norm(out)\n\n        return rearrange(out, '(b t) d -> b t d', b = b)",
    "description": null,
    "url": null
}