{
    "acronym": "sfa",
    "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
    "seed_ids": [
        "flashattn",
        "cosformer",
        "rfa",
        "bigbird",
        "lineartransformer",
        "longformer",
        "reformer",
        "sparsetransformer",
        "transformer",
        "gpt2"
    ],
    "s2id": "d203c764fb5dec2b053be667c8b06e516ea6ef10",
    "abstract": "Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.",
    "authors": [
        "Matteo Pagliardini",
        "Daniele Paliotta",
        "Martin Jaggi",
        "Franccois Fleuret"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.",
    "citationCount": 11,
    "influentialCitationCount": 0,
    "code": "\"\"\" Code 1: QK-Sparse Attention \"\"\"\n\nimport torch\nimport torch.nn.functional as F\n\nimport triton\nimport triton.language as tl\n\nimport math\n\n\n@triton.jit\ndef _fwd_kernel(#debug, sdz, sdh, sdm, sdn,\n    Q, K, V, sm_scale, \n    Out,\n    sqz, sqh, sqm, sqd, # shape = (Z,H,N_CTX_Q,D)\n    skz, skh, skn, skd, # shape = (Z,H,N_CTX_KV,D)\n    svz, svh, svn, svd, # shape = (Z,H,N_CTX_KV,D)\n    soz, soh, som, sod, # shape = (Z,H,N_CTX_Q,D)\n    Q_idx, K_idx, \n    sqiz, sqih, sqim,  # shape = (Z,H,N_CTX_Q)\n    skiz, skih, skin,  # shape = (Z,H,N_CTX_KV)\n    L, M,\n    Z, H, N_CTX_Q, N_CTX_KV, \n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, # will load BLOCK_M queries, and compute self attention by blocks of BLOCK_N keys\n    BLOCK_DMODEL: tl.constexpr # dimensionality of heads: D\n):\n    start_m = tl.program_id(0) # idx of sequence length chunk of size 128 (BLOCK_N)\n    off_hz = tl.program_id(1) # idx of head_batch (unique idx for each head in each batch)\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M) # indices of queries we want to process\n    offs_n = tl.arange(0, BLOCK_N) # indices of keys we want to process, we start from [0, BLOCK_N-1] and update in the loop\n    offs_d = tl.arange(0, BLOCK_DMODEL) # we want to process all the dimensions of a given head\n\n    offs_q = off_hz * sqh + offs_m[:, None] * sqm + offs_d[None, :] * sqd # Q.view(Z*H,N_CTX_Q,D)[off_hz, start_m*BLOCK_M:(start_m+1)*BLOCK_M, :].squeeze() that's a BLOCK_M*D matrix\n    offs_k = off_hz * skh + offs_n[None, :] * skn + offs_d[:, None] * skd # K.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].transpose(1,2).squeeze() that's a D*BLOCK_N matrix\n    offs_v = off_hz * svh + offs_n[:, None] * svn + offs_d[None, :] * svd # V.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].squeeze() that's a BLOCK_N*D matrix\n    offs_qi = off_hz * sqih + offs_m * sqim # Q_idx.view(Z*H,N_CTX_Q)[off_hz, start_m*BLOCK_M:(start_m+1)*BLOCK_M] a vector of BLOCK_M indices \n    offs_ki = off_hz * skih + offs_n * skin # K_idx.view(Z*H,N_CTX_KV)[off_hz, 0:BLOCK_N] a vector of BLOCK_N indices\n\n    # pointers to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    # Load values\n    qi_vals = tl.load(Q_idx + offs_qi, mask=offs_m < N_CTX_Q, other=-1)\n    q_vals = tl.load(Q + offs_q, mask=offs_m[:, None] < N_CTX_Q, other=0) \n    max_qi = tl.max(qi_vals, axis=0) # lagest query index in block\n\n    end_n = 0\n    for _ in range(0, N_CTX_KV, BLOCK_N):\n        ki_vals = tl.load(K_idx + offs_ki, mask=offs_n < N_CTX_KV, other=1e9)\n        min_ki = tl.min(ki_vals, axis=0)\n        if min_ki <= max_qi and min_ki != 1e9:\n            end_n += 1\n        offs_ki += BLOCK_N * skin\n        offs_n += BLOCK_N\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_ki = off_hz * skih + offs_n * skin\n\n    for _ in range(0, end_n):\n        \n        # Load values for K and K_idx\n        ki_vals = tl.load(K_idx + offs_ki, mask=offs_n < N_CTX_KV, other=1e9)\n        k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n\n        # compute qk\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.bfloat16)\n        qk += tl.dot(q_vals, k_vals)\n        qk *= sm_scale\n\n        # causal masking\n        qk = tl.where(qi_vals[:,None] >= ki_vals[None,:], qk, float(\"-inf\"))\n\n        # compute attention weights\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev) # compute new m\n        m_curr_ = tl.where(m_curr != float('-inf'), m_curr, float(0.0))\n        l_prev *= tl.exp(m_prev - m_curr_) # correct old l\n        p = tl.exp(qk - m_curr_[:, None])\n        l_curr = tl.sum(p, 1) + l_prev \n        l_rcp = 1. / l_curr # rescale operands of matmuls\n        l_rcp = tl.where((l_rcp == float('inf')), 0, l_rcp)\n        p *= l_rcp[:, None]\n        acc *= (l_prev * l_rcp)[:, None] # weight for each value vector\n\n        # update acc\n        p = p.to(Q.dtype.element_ty)\n        v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n        acc += tl.dot(p, v_vals) \n\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n\n        # update offsets\n        offs_n += BLOCK_N\n        offs_k += BLOCK_N * skn\n        offs_v += BLOCK_N * svn\n        offs_ki += BLOCK_N * skin\n\n    # store L and M\n    offs_L = off_hz * N_CTX_Q + offs_m # L is of shape (Z*H, N_CTX_Q), here we point to L[off_hz, start_m*Block_M:(start_m+1)*Block_M]\n    offs_M = off_hz * N_CTX_Q + offs_m\n    tl.store(L + offs_L, l_prev, mask=offs_m < N_CTX_Q)\n    tl.store(M + offs_M, m_prev, mask=offs_m < N_CTX_Q)\n    # store results to output\n    offs_o = off_hz * soh + offs_m[:, None] * som + offs_d[None, :] * sod\n    tl.store(Out + offs_o, acc, mask=offs_m[:, None] < N_CTX_Q)\n\n\n@triton.jit\ndef _bwd_preprocess(\n    Out, soz, soh, som, sod,\n    DO, L, slzh, slm,\n    NewDO, Delta, N_CTX_Q,\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_d = tl.arange(0, D_HEAD)\n    # load\n    off_o = off_hz * soh + off_m[:, None] * som + off_d[None, :] * sod\n    off_l = off_hz * slzh + off_m * slm\n    o = tl.load(Out + off_o, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    do = tl.load(DO + off_o, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    denom = tl.load(L + off_l, mask=off_m < N_CTX_Q, other=1.0).to(tl.float32)\n    denom = tl.where(denom == 0, 1.0, denom)\n    # compute\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    # write-back\n    tl.store(NewDO + off_o, do, mask=off_m[:, None] < N_CTX_Q)\n    tl.store(Delta + off_l, delta, mask=off_m < N_CTX_Q)\n\n\n@triton.jit\ndef _bwd_kernel(\n    Q, K, V, sm_scale, Out, DO,\n    DQ, DK, DV,\n    Q_idx, K_idx,\n    sqiz, sqih, sqim,  # shape = (Z,H,N_CTX_Q)\n    skiz, skih, skin,  # shape = (Z,H,N_CTX_KV)\n    L, M,\n    D,\n    sqz, sqh, sqm, sqd,\n    skz, skh, skn, skd,\n    svz, svh, svn, svd,\n    Z, H, N_CTX_Q, N_CTX_KV,\n    num_block_q, num_block_kv,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    \n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n\n    # offset pointers for batch/head\n    Q += off_z * sqz + off_h * sqh\n    K += off_z * skz + off_h * skh\n    V += off_z * svz + off_h * svh\n    DO += off_z * sqz + off_h * sqh\n    DQ += off_z * sqz + off_h * sqh\n    DK += off_z * skz + off_h * skh\n    DV += off_z * svz + off_h * svh\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    \n    # pointer to row-wise quantities in value-like data\n    D_ptrs = D + off_hz * N_CTX_Q # pointer to D.view(Z*H,N_CTX_Q)[off_hz]\n    m_ptrs = M + off_hz * N_CTX_Q # pointer to m.view(Z*H,N_CTX_Q)[off_hz]\n\n    for block_id_n in range(0, num_block_kv):\n\n        start_n = block_id_n * BLOCK_N\n        offs_n = start_n + tl.arange(0, BLOCK_N)\n\n        offs_ki = off_hz * skih + offs_n * skin\n        ki_vals = tl.load(K_idx + offs_ki, mask=offs_n < N_CTX_KV, other=1e9)\n        \n        # pointers for keys and values\n        k_ptrs = K + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        v_ptrs = V + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n\n        # initialize dv amd dk\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n        # Find start block for those keys \n        min_ki = tl.min(ki_vals, axis=0) # lagest query index in block\n        offs_m = tl.arange(0, BLOCK_M)\n        offs_qi = off_hz * sqih + offs_m * sqim \n\n        start_blockidx_m = 0\n        for _ in range(0, N_CTX_Q, BLOCK_M):\n            qi_vals = tl.load(Q_idx + offs_qi, mask=offs_m < N_CTX_Q, other=-1)\n            max_qi = tl.max(qi_vals, axis=0)\n            if max_qi < min_ki and max_qi != -1:\n                start_blockidx_m += 1\n            offs_qi += BLOCK_M * sqim\n            offs_m += BLOCK_M\n\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < N_CTX_KV)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < N_CTX_KV)\n\n        for start_m in range(start_blockidx_m * BLOCK_M, N_CTX_Q, BLOCK_M):\n            offs_m = (start_m + tl.arange(0, BLOCK_M))\n\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            qi_ptrs = Q_idx + (off_hz * sqih + offs_m * sqim)\n            \n            qi = tl.load(qi_ptrs, mask=offs_m < N_CTX_Q, other=-1)\n            q = tl.load(q_ptrs, mask=offs_m[:,None] < N_CTX_Q)\n            qk = tl.dot(q, tl.trans(k))\n            qk = tl.where((qi[:,None] >= ki_vals[None,:]), qk, float(\"-inf\"))\n\n            m = tl.load(m_ptrs + offs_m, mask=offs_m < N_CTX_Q)\n            m_ = tl.where(m != float('-inf'), m, 0.0)\n            p = tl.exp(qk * sm_scale - m_[:, None])\n\n            do = tl.load(do_ptrs, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dv\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n\n            Di = tl.load(D_ptrs + offs_m, mask=offs_m < N_CTX_Q)\n            # compute dp = dot(v, do)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, tl.trans(v))\n            # compute ds = p * (dp - delta[:, None])\n            ds = p * dp * sm_scale\n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n\n            dq = tl.load(dq_ptrs, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dq\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n            tl.store(dq_ptrs, dq, mask=offs_m[:, None] < N_CTX_Q)\n\n        # write-back\n        dv_ptrs = DV + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dk_ptrs = DK + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < N_CTX_KV)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < N_CTX_KV)\n\n\nclass _attention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, q_idx, k_idx, sm_scale):\n        # only support for Ampere now\n        capability = torch.cuda.get_device_capability()\n        if capability[0] < 8:\n            raise RuntimeError(\"Flash attention currently only supported for compute capability >= 80\")\n        BLOCK = 128\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        assert Lq == Lk and Lk == Lv\n        # assert Lk in {16, 32, 64, 128}\n        assert Lk in {64}  # TODO: fix other cases\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        num_warps = 4 if Lk <= 64 else 8\n\n        _fwd_kernel[grid](\n            q, k, v, sm_scale,\n            o,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n            q_idx, k_idx, \n            q_idx.stride(0), q_idx.stride(1), q_idx.stride(2), \n            k_idx.stride(0), k_idx.stride(1), k_idx.stride(2),\n            L, m,\n            q.shape[0], q.shape[1], q.shape[2], k.shape[2],\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n            num_warps=num_warps, num_stages=2\n        )\n\n        ctx.save_for_backward(q, k, v, o, L, m, q_idx, k_idx)\n        ctx.grid = grid\n        ctx.sm_scale = sm_scale\n        ctx.BLOCK_DMODEL = Lk\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        BLOCK = 128\n        q, k, v, o, l, m, q_idx, k_idx = ctx.saved_tensors\n\n        do = do.contiguous()\n        dq = torch.zeros_like(q, dtype=torch.float32)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n        do_scaled = torch.empty_like(do)\n        delta = torch.empty_like(l)\n        _bwd_preprocess[(ctx.grid[0], ctx.grid[1])](\n            o, o.stride(0), o.stride(1), o.stride(2), o.stride(3), do, l, l.stride(0), l.stride(1),\n            do_scaled, delta, q.shape[2],\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n        )\n        num_block_q = ctx.grid[0]\n        num_block_kv = math.ceil(k.shape[2] / BLOCK)\n        _bwd_kernel[(ctx.grid[1],)](\n            q, k, v, ctx.sm_scale,\n            o, do_scaled,\n            dq, dk, dv,\n            q_idx, k_idx,\n            q_idx.stride(0), q_idx.stride(1), q_idx.stride(2), \n            k_idx.stride(0), k_idx.stride(1), k_idx.stride(2),\n            l, m,\n            delta,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], k.shape[2],\n            num_block_q, num_block_kv,\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n            num_stages=1,\n        )\n        return dq, dk, dv, None, None, None\n\n\nattention = _attention.apply\n\n\ndef compact(alphas_mask, v, index=None):\n  \"\"\" v.shape = B, N_CTX, H, dim_head \n      alphas.shape = B, N_CTX, H\n  \"\"\"\n  B, T, H, dim_head = v.shape\n  if index is None:\n    with torch.no_grad():\n        indices_per_head = alphas_mask.sum(dim=-2) \n        buffer_size = indices_per_head.max().int() # first sum computes the num of non-killed elem per head, we take to max of that\n        # sorting: it is very important that the sorting is stable, else we cannot use causal masking\n        sorted = alphas_mask.sort(dim=-2, descending=True, stable=True) # sorted.indices.shape == (B x T x H) , now sorted over sequence T\n        index = sorted.indices[:,:buffer_size,:] # (B x buffer_size x H) expand indices to cover all the dimensions for each heads\n  else:\n    indices_per_head = None\n  compact_v = v.gather(dim=-3, index=index.unsqueeze(-1).expand(-1,-1,-1,dim_head)) # (B x buffer_size x H x dim_head) / expand indices to cover all the dimensions for each heads\n  return compact_v, index, indices_per_head\n\n\n@torch.no_grad()\ndef pad_index(index, indices_per_head, pad_idx=-1):\n  \"\"\" index.shape = B, buffer_size, H  <- index given by `compact`, represents for each batch and timestep the head idx it's originating from\n      indices_per_head.shape = B, H  <- for each head, number of \"active\" timesteps\n  \"\"\"\n  B, buffer_size, H = index.shape\n  index_copy = torch.clone(index).type(torch.int32)\n  mask = torch.arange(buffer_size, device=index.device).view(1,-1,1).expand(B,buffer_size,H) >= indices_per_head.view(B,1,-1)\n  index_copy[mask] = pad_idx\n  return index_copy\n\n\n\ndef attention_fn(q, k, v, sparsity=0.5):\n\n    BATCH, N_CTX, H, D_HEAD = q.shape\n\n    sm_scale = 1.0 / math.sqrt(D_HEAD)\n\n    alphas_q = (torch.rand((BATCH, N_CTX, H), dtype=torch.bfloat16, device=\"cuda\") > sparsity).float()\n    alphas_k = (torch.rand((BATCH, N_CTX, H), dtype=torch.bfloat16, device=\"cuda\") > sparsity).float()\n\n    # Building compact representations\n    q_c, index_q, iph_q = compact(alphas_q, q)\n    k_c, index_k, iph_k = compact(alphas_k, k)\n    v_c, _, _ = compact(alphas_k, v, index=index_k)\n\n    index_q_padded = pad_index(index_q, iph_q, pad_idx=-1) # (B, compact_T_q, nh)\n    index_k_padded = pad_index(index_k, iph_k, pad_idx=1e9) # (B, compact_T_k, nh)\n\n    compact_N_CTX_KV = k_c.shape[1]\n    compact_N_CTX_Q = q_c.shape[1]\n\n    # We need to transpose everything\n    q_c = q_c.view(BATCH, compact_N_CTX_Q, H, D_HEAD).transpose(1, 2).contiguous() # (BATCH, H, compact_N_CTX_Q, D_HEAD)\n    k_c = k_c.view(BATCH, compact_N_CTX_KV, H, D_HEAD).transpose(1, 2).contiguous() # (BATCH, H, compact_N_CTX_KV, D_HEAD)\n    v_c = v_c.view(BATCH, compact_N_CTX_KV, H, D_HEAD).transpose(1, 2).contiguous() # (BATCH, H, compact_N_CTX_KV, D_HEAD)\n    k_c = F.normalize(k_c, p=2, dim=-1).type(torch.bfloat16)\n    index_q_padded = index_q_padded.transpose(1, 2).contiguous() # (BATCH, H, compact_N_CTX_Q)\n    index_k_padded = index_k_padded.transpose(1, 2).contiguous() # (BATCH, H, compact_N_CTX_KV)\n\n    y_c = attention(q_c, k_c, v_c, index_q_padded, index_k_padded, sm_scale).transpose(1,2)\n    y = torch.zeros((BATCH, N_CTX, H, D_HEAD), dtype=torch.bfloat16, device='cuda').scatter(dim=1, index=index_q.long().view(BATCH,-1,H,1).expand(BATCH, -1, H, D_HEAD), src=y_c)\n\n    return y\n\n\n\n\"\"\" Code 2: Hash-Sparse Attention \"\"\"\n\n\n\nimport torch\nimport torch.nn.functional as F\n\nimport triton\nimport triton.language as tl\n\nimport math\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, sm_scale, \n    Out,\n    sqz, sqh, sqm, sqd, # shape = (Z,H,N_CTX_Q,D)\n    skz, skh, skn, skd, # shape = (Z,H,N_CTX_KV,D)\n    svz, svh, svn, svd, # shape = (Z,H,N_CTX_KV,D)\n    soz, soh, som, sod, # shape = (Z,H,N_CTX_Q,D)\n    Q_idx, K_idx, \n    sqiz, sqih, sqim,  # shape = (Z,H,N_CTX_Q)\n    skiz, skih, skin,  # shape = (Z,H,N_CTX_KV)\n    Q_hash, K_hash, \n    sqhz, sqhh, sqhm,  # shape = (Z,H,N_CTX_Q)\n    skhz, skhh, skhn,  # shape = (Z,H,N_CTX_KV)\n    L, M,\n    Z, H, N_CTX_Q, N_CTX_KV, \n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, # will load BLOCK_M queries, and compute self attention by blocks of BLOCK_N keys\n    BLOCK_DMODEL: tl.constexpr # dimensionality of heads: D\n):\n    start_m = tl.program_id(0) # idx of sequence length chunk of size 128 (BLOCK_N)\n    off_hz = tl.program_id(1) # idx of head_batch (unique idx for each head in each batch)\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M) # indices of queries we want to process\n    offs_n = tl.arange(0, BLOCK_N) # indices of keys we want to process, we start from [0, BLOCK_N-1] and update in the loop\n    offs_d = tl.arange(0, BLOCK_DMODEL) # we want to process all the dimensions of a given head\n\n    offs_q = off_hz * sqh + offs_m[:, None] * sqm + offs_d[None, :] * sqd # Q.view(Z*H,N_CTX_Q,D)[off_hz, start_m*BLOCK_M:(start_m+1)*BLOCK_M, :].squeeze() that's a BLOCK_M*D matrix\n    offs_qi = off_hz * sqih + offs_m * sqim # Q_idx.view(Z*H,N_CTX_Q)[off_hz, start_m*BLOCK_M:(start_m+1)*BLOCK_M] a vector of BLOCK_M indices \n    offs_qh = off_hz * sqhh + offs_m * sqhm  \n    offs_kh = off_hz * skhh + offs_n * skhn \n\n    # pointers to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\") \n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    # Load values\n    qi_vals = tl.load(Q_idx + offs_qi, mask=offs_m < N_CTX_Q, other=-1)\n    q_vals = tl.load(Q + offs_q, mask=qi_vals[:, None] >= 0) \n    qh_vals = tl.load(Q_hash + offs_qh, mask=offs_m < N_CTX_Q, other=1e9) #, other=-1)\n    min_q_hash = tl.min(qh_vals, axis=0)\n    qh_vals = tl.where(offs_m < N_CTX_Q, qh_vals, -1)\n    max_q_hash = tl.max(qh_vals, axis=0)\n\n    end_n = 0\n    start_n = 0\n\n    # Increment the start and end to find start and end blocks \n    for _ in range(0, N_CTX_KV, BLOCK_N):\n        kh_vals = tl.load(K_hash + offs_kh, mask=offs_n < N_CTX_KV, other=+1e9)\n        min_kh = tl.min(kh_vals, axis=0)\n        if min_kh <= max_q_hash and min_kh != 1e9:\n            end_n += 1\n        kh_vals = tl.where(offs_n < N_CTX_KV, kh_vals, -1e9)\n        max_kh = tl.max(kh_vals, axis=0)\n        if max_kh < min_q_hash and max_kh != -1e9:\n            start_n += 1\n        offs_n += BLOCK_N\n        offs_kh += BLOCK_N * skhn\n\n    # remove unecessary trailing blocks based on causal structure\n    causal_end_n = end_n\n    offs_n = BLOCK_N * start_n + tl.arange(0, BLOCK_N)\n    offs_ki = off_hz * skih + offs_n * skin\n    max_qi = tl.max(qi_vals, axis=0) # largest query index in block\n    for i in range(start_n, end_n):\n        ki_vals = tl.load(K_idx + offs_ki, mask=offs_n < N_CTX_KV, other=1e9)\n        min_ki = tl.min(ki_vals, axis=0)\n        if min_ki <= max_qi and min_ki != 1e9:\n            causal_end_n = i + 1\n        offs_ki += BLOCK_N * skin\n        offs_n += BLOCK_N\n\n    # re-initialize offsets\n    offs_n = BLOCK_N * start_n + tl.arange(0, BLOCK_N) # indices of keys we want to process, we start from [0, BLOCK_N-1] and update in the loop\n    offs_k = off_hz * skh + offs_n[None, :] * skn + offs_d[:, None] * skd # K.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].transpose(1,2).squeeze() that's a D*BLOCK_N matrix\n    offs_v = off_hz * svh + offs_n[:, None] * svn + offs_d[None, :] * svd # V.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].squeeze() that's a BLOCK_N*D matrix\n    offs_ki = off_hz * skih + offs_n * skin # K_idx.view(Z*H,N_CTX_KV)[off_hz, 0:BLOCK_N] a vector of BLOCK_N indices\n    offs_kh = off_hz * skhh + offs_n * skhn \n\n    for _ in range(start_n, causal_end_n):\n        \n        # Load values for K and K_idx\n        ki_vals = tl.load(K_idx + offs_ki, mask=offs_n < N_CTX_KV, other=1e9)\n        kh_vals = tl.load(K_hash + offs_kh, mask=offs_n < N_CTX_KV, other=-1e9)\n        k_vals = tl.load(K + offs_k, mask=ki_vals[None, :] < 1e9)\n\n        # compute qk\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.bfloat16)\n        qk += tl.dot(q_vals, k_vals)\n        qk *= sm_scale\n\n        # causal masking\n        qk = tl.where((qi_vals[:,None] > ki_vals[None,:]) & (qh_vals[:,None] == kh_vals[None,:]), qk, float(\"-inf\"))\n\n        # compute attention weights\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev) # compute new m\n        m_curr_ = tl.where(m_curr != float('-inf'), m_curr, float(0.0))\n        l_prev *= tl.exp(m_prev - m_curr_) # correct old l\n        p = tl.exp(qk - m_curr_[:, None])\n        l_curr = tl.sum(p, 1) + l_prev \n        l_rcp = 1. / l_curr # rescale operands of matmuls\n        l_rcp = tl.where((l_rcp == float('inf')), 0, l_rcp)\n        p *= l_rcp[:, None]\n        acc *= (l_prev * l_rcp)[:, None] # weight for each value vector\n\n        # update acc\n        p = p.to(Q.dtype.element_ty)\n        v_vals = tl.load(V + offs_v, mask=ki_vals[:, None] < 1e9, other=0)\n        acc += tl.dot(p, v_vals)\n\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n\n        # update offsets\n        offs_n += BLOCK_N\n        offs_k += BLOCK_N * skn\n        offs_v += BLOCK_N * svn\n        offs_ki += BLOCK_N * skin\n        offs_kh += BLOCK_N * skhn\n\n    # store L and M\n    offs_L = off_hz * N_CTX_Q + offs_m # L is of shape (Z*H, N_CTX_Q), here we point to L[off_hz, start_m*Block_M:(start_m+1)*Block_M]\n    offs_M = off_hz * N_CTX_Q + offs_m\n    tl.store(L + offs_L, l_prev, mask=offs_m < N_CTX_Q)\n    tl.store(M + offs_M, m_prev, mask=offs_m < N_CTX_Q)\n    # store results to output\n    offs_o = off_hz * soh + offs_m[:, None] * som + offs_d[None, :] * sod\n    tl.store(Out + offs_o, acc, mask=offs_m[:, None] < N_CTX_Q)\n\n\n@triton.jit\ndef _bwd_preprocess(\n    Out, soz, soh, som, sod,\n    DO, L, slzh, slm,\n    NewDO, Delta, N_CTX_Q,\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_d = tl.arange(0, D_HEAD)\n    # load\n    off_o = off_hz * soh + off_m[:, None] * som + off_d[None, :] * sod\n    off_l = off_hz * slzh + off_m * slm\n    o = tl.load(Out + off_o, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    do = tl.load(DO + off_o, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    denom = tl.load(L + off_l, mask=off_m < N_CTX_Q, other=1.0).to(tl.float32)\n    denom = tl.where(denom == 0, 1.0, denom)\n    # compute\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    # write-back\n    tl.store(NewDO + off_o, do, mask=off_m[:, None] < N_CTX_Q)\n    tl.store(Delta + off_l, delta, mask=off_m < N_CTX_Q)\n\n\n@triton.jit\ndef _bwd_kernel(\n    Q, K, V, sm_scale, Out, DO,\n    DQ, DK, DV,\n    Q_idx, K_idx,\n    sqiz, sqih, sqim,  # shape = (Z,H,N_CTX_Q)\n    skiz, skih, skin,  # shape = (Z,H,N_CTX_KV)\n    Q_hash, K_hash,\n    sqhz, sqhh, sqhm,  # shape = (Z,H,N_CTX_Q)\n    skhz, skhh, skhn,  # shape = (Z,H,N_CTX_KV)\n    L, M,\n    D,\n    sqz, sqh, sqm, sqd,\n    skz, skh, skn, skd,\n    svz, svh, svn, svd,\n    Z, H, N_CTX_Q, N_CTX_KV,\n    num_block_q, num_block_kv,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    \n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n\n    # offset pointers for batch/head\n    Q += off_z * sqz + off_h * sqh\n    K += off_z * skz + off_h * skh\n    V += off_z * svz + off_h * svh\n    DO += off_z * sqz + off_h * sqh\n    DQ += off_z * sqz + off_h * sqh\n    DK += off_z * skz + off_h * skh\n    DV += off_z * svz + off_h * svh\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    \n    # pointer to row-wise quantities in value-like data\n    D_ptrs = D + off_hz * N_CTX_Q # pointer to D.view(Z*H,N_CTX_Q)[off_hz]\n    m_ptrs = M + off_hz * N_CTX_Q # pointer to m.view(Z*H,N_CTX_Q)[off_hz]\n\n    for block_id_n in range(0, num_block_kv):\n\n        start_n = block_id_n * BLOCK_N\n        offs_n = start_n + tl.arange(0, BLOCK_N)\n\n        offs_ki = off_hz * skih + offs_n * skin\n        ki_vals = tl.load(K_idx + offs_ki, mask=offs_n < N_CTX_KV, other=1e9)\n        min_ki = tl.min(ki_vals, axis=0)\n        ki_vals = tl.where(offs_n < N_CTX_KV, ki_vals, -1)\n        \n        # pointers for keys and values\n        k_ptrs = K + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        v_ptrs = V + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n\n        # initialize dv amd dk\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n        # Find start and end block for those keys \n        offs_kh = off_hz * skhh + offs_n * skhn\n        kh_vals = tl.load(K_hash + offs_kh, mask=offs_n < N_CTX_KV, other=1e9)\n        \n        min_k_hash = tl.min(kh_vals, axis=0)\n        kh_vals = tl.where(offs_n < N_CTX_KV, kh_vals, -1)\n        max_k_hash = tl.max(kh_vals, axis=0)\n\n        start_blockidx_m = 0\n        end_blockidx_m = 0\n\n        # Increment the start and end to find start and end blocks \n        offs_m = tl.arange(0, BLOCK_M)\n        offs_qh = off_hz * sqhh + offs_m * sqhm \n        for _ in range(0, N_CTX_Q, BLOCK_M):\n            qh_vals = tl.load(Q_hash + offs_qh, mask=offs_m < N_CTX_Q, other=+1e9)\n            min_qh = tl.min(qh_vals, axis=0)\n            if min_qh <= max_k_hash and min_qh != 1e9:\n                end_blockidx_m += 1\n            qh_vals = tl.where(offs_m < N_CTX_Q, qh_vals, -1e9)\n            max_qh = tl.max(qh_vals, axis=0)\n            if max_qh < min_k_hash and max_qh != -1e9:\n                start_blockidx_m += 1\n            offs_m += BLOCK_M\n            offs_qh += BLOCK_M * sqhm\n\n        # remove unecessary trailing blocks based on causal structure\n        causal_start_n = start_blockidx_m\n        offs_m = BLOCK_M * start_blockidx_m + tl.arange(0, BLOCK_M)\n        offs_qi = off_hz * sqih + offs_m * sqim\n        for i in range(start_blockidx_m, end_blockidx_m):\n            qi_vals = tl.load(Q_idx + offs_qi, mask=offs_m < N_CTX_Q, other=-1)\n            max_qi = tl.max(qi_vals, axis=0)\n            if max_qi < min_ki and max_qi != -1:\n                causal_start_n = i + 1\n            offs_qi += BLOCK_N * skin\n            offs_m += BLOCK_N\n\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < N_CTX_KV)  \n        v = tl.load(v_ptrs, mask=offs_n[:, None] < N_CTX_KV)  \n\n        for start_m in range(causal_start_n * BLOCK_M, end_blockidx_m * BLOCK_M, BLOCK_M):\n            offs_m = (start_m + tl.arange(0, BLOCK_M))\n\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            qi_ptrs = Q_idx + (off_hz * sqih + offs_m * sqim)\n            qh_ptrs = Q_hash + (off_hz * sqhh  + offs_m * sqhm)\n            \n            qi = tl.load(qi_ptrs, mask=offs_m < N_CTX_Q, other=1e9)\n            qh = tl.load(qh_ptrs, mask=offs_m < N_CTX_Q, other=1e9)\n            q = tl.load(q_ptrs, mask=offs_m[:,None] < N_CTX_Q)  \n            qk = tl.dot(q, tl.trans(k))\n            qk = tl.where((qi[:,None] > ki_vals[None,:]) & (qh[:,None] == kh_vals[None,:]), qk, float(\"-inf\"))\n\n            m = tl.load(m_ptrs + offs_m, mask=offs_m < N_CTX_Q)\n            m_ = tl.where(m != float('-inf'), m, 0.0)\n            p = tl.exp(qk * sm_scale - m_[:, None])\n\n            do = tl.load(do_ptrs, mask=offs_m[:,None] < N_CTX_Q)  \n            # compute dv\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n\n            Di = tl.load(D_ptrs + offs_m, mask=offs_m < N_CTX_Q)\n            # compute dp = dot(v, do)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, tl.trans(v))\n            # compute ds = p * (dp - delta[:, None])\n            ds = p * dp * sm_scale\n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n\n            dq = tl.load(dq_ptrs)\n            # compute dq\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n            tl.store(dq_ptrs, dq, mask=offs_m[:, None] < N_CTX_Q)\n\n        # write-back\n        dv_ptrs = DV + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dk_ptrs = DK + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < N_CTX_KV)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < N_CTX_KV)\n\n\nclass _attention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, q_idx, k_idx, q_hash, k_hash, sm_scale):\n        # only support for Ampere now\n        capability = torch.cuda.get_device_capability()\n        if capability[0] < 8:\n            raise RuntimeError(\"Flash attention currently only supported for compute capability >= 80\")\n        BLOCK = 128\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        assert Lq == Lk and Lk == Lv\n        # assert Lk in {16, 32, 64, 128}\n        assert Lk in {64}  # TODO: fix other cases\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        num_warps = 4 if Lk <= 64 else 8\n\n        _fwd_kernel[grid](\n            q, k, v, sm_scale,\n            o,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n            q_idx, k_idx, \n            q_idx.stride(0), q_idx.stride(1), q_idx.stride(2), \n            k_idx.stride(0), k_idx.stride(1), k_idx.stride(2),\n            q_hash, k_hash, \n            q_hash.stride(0), q_hash.stride(1), q_hash.stride(2), \n            k_hash.stride(0), k_hash.stride(1), k_hash.stride(2),\n            L, m,\n            q.shape[0], q.shape[1], N_CTX_Q=q.shape[2], N_CTX_KV=k.shape[2],\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n            num_warps=num_warps, num_stages=2\n        )\n\n        ctx.save_for_backward(q, k, v, o, L, m, q_idx, k_idx, q_hash, k_hash)\n        ctx.grid = grid\n        ctx.sm_scale = sm_scale\n        ctx.BLOCK_DMODEL = Lk\n        return o\n\n\n    @staticmethod\n    def backward(ctx, do):\n        BLOCK = 128\n        q, k, v, o, l, m, q_idx, k_idx, q_hash, k_hash = ctx.saved_tensors\n\n        do = do.contiguous()\n        dq = torch.zeros_like(q, dtype=torch.float32)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n        do_scaled = torch.empty_like(do)\n        delta = torch.empty_like(l)\n        _bwd_preprocess[(ctx.grid[0], ctx.grid[1])](\n            o, o.stride(0), o.stride(1), o.stride(2), o.stride(3), do, l, l.stride(0), l.stride(1),\n            do_scaled, delta, q.shape[2],\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n        )\n\n        num_block_q = ctx.grid[0]\n        num_block_kv = math.ceil(k.shape[2] / BLOCK)\n\n        _bwd_kernel[(ctx.grid[1],)](\n            q, k, v, ctx.sm_scale,\n            o, do_scaled,\n            dq, dk, dv,\n            q_idx, k_idx,\n            q_idx.stride(0), q_idx.stride(1), q_idx.stride(2), \n            k_idx.stride(0), k_idx.stride(1), k_idx.stride(2),\n            q_hash, k_hash,\n            q_hash.stride(0), q_hash.stride(1), q_hash.stride(2), \n            k_hash.stride(0), k_hash.stride(1), k_hash.stride(2),\n            l, m,\n            delta,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], k.shape[2],\n            num_block_q, num_block_kv,\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n            num_stages=1,\n        )\n        return dq, dk, dv, None, None, None, None, None\n    \n\nattention = _attention.apply\n\n\ndef attention_fn(\n    Q,\n    K,\n    V,\n    nb_hash=8,\n    hashes_per_head=False,\n    attention_dropout=None,\n):\n    batch_size, nb_heads, nb_Q, dim_K = Q.size()\n    _, _, nb_K, dim_V = V.size()\n\n    q_idx = (\n        torch.arange(0, nb_Q, dtype=torch.int32, device=Q.device)\n        .view(1, 1, -1)\n        .expand((batch_size, nb_heads, -1))\n    )\n    k_idx = (\n        torch.arange(0, nb_K, dtype=torch.int32, device=Q.device)\n        .view(1, 1, -1)\n        .expand((batch_size, nb_heads, -1))\n    )\n\n    # Compute the hashing\n    rand_matrix = torch.randn(\n        [1, nb_heads if hashes_per_head else 1, dim_K, nb_hash // 2],\n        device=Q.device,\n        dtype=Q.dtype,\n    ).expand(batch_size, nb_heads, -1, -1)\n    rand_matrix = rand_matrix / torch.norm(rand_matrix, p=2, dim=-2, keepdim=True)\n\n    matmul_Q = torch.einsum(\"bhid,bhdl->bhil\", Q, rand_matrix)\n    matmul_K = torch.einsum(\"bhid,bhdl->bhil\", K, rand_matrix)\n\n    hash_Q = torch.argmax(torch.cat([matmul_Q, -matmul_Q], dim=-1), dim=-1)\n    hash_K = torch.argmax(torch.cat([matmul_K, -matmul_K], dim=-1), dim=-1)\n\n    sorted_Q = hash_Q.sort(dim=-1, stable=True)\n    sorted_K = hash_K.sort(dim=-1, stable=True)\n\n    q_idx = torch.gather(q_idx, dim=-1, index=sorted_Q.indices).contiguous()\n    k_idx = torch.gather(k_idx, dim=-1, index=sorted_K.indices).contiguous()\n\n    q_hash = sorted_Q.values.contiguous()\n    k_hash = sorted_K.values.contiguous()\n\n    sm_scale = 1.0 / math.sqrt(Q.size(-1))\n\n    Q = torch.gather(\n        Q, dim=-2, index=sorted_Q.indices.unsqueeze(-1).expand_as(Q)\n    ).contiguous()\n    K = torch.gather(\n        K, dim=-2, index=sorted_K.indices.unsqueeze(-1).expand_as(K)\n    ).contiguous()\n    V = torch.gather(\n        V, dim=-2, index=sorted_K.indices.unsqueeze(-1).expand_as(V)\n    ).contiguous()\n\n    y = attention(Q, K, V, q_idx, k_idx, q_hash, k_hash, sm_scale)\n\n    # reoder y to original query order\n    y_reordered = y.new_zeros(\n        (batch_size, nb_heads, nb_Q, y.size(-1)),\n    ).scatter(\n        dim=-2,\n        index=q_idx.long().unsqueeze(-1).expand(-1, -1, -1, y.size(-1)),\n        src=y,\n    )\n\n    return y_reordered\n\n",
    "description": null,
    "url": null
}