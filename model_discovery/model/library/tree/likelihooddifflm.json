{
    "acronym": "likelihooddifflm",
    "title": "Likelihood-Based Diffusion Language Models ",
    "seed_ids": [
        "contdiffu",
        "selfcondembdiffu",
        "analogbits",
        "diffusionlm",
        "flashattn",
        "gpt2"
    ],
    "s2id": "d9ffb44ee3c8ec0b6692df8a90451384c1edd89b",
    "abstract": "Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.",
    "authors": [
        "Ishaan Gulrajani",
        "Tatsunori Hashimoto"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "This work takes the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autore progressive model.",
    "citationCount": 16,
    "influentialCitationCount": 5,
    "code": "import apex.normalization\nimport flash_attn.flash_attn_interface\nimport flash_attn.ops.fused_dense\nimport lib.utils\nimport mup\nimport numpy as np\nimport lib.rotary\nimport torch\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom torch import nn, optim\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones([dim]))\n        self.dim = dim\n    def forward(self, x):\n        with torch.cuda.amp.autocast(enabled=False):\n            x = F.layer_norm(x.float(), [self.dim])\n        return x * self.weight[None,None,:]\n\ndef residual_linear(x, W, x_skip, residual_scale):\n    \"\"\"x_skip + residual_scale * W @ x\"\"\"\n    dim_out, dim_in = W.shape[0], W.shape[1]\n    return torch.addmm(\n        x_skip.view(-1, dim_out),\n        x.view(-1, dim_in),\n        W.T,\n        alpha=residual_scale\n    ).view(*x.shape[:-1], dim_out)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, n_heads, causal, residual_scale):\n        super().__init__()\n\n        self.causal = causal\n        self.dim = dim\n        self.n_heads = n_heads\n        self.residual_scale = residual_scale\n\n        self.rmsnorm1 = apex.normalization.FusedRMSNorm(dim)\n        self.attn_qkv = nn.Linear(dim, 3*dim, bias=False)\n        self.attn_out = nn.Linear(dim, dim, bias=False)\n\n        self.rmsnorm2 = apex.normalization.FusedRMSNorm(dim)\n        self.mlp = flash_attn.ops.fused_dense.FusedMLP(\n            dim, 4*dim, bias1=False, bias2=False, checkpoint_lvl=1)\n\n    def forward(self, x, rotary_cos_sin, cu_seqlens=None):\n        batch_size, seq_len = x.shape[0], x.shape[1]\n\n        # Self-attention block\n        x_skip = x\n        x = self.rmsnorm1(x)\n        qkv = self.attn_qkv(x)\n        qkv = rearrange(\n            qkv,\n            'b s (three h d) -> b s three h d',\n            three=3, h=self.n_heads\n        )\n        half_dtype = qkv.dtype\n        with torch.cuda.amp.autocast(enabled=False):\n            cos, sin = rotary_cos_sin\n            qkv = lib.rotary.apply_rotary_pos_emb(\n                qkv, cos.to(half_dtype), sin.to(half_dtype)\n            )\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        if cu_seqlens is None:\n            cu_seqlens = torch.arange(\n                0, (batch_size + 1) * seq_len, step=seq_len,\n                dtype=torch.int32, device=qkv.device\n            )\n        x = flash_attn.flash_attn_interface.flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_seqlens, seq_len, 0., causal=self.causal)\n        x = rearrange(x, '(b s) h d -> b s (h d)', b=batch_size)\n        x = residual_linear(\n            x, self.attn_out.weight, x_skip, self.residual_scale\n        )\n\n        # Feedforward block\n        x_skip = x\n        x = self.rmsnorm2(x)\n        x = self.mlp(x)\n        x = torch.add(x_skip, x, alpha=self.residual_scale)\n\n        return x\n\nclass EmbeddingMatrix(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.matrix = nn.Parameter(torch.randn(vocab_size, embed_dim))\n        self.matrix.data /= self.matrix.data.norm(p=2, dim=1, keepdim=True)\n    def forward(self):\n        norm = torch.linalg.norm(self.matrix, dim=1, keepdim=True)\n        return (self.matrix / (norm + 1e-8))\n\nclass NoiseSchedule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.W1 = nn.Parameter(torch.randn(1024, 1))\n        self.b1 = nn.Parameter(torch.randn(1024))\n        self.W2 = nn.Parameter(torch.randn(1, 1024))\n    def forward(self, t):\n        \"\"\"t.shape: [n]\"\"\"\n        W1 = F.softplus(self.W1.double())\n        W2 = 0.01 * F.softplus(self.W2.double())\n        def gamma_tilde(t):\n            h = t[:,None] - 0.5\n            h = (h @ W1.T) + self.b1[None,:].double()\n            h = torch.tanh(h)\n            h = (h @ W2.T)[:,0]\n            return h\n        gamma_tilde_0 = gamma_tilde(torch.tensor([0.], device='cuda'))\n        gamma_tilde_1 = gamma_tilde(torch.tensor([1.], device='cuda'))\n        gamma_tilde_t = gamma_tilde(t)\n        return (\n            (gamma_tilde_t - gamma_tilde_0) /\n            (gamma_tilde_1 - gamma_tilde_0)\n        )\n\nclass GammaBounds(nn.Module):\n    def __init__(self, gamma_0, gamma_1):\n        super().__init__()\n        self.gamma_0 = nn.Parameter(torch.tensor(float(gamma_0)))\n        self.gamma_1 = nn.Parameter(torch.tensor(float(gamma_1)))\n    def forward(self):\n        return self.gamma_0.clone().double(), self.gamma_1.clone().double()\n\nclass DiffusionModel(nn.Module):\n    def __init__(self, dim, embed_dim, n_blocks, n_heads, vocab_size):\n        super().__init__()\n\n        self.input_linear = nn.Linear(embed_dim, dim, bias=False)\n        self.selfcond_linear = nn.Linear(embed_dim, dim, bias=False)\n        self.selfcond_linear.weight.data.zero_()\n        self.gamma_linear = nn.Linear(64, dim, bias=False)\n        self.gamma_linear.weight.data.zero_()\n\n        self.rotary_emb = lib.rotary.Rotary(dim // n_heads)\n\n        residual_scale = float(1./np.sqrt(n_blocks))\n        self.blocks = nn.ModuleList([\n            TransformerBlock(dim, n_heads, False, residual_scale)\n            for i in range(n_blocks)\n        ])\n\n        self.output_norm = lib.models.LayerNorm(dim)\n        self.output_linear = mup.MuReadout(dim, vocab_size)\n        self.output_linear.weight.data.zero_()\n        self.output_linear.bias.data.zero_()\n\n        self.dim = dim\n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n\n    def forward(self, z, gamma, embedding_matrix, bias_scale, x_selfcond,\n        selfcond_mask=None, cu_seqlens=None):\n\n        if selfcond_mask is None:\n            selfcond_mask = torch.ones(z.shape[0], device='cuda')\n\n        alpha_squared = torch.sigmoid(-gamma)[:,None,None]\n        sigma_squared = torch.sigmoid(gamma)[:,None,None]\n        alpha = alpha_squared.sqrt()\n\n        # Rescale input to stdev 1\n        z_variance = (alpha_squared / self.embed_dim) + sigma_squared\n        x = z / z_variance.sqrt().float()\n\n        x = self.input_linear(x)\n\n        x = x + self.selfcond_linear(\n            x_selfcond * float(np.sqrt(self.embed_dim))\n        )\n\n        gamma_embed = torch.linspace(-5., 5., 64 // 2, device='cuda')\n        gamma_embed = gamma_embed.exp()[None,:] * gamma[:,None]\n        gamma_embed = torch.cat([gamma_embed.sin(), gamma_embed.cos()], dim=1)\n        gamma_embed = self.gamma_linear(gamma_embed.float())[:,None,:]\n        x = x + gamma_embed\n\n        rotary_cos_sin = self.rotary_emb(x)\n        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n            for i in range(len(self.blocks)):\n                x = self.blocks[i](x, rotary_cos_sin, cu_seqlens=cu_seqlens)\n\n        x = self.output_norm(x.float())\n\n        x *= self.output_linear.output_mult/self.output_linear.width_mult()\n\n        W = torch.cat([\n            self.output_linear.weight.T,\n            embedding_matrix.T,\n            embedding_matrix.T.detach()\n        ], dim=0)\n        z_scaled_for_bias = bias_scale * (alpha/sigma_squared).float() * z\n        x = torch.cat([\n            x,\n            z_scaled_for_bias * (1 - selfcond_mask.float()[:,None,None]),\n            z_scaled_for_bias * selfcond_mask.float()[:,None,None]\n        ], dim=2)\n        logits = torch.addmm(\n            self.output_linear.bias.view(1, self.vocab_size),\n            x.view(-1, self.dim + 2*self.embed_dim),\n            W.view(self.dim + 2*self.embed_dim, self.vocab_size)\n        ).view(x.shape[0], x.shape[1], self.vocab_size)\n\n        # Comment for 'no categorical reparameterization' ablation\n        x_reconst = F.softmax(logits, dim=2)\n        x_reconst = x_reconst @ torch.cat([\n            embedding_matrix, embedding_matrix.detach()], dim=1)\n        x_reconst = torch.lerp(\n            x_reconst[:,:,:self.embed_dim],\n            x_reconst[:,:,self.embed_dim:],\n            selfcond_mask.float()[:,None,None]\n        )\n\n        return logits, x_reconst\n\nclass AutoregressiveModel(nn.Module):\n    def __init__(self, dim, n_blocks, n_heads, vocab_size, tie_embeddings):\n        super().__init__()\n        self.tie_embeddings = tie_embeddings\n        if not tie_embeddings:\n            self.input_embedding = nn.Embedding(vocab_size, dim)\n        self.rotary_emb = lib.rotary.Rotary(dim // n_heads)\n\n        residual_scale = float(1./np.sqrt(n_blocks))\n        self.blocks = nn.ModuleList([\n            TransformerBlock(dim, n_heads, True, residual_scale)\n            for i in range(n_blocks)\n        ])\n        self.output_norm = apex.normalization.FusedRMSNorm(dim)\n        self.output_linear = mup.MuReadout(dim, vocab_size)\n        self.first_token_logits = nn.Parameter(torch.zeros(vocab_size))\n\n    def forward(self, x):\n        if self.tie_embeddings:\n            x = F.embedding(x, self.output_linear.weight) * float(np.sqrt(3*256))\n        else:\n            x = self.input_embedding(x)\n        rotary_cos_sin = self.rotary_emb(x)\n        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n            for i in range(len(self.blocks)):\n                x = self.blocks[i](x, rotary_cos_sin)\n        x = x.float()\n        x = self.output_norm(x)\n        logits = self.output_linear(x)\n        logits = torch.cat([\n            self.first_token_logits[None,None,:].expand(x.shape[0],-1,-1),\n            logits[:,:-1,:]\n        ], dim=1)\n        return logits",
    "description": null,
    "url": null
}