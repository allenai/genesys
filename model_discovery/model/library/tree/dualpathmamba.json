{
    "acronym": "dualpathmamba",
    "title": "Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation",
    "seed_ids": [
        "mamba"
    ],
    "s2id": "da9178eae82d1ca5492aaecd0151ba49481cb8b1",
    "abstract": "Transformers have been the most successful architecture for various speech modeling tasks, including speech separation. However, the self-attention mechanism in transformers with quadratic complexity is inefficient in computation and memory. Recent models incorporate new layers and modules along with transformers for better performance but also introduce extra model complexity. In this work, we replace transformers with Mamba, a selective state space model, for speech separation. We propose dual-path Mamba, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces. Our experimental results on the WSJ0-2mix data show that our dual-path Mamba models of comparably smaller sizes outperform state-of-the-art RNN model DPRNN, CNN model WaveSplit, and transformer model Sepformer. Code: https://github.com/xi-j/Mamba-TasNet",
    "authors": [
        "Xilin Jiang",
        "Cong Han",
        "N. Mesgarani"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work replaces transformers with Mamba, a selective state space model, for speech separation, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces.",
    "citationCount": 13,
    "influentialCitationCount": 1,
    "code": "'''\nCopied and modified from speechbrain.lobes.models.dual_path\nSkip around every dual path block (intra+inter).\n'''\nimport torch\n\nfrom speechbrain.lobes.models.dual_path import Dual_Path_Model\n\n\nclass SafeGroupNorm(torch.nn.GroupNorm):\n    def forward(self, x):\n        # t = x.dtype\n        # x = x.type(torch.float32)\n        print(\"LayerNorm:\", x.max(), torch.isnan(x).sum()/x.numel())\n        return super().forward(x)\n\nclass Dual_Path_Model_Skip(Dual_Path_Model):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        intra_model,\n        inter_model,\n        num_layers=1,\n        norm=\"ln\",\n        K=200,\n        num_spks=2,\n        skip_around_intra=True,\n        skip_n_block=0,\n        linear_layer_after_inter_intra=True,\n        use_global_pos_enc=False,\n        max_length=20000,\n    ):\n        super(Dual_Path_Model_Skip, self).__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            intra_model=intra_model,\n            inter_model=inter_model,\n            num_layers=num_layers,\n            norm=norm,\n            K=K,\n            num_spks=num_spks,\n            skip_around_intra=skip_around_intra,\n            linear_layer_after_inter_intra=linear_layer_after_inter_intra,\n            use_global_pos_enc=use_global_pos_enc,\n        )\n        self.skip_n_block = skip_n_block\n        print('skip_n_block', skip_n_block)\n\n        # for i in range(self.num_layers):\n           # self.dual_mdl[i].intra_norm = SafeGroupNorm(1, out_channels, 1e-5)\n\n    def forward(self, x):\n        \"\"\"Returns the output tensor.\n\n        Arguments\n        ---------\n        x : torch.Tensor\n            Input tensor of dimension [B, N, L].\n\n        Returns\n        -------\n        out : torch.Tensor\n            Output tensor of dimension [spks, B, N, L]\n            where, spks = Number of speakers\n               B = Batchsize,\n               N = number of filters\n               L = the number of time points\n        \"\"\"\n\n        # before each line we indicate the shape after executing the line\n\n        # [B, N, L]\n        # nan_ratio = torch.isnan(x).float().sum() / x.numel()\n        # print(f'Input NaN: {str(nan_ratio)}')\n        x = self.norm(x)\n        # nan_ratio = torch.isnan(x).float().sum() / x.numel()\n        # print(f'Input norm NaN: {str(nan_ratio)}')\n\n        # [B, N, L]\n        x = self.conv1d(x)\n        # nan_ratio = torch.isnan(x).float().sum() / x.numel()\n        # print(f'Input conv NaN: {str(nan_ratio)}')\n        if self.use_global_pos_enc:\n            x = self.pos_enc(x.transpose(1, -1)).transpose(1, -1) + x * (\n                x.size(1) ** 0.5\n            )\n\n        # [B, N, K, S]\n        x, gap = self._Segmentation(x, self.K)\n\n        # [B, N, K, S]\n        residual = x\n        # nan_ratio = torch.isnan(x).float().sum() / x.numel()\n        # print(f'DP Input NaN: {str(nan_ratio)}')\n\n        # x = self.dual_mdl[0](x)\n        # x = self.dual_mdl[1](x)\n        # x = self.dual_mdl[2](x)\n        # x = self.dual_mdl[3](x)\n        # x = self.dual_mdl[4](x)\n        # x = self.dual_mdl[5](x)\n        # x = self.dual_mdl[6](x)\n        # x = self.dual_mdl[7](x)\n\n        for i in range(self.num_layers):\n            if self.skip_n_block > 0 and \\\n                i % self.skip_n_block == 0 and i != 0: \n                x = 0.5 * x + 0.5 * residual\n            # print(i, x.max())\n\n            x = self.dual_mdl[i](x)\n\n\n            # nan_ratio = torch.isnan(x).float().sum() / x.numel()\n            # print(f'Layer {str(i)} NaN: {str(nan_ratio)}')\n            \n        x = self.prelu(x)\n\n        # print('prelu', x.max())\n\n        # [B, N*spks, K, S]\n        x = self.conv2d(x)\n        B, _, K, S = x.shape\n\n        # print('conv2d', x.max())\n\n        # [B*spks, N, K, S]\n        x = x.view(B * self.num_spks, -1, K, S)\n\n        # [B*spks, N, L]\n        x = self._over_add(x, gap)\n        x = self.output(x) * self.output_gate(x)\n\n       #  print('overlapAdd', x.max())\n\n        # [B*spks, N, L]\n        x = self.end_conv1x1(x)\n\n        # print('end_conv1x1', x.max())\n\n        # [B, spks, N, L]\n        _, N, L = x.shape\n        x = x.view(B, self.num_spks, N, L)\n        x = self.activation(x)\n\n        # [spks, B, N, L]\n        x = x.transpose(0, 1)\n\n        return x",
    "description": null,
    "url": null
}