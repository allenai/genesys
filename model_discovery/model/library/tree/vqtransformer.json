{
    "title": "Linear-Time Transformers via Vector Quantization",
    "acronym": "vqtransformer",
    "s2id": "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b",
    "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}",
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.",
    "citationCount": 5,
    "influentialCitationCount": 0,
    "seed_ids": [
        "rwkv4",
        "mega",
        "s5",
        "gssm",
        "memorizingtrans",
        "brt",
        "flash",
        "cosformer",
        "s4",
        "hierarchitrans",
        "lstransformer",
        "rfa",
        "nystromformer",
        "performer",
        "lineartransformer",
        "routingtransformer",
        "compressivetransformer",
        "mqa",
        "transformerxl",
        "transformer"
    ],
    "code": null
}