{
    "acronym": "mega",
    "title": "Mega: Moving Average Equipped Gated Attention",
    "seed_ids": [
        "s4d",
        "flash",
        "s4",
        "alibi",
        "roformer",
        "hippo",
        "transformerxl",
        "transformer"
    ],
    "s2id": "70e91e16eb321067d9402710e14a40cf28311f73",
    "abstract": "The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.",
    "authors": [
        "Xuezhe Ma",
        "Chunting Zhou",
        "Xiang Kong",
        "Junxian He",
        "Liangke Gui",
        "Graham Neubig",
        "Jonathan May",
        "Luke Zettlemoyer"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": "This paper introduces Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.",
    "citationCount": 121,
    "influentialCitationCount": 27,
    "code": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom fairseq.modules.moving_average_gated_attention import MovingAverageGatedAttention\nfrom fairseq.modules.gated_cross_attention import GatedCrossAttention\nfrom fairseq.modules.normalized_feedforward_network import NormalizedFeedForwardNetwork\nfrom torch import Tensor\n\n\nclass MegaEncoderLayer(nn.Module):\n    \"\"\"Encoder layer block.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n    \"\"\"\n\n    def __init__(self, args):\n        super().__init__()\n        self.embed_dim = args.encoder_embed_dim\n        self.mega_layer = self.build_mega_layer(self.embed_dim, args)\n        if args.encoder_ffn_embed_dim > 0:\n            self.nffn = self.build_nffn_layer(self.embed_dim, args)\n        else:\n            self.nffn = None\n\n    def build_mega_layer(self, embed_dim, args):\n        return MovingAverageGatedAttention(\n            embed_dim=embed_dim,\n            zdim=args.encoder_z_dim,\n            hdim=args.encoder_hidden_dim,\n            ndim=args.encoder_n_dim,\n            dropout=args.dropout,\n            attention_dropout=args.attention_dropout,\n            hidden_dropout=args.hidden_dropout,\n            chunk_size=args.encoder_chunk_size,\n            truncation=args.truncation_length,\n            rel_pos_bias=args.rel_pos_bias,\n            max_positions=args.max_source_positions,\n            activation=args.activation_fn,\n            attention_activation=args.attention_activation_fn,\n            bidirectional=True,\n            norm_type=args.normalization_type,\n            prenorm=args.normalize_before,\n            feature_dropout=args.feature_dropout,\n        )\n\n    def build_nffn_layer(self, embed_dim, args):\n        return NormalizedFeedForwardNetwork(\n            embed_dim=embed_dim,\n            ffn_hidden_dim=args.encoder_ffn_embed_dim,\n            dropout=args.dropout,\n            hidden_dropout=args.activation_dropout,\n            activation=args.activation_fn,\n            norm_type=args.normalization_type,\n            prenorm=args.normalize_before,\n            feature_dropout=args.feature_dropout,\n        )\n\n    def forward(self, x, encoder_padding_mask):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, seq_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"\n        x, _ = self.mega_layer(x, encoder_padding_mask)\n        if self.nffn is not None:\n            x = self.nffn(x)\n\n        return x\n\n\nclass MegaDecoderLayer(nn.Module):\n    \"\"\"Decoder layer block.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n    \"\"\"\n\n    def __init__(self, args, no_cross_attention=False):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.mega_layer = self.build_mega_layer(self.embed_dim, args)\n        self.cross_attn = None if no_cross_attention else self.build_cross_attn(self.embed_dim, args)\n        if args.decoder_ffn_embed_dim > 0:\n            self.nffn = self.build_nffn_layer(self.embed_dim, args)\n        else:\n            self.nffn = None\n\n        self.need_attn = False\n        self.onnx_trace = False\n\n    def build_mega_layer(self, embed_dim, args):\n        return MovingAverageGatedAttention(\n            embed_dim=embed_dim,\n            zdim=args.decoder_z_dim,\n            hdim=args.decoder_hidden_dim,\n            ndim=args.decoder_n_dim,\n            dropout=args.dropout,\n            attention_dropout=args.attention_dropout,\n            hidden_dropout=args.hidden_dropout,\n            chunk_size=args.decoder_chunk_size,\n            truncation=args.truncation_length,\n            rel_pos_bias=args.rel_pos_bias,\n            max_positions=args.max_target_positions,\n            activation=args.activation_fn,\n            attention_activation=args.attention_activation_fn,\n            bidirectional=False,\n            norm_type=args.normalization_type,\n            prenorm=args.normalize_before,\n            feature_dropout=args.feature_dropout,\n        )\n\n    def build_cross_attn(self, embed_dim, args):\n        return GatedCrossAttention(\n            embed_dim=embed_dim,\n            zdim=args.decoder_z_dim,\n            ndim=args.decoder_n_dim,\n            dropout=args.dropout,\n            attention_dropout=args.attention_dropout,\n            hidden_dropout=args.hidden_dropout,\n            activation=args.activation_fn,\n            attention_activation=args.attention_activation_fn,\n            norm_type=args.normalization_type,\n            prenorm=args.normalize_before,\n            feature_dropout=args.feature_dropout,\n            rel_pos_bias=args.rel_pos_bias,\n            max_positions=max(args.max_target_positions, args.max_source_positions),\n        )\n\n    def build_nffn_layer(self, embed_dim, args):\n        return NormalizedFeedForwardNetwork(\n            embed_dim=embed_dim,\n            ffn_hidden_dim=args.decoder_ffn_embed_dim,\n            dropout=args.dropout,\n            hidden_dropout=args.activation_dropout,\n            activation=args.activation_fn,\n            norm_type=args.normalization_type,\n            prenorm=args.normalize_before,\n            feature_dropout=args.feature_dropout,\n        )\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        x,\n        encoder_out: Optional[torch.Tensor] = None,\n        encoder_padding_mask: Optional[torch.Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        attn_mask: Optional[torch.Tensor] = None,\n        decoder_padding_mask: Optional[torch.Tensor] = None,\n        need_attn: bool = False,\n    ):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_out (Tensor): encoder out for cross attention `(src_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor, optional): binary ByteTensor of shape `(batch, src_len)` where padding elements are indicated by ``1``.\n            incremental_state: dictionary for caching incremental states.\n            attn_mask (Tensor): attention mask for autoregressive decoding.\n            decoder_padding_mask: padding mask for target sequence.\n            need_attn (bool, optional): return attention weights.\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"\n        x, attn = self.mega_layer(x=x, padding_mask=decoder_padding_mask,\n                                  incremental_state=incremental_state,\n                                  need_weights=False, attn_mask=attn_mask)\n\n        if self.cross_attn is not None:\n            x, attn = self.cross_attn(query=x, key=encoder_out, value=encoder_out,\n                                      padding_mask=decoder_padding_mask,\n                                      key_padding_mask=encoder_padding_mask,\n                                      incremental_state=incremental_state,\n                                      static_kv=True, need_weights=need_attn)\n\n        if self.nffn is not None:\n            x = self.nffn(x)\n\n        return x, attn, None\n\n    def make_generation_fast_(self, need_attn: bool = False, **kwargs):\n        self.need_attn = need_attn",
    "description": null,
    "url": null
}