{
    "acronym": "infiniti",
    "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
    "seed_ids": [
        "ring",
        "yarn",
        "pi",
        "landmarkattn",
        "compresscontext",
        "unlimiformer",
        "rmt",
        "flashattn",
        "memorizingtrans",
        "lineartransformer",
        "longformer",
        "compressivetransformer",
        "sparsetransformer",
        "transformerxl"
    ],
    "s2id": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4",
    "abstract": "This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.",
    "authors": [
        "Tsendsuren Munkhdalai",
        "Manaal Faruqui",
        "Siddharth Gopal"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.",
    "citationCount": 34,
    "influentialCitationCount": 3,
    "code": "# coding=utf-8\n# Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n#\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch Llama model, with Infini-Attention.\"\"\"\n\nimport os\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import (\n    _prepare_4d_causal_attention_mask,\n)\nfrom transformers.pytorch_utils import (\n    ALL_LAYERNORM_LAYERS,\n    is_torch_greater_or_equal_than_1_13,\n)\nfrom transformers.utils import (\n    logging,\n)\nfrom transformers.utils.import_utils import is_torch_fx_available\n\nfrom transformers import LlamaConfig\n\nfrom .rotary_embedding import (\n    LlamaDynamicNTKScalingRotaryEmbedding,\n    LlamaLinearScalingRotaryEmbedding,\n    LlamaRotaryEmbedding,\n    apply_rotary_pos_emb,\n)\n\nDEBUG = os.environ.get(\"DEBUG\", False)\n\n\ndef debug_print(*args):\n    if DEBUG:\n        print(*args)\n\n# This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n# It means that the function will not be traced through and simply appear as a node in the graph.\nif is_torch_fx_available():\n    if not is_torch_greater_or_equal_than_1_13:\n        import torch.fx\n\n    _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n\n\nlogger = logging.get_logger(__name__)\n\n\n\n# Copied from transformers.models.llama.modeling_llama.repeat_kv\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(\n        batch, num_key_value_heads, n_rep, slen, head_dim\n    )\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.k_proj = nn.Linear(\n            self.hidden_size,\n            self.num_key_value_heads * self.head_dim,\n            bias=config.attention_bias,\n        )\n        self.v_proj = nn.Linear(\n            self.hidden_size,\n            self.num_key_value_heads * self.head_dim,\n            bias=config.attention_bias,\n        )\n        self.o_proj = nn.Linear(\n            self.hidden_size, self.hidden_size, bias=config.attention_bias\n        )\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\n        raise NotImplementedError(\"forward method must be implemented in derived classes\")\n    \n\n\nclass LlamaInfiniAttention(LlamaAttention):\n    def __init__(\n        self,\n        config: LlamaConfig,\n        layer_idx: Optional[int] = None,\n    ):\n        super().__init__(config, layer_idx)\n\n        # Each head has its own gate\n        # init with -100 to make it close to 0 effect at the beginning\n        self.gate = nn.Parameter(torch.full((1, self.num_heads, 1, 1), 0.0))\n        # self.segment_size = config.segment_size\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        memory: Optional[dict] = None,\n        norm_term: Optional[dict] = None,\n        no_memory_update: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        segment = hidden_states  # no need to split in TYPE-2 implementation\n\n        # Pre-allocate tensor for all outputs\n        bsz, _, hidden_dim = hidden_states.size()\n\n        query_states = self.q_proj(segment)\n        key_states = self.k_proj(segment)\n        value_states = self.v_proj(segment)\n\n        # Assuming the presence of batch size and dimension handling as before\n        bsz, q_len, _ = segment.size()  # q_len == self.segment_size\n        query_states = query_states.view(\n            bsz, q_len, self.num_heads, self.head_dim\n        ).transpose(1, 2)\n        key_states = key_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n\n        debug_print(\"Query States Shape:\", query_states.shape)\n        debug_print(\"Key States Shape:\", key_states.shape)\n        debug_print(\"Value States Shape:\", value_states.shape)\n\n        # memory and norm_term should use layer_idx to store the memory and norm_term\n        if no_memory_update:\n            memory = {}\n            norm_term = {}\n            memory_output = None\n        else:\n            # Infini Attention memory does not use PE\n            # Memory retrieval and attention calculation per segment\n            memory_output = self._retrieve_from_memory(\n                query_states,\n                memory.get(self.layer_idx, None) if memory is not None else None,\n                norm_term.get(self.layer_idx, None) if norm_term is not None else None,\n            )\n            debug_print(\"Memory Output Shape:\", memory_output.shape)\n\n        # Update memory with current segment's key and value states\n        if no_memory_update:\n            # do not update memory\n            pass\n        else:\n            updated_memory, updated_norm_term = self._update_memory(\n                key_states,\n                value_states,\n                memory.get(self.layer_idx, None) if memory is not None else None,\n                norm_term.get(self.layer_idx, None) if norm_term is not None else None,\n            )\n            debug_print(\"Memory Output Shape:\", updated_memory.shape)\n            debug_print(\"Updated Memory Shape:\", updated_norm_term.shape)\n            if memory is None and norm_term is None:\n                memory = {}\n                norm_term = {}\n            memory[self.layer_idx] = updated_memory.detach()\n            norm_term[self.layer_idx] = updated_norm_term.detach()\n\n        # Rotary embeddings, set seq_len to q_len as we are processing a segment\n        cos, sin = self.rotary_emb(value_states, position_ids)\n\n        query_states, key_states = apply_rotary_pos_emb(\n            query_states,\n            key_states,\n            cos,  # cos[:, : min(self.segment_size, q_len), :],\n            sin,  # sin[:, : min(self.segment_size, q_len), :],\n            None,\n        )\n\n        # Basic cache\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\n                \"sin\": sin,\n                \"cos\": cos,\n                \"cache_position\": cache_position,\n            }\n            key_states, value_states = past_key_value.update(\n                key_states, value_states, self.layer_idx, cache_kwargs\n            )\n\n        # GQA\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            # causal_mask = causal_mask[\n            #     :, :, : min(self.segment_size, q_len), : key_states.shape[-2]\n            # ]  # FIXME: This is wrong, should be [:, :, :, :self.segment_size]\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        debug_print(\"causal_mask.shape\", causal_mask.shape)\n        debug_print(\"query_states.shape\", query_states.shape)\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n        )\n\n        if memory_output is None:\n            combined_output = attn_output\n        else:\n            combined_output = (\n                F.sigmoid(self.gate) * memory_output\n                + (1 - F.sigmoid(self.gate)) * attn_output\n            )\n\n        # Prepare output for this segment\n        combined_output = combined_output.transpose(1, 2).contiguous()\n        combined_output = combined_output.view(bsz, q_len, self.hidden_size)\n\n        final_output = self.o_proj(combined_output)\n\n        if no_memory_update:\n            memory = None\n            norm_term = None\n\n        return (\n            final_output,\n            None,\n            None,\n            memory,\n            norm_term,\n        )\n\n    def _retrieve_from_memory(self, query_states, memory, norm_term):\n        # query_states: [batch_size, num_heads, seq_len, head_dim]\n\n        # Check if memory is initialized\n        if memory is None or norm_term is None:\n            debug_print(\"[Retrieve] No memory or norm term found\")\n            return torch.zeros_like(query_states)\n\n        debug_print(\"[Retrieve] query_states.shape\", query_states.shape)\n        debug_print(\"[Retrieve] self.memory.shape\", memory.shape)\n\n        # Apply ELU activation\n        query_states = F.elu(query_states) + 1  # ELU activation + 1 for stability\n        memory_output = torch.matmul(\n            # GQA\n            query_states,\n            memory.repeat(1, self.num_key_value_groups, 1, 1),\n        )\n\n        debug_print(\"[Retrieve] memory_output.shape\", memory_output.shape)\n        debug_print(\"[Retrieve] self.norm_term.shape\", norm_term.shape)\n\n        # Broadcast norm_term to the shape of query_states, then sum across head_dim for normalization\n        norm_term_broadcastable = torch.matmul(\n            query_states,\n            # GQA\n            norm_term.transpose(-2, -1).repeat(1, self.num_key_value_groups, 1, 1),\n        )\n        debug_print(\n            \"[Broadcast] norm_term_broadcastable.shape\", norm_term_broadcastable.shape\n        )\n\n        # Perform division\n        memory_output = memory_output / norm_term_broadcastable\n        return memory_output\n\n    def _update_memory(self, key_states, value_states, memory, norm_term):\n        # key_states: [batch_size, num_heads, seq_len, head_dim]\n        # value_states: [batch_size, num_heads, seq_len, value_dim]\n\n        key_states = F.elu(key_states) + 1  # Apply ELU activation\n\n        if memory is not None:\n            memory = memory + torch.matmul(key_states.transpose(-2, -1), value_states)\n        else:\n            memory = torch.matmul(key_states.transpose(-2, -1), value_states)\n\n        if norm_term is not None:\n            norm_term = norm_term + key_states.sum(\n                dim=2, keepdim=True\n            )  # Update normalization term\n        else:\n            norm_term = key_states.sum(\n                dim=2, keepdim=True\n            )  # Initialize normalization term\n\n        debug_print(\"[Update] self.memory.shape\", memory.shape)\n        debug_print(\"[Update] self.norm_term.shape\", norm_term.shape)\n\n        return memory, norm_term\n\n",
    "description": null,
    "url": null
}