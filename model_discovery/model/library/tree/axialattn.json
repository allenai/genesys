{
    "acronym": "axialattn",
    "title": "Axial Attention in Multidimensional Transformers",
    "seed_ids": [
        "sparsetransformer",
        "transformer"
    ],
    "s2id": "366244acdd930e488ae224ab6e2a92dc24aa7e06",
    "abstract": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
    "authors": [
        "Jonathan Ho",
        "Nal Kalchbrenner",
        "Dirk Weissenborn",
        "Tim Salimans"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": "Axial Transformers is proposed, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors that maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation.",
    "citationCount": 439,
    "influentialCitationCount": 35,
    "code": "import torch\nfrom torch import nn\nfrom operator import itemgetter\nfrom axial_attention.reversible import ReversibleSequence\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef map_el_ind(arr, ind):\n    return list(map(itemgetter(ind), arr))\n\ndef sort_and_return_indices(arr):\n    indices = [ind for ind in range(len(arr))]\n    arr = zip(arr, indices)\n    arr = sorted(arr)\n    return map_el_ind(arr, 0), map_el_ind(arr, 1)\n\n# calculates the permutation to bring the input tensor to something attend-able\n# also calculates the inverse permutation to bring the tensor back to its original shape\n\ndef calculate_permutations(num_dimensions, emb_dim):\n    total_dimensions = num_dimensions + 2\n    emb_dim = emb_dim if emb_dim > 0 else (emb_dim + total_dimensions)\n    axial_dims = [ind for ind in range(1, total_dimensions) if ind != emb_dim]\n\n    permutations = []\n\n    for axial_dim in axial_dims:\n        last_two_dims = [axial_dim, emb_dim]\n        dims_rest = set(range(0, total_dimensions)) - set(last_two_dims)\n        permutation = [*dims_rest, *last_two_dims]\n        permutations.append(permutation)\n      \n    return permutations\n\n# helper classes\n\nclass ChanLayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n\n    def forward(self, x):\n        std = torch.var(x, dim = 1, unbiased = False, keepdim = True).sqrt()\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) / (std + self.eps) * self.g + self.b\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        x = self.norm(x)\n        return self.fn(x)\n\nclass Sequential(nn.Module):\n    def __init__(self, blocks):\n        super().__init__()\n        self.blocks = blocks\n\n    def forward(self, x):\n        for f, g in self.blocks:\n            x = x + f(x)\n            x = x + g(x)\n        return x\n\nclass PermuteToFrom(nn.Module):\n    def __init__(self, permutation, fn):\n        super().__init__()\n        self.fn = fn\n        _, inv_permutation = sort_and_return_indices(permutation)\n        self.permutation = permutation\n        self.inv_permutation = inv_permutation\n\n    def forward(self, x, **kwargs):\n        axial = x.permute(*self.permutation).contiguous()\n\n        shape = axial.shape\n        *_, t, d = shape\n\n        # merge all but axial dimension\n        axial = axial.reshape(-1, t, d)\n\n        # attention\n        axial = self.fn(axial, **kwargs)\n\n        # restore to original shape and permutation\n        axial = axial.reshape(*shape)\n        axial = axial.permute(*self.inv_permutation).contiguous()\n        return axial\n\n# axial pos emb\n\nclass AxialPositionalEmbedding(nn.Module):\n    def __init__(self, dim, shape, emb_dim_index = 1):\n        super().__init__()\n        parameters = []\n        total_dimensions = len(shape) + 2\n        ax_dim_indexes = [i for i in range(1, total_dimensions) if i != emb_dim_index]\n\n        self.num_axials = len(shape)\n\n        for i, (axial_dim, axial_dim_index) in enumerate(zip(shape, ax_dim_indexes)):\n            shape = [1] * total_dimensions\n            shape[emb_dim_index] = dim\n            shape[axial_dim_index] = axial_dim\n            parameter = nn.Parameter(torch.randn(*shape))\n            setattr(self, f'param_{i}', parameter)\n\n    def forward(self, x):\n        for i in range(self.num_axials):\n            x = x + getattr(self, f'param_{i}')\n        return x\n\n# attention\n\nclass SelfAttention(nn.Module):\n    def __init__(self, dim, heads, dim_heads = None):\n        super().__init__()\n        self.dim_heads = (dim // heads) if dim_heads is None else dim_heads\n        dim_hidden = self.dim_heads * heads\n\n        self.heads = heads\n        self.to_q = nn.Linear(dim, dim_hidden, bias = False)\n        self.to_kv = nn.Linear(dim, 2 * dim_hidden, bias = False)\n        self.to_out = nn.Linear(dim_hidden, dim)\n\n    def forward(self, x, kv = None):\n        kv = x if kv is None else kv\n        q, k, v = (self.to_q(x), *self.to_kv(kv).chunk(2, dim=-1))\n\n        b, t, d, h, e = *q.shape, self.heads, self.dim_heads\n\n        merge_heads = lambda x: x.reshape(b, -1, h, e).transpose(1, 2).reshape(b * h, -1, e)\n        q, k, v = map(merge_heads, (q, k, v))\n\n        dots = torch.einsum('bie,bje->bij', q, k) * (e ** -0.5)\n        dots = dots.softmax(dim=-1)\n        out = torch.einsum('bij,bje->bie', dots, v)\n\n        out = out.reshape(b, h, -1, e).transpose(1, 2).reshape(b, -1, d)\n        out = self.to_out(out)\n        return out\n\n# axial attention class\n\nclass AxialAttention(nn.Module):\n    def __init__(self, dim, num_dimensions = 2, heads = 8, dim_heads = None, dim_index = -1, sum_axial_out = True):\n        assert (dim % heads) == 0, 'hidden dimension must be divisible by number of heads'\n        super().__init__()\n        self.dim = dim\n        self.total_dimensions = num_dimensions + 2\n        self.dim_index = dim_index if dim_index > 0 else (dim_index + self.total_dimensions)\n\n        attentions = []\n        for permutation in calculate_permutations(num_dimensions, dim_index):\n            attentions.append(PermuteToFrom(permutation, SelfAttention(dim, heads, dim_heads)))\n\n        self.axial_attentions = nn.ModuleList(attentions)\n        self.sum_axial_out = sum_axial_out\n\n    def forward(self, x):\n        assert len(x.shape) == self.total_dimensions, 'input tensor does not have the correct number of dimensions'\n        assert x.shape[self.dim_index] == self.dim, 'input tensor does not have the correct input dimension'\n\n        if self.sum_axial_out:\n            return sum(map(lambda axial_attn: axial_attn(x), self.axial_attentions))\n\n        out = x\n        for axial_attn in self.axial_attentions:\n            out = axial_attn(out)\n        return out\n\n# axial image transformer\n\nclass AxialImageTransformer(nn.Module):\n    def __init__(self, dim, depth, heads = 8, dim_heads = None, dim_index = 1, reversible = True, axial_pos_emb_shape = None):\n        super().__init__()\n        permutations = calculate_permutations(2, dim_index)\n\n        get_ff = lambda: nn.Sequential(\n            ChanLayerNorm(dim),\n            nn.Conv2d(dim, dim * 4, 3, padding = 1),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv2d(dim * 4, dim, 3, padding = 1)\n        )\n\n        self.pos_emb = AxialPositionalEmbedding(dim, axial_pos_emb_shape, dim_index) if exists(axial_pos_emb_shape) else nn.Identity()\n\n        layers = nn.ModuleList([])\n        for _ in range(depth):\n            attn_functions = nn.ModuleList([PermuteToFrom(permutation, PreNorm(dim, SelfAttention(dim, heads, dim_heads))) for permutation in permutations])\n            conv_functions = nn.ModuleList([get_ff(), get_ff()])\n            layers.append(attn_functions)\n            layers.append(conv_functions)            \n\n        execute_type = ReversibleSequence if reversible else Sequential\n        self.layers = execute_type(layers)\n\n    def forward(self, x):\n        x = self.pos_emb(x)\n        return self.layers(x)",
    "description": null,
    "url": null
}