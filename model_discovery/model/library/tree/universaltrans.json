{
    "acronym": "universaltrans",
    "title": "Universal Transformers ",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",
    "authors": [
        "Mostafa Dehghani",
        "Stephan Gouws",
        "O. Vinyals",
        "Jakob Uszkoreit",
        "Lukasz Kaiser"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "tldr": "The Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses issues of parallelizability and global receptive field, is proposed.",
    "citationCount": 673,
    "influentialCitationCount": 65,
    "code": "### TAKEN FROM https://github.com/kolloldas/torchnlp\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.nn.init as I\nimport numpy as np\nimport math\nfrom models.common_layer import EncoderLayer ,DecoderLayer ,LayerNorm ,_gen_bias_mask\n\ndef _gen_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    \"\"\"\n    Generates a [1, length, channels] timing signal consisting of sinusoids\n    Adapted from:\n    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n    \"\"\"\n    position = np.arange(length)\n    num_timescales = channels // 2\n    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))\n    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)\n    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)\n\n    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n    signal = np.pad(signal, [[0, 0], [0, channels % 2]], \n                    'constant', constant_values=[0.0, 0.0])\n    signal =  signal.reshape([1, length, channels])\n\n    return torch.from_numpy(signal).type(torch.FloatTensor)\n\nclass BabiUTransformer(nn.Module):\n    \"\"\"\n    A Transformer Module For BabI data. \n    Inputs should be in the shape story: [batch_size, memory_size, story_len ]\n                                  query: [batch_size, 1, story_len]\n    Outputs will have the shape [batch_size, ]\n    \"\"\"\n    def __init__(self, num_vocab, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=71, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False ):\n        super(BabiUTransformer, self).__init__()\n        self.embedding_dim = embedding_size\n        self.emb = nn.Embedding(num_vocab, embedding_size, padding_idx=0)\n        self.transformer_enc = Encoder(embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                                filter_size, max_length=71, input_dropout=input_dropout, layer_dropout=layer_dropout, \n                                attention_dropout=attention_dropout, relu_dropout=relu_dropout, use_mask=False, act=act)\n\n        self.W = nn.Linear(self.embedding_dim,num_vocab)\n\n\n        # Share the weight matrix between target word embedding & the final logit dense layer\n        self.W.weight = self.emb.weight\n        \n        self.softmax = nn.Softmax(dim=1)\n        ## POSITIONAL MASK\n        self.mask = nn.Parameter(I.constant_(torch.empty(11, self.embedding_dim), 1))\n\n    def forward(self,story, query):\n\n        story_size = story.size()\n        ## STORY ENCODER + MUlt Mask\n        embed = self.emb(story.view(story.size(0), -1))\n        embed = embed.view(story_size+(embed.size(-1),))\n        embed_story = torch.sum(embed*self.mask[:story.size(2),:].unsqueeze(0), 2)\n\n        ## QUERY ENCODER + MUlt Mask\n        query_embed = self.emb(query)\n        embed_query = torch.sum(query_embed.unsqueeze(1)*self.mask[:query.size(1),:], 2)\n\n        ## CONCAT STORY AND QUERY\n        embed = torch.cat([embed_story, embed_query],dim=1)\n\n        ## APPLY TRANSFORMER\n        logit, act = self.transformer_enc(embed)\n        \n        a_hat = self.W(torch.sum(logit,dim=1)/logit.size(1)) ## reduce mean\n\n        return a_hat, self.softmax(a_hat), act\n\n\n\nclass Encoder(nn.Module):\n    \"\"\"\n    A Transformer Encoder module. \n    Inputs should be in the shape [batch_size, length, hidden_size]\n    Outputs will have the shape [batch_size, length, hidden_size]\n    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n    \"\"\"\n    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False, act=False):\n        \"\"\"\n        Parameters:\n            embedding_size: Size of embeddings\n            hidden_size: Hidden size\n            num_layers: Total layers in the Encoder\n            num_heads: Number of attention heads\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            max_length: Max sequence length (required for timing signal)\n            input_dropout: Dropout just after embedding\n            layer_dropout: Dropout for each layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n            use_mask: Set to True to turn on future value masking\n        \"\"\"\n        \n        super(Encoder, self).__init__()\n        \n        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n        ## for t\n        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n\n        self.num_layers = num_layers\n        self.act = act\n        params =(hidden_size, \n                 total_key_depth or hidden_size,\n                 total_value_depth or hidden_size,\n                 filter_size, \n                 num_heads, \n                 _gen_bias_mask(max_length) if use_mask else None,\n                 layer_dropout, \n                 attention_dropout, \n                 relu_dropout)\n\n        self.proj_flag = False\n        if(embedding_size == hidden_size):\n            self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n            self.proj_flag = True\n\n        self.enc = EncoderLayer(*params)\n        \n        self.layer_norm = LayerNorm(hidden_size)\n        self.input_dropout = nn.Dropout(input_dropout)\n        if(self.act):\n            self.act_fn = ACT_basic(hidden_size)\n\n    def forward(self, inputs):\n\n        #Add input dropout\n        x = self.input_dropout(inputs)\n\n        if(self.proj_flag):\n            # Project to hidden size\n            x = self.embedding_proj(x)\n\n        if(self.act):\n            x, (remainders,n_updates) = self.act_fn(x, inputs, self.enc, self.timing_signal, self.position_signal, self.num_layers)\n            return x, (remainders,n_updates)\n        else:\n            for l in range(self.num_layers):\n                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n                x = self.enc(x)\n            return x, None\n\ndef get_attn_key_pad_mask(seq_k, seq_q):\n    ''' For masking out the padding part of key sequence. '''\n    # Expand to fit the shape of key query attention matrix.\n    len_q = seq_q.size(1)\n    PAD = 0\n    padding_mask = seq_k.eq(PAD)\n    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n\n    return padding_mask\n\nclass Decoder(nn.Module):\n    \"\"\"\n    A Transformer Decoder module. \n    Inputs should be in the shape [batch_size, length, hidden_size]\n    Outputs will have the shape [batch_size, length, hidden_size]\n    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n    \"\"\"\n    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0, act=False):\n        \"\"\"\n        Parameters:\n            embedding_size: Size of embeddings\n            hidden_size: Hidden size\n            num_layers: Total layers in the Encoder\n            num_heads: Number of attention heads\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            max_length: Max sequence length (required for timing signal)\n            input_dropout: Dropout just after embedding\n            layer_dropout: Dropout for each layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n        \"\"\"\n        \n        super(Decoder, self).__init__()\n        \n        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n        self.position_signal = _gen_timing_signal(num_layers, hidden_size)\n        self.num_layers = num_layers\n        self.act = act\n        params =(hidden_size, \n                 total_key_depth or hidden_size,\n                 total_value_depth or hidden_size,\n                 filter_size, \n                 num_heads, \n                 _gen_bias_mask(max_length), # mandatory\n                 layer_dropout, \n                 attention_dropout, \n                 relu_dropout)\n\n        self.proj_flag = False\n        if(embedding_size == hidden_size):\n            self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n            self.proj_flag = True\n        self.dec = DecoderLayer(*params) \n        \n        self.layer_norm = LayerNorm(hidden_size)\n        self.input_dropout = nn.Dropout(input_dropout)\n        if(self.act):\n            self.act_fn = ACT_basic(hidden_size)\n    \n    def forward(self, inputs, encoder_output):\n        #Add input dropout\n        x = self.input_dropout(inputs)\n        \n        if(self.proj_flag):\n            # Project to hidden size\n            x = self.embedding_proj(x)\n        \n        if(self.act):\n            x, (remainders,n_updates) = self.act_fn(x, inputs, self.dec, self.timing_signal, self.position_signal, self.num_layers, encoder_output)\n            return x, (remainders,n_updates)\n        else:\n            for l in range(self.num_layers):\n                x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n                x += self.position_signal[:, l, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n                x, _ = self.dec((x, encoder_output))\n        return x\n\n\n\n### CONVERTED FROM https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer_util.py#L1062\nclass ACT_basic(nn.Module):\n    def __init__(self,hidden_size):\n        super(ACT_basic, self).__init__()\n        self.sigma = nn.Sigmoid()\n        self.p = nn.Linear(hidden_size,1)  \n        self.p.bias.data.fill_(1) \n        self.threshold = 1 - 0.1\n\n    def forward(self, state, inputs, fn, time_enc, pos_enc, max_hop, encoder_output=None):\n        # init_hdd\n        ## [B, S]\n        halting_probability = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n        ## [B, S\n        remainders = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n        ## [B, S]\n        n_updates = torch.zeros(inputs.shape[0],inputs.shape[1]).cuda()\n        ## [B, S, HDD]\n        previous_state = torch.zeros_like(inputs).cuda()\n        step = 0\n        # for l in range(self.num_layers):\n        while( ((halting_probability<self.threshold) & (n_updates < max_hop)).byte().any()):\n            # Add timing signal\n            state = state + time_enc[:, :inputs.shape[1], :].type_as(inputs.data)\n            state = state + pos_enc[:, step, :].unsqueeze(1).repeat(1,inputs.shape[1],1).type_as(inputs.data)\n\n            p = self.sigma(self.p(state)).squeeze(-1)\n            # Mask for inputs which have not halted yet\n            still_running = (halting_probability < 1.0).float()\n\n            # Mask of inputs which halted at this step\n            new_halted = (halting_probability + p * still_running > self.threshold).float() * still_running\n\n            # Mask of inputs which haven't halted, and didn't halt this step\n            still_running = (halting_probability + p * still_running <= self.threshold).float() * still_running\n\n            # Add the halting probability for this step to the halting\n            # probabilities for those input which haven't halted yet\n            halting_probability = halting_probability + p * still_running\n\n            # Compute remainders for the inputs which halted at this step\n            remainders = remainders + new_halted * (1 - halting_probability)\n\n            # Add the remainders to those inputs which halted at this step\n            halting_probability = halting_probability + new_halted * remainders\n\n            # Increment n_updates for all inputs which are still running\n            n_updates = n_updates + still_running + new_halted\n\n            # Compute the weight to be applied to the new state and output\n            # 0 when the input has already halted\n            # p when the input hasn't halted yet\n            # the remainders when it halted this step\n            update_weights = p * still_running + new_halted * remainders\n\n            if(encoder_output):\n                state, _ = fn((state,encoder_output))\n            else:\n                # apply transformation on the state\n                state = fn(state)\n\n            # update running part in the weighted state and keep the rest\n            previous_state = ((state * update_weights.unsqueeze(-1)) + (previous_state * (1 - update_weights.unsqueeze(-1))))\n            ## previous_state is actually the new_state at end of hte loop \n            ## to save a line I assigned to previous_state so in the next \n            ## iteration is correct. Notice that indeed we return previous_state\n            step+=1\n        return previous_state, (remainders,n_updates)",
    "description": null,
    "url": null
}