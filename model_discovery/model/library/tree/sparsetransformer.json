{
    "acronym": "sparsetransformer",
    "title": "Generating Long Sequences with Sparse Transformers",
    "seed_ids": [
        "memcompress",
        "transformer",
        "gpt"
    ],
    "s2id": "21da617a0f79aabf94272107184606cefe90ab75",
    "abstract": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
    "authors": [
        "R. Child",
        "S. Gray",
        "Alec Radford",
        "I. Sutskever"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": "This paper introduces sparse factorizations of the attention matrix which reduce this to $O(n)$, and generates unconditional samples that demonstrate global coherence and great diversity, and shows it is possible in principle to use self-attention to model sequences of length one million or more.",
    "citationCount": 1488,
    "influentialCitationCount": 137,
    "code": "import sys\nimport numpy as np\nimport tensorflow as tf\nfrom blocksparse import BlocksparseTransformer\nfrom utils import shape_list, recomputable\n\n\ndef get_attn_mask(n, attn_mode, local_attn_ctx=None):\n    if attn_mode == 'all':\n        b = tf.matrix_band_part(tf.ones([n, n]), -1, 0)\n    elif attn_mode == 'local':\n        bandwidth = local_attn_ctx\n        ctx = tf.minimum(n - 1, bandwidth - 1)\n        b = tf.matrix_band_part(tf.ones([n, n]), ctx, 0)\n    elif attn_mode == 'strided':\n        stride = local_attn_ctx\n        x = tf.reshape(tf.range(n, dtype=tf.int32), [n, 1])\n        y = tf.transpose(x)\n        z = tf.zeros([n, n], dtype=tf.int32)\n        q = z + x\n        k = z + y\n        c1 = q >= k\n        c2 = tf.equal(tf.floormod(q - k, stride), 0)\n        c3 = tf.logical_and(c1, c2)\n        b = tf.cast(c3, tf.float32)\n    else:\n        raise ValueError('Not yet implemented')\n    b = tf.reshape(b, [1, 1, n, n])\n    return b\n\n\ndef strided_transpose(x, n_ctx, local_attn_ctx, blocksize):\n    bT_ctx = n_ctx // local_attn_ctx\n    assert bT_ctx % blocksize == 0, f'{bT_ctx}, {blocksize}'\n    n, t, embd = shape_list(x)\n    x = tf.reshape(x, [n, bT_ctx, local_attn_ctx, embd])\n    x = tf.transpose(x, [0, 2, 1, 3])\n    x = tf.reshape(x, [n, t, embd])\n    return x\n\n\ndef split_heads(x, n):\n    return tf.transpose(split_states(x, n), [0, 2, 1, 3])\n\n\ndef merge_heads(x):\n    return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n\n\ndef split_states(x, n):\n    \"\"\"\n    reshape (batch, pixel, state) -> (batch, pixel, head, head_state)\n    \"\"\"\n    x_shape = shape_list(x)\n    m = x_shape[-1]\n    new_x_shape = x_shape[:-1] + [n, m // n]\n    return tf.reshape(x, new_x_shape)\n\n\ndef merge_states(x):\n    \"\"\"\n    reshape (batch, pixel, head, head_state) -> (batch, pixel, state)\n    \"\"\"\n    x_shape = shape_list(x)\n    new_x_shape = x_shape[:-2] + [np.prod(x_shape[-2:])]\n    return tf.reshape(x, new_x_shape)\n\n\n@recomputable('attention_impl')\ndef attention_impl(q, k, v, heads, attn_mode, local_attn_ctx=None):\n    q = split_heads(q, heads)\n    k = split_heads(k, heads)\n    v = split_heads(v, heads)\n    n_timesteps = shape_list(k)[2]\n    mask = tf.to_float(get_attn_mask(n_timesteps, attn_mode, local_attn_ctx))\n    w = tf.matmul(q, k, transpose_b=True)\n    scale_amount = 1.0 / np.sqrt(shape_list(q)[-1])\n    orig_dtype = q.dtype\n    if orig_dtype == tf.float16:\n        w = tf.cast(w, tf.float32)\n    w = w * scale_amount\n    w = w * mask + -1e9 * (1 - mask)\n    w = tf.nn.softmax(w)\n    w = tf.cast(w, orig_dtype)\n    a = tf.matmul(w, v)\n    a = merge_heads(a)\n    return a\n\n\n@recomputable('blocksparse_attention_impl')\ndef blocksparse_attention_impl(q, k, v, heads, attn_mode, local_attn_ctx=None,\n                               blocksize=32, num_verts=None, vertsize=None):\n    n_ctx = shape_list(q)[1]\n    if attn_mode == 'strided':\n        # Strided attention is implemented on the transposed matrix to provide greater block sparsity\n        q = strided_transpose(q, n_ctx, local_attn_ctx, blocksize)\n        k = strided_transpose(k, n_ctx, local_attn_ctx, blocksize)\n        v = strided_transpose(v, n_ctx, local_attn_ctx, blocksize)\n    n_state = shape_list(q)[-1] // heads\n    bst = get_blocksparse_obj(n_ctx, heads, attn_mode, blocksize, local_attn_ctx, num_verts, vertsize)\n    scale_amount = tf.cast(1.0 / np.sqrt(n_state), tf.float32)\n    w = bst.query_key_op(q, k)\n    w = bst.masked_softmax(w, scale=scale_amount)\n    a = bst.weight_value_op(w, v)\n    if attn_mode == 'strided':\n        n, t, embd = shape_list(a)\n        bT_ctx = n_ctx // local_attn_ctx\n        a = tf.reshape(a, [n, local_attn_ctx, bT_ctx, embd])\n        a = tf.transpose(a, [0, 2, 1, 3])\n        a = tf.reshape(a, [n, t, embd])\n    return a\n\n\ndef get_blocksparse_obj(n_ctx, n_heads, attn_mode, blocksize=32, local_attn_ctx=None, num_verts=4, vertsize=1):\n    '''Defines the block-level sparsity pattern in the attention matrix. Enabled blocks\n    will have the callback called on them in order to define a positionwise sparsity mask.'''\n    n_bctx = n_ctx // blocksize\n    layout = np.ones([n_bctx, n_bctx], dtype=np.bool)\n    extra_diagonals = None\n    block_chunks = None\n\n    if attn_mode in ['all', 'fixed']:\n        pass\n    elif attn_mode == 'local':\n        assert local_attn_ctx % blocksize == 0\n        extra_diagonals = local_attn_ctx // blocksize\n    elif attn_mode == 'strided':\n        bT_ctx = n_ctx // local_attn_ctx\n        assert bT_ctx % blocksize == 0\n        block_chunks = bT_ctx // blocksize\n    else:\n        raise ValueError(f'attn mode {attn_mode} invalid')\n\n    if attn_mode == 'fixed':\n        assert n_heads % num_verts == 0\n        lctx = local_attn_ctx\n        stride = lctx // blocksize\n        assert vertsize <= stride\n        assert stride % vertsize == 0\n        indices = [i for i in range(stride - 1, -1, -1)]\n        indices = np.array(indices).reshape([-1, vertsize])\n        if num_verts == 1:\n            layout = np.zeros([n_bctx, n_bctx], dtype=np.bool)\n            for idx in indices[0]:\n                layout[:, idx::stride] = 1\n            for q_idx in range(n_bctx):\n                # Each thing can attend to its local block\n                row = q_idx // stride\n                layout[q_idx, row * stride:(row + 1) * stride] = 1\n                # Any query cannot attend to keys above it\n                layout[q_idx, q_idx + 1:] = 0\n        else:\n            layouts = []\n            indices = indices[:num_verts]\n            for h in range(n_heads):\n                layout = np.zeros([n_bctx, n_bctx], dtype=np.bool)\n                subindices = indices[h % num_verts]\n                for idx in subindices:\n                    layout[:, idx::stride] = 1\n                for q_idx in range(n_bctx):\n                    # Each position can attend to its local block\n                    row = q_idx // stride\n                    layout[q_idx, row * stride:(row + 1) * stride] = 1\n                    # Any query cannot attend to keys above it\n                    layout[q_idx, q_idx + 1:] = 0\n                layouts.append(layout)\n            layout = np.array(layouts)\n    else:\n        for q_idx, k_idx in np.ndindex(n_bctx, n_bctx):\n            if k_idx > q_idx:\n                layout[q_idx, k_idx] = 0\n            if extra_diagonals and k_idx + extra_diagonals < q_idx:\n                layout[q_idx, k_idx] = 0\n            if block_chunks is not None:\n                layout[q_idx, k_idx] = 0\n                offset = q_idx % block_chunks\n                if k_idx + offset >= q_idx and k_idx <= q_idx:\n                    layout[q_idx, k_idx] = 1\n    bst = BlocksparseTransformer(layout, block_size=blocksize,\n                                 mask_callback=get_callback(attn_mode, local_attn_ctx),\n                                 heads=n_heads)\n    return bst\n\n\ndef get_callback(attn_mode, local_attn_ctx=None):\n    '''Defines a function which returns the positionwise sparsity pattern for every block\n    that is enabled in the blocksparse object\n    '''\n    def cb(blk_shape, head_idx, qry_idx, key_idx, blk_idx):\n        mask = np.ones(blk_shape, dtype=np.bool)\n\n        # on the diagonal blocks mask out the upper diagonal\n        if qry_idx == key_idx:\n            for q, k in np.ndindex(blk_shape):\n                if k > q:\n                    mask[q, k] = 0\n        if attn_mode in ['all', 'strided', 'fixed']:\n            return mask\n        if attn_mode == 'local':\n            bandwidth = local_attn_ctx\n            # convert group indices to absolute indices and mask\n            # according to that\n            q_pos = blk_shape[0] * qry_idx\n            k_pos = blk_shape[1] * key_idx\n            for q, k in np.ndindex(blk_shape):\n                q_ = q + q_pos\n                k_ = k + k_pos\n                if k_ > q_ or k_ + bandwidth <= q_:\n                    mask[q, k] = 0\n            return mask\n        raise ValueError\n    return cb\n\n\nif __name__ == '__main__':\n    n_batch = 4\n    n_ctx = 1024\n    n_embd = 256\n    is_fp16 = len(sys.argv) > 1 and sys.argv[1] == 'fp16'\n\n    dtype = tf.float16 if is_fp16 else tf.float32\n    blocksize = 32\n    # query, key, values should be batch x time x dim.\n    q = tf.random_normal(shape=[4, 1024, 256], dtype=dtype)\n    k = tf.random_normal(shape=[4, 1024, 256], dtype=dtype)\n    v = tf.random_normal(shape=[4, 1024, 256], dtype=dtype)\n\n    full_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"all\", recompute=True)\n    full_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"all\", blocksize=blocksize, recompute=True)\n\n    # # first step of strided attention\n    local_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"local\", local_attn_ctx=32, blocksize=blocksize, recompute=True)\n    local_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"local\", local_attn_ctx=32, recompute=True)\n\n    # # second step of strided attention\n    strided_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"strided\", local_attn_ctx=32, blocksize=blocksize, recompute=True)\n    strided_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"strided\", local_attn_ctx=32, recompute=True)\n\n    # # # the 'fixed' attention pattern\n    fixed = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"fixed\", local_attn_ctx=128, num_verts=4, vertsize=1, blocksize=blocksize, recompute=True)\n    sess = tf.Session()\n\n    fatf, fabs, latf, labs, satf, sabs, fixed_bs = sess.run([\n        full_attn_tf, full_attn_bs, local_attn_tf, local_attn_bs, strided_attn_tf, strided_attn_bs, fixed])\n\n    print(fatf[0])\n    print(fabs[0])\n    print('-----')\n    print(latf[0])\n    print(labs[0])\n    print('-----')\n    print(satf[0])\n    print(sabs[0])\n    print('-----')\n    print(fixed_bs[0])",
    "description": null,
    "url": null
}