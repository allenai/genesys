{
    "acronym": "sinklora",
    "title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
    "seed_ids": [
        "streamingllm",
        "pi",
        "flashattn",
        "bigbird",
        "longformer"
    ],
    "s2id": "7777c897ea1abd046dd30a5d13504ef0afa02fca",
    "abstract": "Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.",
    "authors": [
        "Hengyu Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "# Modified based on https://github.com/dvlab-research/LongLoRA\n\nfrom typing import Optional, Tuple\nimport warnings\nimport torch\nimport transformers\n\n# from einops import rearrange\n# from flash_attn import flash_attn_varlen_qkvpacked_func\n# from flash_attn.bert_padding import unpad_input, pad_input\n\n\ngroup_size_ratio = 1/4\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1).to(q.dtype), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1).to(k.dtype), 2, gather_indices)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\n\ndef get_forward_function(use_flash_attn=True, use_full=False):\n\n    def forward_attention(\n        self,\n        hidden_states: torch.FloatTensor,\n        attention_mask: torch.FloatTensor,\n        position_ids: torch.LongTensor,\n        head_mask: Optional[torch.FloatTensor] = None,\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n    ):\n        # NOTE: compute SS group size\n        bsz, q_len, _ = hidden_states.size()\n        has_layer_past = layer_past is not None\n\n        # Compute QKV\n        # Attention heads [batch, seq_len, hidden_size]\n        #   --> [batch, seq_len, (np * 3 * head_size)]\n        qkv = self.query_key_value(hidden_states)\n\n        # [batch, seq_len, (num_heads * 3 * head_size)]\n        #   --> [batch, seq_len, num_heads, 3 * head_size]\n        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n        qkv = qkv.view(*new_qkv_shape)\n\n        # [batch, seq_len, num_attention_heads, 3 * head_size]\n        #   --> 3 [batch, num_attention_heads, seq_len, head_size]\n        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n        # [bsz, nh, q_len, hd]\n\n        # Compute rotary embeddings on rotary_ndims\n        query_rot = query[..., : self.rotary_ndims]\n        query_pass = query[..., self.rotary_ndims :]\n        key_rot = key[..., : self.rotary_ndims]\n        key_pass = key[..., self.rotary_ndims :]\n\n        # Compute token offset for rotary embeddings (when decoding)\n        seq_len = key.shape[-2]\n        if has_layer_past:\n            seq_len += layer_past[0].shape[-2]\n        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n        query = torch.cat((query, query_pass), dim=-1)\n        key = torch.cat((key, key_pass), dim=-1)\n\n        # Cache QKV values\n        if has_layer_past:\n            past_key = layer_past[0]\n            past_value = layer_past[1]\n            key = torch.cat((past_key, key), dim=-2)\n            value = torch.cat((past_value, value), dim=-2)\n        present = (key, value) if use_cache else None\n\n        # NOTE: apply shift\n        group_size = int(q_len * group_size_ratio)\n        if q_len % group_size > 0:\n            raise ValueError(\"q_len %d should be divisible by group size %d.\" % (q_len, group_size))\n        num_group = q_len // group_size\n        if self.training and not use_full:\n            def shift(qkv, num_heads, head_dim):\n                # qkv = [bsz, nh, q_len, d]\n                qkv = qkv.transpose(1, 2)\n                # qkv = [bsz, q_len, nh, d]\n                qkv[:, :, num_heads//2:] = qkv[:, :, num_heads//2:].roll(-group_size//2, dims=1)\n\n                # -> [bsz * n_group, group_s, nh, d)\n                #   -> [bsz * n_group, nh, group_s, d)\n                qkv = qkv.reshape(bsz * num_group, group_size, num_heads, head_dim).transpose(1, 2)\n                return qkv\n\n            # contiguous is required as self._attn() will attempt to apply .view() on them\n            query = shift(query, self.num_attention_heads, self.head_size).contiguous()\n            key = shift(key, self.num_attention_heads, self.head_size).contiguous()\n            value = shift(value, self.num_attention_heads, self.head_size).contiguous()\n\n            attention_mask = attention_mask[:, :, :group_size, :group_size].repeat(num_group, 1, 1, 1)\n\n        # Compute attention\n        if use_flash_attn:\n            attn_output, attn_weights = _flash_attn(query, key, value, attention_mask, head_mask)\n        else:\n            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n\n        # NOTE: shift back\n        if self.training and not use_full:\n            attn_output = attn_output.transpose(1, 2).contiguous()\n            attn_output = attn_output.reshape(bsz, q_len, self.num_attention_heads, self.head_size)\n            # [bsz, q_len, nh, hd]\n            attn_output[:, :, self.num_attention_heads//2:] = attn_output[:, :, self.num_attention_heads//2:].roll(group_size//2, dims=1)\n            attn_output = attn_output.transpose(1, 2)\n\n        # Reshape outputs\n        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n        attn_output = self.dense(attn_output)\n\n        outputs = (attn_output, present)\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n    return forward_attention\n\n\ndef replace_gpt_neox_attn(use_flash_attn=True, use_full=False):\n    cuda_major, cuda_minor = torch.cuda.get_device_capability()\n    if use_flash_attn and cuda_major < 8:\n        warnings.warn(\n            \"Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.\"\n            \"ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\"\n            \"Resorting to plain attention...\"\n        )\n        use_flash_attn = False\n\n    forward_fn = get_forward_function(use_flash_attn, use_full)\n    transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.forward = forward_fn",
    "description": null,
    "url": null
}