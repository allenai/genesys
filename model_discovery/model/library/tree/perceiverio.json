{
    "acronym": "perceiverio",
    "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
    "seed_ids": [
        "linformer",
        "transformer"
    ],
    "s2id": "9933a5af7895354087baf6c96b64dc8a8973eaed",
    "abstract": "A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain&task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.",
    "authors": [
        "Andrew Jaegle",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Carl Doersch",
        "Catalin Ionescu",
        "David Ding",
        "Skanda Koppula",
        "Andrew Brock",
        "Evan Shelhamer",
        "Olivier J. H'enaff",
        "M. Botvinick",
        "Andrew Zisserman",
        "O. Vinyals",
        "Jo\u00e3o Carreira"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": "This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.",
    "citationCount": 458,
    "influentialCitationCount": 46,
    "code": "import torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange, repeat\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\n# feedforward\n\ndef FeedForward(dim, mult = 4, dropout = 0.):\n    hidden_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, hidden_dim, bias = False),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim, dim, bias = False)\n    )\n\n# rotary positional embedding\n# https://arxiv.org/abs/2104.09864\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, max_seq_len, *, device):\n        seq = torch.arange(max_seq_len, device = device, dtype = self.inv_freq.dtype)\n        freqs = einsum(\"i , j -> i j\", seq, self.inv_freq)\n        return torch.cat((freqs, freqs), dim = -1)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (j d) -> ... j d\", j = 2)\n    x1, x2 = x.unbind(dim = -2)\n    return torch.cat((-x2, x1), dim = -1)\n\n\ndef apply_rotary_pos_emb(pos, t):\n    seq_len, rotate_dim = t.shape[-2], pos.shape[-1]\n    pos = pos[..., -seq_len:, :]\n    t, t_pass = t[..., :rotate_dim], t[..., rotate_dim:]\n    t = (t * pos.cos()) + (rotate_half(t) * pos.sin())\n    return torch.cat((t, t_pass), dim = -1)\n\n# attention\n\nclass CausalAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        inner_dim = heads * dim_head\n\n        self.norm = nn.LayerNorm(dim)\n        self.dropout = nn.Dropout(dropout)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n\n    def forward(self, x, rotary_pos_emb = None):\n        x = self.norm(x)\n\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n\n        q = q * self.scale\n\n        if exists(rotary_pos_emb):\n            q = apply_rotary_pos_emb(rotary_pos_emb, q)\n            k = apply_rotary_pos_emb(rotary_pos_emb, k)\n\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n\n        i, j = sim.shape[-2:]\n        causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)\n        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        attn = sim.softmax(dim = -1)\n        attn = self.dropout(attn)\n\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass CausalPrefixAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        max_heads_process = 2,\n        dropout = 0.,\n        cross_attn_dropout = 0.\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        self.max_heads_process = max_heads_process\n\n        inner_dim = heads * dim_head\n\n        self.norm = nn.LayerNorm(dim)\n        self.context_norm = nn.LayerNorm(dim)\n        self.dropout = nn.Dropout(dropout)\n\n        self.cross_attn_dropout = cross_attn_dropout # they drop out a percentage of the prefix during training, shown to help prevent overfitting\n\n        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n    def forward(self, x, context, context_mask = None, rotary_pos_emb = None):\n        batch, context_len, device = x.shape[0], context.shape[-2], x.device\n\n        q_rotary_pos_emb = rotary_pos_emb\n        k_rotary_pos_emb = rotary_pos_emb\n\n        # take care of cross attention dropout\n\n        if self.training and self.cross_attn_dropout > 0.:\n            rand = torch.zeros((batch, context_len), device = device).uniform_()\n            keep_context_len = context_len - int(context_len * self.cross_attn_dropout)\n            keep_indices = rand.topk(keep_context_len, dim = -1).indices\n            keep_mask = torch.zeros_like(rand).scatter_(1, keep_indices, 1).bool()\n\n            context = rearrange(context[keep_mask], '(b n) d -> b n d', b = batch)\n\n            if exists(context_mask):\n                context_mask = rearrange(context_mask[keep_mask], '(b n) -> b n', b = batch)\n\n            # operate on rotary position embeddings for keys\n\n            k_rotary_pos_emb = repeat(k_rotary_pos_emb, '... -> b ...', b = batch)\n            k_rotary_pos_emb_context, k_rotary_pos_emb_seq = k_rotary_pos_emb[:, :context_len], k_rotary_pos_emb[:, context_len:]\n            k_rotary_pos_emb_context = rearrange(k_rotary_pos_emb_context[keep_mask], '(b n) d -> b n d', b = batch)\n\n            k_rotary_pos_emb = torch.cat((k_rotary_pos_emb_context, k_rotary_pos_emb_seq), dim = 1)\n            k_rotary_pos_emb = rearrange(k_rotary_pos_emb, 'b n d -> b 1 n d')\n\n        # normalization\n\n        x = self.norm(x)\n        context = self.context_norm(context)\n\n        # derive queries, keys, values\n\n        q = self.to_q(x)\n\n        k_input, v_input = self.to_kv(x).chunk(2, dim = -1)\n        k_context, v_context = self.to_kv(context).chunk(2, dim = -1)\n\n        k = torch.cat((k_context, k_input), dim = 1)\n        v = torch.cat((v_context, v_input), dim = 1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n\n        q = q * self.scale\n\n        # rotate queries and keys with rotary embeddings\n\n        if exists(rotary_pos_emb):\n            q = apply_rotary_pos_emb(q_rotary_pos_emb, q)\n            k = apply_rotary_pos_emb(k_rotary_pos_emb, k)\n\n        # take care of masking\n\n        i, j = q.shape[-2], k.shape[-2]\n        mask_value = -torch.finfo(q.dtype).max\n\n        if exists(context_mask):\n            mask_len = context_mask.shape[-1]\n            context_mask = F.pad(context_mask, (0, max(j - mask_len, 0)), value = True)\n            context_mask = rearrange(context_mask, 'b j -> b 1 1 j')\n\n        causal_mask = torch.ones((i, j), device = x.device, dtype = torch.bool).triu(j - i + 1)\n\n        # process in chunks of heads\n\n        out = []\n\n        max_heads = self.max_heads_process\n\n        for q_chunk, k_chunk, v_chunk in zip(q.split(max_heads, dim = 1), k.split(max_heads, dim = 1), v.split(max_heads, dim = 1)):\n            sim = einsum('b h i d, b h j d -> b h i j', q_chunk, k_chunk)\n\n            if exists(context_mask):\n                sim = sim.masked_fill(~context_mask, mask_value)\n\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n            attn = sim.softmax(dim = -1)\n            attn = self.dropout(attn)\n\n            out_chunk = einsum('b h i j, b h j d -> b h i d', attn, v_chunk)\n            out.append(out_chunk)\n\n        # concat all the heads together\n\n        out = torch.cat(out, dim = 1)\n\n        # merge heads and then combine with linear\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n\n        return self.to_out(out)\n\nclass PerceiverAR(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        depth,\n        max_seq_len,\n        cross_attn_seq_len,\n        dim_head = 64,\n        heads = 8,\n        dropout = 0.,\n        cross_attn_dropout = 0.,\n        ff_mult = 4,\n        perceive_depth = 1,\n        perceive_max_heads_process = 2 # processes the heads in the perceiver layer in chunks to lower peak memory, in the case the prefix is really long\n    ):\n        super().__init__()\n        assert max_seq_len > cross_attn_seq_len, 'max_seq_len must be greater than cross_attn_seq_len, the length of the sequence for which to cross attend to \"perceiver\" style'\n        self.max_seq_len = max_seq_len\n        self.cross_attn_seq_len = cross_attn_seq_len\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.rotary_pos_emb = RotaryEmbedding(dim = max(32, dim_head // 2))\n\n        self.perceive_layers  = nn.ModuleList([])\n\n        for _ in range(perceive_depth):\n            self.perceive_layers.append(nn.ModuleList([\n                CausalPrefixAttention(dim = dim, dim_head = dim_head, heads = heads, max_heads_process = perceive_max_heads_process, dropout = dropout, cross_attn_dropout = cross_attn_dropout),\n                FeedForward(dim, mult = ff_mult, dropout = dropout)\n            ]))\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                CausalAttention(dim = dim, dim_head = dim_head, heads = heads),\n                FeedForward(dim, mult = ff_mult, dropout = dropout),\n            ]))\n\n        self.to_logits = nn.Linear(dim, num_tokens, bias = False)\n\n    def forward(\n        self,\n        x,\n        prefix_mask = None,\n        labels = None\n    ):\n        seq_len, device = x.shape[1], x.device\n        assert self.cross_attn_seq_len < seq_len <= self.max_seq_len\n\n        x = self.token_emb(x)\n        x = x + self.pos_emb(torch.arange(seq_len, device = device))\n\n        # rotary positional embedding\n\n        rotary_pos_emb = self.rotary_pos_emb(seq_len, device = device)\n\n        # divide into prefix to cross attend to and sequence to self attend to\n\n        prefix, x = x[:, :self.cross_attn_seq_len], x[:, self.cross_attn_seq_len:]\n\n        # initial perceiver attention and feedforward (one cross attention)\n\n        for cross_attn, ff in self.perceive_layers:\n            x = cross_attn(x, prefix, context_mask = prefix_mask, rotary_pos_emb = rotary_pos_emb) + x\n            x = ff(x) + x\n\n        # layers\n\n        for attn, ff in self.layers:\n            x = attn(x, rotary_pos_emb = rotary_pos_emb) + x\n            x = ff(x) + x\n\n        # to logits\n\n        logits = self.to_logits(x)\n\n        # take care of cross entropy loss if labels are provided\n\n        if not exists(labels):\n            return logits\n\n        labels = labels[:, self.cross_attn_seq_len:]\n        return F.cross_entropy(rearrange(logits, 'b n c -> b c n'), labels, ignore_index = 0)",
    "description": null,
    "url": null
}