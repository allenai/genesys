{
    "acronym": "s5",
    "title": "Simplified State Space Layers for Sequence Modeling",
    "seed_ids": [
        "liquids4",
        "mega",
        "httyh",
        "s4d",
        "dssm",
        "s4",
        "performer",
        "hippo",
        "lineartransformer",
        "linformer",
        "longformer",
        "reformer"
    ],
    "s2id": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
    "abstract": "Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.",
    "authors": [
        "Jimmy Smith",
        "Andrew Warrington",
        "Scott W. Linderman"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": "A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.",
    "citationCount": 232,
    "influentialCitationCount": 28,
    "code": "from functools import partial\nimport jax\nimport jax.numpy as np\nfrom flax import linen as nn\nfrom jax.nn.initializers import lecun_normal, normal\n\nfrom jax import random\nimport jax.numpy as np\nfrom jax.nn.initializers import lecun_normal\nfrom jax.numpy.linalg import eigh\n\n\n\ndef log_step_initializer(dt_min=0.001, dt_max=0.1):\n    \"\"\" Initialize the learnable timescale Delta by sampling\n         uniformly between dt_min and dt_max.\n         Args:\n             dt_min (float32): minimum value\n             dt_max (float32): maximum value\n         Returns:\n             init function\n     \"\"\"\n    def init(key, shape):\n        \"\"\" Init function\n             Args:\n                 key: jax random key\n                 shape tuple: desired shape\n             Returns:\n                 sampled log_step (float32)\n         \"\"\"\n        return random.uniform(key, shape) * (\n            np.log(dt_max) - np.log(dt_min)\n        ) + np.log(dt_min)\n\n    return init\n\n\ndef init_log_steps(key, input):\n    \"\"\" Initialize an array of learnable timescale parameters\n         Args:\n             key: jax random key\n             input: tuple containing the array shape H and\n                    dt_min and dt_max\n         Returns:\n             initialized array of timescales (float32): (H,)\n     \"\"\"\n    H, dt_min, dt_max = input\n    log_steps = []\n    for i in range(H):\n        key, skey = random.split(key)\n        log_step = log_step_initializer(dt_min=dt_min, dt_max=dt_max)(skey, shape=(1,))\n        log_steps.append(log_step)\n\n    return np.array(log_steps)\n\n\ndef init_VinvB(init_fun, rng, shape, Vinv):\n    \"\"\" Initialize B_tilde=V^{-1}B. First samples B. Then compute V^{-1}B.\n        Note we will parameterize this with two different matrices for complex\n        numbers.\n         Args:\n             init_fun:  the initialization function to use, e.g. lecun_normal()\n             rng:       jax random key to be used with init function.\n             shape (tuple): desired shape  (P,H)\n             Vinv: (complex64)     the inverse eigenvectors used for initialization\n         Returns:\n             B_tilde (complex64) of shape (P,H,2)\n     \"\"\"\n    B = init_fun(rng, shape)\n    VinvB = Vinv @ B\n    VinvB_real = VinvB.real\n    VinvB_imag = VinvB.imag\n    return np.concatenate((VinvB_real[..., None], VinvB_imag[..., None]), axis=-1)\n\n\ndef trunc_standard_normal(key, shape):\n    \"\"\" Sample C with a truncated normal distribution with standard deviation 1.\n         Args:\n             key: jax random key\n             shape (tuple): desired shape, of length 3, (H,P,_)\n         Returns:\n             sampled C matrix (float32) of shape (H,P,2) (for complex parameterization)\n     \"\"\"\n    H, P, _ = shape\n    Cs = []\n    for i in range(H):\n        key, skey = random.split(key)\n        C = lecun_normal()(skey, shape=(1, P, 2))\n        Cs.append(C)\n    return np.array(Cs)[:, 0]\n\n\ndef init_CV(init_fun, rng, shape, V):\n    \"\"\" Initialize C_tilde=CV. First sample C. Then compute CV.\n        Note we will parameterize this with two different matrices for complex\n        numbers.\n         Args:\n             init_fun:  the initialization function to use, e.g. lecun_normal()\n             rng:       jax random key to be used with init function.\n             shape (tuple): desired shape  (H,P)\n             V: (complex64)     the eigenvectors used for initialization\n         Returns:\n             C_tilde (complex64) of shape (H,P,2)\n     \"\"\"\n    C_ = init_fun(rng, shape)\n    C = C_[..., 0] + 1j * C_[..., 1]\n    CV = C @ V\n    CV_real = CV.real\n    CV_imag = CV.imag\n    return np.concatenate((CV_real[..., None], CV_imag[..., None]), axis=-1)\n\n\n# Discretization functions\ndef discretize_bilinear(Lambda, B_tilde, Delta):\n    \"\"\" Discretize a diagonalized, continuous-time linear SSM\n        using bilinear transform method.\n        Args:\n            Lambda (complex64): diagonal state matrix              (P,)\n            B_tilde (complex64): input matrix                      (P, H)\n            Delta (float32): discretization step sizes             (P,)\n        Returns:\n            discretized Lambda_bar (complex64), B_bar (complex64)  (P,), (P,H)\n    \"\"\"\n    Identity = np.ones(Lambda.shape[0])\n\n    BL = 1 / (Identity - (Delta / 2.0) * Lambda)\n    Lambda_bar = BL * (Identity + (Delta / 2.0) * Lambda)\n    B_bar = (BL * Delta)[..., None] * B_tilde\n    return Lambda_bar, B_bar\n\n\ndef discretize_zoh(Lambda, B_tilde, Delta):\n    \"\"\" Discretize a diagonalized, continuous-time linear SSM\n        using zero-order hold method.\n        Args:\n            Lambda (complex64): diagonal state matrix              (P,)\n            B_tilde (complex64): input matrix                      (P, H)\n            Delta (float32): discretization step sizes             (P,)\n        Returns:\n            discretized Lambda_bar (complex64), B_bar (complex64)  (P,), (P,H)\n    \"\"\"\n    Identity = np.ones(Lambda.shape[0])\n    Lambda_bar = np.exp(Lambda * Delta)\n    B_bar = (1/Lambda * (Lambda_bar-Identity))[..., None] * B_tilde\n    return Lambda_bar, B_bar\n\n\n# Parallel scan operations\n@jax.vmap\ndef binary_operator(q_i, q_j):\n    \"\"\" Binary operator for parallel scan of linear recurrence. Assumes a diagonal matrix A.\n        Args:\n            q_i: tuple containing A_i and Bu_i at position i       (P,), (P,)\n            q_j: tuple containing A_j and Bu_j at position j       (P,), (P,)\n        Returns:\n            new element ( A_out, Bu_out )\n    \"\"\"\n    A_i, b_i = q_i\n    A_j, b_j = q_j\n    return A_j * A_i, A_j * b_i + b_j\n\n\ndef apply_ssm(Lambda_bar, B_bar, C_tilde, input_sequence, conj_sym, bidirectional):\n    \"\"\" Compute the LxH output of discretized SSM given an LxH input.\n        Args:\n            Lambda_bar (complex64): discretized diagonal state matrix    (P,)\n            B_bar      (complex64): discretized input matrix             (P, H)\n            C_tilde    (complex64): output matrix                        (H, P)\n            input_sequence (float32): input sequence of features         (L, H)\n            conj_sym (bool):         whether conjugate symmetry is enforced\n            bidirectional (bool):    whether bidirectional setup is used,\n                                  Note for this case C_tilde will have 2P cols\n        Returns:\n            ys (float32): the SSM outputs (S5 layer preactivations)      (L, H)\n    \"\"\"\n    Lambda_elements = Lambda_bar * np.ones((input_sequence.shape[0],\n                                            Lambda_bar.shape[0]))\n    Bu_elements = jax.vmap(lambda u: B_bar @ u)(input_sequence)\n\n    _, xs = jax.lax.associative_scan(binary_operator, (Lambda_elements, Bu_elements))\n\n    if bidirectional:\n        _, xs2 = jax.lax.associative_scan(binary_operator,\n                                          (Lambda_elements, Bu_elements),\n                                          reverse=True)\n        xs = np.concatenate((xs, xs2), axis=-1)\n\n    if conj_sym:\n        return jax.vmap(lambda x: 2*(C_tilde @ x).real)(xs)\n    else:\n        return jax.vmap(lambda x: (C_tilde @ x).real)(xs)\n\n\nclass S5SSM(nn.Module):\n    Lambda_re_init: np.DeviceArray\n    Lambda_im_init: np.DeviceArray\n    V: np.DeviceArray\n    Vinv: np.DeviceArray\n\n    H: int\n    P: int\n    C_init: str\n    discretization: str\n    dt_min: float\n    dt_max: float\n    conj_sym: bool = True\n    clip_eigs: bool = False\n    bidirectional: bool = False\n    step_rescale: float = 1.0\n\n    \"\"\" The S5 SSM\n        Args:\n            Lambda_re_init (complex64): Real part of init diag state matrix  (P,)\n            Lambda_im_init (complex64): Imag part of init diag state matrix  (P,)\n            V           (complex64): Eigenvectors used for init           (P,P)\n            Vinv        (complex64): Inverse eigenvectors used for init   (P,P)\n            H           (int32):     Number of features of input seq \n            P           (int32):     state size\n            C_init      (string):    Specifies How C is initialized\n                         Options: [trunc_standard_normal: sample from truncated standard normal \n                                                        and then multiply by V, i.e. C_tilde=CV.\n                                   lecun_normal: sample from Lecun_normal and then multiply by V.\n                                   complex_normal: directly sample a complex valued output matrix \n                                                    from standard normal, does not multiply by V]\n            conj_sym    (bool):    Whether conjugate symmetry is enforced\n            clip_eigs   (bool):    Whether to enforce left-half plane condition, i.e.\n                                   constrain real part of eigenvalues to be negative. \n                                   True recommended for autoregressive task/unbounded sequence lengths\n                                   Discussed in https://arxiv.org/pdf/2206.11893.pdf.\n            bidirectional (bool):  Whether model is bidirectional, if True, uses two C matrices\n            discretization: (string) Specifies discretization method \n                             options: [zoh: zero-order hold method,\n                                       bilinear: bilinear transform]\n            dt_min:      (float32): minimum value to draw timescale values from when \n                                    initializing log_step\n            dt_max:      (float32): maximum value to draw timescale values from when \n                                    initializing log_step\n            step_rescale:  (float32): allows for uniformly changing the timescale parameter, e.g. after training \n                                    on a different resolution for the speech commands benchmark\n    \"\"\"\n\n    def setup(self):\n        \"\"\"Initializes parameters once and performs discretization each time\n           the SSM is applied to a sequence\n        \"\"\"\n\n        if self.conj_sym:\n            # Need to account for case where we actually sample real B and C, and then multiply\n            # by the half sized Vinv and possibly V\n            local_P = 2*self.P\n        else:\n            local_P = self.P\n\n        # Initialize diagonal state to state matrix Lambda (eigenvalues)\n        self.Lambda_re = self.param(\"Lambda_re\", lambda rng, shape: self.Lambda_re_init, (None,))\n        self.Lambda_im = self.param(\"Lambda_im\", lambda rng, shape: self.Lambda_im_init, (None,))\n        if self.clip_eigs:\n            self.Lambda = np.clip(self.Lambda_re, None, -1e-4) + 1j * self.Lambda_im\n        else:\n            self.Lambda = self.Lambda_re + 1j * self.Lambda_im\n\n        # Initialize input to state (B) matrix\n        B_init = lecun_normal()\n        B_shape = (local_P, self.H)\n        self.B = self.param(\"B\",\n                            lambda rng, shape: init_VinvB(B_init,\n                                                          rng,\n                                                          shape,\n                                                          self.Vinv),\n                            B_shape)\n        B_tilde = self.B[..., 0] + 1j * self.B[..., 1]\n\n        # Initialize state to output (C) matrix\n        if self.C_init in [\"trunc_standard_normal\"]:\n            C_init = trunc_standard_normal\n            C_shape = (self.H, local_P, 2)\n        elif self.C_init in [\"lecun_normal\"]:\n            C_init = lecun_normal()\n            C_shape = (self.H, local_P, 2)\n        elif self.C_init in [\"complex_normal\"]:\n            C_init = normal(stddev=0.5 ** 0.5)\n        else:\n            raise NotImplementedError(\n                   \"C_init method {} not implemented\".format(self.C_init))\n\n        if self.C_init in [\"complex_normal\"]:\n            if self.bidirectional:\n                C = self.param(\"C\", C_init, (self.H, 2 * self.P, 2))\n                self.C_tilde = C[..., 0] + 1j * C[..., 1]\n\n            else:\n                C = self.param(\"C\", C_init, (self.H, self.P, 2))\n                self.C_tilde = C[..., 0] + 1j * C[..., 1]\n\n        else:\n            if self.bidirectional:\n                self.C1 = self.param(\"C1\",\n                                     lambda rng, shape: init_CV(C_init, rng, shape, self.V),\n                                     C_shape)\n                self.C2 = self.param(\"C2\",\n                                     lambda rng, shape: init_CV(C_init, rng, shape, self.V),\n                                     C_shape)\n\n                C1 = self.C1[..., 0] + 1j * self.C1[..., 1]\n                C2 = self.C2[..., 0] + 1j * self.C2[..., 1]\n                self.C_tilde = np.concatenate((C1, C2), axis=-1)\n\n            else:\n                self.C = self.param(\"C\",\n                                    lambda rng, shape: init_CV(C_init, rng, shape, self.V),\n                                    C_shape)\n\n                self.C_tilde = self.C[..., 0] + 1j * self.C[..., 1]\n\n        # Initialize feedthrough (D) matrix\n        self.D = self.param(\"D\", normal(stddev=1.0), (self.H,))\n\n        # Initialize learnable discretization timescale value\n        self.log_step = self.param(\"log_step\",\n                                   init_log_steps,\n                                   (self.P, self.dt_min, self.dt_max))\n        step = self.step_rescale * np.exp(self.log_step[:, 0])\n\n        # Discretize\n        if self.discretization in [\"zoh\"]:\n            self.Lambda_bar, self.B_bar = discretize_zoh(self.Lambda, B_tilde, step)\n        elif self.discretization in [\"bilinear\"]:\n            self.Lambda_bar, self.B_bar = discretize_bilinear(self.Lambda, B_tilde, step)\n        else:\n            raise NotImplementedError(\"Discretization method {} not implemented\".format(self.discretization))\n\n    def __call__(self, input_sequence):\n        \"\"\"\n        Compute the LxH output of the S5 SSM given an LxH input sequence\n        using a parallel scan.\n        Args:\n             input_sequence (float32): input sequence (L, H)\n        Returns:\n            output sequence (float32): (L, H)\n        \"\"\"\n        ys = apply_ssm(self.Lambda_bar,\n                       self.B_bar,\n                       self.C_tilde,\n                       input_sequence,\n                       self.conj_sym,\n                       self.bidirectional)\n\n        # Add feedthrough matrix output Du;\n        Du = jax.vmap(lambda u: self.D * u)(input_sequence)\n        return ys + Du\n\n\ndef init_S5SSM(H,\n               P,\n               Lambda_re_init,\n               Lambda_im_init,\n               V,\n               Vinv,\n               C_init,\n               discretization,\n               dt_min,\n               dt_max,\n               conj_sym,\n               clip_eigs,\n               bidirectional\n               ):\n    \"\"\"Convenience function that will be used to initialize the SSM.\n       Same arguments as defined in S5SSM above.\"\"\"\n    return partial(S5SSM,\n                   H=H,\n                   P=P,\n                   Lambda_re_init=Lambda_re_init,\n                   Lambda_im_init=Lambda_im_init,\n                   V=V,\n                   Vinv=Vinv,\n                   C_init=C_init,\n                   discretization=discretization,\n                   dt_min=dt_min,\n                   dt_max=dt_max,\n                   conj_sym=conj_sym,\n                   clip_eigs=clip_eigs,\n                   bidirectional=bidirectional)",
    "description": null,
    "url": null
}