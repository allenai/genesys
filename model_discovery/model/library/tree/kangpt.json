{
    "acronym": "kangpt",
    "title": "Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling",
    "seed_ids": [
        "gpt3",
        "transformer"
    ],
    "s2id": null,
    "abstract": null,
    "authors": [
        "Aditya N Ganesh"
    ],
    "venue": null,
    "year": 2024,
    "tldr": null,
    "citationCount": null,
    "influentialCitationCount": null,
    "code": "",
    "description": "Kolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while MLPs have activation functions on nodes. This simple change makes KANs better (sometimes much better!) than MLPs in terms of both model accuracy and interpretability. ",
    "url": "https://adityang.github.io/kan-gpt/"
}