{
    "acronym": "rmt",
    "title": "Recurrent Memory Transformer",
    "seed_ids": [
        "transformerxl",
        "transformer"
    ],
    "s2id": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca",
    "abstract": "Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.",
    "authors": [
        "Aydar Bulatov",
        "Yuri Kuratov",
        "M. Burtsev"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": "Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.",
    "citationCount": 68,
    "influentialCitationCount": 10,
    "code": "import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import PositionwiseFF\n\nsys.path.append('utils')\n\n\n\nclass RelMultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False):\n        super(RelMultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n    def _parallelogram_mask(self, h, w, left=False):\n        mask = torch.ones((h, w)).byte()\n        m = min(h, w)\n        mask[:m,:m] = torch.triu(mask[:m,:m])\n        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n\n        if left:\n            return mask\n        else:\n            return mask.flip(0)\n\n    def _shift(self, x, qlen, klen, mask, left=False):\n        if qlen > 1:\n            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n                                    device=x.device, dtype=x.dtype)\n        else:\n            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n\n        if left:\n            mask = mask.flip(1)\n            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n        else:\n            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n\n        x = x_padded.masked_select(mask[:,:,None,None]) \\\n                    .view(qlen, klen, x.size(2), x.size(3))\n\n        return x\n\n    def _rel_shift(self, x, zero_triu=False):\n        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n                               device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n\n        x = x_padded[1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(0), x.size(1)))\n            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        raise NotImplementedError\n\nclass RelPartialLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelPartialLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n\n    def forward(self, w, r, r_w_bias, r_r_bias, attn_mask=None, mems=None):\n        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n\n        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + r_r_bias\n        BD = torch.einsum('ibnd,jnd->ijbn', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[None,:,:,None].bool(), -float('inf')).type_as(attn_score)\n            elif attn_mask.dim() == 3:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[:,:,:,None].bool(), -float('inf')).type_as(attn_score)\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass RelLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n    def forward(self, w, r_emb, r_w_bias, r_bias, attn_mask=None, mems=None):\n        # r_emb: [klen, n_head, d_head], used for term B\n        # r_w_bias: [n_head, d_head], used for term C\n        # r_bias: [klen, n_head], used for term D\n\n        qlen, bsz = w.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)\n\n        if klen > r_emb.size(0):\n            r_emb_pad = r_emb[0:1].expand(klen-r_emb.size(0), -1, -1)\n            r_emb = torch.cat([r_emb_pad, r_emb], 0)\n            r_bias_pad = r_bias[0:1].expand(klen-r_bias.size(0), -1)\n            r_bias = torch.cat([r_bias_pad, r_bias], 0)\n        else:\n            r_emb = r_emb[-klen:]\n            r_bias = r_bias[-klen:]\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias[None]                                   # qlen x bsz x n_head x d_head\n\n        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n        B_ = torch.einsum('ibnd,jnd->ijbn', (w_head_q, r_emb))                  # qlen x klen x bsz x n_head\n        D_ = r_bias[None, :, None]                                              # 1    x klen x 1   x n_head\n        BD = self._rel_shift(B_ + D_)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None].bool(), -float('inf'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None].bool(), -float('inf'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\n\nclass RelLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelLearnableMultiHeadAttn(n_head, d_model, d_head, dropout,\n                                         **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n\n    def forward(self, dec_inp, r_emb, r_w_bias, r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r_emb, r_w_bias, r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelPartialLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelPartialLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n                            d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get('pre_lnorm'))\n\n    def forward(self, dec_inp, r, r_w_bias, r_r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\n\n\n",
    "description": null,
    "url": null
}