{
    "acronym": "reformer",
    "title": "Reformer: The Efficient Transformer",
    "seed_ids": [
        "productkeymem",
        "sparsetransformer",
        "transformer",
        "gpt2",
        "bert"
    ],
    "s2id": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
    "authors": [
        "Nikita Kitaev",
        "Lukasz Kaiser",
        "Anselm Levskaya"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "tldr": "This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.",
    "citationCount": 1881,
    "influentialCitationCount": 222,
    "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial, reduce, wraps\nfrom operator import mul\n\nfrom local_attention import LocalAttention\n\n#constants\n\nTOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work\n\n# helper fns\n\ndef exists(val):\n    return val is not None\n\ndef sort_key_val(t1, t2, dim=-1):\n    values, indices = t1.sort(dim=dim)\n    t2 = t2.expand_as(t1)\n    return values, t2.gather(dim, indices)\n\ndef batched_index_select(values, indices):\n    last_dim = values.shape[-1]\n    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n\ndef process_inputs_chunk(fn, chunks=1, dim=0):\n    def inner_fn(*args, **kwargs):\n        keys, values, len_args = kwargs.keys(), kwargs.values(), len(args)\n        chunked_args = list(zip(*map(lambda x: x.chunk(chunks, dim=dim), list(args) + list(values))))\n        all_args = map(lambda x: (x[:len_args], dict(zip(keys, x[len_args:]))), chunked_args)\n        outputs = [fn(*c_args, **c_kwargs) for c_args, c_kwargs in all_args]\n        return tuple(map(lambda x: torch.cat(x, dim=dim), zip(*outputs)))\n    return inner_fn\n\ndef chunked_sum(tensor, chunks=1):\n    *orig_size, last_dim = tensor.shape\n    tensor = tensor.reshape(-1, last_dim)\n    summed_tensors = [c.sum(dim=-1) for c in tensor.chunk(chunks, dim=0)]\n    return torch.cat(summed_tensors, dim=0).reshape(orig_size)\n\ndef default(val, default_val):\n    return default_val if val is None else val\n\ndef cast_tuple(x):\n    return x if isinstance(x, tuple) else (x,)\n\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\n\ndef cache_fn(f):\n    cache = None\n    @wraps(f)\n    def cached_fn(*args, **kwargs):\n        nonlocal cache\n        if cache is not None:\n            return cache\n        cache = f(*args, **kwargs)\n        return cache\n    return cached_fn\n\ndef cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n    def inner_fn(fn):\n        @wraps(fn)\n        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n            namespace_str = str(default(key_namespace, ''))\n            _cache = getattr(self, cache_attr)\n            _keyname = f'{cache_namespace}:{namespace_str}'\n\n            if fetch:\n                val = _cache[_keyname]\n                if reexecute:\n                    fn(self, *args, **kwargs)\n            else:\n                val = fn(self, *args, **kwargs)\n                if set_cache:\n                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n            return val\n        return wrapper\n    return inner_fn\n\ndef expand_dim(dim, k, t):\n    t = t.unsqueeze(dim)\n    expand_shape = [-1] * len(t.shape)\n    expand_shape[dim] = k\n    return t.expand(*expand_shape)\n\ndef merge_dims(ind_from, ind_to, tensor):\n    shape = list(tensor.shape)\n    arr_slice = slice(ind_from, ind_to + 1)\n    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\n    return tensor.reshape(*shape)\n\ndef split_at_index(dim, index, t):\n    pre_slices = (slice(None),) * dim\n    l = (*pre_slices, slice(None, index))\n    r = (*pre_slices, slice(index, None))\n    return t[l], t[r]\n\n# helper classes\n\nclass Chunk(nn.Module):\n    def __init__(self, chunks, fn, along_dim = -1):\n        super().__init__()\n        self.dim = along_dim\n        self.chunks = chunks\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        if self.chunks == 1:\n            return self.fn(x, **kwargs)\n        chunks = x.chunk(self.chunks, dim = self.dim)\n        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)\n\n# LSH attention as described in https://openreview.net/pdf?id=rkgNKkHtvB\n# adapted from trax, stripped to what paper said needed to work\n# namely that buckets need to be at least 64 with 8 rounds of hashing\n# https://github.com/google/trax/blob/master/trax/layers/research/efficient_attention.py#L442\n\nclass LSHAttention(nn.Module):\n    def __init__( self,\n                  dropout = 0.,\n                  bucket_size = 64,\n                  n_hashes = 8,\n                  causal = False,\n                  allow_duplicate_attention = True,\n                  attend_across_buckets = True,\n                  rehash_each_round = True,\n                  drop_for_hash_rate = 0.0,\n                  random_rotations_per_head = False,\n                  return_attn = False):\n        super().__init__()\n        if dropout >= 1.0:\n            raise ValueError('Dropout rates must be lower than 1.')\n\n        self.dropout = nn.Dropout(dropout)\n        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n\n        assert rehash_each_round or allow_duplicate_attention, (\n            'The setting {allow_duplicate_attention=False, rehash_each_round=False}'\n            ' is not implemented.')\n\n        self.causal = causal\n        self.bucket_size = bucket_size\n\n        self.n_hashes = n_hashes\n\n        self._allow_duplicate_attention = allow_duplicate_attention\n        self._attend_across_buckets = attend_across_buckets\n        self._rehash_each_round = rehash_each_round\n        self._random_rotations_per_head = random_rotations_per_head\n\n        # will expend extra computation to return attention matrix\n        self._return_attn = return_attn\n\n        # cache buckets for reversible network, reported by authors to make Reformer work at depth\n        self._cache = {}\n\n    @cache_method_decorator('_cache', 'buckets', reexecute=True)\n    def hash_vectors(self, n_buckets, vecs):\n        batch_size = vecs.shape[0]\n        device = vecs.device\n\n        # See https://arxiv.org/pdf/1509.02897.pdf\n        # We sample a different random rotation for each round of hashing to\n        # decrease the probability of hash misses.\n        assert n_buckets % 2 == 0\n\n        rot_size = n_buckets\n\n        rotations_shape = (\n            batch_size if self._random_rotations_per_head else 1,\n            vecs.shape[-1],\n            self.n_hashes if self._rehash_each_round else 1,\n            rot_size // 2)\n\n        random_rotations = torch.randn(rotations_shape, dtype=vecs.dtype, device=device).expand(batch_size, -1, -1, -1)\n\n        dropped_vecs = self.dropout_for_hash(vecs)\n        rotated_vecs = torch.einsum('btf,bfhi->bhti', dropped_vecs, random_rotations)\n\n        if self._rehash_each_round:\n            # rotated_vectors size [batch,n_hash,seq_len,buckets]\n            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n            buckets = torch.argmax(rotated_vecs, dim=-1)\n        else:\n            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n            # In this configuration, we map each item to the top self.n_hashes buckets\n            rotated_vecs = torch.squeeze(rotated_vecs, 1)\n            bucket_range = torch.arange(rotated_vecs.shape[-1], device=device)\n            bucket_range = torch.reshape(bucket_range, (1, -1))\n            bucket_range = bucket_range.expand_as(rotated_vecs)\n\n            _, buckets = sort_key_val(rotated_vecs, bucket_range, dim=-1)\n            # buckets size [batch size, seq_len, buckets]\n            buckets = buckets[... , -self.n_hashes:].transpose(1, 2)\n\n        # buckets is now (self.n_hashes, seq_len). Next we add offsets so that\n        # bucket numbers from different hashing rounds don't overlap.\n        offsets = torch.arange(self.n_hashes, device=device)\n        offsets = torch.reshape(offsets * n_buckets, (1, -1, 1))\n        buckets = torch.reshape(buckets + offsets, (batch_size, -1,))\n        return buckets\n\n    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, pos_emb = None, **kwargs):\n        batch_size, seqlen, dim, device = *qk.shape, qk.device\n\n        query_len = default(query_len, seqlen)\n        is_reverse = kwargs.pop('_reverse', False)\n        depth = kwargs.pop('_depth', None)\n\n        assert seqlen % (self.bucket_size * 2) == 0, f'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}'\n\n        n_buckets = seqlen // self.bucket_size\n        buckets = self.hash_vectors(n_buckets, qk, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n\n        # We use the same vector as both a query and a key.\n        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n\n        total_hashes = self.n_hashes\n\n        ticker = torch.arange(total_hashes * seqlen, device=device).unsqueeze(0).expand_as(buckets)\n        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n        buckets_and_t = buckets_and_t.detach()\n\n        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n        _, undo_sort = sticker.sort(dim=-1)\n        del ticker\n\n        sbuckets_and_t = sbuckets_and_t.detach()\n        sticker = sticker.detach()\n        undo_sort = undo_sort.detach()\n\n        if exists(pos_emb):\n            qk = apply_rotary_pos_emb(qk, pos_emb)\n\n        st = (sticker % seqlen)\n        sqk = batched_index_select(qk, st)\n        sv = batched_index_select(v, st)\n\n        # Split off a \"bin\" axis so that attention only occurs within chunks.\n        chunk_size = total_hashes * n_buckets\n        bq_t = bkv_t = torch.reshape(st, (batch_size, chunk_size, -1))\n        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n\n        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n        # fine because they effectively provide a learnable temperature for the\n        # attention softmax, but normalizing keys is needed so that similarity for\n        # the purposes of attention correctly corresponds to hash locality.\n        bq = bqk\n        bk = F.normalize(bqk, p=2, dim=-1).type_as(bq)\n\n        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n        # boundaries might occur in the middle of a sequence of items from the\n        # same bucket, so this increases the chances of attending to relevant items.\n        def look_one_back(x):\n            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n            return torch.cat([x, x_extra], dim=2)\n\n        bk = look_one_back(bk)\n        bv = look_one_back(bv)\n        bkv_t = look_one_back(bkv_t)\n\n        # Dot-product attention.\n        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n        masked_value = max_neg_value(dots)\n\n        # Mask for post qk attention logits of the input sequence\n        if input_attn_mask is not None:\n            input_attn_mask = F.pad(input_attn_mask, (0, seqlen - input_attn_mask.shape[-1], 0, seqlen - input_attn_mask.shape[-2]), value=True)\n            dot_attn_indices = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n            input_attn_mask = input_attn_mask.reshape(batch_size, -1)\n            dot_attn_indices = dot_attn_indices.reshape(batch_size, -1)\n            mask = input_attn_mask.gather(1, dot_attn_indices).reshape_as(dots)\n            dots.masked_fill_(~mask, masked_value)\n            del mask\n\n        # Input mask for padding in variable lengthed sequences\n        if input_mask is not None:\n            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n            mkv = look_one_back(mq)\n            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n            dots.masked_fill_(~mask, masked_value)\n            del mask\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n            if seqlen > query_len:\n                mask = mask & (bkv_t[:, :, None, :] < query_len)\n            dots.masked_fill_(mask, masked_value)\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, chunk_size, -1))\n            bkv_buckets = look_one_back(bkv_buckets)\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, masked_value)\n            del bucket_mask\n\n        # Don't double-count query-key pairs across multiple rounds of hashing.\n        # There are two possible strategies here. (1) The default is to count how\n        # many times a query-key pair is repeated, and to lower its log-prob\n        # correspondingly at each repetition. (2) When hard_k is set, the code\n        # instead masks all but the first occurence of each query-key pair.\n        if not self._allow_duplicate_attention:\n            locs1 = undo_sort // bq_t.shape[-1]\n            locs2 = (locs1 + 1) % chunk_size\n            if not self._attend_across_buckets:\n                locs1 = buckets * chunk_size + locs1\n                locs2 = buckets * chunk_size + locs2\n            locs = torch.cat([\n                torch.reshape(locs1, (batch_size, total_hashes, seqlen)),\n                torch.reshape(locs2, (batch_size, total_hashes, seqlen)),\n            ], 1).permute((0, 2, 1))\n\n            slocs = batched_index_select(locs, st)\n            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * total_hashes))\n\n            b_locs1 = b_locs[:, :, :, None, :total_hashes]\n\n            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, total_hashes))\n            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n            bkv_locs = look_one_back(b_locs)\n\n            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n            # for memory considerations, chunk summation of last dimension for counting duplicates\n            dup_counts = chunked_sum(dup_counts, chunks=(total_hashes * batch_size))\n            dup_counts = dup_counts.detach()\n            assert dup_counts.shape == dots.shape\n            dots = dots - torch.log(dup_counts + 1e-9)\n            del dup_counts\n\n        # Softmax.\n        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n        dots = torch.exp(dots - dots_logsumexp).type_as(dots)\n        dropped_dots = self.dropout(dots)\n\n        bo = torch.einsum('buij,buje->buie', dropped_dots, bv)\n        so = torch.reshape(bo, (batch_size, -1, dim))\n        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n\n        # unsort logits\n        o = batched_index_select(so, undo_sort)\n        logits = slogits.gather(1, undo_sort)\n\n        o = torch.reshape(o, (batch_size, total_hashes, seqlen, dim))\n        logits = torch.reshape(logits, (batch_size, total_hashes, seqlen, 1))\n\n        if query_len != seqlen:\n            query_slice = (slice(None), slice(None), slice(0, query_len))\n            o, logits = o[query_slice], logits[query_slice]\n\n        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True))\n        out = torch.sum(o * probs, dim=1)\n\n        attn = torch.empty(0, device=device)\n\n        # return unsorted attention weights\n        if self._return_attn:\n            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n            attn_unsort = attn_unsort.view(batch_size * total_hashes, -1).long()\n            unsorted_dots = torch.zeros(batch_size * total_hashes, seqlen * seqlen, device=device)\n            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n            del attn_unsort\n            unsorted_dots = unsorted_dots.reshape(batch_size, total_hashes, seqlen, seqlen)\n            attn = torch.sum(unsorted_dots[:, :, 0:query_len, :] * probs, dim=1)\n\n        # return output, attention matrix, and bucket distribution\n        return out, attn, buckets\n\n# Shared qk attention, using either full or LSH attention\n\nclass LSHSelfAttention(nn.Module):\n    def __init__(self, dim, heads = 8, bucket_size = 64, n_hashes = 8, causal = False, dim_head = None, attn_chunks = 1, random_rotations_per_head = False, attend_across_buckets = True, allow_duplicate_attention = True, num_mem_kv = 0, one_value_head = False, use_full_attn = False, full_attn_thres = None, return_attn = False, post_attn_dropout = 0., dropout = 0., n_local_attn_heads = 0, **kwargs):\n        super().__init__()\n        assert dim_head or (dim % heads) == 0, 'dimensions must be divisible by number of heads'\n        assert n_local_attn_heads < heads, 'local attention heads must be less than number of heads'\n\n        dim_head = default(dim_head, dim // heads)\n        dim_heads = dim_head * heads\n\n        self.dim = dim\n        self.heads = heads\n        self.dim_head = dim_head\n        self.attn_chunks = default(attn_chunks, 1)\n\n        self.v_head_repeats = (heads if one_value_head else 1)\n        v_dim = dim_heads // self.v_head_repeats\n\n        self.toqk = nn.Linear(dim, dim_heads, bias = False)\n        self.tov = nn.Linear(dim, v_dim, bias = False)\n        self.to_out = nn.Linear(dim_heads, dim)\n\n        self.bucket_size = bucket_size\n        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, random_rotations_per_head=random_rotations_per_head, attend_across_buckets = attend_across_buckets,  allow_duplicate_attention = allow_duplicate_attention, return_attn = return_attn, dropout = dropout, **kwargs)\n        self.full_attn = FullQKAttention(causal=causal, dropout=dropout)\n        self.post_attn_dropout = nn.Dropout(post_attn_dropout)\n\n        self.use_full_attn = use_full_attn\n        self.full_attn_thres = default(full_attn_thres, bucket_size)\n\n        self.num_mem_kv = num_mem_kv\n        self.mem_kv = nn.Parameter(torch.randn(1, num_mem_kv, dim, requires_grad=True)) if num_mem_kv > 0 else None\n\n        self.n_local_attn_heads = n_local_attn_heads\n        self.local_attn = LocalAttention(window_size=bucket_size * 2, causal=causal, dropout=dropout, shared_qk=True, look_forward=(1 if not causal else 0))\n\n        self.callback = None\n\n    def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, context_mask = None, pos_emb = None, **kwargs):\n        device, dtype = x.device, x.dtype\n        b, t, e, h, dh, m, l_h = *x.shape, self.heads, self.dim_head, self.num_mem_kv, self.n_local_attn_heads\n\n        mem_kv = default(self.mem_kv, torch.empty(b, 0, e, dtype=dtype, device=device))\n        mem = mem_kv.expand(b, m, -1)\n\n        keys = default(keys, torch.empty(b, 0, e, dtype=dtype, device=device))\n        c = keys.shape[1]\n\n        kv_len = t + m + c\n        use_full_attn = self.use_full_attn or kv_len <= self.full_attn_thres\n\n        x = torch.cat((x, mem, keys), dim=1)\n        qk = self.toqk(x)\n        v = self.tov(x)\n        v = v.repeat(1, 1, self.v_head_repeats)\n\n        def merge_heads(v):\n            return v.view(b, kv_len, h, -1).transpose(1, 2)\n\n        def split_heads(v):\n            return v.view(b, h, t, -1).transpose(1, 2).contiguous()\n\n        merge_batch_and_heads = partial(merge_dims, 0, 1)\n\n        qk, v = map(merge_heads, (qk, v))\n\n        has_local = l_h > 0\n        lsh_h = h - l_h\n\n        split_index_fn = partial(split_at_index, 1, l_h)\n        (lqk, qk), (lv, v) = map(split_index_fn, (qk, v))\n        lqk, qk, lv, v = map(merge_batch_and_heads, (lqk, qk, lv, v))\n\n        masks = {}\n        if input_mask is not None or context_mask is not None:\n            default_mask = torch.tensor([True], device=device)\n            i_mask = default(input_mask, default_mask.expand(b, t))\n            m_mask = default_mask.expand(b, m)\n            c_mask = default(context_mask, default_mask.expand(b, c))\n            mask = torch.cat((i_mask, m_mask, c_mask), dim=1)\n            mask = merge_batch_and_heads(expand_dim(1, lsh_h, mask))\n            masks['input_mask'] = mask\n\n        if input_attn_mask is not None:\n            input_attn_mask = merge_batch_and_heads(expand_dim(1, lsh_h, input_attn_mask))\n            masks['input_attn_mask'] = input_attn_mask\n\n        attn_fn = self.lsh_attn if not use_full_attn else self.full_attn\n        partial_attn_fn = partial(attn_fn, query_len = t, pos_emb = pos_emb, **kwargs)\n        attn_fn_in_chunks = process_inputs_chunk(partial_attn_fn, chunks = self.attn_chunks)\n\n        out, attn, buckets = attn_fn_in_chunks(qk, v, **masks)\n\n        if self.callback is not None:\n            self.callback(attn.reshape(b, lsh_h, t, -1), buckets.reshape(b, lsh_h, -1))\n\n        if has_local:\n            lqk, lv = lqk[:, :t], lv[:, :t]\n            local_out = self.local_attn(lqk, lqk, lv, input_mask=input_mask)\n            local_out = local_out.reshape(b, l_h, t, -1)\n            out = out.reshape(b, lsh_h, t, -1)\n            out = torch.cat((local_out, out), dim=1)\n\n        out = split_heads(out).view(b, t, -1)\n        out = self.to_out(out)\n        return self.post_attn_dropout(out)\n",
    "description": null,
    "url": null
}