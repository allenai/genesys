{
    "acronym": "ttt",
    "title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States",
    "seed_ids": [
        "mamba"
    ],
    "s2id": "35ab93f41115e860bee5e202b71061addfd0fd5d",
    "abstract": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
    "authors": [
        "Yu Sun",
        "Xinhao Li",
        "Karan Dalal",
        "Jiarui Xu",
        "Arjun Vikram",
        "Genghan Zhang",
        "Yann Dubois",
        "Xinlei Chen",
        "Xiaolong Wang",
        "Sanmi Koyejo",
        "Tatsunori Hashimoto",
        "Carlos Guestrin"
    ],
    "venue": "",
    "year": 2024,
    "tldr": "With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": "# gab.py\n\nimport torch\nimport torch.nn as nn\n\nfrom model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n\n\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\n\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n# from transformers.utils.import_utils import is_causal_conv1d_available # CAUSE EARLY INIT CUDA\n\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\nlogger = logging.get_logger(__name__)\n\n\n# YOU CAN IMPORT MORE MODULES HERE #\n\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n\n\n# TODO: Further simplify, a lot of code are actually RoPE\n\n\n\n\n########################\n### Backbone Modules ###\n########################\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef permute_qk(q, k):\n    # NOTE: EasyLM and transformers use different method to compute rotary emebdding\n    # we manually reorder the dim here to match our JAX implementation\n    # which may not be optimal for speed\n    # reference: https://github.com/young-geng/EasyLM/blob/981a2ed9630f44258a94b6f44dff2b7bd203ae8d/EasyLM/models/llama/convert_hf_to_easylm.py#L33\n    bsz, num_head, seq_len, head_dim = q.shape\n    q = q.reshape(bsz, num_head, seq_len, head_dim // 2, 2).transpose(3, 4).reshape(bsz, num_head, seq_len, head_dim)\n    k = k.reshape(bsz, num_head, seq_len, head_dim // 2, 2).transpose(3, 4).reshape(bsz, num_head, seq_len, head_dim)\n\n    return q, k\n\n\ndef undo_permute_qk(q, k):\n    # NOTE: EasyLM and transformers use different method to compute rotary emebdding\n    # we manually undo the reorder the dim here to match our JAX implementation\n    # which may not be optimal for speed\n    # reference: https://github.com/young-geng/EasyLM/blob/981a2ed9630f44258a94b6f44dff2b7bd203ae8d/EasyLM/models/llama/convert_hf_to_easylm.py#L33\n    bsz, num_head, seq_len, head_dim = q.shape\n    q = q.reshape(bsz, num_head, seq_len, 2, head_dim // 2).transpose(3, 4).reshape(bsz, num_head, seq_len, head_dim)\n    k = k.reshape(bsz, num_head, seq_len, 2, head_dim // 2).transpose(3, 4).reshape(bsz, num_head, seq_len, head_dim)\n\n    return q, k\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nclass SwiGluMLP(nn.Module):\n    def __init__(self, hidden_size,intermediate_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN['silu']\n\n    def forward(self, x):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n        return down_proj\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        max_position_embeddings=16,\n        base=10000,\n        device=None,\n        scaling_factor=1.0,\n    ):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass Conv(nn.Module):\n    def __init__(self, hidden_size, conv_kernel, rms_norm_eps):\n        super().__init__()\n        self.norm = RMSNorm(hidden_size, eps=rms_norm_eps)\n        self.conv = nn.Conv1d(\n            hidden_size,\n            hidden_size,\n            bias=True,\n            kernel_size=conv_kernel,\n            groups=hidden_size,\n            padding=conv_kernel - 1,\n        )\n\n    def __call__(self, hidden_states):\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states)\n        # [B, C, L]\n        hidden_states = hidden_states.transpose(1, 2)\n\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights, self.conv.bias, activation=None)\n\n        # [B, L, C]\n        hidden_states = hidden_states.transpose(1, 2)\n\n        return hidden_states\n\n\n#########################\n### TTT Layer Modules ###\n#########################\n\n\ndef scan(f, init, xs, out, checkpoint_group=0):\n    \"\"\"Minic jax.lax.scan function.\"\"\"\n    carry = init\n    if isinstance(xs, dict):\n        num_items = len(next(iter(xs.values())))\n    else:\n        num_items = len(xs[0])\n\n    def scan_fn(carry, i_start, i_end):\n        for i in range(i_start, i_end):\n            if isinstance(xs, dict):\n                x = {key: tensor[i] for key, tensor in xs.items()}\n            else:\n                x = [x[i] for x in xs]\n            carry, y = f(carry, x)\n            out[i] = y\n        return carry\n\n    if checkpoint_group > 0:\n        ckpt_every_n = num_items // checkpoint_group\n        for k in range(0, num_items, ckpt_every_n):\n            carry = torch.utils.checkpoint.checkpoint(\n                scan_fn, carry, k, min(k + ckpt_every_n, num_items), use_reentrant=False\n            )\n    else:\n        carry = scan_fn(carry, 0, num_items)\n\n    return carry, out\n\n\ndef ln_fwd(x, gamma, beta, eps=1e-6):\n    \"Batch forward for LayerNorm.\"\n\n    # Mean and variance computation\n    mu = x.mean(dim=-1, keepdim=True)\n    var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n    # Normalization\n    std = torch.sqrt(var + eps)\n    x_hat = (x - mu) / std\n\n    # Scale and shift\n    y = gamma * x_hat + beta\n\n    return y\n\n\ndef ln_fused_l2_bwd(x, l2_target, gamma, beta, eps=1e-6):\n    \"Batch backward for LayerNorm fused with L2 loss.\"\n    D = x.shape[-1]\n\n    # Mean and variance computation\n    mu = x.mean(dim=-1, keepdim=True)\n    var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n    # Normalization\n    std = torch.sqrt(var + eps)\n    x_hat = (x - mu) / std\n\n    # Scale and shift\n    y = gamma * x_hat + beta\n\n    grad_output = y - l2_target\n    grad_x_hat = grad_output * gamma\n    z = (\n        (1.0 / D)\n        * (\n            D * grad_x_hat\n            - grad_x_hat.sum(dim=-1, keepdim=True)\n            - x_hat * (grad_x_hat * x_hat).sum(dim=-1, keepdim=True)\n        )\n        / std\n    )\n\n    return z\n\nclass TTTLinear(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, scan_checkpoint_group_size, conv_kernel, mini_batch_size,rope_theta,ttt_base_lr):\n        super().__init__()\n\n        self.num_heads = num_attention_heads\n        self.width = hidden_size\n        self.hidden_size = hidden_size\n        self.head_dim = self.width // self.num_heads\n        self.mini_batch_size = mini_batch_size\n        self.rope_theta = rope_theta\n        self.ttt_base_lr = ttt_base_lr\n\n        # token_idx is a scale factor that scale the summation in Eqn. 4\n        token_idx = 1.0 / torch.arange(1, self.mini_batch_size + 1)\n        self.register_buffer(\"token_idx\", token_idx, persistent=False)\n        # make the scale factor learnable\n        self.learnable_token_idx = nn.Parameter(torch.zeros((self.mini_batch_size,)))\n\n        self.conv_kernel = conv_kernel\n        self._init_qkvo_proj()\n        self._init_rope()\n        # Learnable eta in Sec. 2.7\n        self._init_ttt_lr_gate()\n        self._init_ttt_ln()\n\n        self.post_norm = nn.LayerNorm(self.width, eps=1e-6)\n\n        self.scan_checkpoint_group_size = scan_checkpoint_group_size\n\n        # TTT model initialization for TTT-Linear\n        self.W1 = nn.Parameter(torch.normal(0, 0.02, size=(self.num_heads, self.head_dim, self.head_dim)))\n        self.b1 = nn.Parameter(torch.zeros(self.num_heads, 1, self.head_dim))\n\n\n    def _init_qkvo_proj(self):\n        self.q_proj = nn.Linear(self.width, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.width, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.width, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.width, self.num_heads * self.head_dim, bias=False)\n\n    def _init_rope(self):\n        self.rope_theta = self.rope_theta\n        self.rotary_emb = RotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.mini_batch_size,\n            base=self.rope_theta,\n        )\n\n    def _init_ttt_lr_gate(self):\n        # [width, 1]\n        linear_weight_data = nn.Linear(self.width, 1, bias=True).weight.data\n        # prepending head dim -> [num_heads, width, 1]\n        self.learnable_ttt_lr_weight = nn.Parameter(\n            torch.stack(\n                [torch.normal(0, 0.02, size=linear_weight_data.shape) for _ in range(self.num_heads)],\n                dim=0,\n            )\n        )\n        linear_bias_data = nn.Linear(self.width, 1, bias=True).bias.data\n        # init bias to 0 following original JAX impl.\n        # [num_heads, 1]\n        self.learnable_ttt_lr_bias = nn.Parameter(\n            torch.stack(\n                [torch.zeros_like(linear_bias_data) for _ in range(self.num_heads)],\n                dim=0,\n            )\n        )\n\n    def _init_ttt_ln(self):\n        ln_weight_data = nn.LayerNorm(self.head_dim).weight.data\n        # prepending head dim -> [num_heads, width]\n        self.ttt_norm_weight = nn.Parameter(torch.tile(ln_weight_data.unsqueeze(0), (self.num_heads, 1)))\n        ln_bias_data = nn.LayerNorm(self.head_dim).bias.data\n        self.ttt_norm_bias = nn.Parameter(torch.tile(ln_bias_data.unsqueeze(0), (self.num_heads, 1)))\n\n    def get_qkv_projections(self, hidden_states):\n        XQ, XK, XV = (\n            self.q_proj(hidden_states),\n            self.k_proj(hidden_states),\n            self.v_proj(hidden_states),\n        )\n        return XQ, XK, XV\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def get_eta(self, X, mini_batch_size):\n        # [B, num_heads, num_mini_batch, mini_batch_size, 1]\n        ttt_lr = torch.einsum(\"bnkc,hdc->bhnkd\", X, self.learnable_ttt_lr_weight) + self.learnable_ttt_lr_bias.reshape(\n            1, -1, 1, 1, 1\n        )\n        ttt_lr = F.sigmoid(ttt_lr)\n\n        # [B, num_heads, num_mini_batch, 1, mini_batch_size]\n        ttt_lr = ttt_lr.permute(0, 1, 2, 4, 3)\n        ttt_lr_eta = self.ttt_base_lr * ttt_lr / self.head_dim\n\n        # [B, L]\n        token_idx = self.token_idx + self.learnable_token_idx\n        token_idx = token_idx[0 : mini_batch_size]\n\n        # token idx should be greast than 0\n        token_idx = torch.clamp_min(token_idx, 0.0)\n\n        # NOTE: token_eta is a scale factor that applies to each token in the mini-batch\n        # [B, num_heads, num_mini_batch, mini_batch_size, 1]\n        token_eta = torch.broadcast_to(\n            token_idx.reshape(1, 1, 1, mini_batch_size, 1),\n            (X.shape[0], self.num_heads, X.shape[1], mini_batch_size, 1),\n        )\n\n        return token_eta, ttt_lr_eta\n\n    def get_ttt_inputs(self, inputs, mini_batch_size):\n        XQ = inputs[\"XQ\"]\n        XK = inputs[\"XK\"]\n        XV = inputs[\"XV\"]\n        X = inputs[\"X\"]\n        B, L, C = X.shape\n        num_mini_batch = L // mini_batch_size\n        # [B ,num_mini_batch, mini_batch_size, C]\n        X = X.reshape(B, num_mini_batch, mini_batch_size, self.width)\n\n        XQ = XQ.reshape(B, self.num_heads, L // mini_batch_size, mini_batch_size, self.head_dim)\n        XK = XK.reshape(B, self.num_heads, L // mini_batch_size, mini_batch_size, self.head_dim)\n        XV = XV.reshape(B, self.num_heads, L // mini_batch_size, mini_batch_size, self.head_dim)\n\n        token_eta, ttt_lr_eta = self.get_eta(X, mini_batch_size)\n        eta = token_eta * ttt_lr_eta\n        # decouple token_coeff and ilr_coeff for decoding\n        inputs = {\n            \"XQ\": XQ,\n            \"XK\": XK,\n            \"XV\": XV,\n            \"eta\": eta,\n            \"token_eta\": token_eta,\n            \"ttt_lr_eta\": ttt_lr_eta,\n        }\n        return inputs\n\n    def ttt(\n        self,\n        inputs,\n        mini_batch_size,\n        last_mini_batch_params_dict,\n    ):\n        if mini_batch_size is None:\n            mini_batch_size = self.mini_batch_size\n\n        # [B, num_heads, num_mini_batch, mini_batch_size, head_dim]\n        B = inputs[\"XV\"].shape[0]\n        num_mini_batch = inputs[\"XV\"].shape[2]\n        L = inputs[\"XV\"].shape[2] * inputs[\"XV\"].shape[3]\n        device = inputs[\"XV\"].device\n        dtype = inputs[\"XV\"].dtype\n\n        # NOTE:\n        # for prefilling, we will always use dual form for faster computation\n        # we need to use primal form if mini_batch_size is not a multiple of self.mini_batch_size\n        # since we need store the gradient for the next mini-batch computation\n        use_dual_form = True\n\n        def compute_mini_batch(params_dict, inputs):\n            # [B, nh, f, f], nh=num_heads, f=head_dim\n            W1_init = params_dict[\"W1_states\"]\n            # [B, nh, 1, f]\n            b1_init = params_dict[\"b1_states\"]\n\n            # [B,nh,K,f], K=mini_batch_size\n            XQ_mini_batch = inputs[\"XQ\"]\n            XV_mini_batch = inputs[\"XV\"]\n            XK_mini_batch = inputs[\"XK\"]\n            # [B, nh, K, 1]\n            eta_mini_batch = inputs[\"eta\"]\n            token_eta_mini_batch = inputs[\"token_eta\"]\n            ttt_lr_eta_mini_batch = inputs[\"ttt_lr_eta\"]\n\n            X1 = XK_mini_batch\n            # [B,nh,K,f] @ [B,nh,f,f] -> [B,nh,K,f]\n            Z1 = X1 @ W1_init + b1_init\n            reconstruction_target = XV_mini_batch - XK_mini_batch\n\n            ln_weight = self.ttt_norm_weight.reshape(self.num_heads, 1, self.head_dim)\n            ln_bias = self.ttt_norm_bias.reshape(self.num_heads, 1, self.head_dim)\n            # [B,nh,K,f]\n            grad_l_wrt_Z1 = ln_fused_l2_bwd(Z1, reconstruction_target, ln_weight, ln_bias)\n\n            if use_dual_form:\n                # [B,nh,K,K]\n                Attn1 = torch.tril(XQ_mini_batch @ X1.transpose(-2, -1))\n                # [B,nh,1,f] - [B,nh,K,K] @ [B,nh,K,f] -> [B,nh,K,f]\n                b1_bar = b1_init - torch.tril(eta_mini_batch) @ grad_l_wrt_Z1\n                # [B,nh,K,f] @ [B,nh,f,f] - ([B,nh,K,1] * [B,nh,K,K]) @ [B,nh,K,f] + [B,nh,K,f]\n                Z1_bar = XQ_mini_batch @ W1_init - (eta_mini_batch * Attn1) @ grad_l_wrt_Z1 + b1_bar\n\n                last_eta_mini_batch = eta_mini_batch[:, :, -1, :, None]\n                # [B,nh,f,f] - [B,nh,f,K] @ [B,nh,K,f]\n                W1_last = W1_init - (last_eta_mini_batch * X1).transpose(-1, -2) @ grad_l_wrt_Z1\n                # [B,nh,1,f]\n                b1_last = b1_init - torch.sum(last_eta_mini_batch * grad_l_wrt_Z1, dim=-2, keepdim=True)\n                grad_W1_last = torch.zeros_like(W1_last)\n                grad_b1_last = torch.zeros_like(b1_last)\n            else:\n                ttt_lr_eta_mini_batch = torch.broadcast_to(\n                    ttt_lr_eta_mini_batch,\n                    (\n                        *ttt_lr_eta_mini_batch.shape[:2],\n                        mini_batch_size,\n                        mini_batch_size,\n                    ),\n                )\n\n                # [B, nh, K, f, f]\n                grad_W1 = torch.einsum(\"bhki,bhkj->bhkij\", X1, grad_l_wrt_Z1)\n                grad_W1 = torch.einsum(\"bhnk,bhkij->bhnij\", torch.tril(ttt_lr_eta_mini_batch), grad_W1)\n                grad_W1 = grad_W1 + params_dict[\"W1_grad\"].unsqueeze(2)\n                # [B, nh, K, f]\n                grad_b1 = torch.einsum(\"bhnk,bhki->bhni\", torch.tril(ttt_lr_eta_mini_batch), grad_l_wrt_Z1)\n                grad_b1 = grad_b1 + params_dict[\"b1_grad\"]\n\n                W1_bar = W1_init.unsqueeze(2) - grad_W1 * token_eta_mini_batch.unsqueeze(-1)\n                b1_bar = b1_init - grad_b1 * token_eta_mini_batch\n\n                # [B, nh, K, 1, f] @ [B, nh, K, f, f]\n                Z1_bar = (XQ_mini_batch.unsqueeze(3) @ W1_bar).squeeze(3) + b1_bar\n\n                W1_last = W1_bar[:, :, -1]\n                b1_last = b1_bar[:, :, -1:]\n                grad_W1_last = grad_W1[:, :, -1]\n                grad_b1_last = grad_b1[:, :, -1:]\n\n            Z1_bar = ln_fwd(Z1_bar, ln_weight, ln_bias)\n\n            XQW_mini_batch = XQ_mini_batch + Z1_bar\n\n            last_param_dict = {\n                \"W1_states\": W1_last,\n                \"b1_states\": b1_last,\n                \"W1_grad\": grad_W1_last,\n                \"b1_grad\": grad_b1_last,\n            }\n            return last_param_dict, XQW_mini_batch\n\n        if last_mini_batch_params_dict is not None:\n            init_params_dict = last_mini_batch_params_dict\n        else:\n            init_params_dict = {\n                \"W1_states\": torch.tile(self.W1.unsqueeze(0), dims=(B, 1, 1, 1)),\n                \"b1_states\": torch.tile(self.b1.unsqueeze(0), dims=(B, 1, 1, 1)),\n            }\n            init_params_dict.update(W1_grad=torch.zeros_like(init_params_dict[\"W1_states\"]))\n            init_params_dict.update(b1_grad=torch.zeros_like(init_params_dict[\"b1_states\"]))\n\n        # [B,num_heads, num_mini_batch, mini_batch_size, f] -> [num_mini_batch, B, num_heads, mini_batch_size, f]\n        inputs = tree_map(lambda x: x.permute(2, 0, 1, 3, 4), inputs)\n\n        # allocate output tensor\n        XQW_batch = torch.empty(\n            (num_mini_batch, B, self.num_heads, mini_batch_size, self.head_dim),\n            device=device,\n            dtype=dtype,\n        )\n        # XQW_batch: [num_mini_batch, B, num_heads, mini_batch_size, head_dim]\n        batch_params_dict, XQW_batch = scan(\n            compute_mini_batch,\n            init_params_dict,\n            inputs,\n            XQW_batch,\n            self.scan_checkpoint_group_size if self.training else 0,\n        )\n\n        # [num_mini_batch, B, num_heads, mini_batch_size, head_dim] -> [B, num_mini_batch, mini_batch_size, num_heads, head_dim]\n        XQW_batch = XQW_batch.permute(1, 0, 3, 2, 4)\n        # [B, L, C]\n        XQW_batch = XQW_batch.reshape(B, L, self.width)\n        return XQW_batch, batch_params_dict\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_ids: Optional[torch.LongTensor] = None,\n    ):\n        B, L = hidden_states.shape[:2]\n        reminder_len = L % self.mini_batch_size\n        num_mini_batch = L // self.mini_batch_size\n        last_mini_batch_params_dict = None\n\n        XQ, XK, XV = self.get_qkv_projections(hidden_states)\n\n        # [B, L, C] -> [B, L, num_heads, head_dim] -> [B, num_heads, L, head_dim]\n        XQ = XQ.reshape(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        XK = XK.reshape(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        XV = XV.reshape(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(XV, position_ids % self.mini_batch_size)\n\n        # permute_qk and undo_permute_qk is just for aligning pytorch with jax pre-training\n        XQ, XK = permute_qk(XQ, XK)\n        XQ, XK = apply_rotary_pos_emb(XQ, XK, cos, sin)\n        XQ, XK = undo_permute_qk(XQ, XK)\n\n        output_hidden_states = []\n        # when input sequence length is not a multiple of mini_batch_size\n        # we need to compute them seperately, when computing the reminder,\n        # we will need the last_mini_batch_params_dict to continue TTT learning\n        if num_mini_batch > 0:\n            inputs = {\n                \"XQ\": XQ[:, :, : num_mini_batch * self.mini_batch_size],\n                \"XK\": XK[:, :, : num_mini_batch * self.mini_batch_size],\n                \"XV\": XV[:, :, : num_mini_batch * self.mini_batch_size],\n                \"X\": hidden_states[:, : num_mini_batch * self.mini_batch_size],\n            }\n            output_mod, last_mini_batch_params_dict = self.ttt(\n                self.get_ttt_inputs(inputs, self.mini_batch_size),\n                mini_batch_size=self.mini_batch_size,\n                last_mini_batch_params_dict=last_mini_batch_params_dict,\n            )\n            output_hidden_states.append(output_mod)\n        if reminder_len > 0:\n            inputs = {\n                \"XQ\": XQ[:, :, -reminder_len:],\n                \"XK\": XK[:, :, -reminder_len:],\n                \"XV\": XV[:, :, -reminder_len:],\n                \"X\": hidden_states[:, -reminder_len:],\n            }\n            output_reminder, _ = self.ttt(\n                self.get_ttt_inputs(inputs, reminder_len),\n                mini_batch_size=reminder_len,\n                last_mini_batch_params_dict=last_mini_batch_params_dict,\n            )\n            output_hidden_states.append(output_reminder)\n\n        output_hidden_states = torch.cat(output_hidden_states, dim=1)\n        output_hidden_states = self.post_norm(output_hidden_states)\n        output_hidden_states = self.o_proj(output_hidden_states)\n\n        return output_hidden_states\n\n\n\nclass GAB(GABBase):\n    \"\"\"Generalized Autoregressive Block\n        Input:        X: (batch, seqlen, embed_dim)\n        Output:       Y: (batch, seqlen, embed_dim)\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n    \"\"\"\n    def __init__(self,embed_dim: int, device=None,dtype=None,\n                 scan_checkpoint_group_size=4,conv_kernel=4,\n                mini_batch_size=16,rope_theta=10000.0,rms_norm_eps=1e-6,ttt_base_lr=1.0,\n                 **kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n        # argv: list of hyperparameters\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\n        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n        \n        # COMPLETING THE CODE HERE #\n        self.hidden_size = embed_dim\n\n        num_attention_heads = max(4,embed_dim//64)\n        self.seq_modeling_block = TTTLinear(\n            hidden_size=embed_dim,\n            num_attention_heads=num_attention_heads,\n            scan_checkpoint_group_size=scan_checkpoint_group_size,\n            conv_kernel=conv_kernel,\n            mini_batch_size=mini_batch_size,\n            rope_theta=rope_theta,\n            ttt_base_lr=ttt_base_lr\n        )\n\n        self.mlp = SwiGluMLP(embed_dim, int(embed_dim * 2.5))\n        self.conv = Conv(embed_dim, conv_kernel, rms_norm_eps)\n\n        self.seq_norm = RMSNorm(embed_dim, eps=rms_norm_eps)\n        self.ffn_norm = RMSNorm(embed_dim, eps=rms_norm_eps)\n\n        self.seq_modeling_block = self.seq_modeling_block.to(device=device, dtype=dtype)\n        self.mlp = self.mlp.to(device=device, dtype=dtype)\n        self.conv = self.conv.to(device=device, dtype=dtype)\n        self.seq_norm = self.seq_norm.to(device=device, dtype=dtype)\n        self.ffn_norm = self.ffn_norm.to(device=device, dtype=dtype)\n\n\n    # YOU CAN ADD MORE FUNCTIONS HERE #\n\n\n    def _forward(self,X,**kwargs): # type hints are optional but recommended\n\n        # THE CODE HERE MUST BE COMPLETED #\n        hidden_states = X\n        position_ids = torch.arange(\n            0,\n            X.shape[1],\n            dtype=torch.long,\n            device=X.device,\n        ).unsqueeze(0)\n\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states)\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n\n        hidden_states = self.seq_norm(hidden_states)\n\n        # TTT Layer\n        hidden_states = self.seq_modeling_block(\n            hidden_states=hidden_states,\n            position_ids=position_ids,\n        )\n        hidden_states = residual + hidden_states\n\n        # Feed-Forward-Network\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n    \n    \n\"\"\" The dictionary of hyperparameters for constructing a GAB layer\n    embed_dim, device, dtype should NOT be included in gab_config\n\"\"\"\ngab_config = { # No need to change, applied by all sizes of TTT\n    # THE HYPERPARAMETERS OF ADDITIONAL ARGUMENTS IN GAB CLASS #\n    'scan_checkpoint_group_size': 0, #4, buggy now\n    'conv_kernel': 4,\n    'mini_batch_size': 16,\n    'rope_theta': 10000.0,\n    'rms_norm_eps': 1e-6,\n    'ttt_base_lr': 1.0\n}\n",
    "description": null,
    "url": null
}