{
    "acronym": "lbcontextcompress",
    "title": "Empower Your Model with Longer and Better Context Comprehension",
    "seed_ids": [
        "bert"
    ],
    "s2id": "58d1a002a0ff0aa40b6633f0a7073d48f1cdff53",
    "abstract": "Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on\"why models are unable to compensate or strengthen their capabilities on their own\". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted on the challenging XSum dataset using LLaMa-7b model with context token length ranging from 800 to 1900. Results demonstrate that we achieve substantial improvements compared with the original generation results evaluated by GPT4.",
    "authors": [
        "Yifei Gao",
        "Lei Wang",
        "Jun Fang",
        "Long-Hua Hu",
        "Jun Cheng"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "The nature of information transfer within LLMs is investigated and a novel technique called Attention Transition is proposed that empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency.",
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\n\nfrom .configuration_llama import LlamaConfig\nfrom .modules import LlamaRotaryEmbedding, apply_rotary_pos_emb\n\n\nlogger = logging.get_logger(__name__)\n\n\n\n\"\"\"\nAssume that we have batch inputs that have the dimension [bsz, seq_len], what we want to do is:\n1. We do not change the attention weights of the first and the last two layers, because:\n    The first layer's attention is irregular\n    The last two layers' attention variance is much bigger than the previous ones\n2. Decide a chunk of tokens for each layer that apply the attention transition\n    Sequentially eliminate and add attention weights on different regions\n    Within one layer, we just focus on a specific region and ignore other parts\n    With layer count increasing, the attention should be dispensed to positions more closer to the end\n3. Decide a number for the attention lowest boundary to eliminate\n    Do not change the original distribution so much\n    Do not affect the attention weights that are big enough\n    We want to eliminate 10%-20% of the whole attention, which will not change the original distribution so much\n\"\"\"\ndef attention_handler(seq_len, one_layer_inter_use=2):\n    \"\"\"\n    Args:\n    seq_len: the input length\n    one_layer_inter_use: dicide how many intervals will be strengthened within one layer\n    return:\n        layer_indices: the sequence of layers to implement attention transition\n        start_end_tuple_lists: a list of start_end_tuple lists\n    \"\"\"\n    if seq_len < 400:\n        return \n\n\n    # We divide our seq_len into 29+2+1+one_layer_inter_use-1 intervals and only strengthen the middle intervals, except for 0,1,-1\n    total_interval = 31 + one_layer_inter_use\n    interval = int(seq_len / total_interval) - int(seq_len / total_interval) % 5\n    layer_indices = []\n    start_end_tuple_lists = []\n    # We use total 29 layers, except for 0,-2,-1\n\n    for i in range(29):\n\n        layer_indices.append(i+1)\n        start_end_tuple_list = []\n        for j in range(one_layer_inter_use):\n            start = (i+j+2) * interval\n            end = (i+j+3) * interval\n            tup = (start, end)\n            start_end_tuple_list.append(tup)\n        start_end_tuple_lists.append(start_end_tuple_list)\n\n    return layer_indices, start_end_tuple_lists, interval\n\n\"\"\"\nAttention Dispension is aimed to eliminate attention weights lower than some number and add them on a specific region's attention\nThings to emphasis:\n1. Attention eliminated shold not affect the original layer output so much\n2. Attntion dispensation is functioned only on a specific region, while elimination functions on all attention exists\n3. When dispense attention weights, we just dispense attention weights on previous region\n\"\"\"\n\ndef attention_dispense(attn_weights, start_end_tuple, alpha=0.5, beta=0.1, strategy='adding'):\n    \"\"\"\n    args:\n    attn_weights: attention weights to be implemented, which has the size (bsz, num_heads, seq_len, seq_len)\n        Notice that for attention weights, attentions of which positions further than the current one are 0\n    start_end_tuple: A tuple consists start position and end postion to implement\n    \"\"\"\n    start, end = start_end_tuple\n    start = int(start)\n    end = int(end)\n    interval = end - start\n    # Use attn_mask to eliminate attentions\n    lower_boundary = alpha / end\n\n    with torch.no_grad():\n        # Attention Elimination functions on all attentions\n        attn_rebuild_mask = torch.where(attn_weights[:,:,start:end,:end]>=lower_boundary, 1, 0)\n        attn_weights[:,:,start:end,:end] = attn_weights[:,:,start:end,:end] * attn_rebuild_mask\n        attn_weights[:,:,start:end,0] = attn_weights[:,:,start:end,0] / 2\n        \n        # bsz * head_dim * seq_len\n        weights_sum = torch.sum(attn_weights[:,:,start:end,:end], dim=-1, dtype=torch.float32)\n        attn_weights_eliminaed = torch.ones_like(weights_sum) - weights_sum\n\n        #torch.save(attn_weights_eliminaed, '/root/autodl-fs/eliminated_weights')\n            \n        # Distribute the attention weights by adding or by multiplying a coefficient.\n        \n        # Distribute by adding\n        if strategy == 'adding':\n        # Attention dispension is sent to the previous attention interval, not all attentions\n            if start != interval:\n                # 0.3:0.7\n                # Because some inervals have no weight bigger than lower_boundary, we use 0.001 to avoid 0\n                prev_mask = torch.where(attn_weights[:,:, start:end, start-2*interval:end-interval]>=lower_boundary, 1, 0.01)\n                #print('1',prev_mask.size())\n                nonzero_num = torch.sum(prev_mask, dim=-1, dtype=torch.float32)\n                average_weights = torch.div(attn_weights_eliminaed,nonzero_num) * beta\n                add_matrix = average_weights[...,None]\n                # 0.3:0.7\n                attn_weights[:,:,start:end,start-interval:end-interval] = attn_weights[:,:,start:end,start-interval:end-interval] + 7 * add_matrix * prev_mask[:,:,:,-interval:]\n                attn_weights[:,:,start:end,start-2*interval:end-2*interval] = attn_weights[:,:,start:end,start-2*interval:end-2*interval] + 3 * add_matrix * prev_mask[:,:,:,:interval]\n            else:\n                prev_mask = torch.where(attn_weights[:,:, start:end, :end-interval]>=lower_boundary, 1, 0.001)\n                #print('2',prev_mask.size())\n                nonzero_num = torch.sum(prev_mask, dim=-1, dtype=torch.float32)\n                average_weights = torch.div(attn_weights_eliminaed,nonzero_num) * beta\n                #average_weights = torch.div(attn_weights_eliminaed,nonzero_num)\n                add_matrix = average_weights[...,None]\n                attn_weights[:,:,start:end,start-interval:end-interval] = attn_weights[:,:,start:end,start-interval:end-interval] + 10 * add_matrix * prev_mask[:,:,:,-interval:]\n        if strategy == 'multi':\n            # Distribute by multiplying\n            prev_attn_sum = torch.sum(attn_weights[:,:, start:end, :end], dim=-1)\n            multi_coeff = attn_weights_eliminaed / prev_attn_sum\n            attn_weights[:,:,start:end, :end] = attn_weights[:,:,start:end,:end] * multi_coeff[...,None]\n            \n    return attn_weights\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        start_end_tuple_list: Optional[Tuple] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        attn_transition: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        #hidden_states.to(self.q_proj.weight.device)\n        #print(self.q_proj.weight.device, hidden_states.device)\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids.to(query_states.device))\n        # [bsz, nh, t, hd]\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask.to(attn_weights.device)\n            attn_weights = torch.max(\n                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n            )\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32)\n        \n        #########################################################################################\n        # Use attention transition in attention layer\n        #########################################################################################\n        \n        if attn_transition and start_end_tuple_list is not None:\n            for start_end_tuple in start_end_tuple_list:\n                attn_weights = attention_dispense(attn_weights=attn_weights, start_end_tuple=start_end_tuple)\n        \n        attn_output = torch.matmul(attn_weights.to(query_states.dtype), value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n        \n        return attn_output, attn_weights, past_key_value\n\n",
    "description": null,
    "url": null
}