{
    "acronym": "multistreamtrans",
    "title": "Multi-Stream Transformers",
    "seed_ids": [
        "transformer",
        "bert"
    ],
    "s2id": "177e21dbf45d01a0f585bb46e548d795b441aad9",
    "abstract": "Transformer-based encoder-decoder models produce a fused token-wise representation after every encoder layer. We investigate the effects of allowing the encoder to preserve and explore alternative hypotheses, combined at the end of the encoding process. To that end, we design and examine a $\\textit{Multi-stream Transformer}$ architecture and find that splitting the Transformer encoder into multiple encoder streams and allowing the model to merge multiple representational hypotheses improves performance, with further improvement obtained by adding a skip connection between the first and the final encoder layer.",
    "authors": [
        "M. Burtsev",
        "Anna Rumshisky"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": "It is found that splitting the Transformer encoder into multiple encoder streams and allowing the model to merge multiple representational hypotheses improves performance, with further improvement obtained by adding a skip connection between the first and the final encoder layer.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "import torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat, reduce\n\nfrom einops.layers.torch import Rearrange\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\n\ndef rearrange_all(tensors, *args, **kwargs):\n    return map(lambda t: rearrange(t, *args, **kwargs), tensors)\n\n# feedforward\n\nclass GroupLayerNorm(nn.Module):\n    def __init__(self, dim, groups = 1, eps = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.groups = groups\n        self.g = nn.Parameter(torch.ones(1, groups, dim, 1))\n        self.b = nn.Parameter(torch.zeros(1, groups, dim, 1))\n\n    def forward(self, x):\n        x = rearrange(x, 'b (g d) n -> b g d n', g = self.groups)\n        std = torch.var(x, dim = 2, unbiased = False, keepdim = True).sqrt()\n        mean = torch.mean(x, dim = 2, keepdim = True)\n        out = (x - mean) / (std + self.eps) * self.g + self.b\n        return rearrange(out, 'b g d n -> b (g d) n')\n\nclass PreNorm(nn.Module):\n    def __init__(\n        self,\n        dim,\n        fn,\n        groups = 1\n    ):\n        super().__init__()\n        self.norm = GroupLayerNorm(dim, groups = groups)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        mult = 4,\n        groups = 1\n    ):\n        super().__init__()\n        input_dim = dim * groups\n        hidden_dim = dim * mult * groups\n\n        self.net = nn.Sequential(\n            nn.Conv1d(input_dim, hidden_dim, 1, groups = groups),\n            nn.GELU(),\n            nn.Conv1d(hidden_dim, input_dim, 1, groups = groups)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        causal = False,\n        groups = 1\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.groups = groups\n        self.heads = heads\n        self.causal = causal\n        input_dim = dim * groups\n        inner_dim = dim_head * heads * groups\n\n        self.to_q = nn.Conv1d(input_dim, inner_dim, 1, bias = False)\n        self.to_kv = nn.Conv1d(input_dim, inner_dim * 2, 1, bias = False)\n        self.to_out = nn.Conv1d(inner_dim, input_dim, 1)\n\n    def forward(self, x, mask = None, context = None):\n        n, device, h, g, causal = x.shape[2], x.device, self.heads, self.groups, self.causal\n        context = default(context, x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = 1))\n        q, k, v = rearrange_all((q, k, v), 'b (g h d) n -> (b g h) n d', g = g, h = h)\n\n        q = q * self.scale\n\n        sim = einsum('b i d, b j d -> b i j', q, k)\n\n        if exists(mask):\n            mask = repeat(mask, 'b n -> (b g h) n', h = h, g = g)\n            mask = rearrange(mask, 'b n -> b n ()') * rearrange(mask, 'b n -> b () n')\n            mask_value = max_neg_value(sim)\n            sim = sim.masked_fill(~mask, mask_value)\n\n        if causal:\n            causal_mask = torch.ones((n, n), device = device).triu(1).bool()\n            mask_value = max_neg_value(sim)\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n        attn = sim.softmax(dim = -1)\n        out = einsum('b i j, b j d -> b i d', attn, v)\n        out = rearrange(out, '(b g h) n d -> b (g h d) n', h = h, g = g)\n        return self.to_out(out)\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        causal = False,\n        dim_head = 64,\n        heads = 8,\n        ff_mult = 4,\n        groups = 1\n    ):\n        super().__init__()\n        self.attn = PreNorm(dim, Attention(dim = dim, dim_head = dim_head, heads = heads, causal = causal, groups = groups), groups = groups)\n        self.ff = PreNorm(dim, FeedForward(dim = dim, mult = ff_mult, groups = groups), groups = groups)\n\n    def forward(self, x, mask = None):\n        x = self.attn(x, mask = mask) + x\n        x = self.ff(x) + x\n        return x\n\n# main class\n\nclass MultistreamTransformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        num_tokens,\n        max_seq_len,\n        causal = False,\n        dim_head = 64,\n        heads = 8,\n        ff_mult = 4,\n        num_streams = 1\n    ):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        self.num_streams = num_streams\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.pos_emb = nn.Embedding(max_seq_len, dim)\n\n        self.layers = nn.ModuleList([])\n        self.pre_transformer_block = TransformerBlock(dim = dim, causal = causal, dim_head = dim_head, heads = heads)\n\n        for _ in range(depth):\n            self.layers.append(TransformerBlock(dim = dim, causal = causal, dim_head = dim_head, heads = heads, groups = num_streams))\n\n        if num_streams > 1:\n            self.query = nn.Parameter(torch.randn(dim))\n            self.attn_pool = Attention(dim = dim, dim_head = dim_head, heads = heads)\n\n        self.post_transformer_block = TransformerBlock(dim = dim, causal = causal, dim_head = dim_head, heads = heads,)\n\n        self.to_logits = nn.Sequential(\n            Rearrange('b d n -> b n d'),\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_tokens)\n        )\n\n    def forward(self, x, mask = None):\n        b, n, d, device, is_multistream = *x.shape, self.dim, x.device, (self.num_streams > 1)\n        x = self.token_emb(x)\n\n        pos_emb = self.pos_emb(torch.arange(n, device = device))\n        pos_emb = rearrange(pos_emb, 'n d -> () n d')\n\n        x = x + pos_emb\n        x = rearrange(x, 'b n d -> b d n')\n\n        x = self.pre_transformer_block(x, mask = mask)\n        layers = [x]\n\n        if is_multistream:\n            x = repeat(x, 'b d n -> b (s d) n', s = self.num_streams)\n\n        for block in self.layers:\n            x = block(x, mask = mask)\n            layers.append(x)\n\n        if is_multistream:\n            layers = list(map(lambda t: rearrange(t, 'b (s d) n -> (b n) d s', d = d), layers))\n            layer_tokens = torch.cat(layers, dim = -1)\n\n            query = repeat(self.query, 'd -> b d ()', b = layer_tokens.shape[0])\n            x = self.attn_pool(query, context = layer_tokens)\n            x = rearrange(x, '(b n) d () -> b d n', n = n)\n\n        x = self.post_transformer_block(x, mask = mask)\n        return self.to_logits(x)",
    "description": null,
    "url": null
}