{
    "acronym": "butterfly",
    "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations",
    "seed_ids": [],
    "s2id": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582",
    "abstract": "Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.",
    "authors": [
        "Tri Dao",
        "Albert Gu",
        "Matthew Eichhorn",
        "A. Rudra",
        "C. R\u00e9"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2019,
    "tldr": "This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.",
    "citationCount": 84,
    "influentialCitationCount": 13,
    "code": "import math\nimport numbers\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport torch_butterfly\nfrom torch_butterfly.complex_utils import real_dtype_to_complex, complex_reshape\nfrom torch_butterfly.multiply_base4 import twiddle_base2_to_base4\n\n\n\ndef butterfly_multiply(twiddle, input, increasing_stride=True, output_size=None):\n    batch_size, nstacks, input_size = input.shape\n    nblocks = twiddle.shape[1]\n    log_n = twiddle.shape[2]\n    n = 1 << log_n\n    assert twiddle.shape == (nstacks, nblocks, log_n, n // 2, 2, 2)\n    # Pad or trim input to size n\n    input = F.pad(input, (0, n - input_size)) if input_size < n else input[:, :, :n]\n    output_size = n if output_size is None else output_size\n    assert output_size <= n\n    output = input.contiguous()\n    cur_increasing_stride = increasing_stride\n    for block in range(nblocks):\n        for idx in range(log_n):\n            log_stride = idx if cur_increasing_stride else log_n - 1 - idx\n            stride = 1 << log_stride\n            # shape (nstacks, n // (2 * stride), 2, 2, stride)\n            t = twiddle[:, block, idx].view(\n                nstacks, n // (2 * stride), stride, 2, 2).permute(0, 1, 3, 4, 2)\n            output_reshape = output.view(\n                batch_size, nstacks, n // (2 * stride), 1, 2, stride)\n            output = (t * output_reshape).sum(dim=4)\n        cur_increasing_stride = not cur_increasing_stride\n    return output.view(batch_size, nstacks, n)[:, :, :output_size]\n\n\nclass Butterfly(nn.Module):\n    \"\"\"Product of log N butterfly factors, each is a block 2x2 of diagonal matrices.\n    Compatible with torch.nn.Linear.\n\n    Parameters:\n        in_size: size of input\n        out_size: size of output\n        bias: If set to False, the layer will not learn an additive bias.\n                Default: ``True``\n        complex: whether complex or real\n        increasing_stride: whether the first butterfly block will multiply with increasing stride\n            (e.g. 1, 2, ..., n/2) or decreasing stride (e.g., n/2, n/4, ..., 1).\n        init: a torch.Tensor, or 'randn', 'ortho', 'identity', 'fft_no_br', or 'ifft_no_br'.\n            Whether the weight matrix should be initialized to from randn twiddle, or to be\n            randomly orthogonal/unitary, or to be the identity matrix, or the normalized FFT/iFFT\n            twiddle (without the bit-reversal permutation).\n        nblocks: number of B or B^T blocks. The B and B^T will alternate.\n    \"\"\"\n\n    def __init__(self, in_size, out_size, bias=True, complex=False,\n                 increasing_stride=True, init='randn', nblocks=1):\n        super().__init__()\n        self.in_size = in_size\n        self.log_n = log_n = int(math.ceil(math.log2(in_size)))\n        self.n = n = 1 << log_n\n        self.out_size = out_size\n        self.nstacks = int(math.ceil(out_size / self.n))\n        self.complex = complex\n        self.increasing_stride = increasing_stride\n        assert nblocks >= 1\n        self.nblocks = nblocks\n        dtype = torch.get_default_dtype() if not self.complex else real_dtype_to_complex[torch.get_default_dtype()]\n        twiddle_shape = (self.nstacks, nblocks, log_n, n // 2, 2, 2)\n        if isinstance(init, torch.Tensor):\n            self.init = None\n            assert init.shape == twiddle_shape\n            assert init.dtype == dtype\n            self.twiddle = nn.Parameter(init.clone())\n        else:\n            assert init in ['empty', 'randn', 'ortho', 'identity', 'fft_no_br', 'ifft_no_br']\n            self.init = init\n            self.twiddle = nn.Parameter(torch.empty(twiddle_shape, dtype=dtype))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_size, dtype=dtype))\n        else:\n            self.register_parameter('bias', None)\n        self.twiddle._is_structured = True  # Flag to avoid weight decay\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        \"\"\"Initialize bias the same way as torch.nn.Linear.\"\"\"\n        if self.bias is not None:\n            bound = 1 / math.sqrt(self.in_size)\n            nn.init.uniform_(self.bias, -bound, bound)\n        twiddle = self.twiddle\n        if self.init is None or self.init == 'empty':\n            return\n        elif self.init == 'randn':\n            # complex randn already has the correct scaling of stddev=1.0\n            scaling = 1.0 / math.sqrt(2)\n            with torch.no_grad():\n                twiddle.copy_(torch.randn(twiddle.shape, dtype=twiddle.dtype) * scaling)\n        elif self.init == 'ortho':\n            twiddle_core_shape = twiddle.shape[:-2]\n            if not self.complex:\n                theta = torch.rand(twiddle_core_shape) * math.pi * 2\n                c, s = torch.cos(theta), torch.sin(theta)\n                det = torch.randint(0, 2, twiddle_core_shape, dtype=c.dtype) * 2 - 1  # Rotation (+1) or reflection (-1)\n                with torch.no_grad():\n                    twiddle.copy_(torch.stack((torch.stack((det * c, -det * s), dim=-1),\n                                               torch.stack((s, c), dim=-1)), dim=-2))\n            else:\n                # Sampling from the Haar measure on U(2) is a bit subtle.\n                # Using the parameterization here: http://home.lu.lv/~sd20008/papers/essays/Random%20unitary%20[paper].pdf\n                phi = torch.asin(torch.sqrt(torch.rand(twiddle_core_shape)))\n                c, s = torch.cos(phi), torch.sin(phi)\n                alpha, psi, chi = torch.rand((3, ) + twiddle_core_shape) * math.pi * 2\n                A = torch.exp(1j * (alpha + psi)) * c\n                B = torch.exp(1j * (alpha + chi)) * s\n                C = -torch.exp(1j * (alpha - chi)) * s\n                D = torch.exp(1j * (alpha - psi)) * c\n                with torch.no_grad():\n                    twiddle.copy_(torch.stack((torch.stack((A, B), dim=-1),\n                                               torch.stack((C, D), dim=-1)), dim=-2))\n        elif self.init == 'identity':\n            twiddle_eye = torch.eye(2, dtype=twiddle.dtype).reshape(1, 1, 1, 1, 2, 2)\n            twiddle_eye = twiddle_eye.expand(*twiddle.shape).contiguous()\n            with torch.no_grad():\n                twiddle.copy_(twiddle_eye)\n        elif self.init in ['fft_no_br', 'ifft_no_br']:\n            assert self.complex, 'fft_no_br/ifft_no_br init requires Butterfly to be complex'\n            special_fn = (torch_butterfly.special.fft if self.init == 'fft_no_br'\n                          else torch_butterfly.special.ifft)\n            b_fft = special_fn(self.n, normalized=True, br_first=self.increasing_stride,\n                               with_br_perm=False)\n            with torch.no_grad():\n                twiddle[:, 0] = b_fft.twiddle\n            if self.nblocks > 1:\n                twiddle_eye = torch.eye(2, dtype=twiddle.dtype).reshape(1, 1, 1, 1, 2, 2)\n                twiddle_eye = twiddle_eye.expand(*twiddle[:, 1:].shape).contiguous()\n                with torch.no_grad():\n                    twiddle[:, 1:] = twiddle_eye\n\n    def forward(self, input, transpose=False, conjugate=False, subtwiddle=False):\n        \"\"\"\n        Parameters:\n            input: (batch, *, in_size)\n            transpose: whether the butterfly matrix should be transposed.\n            conjugate: whether the butterfly matrix should be conjugated.\n            subtwiddle: allow using only part of the parameters for smaller input.\n                Could be useful for weight sharing.\n                out_size is set to self.nstacks * self.n in this case\n        Return:\n            output: (batch, *, out_size)\n        \"\"\"\n        twiddle = self.twiddle\n        output = self.pre_process(input)\n        output_size = self.out_size if self.nstacks == 1 else None\n        if subtwiddle:\n            log_n = int(math.ceil(math.log2(input.size(-1))))\n            n = 1 << log_n\n            twiddle = (twiddle[:, :, :log_n, :n // 2] if self.increasing_stride\n                       else twiddle[:, :, -log_n:, :n // 2])\n            output_size = None\n        if conjugate and self.complex:\n            twiddle = twiddle.conj()\n        if not transpose:\n            output = butterfly_multiply(twiddle, output, self.increasing_stride, output_size)\n        else:\n            twiddle = twiddle.transpose(-1, -2).flip([1, 2])\n            last_increasing_stride = self.increasing_stride != ((self.nblocks - 1) % 2 == 1)\n            output = butterfly_multiply(twiddle, output, not last_increasing_stride, output_size)\n        if not subtwiddle:\n            return self.post_process(input, output)\n        else:\n            return self.post_process(input, output, out_size=output.size(-1))\n\n    def pre_process(self, input):\n        # Reshape to (N, in_size)\n        input_size = input.size(-1)\n        output = complex_reshape(input, -1, input_size)\n        batch = output.shape[0]\n        output = output.unsqueeze(1).expand(batch, self.nstacks, input_size)\n        return output\n\n    def post_process(self, input, output, out_size=None):\n        if out_size is None:\n            out_size = self.out_size\n        batch = output.shape[0]\n        output = output.view(batch, self.nstacks * output.size(-1))\n        if out_size != output.shape[-1]:  # Take top rows\n            output = output[:, :out_size]\n        if self.bias is not None:\n            output = output + self.bias[:out_size]\n        return output.view(*input.size()[:-1], out_size)\n\n    def __imul__(self, scale):\n        \"\"\"In-place multiply the whole butterfly matrix by some scale factor, by multiplying the\n        twiddle.\n        Scale must be nonnegative\n        \"\"\"\n        assert isinstance(scale, numbers.Number)\n        assert scale >= 0\n        self.twiddle *= scale ** (1.0 / self.twiddle.shape[1] / self.twiddle.shape[2])\n        return self\n\n    def diagonal_multiply_(self, diagonal, diag_first):\n        \"\"\" Combine a Butterfly and a diagonal into another Butterfly.\n        Only support nstacks==1 for now.\n        Parameters:\n            diagonal: size (in_size,) if diag_first, else (out_size,). Should be of type complex\n                if butterfly.complex == True.\n            diag_first: If True, the map is input -> diagonal -> butterfly.\n                If False, the map is input -> butterfly -> diagonal.\n        \"\"\"\n        return torch_butterfly.combine.diagonal_butterfly(self, diagonal, diag_first, inplace=True)\n\n    def to_base4(self):\n        with torch.no_grad():\n            twiddle4, twiddle2 = twiddle_base2_to_base4(self.twiddle, self.increasing_stride)\n        new = torch_butterfly.ButterflyBase4(self.in_size, self.out_size, self.bias is not None,\n                                             self.complex, self.increasing_stride,\n                                             init=(twiddle4, twiddle2),\n                                             nblocks=self.nblocks).to(self.twiddle.device)\n        if new.bias is not None:\n            with torch.no_grad():\n                new.bias.copy_(self.bias)\n        return new\n\n    def extra_repr(self):\n        s = 'in_size={}, out_size={}, bias={}, complex={}, increasing_stride={}, init={}, nblocks={}'.format(\n            self.in_size, self.out_size, self.bias is not None, self.complex, self.increasing_stride, self.init, self.nblocks,)\n        return s\n\n\nclass ButterflyUnitary(Butterfly):\n    \"\"\"Same as Butterfly, but constrained to be unitary\n    Compatible with torch.nn.Linear.\n\n    Parameters:\n        in_size: size of input\n        out_size: size of output\n        bias: If set to False, the layer will not learn an additive bias.\n                Default: ``True``\n        increasing_stride: whether the first butterfly block will multiply with increasing stride\n            (e.g. 1, 2, ..., n/2) or decreasing stride (e.g., n/2, n/4, ..., 1).\n        nblocks: number of B or B^T blocks. The B and B^T will alternate.\n    \"\"\"\n\n    def __init__(self, in_size, out_size, bias=True, increasing_stride=True, nblocks=1):\n        nn.Module.__init__(self)\n        self.in_size = in_size\n        self.log_n = log_n = int(math.ceil(math.log2(in_size)))\n        self.n = n = 1 << log_n  # Will zero-pad input if in_size is not a power of 2\n        self.out_size = out_size\n        self.nstacks = int(math.ceil(out_size / self.n))\n        self.complex = True\n        self.increasing_stride = increasing_stride\n        assert nblocks >= 1\n        self.nblocks = nblocks\n        complex_dtype = real_dtype_to_complex[torch.get_default_dtype()]\n        twiddle_shape = (self.nstacks, nblocks, log_n, n // 2, 4)\n        self.init = 'ortho'\n        self.twiddle = nn.Parameter(torch.empty(twiddle_shape))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_size, dtype=complex_dtype))\n        else:\n            self.register_parameter('bias', None)\n        self.twiddle._is_structured = True  # Flag to avoid weight decay\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        \"\"\"Initialize bias the same way as torch.nn.Linear.\"\"\"\n        # Sampling from the Haar measure on U(2) is a bit subtle.\n        # Using the parameterization here: http://home.lu.lv/~sd20008/papers/essays/Random%20unitary%20[paper].pdf\n        twiddle_core_shape = self.twiddle.shape[:-1]\n        phi = torch.asin(torch.sqrt(torch.rand(twiddle_core_shape)))\n        alpha, psi, chi = torch.rand((3, ) + twiddle_core_shape) * math.pi * 2\n        with torch.no_grad():\n            self.twiddle.copy_(torch.stack([phi, alpha, psi, chi], dim=-1))\n        if self.bias is not None:\n            bound = 1 / math.sqrt(self.in_size)\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input, transpose=False, conjugate=False, subtwiddle=False):\n        \"\"\"\n        Parameters:\n            input: (batch, *, in_size)\n            transpose: whether the butterfly matrix should be transposed.\n            conjugate: whether the butterfly matrix should be conjugated.\n            subtwiddle: allow using only part of the parameters for smaller input.\n                Could be useful for weight sharing.\n                out_size is set to self.nstacks * self.n in this case\n        Return:\n            output: (batch, *, out_size)\n        \"\"\"\n        phi, alpha, psi, chi = torch.unbind(self.twiddle, -1)\n        c, s = torch.cos(phi), torch.sin(phi)\n        # Pytorch 1.7 doesn't support complex exp backward so we have to use cos/sin\n        A = torch.stack((c * torch.cos(alpha + psi), c * torch.sin(alpha + psi)), dim=-1)\n        B = torch.stack((s * torch.cos(alpha + chi), s * torch.sin(alpha + chi)), dim=-1)\n        C = torch.stack((-s * torch.cos(alpha - chi), -s * torch.sin(alpha - chi)), dim=-1)\n        D = torch.stack((c * torch.cos(alpha - psi), c * torch.sin(alpha - psi)), dim=-1)\n        twiddle = torch.stack([torch.stack([A, B], dim=-2),\n                               torch.stack([C, D], dim=-2)], dim=-3)\n        twiddle = torch.view_as_complex(twiddle)\n        output = self.pre_process(input)\n        output_size = self.out_size if self.nstacks == 1 else None\n        if subtwiddle:\n            log_n = int(math.ceil(math.log2(input.size(-1))))\n            n = 1 << log_n\n            twiddle = (twiddle[:, :, :log_n, :n // 2] if self.increasing_stride\n                       else twiddle[:, :, -log_n:, :n // 2])\n            output_size = None\n        if conjugate and self.complex:\n            twiddle = twiddle.conj()\n        if not transpose:\n            output = butterfly_multiply(twiddle, output, self.increasing_stride, output_size)\n        else:\n            twiddle = twiddle.transpose(-1, -2).flip([1, 2])\n            last_increasing_stride = self.increasing_stride != ((self.nblocks - 1) % 2 == 1)\n            output = butterfly_multiply(twiddle, output, not last_increasing_stride, output_size)\n        if not subtwiddle:\n            return self.post_process(input, output)\n        else:\n            return self.post_process(input, output, out_size=output.size(-1))\n\n    __imul__ = None\n    to_base4 = None\n\n    def extra_repr(self):\n        s = 'in_size={}, out_size={}, bias={}, increasing_stride={}, nblocks={}'.format(\n            self.in_size, self.out_size, self.bias is not None, self.increasing_stride, self.nblocks,)\n        return s\n\n\nclass ButterflyBmm(Butterfly):\n    \"\"\"Same as Butterfly, but performs batched matrix multiply.\n    Compatible with torch.nn.Linear.\n\n    Parameters:\n        in_size: size of input\n        out_size: size of output\n        matrix_batch: how many butterfly matrices\n        bias: If set to False, the layer will not learn an additive bias.\n                Default: ``True``\n        complex: whether complex or real\n        increasing_stride: whether the first butterfly block will multiply with increasing stride\n            (e.g. 1, 2, ..., n/2) or decreasing stride (e.g., n/2, n/4, ..., 1).\n        init: 'randn', 'ortho', or 'identity'. Whether the weight matrix should be initialized to\n            from randn twiddle, or to be randomly orthogonal/unitary, or to be the identity matrix.\n        nblocks: number of B or B^T blocks. The B and B^T will alternate.\n    \"\"\"\n\n    def __init__(self, in_size, out_size, matrix_batch=1, bias=True, complex=False,\n                 increasing_stride=True, init='randn', nblocks=1):\n        nn.Module.__init__(self)\n        self.in_size = in_size\n        self.log_n = log_n = int(math.ceil(math.log2(in_size)))\n        self.n = n = 1 << log_n\n        self.out_size = out_size\n        self.matrix_batch = matrix_batch\n        self.nstacks = int(math.ceil(out_size / self.n))\n        self.complex = complex\n        self.increasing_stride = increasing_stride\n        assert nblocks >= 1\n        self.nblocks = nblocks\n        dtype = torch.get_default_dtype() if not self.complex else real_dtype_to_complex[torch.get_default_dtype()]\n        twiddle_shape = (self.matrix_batch * self.nstacks, nblocks, log_n, n // 2, 2, 2)\n        if isinstance(init, torch.Tensor):\n            self.init = None\n            assert init.shape == twiddle_shape\n            assert init.dtype == dtype\n            self.twiddle = nn.Parameter(init.clone())\n        else:\n            assert init in ['randn', 'ortho', 'identity', 'fft_no_br', 'ifft_no_br']\n            self.init = init\n            self.twiddle = nn.Parameter(torch.empty(twiddle_shape, dtype=dtype))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(self.matrix_batch, out_size, dtype=dtype))\n        else:\n            self.register_parameter('bias', None)\n        self.twiddle._is_structured = True  # Flag to avoid weight decay\n        self.reset_parameters()\n\n    def forward(self, input, transpose=False, conjugate=False):\n        \"\"\"\n        Parameters:\n            input: (batch, *, matrix_batch, in_size)\n            transpose: whether the butterfly matrix should be transposed.\n            conjugate: whether the butterfly matrix should be conjugated.\n        Return:\n            output: (batch, *, matrix_batch, out_size)\n        \"\"\"\n        return super().forward(input, transpose, conjugate, subtwiddle=False)\n\n    def pre_process(self, input):\n        # Reshape to (N, matrix_batch, in_size)\n        input_size = input.size(-1)\n        assert input.size(-2) == self.matrix_batch\n        output = complex_reshape(input, -1, self.matrix_batch, input_size)\n        batch = output.shape[0]\n        output = output.unsqueeze(2).expand(batch, self.matrix_batch, self.nstacks, input_size)\n        output = output.reshape(batch, self.matrix_batch * self.nstacks, input_size)\n        return output\n\n    def post_process(self, input, output, out_size=None):\n        if out_size is None:\n            out_size = self.out_size\n        batch = output.shape[0]\n        output = output.view(batch, self.matrix_batch, self.nstacks * output.size(-1))\n        if out_size != output.shape[-1]:  # Take top rows\n            output = output[:, :, :out_size]\n        if self.bias is not None:\n            output = output + self.bias[:, :out_size]\n        return output.view(*input.size()[:-2], self.matrix_batch, self.out_size)\n\n    to_base4 = None\n\n    def extra_repr(self):\n        s = 'in_size={}, out_size={}, matrix_batch={}, bias={}, complex={}, increasing_stride={}, init={}, nblocks={}'.format(\n            self.in_size, self.out_size, self.matrix_batch, self.bias is not None, self.complex, self.increasing_stride, self.init, self.nblocks,)\n        return s",
    "description": null,
    "url": null
}