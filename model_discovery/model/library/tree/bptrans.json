{
    "acronym": "bptrans",
    "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning",
    "seed_ids": [
        "sparsetransformer",
        "transformerxl",
        "transformer"
    ],
    "s2id": "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
    "abstract": "The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields $O(k\\cdot n\\log (n/k))$ connections where $k$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.",
    "authors": [
        "Zihao Ye",
        "Qipeng Guo",
        "Quan Gan",
        "Xipeng Qiu",
        "Zheng Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": "Ad adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), BP-Transformer (BPT for short) is proposed, which has a superior performance for long text than previous self-attention models.",
    "citationCount": 73,
    "influentialCitationCount": 4,
    "code": "import torch.nn as nn\nimport numpy as np\nimport dgl.function as fn\nimport torch as th\nfrom dgl.nn.pytorch.softmax import edge_softmax\nfrom .op import *\n\nclass PositionwiseFeedForward(nn.Module):\n    '''\n    FFN(x) = max(0, x @ W_1 + b_1) @ W_2 + b_2\n    '''\n    def __init__(self, dim_model, dim_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(dim_model, dim_ff)\n        self.w_2 = nn.Linear(dim_ff, dim_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(th.relu(self.w_1(x))))\n\n\nclass BPTBlock(nn.Module):\n    MAX_ETYPE = 1000\n    def __init__(self, dim_model, h, dim_ff, rel_pos=False, drop_h=0.1, drop_a=0.1):\n        super(BPTBlock, self).__init__()\n        self.dim_model = dim_model\n        self.h = h\n        self.dim_ff = dim_ff\n        self.d_k = self.dim_model // self.h\n        self.rel_pos = rel_pos\n        self.drop_h = nn.Dropout(drop_h)\n        self.drop_att = nn.Dropout(drop_a)\n        self.norm_in = nn.LayerNorm(self.dim_model)\n        self.norm_inter = nn.LayerNorm(self.dim_model)\n\n        self.proj_q = nn.Linear(dim_model, self.d_k * self.h, bias=False)\n        self.proj_k = nn.Linear(dim_model, self.d_k * self.h, bias=False)\n        self.proj_v = nn.Linear(dim_model, self.d_k * self.h, bias=False)\n        self.proj_o = nn.Linear(self.d_k * self.h, dim_model, bias=False)\n\n        if self.rel_pos:\n            self.embed_ak = nn.Embedding(self.MAX_ETYPE, self.d_k)\n\n        self.ffn = nn.Sequential(\n            PositionwiseFeedForward(self.dim_model, self.dim_ff, dropout=drop_h),\n            nn.Dropout(drop_h)\n        )\n\n    def reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, g, csr):\n        h = g.ndata['h'] # get pos embedding\n        if self.rel_pos:\n            g.edata['ak'] = self.embed_ak(g.edata['etype'])\n\n        # get in and out csr\n        out_csr, in_csr, ROW, INDPTR_R, COL, INDPTR_C = csr \n        # get queries, keys and values\n        g.ndata['q'] = self.proj_q(h).view(-1, self.h, self.d_k)\n        g.ndata['k'] = self.proj_k(h).view(-1, self.h, self.d_k)\n        g.ndata['v'] = self.proj_v(h).view(-1, self.h, self.d_k)\n\n        e = masked_mm(\n            ROW, INDPTR_R, out_csr[2], out_csr[1], COL, INDPTR_C, in_csr[2], in_csr[1], g.ndata['k'], g.ndata['q'])\n\n        e_rel = 0\n        if self.rel_pos:\n            e_rel = node_mul_edge(COL, INDPTR_C, in_csr[2], g.ndata['q'], g.edata['ak'])\n\n        e = self.drop_att(sparse_softmax(COL, INDPTR_C, in_csr[2], (e_rel + e) / np.sqrt(self.d_k)))\n        a = vec_spmm(\n            COL, INDPTR_C, in_csr[2], in_csr[1],\n            ROW, INDPTR_R, out_csr[2], out_csr[1],\n            e, g.ndata['v']).view(-1, self.d_k * self.h)\n        o = self.drop_h(self.proj_o(a))\n        h = self.norm_in(h + o)\n        h = self.norm_inter(h + self.ffn(h))\n        g.ndata['h'] = h\n\n\nclass BPTMemBlock(nn.Module):\n    MAX_ETYPE = 1000\n    def __init__(self, dim_model, h, dim_ff, rel_pos=False, drop_h=0.1, drop_a=0.1):\n        super(BPTMemBlock, self).__init__()\n        self.dim_model = dim_model\n        self.h = h\n        self.dim_ff = dim_ff\n        self.d_k = self.dim_model // self.h\n        self.rel_pos = rel_pos\n        self.drop_h = nn.ModuleList([nn.Dropout(drop_h) for _ in range(2)])\n        self.drop_att = nn.ModuleList([nn.Dropout(drop_a) for _ in range(2)])\n        self.norm_in = nn.ModuleList([nn.LayerNorm(self.dim_model) for _ in range(2)])\n        self.norm_inter = nn.LayerNorm(self.dim_model)\n\n        self.proj_q = nn.ModuleList([nn.Linear(dim_model, self.d_k * self.h, bias=False) for _ in range(2)])\n        self.proj_k = nn.ModuleList([nn.Linear(dim_model, self.d_k * self.h, bias=False) for _ in range(2)])\n        self.proj_v = nn.ModuleList([nn.Linear(dim_model, self.d_k * self.h, bias=False) for _ in range(2)])\n        self.proj_o = nn.ModuleList([nn.Linear(self.d_k * self.h, dim_model, bias=False) for _ in range(2)])\n\n        if self.rel_pos:\n            self.embed_ak = nn.Embedding(self.MAX_ETYPE, self.d_k)\n\n        self.ffn = nn.Sequential(\n            PositionwiseFeedForward(self.dim_model, self.dim_ff, dropout=drop_h),\n            nn.Dropout(drop_h)\n        )\n\n    def reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, g, csr, mem, csr_inter):\n        # Part I: self-attention\n        h = g.ndata['h']\n        if self.rel_pos:\n            g.edata['ak'] = self.embed_ak(g.edata['etype'])\n\n        # get csr\n        out_csr, in_csr, ROW, INDPTR_R, COL, INDPTR_C = csr\n        # get queries, keys and values\n        g.ndata['q'] = self.proj_q[0](h).view(-1, self.h, self.d_k)\n        g.ndata['k'] = self.proj_k[0](h).view(-1, self.h, self.d_k)\n        g.ndata['v'] = self.proj_v[0](h).view(-1, self.h, self.d_k)\n\n        e = masked_mm(\n            ROW, INDPTR_R, out_csr[2], out_csr[1], COL, INDPTR_C, in_csr[2], in_csr[1], g.ndata['k'], g.ndata['q'])\n\n        e_rel = 0\n        if self.rel_pos:\n            e_rel = node_mul_edge(COL, INDPTR_C, in_csr[2], g.ndata['q'], g.edata['ak'])\n\n        e = self.drop_att[0](sparse_softmax(COL, INDPTR_C, in_csr[2], (e_rel + e) / np.sqrt(self.d_k)))\n        a = vec_spmm(\n            COL, INDPTR_C, in_csr[2], in_csr[1],\n            ROW, INDPTR_R, out_csr[2], out_csr[1],\n            e, g.ndata['v']).view(-1, self.d_k * self.h)\n        o = self.drop_h[0](self.proj_o[0](a))\n        h = self.norm_in[0](h + o)\n\n        # Part II: attention to memory\n        # get csr\n        out_csr, in_csr, ROW, INDPTR_R, COL, INDPTR_C = csr_inter\n        \n        if (h.shape[0] < mem.shape[0]):\n            h = th.cat([h, h.new_zeros(mem.shape[0] - h.shape[0], *h.shape[1:])], 0)\n\n        # get queries, keys\n        q = self.proj_q[1](h).view(-1, self.h, self.d_k)\n        k = self.proj_k[1](mem).view(-1, self.h, self.d_k)\n        v = self.proj_v[1](mem).view(-1, self.h, self.d_k)\n\n        e = masked_mm(\n            ROW, INDPTR_R, out_csr[2], out_csr[1], COL, INDPTR_C, in_csr[2], in_csr[1], k, q)\n        e = self.drop_att[1](sparse_softmax(COL, INDPTR_C, in_csr[2], e / np.sqrt(self.d_k)))\n        a = vec_spmm(\n            COL, INDPTR_C, in_csr[2], in_csr[1],\n            ROW, INDPTR_R, out_csr[2], out_csr[1],\n            e, v).view(-1, self.d_k * self.h)\n        # this would be deprecated after using bipartite graph.\n        h = h[:g.number_of_nodes()]\n        a = a[:g.number_of_nodes()]\n\n        o = self.drop_h[1](self.proj_o[1](a))\n        h = self.norm_in[1](h + o)\n\n        # FFN\n        h = self.norm_inter(h + self.ffn(h))\n        g.ndata['h'] = h\n\n    def infer(self,\n              g,\n              nids_eq_pos,\n              eids_eq_pos,\n              nids_eq_pos_leaf,\n              g_inter, readout_ids):\n        # Part I: self-attention\n        h = g.nodes[nids_eq_pos].data['h']\n        if self.rel_pos:\n            g.edges[eids_eq_pos].data['ak'] = self.embed_ak(g.edges[eids_eq_pos].data['etype'])\n\n        g.nodes[nids_eq_pos].data['q'] = self.proj_q[0](h).view(-1, self.h, self.d_k)\n        g.nodes[nids_eq_pos].data['k'] = self.proj_k[0](h).view(-1, self.h, self.d_k)\n        g.nodes[nids_eq_pos].data['v'] = self.proj_v[0](h).view(-1, self.h, self.d_k)\n\n        g.apply_edges(\n            lambda edges: {'e': (edges.src['k'] * edges.dst['q']).sum(dim=-1, keepdim=True)},\n            eids_eq_pos\n        )\n        e = g.edges[eids_eq_pos].data['e']\n        # relative positional encoding\n        if self.rel_pos:\n            g.apply_edges(\n                lambda edges: {'e_rel': (edges.data['ak'].unsqueeze(1) * edges.dst['q']).sum(dim=-1, keepdim=True)},\n                eids_eq_pos\n            )\n            e = e + g.edges[eids_eq_pos].data['e_rel']\n        # softmax\n        g.edges[eids_eq_pos].data['a'] = self.drop_att[0](\n            edge_softmax(\n                g, e / np.sqrt(self.d_k),\n                eids_eq_pos\n            )\n        )\n        # spmm\n        g.send_and_recv(eids_eq_pos, fn.u_mul_e('v', 'a', 'm'), fn.sum('m', 'o'))\n        o = g.nodes[nids_eq_pos].data['o'].view(-1, self.d_k * self.h)\n        o = self.drop_h[0](self.proj_o[0](o))\n        g.nodes[nids_eq_pos].data['h'] = self.norm_in[0](h + o)\n\n        # Part II: attend to memory\n        h = g.nodes[nids_eq_pos_leaf].data['h']\n        q = self.proj_q[1](h).view(-1, self.h, self.d_k)\n        g_inter.nodes[readout_ids].data['q'] = q\n        g_inter.apply_edges(\n            lambda edges: {'e': (edges.src['k'] * edges.dst['q']).sum(dim=-1, keepdim=True)})\n        # softmax\n        g_inter.edata['a'] = self.drop_att[1](edge_softmax(g_inter, g_inter.edata['e'] / np.sqrt(self.d_k)))\n        # spmm\n        g_inter.update_all(fn.u_mul_e('v', 'a', 'm'), fn.sum('m', 'o'))\n        o = g_inter.nodes[readout_ids].data['o'].view(-1, self.d_k * self.h)\n        o = self.drop_h[1](self.proj_o[1](o))\n        g.nodes[nids_eq_pos_leaf].data['h'] = h + o\n        h = self.norm_in[1](g.nodes[nids_eq_pos].data['h'])\n\n        # FFN\n        h = self.norm_inter(h + self.ffn(h))\n        g.nodes[nids_eq_pos].data['h'] = h\n",
    "description": null,
    "url": null
}