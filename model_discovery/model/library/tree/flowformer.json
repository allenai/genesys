{
    "acronym": "flowformer",
    "title": "Flowformer: Linearizing Transformers with Conservation Flows ",
    "seed_ids": [
        "cosformer",
        "rfa",
        "nystromformer",
        "performer",
        "bigbird",
        "clusteredattn",
        "lineartransformer",
        "linformer",
        "sparsetransformer",
        "transformer",
        "bert"
    ],
    "s2id": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
    "abstract": "Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.",
    "authors": [
        "Haixu Wu",
        "Jialong Wu",
        "Jiehui Xu",
        "Jianmin Wang",
        "Mingsheng Long"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": "This paper linearize Transformers free from specific inductive biases based on the flow network theory and proposes the Flow-Attention mechanism of linear complexity, which yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning.",
    "citationCount": 54,
    "influentialCitationCount": 6,
    "code": "import math\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nfrom fairseq.modules.quant_noise import quant_noise\nfrom torch import Tensor, nn\nfrom torch.nn import Parameter\n\n\n## flow attention\n@with_incremental_state\nclass MultiheadAttention(nn.Module):\n    \"\"\"Multi-headed Flow_Attention.\n    \"\"\"\n\n    def __init__(\n            self,\n            embed_dim,\n            num_heads,\n            kdim=None,\n            vdim=None,\n            dropout=0.0,\n            bias=True,\n            add_bias_kv=False,\n            add_zero_attn=False,\n            self_attention=False,\n            encoder_decoder_attention=False,\n            q_noise=0.0,\n            qn_block_size=8,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n        self.num_heads = num_heads\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n\n        self.head_dim = embed_dim // num_heads\n        assert (\n                self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            \"Self-attention requires query, key and \" \"value to be of the same size\"\n        )\n\n        self.k_proj = quant_noise(\n            nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.v_proj = quant_noise(\n            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.q_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        self.out_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self.reset_parameters()\n        self.onnx_trace = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def reset_parameters(self):\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_proj.weight)\n            nn.init.xavier_uniform_(self.v_proj.weight)\n            nn.init.xavier_uniform_(self.q_proj.weight)\n\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def kernel(self, x):\n        x = torch.sigmoid(x)\n        return x\n\n    def causal_dot_product(self, q, k, v):\n        kv = torch.einsum(\"nld,nlm->nldm\", k, v)\n        kv = torch.cumsum(kv, dim=1)\n        qkv = torch.einsum(\"nld,nldm->nlm\", q, kv)\n        return qkv\n\n    def forward(\n            self,\n            query,\n            key: Optional[Tensor],\n            value: Optional[Tensor],\n            key_padding_mask: Optional[Tensor] = None,\n            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n            need_weights: bool = True,\n            static_kv: bool = False,\n            attn_mask: Optional[Tensor] = None,\n            before_softmax: bool = False,\n            need_head_weights: bool = False,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        if need_head_weights:\n            need_weights = True\n\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = tgt_len\n        assert embed_dim == self.embed_dim, f\"query dim {embed_dim} != {self.embed_dim}\"\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        if key is not None:\n            src_len, key_bsz, _ = key.size()\n            if not torch.jit.is_scripting():\n                assert key_bsz == bsz\n                assert value is not None\n                assert src_len, bsz == value.shape[:2]\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and \"prev_key\" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n\n        if self.bias_k is not None:\n            assert self.bias_v is not None\n            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n                    ],\n                    dim=1,\n                )\n\n        q = (\n            q.contiguous()\n                .view(tgt_len, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                    .view(-1, bsz * self.num_heads, self.head_dim)\n                    .transpose(0, 1)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                    .view(-1, bsz * self.num_heads, self.head_dim)\n                    .transpose(0, 1)\n            )\n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n            if \"prev_key\" in saved_state:\n                _prev_key = saved_state[\"prev_key\"]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n                src_len = k.size(1)\n            if \"prev_value\" in saved_state:\n                _prev_value = saved_state[\"prev_value\"]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if \"prev_key_padding_mask\" in saved_state:\n                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n            assert k is not None and v is not None\n            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        assert k.size(1) == src_len\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        if self.add_zero_attn:\n            assert v is not None\n            src_len += 1\n            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n                            key_padding_mask\n                        ),\n                    ],\n                    dim=1,\n                )\n\n        # kernel\n        q = self.kernel(q)\n        k = self.kernel(k)\n        if attn_mask is not None:\n            # mask v\n            if key_padding_mask is not None:\n                v = v.view(bsz, self.num_heads, src_len, self.head_dim)\n                v.masked_fill(key_padding_mask.unsqueeze(1).to(torch.bool).unsqueeze(-1), float(0))\n                v = v.view(bsz * self.num_heads, src_len, self.head_dim)\n\n            # (1) incoming and outgoing flow\n            sink_incoming = 1.0 / (torch.einsum(\"nld,nld->nl\", q + 1e-6, k.cumsum(dim=1) + 1e-6))\n            sink_incoming_keep = sink_incoming\n            source_outgoing = 1.0 / (torch.einsum(\"nld,nld->nl\", k + 1e-6, q.cumsum(dim=1) + 1e-6))\n            # approximate normalized col and row\n            normal = (((torch.arange(q.shape[1])).float() + 1.0)).cuda()[None, :]\n            sink_incoming = sink_incoming * normal\n            source_outgoing = source_outgoing * normal\n            # (2) conservation property\n            conserved_sink = torch.einsum(\"nld,nld->nl\", q + 1e-6,\n                                          (k * source_outgoing[:, :, None]).cumsum(dim=1) + 1e-6) / normal\n            conserved_source = torch.einsum(\"nld,nld->nl\", k + 1e-6,\n                                            (q * sink_incoming[:, :, None]).cumsum(dim=1) + 1e-6) / normal\n            conserved_source = torch.clamp(conserved_source, min=-5.0, max=5.0)\n            # (3) competition & allocation\n            sink_allocation = torch.sigmoid(conserved_sink)\n            conserved_source = torch.exp(conserved_source)\n            source_competition = (conserved_source / conserved_source.cumsum(dim=-1)) * normal\n            # multiply\n            output_new = (self.causal_dot_product(q * sink_incoming_keep[:, :, None],  # for value normalization\n                                              k,\n                                              v * source_competition[:, :, None])  # competition\n                      * sink_allocation[:, :, None])  # allocation\n            attn = output_new\n        else:\n            raise NotImplementedError(\"only support causal task\")\n\n        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n        if self.onnx_trace and attn.size(1) == 1:\n            # when ONNX tracing a single decoder step (sequence length == 1)\n            # the transpose is a no-op copy before view, thus unnecessary\n            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n        else:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n        attn = self.out_proj(attn)\n        return attn, None\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n            key_padding_mask: Optional[Tensor],\n            prev_key_padding_mask: Optional[Tensor],\n            batch_size: int,\n            src_len: int,\n            static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            if src_len > prev_key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n                    device=prev_key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [prev_key_padding_mask.float(), filler.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = prev_key_padding_mask.float()\n        elif key_padding_mask is not None:\n            if src_len > key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - key_padding_mask.size(1)),\n                    device=key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [filler.float(), key_padding_mask.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = key_padding_mask.float()\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    @torch.jit.export\n    def reorder_incremental_state(\n            self,\n            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n            new_order: Tensor,\n    ):\n        \"\"\"Reorder buffered internal state (for incremental generation).\"\"\"\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            for k in input_buffer.keys():\n                input_buffer_k = input_buffer[k]\n                if input_buffer_k is not None:\n                    if self.encoder_decoder_attention and input_buffer_k.size(\n                            0\n                    ) == new_order.size(0):\n                        break\n                    input_buffer[k] = input_buffer_k.index_select(0, new_order)\n            incremental_state = self._set_input_buffer(incremental_state, input_buffer)\n        return incremental_state\n\n    def _get_input_buffer(\n            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, \"attn_state\")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n            self,\n            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n            buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n\n    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + \".\" if name != \"\" else \"\"\n        items_to_add = {}\n        keys_to_remove = []\n        for k in state_dict.keys():\n            if k.endswith(prefix + \"in_proj_weight\"):\n                # in_proj_weight used to be q + k + v with same dimensions\n                dim = int(state_dict[k].shape[0] / 3)\n                items_to_add[prefix + \"q_proj.weight\"] = state_dict[k][:dim]\n                items_to_add[prefix + \"k_proj.weight\"] = state_dict[k][dim: 2 * dim]\n                items_to_add[prefix + \"v_proj.weight\"] = state_dict[k][2 * dim:]\n\n                keys_to_remove.append(k)\n\n                k_bias = prefix + \"in_proj_bias\"\n                if k_bias in state_dict.keys():\n                    dim = int(state_dict[k].shape[0] / 3)\n                    items_to_add[prefix + \"q_proj.bias\"] = state_dict[k_bias][:dim]\n                    items_to_add[prefix + \"k_proj.bias\"] = state_dict[k_bias][\n                                                           dim: 2 * dim\n                                                           ]\n                    items_to_add[prefix + \"v_proj.bias\"] = state_dict[k_bias][2 * dim:]\n\n                    keys_to_remove.append(prefix + \"in_proj_bias\")\n\n        for k in keys_to_remove:\n            del state_dict[k]\n\n        for key, value in items_to_add.items():\n            state_dict[key] = value",
    "description": null,
    "url": null
}