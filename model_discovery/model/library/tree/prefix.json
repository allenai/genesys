{
    "acronym": "prefix",
    "title": "Just read twice: closing the recall gap for recurrent language models",
    "seed_ids": [],
    "s2id": "0671179a1bc13419d725b15af49934a99e10f4b8",
    "abstract": "Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.",
    "authors": [
        "Simran Arora",
        "Aman Timalsina",
        "Aaryan Singhal",
        "Benjamin Spector",
        "Sabri Eyuboglu",
        "Xinyi Zhao",
        "Ashish Rao",
        "Atri Rudra",
        "Christopher R'e"
    ],
    "venue": "",
    "year": 2024,
    "tldr": "This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "# Copyright (c) 2024, Simran Arora\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom typing import Optional\nfrom train.src.generation import InferenceParams\n\ntry:\n    from fla.ops.based import fused_chunk_based, parallel_based\n    from fla.ops.based.naive import naive_parallel_based\n    print(f\"Successfully imported the FLA triton kernels! \")\nexcept:\n    print(f\"Could not import the FLA triton kernels... \")\n    \n\nclass TaylorExp(nn.Module):\n    \"\"\"\n    Feature map to compute 2nd-order Taylor approx. of exp(q^T k / sqrt(d))\n    \"\"\"\n    def __init__(\n            self, \n            input_dim: int, \n            scale_dim: Optional[int] = None, \n            eps: float = 1e-12,\n            head_dim_idx: int = -1, \n            **kwargs: any\n        ):\n        super().__init__()\n        self.input_dim = input_dim\n        self.head_dim_idx = head_dim_idx     \n        self.eps = eps\n        self.r2  = math.sqrt(2)\n        if scale_dim is None:\n            scale_dim = self.input_dim\n        self.rd  = math.sqrt(scale_dim)\n        self.rrd = math.sqrt(self.rd)\n        self.tril_indices = torch.tril_indices(self.input_dim, self.input_dim, -1)\n        \n    def forward(self, x: torch.Tensor):\n        # Get 2nd-order terms (rearrange(x * x), '... m n -> ... (m n)')\n        x2 = (x.unsqueeze(-1) * x.unsqueeze(-2)).flatten(start_dim=-2) / self.r2\n        return torch.cat([x[..., :1] ** 0, \n                          x / self.rrd, x2 / self.rd], dim=self.head_dim_idx)\n\n\nclass PrefixLinearAttention(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        num_heads=None,\n        feature_dim: int = 16,\n        eps: float = 1e-12,\n        scale_dim: Optional[int] = None,\n        enc_length = None,\n        dec_length = None,\n        device=None,\n        dtype=None,\n        layer_idx=-1,\n        implementation=\"default\",\n        use_mask=True,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        \n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n\n        self.layer_idx = layer_idx\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_heads_kv = num_heads\n        assert (self.num_heads % self.num_heads_kv == 0), \"num_heads must be divisible by num_heads_kv\"\n        assert self.embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n        self.head_dim = self.embed_dim // num_heads\n        self.softmax_scale = 1.0 / math.sqrt(self.head_dim)\n        self.enc_length = enc_length\n        self.dec_length = dec_length\n    \n        # For linear attention\n        self.d_model = embed_dim\n        self.feature_dim = feature_dim\n        feature_map_kwargs = {\n            'input_dim': self.feature_dim,\n            'head_dim_idx': -1,\n            'eps': 1e-12,\n            \"scale_dim\": scale_dim,\n        }\n        self.feature_map = TaylorExp(**feature_map_kwargs)\n        self.proj_q = nn.Linear(self.d_model, self.feature_dim * self.num_heads, bias=False)\n        self.proj_k = nn.Linear(self.d_model, self.feature_dim * self.num_heads, bias=False)\n        self.proj_v = nn.Linear(self.d_model, self.num_heads_kv * self.head_dim, bias=False)\n\n        self.proj_k_enc = nn.Linear(self.d_model, self.feature_dim * self.num_heads, bias=False)\n        self.proj_v_enc = nn.Linear(self.d_model, self.num_heads_kv * self.head_dim, bias=False)\n        self.out_proj = nn.Linear(self.num_heads * self.head_dim, self.d_model, bias=False)\n        self.dropout = nn.Identity()\n        self.eps = eps\n\n        self.implementation = implementation\n        self.use_mask = use_mask\n\n        print(f\"{self.enc_length=}, {self.dec_length=}, {self.implementation=}, {self.use_mask=}\")\n\n    def forward(\n        self,\n        x,\n        inference_params: InferenceParams = None,\n        **kwargs,\n    ):\n        batch, seqlen = x.shape[:2]\n        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n        q = q.view(batch, seqlen, self.num_heads, self.feature_dim).transpose(1, 2)\n        k = k.view(batch, seqlen, self.num_heads, self.feature_dim).transpose(1, 2)\n        v = v.view(batch, seqlen, self.num_heads, self.head_dim).transpose(1, 2)\n\n        mask = None\n        num_zeros = None\n        if 'mask' in kwargs:\n            mask = kwargs['mask']\n        elif 'attn_mask' in kwargs:\n            mask = kwargs['attn_mask']\n        elif inference_params is not None:\n            try:\n                mask = inference_params.mask \n            except:\n                mask = None\n\n        # Encoder part\n        k_enc, v_enc = None, None\n        if inference_params is None or inference_params.seqlen_offset == 0: # We only do encoder in this region\n            enc_length = min(self.enc_length, seqlen) # In inference, enc_length could be > seqlen\n            x_enc = x[:, :enc_length]\n            k_enc, v_enc = self.proj_k_enc(x_enc), self.proj_v_enc(x_enc)\n            k_enc = k_enc.view(batch, enc_length, self.num_heads, self.feature_dim).transpose(1, 2)\n            v_enc = v_enc.view(batch, enc_length, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Applying masks \n        # if mask is not None and q.shape[2] > 1 and mask.shape[1] <= enc_length and self.use_mask: \n        #     # Second condition checks that we're in prefill\n        #     # num_zeros = mask.shape[1] - torch.sum(mask[0], dim=-1) \n        #     if len(mask.shape) == 4:\n        #         lin_attn_mask = (mask == 0)[:, :1, -1, :][..., None]  # b, 1, k_len, 1\n        #     else:\n        #         lin_attn_mask = mask[:, None, :, None]  # b, 1, k_len, 1\n        #     lin_attn_mask = lin_attn_mask.to(torch.bool)\n        #     k = k.masked_fill(~lin_attn_mask, 0)\n        #     k_enc = k_enc.masked_fill(~lin_attn_mask, 0)\n\n        if inference_params is None:\n            y = self.parallel_forward(x, q, k, v, k_enc, v_enc, mask=mask)\n            return y\n        \n        else:\n            # check if we're doing prefill or generation\n            if inference_params.seqlen_offset > 0: \n                \n                # recurrent generation\n                kv_state_dec, k_state_dec = self._get_inference_cache(inference_params)\n                q, k = self.feature_map(q), self.feature_map(k)\n                if k_enc is not None: \n                    assert 0, print(f\"Need to left pad your prefill!\")\n                return self.recurrent_forward(x, kv_state_dec, k_state_dec, q, k, v)\n            \n            else:  \n                y = self.parallel_forward(x, q, k, v, k_enc, v_enc, mask=mask)\n                in_dt = q.dtype \n                dt = q.dtype #torch.float32\n\n                # Encoder part\n                k_enc = self.feature_map(k_enc)\n                kv_state_enc = torch.einsum(\"bhnd,bhnf->bhfd\", k_enc.to(dt), v_enc.to(dt))[:, :, None]\n                k_state_enc = k_enc.sum(dim=2)[:, :, None, None]\n\n                # Decoder part\n                q, k = self.feature_map(q), self.feature_map(k)\n                kv_state_dec = torch.einsum(\"bhnd,bhnf->bhfd\", k.to(dt), v.to(dt))[:, :, None]\n                k_state_dec = k.to(dt).sum(dim=2)[:, :, None, None]\n\n                # Combined State\n                kv_state_dec += kv_state_enc\n                k_state_dec += k_state_enc\n\n                if self.layer_idx in inference_params.key_value_memory_dict:\n                    # update the state in-place when graph caching is enabled\n                    inference_params.key_value_memory_dict[self.layer_idx][0].copy_(kv_state_dec.to(in_dt))\n                    inference_params.key_value_memory_dict[self.layer_idx][1].copy_(k_state_dec.to(in_dt))\n                else: \n                    inference_params.key_value_memory_dict[self.layer_idx] = (kv_state_dec.to(in_dt), k_state_dec.to(in_dt))\n                return y\n\n\n    def parallel_forward(self, x, q, k, v, k_enc=None, v_enc=None, mask=None):\n        batch, seqlen = x.shape[:2]\n        k_enc = self.feature_map(k_enc)\n        in_dt = q.dtype \n        dt = q.dtype #torch.float32\n\n        # Standard attention\n        if self.implementation == \"default\": \n            q, k = self.feature_map(q), self.feature_map(k)\n            A_qk = torch.einsum(\"bhnd,bhmd->bhnm\", q.to(dt), k.to(dt)) \n            try:\n                A_qk = torch.tril(A_qk)\n            except:\n                cumsum_matrix = torch.tril(torch.ones((seqlen, seqlen), device=A_qk.device))\n                A_qk = A_qk * cumsum_matrix\n            y1 = torch.einsum(\"bhnm,bhme->bhne\", A_qk.to(dt), v.to(dt))\n\n        elif self.implementation == \"fla_parallel\": \n            y1, z1 = parallel_based(q, k, v, use_scale=True, use_normalize=False, return_both=True) # false to handle denom seperately\n            q = self.feature_map(q)\n        else:\n            assert 0, print(f\"Invalid implementation: {self.implementation}\")\n\n        # Cross attention\n        A_qk_2 = torch.einsum(\"bhnd,bhmd->bhnm\", q.to(dt), k_enc.to(dt))\n        y2 = torch.einsum(\"bhnm,bhme->bhne\", A_qk_2.to(dt), v_enc.to(dt))\n\n        # Denominator\n        if self.implementation == \"default\": \n            k_state = k_enc.to(dt).sum(dim=2, keepdim=True) +  k.to(dt).cumsum(2)\n            z = 1 / ((q.to(dt) * k_state.to(dt)).sum(dim=-1)  + self.eps)\n            y1 = y1 * z[..., None]\n            output_1 = rearrange(y1, 'b h l d -> b l (h d)')\n            y2 = y2 * z[..., None]\n            output_2 = rearrange(y2, 'b h l d -> b l (h d)')\n            output = output_1 + output_2\n\n        elif self.implementation == \"fla_parallel\": \n            z2 = (q * k_enc.to(dt).sum(dim=2, keepdim=True)).sum(dim=-1)\n            z = 1 / ((z2.to(dt) + z1.to(dt)) + self.eps)\n            output = (y1 +  y2) * z[..., None]\n            output = rearrange(output, 'b h l d -> b l (h d)')\n\n        else:\n            assert 0, print(f\"Invalid implementation: {self.implementation}\")\n            \n        output = self.out_proj(output)\n        return output.to(in_dt)\n\n\n    def recurrent_forward(self, x, kv_state, k_state, q, k, v):\n        b, h, l, d = q.shape\n        assert l == 1, f'q.shape is {q.shape} but should be ({b}, {h}, 1, {d})'\n        in_dt = q.dtype \n        dt = q.dtype #torch.float32\n\n        # Expand dims for broadcasting to compute linear attention\n        q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)\n        kv_state += k[:, :, -1:] * v[:, :, -1:]\n        k_state  += k[:, :, -1:]\n\n        num_attn = (q.to(dt) * kv_state.to(dt)).sum(dim=-1)\n        y = num_attn / ((q.to(dt) * k_state.to(dt)).sum(dim=-1) + self.eps) \n        y = rearrange(y, 'b h l d -> b l (h d)')\n        return self.out_proj(y).to(in_dt)\n\n    \n    def expanded_size(self):\n        return self.feature_dim ** 2 + self.feature_dim + 1\n    \n\n    def allocate_inference_cache(self, batch_size: int, max_seqlen: int, dtype=None, **kwargs):\n        \"\"\"Creates a state tensor of shape ...\"\"\"\n\n        kv_shape = (\n            batch_size, self.num_heads, 1, self.head_dim, self.expanded_size()\n        )\n        k_shape = (\n            batch_size, self.num_heads, 1, 1, self.expanded_size()\n        )\n        \n        kv_state_dec = torch.zeros(*kv_shape, dtype=dtype, device=self.out_proj.weight.device)\n        k_state_dec = torch.zeros(*k_shape, dtype=dtype, device=self.out_proj.weight.device)\n\n        return (kv_state_dec, k_state_dec)\n     \n    \n    def _get_inference_cache(self, inference_params: InferenceParams):\n        return inference_params.key_value_memory_dict[self.layer_idx]\n",
    "description": null,
    "url": null
}