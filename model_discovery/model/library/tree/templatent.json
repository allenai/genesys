{
    "acronym": "templatent",
    "title": "Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning",
    "seed_ids": [
        "brt",
        "lstransformer",
        "transformer"
    ],
    "s2id": "d6a0dfd5f39222d8924b7727a0a49f81fa247d71",
    "abstract": "Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.",
    "authors": [
        "Aniket Didolkar",
        "Kshitij Gupta",
        "Anirudh Goyal",
        "Alex Lamb",
        "Nan Rosemary Ke",
        "Y. Bengio"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": "The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.",
    "citationCount": 14,
    "influentialCitationCount": 3,
    "code": "\"\"\"Model file that houses the architecture of the model.\"\"\"\n\nfrom typing import List, Tuple\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nclass PatchEmbed(layers.Layer):\n    \"\"\"Image to Patch Embedding.\n\n    Args:\n        image_size (`Tuple[int]`): Size of the input image.\n        patch_size (`Tuple[int]`): Size of the patch.\n        embed_dim (`int`): Dimension of the embedding.\n        chunk_size (`int`): Number of patches to be chunked.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: Tuple[int],\n        patch_size: Tuple[int],\n        embed_dim: int,\n        chunk_size: int,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        # Compute the patch resolution.\n        patch_resolution = [\n            image_size[0] // patch_size[0],\n            image_size[1] // patch_size[1],\n        ]\n\n        # Store the parameters.\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.patch_resolution = patch_resolution\n        self.num_patches = patch_resolution[0] * patch_resolution[1]\n\n        # Define the positions of the patches.\n        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n\n        # Create the layers.\n        self.projection = layers.Conv2D(\n            filters=embed_dim,\n            kernel_size=patch_size,\n            strides=patch_size,\n            name=\"projection\",\n        )\n        self.flatten = layers.Reshape(\n            target_shape=(-1, embed_dim),\n            name=\"flatten\",\n        )\n        self.position_embedding = layers.Embedding(\n            input_dim=self.num_patches,\n            output_dim=embed_dim,\n            name=\"position_embedding\",\n        )\n        self.layernorm = keras.layers.LayerNormalization(\n            epsilon=1e-5,\n            name=\"layernorm\",\n        )\n        self.chunking_layer = layers.Reshape(\n            target_shape=(self.num_patches // chunk_size, chunk_size, embed_dim),\n            name=\"chunking_layer\",\n        )\n\n    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, int, int, int]:\n        \"\"\"Call function.\n\n        Args:\n            inputs (`tf.Tensor`): Input tensor.\n\n        Returns:\n            `Tuple[tf.Tensor, int, int, int]`: Tuple of the projected input, number of patches,\n                patch resolution, and embedding dimension.\n        \"\"\"\n        # Project the inputs to the embedding dimension.\n        x = self.projection(inputs)\n\n        # Flatten the pathces and add position embedding.\n        x = self.flatten(x)\n        x = x + self.position_embedding(self.positions)\n\n        # Normalize the embeddings.\n        x = self.layernorm(x)\n\n        # Chunk the tokens.\n        x = self.chunking_layer(x)\n\n        return x\n\n\nclass FeedForwardNetwork(layers.Layer):\n    \"\"\"Feed Forward Network.\n\n    Args:\n        dims (`int`): Number of units in FFN.\n        dropout (`float`): Dropout probability for FFN.\n    \"\"\"\n\n    def __init__(self, dims: int, dropout: float, **kwargs):\n        super().__init__(**kwargs)\n\n        # Create the layers.\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(units=4 * dims, activation=tf.nn.gelu),\n                layers.Dense(units=dims),\n                layers.Dropout(rate=dropout),\n            ],\n            name=\"ffn\",\n        )\n        self.add = layers.Add(\n            name=\"add\",\n        )\n        self.layernorm = layers.LayerNormalization(\n            epsilon=1e-5,\n            name=\"layernorm\",\n        )\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\"Call function.\n\n        Args:\n            inputs (`tf.Tensor`): Input tensor.\n\n        Returns:\n            `tf.Tensor`: Output tensor.\"\"\"\n        # Apply the FFN.\n        x = self.layernorm(inputs)\n        x = self.add([inputs, self.ffn(x)])\n        return x\n\n\nclass BaseAttention(layers.Layer):\n    \"\"\"Base Attention Module.\n\n    Args:\n        num_heads (`int`): Number of attention heads.\n        key_dim (`int`): Size of each attention head for key.\n        dropout (`float`): Dropout probability for attention module.\n    \"\"\"\n\n    def __init__(self, num_heads: int, key_dim: int, dropout: float, **kwargs):\n        super().__init__(**kwargs)\n        self.mha = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=key_dim,\n            dropout=dropout,\n            name=\"mha\",\n        )\n        self.q_layernorm = layers.LayerNormalization(\n            epsilon=1e-5,\n            name=\"q_layernorm\",\n        )\n        self.k_layernorm = layers.LayerNormalization(\n            epsilon=1e-5,\n            name=\"k_layernorm\",\n        )\n        self.v_layernorm = layers.LayerNormalization(\n            epsilon=1e-5,\n            name=\"v_layernorm\",\n        )\n        self.add = layers.Add(\n            name=\"add\",\n        )\n\n        self.attn_scores = None\n\n    def call(\n        self, input_query: tf.Tensor, key: tf.Tensor, value: tf.Tensor\n    ) -> tf.Tensor:\n        \"\"\"Call function.\n\n        Args:\n            input_query (`tf.Tensor`): Input query tensor.\n            key (`tf.Tensor`): Key tensor.\n            value (`tf.Tensor`): Value tensor.\n\n        Returns:\n            `tf.Tensor`: Output tensor.\"\"\"\n        # Apply the attention module.\n        query = self.q_layernorm(input_query)\n        key = self.k_layernorm(key)\n        value = self.v_layernorm(value)\n        (attn_outs, attn_scores) = self.mha(\n            query=query,\n            key=key,\n            value=value,\n            return_attention_scores=True,\n        )\n\n        # Save the attention scores for later visualization.\n        self.attn_scores = attn_scores\n\n        # Add the input to the attention output.\n        x = self.add([input_query, attn_outs])\n        return x\n\n\nclass AttentionWithFFN(layers.Layer):\n    \"\"\"Attention with Feed Forward Network.\n\n    Args:\n        ffn_dims (`int`): Number of units in FFN.\n        ffn_dropout (`float`): Dropout probability for FFN.\n        num_heads (`int`): Number of attention heads.\n        key_dim (`int`): Size of each attention head for key.\n        attn_dropout (`float`): Dropout probability for attention module.\n    \"\"\"\n\n    def __init__(\n        self,\n        ffn_dims: int,\n        ffn_dropout: float,\n        num_heads: int,\n        key_dim: int,\n        attn_dropout: float,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        # Create the layers.\n        self.attention = BaseAttention(\n            num_heads=num_heads,\n            key_dim=key_dim,\n            dropout=attn_dropout,\n            name=\"base_attn\",\n        )\n        self.ffn = FeedForwardNetwork(\n            dims=ffn_dims,\n            dropout=ffn_dropout,\n            name=\"ffn\",\n        )\n\n        self.attn_scores = None\n\n    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor) -> tf.Tensor:\n        \"\"\"Call function.\n\n        Args:\n            query (`tf.Tensor`): Input query tensor.\n            key (`tf.Tensor`): Key tensor.\n            value (`tf.Tensor`): Value tensor.\n\n        Returns:\n            `tf.Tensor`: Output tensor.\n        \"\"\"\n        # Apply the attention module.\n        x = self.attention(query, key, value)\n\n        # Save the attention scores for later visualization.\n        self.attn_scores = self.attention.attn_scores\n\n        # Apply the FFN.\n        x = self.ffn(x)\n        return x\n\n\nclass CustomRecurrentCell(layers.Layer):\n    \"\"\"Custom Recurrent Cell.\n\n    Args:\n        chunk_size (`int`): Number of tokens in a chunk.\n        r (`int`): One Cross Attention per **r** Self Attention.\n        num_layers (`int`): Number of layers.\n        ffn_dims (`int`): Number of units in FFN.\n        ffn_dropout (`float`): Dropout probability for FFN.\n        num_heads (`int`): Number of attention heads.\n        key_dim (`int`): Size of each attention head for key.\n        attn_dropout (`float`): Dropout probability for attention module.\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int,\n        r: int,\n        num_layers: int,\n        ffn_dims: int,\n        ffn_dropout: float,\n        num_heads: int,\n        key_dim: int,\n        attn_dropout: float,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        # Save the arguments.\n        self.chunk_size = chunk_size\n        self.r = r\n        self.num_layers = num_layers\n        self.ffn_dims = ffn_dims\n        self.ffn_droput = ffn_dropout\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.attn_dropout = attn_dropout\n\n        # Create the state_size and output_size. This is important for\n        # custom recurrence logic.\n        self.state_size = tf.TensorShape([chunk_size, ffn_dims])\n        self.output_size = tf.TensorShape([chunk_size, ffn_dims])\n\n        self.get_attn_scores = False\n        self.attn_scores = []\n\n        ########################################################################\n        # Perceptual Module\n        ########################################################################\n        perceptual_module = list()\n        for layer_idx in range(num_layers):\n            perceptual_module.append(\n                AttentionWithFFN(\n                    ffn_dims=ffn_dims,\n                    ffn_dropout=ffn_dropout,\n                    num_heads=num_heads,\n                    key_dim=key_dim,\n                    attn_dropout=attn_dropout,\n                    name=f\"pm_self_attn_{layer_idx}\",\n                )\n            )\n            if layer_idx % r == 0:\n                perceptual_module.append(\n                    AttentionWithFFN(\n                        ffn_dims=ffn_dims,\n                        ffn_dropout=ffn_dropout,\n                        num_heads=num_heads,\n                        key_dim=key_dim,\n                        attn_dropout=attn_dropout,\n                        name=f\"pm_cross_attn_ffn_{layer_idx}\",\n                    )\n                )\n        self.perceptual_module = perceptual_module\n\n        ########################################################################\n        # Temporal Latent Bottleneck Module\n        ########################################################################\n        self.tlb_module = AttentionWithFFN(\n            ffn_dims=ffn_dims,\n            ffn_dropout=ffn_dropout,\n            num_heads=num_heads,\n            key_dim=key_dim,\n            attn_dropout=attn_dropout,\n            name=f\"tlb_cross_attn_ffn\",\n        )\n\n    def call(self, inputs, states) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n        \"\"\"Call function.\n\n        Args:\n            inputs (`tf.Tensor`): Input tensor.\n            states (`List[tf.Tensor]`): List of state tensors.\n\n        Returns:\n            `Tuple[tf.Tensor, List[tf.Tensor]]`: Tuple of output tensor and list\n                of state tensors.\n        \"\"\"\n        # inputs => (batch, chunk_size, dims)\n        # states => [(batch, chunk_size, units)]\n        slow_stream = states[0]\n        fast_stream = inputs\n\n        for layer_idx, layer in enumerate(self.perceptual_module):\n            fast_stream = layer(query=fast_stream, key=fast_stream, value=fast_stream)\n\n            if layer_idx % self.r == 0:\n                fast_stream = layer(\n                    query=fast_stream, key=slow_stream, value=slow_stream\n                )\n\n        slow_stream = self.tlb_module(\n            query=slow_stream, key=fast_stream, value=fast_stream\n        )\n\n        # Save the attention scores for later visualization.\n        if self.get_attn_scores:\n            self.attn_scores.append(self.tlb_module.attn_scores)\n\n        return fast_stream, [slow_stream]\n",
    "description": null,
    "url": null
}