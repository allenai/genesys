{
    "title": "Retentive network: a successor to transformer for large language models",
    "acronym": "retnet",
    "s2id": "240103933ffe3dac2179cc160a2bd91299357a53",
    "abstract": "In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.",
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.",
    "citationCount": 143,
    "influentialCitationCount": 18,
    "seed_ids": [
        "gqa",
        "hyena",
        "flashconv",
        "lex",
        "flashattn",
        "roformer",
        "lineartransformer",
        "transformer"
    ],
    "code": "# gab.py\n\nimport torch\nimport torch.nn as nn\n\nfrom model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\nfrom transformers.activations import ACT2FN\nfrom typing import Dict, List, Optional, Tuple, Union\n\n\n\n##### WIP, TERRIBLE, SUPER SLOW !!! #####\n\n# YOU CAN IMPORT MORE MODULES HERE #\n\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n\n\nfrom torchtune.modules import RMSNorm\n\n\n\nclass RetNetRelPos(nn.Module):\n    def __init__(self,embed_dim, chunk_size, retention_heads, device=None, dtype=None):\n        super().__init__()\n        num_heads = retention_heads\n\n        angle = 1.0 / (\n            10000 ** torch.linspace(0, 1, embed_dim // num_heads // 2, device=device, dtype=dtype)\n        )\n        angle = angle.unsqueeze(-1).repeat(1, 2).flatten()\n        decay = torch.log(\n            1 - 2 ** (-5 - torch.arange(num_heads, dtype=dtype, device=device))\n        )\n        self.register_buffer(\"angle\", angle)\n        self.register_buffer(\"decay\", decay)\n        self.recurrent_chunk_size = chunk_size\n        self.device = device\n        self.dtype = dtype\n\n    def forward(\n        self,\n        slen,\n        forward_impl=\"parallel\",\n        recurrent_chunk_size=None,\n        retention_mask=None,\n        get_decay_scale=True,\n    ):\n        if forward_impl == \"recurrent\":\n            sin = torch.sin(self.angle * (slen - 1))\n            cos = torch.cos(self.angle * (slen - 1))\n            retention_rel_pos = ((sin, cos), self.decay.view(1, -1, 1, 1).exp())\n        elif forward_impl == \"chunkwise\":\n            if recurrent_chunk_size is None:\n                recurrent_chunk_size = self.recurrent_chunk_size\n            index = torch.arange(slen).to(self.decay)\n            sin = torch.sin(index[:, None] * self.angle[None, :])\n            cos = torch.cos(index[:, None] * self.angle[None, :])\n\n            block_index = torch.arange(recurrent_chunk_size).to(self.decay)\n            mask = torch.tril(\n                torch.ones(recurrent_chunk_size, recurrent_chunk_size)\n            ).to(self.decay)\n            mask = torch.masked_fill(\n                block_index[:, None] - block_index[None, :], ~mask.bool(), float(\"inf\")\n            )\n            mask = torch.exp(mask * self.decay[:, None, None])\n            mask = torch.nan_to_num(mask)\n            mask = mask.unsqueeze(0)  # [1, h, t, t]\n            # TODO: need to handle retention_mask\n            # scaling\n            value_inner_decay = mask[:, :, -1] / mask[:, :, -1].sum(\n                dim=-1, keepdim=True\n            )\n            value_inner_decay = value_inner_decay.unsqueeze(-1)\n            scale = mask.sum(dim=-1, keepdim=True).sqrt()\n            inner_mask = mask / scale\n\n            cross_decay = torch.exp(self.decay * recurrent_chunk_size)\n            query_inner_decay = torch.exp(self.decay[:, None] * (block_index + 1))\n            cross_decay = cross_decay[None, :, None, None]\n            query_inner_decay = query_inner_decay[None, :, :, None] / (\n                scale / mask[:, :, -1].sum(dim=-1)[:, :, None, None]\n            )\n            # decay_scale (used for kv cache)\n            if get_decay_scale:\n                decay_scale = self.compute_decay_scale(slen, retention_mask).to(self.device, self.dtype)\n            else:\n                decay_scale = None\n            retention_rel_pos = (\n                (\n                    sin.to(self.device, self.dtype), \n                    cos.to(self.device, self.dtype), \n                ),\n                (\n                    inner_mask.to(self.device, self.dtype), \n                    cross_decay.to(self.device, self.dtype), \n                    query_inner_decay.to(self.device, self.dtype), \n                    value_inner_decay.to(self.device, self.dtype), \n                    decay_scale,\n                ),\n            )\n        else:  # parallel\n            index = torch.arange(slen).to(self.decay)\n            sin = torch.sin(index[:, None] * self.angle[None, :])\n            cos = torch.cos(index[:, None] * self.angle[None, :])\n            mask = torch.tril(torch.ones(slen, slen)).to(self.decay)\n            mask = torch.masked_fill(\n                index[:, None] - index[None, :], ~mask.bool(), float(\"inf\")\n            )\n            mask = torch.exp(mask * self.decay[:, None, None])\n            mask = torch.nan_to_num(mask)\n            mask = mask.unsqueeze(0)  # [1, h, t, t]\n            if retention_mask is not None:\n                # this is required for left padding\n                mask = mask * retention_mask.float().view(-1, 1, 1, slen).to(mask)\n\n            # scaling\n            mask = mask / mask.sum(dim=-1, keepdim=True).sqrt()\n            mask = torch.nan_to_num(mask, nan=0.0)\n            # decay_scale (used for kv cache)\n            if get_decay_scale:\n                decay_scale = self.compute_decay_scale(slen, retention_mask).to(self.device, self.dtype)\n            else:\n                decay_scale = None\n            # mask processing for intra decay\n            if retention_mask is not None:\n                max_non_zero = (\n                    torch.cumsum(retention_mask, dim=-1).max(dim=-1).indices\n                )  # [b,]\n                intra_decay = mask[range(mask.shape[0]), :, max_non_zero]\n            else:\n                intra_decay = mask[:, :, -1]\n\n            retention_rel_pos = (\n                (\n                    sin.to(self.device, self.dtype), \n                    cos.to(self.device, self.dtype), \n                ), (\n                    mask.to(self.device, self.dtype), \n                    intra_decay.to(self.device, self.dtype), \n                    decay_scale, \n                ))\n\n        return retention_rel_pos\n\n    def compute_decay_scale(self, slen, retention_mask=None):\n        exponent = torch.arange(slen, device=self.decay.device).float()\n        decay_scale = self.decay.exp().view(-1, 1) ** exponent.view(1, -1)  # [h, t]\n        if retention_mask is not None:\n            seqlen = retention_mask.sum(dim=-1)  # [b,]\n            bsz = seqlen.size(0)\n            decay_scale = decay_scale.unsqueeze(0).repeat(bsz, 1, 1)  # [b, h, t]\n            for i, pos in enumerate(seqlen):\n                # the formula for decay_scale is `sum(gamma^i) for i in [0, slen).`\n                # Since the retention_mask is 0 for padding, we can set the decay_scale\n                # to 0 for the padding positions.\n                decay_scale[i, :, pos.item() :] = 0\n        else:\n            bsz = 1\n        decay_scale = decay_scale.sum(-1).view(bsz, -1, 1, 1)  # [b, h, 1, 1]\n        return decay_scale\n\n\ndef rotate_every_two(x):\n    x1 = x[:, :, :, ::2]\n    x2 = x[:, :, :, 1::2]\n    x = torch.stack((-x2, x1), dim=-1)\n    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n\ndef theta_shift(x, sin, cos):\n    return (x * cos) + (rotate_every_two(x) * sin)\n\n\ndef split_heads(tensors, bsz, seqlen, num_heads):\n    assert isinstance(tensors, (tuple, list))\n    return [x.view(bsz, seqlen, num_heads, -1).transpose(1, 2) for x in tensors]\n\n\nclass MultiScaleRetention(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        heads,\n        use_bias=False,\n        tensor_parallel=False,\n        device=None,\n        dtype=None,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.value_dim = int(embed_dim*1.5)\n        self.num_heads = heads\n        self.head_dim = self.value_dim // self.num_heads\n        self.key_dim = self.embed_dim // self.num_heads\n        self.scaling = self.key_dim**-0.5\n\n        self.gate_fn = ACT2FN['swish']\n\n        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=use_bias, device=device, dtype=dtype)\n        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=use_bias, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(self.embed_dim, self.value_dim, bias=use_bias, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(self.embed_dim, self.value_dim, bias=use_bias, device=device, dtype=dtype)\n\n        self.out_proj = nn.Linear(self.value_dim, self.embed_dim, bias=use_bias, device=device, dtype=dtype)\n\n        self.group_norm = RMSNorm(\n            self.head_dim, \n        ).to(device=device, dtype=dtype)\n        self.reset_parameters()\n\n        if tensor_parallel:\n            self.decay_proj = nn.Linear(self.num_heads, self.num_heads, bias=False, device=device, dtype=dtype)\n        else:\n            self.decay_proj = None\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.q_proj.weight, gain=2**-2.5)\n        nn.init.xavier_uniform_(self.k_proj.weight, gain=2**-2.5)\n        nn.init.xavier_uniform_(self.v_proj.weight, gain=2**-2.5)\n        nn.init.xavier_uniform_(self.g_proj.weight, gain=2**-2.5)\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=2**-1)\n\n    def parallel_retention(self, q, k, v, decay_mask):\n        \"\"\"\n        q,  # bsz * num_head * len * qk_dim\n        k,  # bsz * num_head * len * qk_dim\n        v,  # bsz * num_head * len * v_dim\n        decay_mask,  # (1 or bsz) * num_head * len * len\n        \"\"\"\n        decay_mask, intra_decay, scale = decay_mask\n        # just return retention_rel_pos projected\n        # TODO: for shardformer\n        if self.decay_proj is not None:\n            decay_mask = self.decay_proj(decay_mask.transpose(-1, -3)).transpose(-3, -1)\n\n        # [b, h, t, t]\n        retention = q @ k.transpose(-1, -2)  # (scaled dot-product)\n        retention = retention * decay_mask\n\n        # invariant after normalization\n        retention = retention / retention.detach().abs().sum(\n            dim=-1, keepdim=True\n        ).clamp(min=1, max=5e4)\n\n        output = retention @ v  # [b, h, t, v_dim / h]\n        output = output.transpose(1, 2)  # [b, t, h, v_dim / h]\n\n        if self.training:  # skip cache\n            return output, None, retention\n\n        if self.decay_proj is not None:\n            intra_decay = self.decay_proj(intra_decay.transpose(-1, -2)).transpose(\n                -2, -1\n            )\n\n        # kv cache: [b, h, t, v_dim, qk_dim]\n        current_kv = k.unsqueeze(-2) * v.unsqueeze(-1)\n        intra_decay = intra_decay[:, :, :, None, None]  # [b, h, t, 1, 1]\n        current_kv = (current_kv * intra_decay).sum(2)  # [b, h, v_dim, qk_dim]\n\n        cache = {\"prev_key_value\": current_kv, \"scale\": scale}\n        return output, cache, retention\n\n    def recurrent_retention(\n        self, q, k, v, decay, past_key_value=None, retention_mask=None\n    ):\n        \"\"\"\n        q, k, v, # bsz * num_head * 1 * qkv_dim\n        past_key_value:\n            - \"prev_key_value\"  # bsz * num_head * v_dim * qk_dim\n            - \"scale\"  # (1 or bsz) * num_head * 1 * 1\n        decay # (1 or bsz) * num_head * 1 * 1\n        retention_mask # bsz * 1\n        \"\"\"\n        if retention_mask is not None:\n            retention_mask = retention_mask.float().view(-1, 1, 1, 1).to(decay)\n        else:\n            retention_mask = torch.ones(k.size(0), 1, 1, 1).to(decay).to(k.device,k.dtype)  \n        # (b, h, v_dim, qk_dim)\n        current_kv = k * v.transpose(-1, -2) * retention_mask\n\n        if past_key_value is not None and \"prev_key_value\" in past_key_value:\n            prev_kv = past_key_value[\"prev_key_value\"]\n            prev_scale = past_key_value[\"scale\"]\n            scale = torch.where(retention_mask == 0, prev_scale, prev_scale * decay + 1)\n            # connect prev_kv and current_kv\n            # how much to decay prev_kv\n            decay_amount = prev_scale.sqrt() * decay / scale.sqrt()\n            decay_amount = torch.where(retention_mask == 0, 1, decay_amount)\n            prev_kv = prev_kv * decay_amount  # decay prev_kv\n            current_kv = current_kv / scale.sqrt()  # scale current_kv\n            current_kv = torch.nan_to_num(\n                current_kv, nan=0.0\n            )  # remove nan, scale might be 0\n\n            current_kv = prev_kv + current_kv\n        else:\n            scale = torch.ones_like(decay).to(k.device,k.dtype)\n            # when retention_mask is 0 at the beginning, setting scale to 1 will\n            # make the first retention to use the padding incorrectly. Hence,\n            # setting it to 0 here. This is a little ugly, so we might want to\n            # change this later. TODO: improve\n            scale = torch.where(retention_mask == 0, torch.zeros_like(decay), scale)\n\n        output = torch.sum(q * current_kv, dim=3).unsqueeze(1)  # (b, 1, h, d_v)\n\n        cache = {\"prev_key_value\": current_kv, \"scale\": scale}\n        return output, cache\n\n    def chunkwise_retention(self, q, k, v, decay_mask):\n        \"\"\"\n        q, k, v,  # bsz * num_head * seqlen * qkv_dim\n        past_key_value:\n            - \"prev_key_value\"  # bsz * num_head * v_dim * qk_dim\n            - \"scale\"  # (1 or bsz) * num_head * 1 * 1\n        decay_mask,  # 1 * num_head * chunk_size * chunk_size\n        cross_decay,  # 1 * num_head * 1 * 1\n        inner_decay,  # 1 * num_head * chunk_size * 1\n        \"\"\"\n        # TODO: not working properly\n        (\n            decay_mask,\n            cross_decay,\n            query_inner_decay,\n            value_inner_decay,\n            decay_scale,\n        ) = decay_mask\n        bsz, _, tgt_len, _ = v.size()\n        chunk_len = decay_mask.size(-1)\n        assert tgt_len % chunk_len == 0\n        num_chunks = tgt_len // chunk_len\n\n        # [b, n_c, h, t_c, qkv_dim]\n        q = q.view(bsz, self.num_heads, num_chunks, chunk_len, self.key_dim).transpose(\n            1, 2\n        )\n        k = k.view(bsz, self.num_heads, num_chunks, chunk_len, self.key_dim).transpose(\n            1, 2\n        )\n        v = v.view(bsz, self.num_heads, num_chunks, chunk_len, self.head_dim).transpose(\n            1, 2\n        )\n\n        k_t = k.transpose(-1, -2)\n\n        qk_mat = q @ k_t  # [b, n_c, h, t_c, t_c]\n        qk_mat = qk_mat * decay_mask.unsqueeze(1)\n        inner_scale = qk_mat.detach().abs().sum(dim=-1, keepdim=True).clamp(min=1)\n        qk_mat = qk_mat / inner_scale\n        # [b, n_c, h, t_c, v_dim]\n        inner_output = torch.matmul(qk_mat, v)\n\n        # reduce kv in one chunk\n        # [b, n_c, h, qk_dim, v_dim]\n        kv = k_t @ (v * value_inner_decay)\n        # kv = kv.view(bsz, num_chunks, self.num_heads, self.key_dim, self.head_dim)\n\n        kv_recurrent = []\n        cross_scale = []\n        kv_state = torch.zeros(bsz, self.num_heads, self.key_dim, self.head_dim).to(v)\n        kv_scale = torch.ones(bsz, self.num_heads, 1, 1).to(v)\n\n        # accumulate kv by loop\n        for i in range(num_chunks):\n            kv_recurrent.append(kv_state / kv_scale)\n            cross_scale.append(kv_scale)\n            kv_state = kv_state * cross_decay + kv[:, i]\n            kv_scale = (\n                kv_state.detach()\n                .abs()\n                .sum(dim=-2, keepdim=True)\n                .max(dim=-1, keepdim=True)\n                .values.clamp(min=1)\n            )\n\n        kv_recurrent = torch.stack(kv_recurrent, dim=1)\n        cross_scale = torch.stack(cross_scale, dim=1)\n\n        all_scale = torch.maximum(inner_scale, cross_scale)\n        align_inner_scale = all_scale / inner_scale\n        align_cross_scale = all_scale / cross_scale\n\n        cross_output = (q * query_inner_decay.unsqueeze(1)) @ kv_recurrent\n        output = inner_output / align_inner_scale + cross_output / align_cross_scale\n        output = output.transpose(2, 3)  # [b, n_c, t_c, h, v_dim]\n\n        cache = {\"prev_key_value\": kv_state.transpose(-2, -1), \"scale\": decay_scale}\n        return output, cache\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        rel_pos: Tuple[Tuple[torch.Tensor]],\n        retention_mask: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        forward_impl: str = \"parallel\",\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, Optional[torch.FloatTensor]]:\n        B, T, H = hidden_states.size()\n        (sin, cos), decay_mask = rel_pos\n        # projections\n        q = self.q_proj(hidden_states)\n        k = self.k_proj(hidden_states)\n        v = self.v_proj(hidden_states)\n        g = self.g_proj(hidden_states)\n        # multi-head\n        q, k, v = split_heads((q, k, v), B, T, self.num_heads)\n        k *= self.scaling  # for scaled dot product\n        # rotate\n        # NOTE: theta_shift has bug with mps device.\n        qr = theta_shift(q, sin, cos)\n        kr = theta_shift(k, sin, cos)\n\n        # retention\n        if forward_impl == \"parallel\":\n            retention_out, curr_kv, retention_weights = self.parallel_retention(\n                qr, kr, v, decay_mask\n            )\n        elif forward_impl == \"recurrent\":\n            retention_out, curr_kv = self.recurrent_retention(\n                qr,\n                kr,\n                v,\n                decay_mask,\n                past_key_value=past_key_value,\n                retention_mask=retention_mask,\n            )\n        elif forward_impl == \"chunkwise\":\n            retention_out, curr_kv = self.chunkwise_retention(qr, kr, v, decay_mask)\n        else:\n            raise ValueError(f\"forward_impl {forward_impl} not supported.\")\n\n        # concaat heads\n        normed = self.group_norm(retention_out).reshape(B, T, self.value_dim)\n        # out gate & proj\n        out = self.gate_fn(g) * normed\n        out = self.out_proj(out)\n\n        outputs = (out, curr_kv)\n        return outputs\n    \n\nclass GAB(GABBase):\n    \"\"\"Generalized Autoregressive Block\n        Input:        X: (batch, seqlen, embed_dim)\n        Output:       Y: (batch, seqlen, embed_dim)\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n    \"\"\"\n    def __init__(self,embed_dim: int, device=None,dtype=None,heads=3,chunk_size=64,mode='chunkwise',**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n        # argv: list of hyperparameters\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\n        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n        \n        # COMPLETING THE CODE HERE #\n        self.embed_dim = embed_dim\n        self.chunk_size=chunk_size\n        self.mode=mode\n        self.heads = heads\n        ffn_size = 4 * embed_dim\n        self.retnet_rel_pos = RetNetRelPos(embed_dim, chunk_size=chunk_size, retention_heads=heads, device=device, dtype=dtype)\n\n        self.retention = MultiScaleRetention(embed_dim, heads, device=device,dtype=dtype)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, ffn_size),\n            nn.GELU(),\n            nn.Linear(ffn_size, embed_dim)\n        ).to(device=device,dtype=dtype)\n        self.layer_norm_1 = nn.LayerNorm(embed_dim).to(device=device,dtype=dtype)\n        self.layer_norm_2 = nn.LayerNorm(embed_dim).to(device=device,dtype=dtype)\n\n\n    # YOU CAN ADD MORE FUNCTIONS HERE #\n    \n\n    def _forward(self,X,**kwargs): # type hints are optional but recommended\n\n        # THE CODE HERE MUST BE COMPLETED #\n        slen=X.shape[1]\n        retention_rel_pos = self.retnet_rel_pos(\n            slen,\n            get_decay_scale=not self.training,\n            forward_impl=self.mode,\n        )\n        X = self.layer_norm_1(X)\n        Y,_ = self.retention(\n            X,\n            retention_rel_pos,\n            forward_impl=self.mode,\n        )\n        Y=Y+X\n        # Y = self.retention(self.layer_norm_1(X)) + X\n        X = self.ffn(self.layer_norm_2(Y)) + Y\n        return X\n    \n\"\"\" The dictionary of hyperparameters for constructing a GAB layer\n    embed_dim, device, dtype should NOT be included in gab_config\n\"\"\"\ngab_config = {\n    # THE HYPERPARAMETERS OF ADDITIONAL ARGUMENTS IN GAB CLASS #\n    'heads': 8,\n    'chunk_size': 64,\n    'mode': 'recurrent'\n}\n"
}