{
    "acronym": "recurattnnet",
    "title": "Recurrent Attention Networks for Long-text Modeling",
    "seed_ids": [
        "brt",
        "roformer",
        "longformer",
        "transformer",
        "bert"
    ],
    "s2id": "b1fc02780a21bc0f61d8d8e70a8e8f6987c06bb0",
    "abstract": "Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.",
    "authors": [
        "Xianming Li",
        "Zongxi Li",
        "Xiaotian Luo",
        "Haoran Xie",
        "Xing Lee",
        "Yingbin Zhao",
        "Fu Lee Wang",
        "Qing Li"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": "A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.",
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": "# -*- coding: utf-8 -*-\n\n\"\"\" Implementation of Recurrent Attention Network\n\"\"\"\n\nfrom typing import Dict, Optional, Tuple, List, Callable, Union\n\nimport tensorflow as tf\nfrom langml import keras, L, K\nfrom langml.layers import SineCosinePositionEmbedding, LayerNorm\nfrom langml.tensor_typing import Tensors, Initializer, Activation\n\nfrom rannet.utils import triangular_causal_mask, prefix_causal_mask, standard_normalize\nfrom rannet.attention import SelfAttention, apply_rotary_position_embeddings\n\n\ndef align(tensor: Tensors, axes: int, ndim: Optional[int] = None) -> Tensors:\n    assert len(axes) == K.ndim(tensor)\n    assert ndim or min(axes) >= 0\n    ndim = ndim or max(axes) + 1\n    indices = [None] * ndim\n    for i in axes:\n        indices[i] = slice(None)\n    return tensor[indices]\n\n\ndef sequence_masking(x: Tensors,\n                     mask: Optional[Tensors] = None,\n                     value: Union[str, float] = '-inf',\n                     axis: Optional[int] = None) -> Tensors:\n    \"\"\"mask sequence\n    Args:\n        x: input tensor\n        mask: mask of input tensor\n    \"\"\"\n    if mask is None:\n        return x\n    if isinstance(value, str):\n        assert value in ['-inf', 'inf'], 'if value is a str, please choose it from [`-inf`, `inf`]'\n    x_dtype = K.dtype(x)\n    if x_dtype == 'bool':\n        x = K.cast(x, 'int32')\n    if K.dtype(mask) != K.dtype(x):\n        mask = K.cast(mask, K.dtype(x))\n    if value == '-inf':\n        value = -1e12\n    elif value == 'inf':\n        value = 1e12\n    if axis is None:\n        axis = 1\n    elif axis < 0:\n        axis = K.ndim(x) + axis\n    assert axis > 0, 'axis must be greater than 0'\n    mask = align(mask, [0, axis], K.ndim(x))\n    value = K.cast(value, K.dtype(x))\n    x = x * mask + value * (1 - mask)\n    if x_dtype == 'bool':\n        x = K.cast(x, 'bool')\n    return x\n\n\n\nclass GatedLinearUnit(L.Layer):\n    def __init__(self,\n                 units: int,\n                 kernel_initializer: Initializer = 'glorot_normal',\n                 **kwargs):\n        super(GatedLinearUnit, self).__init__(**kwargs)\n        self.units = units\n        self.kernel_initializer = kernel_initializer\n        self.supports_masking = True\n\n    def get_config(self) -> dict:\n        config = {\n            \"units\": self.units,\n            \"kernel_initializer\": keras.initializers.serialize(self.kernel_initializer),\n        }\n        base_config = super(GatedLinearUnit, self).get_config()\n\n        return dict(base_config, **config)\n\n    def build(self, input_shape: Tensors):\n        super(GatedLinearUnit, self).build(input_shape)\n        self.linear = L.Dense(self.units, kernel_initializer=self.kernel_initializer, name='dense-t')\n        self.sigmoid = L.Dense(self.units, activation='sigmoid',\n                               kernel_initializer=self.kernel_initializer, name='dense-g')\n\n    def call(self, inputs: Tensors, mask: Optional[Tensors] = None):\n        return self.linear(inputs) * self.sigmoid(inputs)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def compute_mask(self, inputs: Tensors, mask: Optional[Tensors] = None):\n        if isinstance(mask, list):\n            mask = mask[0]\n        return mask\n\n    @staticmethod\n    def get_custom_objects() -> dict:\n        return {'GatedLinearUnit': GatedLinearUnit}\n\n\nclass PosMultiHeadAttention(L.Layer):\n    def __init__(\n        self,\n        head_num,\n        head_size,\n        use_bias=True,\n        dropout_rate=None,\n        return_attention=False,\n        activation=None,\n        kernel_initializer='glorot_uniform',\n        apply_relative_position=True,\n        is_residual=True,\n        **kwargs\n    ):\n        super(PosMultiHeadAttention, self).__init__(**kwargs)\n        self.head_num = head_num\n        self.head_size = head_size\n        self.use_bias = use_bias\n        self.dropout_rate = dropout_rate\n        self.return_attention = return_attention\n        self.activation = keras.activations.get(activation)\n        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n        self.apply_relative_position = apply_relative_position\n        self.is_residual = is_residual\n\n    def get_config(self):\n        config = {\n            'head_num': self.head_num,\n            'head_size': self.head_size,\n            'use_bias': self.use_bias,\n            'dropout_rate': self.dropout_rate,\n            'return_attention': self.return_attention,\n            'activation': keras.activations.serialize(self.activation),\n            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n            'apply_relative_position': self.apply_relative_position,\n            'is_residual': self.is_residual,\n        }\n        base_config = super(PosMultiHeadAttention, self).get_config()\n\n        return dict(base_config, **config)\n\n    def build(self, input_shape):\n        super(PosMultiHeadAttention, self).build(input_shape)\n\n        if not isinstance(input_shape, list):\n            input_shape = [input_shape]\n        feature_dim = int(input_shape[0][-1])\n        self.q_dense = L.Dense(\n            units=self.head_size * self.head_num,\n            activation=self.activation,\n            use_bias=self.use_bias,\n            kernel_initializer=self.kernel_initializer,\n            name='dense-q',\n        )\n        self.k_dense = L.Dense(\n            units=self.head_size * self.head_num,\n            activation=self.activation,\n            use_bias=self.use_bias,\n            kernel_initializer=self.kernel_initializer,\n            name='dense-k',\n        )\n        self.v_dense = L.Dense(\n            units=self.head_size * self.head_num,\n            activation=self.activation,\n            use_bias=self.use_bias,\n            kernel_initializer=self.kernel_initializer,\n            name='dense-v',\n        )\n        self.o_dense = L.Dense(\n            units=feature_dim,\n            activation=self.activation,\n            use_bias=self.use_bias,\n            kernel_initializer=self.kernel_initializer,\n            name='dense-o',\n        )\n        if self.is_residual:\n            self.glu = GatedLinearUnit(\n                feature_dim, kernel_initializer=self.kernel_initializer, name='glu')\n            self.layernorm = LayerNorm(name='layernorm')\n\n    def call(self, inputs, mask=None, attn_bias=None, **kwargs):\n        if isinstance(inputs, list):\n            q, k, v = inputs\n        else:\n            q = k = v = inputs\n        if isinstance(mask, list):\n            q_mask, k_mask, v_mask = mask\n        else:\n            q_mask = k_mask = v_mask = mask\n\n        qw = self.q_dense(q)\n        kw = self.k_dense(k)\n        vw = self.v_dense(v)\n\n        batch_size = K.shape(qw)[0]\n        qw = K.reshape(qw, (batch_size, -1, self.head_num, self.head_size))\n        kw = K.reshape(kw, (batch_size, -1, self.head_num, self.head_size))\n        vw = K.reshape(vw, (batch_size, -1, self.head_num, self.head_size))\n\n        o, attn = self.compute_attention(\n            [qw, kw, vw],\n            mask=[q_mask, k_mask, v_mask],\n            attn_bias=attn_bias,\n        )\n\n        shape, int_shape = K.shape(o), K.int_shape(o)\n        o = K.reshape(o, (shape[0], shape[1], int_shape[2] * int_shape[3]))  # (B, L, N * ND)\n        o = self.o_dense(o)\n        if self.is_residual:\n            # residual with raw input (q)\n            if self.dropout_rate > 0:\n                o = L.Dropout(self.dropout_rate)(o)\n            o = o + self.glu(q)\n            o = self.layernorm(o)\n        if self.return_attention:\n            return [o, attn]\n        return o\n\n    def compute_attention(self, inputs, mask=None, attn_bias=None):\n        qw, kw, vw = inputs\n        if mask is not None:\n            _, _, v_mask = mask\n        else:\n            v_mask = None\n        # apply\n        if self.apply_relative_position:\n            pos = SineCosinePositionEmbedding(\"zero\", output_dim=self.head_size)(kw)\n            qw, kw = apply_rotary_position_embeddings(pos, qw, kw)\n        # Attention\n        e = tf.einsum('bjhd,bkhd->bhjk', qw, kw)\n        e /= self.head_size**0.5\n        if attn_bias is not None:\n            if K.ndim(attn_bias) == 3:\n                attn_bias = align(attn_bias, [0, -2, -1], K.ndim(e))\n            e += attn_bias\n        e = sequence_masking(e, v_mask, '-inf', -1)\n        attn = K.softmax(e)\n        if self.dropout_rate:\n            attn = L.Dropout(self.dropout_rate)(attn)\n        o = tf.einsum('bhjk,bkhd->bjhd', attn, vw)\n        return o, attn\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            q_shape, _, v_shape = input_shape\n        else:\n            q_shape = _ = v_shape = input_shape\n        output_shape = q_shape[:-1] + (v_shape[-1],)\n        if self.return_attention:\n            attention_shape = (q_shape[0], self.head_num, q_shape[1], v_shape[1])\n            return [output_shape, attention_shape]\n        return output_shape\n\n    def compute_mask(self, inputs, mask=None):\n        if isinstance(mask, list):\n            mask = mask[0]\n        if self.return_attention:\n            return [mask, None]\n        return mask\n\n    @staticmethod\n    def get_custom_objects() -> dict:\n        return {'PosMultiHeadAttention': PosMultiHeadAttention}\n\n\ndef ran(inputs: Tensors,\n        encode_attn: PosMultiHeadAttention,\n        cell: Optional[Tensors] = None,\n        segments: Optional[Tensors] = None,\n        cell_initializer: Optional[Callable] = None,\n        cell_glu: Optional[Callable] = None,\n        cell_residual_layernorm: Optional[Callable] = None,\n        mask: Optional[Tensors] = None,\n        apply_lm_mask: bool = False,\n        apply_seq2seq_mask: bool = False,\n        window_size: int = 128,\n        concat_layernorm=None,\n        memory_review=None,\n        dropout_rate: float = 0.0,\n        min_window_size: int = 16,\n        cell_pooling: str = 'last'):\n    \"\"\" Core implementation\n    \"\"\"\n\n    def do_step(cell, current_input, mask=None, segment=None):\n        current_input = K.concatenate((cell, current_input), axis=1)  # (B, 1 + W, D)\n        # avoid gradient explosion\n        # x' = \\gamma * \\frac{x - \\mu}{\\sigma} + \\beta\n        current_input = concat_layernorm(current_input)\n        if dropout_rate > 0:\n            current_input = L.Dropout(dropout_rate)(current_input)\n\n        attn_mask = None\n        if mask is not None:\n            cell_mask = K.ones_like(mask[:, :1])\n            attn_mask = K.concatenate((cell_mask, mask), axis=1)\n\n        attn_bias = None\n        if apply_seq2seq_mask:\n            # apply prefix causal mask (unilm mask)\n            cell_segment = K.zeros_like(segment[:, :1])\n            segment = K.concatenate((cell_segment, segment), axis=1)\n            attn_bias = prefix_causal_mask(segment)\n        elif apply_lm_mask:\n            # apply causal mask\n            attn_bias = triangular_causal_mask(K.shape(current_input)[1])\n\n        # encoder\n        output = encode_attn(current_input, mask=attn_mask, attn_bias=attn_bias)\n        cell, output = output[:, 0], output[:, 1:]\n        cell = K.expand_dims(cell, axis=1)  # (B, 1, D)\n\n        # normalize output and cell seperately\n        output = standard_normalize(output)\n        if dropout_rate > 0:\n            output = L.Dropout(dropout_rate)(output)\n\n        cell = standard_normalize(cell)\n        if dropout_rate > 0:\n            cell = L.Dropout(dropout_rate)(cell)\n\n        return cell, output\n\n    def do_first_step(counter, ranges, range_size, cell=None, mask=None):\n        ind = tf.cond(\n            range_size > 0,\n            true_fn=lambda: K.gather(ranges, counter),  # time_steps > window_size\n            false_fn=lambda: K.arange(\n                0,\n                K.shape(inputs)[1] + 1, K.shape(inputs)[1],\n                dtype='int32'),  # [0, K.shape(inputs)[1]], time_steps < window_size\n        )\n\n        # init cell\n        if cell is None:\n            # initialize cell with zeros\n            cell = K.zeros_like(inputs[:, 0])\n            cell = K.expand_dims(cell, axis=1)\n        else:\n            if K.ndim(cell) == 2:\n                cell = K.expand_dims(cell, axis=1)\n\n        cell_t = cell_initializer(cell)\n        if dropout_rate > 0:\n            cell_t = L.Dropout(dropout_rate)(cell_t)\n        cell = tf.cond(\n            tf.equal(tf.math.count_nonzero(cell), 0),\n            lambda: cell_t,  # apply dense layer\n            lambda: cell\n        )\n\n        # slice window input\n        current_input = inputs[:, ind[0]: ind[1]]  # (B, W, D)\n        current_mask = mask[:, ind[0]: ind[1]] if mask is not None else None\n        current_segment = segments[:, ind[0]: ind[1]] if segments is not None else None\n\n        # encode\n        cell, output = do_step(cell, current_input, mask=current_mask, segment=current_segment)\n\n        cells = cell\n        prev_cell = cell\n        return counter + 1, cell, prev_cell, output, cells\n\n    def do_window_step(counter, ranges, range_size, cell, prev_cell=None, outputs=None, cells=None, mask=None):\n        ind = K.gather(ranges, counter)\n        current_input = inputs[:, ind[0]: ind[1]]  # (B, W, D)\n        current_mask = mask[:, ind[0]: ind[1]] if mask is not None else None\n        current_segment = segments[:, ind[0]: ind[1]] if segments is not None else None\n\n        cell, current_output = do_step(cell, current_input, mask=current_mask, segment=current_segment)\n        outputs = K.concatenate((outputs, current_output), axis=1)\n        cells = K.concatenate((cells, cell), axis=1)\n\n        # cell with residual\n        cell_t = cell\n        # cell += prev_cell\n        cell = cell + cell_glu(prev_cell)\n        cell = cell_residual_layernorm(cell)\n        if dropout_rate > 0:\n            cell = L.Dropout(dropout_rate)(cell)\n        prev_cell = cell_t\n        return counter + 1, ranges, range_size, cell, prev_cell, outputs, cells, mask\n\n    def do_final_step(last_ind, cell, prev_cell, outputs, cells, mask=None):\n        current_input = inputs[:, last_ind[1]:]\n        current_mask = mask[:, last_ind[1]:]\n        current_segment = segments[:, last_ind[1]:] if segments is not None else None\n        cell, current_output = do_step(cell, current_input, mask=current_mask, segment=current_segment)\n        outputs = K.concatenate((outputs, current_output), axis=1)\n        cells = K.concatenate((cells, cell), axis=1)\n\n        # cell with residual\n        # cell += prev_cell\n        cell = cell + cell_glu(prev_cell)\n        cell = cell_residual_layernorm(cell)\n        return cell, outputs, cells\n\n    def rearrange_ranges(ranges, range_size, seq_len):\n        # if ranges is empty, ranges[-1] will raise error\n        last_range = K.gather(ranges, range_size - 1)\n        start = last_range[0]\n        end = last_range[1]\n        final_len = seq_len - end\n        ranges, flag = tf.cond(\n            tf.logical_and(end > start, final_len < min_window_size),\n            true_fn=lambda: (\n                K.concatenate((ranges[:-1], K.reshape(K.stack((start, seq_len + 1)), (1, -1))), axis=0),\n                K.constant(True, dtype='bool')),\n            false_fn=lambda: (ranges, K.constant(False, dtype='bool'))\n        )\n        # new range size\n        range_size = K.shape(ranges)[0]\n        return ranges, range_size, flag\n\n    seq_len = K.shape(inputs)[1]\n    ranges = K.arange(0, seq_len, window_size)\n    starts = K.expand_dims(ranges[0:-1:], axis=1)\n    ends = K.expand_dims(ranges[1::], axis=1)\n    ranges = K.concatenate([starts, ends], axis=-1)\n    range_size = K.shape(ranges)[0]\n\n    # rearrange ranges to ensure the step size of the final-step is greater than min_window_size\n    rearrange_flag = K.constant(False, dtype='bool')\n    ranges, range_size, rearrange_flag = tf.cond(\n        range_size > 0,\n        true_fn=lambda: rearrange_ranges(ranges, range_size, seq_len),\n        false_fn=lambda: (ranges, range_size, rearrange_flag)\n    )\n\n    counter = K.constant(0, dtype='int32', name=\"counter\")\n    # first step\n    counter, cell, prev_cell, outputs, cells = do_first_step(counter, ranges, range_size, cell=cell, mask=mask)\n    # window loop\n    loop_outputs = tf.while_loop(\n        cond=lambda counter, *_: counter < range_size,\n        body=do_window_step,\n        loop_vars=[counter, ranges, range_size, cell, prev_cell, outputs, cells, mask],\n        shape_invariants=[\n            counter.get_shape(), ranges.get_shape(), range_size.get_shape(), cell.get_shape(), prev_cell.get_shape(),\n            outputs.get_shape(), tf.TensorShape([None, None, K.int_shape(inputs)[-1]]), mask.get_shape()],\n        name='window-loop',\n    )\n    cell, prev_cell, outputs, cells = loop_outputs[3], loop_outputs[4], loop_outputs[5], loop_outputs[6]\n    # final step\n    cell, outputs, cells = tf.cond(\n        tf.logical_and(range_size > 0, tf.logical_not(rearrange_flag)),\n        true_fn=lambda: do_final_step(K.gather(ranges, range_size - 1), cell, prev_cell, outputs, cells, mask=mask),\n        false_fn=lambda: (cell, outputs, cells),\n        name='final-step'\n    )\n\n    if isinstance(memory_review, SelfAttention):\n        outputs = memory_review([outputs, cells, cells], mask=mask)\n    else:\n        outputs = memory_review(outputs)\n    last_cell = K.squeeze(cell, axis=1)  # (B, D)\n    if cell_pooling == 'last':\n        cell = last_cell\n    elif cell_pooling == 'mean':\n        cell = K.mean(cells, axis=1)\n    elif cell_pooling == 'max':\n        cell = K.max(cells, axis=1)\n    else:\n        raise ValueError('Please specify `cell_pooling` from [`last`, `mean`, `max`]')\n    return last_cell, outputs, cell\n\n",
    "description": null,
    "url": null
}