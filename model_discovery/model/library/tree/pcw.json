{
    "acronym": "pcw",
    "title": "Parallel Context Windows for Large Language Models",
    "seed_ids": [
        "stm",
        "roformer",
        "gpt2"
    ],
    "s2id": "980e55d9226cac302d0fae7732da4e67b8bc952c",
    "abstract": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\u201cwindows\u201d), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows.",
    "authors": [
        "Nir Ratner",
        "Yoav Levine",
        "Yonatan Belinkov",
        "Ori Ram",
        "Inbal Magar",
        "Omri Abend",
        "Ehud D. Karpas",
        "A. Shashua",
        "Kevin Leyton-Brown",
        "Y. Shoham"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": "This work presents Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training, to carve a long context into chunks (\u201cwindows\u201d), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows.",
    "citationCount": 41,
    "influentialCitationCount": 6,
    "code": "from typing import List, Tuple, Optional, Dict\n\nimport numpy as np\nimport torch\nfrom transformers import PreTrainedTokenizerBase, PreTrainedModel\n\nfrom utils import n_tokens_in_prompt\n\n\nimport numpy as np\nimport torch\nfrom numpy import typing as npt\nfrom transformers import LogitsProcessor\n\nLOGIT_BIAS = 100\n\n\nclass RestrictiveTokensLogitsProcessor(LogitsProcessor):\n    \"\"\" Restrictive decoding is done by adding logits_bias to the relevant tokens. Based on:\n    https://help.openai.com/en/articles/5247780-using-logit-bias-to-define-token-probability\n    \"\"\"\n\n    def __init__(self,\n                 restrictive_token_ids: npt.NDArray[int],\n                 eos_token_id: int,\n                 prompt_length_to_skip: int = 0,\n                 logits_bias: int = LOGIT_BIAS):\n        self.restrictive_token_ids = restrictive_token_ids\n        self.eos_token_id = eos_token_id\n        self.logits_bias = logits_bias\n        self.prompt_length_to_skip = prompt_length_to_skip\n        self.mask = np.ones(restrictive_token_ids.shape[0], dtype=bool)\n\n        self._preprocess_restrictive_array()\n\n    def _preprocess_restrictive_array(self):\n        # extend restrictive_token_ids to include eos as last token for each sequence\n        if not (self.restrictive_token_ids[:, -1] == self.eos_token_id).all():\n            self.restrictive_token_ids = np.column_stack(\n                (self.restrictive_token_ids, np.ones(self.restrictive_token_ids.shape[0]) * self.eos_token_id)). \\\n                astype(int)\n\n    def update_new_prompt_length_to_skip(self, prompt_length_to_skip: int):\n        self.prompt_length_to_skip = prompt_length_to_skip\n        self.mask = np.ones(self.restrictive_token_ids.shape[0], dtype=bool)\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        assert input_ids.shape[0] == 1, \"This implementation doesn't support batching\"\n        new_tokens_length = input_ids.shape[-1] - self.prompt_length_to_skip\n        if new_tokens_length > 0:\n            self.mask = self.mask & (self.restrictive_token_ids[:, new_tokens_length - 1] == input_ids[\n                0, -1].item())\n        scores[:, self.restrictive_token_ids[self.mask, new_tokens_length]] += self.logits_bias\n        return scores\n\n\ndef combine_past_key_values(past_lst: List[Tuple[Tuple[torch.Tensor]]], longest_window_id: int) -> \\\n        Tuple[Tuple[torch.Tensor, torch.Tensor]]:\n    # We eliminate all but one bos token from windows to avoid multiple bos, which deterred our results.\n    n_layers = len(past_lst[0])\n    longest_window = past_lst[longest_window_id]\n    all_windows_except_longest = past_lst[:longest_window_id] + past_lst[longest_window_id + 1:]\n    return tuple(\n        (torch.cat([longest_window[i][0]] + [c[i][0][:, :, 1:, :] for c in all_windows_except_longest], dim=2),\n         torch.cat([longest_window[i][1]] + [c[i][1][:, :, 1:, :] for c in all_windows_except_longest], dim=2))\n        for i in range(n_layers))\n\n\ndef generate_pcw_position_ids(attention_mask: torch.Tensor, max_window_size: int,\n                              past_key_values: Tuple[Tuple[torch.Tensor]],\n                              sum_windows_size: int, windows_key_values: Tuple[Tuple[torch.Tensor]]) -> torch.Tensor:\n    position_ids = attention_mask.long().cumsum(-1) - 1\n    n_task_tokens = position_ids.shape[1] - sum_windows_size\n    position_ids[0, -n_task_tokens:] = torch.arange(max_window_size, max_window_size + n_task_tokens, 1)\n    position_ids.masked_fill_(attention_mask == 0, 1)\n    if past_key_values:  # i.e., first token is already generated\n        position_ids = position_ids[:, -1].unsqueeze(-1)\n    elif windows_key_values:  # i.e., we are in the first token generation\n        position_ids = position_ids[:, sum_windows_size:]\n    return position_ids\n\n\nclass PCWModelWrapper:\n    def __init__(self,\n                 model: PreTrainedModel,\n                 tokenizer: PreTrainedTokenizerBase,\n                 device: str,\n                 context_window_size: int,\n                 right_indentation: bool = False\n                 ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.context_window_size = context_window_size\n        self.device = device\n        # Left indentation is the default behavior as explained in the paper.\n        self.right_indentation = right_indentation\n\n    def _get_windows(self, texts: List[str]) -> List[Dict]:\n        windows = []\n        if self.right_indentation:\n            max_window_size = max(n_tokens_in_prompt(self.tokenizer, t, add_special_tokens=True) for t in texts)\n\n        for text in texts:\n            encoded_input_window = self.tokenizer(text, return_tensors='pt').to(self.device)\n            window_size = encoded_input_window['input_ids'].shape[1]\n            if self.right_indentation:\n                shift = max_window_size - window_size\n                encoded_input_window[\"position_ids\"] = encoded_input_window[\"attention_mask\"].cumsum(-1) - 1 + shift\n            with torch.no_grad():\n                output = self.model(**encoded_input_window)\n            windows.append({'text': text,\n                            'encoded_input': encoded_input_window,\n                            'attention_mask': encoded_input_window['attention_mask'],\n                            'window_size': window_size,\n                            'output': output,\n                            'past': output['past_key_values']})\n        return windows\n\n    def get_contexts_cache(self, contexts: List[str]) -> Dict:\n        windows = self._get_windows(contexts)\n        windows_sizes = [window['window_size'] for window in windows]\n        j = np.argmax(windows_sizes)\n        # Windows contain bos tokens, we remove all but one to avoid multiple bos\n        return {'past_key_values': combine_past_key_values([window['past'] for window in windows], j),\n                'max_window_size': max(windows_sizes),\n                'past_attention_mask': torch.cat(\n                    [windows[j]['attention_mask']] + [window['attention_mask'][:, 1:] for window in\n                                                      windows[:j] + windows[j + 1:]], dim=1),\n                'sum_windows_size': sum(windows_sizes) - (len(windows) - 1)}\n\n    def pcw_generate(self,\n                     contexts: Optional[List[str]] = None,\n                     task_text: Optional[str] = None,\n                     contexts_cache: Optional[Dict] = None,\n                     restrictive_logit_preprocessor: Optional[RestrictiveTokensLogitsProcessor] = None,\n                     **kwargs\n                     ) -> str:\n        \"\"\"Note: Batching is not supported by PCW at the moment. \"\"\"\n        assert (contexts is None) != (\n                contexts_cache is None), \"pcw_generate should work with contexts or cache, not with both!\"\n        cache = contexts_cache or self.get_contexts_cache(contexts)\n        encoded_task_text = self.tokenizer(task_text, add_special_tokens=False, return_tensors='pt').to(self.device)\n        if restrictive_logit_preprocessor:\n            restrictive_logit_preprocessor.update_new_prompt_length_to_skip(encoded_task_text['input_ids'].shape[1])\n            kwargs['logits_processor'] = [restrictive_logit_preprocessor]\n        combined_attention_mask = torch.cat((cache['past_attention_mask'], encoded_task_text['attention_mask']),\n                                            dim=1).to(self.device)\n        with torch.no_grad():\n            res = self.model.generate(input_ids=encoded_task_text['input_ids'],\n                                      attention_mask=combined_attention_mask,\n                                      windows_key_values=cache['past_key_values'],\n                                      max_window_size=cache['max_window_size'],\n                                      sum_windows_size=cache['sum_windows_size'],\n                                      pad_token_id=self.tokenizer.eos_token_id,\n                                      **kwargs)[0]\n        res = res[:-1] if res[-1] == self.tokenizer.eos_token_id else res\n        return self.tokenizer.decode(res[encoded_task_text['input_ids'].shape[1]:])",
    "description": null,
    "url": null
}