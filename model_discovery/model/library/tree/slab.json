{
    "acronym": "slab",
    "title": "SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization",
    "seed_ids": [
        "densemamba",
        "performer"
    ],
    "s2id": "c1d654386de0cee2561001e2de2ad444430323d1",
    "abstract": "Transformers have become foundational architectures for both natural language and computer vision tasks. However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of Flatten-Swin with $0.1\\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency.Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.",
    "authors": [
        "Jialong Guo",
        "Xinghao Chen",
        "Yehui Tang",
        "Yunhe Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "A novel method named PRepBN is proposed to progressively replace LayerNorm with re-parameterized BatchNorm in training and a simplified linear attention (SLA) module is proposed that is simple yet effective to achieve strong performance.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\n\nfrom transformers.utils import logging\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom functools import partial\nfrom llama import LlamaAttention, LlamaMLP, LlamaRMSNorm\n\n\nlogger = logging.get_logger(__name__)\n\n\n\nclass RepBN(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super(RepBN, self).__init__()\n        self.alpha = nn.Parameter(torch.zeros(1))\n        self.bn = nn.BatchNorm1d(channels, eps=eps, momentum=0.1)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        # nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bn.weight)\n        nn.init.zeros_(self.bn.bias)\n\n    def forward(self, x, pad_mask=None):\n        B, T, C = x.shape\n        # construct the mask_input, size to be (BxL) x C: L is the real length here\n        if pad_mask is None or not self.training:\n            mask_input = x.contiguous().view(-1, C)\n        else:\n            bn_mask = (pad_mask == 1)\n            mask_input = x[bn_mask, :]\n            mask_input = mask_input.contiguous().view(-1, C)\n\n        o_bn = self.bn(mask_input)\n\n        if pad_mask is None or not self.training:\n            output = o_bn.view(B, T, C)\n        else:\n            output = x.clone()\n            output[bn_mask, :] = o_bn\n\n        x = output + self.alpha * x\n        return x\n\n\nclass LinearNorm(nn.Module):\n    def __init__(self, dim, norm1, norm2, eps=1e-5, warm=10000, step=18000, r0=1.0):\n        super(LinearNorm, self).__init__()\n        self.register_buffer('num_step', torch.tensor(0))\n        self.register_buffer('warm', torch.tensor(warm))\n        self.register_buffer('iter', torch.tensor(step))\n        self.register_buffer('total_step', torch.tensor(step))\n        self.r0 = r0\n        self.norm1 = norm1(dim, eps)\n        self.norm2 = norm2(dim, eps)\n\n    def forward(self, x, pad_mask=None):\n        if self.training:\n            if self.warm > 0:\n                if self.num_step % 16 == 0:\n                    self.warm.copy_(self.warm - 1)\n                x = self.norm1(x)\n            else:\n                lamda = self.r0 * self.iter / self.total_step\n                if self.iter > 0:\n                    if self.num_step % 16 == 0:\n                        self.iter.copy_(self.iter - 1)\n                x1 = self.norm1(x)\n                x2 = self.norm2(x, pad_mask)\n                x = lamda * x1 + (1 - lamda) * x2\n            self.num_step.copy_((self.num_step + 1) % 16)\n        else:\n            x = self.norm2(x, pad_mask)\n        return x\n\n\nlinearnorm = partial(LinearNorm, norm1=LlamaRMSNorm, norm2=RepBN)\n# linearnorm = LlamaRMSNorm\n# linearnorm = RepBN\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.intermediate_size=config.intermediate_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        # self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        # self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.input_layernorm = linearnorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = linearnorm(config.hidden_size, eps=config.rms_norm_eps)\n        # self.input_layernorm = nn.Identity()\n        # self.post_attention_layernorm = nn.Identity()\n\n    def merge_bn(self):\n        # Attention\n        miu = self.input_layernorm.norm2.bn.running_mean\n        sigma2 = self.input_layernorm.norm2.bn.running_var\n        gamma = self.input_layernorm.norm2.bn.weight\n        beta = self.input_layernorm.norm2.bn.bias\n        eps = self.input_layernorm.norm2.bn.eps\n        alpha = self.input_layernorm.norm2.alpha\n\n        w_q = self.self_attn.q_proj.weight.data.transpose(0, 1)\n        w_k = self.self_attn.k_proj.weight.data.transpose(0, 1)\n        w_v = self.self_attn.v_proj.weight.data.transpose(0, 1)\n\n        self.self_attn.q_proj = nn.Linear(self.self_attn.hidden_size, self.self_attn.num_heads * self.self_attn.head_dim, bias=True).to(w_q.device)\n        self.self_attn.k_proj = nn.Linear(self.self_attn.hidden_size, self.self_attn.num_heads * self.self_attn.head_dim, bias=True).to(w_q.device)\n        self.self_attn.v_proj = nn.Linear(self.self_attn.hidden_size, self.self_attn.num_heads * self.self_attn.head_dim, bias=True).to(w_q.device)\n\n        a = gamma / torch.sqrt(sigma2 + eps) + alpha\n        b = beta - gamma * miu / torch.sqrt(sigma2 + eps)\n        a = torch.diag(a)\n\n        w_q_n = (a @ w_q).transpose(0, 1)\n        b_q_n = (b.unsqueeze(0) @ w_q).squeeze(0)\n        self.self_attn.q_proj.weight.data.copy_(w_q_n)\n        self.self_attn.q_proj.bias.data.copy_(b_q_n)\n        w_k_n = (a @ w_k).transpose(0, 1)\n        b_k_n = (b.unsqueeze(0) @ w_k).squeeze(0)\n        self.self_attn.k_proj.weight.data.copy_(w_k_n)\n        self.self_attn.k_proj.bias.data.copy_(b_k_n)\n        w_v_n = (a @ w_v).transpose(0, 1)\n        b_v_n = (b.unsqueeze(0) @ w_v).squeeze(0)\n        self.self_attn.v_proj.weight.data.copy_(w_v_n)\n        self.self_attn.v_proj.bias.data.copy_(b_v_n)\n        self.input_layernorm = nn.Identity()\n\n        # mlp\n        miu = self.post_attention_layernorm.norm2.bn.running_mean\n        sigma2 = self.post_attention_layernorm.norm2.bn.running_var\n        gamma = self.post_attention_layernorm.norm2.bn.weight\n        beta = self.post_attention_layernorm.norm2.bn.bias\n        eps = self.post_attention_layernorm.norm2.bn.eps\n        alpha = self.post_attention_layernorm.norm2.alpha\n\n        w_g = self.mlp.gate_proj.weight.data.transpose(0, 1)\n        w_u = self.mlp.up_proj.weight.data.transpose(0, 1)\n\n        self.mlp.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=True)\n        self.mlp.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=True)\n\n        a = gamma / torch.sqrt(sigma2 + eps) + alpha\n        b = beta - gamma * miu / torch.sqrt(sigma2 + eps)\n        a = torch.diag(a)\n\n        w_g_n = (a @ w_g).transpose(0, 1)\n        b_g_n = (b.unsqueeze(0) @ w_g).squeeze(0)\n        self.mlp.gate_proj.weight.data.copy_(w_g_n)\n        self.mlp.gate_proj.bias.data.copy_(b_g_n)\n        w_u_n = (a @ w_u).transpose(0, 1)\n        b_u_n = (b.unsqueeze(0) @ w_u).squeeze(0)\n        self.mlp.up_proj.weight.data.copy_(w_u_n)\n        self.mlp.up_proj.bias.data.copy_(b_u_n)\n        self.post_attention_layernorm = nn.Identity()\n        return\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        pad_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        # hidden_states = self.input_layernorm(hidden_states, pad_mask)\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        # hidden_states = self.post_attention_layernorm(hidden_states, pad_mask)\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n        # breakpoint()\n        return outputs\n\n",
    "description": null,
    "url": null
}