{
    "acronym": "cvt",
    "title": "Building Blocks for a Complex-Valued Transformer Architecture",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "d6e09d3274df2c717e47fa15575f493422309a10",
    "abstract": "Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into \u211d2. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.",
    "authors": [
        "Florian Eilers",
        "Xiaoyi Jiang"
    ],
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "tldr": "This work adds to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain and shows improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.",
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": "from typing import Optional\nfrom functools import partial\n\nimport torch\nfrom torch import cfloat\nimport torch.nn.functional as F\nfrom torch import nn, einsum, Tensor\nfrom torch.nn import Module, ModuleList\n\nfrom einops import rearrange, repeat, reduce\nfrom einops.layers.torch import Rearrange\n\nfrom collections import namedtuple\nfrom functools import wraps\nfrom packaging import version\n\n\n# constants\n\nEfficientAttentionConfig = namedtuple('EfficientAttentionConfig', ['enable_flash', 'enable_math', 'enable_mem_efficient'])\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\n# tensor functions\n\ndef create_causal_mask(i, j, device):\n    return torch.ones((i, j), device = device, dtype = torch.bool).triu(j - i + 1)\n\n# main class\n\nclass Attend(nn.Module):\n    def __init__(\n        self,\n        *,\n        dropout = 0.,\n        causal = False,\n        heads = None,\n        scale = None,\n        flash = False,\n    ):\n        super().__init__()\n        self.scale = scale\n\n        self.causal = causal\n        self.create_causal_mask = create_causal_mask\n\n        self.dropout = dropout\n        self.attn_dropout = nn.Dropout(dropout)\n\n        # flash attention\n\n        self.flash = flash\n        assert not (flash and version.parse(torch.__version__) < version.parse('2.0.0')), 'in order to use flash attention, you must be using pytorch 2.0 or above'\n\n        # determine efficient attention configs for cuda and cpu\n\n        self.cpu_config = EfficientAttentionConfig(True, True, True)\n        self.cuda_config = None\n\n        if not torch.cuda.is_available() or not flash:\n            return\n\n        device_properties = torch.cuda.get_device_properties(torch.device('cuda'))\n\n        major, minor = device_properties.major, device_properties.minor\n\n        if (major, minor) == (8, 0):\n            print_once('A100 GPU detected, using flash attention if input tensor is on cuda')\n            self.cuda_config = EfficientAttentionConfig(True, False, False)\n        elif (major, minor) == (9, 0):\n            print_once('H100 GPU detected, using flash attention')\n            self.cuda_config = EfficientAttentionConfig(True, False, False)\n        else:\n            print_once('Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda')\n            self.cuda_config = EfficientAttentionConfig(False, True, True)\n\n    def flash_attn(\n        self,\n        q, k, v,\n        mask = None\n    ):\n        batch, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n\n        # Check if mask exists and expand to compatible shape\n        # The mask is B L, so it would have to be expanded to B H N L\n\n        causal = self.causal\n\n        # in the case of kv caching with one token (q_len == 1), just turn off causal masking\n        # in speculative decoding, this may go up to 5-6, so right aligned causal mask will be needed there\n\n        if q_len == 1 and causal:\n            causal = False\n\n        # expand key padding mask\n\n        if exists(mask):\n            assert mask.ndim == 4\n            mask = mask.expand(batch, heads, q_len, k_len)\n\n        # handle kv cache - this should be bypassable in updated flash attention 2\n\n        if k_len > q_len and causal:\n            causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n            if not exists(mask):\n                mask = ~causal_mask\n            else:\n                mask = mask & ~causal_mask\n            causal = False\n\n        # manually handle causal mask, if another mask was given\n\n        row_is_entirely_masked = None\n\n        if exists(mask) and causal:\n            causal_mask = self.create_causal_mask(q_len, k_len, device = device)\n            mask = mask & ~causal_mask\n\n            # protect against an entire row being masked out\n\n            row_is_entirely_masked = ~mask.any(dim = -1)\n            mask[..., 0] = mask[..., 0] | row_is_entirely_masked\n\n            causal = False\n\n        # Check if there is a compatible device for flash attention\n\n        config = self.cuda_config if is_cuda else self.cpu_config\n\n        # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\n        \n        with torch.backends.cuda.sdp_kernel(**config._asdict()):\n            out = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask = mask,\n                dropout_p = self.dropout if self.training else 0., \n                is_causal = causal\n            )\n\n        # for a row that is entirely masked out, should zero out the output of that row token\n\n        if exists(row_is_entirely_masked):\n            out = out.masked_fill(row_is_entirely_masked[..., None], 0.)\n\n        return out\n\n    def forward(\n        self,\n        q, k, v,\n        mask = None\n    ):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        n, heads, kv_heads, device = q.shape[-2], q.shape[1], k.shape[1], q.device\n\n        scale = default(self.scale, q.shape[-1] ** -0.5)\n\n        if self.flash:\n            return self.flash_attn(q, k, v, mask = mask)\n\n        kv_einsum_eq = 'b j d' if k.ndim == 3 else 'b h j d'\n\n        sim = einsum(f'b h i d, {kv_einsum_eq} -> b h i j', q, k) * scale\n\n        i, j, dtype = *sim.shape[-2:], sim.dtype\n\n        mask_value = -torch.finfo(sim.dtype).max\n\n        if exists(mask):\n            sim = sim.masked_fill(~mask, mask_value)\n\n        if self.causal and n > 1:\n            causal_mask = self.create_causal_mask(i, j, device = device)\n            sim = sim.masked_fill(causal_mask, mask_value)\n\n        attn = sim.softmax(dim = -1)\n        attn = attn.type(dtype)\n\n        attn = self.attn_dropout(attn)\n\n        out = einsum(f'b h i j, {kv_einsum_eq} -> b h i d', attn, v)\n\n        return out\n    \n\n# helpers\n\ndef exists(v):\n    return v is not None\n\ndef default(v, d):\n    return v if exists(v) else d\n\n# helper tensor functions\n\ndef modulate_with_rotation(x, m):\n    if m.dtype == cfloat:\n        m = m.abs()\n\n    rot = m.cos() + 1.j * m.sin()\n    return x * rot\n\n# complex attention\n# https://arxiv.org/abs/2306.09827\n\ndef complex_attention_real(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attend: Attend,\n    mask: Optional[Tensor] = None\n):\n    \"\"\"\n    section 4.1 equation 8\n    \"\"\"\n\n    assert all([t.dtype == cfloat for t in (q, k, v)])\n    q, k, v = map(torch.view_as_real, (q, k, v))\n    q, k, v = map(lambda t: rearrange(t, '... d c -> ... (d c)'), (q, k, v))\n\n    o = attend(q, k, v, mask = mask)\n\n    o = rearrange(o, '... (d c) -> ... d c', c = 2)\n    return torch.view_as_complex(o)\n\n# complex attention - Yang et al\n# https://arxiv.org/abs/1910.10202\n\ndef complex_attention_complete(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attend: Attend,\n    mask: Optional[Tensor] = None\n):\n    \"\"\"\n    section 3.2 equation 3\n    \"\"\"\n    batch, device = q.shape[0], q.device\n\n    assert all([t.dtype == cfloat for t in (q, k, v)])\n    q, k, v = map(torch.view_as_real, (q, k, v))\n\n    # complex attention =    (MH(A, A, A) \u2212 MH(A, B, B) \u2212 MH(B, A, B) \u2212 MH(B, B, A))\n    #                     + i(MH(A, A, B) + MH(A, B, A) + MH(B, A, A) \u2212 MH(B, B, B))\n\n    q = repeat(q, 'b h n d c -> (c r b) h n d', r = 2)\n    k = repeat(k, 'b h n d c -> (r c b) h n d', r = 2)\n    v = repeat(v, 'b h n d c -> (r b) h n (d c)', r = 4)\n\n    if exists(mask):\n        mask = repeat(mask, 'b ... -> (r b) ...', r = 4)\n\n    o = attend(q, k, v, mask = mask)\n\n    o = rearrange(o, '(r b) ... (d c) -> (r c) b ... d', r = 4, c = 2)\n\n    indices = torch.tensor([0, 3, 5, 6, 1, 2, 4, 7], dtype = torch.long, device = device)\n\n    o = rearrange(o[indices], '(r c) ... -> ... c r', c = 2)\n\n    sign = torch.tensor([\n        [1., -1., -1., -1.],   # real component\n        [1.,  1.,  1., -1.]    # imag component\n    ], dtype = o.dtype, device = device)\n\n    o = (o * sign).sum(dim = -1)\n\n    return torch.view_as_complex(o)\n\n# complex multihead attention\n\nclass ComplexMultiheadAttention(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        causal = False,\n        dim_head = 32,\n        heads = 8,\n        complete_complex = False, # whether to use complete complex formulation (Yang et al.) or just the real component, which reduces down to usual dot product on real and imaginary components flattened into the feature dimension\n        flash = False\n    ):\n        super().__init__()\n        dim_inner = heads * dim_head\n\n        self.to_q = nn.Linear(dim, dim_inner, bias = False, dtype = cfloat)\n        self.to_kv = nn.Linear(dim, dim_inner * 2, bias = False, dtype = cfloat)\n        self.to_out = nn.Linear(dim_inner, dim, bias = False, dtype = cfloat)\n\n        maybe_flash_attn = Attend(\n            causal = causal,\n            heads = heads,\n            flash = flash\n        )\n\n        complex_attention = complex_attention_complete if complete_complex else complex_attention_real\n        self.attend = partial(complex_attention, attend = maybe_flash_attn)\n\n        self.split_heads = Rearrange('b n (h d) -> b h n d', h = heads)\n        self.merge_heads = Rearrange('b h n d -> b n (h d)')\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        rotary_emb = None\n    ):\n        has_context = exists(context)\n        context = default(context, x)\n\n        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n        q, k, v = map(self.split_heads, (q, k, v))\n\n        if exists(rotary_emb):\n            q = q * rotary_emb\n            k = k * rotary_emb\n\n        o = self.attend(q, k, v, mask = mask)\n\n        o = self.merge_heads(o)\n        return self.to_out(o)\n\n# rmsnorm\n\nclass ComplexRMSNorm(Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.scale = dim ** -0.5\n        self.gamma = nn.Parameter(torch.ones(dim, dtype = cfloat))\n\n    def forward(self, x):\n        return F.normalize(x, dim = -1) * self.gamma * self.scale\n\n# feedforward with mod relu\n# https://arxiv.org/abs/1511.06464v4\n\nclass ModReLU(Module):\n    def __init__(self, relu_squared = False):\n        super().__init__()\n        self.pow = 2 if relu_squared else 1\n        self.bias = nn.Parameter(torch.tensor(0.))\n\n    def forward(self, x):\n        real = F.relu(torch.abs(x) + self.bias) ** self.pow\n        imag = torch.exp(1.j * torch.angle(x))\n        return real + imag\n\n\ndef ComplexFeedForward(dim, mult = 4, relu_squared = False):\n    dim_inner = dim * mult\n    return nn.Sequential(\n        nn.Linear(dim, dim_inner, dtype = cfloat),\n        ModReLU(relu_squared = relu_squared),\n        nn.Linear(dim_inner, dim, dtype = cfloat)\n    )\n\n# rotary embeddings\n# formulated for complex numbers\n\nclass RotaryEmbedding(Module):\n    def __init__(self, dim, base = 10000):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n    @property\n    def device(self):\n        return self.inv_freq.device\n\n    def forward(self, seq_len):\n        t = torch.arange(seq_len, device = self.device).type_as(self.inv_freq)\n        freqs = einsum('i, j -> i j', t, self.inv_freq)\n        return torch.cos(freqs) + 1.j * torch.sin(freqs)\n\n# complex transformer\n\nclass ComplexTransformer(Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth,\n        num_tokens: Optional[int] = None,\n        causal = False,\n        dim_head = 32,\n        heads = 8,\n        ff_mult = 4,\n        relu_squared = True,\n        complete_complex = False,\n        rotary_emb = True,\n        flash_attn = True\n    ):\n        super().__init__()\n\n        self.has_embed = exists(num_tokens)\n\n        if exists(num_tokens):\n            self.embed = nn.Parameter(torch.randn((num_tokens, dim), dtype = cfloat))\n\n        self.rotary_emb = None\n        if rotary_emb:\n            self.rotary_emb = RotaryEmbedding(dim_head)\n\n        self.layers = ModuleList([])\n        for _ in range(depth):\n            self.layers.append(ModuleList([\n                ComplexRMSNorm(dim),\n                ComplexMultiheadAttention(dim = dim, dim_head = dim_head, heads = heads, causal = causal, complete_complex = complete_complex, flash = flash_attn),\n                ComplexRMSNorm(dim),\n                ComplexFeedForward(dim = dim, mult = ff_mult, relu_squared = relu_squared)\n            ]))\n\n        self.norm = ComplexRMSNorm(dim)\n\n        self.to_logits = nn.Linear(dim, num_tokens, dtype = cfloat)\n\n    def forward(\n        self,\n        x,\n        context = None,\n        mask = None,\n        return_abs_logits = False,\n        return_real_logits = False\n    ):\n        if self.has_embed:\n            x = self.embed[x]\n\n        seq_len = x.shape[-2]\n        rotary_emb = None\n\n        if exists(self.rotary_emb):\n            rotary_emb = self.rotary_emb(seq_len)\n\n        for attn_norm, attn, ff_norm, ff in self.layers:\n            x = attn(attn_norm(x), context = context, mask = mask, rotary_emb = rotary_emb) + x\n            x = ff(ff_norm(x)) + x\n\n        x = self.norm(x)\n\n        if not self.has_embed:\n            return x\n\n        logits = self.to_logits(x)\n\n        # don't know the complex network literature well enough to know whether to choose abs or angle\n\n        assert (int(return_abs_logits) + int(return_real_logits)) <= 1\n\n        if return_abs_logits:\n            logits = logits.abs()\n        elif return_real_logits:\n            logits = logits.real\n\n        return logits",
    "description": null,
    "url": null
}