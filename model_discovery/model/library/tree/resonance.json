{
    "acronym": "resonance",
    "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
    "seed_ids": [
        "pi",
        "alibi",
        "roformer",
        "compressivetransformer"
    ],
    "s2id": "f016f079ee63a0487756f895c1d93ff0110d3ecd",
    "abstract": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position better and more robustly. Our extensive LLM experiments also show superior performance after applying Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on both upstream language modeling tasks and a variety of downstream long-text applications.",
    "authors": [
        "Suyuchen Wang",
        "I. Kobyzev",
        "Peng Lu",
        "Mehdi Rezagholizadeh",
        "Bang Liu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "Resonance RoPE is introduced, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs.",
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": "import math\nimport torch\nfrom einops import repeat\n\nimport math\nimport torch\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb_scaled(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\n# Inverse dim formula to find dim based on number of rotations\ndef find_correction_dim(num_rotations, dim, base=10000, max_position_embeddings=2048):\n    return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (2 * math.log(base))\n\n\n# Find dim range bounds based on rotations\ndef find_correction_range(low_rot, high_rot, dim, base=10000, max_position_embeddings=2048):\n    low = math.floor(find_correction_dim(\n        low_rot, dim, base, max_position_embeddings))\n    high = math.ceil(find_correction_dim(\n        high_rot, dim, base, max_position_embeddings))\n    return max(low, 0), min(high, dim - 1)  # Clamp values just in case\n\n\ndef linear_ramp_mask(min, max, dim):\n    if min == max:\n        max += 0.001  # Prevent singularity\n\n    linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n    ramp_func = torch.clamp(linear_func, 0, 1)\n    return ramp_func\n\n\ndef get_mscale(scale=1.0):\n    if scale <= 1:\n        return 1.0\n    return 0.1 * math.log(scale) + 1.0\n\nclass ResonanceRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        assert dim % 2 == 0, 'dim must be multiple of 2 for Resonance RoPE.'\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        self._register_buffers(device)\n\n        # Build here to make `torch.jit.trace` work.\n        self._set_cos_sin_cache(\n            seq_len=max_position_embeddings, device=self.r_inv_freq.device, dtype=torch.get_default_dtype()\n        )\n\n    def _register_buffers(self, device):\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n        r_wavelengths = torch.round(2 * math.pi / inv_freq)\n        r_inv_freq = 2 * math.pi / r_wavelengths\n        self.register_buffer(\"r_inv_freq\", r_inv_freq, persistent=False)\n        self.register_buffer(\"r_wavelengths\", r_wavelengths, persistent=False)\n\n    def compute_freqs(self, seq_len, device):\n        freqs_list = list()\n        for i in range(self.dim // 2):\n            if seq_len >= self.r_wavelengths[i].item():\n                t_i = torch.arange(self.r_wavelengths[i], device=device, dtype=self.r_inv_freq.dtype)\n                current_freq = repeat(t_i * self.r_inv_freq[i], 'l -> (repeat l)',\n                                      repeat=math.ceil(seq_len / self.r_wavelengths[i].item()))[:seq_len]\n                freqs_list.append(current_freq)\n            else:\n                t_i = torch.arange(self.max_seq_len_cached, device=device, dtype=self.r_inv_freq.dtype)\n                current_freq = t_i * self.r_inv_freq[i]\n                freqs_list.append(current_freq)\n\n        freqs = torch.stack(freqs_list, dim=1)\n        return freqs\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        if seq_len > 70:\n            print('Warning')\n\n        freqs = self.compute_freqs(seq_len, device)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        # emb = torch.cat((freqs, freqs), dim=-1)\n        emb = freqs\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        if seq_len > self.max_seq_len_cached:\n            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=torch.get_default_dtype())\n\n        return (\n            self.cos_cached[:, :, :seq_len, ...],\n            self.sin_cached[:, :, :seq_len, ...],\n        )\n\n\nclass ResonanceLinearScaledRotaryEmbedding(ResonanceRotaryEmbedding):\n    \"\"\"ResonanceRotaryEmbedding extended with linear scaling.\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def compute_freqs(self, seq_len, device):\n        freqs_list = list()\n        for i in range(self.dim // 2):\n            if seq_len < self.r_wavelength[i].item():\n                t_i = torch.arange(self.r_wavelength[i], device=device, dtype=self.inv_freq.dtype)\n                t_i /= self.scaling_factor\n                current_freq = repeat(t_i * self.r_inv_freq[i], 'l -> (repeat l)',\n                                      repeat=math.ceil(seq_len // self.r_wavelength[i].item()))[:seq_len]\n                freqs_list.append(current_freq)\n            else:\n                t_i = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n                t_i /= self.scaling_factor\n                current_freq = t_i * self.r_inv_freq[i]\n                freqs_list.append(current_freq)\n\n        freqs = torch.stack(freqs_list, dim=1)\n        return freqs\n\n\nclass ResonanceNTKScaledRotaryEmbedding(ResonanceRotaryEmbedding):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        base = base * scaling_factor ** (dim / (dim - 2))\n        super().__init__(dim, max_position_embeddings, base, device)\n\n\nclass ResonanceYaRNScaledRotaryEmbedding(ResonanceRotaryEmbedding):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0, attn_factor=1,\n                 beta_fast=2, beta_slow=1):\n        self.scale = scaling_factor\n        self.attn_factor = attn_factor\n        self.beta_fast = beta_fast\n        self.beta_slow = beta_slow\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _register_buffers(self, device):\n        pos_freqs = self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)\n        inv_freq_extrapolation = 1.0 / pos_freqs\n        inv_freq_interpolation = 1.0 / (self.scale * pos_freqs)\n\n        low, high = find_correction_range(self.beta_fast, self.beta_slow, self.dim, self.base,\n                                          self.max_position_embeddings)\n        inv_freq_mask = (1 - linear_ramp_mask(low, high, self.dim // 2).float().to(\n            device))  # Get n-d rotational scaling corrected for extrapolation\n        inv_freq = inv_freq_interpolation * (1 - inv_freq_mask) + inv_freq_extrapolation * inv_freq_mask\n\n        r_wavelengths = torch.round(2 * math.pi / inv_freq)\n        r_inv_freq = 2 * math.pi / r_wavelengths\n        self.register_buffer(\"r_inv_freq\", r_inv_freq, persistent=False)\n        self.register_buffer(\"r_wavelengths\", r_wavelengths, persistent=False)\n\n        self.mscale = float(\n            get_mscale(self.scale) * self.attn_factor)  # Get n-d magnitude scaling corrected for interpolation\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n\n        freqs = self.compute_freqs(seq_len, device)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        # emb = torch.cat((freqs, freqs), dim=-1)\n        emb = freqs\n        self.register_buffer(\"cos_cached\", (emb.cos() * self.mscale)[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", (emb.sin() * self.mscale)[None, None, :, :].to(dtype), persistent=False)\n\n\nif __name__ == '__main__':\n    from src.models.components.positional_embedding import RotaryEmbedding\n\n    emb_original = RotaryEmbedding(dim=32)\n    emb = ResonanceRotaryEmbedding(dim=32)\n    x = torch.tensor(list(range(64)), dtype=torch.float32)\n    v_original = emb_original(x, seq_len=64)\n    v_resonance = emb(x, seq_len=64)\n    print(v_original)\n    print(v_resonance)",
    "description": null,
    "url": null
}