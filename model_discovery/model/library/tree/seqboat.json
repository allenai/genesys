{
    "acronym": "seqboat",
    "title": "Sparse Modular Activation for Efficient Sequence Modeling",
    "seed_ids": [
        "h3",
        "mega",
        "s5",
        "s4d",
        "flash",
        "s4",
        "roformer",
        "performer",
        "linformer",
        "transformerxl",
        "transformer"
    ],
    "s2id": "d2d0371158803df93a249c9f7237ffd79b875816",
    "abstract": "Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.",
    "authors": [
        "Liliang Ren",
        "Yang Liu",
        "Shuo Wang",
        "Yichong Xu",
        "Chenguang Zhu",
        "Chengxiang Zhai"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.",
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": "# Copyright (c) Liliang Ren.\n# All rights reserved.\n\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\nfrom torch.nn import Parameter\n\nfrom fairseq import utils\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom fairseq.modules.fairseq_dropout import FairseqDropout, FairseqFeatureDropout\nfrom fairseq.modules.sequence_norm import SequenceNorm\nfrom fairseq.modules.exponential_moving_average import MultiHeadEMA\nfrom fairseq.modules.seqboat_utils import * \n\nfrom fairseq.modules.s4d_layer import S4D\n\n@with_incremental_state\nclass SeqBoatUnit(nn.Module):\n\n    def __init__(\n        self,\n        embed_dim,\n        zdim,\n        hdim,\n        ndim,\n        dropout=0.0,\n        attention_dropout=0.0,\n        hidden_dropout=0.0,\n        activation='silu',\n        attention_activation='relu2',\n        bidirectional=False,\n        chunk_size=-1,\n        truncation=None,\n        norm_type='layernorm',\n        prenorm=True,\n        norm_affine=True,\n        feature_dropout=False,\n        rel_pos_bias='simple',\n        max_positions=1024,\n        export=False,\n        density_threshold=0.9,\n        max_steps = 180000,\n        init_temp_scale=0.3,\n        local_pos = False,\n        always_act = False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.hdim = hdim\n        self.zdim = zdim\n        self.ndim = ndim\n        self.activation = utils.get_activation_fn(activation=activation)\n        self.attention_activation = attention_activation\n        self.truncation = truncation\n\n        dropout_module = FairseqFeatureDropout if feature_dropout else FairseqDropout\n        self.dropout = dropout_module(dropout, module_name=self.__class__.__name__)\n        self.hidden_dropout = dropout_module(hidden_dropout, module_name=self.__class__.__name__)\n\n        local = True \n        self.local = local and chunk_size>0\n        self.latent_conf = not always_act\n        self.local_pos = self.local and local_pos and rel_pos_bias == 'simple'\n        if self.local:\n            density_threshold = 1\n            self.window_size = chunk_size\n            chunk_size = -1\n        \n        self.s4d= False\n        \n        self.use_rot_bias=False\n\n        self.scaling = self.zdim ** -0.5 \n        self.chunk_size = chunk_size\n        self.prenorm = prenorm\n        self.norm = SequenceNorm(norm_type, embed_dim, affine=norm_affine, export=export)\n            \n        if self.s4d:\n            self.move = S4D(embed_dim, ndim=ndim, bidirectional=bidirectional, truncation=truncation)\n        else:\n            self.move = MultiHeadEMA(embed_dim, ndim=ndim, bidirectional=bidirectional, truncation=truncation, truc_norm=True)\n\n\n        self.gamma = Parameter(torch.Tensor(2, zdim))\n        self.beta = Parameter(torch.Tensor(2, zdim))\n\n        # Attention dropout is standard dropout\n        self.attention_dropout = FairseqDropout(attention_dropout, module_name=self.__class__.__name__)\n        self.rel_pos_type = rel_pos_bias\n        self.max_positions = max_positions\n        \n        if rel_pos_bias == 'simple':\n            max_positions = max_positions if chunk_size < 0 else chunk_size\n            if self.local_pos:\n                max_positions = self.window_size*3 # Most 3x with non-causal\n            self.rel_pos_bias = FastRelativePositionalBias(max_positions)\n\n        elif rel_pos_bias == 'rotary':\n            self.max_pos = max_positions\n            self.freqs_cis = precompute_freqs_cis(\n                self.zdim, max_positions\n            )\n\n            if self.use_rot_bias:\n                self.rot_a = nn.Parameter(torch.Tensor(1, self.zdim))\n                self.rot_b = nn.Parameter(torch.Tensor(1, self.zdim))\n            \n        elif rel_pos_bias == 't5':\n            self.rel_pos_bias =  T5RelativePositionBias(scale = 1, num_buckets = 32 ,max_distance = 128)\n        else:\n            raise ValueError('unknown relative position bias: {}'.format(rel_pos_bias))\n\n\n        self.use_sigmoid = False\n        if self.use_sigmoid:\n            num_options = 1\n        else:\n            num_options = 2\n        self.density_threshold = density_threshold\n        \n        if self.latent_conf:\n            self.q_router = LatentConfigurator(\n                dim = embed_dim,\n                num_vars = num_options,\n                max_steps= max_steps,\n                init_temp_scale= init_temp_scale\n            ) \n \n        \n       \n        self.mx_proj = nn.Linear(embed_dim, zdim +hdim+hdim)\n        self.opt_dim = num_options \n        if self.latent_conf:\n            self.hx_proj = nn.Linear(embed_dim, self.opt_dim+embed_dim)\n        else:\n            self.hx_proj = nn.Linear(embed_dim, embed_dim)\n        \n        self.h_proj = nn.Linear(hdim, embed_dim)\n\n        self.reset_parameters()\n\n        self.onnx_trace = False\n        self.tpu = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def prepare_for_tpu_(self, **kwargs):\n        self.tpu = True\n\n    def reset_parameters(self):\n        with torch.no_grad():\n            std = 0.02\n\n            Normal(self.mx_proj.weight, mean=0.0, std=std)\n            nn.init.constant_(self.mx_proj.bias, 0.0)\n\n            Normal(self.h_proj.weight, mean=0.0, std=std)\n            nn.init.constant_(self.h_proj.bias, 0.0)\n\n            Normal(self.gamma, mean=0.0, std=std)\n\n            nn.init.constant_(self.beta, 0.0)\n            \n            if self.rel_pos_type == 'rotary' and self.use_rot_bias:\n                Normal(self.rot_a, mean=0.0, std=std)\n                Normal(self.rot_b, mean=0.0, std=std)\n\n            Normal(self.hx_proj.weight, mean=0.0, std=std)\n            nn.init.constant_(self.hx_proj.bias, 0.0)\n\n\n\n    def local_attention(self, q, k, v, attn_bias, mask, causal):\n        \n        # attn_bias: b,k,c,c\n        shape, autopad, pad_value, window_size = q.shape, True, -1, self.window_size\n        exact_windowsize = True\n        if causal:\n            look_backward = 1\n            look_forward = 0\n        else:\n            look_backward = 1\n            look_forward = 1\n            \n        # https://github.com/arogozhnikov/einops/blob/master/docs/4-pack-and-unpack.ipynb\n        (q, packed_shape), (k, _), (v, _) = map(lambda t: pack([t], '* n d'), (q, k, v))\n\n        # auto padding\n        if autopad:\n            orig_seq_len = q.shape[1]\n            (needed_pad, q), (_, k), (_, v) = map(lambda t: pad_to_multiple(t, self.window_size, dim = -2), (q, k, v))\n            \n        b, n, dim_head, device, dtype = *q.shape, q.device, q.dtype\n\n        assert (n % window_size) == 0, f'sequence length {n} must be divisible by window size {window_size} for local attention'\n\n        windows = n // window_size\n\n\n        seq = torch.arange(n, device = device)\n        b_t = rearrange(seq, '(w n) -> 1 w n', w = windows, n = window_size)\n\n        # bucketing\n        bq, bk, bv = map(lambda t: rearrange(t, 'b (w n) d -> b w n d', w = windows), (q, k, v))\n\n        look_around_kwargs = dict(\n            backward =  look_backward,\n            forward =  look_forward,\n            pad_value = pad_value\n        )\n\n        bk = look_around(bk, **look_around_kwargs)\n        bv = look_around(bv, **look_around_kwargs)\n\n\n        # calculate positions for masking\n        bq_t = b_t\n        bq_k = look_around(b_t, **look_around_kwargs)\n\n        bq_t = rearrange(bq_t, '... i -> ... i 1')\n        bq_k = rearrange(bq_k, '... j -> ... 1 j')\n\n        pad_mask = bq_k == pad_value\n\n        sim = einsum('b h i e, b h j e -> b h i j', bq, bk)\n        \n        if self.local_pos:\n            sim = sim + self.rel_pos_bias(sim.shape[-1])[:sim.shape[-2],:]\n        elif exists(attn_bias):\n            if autopad:\n                _,attn_bias = pad_to_multiple(attn_bias, self.window_size, dim = -2)\n                _,attn_bias = pad_to_multiple(attn_bias, self.window_size, dim = -1)\n            attn_bias = look_around_2d(attn_bias,window_size,look_backward,look_forward)\n            ab,ak,aw,ac,av = attn_bias.shape\n            sim = sim + attn_bias.view(-1,aw,ac,av)\n\n\n        if self.attention_activation ==\"softmax\":\n            mask_value =  TOKEN_SELF_ATTN_VALUE\n        else:\n            mask_value = 0\n\n        if causal:\n            causal_mask = bq_t < bq_k\n\n            if exact_windowsize:\n                max_causal_window_size = (self.window_size * look_backward)\n                causal_mask = causal_mask | (bq_t > (bq_k + max_causal_window_size))\n\n            sim = sim.masked_fill(causal_mask, mask_value)\n            del causal_mask\n\n        # masking out for exact window size for non-causal\n        # as well as masking out for padding value\n        if not causal and exact_windowsize:\n            max_backward_window_size = (self.window_size * look_backward)\n            max_forward_window_size = (self.window_size * look_forward)\n            window_mask = ((bq_k - max_forward_window_size) > bq_t) | (bq_t > (bq_k + max_backward_window_size)) | pad_mask\n            sim = sim.masked_fill(window_mask, mask_value)\n        else:\n            sim = sim.masked_fill(pad_mask, mask_value)\n\n        # take care of key padding mask passed in\n        if exists(mask):\n            batch = mask.shape[0]\n            assert (b % batch) == 0\n\n            if autopad:\n                _, mask = pad_to_multiple(mask, window_size, dim = -1, value = False)\n\n            mask = rearrange(mask, '... (w n) -> (...) w n', w = windows, n = window_size)\n            mask = look_around(mask, **{**look_around_kwargs, 'pad_value': False})\n            mask = rearrange(mask, '... j -> ... 1 j')\n            sim = sim.masked_fill(~mask, mask_value)\n            del mask\n        \n        # attention\n        if self.attention_activation == 'softmax':\n            attn = utils.softmax(sim, dim=-1).type_as(sim)\n        elif self.attention_activation == 'relu2':\n            attn = utils.relu2(sim).type_as(sim)\n        elif self.attention_activation == 'laplace':\n            attn = utils.laplace(sim).type_as(sim)      \n        else:\n            raise ValueError('Unknown attention activation function: {}'.format(self.attention_activation))\n\n        attn = self.attention_dropout(attn)\n\n        # aggregation\n        out = einsum('b h i j, b h j e -> b h i e', attn, bv)\n        out = rearrange(out, 'b w n d -> b (w n) d')\n\n        if autopad:\n            out = out[:, :orig_seq_len, :]\n\n        out, *_ = unpack(out, packed_shape, '* n d')\n        \n        return out\n\n\n\n    def element_attention(self, q, k, v, padding_mask, attn_mask, tick,compress, seq_len):\n        # padding_mask: b,k,c\n        \n        slen = k.size(2)\n        causal = attn_mask is not None\n\n        if self.local_pos:\n            bias = None\n        elif self.rel_pos_type == 'rotary':\n            if self.use_rot_bias:\n                bias_a = apply_rotary_emb(self.rot_a[None,None,:,:].expand(k.size(0),k.size(1),slen,-1),tick)\n                bias_b = apply_rotary_emb(self.rot_b[None,None,:,:].expand(k.size(0),k.size(1),slen,-1),tick)\n                bias = torch.matmul(bias_a,bias_b.transpose(-1,-2))\n            else:\n                q = apply_rotary_emb(q, tick)\n                k = apply_rotary_emb(k, tick)\n                bias = None\n            \n        elif self.rel_pos_type == 't5':\n            bias = self.rel_pos_bias(tick,tick,causal)\n        else:\n            # C x C\n            bias = self.rel_pos_bias(seq_len)\n\n            if slen != q.size(2):\n                assert q.size(2) == 1\n                # 1 x C\n                bias = bias[-1:]\n\n            \n            if compress:\n                bias = F.pad(bias,(1,0,1,0))\n                bias = bias[tick]\n                bias = torch.gather(bias.float(), -1, tick[:,:,None,:].expand(-1,-1,bias.shape[-2],-1)).type_as(bias)\n            else:\n                bias = bias[None,None,:,:]\n        \n        # bias : # b k c c\n        \n        if self.attention_activation == 'softmax':\n            q = q * self.scaling\n            if padding_mask is not None:\n                padding_mask_all = padding_mask.all(dim=-1, keepdim=True)\n                padding_mask = torch.logical_and(padding_mask, ~padding_mask_all)\n        else:\n            if padding_mask is not None:\n                # B x K x C\n                inverse_mask = ~padding_mask\n                if not self.local:\n                    # B x K x 1\n                    lengths = inverse_mask.sum(dim=-1, keepdim=True)\n                    # B x K x 1 x 1\n                    lengths = lengths.clamp(min=1).unsqueeze(-1)\n\n            else:\n                lengths = slen\n                inverse_mask = None\n    \n            if self.local:\n                if causal:\n                    lengths = self.window_size\n                else:\n                    lengths = self.window_size*2\n            elif attn_mask is not None:\n                # C x 1\n                lengths = attn_mask.sum(dim=-1, keepdim=True)   \n            # q: b k c h\n            # tick: b k c \n\n            q =q / lengths\n            \n        if self.local:\n            h = self.local_attention(q, k, v, bias, ~padding_mask, causal) #1 for keep\n        else:\n            # B x K x C x C1 (C1 can be non-equal to C)\n            qk = torch.matmul(q, k.transpose(2, 3)) \n            if bias is not None:\n                qk += bias\n\n            if self.attention_activation == 'softmax':\n                if attn_mask is not None:\n                    qk = qk + attn_mask\n\n                if padding_mask is not None:\n                    qk = qk.masked_fill(padding_mask.unsqueeze(2).to(torch.bool), float('-inf'))\n                attn_weights = utils.softmax(qk, dim=-1).type_as(qk)\n            \n            else:\n                if self.attention_activation == 'relu2':\n                    attn_weights = utils.relu2(qk).type_as(qk)\n                elif self.attention_activation == 'laplace':\n                    attn_weights = utils.laplace(qk).type_as(qk)      \n                else:\n                    raise ValueError('Unknown attention activation function: {}'.format(self.attention_activation))\n\n                if inverse_mask is not None:\n                    attn_weights = attn_weights * inverse_mask.unsqueeze(2)\n\n                if attn_mask is not None:\n                    attn_weights = attn_weights * attn_mask\n                \n            kernel = self.attention_dropout(attn_weights)\n            # B xK x C x E  -> B x L x E -> L x B x E\n            h = torch.matmul(kernel, v)\n\n        return h\n\n\n    def forward(\n        self,\n        x,\n        padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        before_attn_fn: bool = False,\n        act_bias: float = 0.0,\n        verbose: bool=False,\n        layer_idx: int=0,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_attn_fn (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n        \"\"\"\n\n        seq_len, bsz, embed_dim = x.size()\n\n        assert embed_dim == self.embed_dim\n\n        saved_state = None\n\n        residual = x\n        if self.prenorm:\n           x = self.norm(x)\n\n\n        # L x B x D\n        mx = self.move(x, padding_mask, incremental_state)\n\n\n        mx = self.activation(mx)\n        mx = self.dropout(mx)\n        \n\n        mx = mx.transpose(0,1)\n       \n        if saved_state is not None:\n            # assert self.chunk_size < 0 or q.size(1) <= self.chunk_size\n            # saved states are stored with shape (bsz, seq_len, dim)\n            if \"prev_mx\" in saved_state:\n                prev_mx= saved_state[\"prev_mx\"]\n                assert prev_mx is not None\n                assert mx is not None\n                mx = torch.cat([prev_mx, mx], dim=1)\n\n            prev_padding_mask: Optional[Tensor] = None\n            if \"prev_padding_mask\" in saved_state:\n                prev_padding_mask = saved_state[\"prev_padding_mask\"]\n            padding_mask = SeqBoatUnit._append_prev_padding_mask(\n                padding_mask=padding_mask,\n                prev_padding_mask=prev_padding_mask,\n                batch_size=bsz,\n                seq_len=mx.size(1),\n            )\n            \n            if self.chunk_size < 0:\n                if self.local:\n                    max_len = self.truncation\n                    saved_state[\"prev_mx\"] = mx[:,-max_len:]\n                    saved_state[\"prev_key_padding_mask\"] = padding_mask\n                else:\n                    saved_state[\"prev_mx\"] = mx\n                    saved_state[\"prev_key_padding_mask\"] = padding_mask\n            else:\n                curr_len = mx.size(1) % self.chunk_size\n                if curr_len == 0:\n                    if \"prev_mx\" in saved_state:\n                        del saved_state[\"prev_mx\"]\n                        del saved_state[\"prev_key_padding_mask\"]\n                else:\n                    saved_state[\"prev_mx\"] = mx\n                    saved_state[\"prev_key_padding_mask\"] = padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            self._set_input_buffer(incremental_state, saved_state)\n\n        seq_len = mx.size(1)\n        ctx_len = seq_len\n        \n        tick=None\n        if self.rel_pos_type == 'rotary':\n            \n            if seq_len>self.max_pos:\n                self.max_pos = seq_len  \n                self.freqs_cis=precompute_freqs_cis(\n                    self.zdim, seq_len\n                ).to(x.device)\n            else:\n                self.freqs_cis = self.freqs_cis.to(x.device)\n            tick = self.freqs_cis[None,:seq_len,:].expand(bsz,-1,-1)\n            \n            \n        if self.chunk_size < 0:\n            # B x L x S -> B x 1 x L x S\n            mx = mx.unsqueeze(1)\n            if self.rel_pos_type == 'rotary':\n                tick = tick.unsqueeze(1)\n            if padding_mask is not None:\n                # B x L -> B x 1 x L\n                padding_mask = padding_mask.unsqueeze(1)\n        else:\n            if seq_len < self.chunk_size:\n                mx = mx.unsqueeze(1)\n                if self.rel_pos_type == 'rotary':\n                    tick = tick.unsqueeze(1)\n            else:\n                # B x L x S -> B x K x C x S\n                nc = seq_len // self.chunk_size\n                mx = mx.reshape(bsz, nc, self.chunk_size, -1)\n                if self.rel_pos_type == 'rotary':\n                    tick = tick.reshape(bsz, nc, self.chunk_size, -1)\n            if ctx_len < self.chunk_size:\n                if padding_mask is not None:\n                    padding_mask = padding_mask.unsqueeze(1)\n            else:\n                # B x L x S -> B x K x C x S\n                nc = ctx_len // self.chunk_size\n                if padding_mask is not None:\n                    # B x L -> B x K x C\n                    padding_mask = padding_mask.view(bsz, nc, self.chunk_size)\n        \n        \n        seq_len = mx.shape[-2]\n        cks= mx.shape[1]\n        \n        hu = self.hx_proj(mx)\n       \n        if self.latent_conf: \n            u, hx= torch.split(hu, [self.opt_dim, self.embed_dim], dim=-1)\n        else:\n            hx = hu\n\n\n        if not self.rel_pos_type == 'rotary' and not self.local_pos:\n            tick = torch.arange(1,1+seq_len,device =mx.device)[None,None,:,None].expand(bsz,cks,-1,-1)\n        if verbose:\n            span_tick = torch.arange(1,1+seq_len,device =mx.device)[None,None,:,None].expand(bsz,cks,-1,-1)\n            \n        if self.latent_conf:\n            result = self.q_router(u, act_bias=act_bias)\n            \n            px = result[\"probs\"]\n            loss  = [0]\n            if verbose:\n                print(\"temp \",result[\"temp\"].item())\n            if self.use_sigmoid:\n                q_p = px\n                mask_q = q_p>0.5\n            else:\n                q_p, q_ind = px.max(dim=-1, keepdim = True) # B x K x C x 1\n                mask_q = (q_ind==1) # B x K x C x 1\n                    \n            if padding_mask is not None:\n                pad_mask = ~padding_mask.unsqueeze(-1).bool()\n                mask_q *= pad_mask\n\n  \n            q_probs = (q_p * mask_q)\n\n            max_sl = mask_q.sum(-2).max()\n            density = max_sl/mask_q.shape[-2]\n\n            if verbose:\n                torch.save(q_probs, \"act_\"+str(layer_idx)+\".pt\")\n                print(\"ACT_Time\\t\", mask_q.float().mean(-2).mean().item(),\"\\t\",mask_q.float().mean(-2).std().item())\n            compress = density < self.density_threshold\n            index_q = None\n            if compress:\n                \n                index_q = (mask_q*torch.cumsum(mask_q.long(),dim=-2))\n\n                mx = compress_seq(mx,index_q.expand(-1,-1,-1,mx.shape[-1]), max_sl ,dim = -2) # mq, B, c\n                \n                if not self.local_pos:\n                    tick = compress_seq(tick,index_q.expand(-1,-1,-1,tick.shape[-1]), max_sl ,dim = -2) # mq, B, c\n                if verbose:\n                    span_tick = compress_seq(span_tick,index_q.expand(-1,-1,-1,span_tick.shape[-1]), max_sl ,dim = -2) # mq, B, c\n         \n                pad_mask = compress_seq(mask_q,index_q.expand(-1,-1, -1,1), max_sl ,dim = -2) # mq, B, 1\n                padding_mask = ~pad_mask.squeeze(-1)\n                \n                if attn_mask is not None:\n                    assert len(attn_mask.shape) == 2\n                    if max_sl==0:\n                        attn_mask = attn_mask[:1, :1]\n                    else:\n                        attn_mask = attn_mask[:max_sl, :max_sl]\n            else:\n                \n                padding_mask = ~mask_q.squeeze(-1)\n        else:\n            compress= False\n            loss = [0]\n        \n        if verbose:\n            wsize = self.window_size if self.local else seq_len\n            avg_span = calc_avg_attn_span(span_tick.squeeze(-1),win = wsize ,causal = attn_mask is not None)\n            print(\"AVG_SPAN: \", (avg_span.sum()/((avg_span!=0).sum()+1e-6)).item())\n        if not self.rel_pos_type == 'rotary' and not self.local_pos:\n            tick = tick.squeeze(-1)\n        # L x B x E\n        base = self.mx_proj(mx)\n            \n        z, r, v = torch.split(self.activation(base), [self.zdim, self.hdim, self.hdim], dim=-1)\n        # L x B x S -> L x B x 1 x S -> L x B x 2 x S\n        z = z.unsqueeze(-2) * self.gamma + self.beta\n\n        # L x B x 2 x S -> L x B x S\n        q, k = torch.unbind(z, dim=-2)\n    \n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if padding_mask is not None and padding_mask.dim() == 0:\n            padding_mask = None\n\n        v = self.hidden_dropout(v, batch_first=True)\n        \n        h = self.element_attention(q, k, v, padding_mask, attn_mask, tick, compress, seq_len)\n\n        h = self.h_proj(h*r)\n        if attn_mask is None:\n            h = self.dropout(h)\n\n        if self.latent_conf:\n            if compress:\n                h = extract(h, index_q)\n            h = h*q_probs\n        \n        h = h.view(bsz, -1, self.embed_dim).transpose(0, 1)\n        hx = hx.view(bsz, -1, self.embed_dim).transpose(0, 1)\n\n        if saved_state is not None:\n            slen = residual.shape[0]\n            hx = hx[-slen:,:,:]\n            h = h[-slen:,:,:]\n            \n        if attn_mask is None:\n            out = hx + h + residual\n        else:\n            out = self.dropout(hx + h) + residual\n            \n        out = self.activation(out)\n\n        if not self.prenorm:\n            out = self.norm(out)     \n\n        return out, None, loss\n\n    @staticmethod\n    def _append_prev_padding_mask(\n        padding_mask: Optional[Tensor],\n        prev_padding_mask: Optional[Tensor],\n        batch_size: int,\n        seq_len: int,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_padding_mask is not None and padding_mask is not None:\n            new_padding_mask = torch.cat([prev_padding_mask, padding_mask], dim=1)\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_padding_mask is not None:\n            filler = torch.zeros((batch_size, seq_len - prev_padding_mask.size(1)), device=prev_padding_mask.device)\n            new_padding_mask = torch.cat([prev_padding_mask, filler.bool()], dim=1)\n        elif padding_mask is not None:\n            filler = torch.zeros((batch_size, seq_len - padding_mask.size(1)), device=padding_mask.device)\n            new_padding_mask = torch.cat([filler.bool(), padding_mask], dim=1)\n        else:\n            new_padding_mask = prev_padding_mask\n        return new_padding_mask\n\n\nif __name__==\"__main__\":\n    torch.set_printoptions(precision=20)\n    torch.manual_seed(1)\n    layer1 = SeqBoatUnit(10,20,30,40, window_size=0, max_positions=4096,)\n    torch.manual_seed(1)\n    layer2 = SeqBoatUnit(10,20,30,40, window_size=100, max_positions=4096,)\n    x = torch.rand(4096,22,10)\n    print(layer1(x))\n\n    x = torch.rand(4096,22,10)\n    x.requires_grad_()\n    # x1=torch.clone(x)\n    # x1.requires_grad_()\n    torch.manual_seed(1)\n    out1  = layer1(x)[0]\n    torch.manual_seed(1)\n    out2 = layer2(x)[0]\n    def calc_max_diff(out1,out2):\n        nz= (out1!=out2).nonzero()\n        print((out1[nz]-out2[nz]).abs().max())\n    calc_max_diff(out1,out2)\n    #assert (out1==out2).all()\n    loss = out1.sum()\n    loss.backward()\n    xmx = torch.clone(layer1.mx_proj.weight.grad)\n    xg1 = torch.clone(x.grad)\n    loss = out2.sum()\n    loss.backward()\n    #assert (xg1==x.grad/2).all()\n    calc_max_diff(xg1,x.grad/2)\n\n    print(xmx[-1,1])\n    print(layer2.mx_proj.weight.grad[-1,1])\n    print(xmx[-1,1] == layer2.mx_proj.weight.grad[-1,1])\n    comp= xmx==layer2.mx_proj.weight.grad\n    print(comp)\n    assert comp.all()\n    ",
    "description": null,
    "url": null
}