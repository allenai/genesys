{
    "acronym": "clusteredattn",
    "title": "Fast Transformers with Clustered Attention",
    "seed_ids": [
        "reformer",
        "transformer"
    ],
    "s2id": "cd4ffe5e014601a3d6b64121355d29a730591490",
    "abstract": "Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them prohibitively expensive for large sequences. To address this, we propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. To further improve this approximation, we use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. This results in a model with linear complexity with respect to the sequence length for a fixed number of clusters. We evaluate our approach on two automatic speech recognition datasets and show that our model consistently outperforms vanilla transformers for a given computational budget. Finally, we demonstrate that our model can approximate arbitrarily complex attention distributions with a minimal number of clusters by approximating a pretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no loss in performance.",
    "authors": [
        "Apoorv Vyas",
        "Angelos Katharopoulos",
        "Franccois Fleuret"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "tldr": "Closed attention is proposed, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids, which results in a model with linear complexity with respect to the sequence length for a fixed number of clusters.",
    "citationCount": 125,
    "influentialCitationCount": 7,
    "code": "#\n# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/\n# Written by Angelos Katharopoulos <angelos.katharopoulos@idiap.ch>,\n# Apoorv Vyas <avyas@idiap.ch>\n#\n\n\"\"\"Implement clustered self attention.\"\"\"\n\nfrom math import sqrt\n\nimport torch\nimport torch.autograd\nfrom torch.nn import Dropout, Module\nfrom torch.nn.init import normal_\n\nfrom ..events import EventDispatcher\nfrom ..aggregate import clustered_aggregate, clustered_broadcast\nfrom ..clustering.hamming import cluster\nfrom ..hashing import compute_hashes\n\n\nclass _GroupQueries(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, Q, clusters, counts, lengths):\n        factors = 1./counts.float()\n        q_grouped = clustered_aggregate(Q, clusters, factors, lengths)\n        ctx.save_for_backward(clusters, counts, factors)\n\n        return q_grouped\n\n    @staticmethod\n    def backward(ctx, grad_q_grouped):\n        clusters, counts, factors = ctx.saved_tensors\n        grad_q = clustered_broadcast(grad_q_grouped, clusters, counts, factors)\n\n        return grad_q, None, None, None\n\n\nclass _BroadcastValues(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, v_grouped, clusters, counts, lengths):\n        factors = torch.ones_like(counts, dtype=v_grouped.dtype)\n        V = clustered_broadcast(v_grouped, clusters, counts, factors)\n        ctx.save_for_backward(clusters, counts, factors, lengths)\n\n        return V\n\n    @staticmethod\n    def backward(ctx, grad_v):\n        clusters, counts, factors, lengths = ctx.saved_tensors\n        grad_v_grouped = clustered_aggregate(grad_v, clusters, factors, lengths)\n\n        return grad_v_grouped, None, None, None\n\n\nclass ClusteredAttention(Module):\n    \"\"\"Use LSH and clustering in the resulting Hamming space to group queries\n    that will have minimal L2 distance from each other.\n\n    Given the queries, keys, and values as Q, K, and V respectively, we\n    first cluster the queries in \"C\" groups and compute the \"C\" query centroids\n    Q_c.\n\n    We now use to the centroids Q_c to compute the attention using:\n\n        V'_c = softmax(Q_c.mm(K.t()), dim=-1).mm(V).\n\n    Now the computed values V'_c are \"broadcasted\" back to the query members\n    of the corresponding cluster.\n\n    Arguments\n    ---------\n        clusters: How many clusters to group the queries into\n        iterations: The number of lloyd iterations to perform (default: 10)\n        bits: How many bits to use for the hash (default: 32)\n        hash_bias: If true, hamming distance proportional to L2 distance\n                   If false, hamming distance proportional to cosine distance\n                   (default: True)\n        softmax_temp: The temperature to use for the softmax attention.\n                      (default: 1/sqrt(d_keys) where d_keys is computed at\n                      runtime)\n        attention_dropout: The dropout rate to apply to the attention\n                           (default: 0.1)\n        event_dispatcher: str or EventDispatcher instance to be used by this\n                          module for dispatching events (default: the default\n                          global dispatcher)\n    \"\"\"\n    def __init__(self, clusters, iterations=10, bits=32,\n                 hash_bias=True, softmax_temp=None, attention_dropout=0.1,\n                 event_dispatcher=\"\"):\n        super(ClusteredAttention, self).__init__()\n        self.clusters = clusters\n        self.iterations = iterations\n        self.bits = bits\n        self.hash_bias = hash_bias\n        self.softmax_temp = softmax_temp\n        self.dropout = Dropout(attention_dropout)\n        self.event_dispatcher = EventDispatcher.get(event_dispatcher)\n\n    def _create_query_groups(self, Q, query_lengths):\n        N, H, L, E = Q.shape\n\n        # Compute the hashes for all the queries\n        planes = Q.new_empty((self.bits, E+1))\n        normal_(planes)\n        if not self.hash_bias:\n            planes[:, -1] = 0\n        hashes = compute_hashes(Q.view(N*H*L, E), planes).view(N, H, L)\n\n        # Cluster the hashes and return the cluster index per query\n        clusters, counts =  cluster(\n            hashes,\n            query_lengths._lengths.int(),\n            clusters=self.clusters,\n            iterations=self.iterations,\n            bits=self.bits\n        )\n        sorted_clusters, sorted_indx = torch.sort(clusters, dim=-1)\n        return (sorted_clusters, counts), sorted_indx\n\n    def _group_queries(self, Q, groups, lengths):\n        \"\"\"Aggregate the Qs based on the index of cluster they belong to. Make\n        sure to allow for gradient propagation backwards from the grouped\n        queries to each query.\"\"\"\n        q_grouped = _GroupQueries.apply(Q, *groups, lengths)\n        return q_grouped\n\n    def _broadcast_values(self, V, groups, lengths):\n        \"\"\"Broadcast the values back to the correct positions but make sure\n        that the gradient flows properly.\"\"\"\n        V_new = _BroadcastValues.apply(V.contiguous(), *groups, lengths)\n        return V_new\n\n    def forward(self, queries, keys, values, attn_mask, query_lengths,\n                key_lengths):\n        # Make sure that there is no attention mask\n        assert attn_mask.all_ones, (\"Clustered attention cannot use an \"\n                                    \"arbitrary attention mask.\")\n\n        queries = queries.permute(0,2,1,3).contiguous()\n        keys = keys.permute(0,2,1,3).contiguous()\n        values = values.permute(0,2,1,3).contiguous()\n\n        N, H, L, E = queries.shape\n        _, _, S, D = values.shape\n        softmax_temp = self.softmax_temp or 1./sqrt(E)\n\n        # Cluster the queries into groups\n        groups, sorted_indx = self._create_query_groups(queries, query_lengths)\n        # Re-organize queries so that first group belong to first cluster\n        # next to second cluster and so on. This improves kernel implementations.\n        # Note that this step is introduced after NeurIPS submission and\n        # now the complexity is O(N log(N)).\n        q_offset = torch.arange(N*H, device=queries.device).unsqueeze(-1) * L\n        q_flat = (sorted_indx.view(N*H, -1) + q_offset).reshape(-1)\n        s_queries = queries.reshape(-1, E).index_select(0, q_flat).view(N,H,L,E)\n\n        # Aggregate the re-arranged queries.\n        Q_grouped = self._group_queries(s_queries, groups, query_lengths._lengths.int())\n        # Compute the attention\n        QK = torch.einsum(\"nhle,nhse->nhls\", Q_grouped, keys)\n        QK = QK + key_lengths.additive_matrix[:, None, None, :]\n        A = self.dropout(torch.softmax(softmax_temp * QK, dim=-1))\n        V = torch.einsum(\"nhls,nhsd->nhld\", A, values)\n\n        # Broadcast grouped attention\n        V_broadcast = self._broadcast_values(V, groups, query_lengths._lengths.int())\n\n        # Reverse the previous mapping\n        rev_indx = torch.argsort(sorted_indx, dim=-1)\n        q_rev_flat = (rev_indx.view(N*H, -1) + q_offset).reshape(-1)\n        V_new = V_broadcast.reshape(-1, D).index_select(0, q_rev_flat).view(N,H,L,D)\n        V_new = V_new.permute(0, 2, 1, 3).contiguous()\n        return V_new\n\n\n",
    "description": null,
    "url": null
}