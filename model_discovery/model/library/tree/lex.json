{
    "acronym": "lex",
    "title": "A Length-Extrapolatable Transformer",
    "seed_ids": [
        "roformer",
        "bigbird",
        "linformer",
        "transformerxl",
        "transformer",
        "bert"
    ],
    "s2id": "9575afb5702bc33d7df14c48feeee5901ea00369",
    "abstract": "Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define attention resolution as an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We evaluate different Transformer variants with language modeling. Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings. The code will be available at https://aka.ms/LeX-Transformer.",
    "authors": [
        "Yutao Sun",
        "Li Dong",
        "Barun Patra",
        "Shuming Ma",
        "Shaohan Huang",
        "Alon Benhaim",
        "Vishrav Chaudhary",
        "Xia Song",
        "Furu Wei"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": "This paper introduces a relative position embedding to explicitly maximize attention resolution in Transformers and uses blockwise causal attention during inference for better resolution.",
    "citationCount": 70,
    "influentialCitationCount": 2,
    "code": "# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n\nimport torch\nimport torch.nn as nn\nimport math\n\nimport torch.nn.functional as F\nfrom apex.normalization import FusedLayerNorm as LayerNorm\n\nimport copy\n\n\ndef MultiwayWrapper(args, module, dim=0):\n    if args.multiway:\n        return MultiwayNetwork(module, dim=dim)\n    return module\n\n\ndef set_split_position(position):\n    def apply_fn(module):\n        if hasattr(module, \"split_position\"):\n            module.split_position = position\n\n    return apply_fn\n\n\nclass MultiwayNetwork(nn.Module):\n    def __init__(self, module, dim=0):\n        super().__init__()\n        self.dim = dim\n        self.A = module\n        self.B = copy.deepcopy(module)\n        self.B.reset_parameters()\n        self.split_position = -1\n\n    def forward(self, x, **kwargs):\n        if self.split_position == -1:\n            return self.A(x, **kwargs)\n        if self.split_position == 0:\n            return self.B(x, **kwargs)\n        x1, x2 = torch.split(\n            x,\n            [self.split_position, x.size(self.dim) - self.split_position],\n            dim=self.dim,\n        )\n        # x1, x2 = x[:self.split_position], x[self.split_position:]\n        y1, y2 = self.A(x1, **kwargs), self.B(x2, **kwargs)\n        return torch.cat([y1, y2], dim=self.dim)\n\n\ndef fixed_pos_embedding(x):\n    seq_len, dim = x.shape\n    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim) / dim))\n    sinusoid_inp = (\n        torch.einsum(\"i , j -> i j\", torch.arange(0, seq_len, dtype=torch.float), inv_freq).to(x)\n    )\n    return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)\n\n\nclass XPos(nn.Module):\n    def __init__(\n        self, head_dim, scale_base = 512\n    ):\n        super().__init__()\n        self.head_dim = head_dim\n        self.scale_base = scale_base\n        self.register_buffer(\n            \"scale\", (torch.arange(0, head_dim, 2) + 0.4 * head_dim) / (1.4 * head_dim)\n        )\n\n    def forward(self, len):\n        scale = self.scale ** (torch.arange(0, len, 1) - len // 2).to(self.scale).div(self.scale_base)[:, None]\n        sin, cos = fixed_pos_embedding(scale)\n        return (sin, cos, scale)\n    \n    \n\ndef rotate_every_two(x):\n    x1 = x[:, :, ::2]\n    x2 = x[:, :, 1::2]\n    x = torch.stack((-x2, x1), dim=-1)\n    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n\ndef duplicate_interleave(m):\n    \"\"\"\n    A simple version of `torch.repeat_interleave` for duplicating a matrix while interleaving the copy.\n    \"\"\"\n    dim0 = m.shape[0]\n    m = m.view(-1, 1)  # flatten the matrix\n    m = m.repeat(1, 2)  # repeat all elements into the 2nd dimension\n    m = m.view(dim0, -1)  # reshape into a matrix, interleaving the copy\n    return m\n\ndef apply_rotary_pos_emb(x, sin, cos, scale=1):\n    sin, cos = map(lambda t: duplicate_interleave(t * scale), (sin, cos))\n    # einsum notation for lambda t: repeat(t[offset:x.shape[1]+offset,:], \"n d -> () n () (d j)\", j=2)\n    return (x * cos) + (rotate_every_two(x) * sin)\n\nclass MultiheadAttention(nn.Module):\n    def __init__(\n        self,\n        args,\n        embed_dim,\n        num_heads,\n        dropout=0.0,\n        self_attention=False,\n        encoder_decoder_attention=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scaling = self.head_dim**-0.5\n        self.block_size = args.block_size\n        self.half_block_size = self.block_size // 2\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n        assert self.self_attention ^ self.encoder_decoder_attention\n\n        self.k_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=True))\n        self.v_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=True))\n        self.q_proj = MultiwayWrapper(args, nn.Linear(embed_dim, embed_dim, bias=True))\n        self.out_proj = MultiwayWrapper(\n            args, nn.Linear(embed_dim, embed_dim, bias=True)\n        )\n        self.inner_attn_ln = (\n            MultiwayWrapper(args, LayerNorm(self.embed_dim))\n            if subln and self.self_attention\n            else None\n        )\n        self.dropout_module = torch.nn.Dropout(dropout, inplace=True)\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        nn.init.constant_(self.out_proj.bias, 0.0)\n\n    def forward(\n        self,\n        query,\n        key,\n        value,\n        incremental_state=None,\n        key_padding_mask=None,\n        attn_mask=None,\n        rel_pos=None,\n    ):\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = tgt_len\n        assert embed_dim == self.embed_dim, f\"query dim {embed_dim} != {self.embed_dim}\"\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n\n        src_len, key_bsz, _ = key.size()\n        assert key_bsz == bsz, f\"{query.size(), key.size()}\"\n        assert value is not None\n        assert src_len, bsz == value.shape[:2]\n\n        q = self.q_proj(query) # tgt_len, bsz, dim\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n        q *= self.scaling\n        if self.block_size > 0 and tgt_len > self.block_size: # divide block\n            assert tgt_len % self.half_block_size == 0\n            if incremental_state is not None:\n                incremental_state[\"prev_key\"] = k.view(\n                    bsz, self.num_heads, -1, self.head_dim\n                )\n                incremental_state[\"prev_value\"] = v.view(\n                    bsz, self.num_heads, -1, self.head_dim\n                )\n\n            q = q.view(-1, self.half_block_size, bsz * self.num_heads, self.head_dim).transpose(1, 2).reshape(-1, self.half_block_size, self.head_dim)\n            k = F.pad(k, (0, 0, 0, 0, self.half_block_size, 0)).unfold(0, self.block_size, self.half_block_size).reshape(-1, self.head_dim, self.block_size).transpose(1, 2)\n            v = F.pad(v, (0, 0, 0, 0, self.half_block_size, 0)).unfold(0, self.block_size, self.half_block_size).reshape(-1, self.head_dim, self.block_size).transpose(1, 2)\n            bsz *= tgt_len // self.half_block_size\n            tgt_len = self.half_block_size\n            src_len = self.block_size\n            \n        else:\n            q = q.view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n            k = k.view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n            v = v.view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n            if incremental_state is not None:\n                if \"prev_key\" in incremental_state:\n                    prev_key = incremental_state[\"prev_key\"].view(\n                        bsz * self.num_heads, -1, self.head_dim\n                    )\n                    prev_value = incremental_state[\"prev_value\"].view(\n                        bsz * self.num_heads, -1, self.head_dim\n                    )\n                    k = torch.cat([prev_key, k], dim=1)\n                    v = torch.cat([prev_value, v], dim=1)\n                incremental_state[\"prev_key\"] = k.view(\n                    bsz, self.num_heads, -1, self.head_dim\n                )\n                incremental_state[\"prev_value\"] = v.view(\n                    bsz, self.num_heads, -1, self.head_dim\n                )\n                src_len = k.size(1)\n\n        if isinstance(rel_pos, tuple): # XPos implementation\n            sin, cos, scale = rel_pos\n            if self.self_attention:\n                k = apply_rotary_pos_emb(k, sin, cos, scale = 1 / scale)\n                q = apply_rotary_pos_emb(q, sin[-q.shape[1]:], cos[-q.shape[1]:], scale = scale[-q.shape[1]:])\n            else:\n                k = apply_rotary_pos_emb(k, sin[:k.shape[1]], cos[:k.shape[1]], scale = 1 / scale[:k.shape[1]])\n                q = apply_rotary_pos_emb(q, sin[k.shape[1]:], cos[k.shape[1]:], scale = scale[k.shape[1]:])\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        if attn_mask is not None:\n            attn_weights = torch.nan_to_num(attn_weights)\n            attn_mask = attn_mask.unsqueeze(0)\n            attn_weights += attn_mask\n\n        if key_padding_mask is not None:\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights.masked_fill(\n                key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                float(\"-inf\"),\n            )\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if isinstance(rel_pos, torch.Tensor):\n            rel_pos = rel_pos.view(attn_weights.size())\n            attn_weights = attn_weights + rel_pos\n\n        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).type_as(\n            attn_weights\n        )\n        attn_probs = self.dropout_module(attn_weights)\n        attn = torch.bmm(attn_probs, v)\n        if bsz > key_bsz: # merge block\n            attn = attn.view(-1, key_bsz * self.num_heads, self.half_block_size, self.head_dim).transpose(1, 2).reshape(-1, key_bsz, embed_dim)\n        else:\n            attn = attn.transpose(0, 1).reshape(-1, bsz, embed_dim)\n\n        if self.inner_attn_ln is not None:\n            attn = self.inner_attn_ln(attn)\n\n        attn = self.out_proj(attn)\n        attn_weights = attn_weights.view(\n            bsz, self.num_heads, tgt_len, src_len\n        ).transpose(1, 0)\n        return attn, attn_weights",
    "description": null,
    "url": null
}