{
    "acronym": "infiniteformer",
    "title": "\u221e-former: Infinite Memory Transformer",
    "seed_ids": [],
    "s2id": "74adfb26adfd9d7aac3818a9ebb1371e174ed207",
    "abstract": "N/A",
    "authors": [
        "Pedro Henrique Martins",
        "Zita Marinho",
        "Andr\u00e9 F. T. Martins"
    ],
    "venue": "Volume 1",
    "year": 2022,
    "tldr": "N/A",
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": "# coding: utf-8\n\"\"\"\nAttention modules\n\"\"\"\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom entmax import Sparsemax\nfrom basis_functions import BasisFunctions\n\nimport numpy as np\n\n\ntorch.autograd.set_detect_anomaly(True)\n\n\n\nclass GaussianBasisFunctions(BasisFunctions):\n    \"\"\"Function phi(t) = Gaussian(t; mu, sigma_sq).\"\"\"\n    def __init__(self, mu, sigma):\n        self.mu = mu.unsqueeze(0)\n        self.sigma = sigma.unsqueeze(0)\n\n    def __repr__(self):\n        return f\"GaussianBasisFunction(mu={self.mu}, sigma={self.sigma})\"\n\n    def __len__(self):\n        \"\"\"Number of basis functions.\"\"\"\n        return self.mu.size(1)\n\n    def _phi(self, t):\n        return 1. / math.sqrt(2 * math.pi) * torch.exp(-.5 * t**2)\n\n    def _Phi(self, t):\n        return .5 * (1 + torch.erf(t / math.sqrt(2)))\n\n    def _integrate_product_of_gaussians(self, mu, sigma_sq):\n        sigma = torch.sqrt(self.sigma ** 2 + sigma_sq)\n        return self._phi((mu - self.mu) / sigma) / sigma\n\n    def evaluate(self, t):\n        return self._phi((t - self.mu) / self.sigma) / self.sigma\n\n    def batch_evaluate(self, t):\n        t= t.repeat(self.mu.size(0),1) - self.mu.repeat(t.size(0),1).transpose(1,0)\n        return phi(t / self.sigma) / self.sigma\n\n    def integrate_t2_times_psi(self, a, b):\n        \"\"\"Compute integral int_a^b (t**2) * psi(t).\"\"\"\n        return (self.mu**2 + self.sigma**2) * (\n            self._Phi((b - self.mu) / self.sigma) - self._Phi((a - self.mu) / self.sigma)\n        ) - (\n            self.sigma * (b + self.mu) * self._phi((b - self.mu) / self.sigma)\n        ) + (\n            self.sigma * (a + self.mu) * self._phi((a - self.mu) / self.sigma)\n        )\n\n    def integrate_t_times_psi(self, a, b):\n        \"\"\"Compute integral int_a^b t * psi(t).\"\"\"\n        return self.mu * (\n            self._Phi((b - self.mu) / self.sigma) - self._Phi((a - self.mu) / self.sigma)\n        ) - self.sigma * (\n            self._phi((b - self.mu) / self.sigma) - self._phi((a - self.mu) / self.sigma)\n        )\n\n    def integrate_psi(self, a, b):\n        \"\"\"Compute integral int_a^b psi(t).\"\"\"\n        return self._Phi((b - self.mu) / self.sigma) - self._Phi((a - self.mu) / self.sigma)\n\n    def integrate_t2_times_psi_gaussian(self, mu, sigma_sq):\n        \"\"\"Compute integral int N(t; mu, sigma_sq) * t**2 * psi(t).\"\"\"\n        S_tilde = self._integrate_product_of_gaussians(mu, sigma_sq)\n        mu_tilde = (\n            self.mu * sigma_sq + mu * self.sigma ** 2\n        ) / (\n            self.sigma ** 2 + sigma_sq\n        )\n        sigma_sq_tilde = ((self.sigma ** 2) * sigma_sq) / (self.sigma ** 2 + sigma_sq)\n        return S_tilde * (mu_tilde ** 2 + sigma_sq_tilde)\n\n    def integrate_t_times_psi_gaussian(self, mu, sigma_sq):\n        \"\"\"Compute integral int N(t; mu, sigma_sq) * t * psi(t).\"\"\"\n        S_tilde = self._integrate_product_of_gaussians(mu, sigma_sq)\n        mu_tilde = (\n            self.mu * sigma_sq + mu * self.sigma ** 2\n        ) / (\n            self.sigma ** 2 + sigma_sq\n        )\n        return S_tilde * mu_tilde\n\n    def integrate_psi_gaussian(self, mu, sigma_sq):\n        \"\"\"Compute integral int N(t; mu, sigma_sq) * psi(t).\"\"\"\n        return self._integrate_product_of_gaussians(mu, sigma_sq)\n\nclass ContinuousSoftmaxFunction(torch.autograd.Function):\n\n    @classmethod\n    def _expectation_phi_psi(cls, ctx, mu, sigma_sq):\n        \"\"\"Compute expectation of phi(t) * psi(t).T under N(mu, sigma_sq).\"\"\"\n        num_basis = [len(basis_functions) for basis_functions in ctx.psi]\n        total_basis = sum(num_basis)\n        V = torch.zeros((mu.shape[0], 2, total_basis), dtype=ctx.dtype,device=ctx.device)\n        offsets = torch.cumsum(torch.IntTensor(num_basis).to(ctx.device), dim=0)\n        start = 0\n        for j, basis_functions in enumerate(ctx.psi):\n            V[:, 0, start:offsets[j]] = basis_functions.integrate_t_times_psi_gaussian(mu, sigma_sq)\n            V[:, 1, start:offsets[j]] = basis_functions.integrate_t2_times_psi_gaussian(mu, sigma_sq)\n            start = offsets[j]\n        return V\n\n    @classmethod\n    def _expectation_psi(cls, ctx, mu, sigma_sq):\n        \"\"\"Compute expectation of psi under N(mu, sigma_sq).\"\"\"\n        num_basis = [len(basis_functions) for basis_functions in ctx.psi]\n        total_basis = sum(num_basis)\n        r = torch.zeros(mu.shape[0], total_basis, dtype=ctx.dtype, device=ctx.device)\n        offsets = torch.cumsum(torch.IntTensor(num_basis).to(ctx.device), dim=0)\n        start = 0\n        for j, basis_functions in enumerate(ctx.psi):\n            r[:, start:offsets[j]] = basis_functions.integrate_psi_gaussian(mu, sigma_sq)\n            start = offsets[j]\n        return r\n\n    @classmethod\n    def _expectation_phi(cls, ctx, mu, sigma_sq):\n        \"\"\"Compute expectation of phi under N(mu, sigma_sq).\"\"\"\n        v = torch.zeros(mu.shape[0], 2, dtype=ctx.dtype, device=ctx.device)\n        v[:, 0] = mu.squeeze(1)\n        v[:, 1] = (mu**2 + sigma_sq).squeeze(1)\n        return v\n\n    @classmethod\n    def forward(cls, ctx, theta, psi):\n        # We assume a Gaussian.\n        # We have:\n        # theta = [mu/sigma**2, -1/(2*sigma**2)],\n        # phi(t) = [t, t**2],\n        # p(t) = Gaussian(t; mu, sigma**2).\n        ctx.dtype = theta.dtype\n        ctx.device = theta.device\n        ctx.psi = psi\n        sigma_sq = (-.5 / theta[:, 1]).unsqueeze(1)\n        mu = theta[:, 0].unsqueeze(1) * sigma_sq\n        r = cls._expectation_psi(ctx, mu, sigma_sq)\n        ctx.save_for_backward(mu, sigma_sq, r)\n        return r\n\n    @classmethod\n    def backward(cls, ctx, grad_output):\n        mu, sigma_sq, r = ctx.saved_tensors\n        J = cls._expectation_phi_psi(ctx, mu, sigma_sq)\n        e_phi = cls._expectation_phi(ctx, mu, sigma_sq)\n        e_psi = cls._expectation_psi(ctx, mu, sigma_sq)\n        J -= torch.bmm(e_phi.unsqueeze(2), e_psi.unsqueeze(1))\n        grad_input = torch.matmul(J, grad_output.unsqueeze(2)).squeeze(2)\n        return grad_input, None\n\n\nclass ContinuousSoftmax(nn.Module):\n    def __init__(self, psi=None):\n        super(ContinuousSoftmax, self).__init__()\n        self.psi = psi\n\n    def forward(self, theta):\n        return ContinuousSoftmaxFunction.apply(theta, self.psi)\n\nclass ContinuousSparsemaxFunction(torch.autograd.Function):\n\n    @classmethod\n    def _integrate_phi_times_psi(cls, ctx, a, b):\n        \"\"\"Compute integral int_a^b phi(t) * psi(t).T.\"\"\"\n        num_basis = [len(basis_functions) for basis_functions in ctx.psi]\n        total_basis = sum(num_basis)\n        V = torch.zeros((a.shape[0], 2, total_basis), dtype=ctx.dtype, device=ctx.device)\n        offsets = torch.cumsum(torch.IntTensor(num_basis).to(ctx.device), dim=0)\n        start = 0\n        for j, basis_functions in enumerate(ctx.psi):\n            V[:, 0, start:offsets[j]] = basis_functions.integrate_t_times_psi(a, b)\n            V[:, 1, start:offsets[j]] = basis_functions.integrate_t2_times_psi(a, b)\n            start = offsets[j]\n        return V\n\n    @classmethod\n    def _integrate_psi(cls, ctx, a, b):\n        \"\"\"Compute integral int_a^b psi(t).\"\"\"\n        num_basis = [len(basis_functions) for basis_functions in ctx.psi]\n        total_basis = sum(num_basis)\n        v = torch.zeros(a.shape[0], total_basis, dtype=ctx.dtype, device=ctx.device)\n        offsets = torch.cumsum(torch.IntTensor(num_basis).to(ctx.device), dim=0)\n        start = 0\n        for j, basis_functions in enumerate(ctx.psi):\n            v[:, start:offsets[j]] = basis_functions.integrate_psi(a, b)\n            start = offsets[j]\n        return v\n\n    @classmethod\n    def _integrate_phi(cls, ctx, a, b):\n        \"\"\"Compute integral int_a^b phi(t).\"\"\"\n        v = torch.zeros(a.shape[0], 2, dtype=ctx.dtype, device=ctx.device)\n        v[:, 0] = ((b**2 - a**2) / 2).squeeze(1)\n        v[:, 1] = ((b**3 - a**3) / 3).squeeze(1)\n        return v\n\n    @classmethod\n    def forward(cls, ctx, theta, psi):\n        # We assume a truncated parabola.\n        # We have:\n        # theta = [mu/sigma**2, -1/(2*sigma**2)],\n        # phi(t) = [t, t**2],\n        # p(t) = [theta.dot(phi(t)) - A]_+,\n        # supported on [mu - a, mu + a].\n        ctx.dtype = theta.dtype\n        ctx.device = theta.device\n        ctx.psi = psi\n        sigma = torch.sqrt(-.5 / theta[:, 1])\n        mu = theta[:, 0] * sigma ** 2\n        A = -.5 * (3. / (2 * sigma)) ** (2. / 3)\n        a = torch.sqrt(-2 * A) * sigma\n        A += mu ** 2 / (2 * sigma ** 2)\n        left = (mu - a).unsqueeze(1)\n        right = (mu + a).unsqueeze(1)\n        V = cls._integrate_phi_times_psi(ctx, left, right)\n        u = cls._integrate_psi(ctx, left, right)\n        r = torch.matmul(theta.unsqueeze(1), V).squeeze(1) - A.unsqueeze(1) * u\n        ctx.save_for_backward(mu, a, V, u)\n        return r\n\n    @classmethod\n    def backward(cls, ctx, grad_output):\n        mu, a, V, u = ctx.saved_tensors\n        # J.T = int_{-a}^{+a} phi(t+mu)*psi(t+mu).T\n        # - (int_{-a}^{+a} phi(t+mu)) * (int_{-a}^{+a} psi(t+mu).T) / (2*a)\n        left = (mu - a).unsqueeze(1)\n        right = (mu + a).unsqueeze(1)\n        i_phi = cls._integrate_phi(ctx, left, right)\n        ger = torch.bmm(i_phi.unsqueeze(2), u.unsqueeze(1))\n        # ger = torch.einsum('bi,bj->bij', (i_phi, u))\n        J = V - ger / (2 * a.unsqueeze(1).unsqueeze(2))\n        grad_input = torch.matmul(J, grad_output.unsqueeze(2)).squeeze(2)\n        return grad_input, None\n\n\nclass ContinuousSparsemax(nn.Module):\n    def __init__(self, psi=None):\n        super(ContinuousSparsemax, self).__init__()\n        self.psi = psi\n\n    def forward(self, theta):\n        return ContinuousSparsemaxFunction.apply(theta, self.psi)\n\n\nclass LongTermAttention(nn.Module):\n    def __init__(self, head_size: int , length: int, target_len: int,  attn_func: str, attn_num_basis: int,\n                  attn_drop: float, infinite_memory: bool, n_layers: int,\n                  n_heads: int, d_model: int, mask: bool, mask_type: str,\n                  kl_regularizer, sigma_0, mu_0, **kwargs):\n\n        super(LongTermAttention, self).__init__()\n\n        self.device = 'cuda'\n\n        self.length = length #memory length\n\n        self.target_len = target_len #target length / transformer length\n\n        self.head_size = head_size\n\n        self.attn_num_basis = attn_num_basis\n\n        self.attn_func = attn_func # normalizing function\n\n        self.n_head = n_heads\n        \n        self.proj_query = nn.Linear(n_heads*head_size, n_heads*head_size, bias=False)\n        self.proj_key = nn.Linear(n_heads*head_size, n_heads*head_size, bias=False)\n        self.proj_value = nn.Linear(n_heads*head_size, n_heads*head_size, bias=False)\n\n        self.attn_dropout = nn.Dropout(attn_drop)\n\n        self.attn_out = nn.Linear(n_heads*head_size, d_model, bias=False)\n\n        self.mask=mask\n        self.mask_type=mask_type\n        self.kl_regularizer = kl_regularizer\n        self.sigma_0 = sigma_0\n        self.mu_0 = mu_0\n\n        if self.mask and not self.share_mask:\n            if self.mask_type=='cnn':\n                self.mask_net=torch.nn.Conv1d(n_heads*head_size, n_heads*head_size,3,padding=1)\n\n        self.infinite_memory = infinite_memory # whether the memory is infinite\n\n        self.nb_samples=512 # number of samples used for update\n        self.tau = 0.75 #compressing factor\n\n        self.x_past=None # previous memory vectors\n        self.B_past=None # previous coefficient matrix\n\n        self.ridge_penalty=1 # ridge penalty\n        padding = True\n\n        self.spacing='linear'\n\n        def compute_G(l, psi, positions, padding=True):\n\n            F = torch.zeros(self.attn_num_basis, positions.size(0))\n\n            basis_functions = psi\n            F[:, :] = basis_functions.evaluate(positions.unsqueeze(1)).t()\n\n            I = torch.eye(self.attn_num_basis)\n            G = F.t().matmul((F.matmul(F.t()) + self.ridge_penalty * I).inverse())\n\n            if padding:\n                if l % 2:\n                    G = G[((l-1)//2):(-(l-1)//2), :]\n                else:\n                    G = G[(l//2):-(l//2), :]\n\n            return G.to(self.device)\n\n        self.mu = nn.Linear(attn_num_basis, 1, bias=False)\n        self.sigma = nn.Linear(attn_num_basis, 1, bias=False)\n        self.softplus = torch.nn.Softplus()\n\n        # normalizing function\n        if attn_func == 'softmax':\n            self.transform = ContinuousSoftmax(psi=None)\n        elif attn_func == 'sparsemax':\n            self.transform = ContinuousSparsemax(psi=None)\n        else:\n            assert False\n\n        # get basis functions psi\n        sigmas = [.01,.05] # basis function sigmas\n        if attn_num_basis % len(sigmas):\n            attn_num_basis += (len(sigmas) - attn_num_basis % len(sigmas))\n\n        self.psi=[None]\n        self.Gs=[None for _ in range(length+1)]\n        self.psi=[None]\n        lengths=[]\n\n        for i in range(length):\n            self.psi.append([])\n            if (i+1)%target_len==0:\n                lengths.append(i+1)\n        if length not in lengths:\n            lengths.append(length)\n            \n        for l in lengths:\n            # get positions for memory vectors\n            self.add_gaussian_basis_functions(self.psi[l], attn_num_basis, sigmas, device=self.device)\n\n            if self.spacing=='linear':\n                if padding:\n                    if l % 2:\n                        shift = 1 / float(l)\n                        positions = torch.linspace(-.5+shift, 1.5-shift, 2*l-1).to(self.device)\n                    else:\n                        shift = 1 / float(2*l)\n                        positions = torch.linspace(-.5+shift, 1.5-shift, 2*l).to(self.device)\n                else:\n                    shift = 1 / float(2*l)\n                    positions = torch.linspace(shift, 1-shift, l).to(self.device)\n            elif self.spacing=='log':\n                if padding:\n                    if l % 2:\n                        shift = 1 / float(l)\n                        positions = torch.linspace(-.5+shift, 1.5-shift, 2*l-1).to(self.device)\n                    else:\n                        shift = 1 / float(2*l)\n                        positions = torch.linspace(-.5+shift, 1.5-shift, 2*l).to(self.device)\n\n                    pos = np.e**(np.log(1+1)*torch.arange(1,length+1)/length)-1\n                    positions = torch.cat([positions[:int(l/2)],pos.to(self.device),positions[-int(l/2):]])\n\n                else:\n                    positions = np.e**(np.log(1+1)*torch.arange(1,length+1)/length)-1\n        \n            # compute basis functions\n            self.Gs[l]=compute_G(l, self.psi[l][0], positions, padding=padding) # [L,N]\n            self.positions = positions[int(l/2):-int(l/2)]\n\n        # compute samples for memory update\n        if self.infinite_memory:\n            tm_tau = torch.arange(1,self.nb_samples+1).float()\n            tm_l = torch.arange(self.nb_samples+1,length+self.nb_samples+1).float()\n            tm_tau = tm_tau*self.tau/self.nb_samples # positions of old vectors\n            tm_l = self.tau + (1-self.tau)*(tm_l-self.nb_samples)/length # positions of new vectors\n\n            positions_inf = torch.cat([tm_tau, tm_l],0).to(self.device) # positions\n\n            if padding:\n                if l % 2:\n                    shift = 1 / float(length+self.nb_samples)\n                    positions_pad_ = torch.linspace(-.5+shift, 0, 2*(length+self.nb_samples)-1).to(self.device)\n                else:\n                    shift = 1 / float(2*length+self.nb_samples)\n                    positions_pad = torch.linspace(-.5+shift, 1.5-shift, 2*(length+self.nb_samples)).to(self.device)\n                positions_pad_ = torch.FloatTensor([i for i in positions_pad if i<0]).to(self.device)\n                positions_pad__ = torch.FloatTensor([i for i in positions_pad if i>1]).to(self.device)\n                positions_inf = torch.cat([positions_pad_,positions_inf,positions_pad__], dim=0)\n\n            self.samples=None\n            for t in tm_tau:\n                if self.samples is None:\n                    self.samples = self.psi[length][0].evaluate(t/self.tau)\n                else:\n                    self.samples = torch.cat([self.samples,self.psi[length][0].evaluate(t/self.tau)], dim=0)\n            # compute G for the infinite case\n            self.G_inf = compute_G(self.nb_samples+length, self.psi[length][0], positions_inf, padding=padding) #[L+nb_samples,N]\n\n\n        elif self.attn_func=='sparsemax':\n            self.sparsemax = Sparsemax(dim=-1)\n\n\n    def add_gaussian_basis_functions(self, psi, nb_basis, sigmas, device):\n        mu, sigma = torch.meshgrid(torch.linspace(0, 1, nb_basis // len(sigmas)), torch.Tensor(sigmas))\n        mu = mu.flatten().to(device)\n        sigma = sigma.flatten().to(device)\n        self.basis_mu=mu\n        self.basis_sigma=sigma\n        assert mu.size(0) == nb_basis\n        psi.append(GaussianBasisFunctions(mu=mu, sigma=sigma))\n\n    def score(self, query, keys):\n        query = query/ (self.d_head ** 0.5) # divide by sqrt(d_head) [B,h,q,d]\n        keys = keys.transpose(-1, -2) #[B,h,d,N]\n        scores = torch.matmul(query, keys) #[B,h,q,N] \n        return scores\n\n    def value_function(self, x, inf=False, layer_n=None, doc_final=False):\n        if inf:\n            G = self.G_inf # [nb_sample+L,N]\n        else:\n            G = self.Gs[x.size(-1)] # [L,N]\n\n        B = torch.matmul(x, G) # [B,e,N]\n        B = B.permute(0,2,1) # [B,N,e]\n\n        return B\n\n    def update_inf(self, x, layer_n=None, doc_final=False):\n        if self.B_past is not None:       \n            xm_tau = self.B_past.transpose(-1,-2).matmul(self.samples.transpose(0,1)) # [B,e,nb_samples]\n            \n            x = torch.cat([xm_tau,x], dim=2) # [B,e,nb_samples+L]\n            B = self.value_function(x, inf=True, layer_n=layer_n, doc_final=doc_final) # [B,N,e]\n        else:\n            B = self.value_function(x, layer_n=layer_n, doc_final=doc_final)\n        \n        self.B_past=B.detach()\n        self.x_past=x\n        return B\n\n\n    def forward(self, k, q,new_doc=False, layer_n=0, reg_mask=None, doc_final=False):\n        \n        batch_size = k.size(1) #batch size\n        qlen = q.size(0) #query length\n        klen = k.size(0) #key length\n        self.d_head = self.head_size #head size\n\n        # clean memory if going through different document\n        if new_doc:\n            self.B_past=None \n            self.x_past=None\n\n        k = k.permute(1,2,0) # [B,e,L]\n        if self.mask and not self.share_mask:\n            reg_mask=torch.sigmoid(self.mask_net(k))\n            k = k*reg_mask\n\n        # perform memory update\n        if self.infinite_memory:\n            B = self.update_inf(k, layer_n=layer_n, doc_final=doc_final)\n        else: \n            B = self.value_function(k.view(klen,batch_size,-1, layer_n=layer_n, doc_final=doc_final)) # [B,N,e]\n\n        \n        query = q.permute(1,0,2)\n        keys = self.proj_key(B)\n        values = self.proj_value(B)\n\n        query = query.view(batch_size,qlen,self.n_head,self.d_head).transpose(1,2) # [B,h,q,d]\n        keys = keys.view(batch_size,self.attn_num_basis,self.n_head,self.d_head).transpose(1,2) # [B,h,N,d]\n        values = values.view(batch_size,self.attn_num_basis,self.n_head,self.d_head).transpose(1,2) # [B,h,N,d]\n        \n        #compute scores\n        scores = self.score(query, keys) #[B,h,q,N] \n\n        mu = torch.sigmoid(self.mu(scores)) #[B,h,q] \n        sigma_sq = self.softplus(self.sigma(scores))#[B,h,q] \n        \n        mu = mu.view(-1)\n        sigma_sq = torch.clamp(sigma_sq, min=1e-6).view(-1)\n\n        if self.kl_regularizer:\n            sigma_0_sq = self.sigma_0**2\n            if self.mu_0 is None:\n                kl_reg = 1/2 * ( sigma_sq.view(batch_size,-1) / sigma_0_sq - \n                            torch.log(sigma_sq.view(batch_size,-1)/sigma_0_sq) -1 )\n            else:\n                kl_reg = 1/2 * ( sigma_sq.view(batch_size,-1) / sigma_0_sq - \n                            torch.log(sigma_sq.view(batch_size,-1)/sigma_0_sq) -1 +\n                            (mu.view(batch_size,-1) - self.mu_0)**2 / sigma_0_sq )\n\n\n        theta = torch.zeros(batch_size*self.n_head*qlen, 2, device=self.device)  # [B*h*q, 2]\n        theta[:, 0] = mu / sigma_sq\n        theta[:, 1] = -1. / (2. * sigma_sq)\n\n        # get basis functions\n        self.transform.psi = self.psi[klen]\n\n        #compute basis functions expectation\n        r = self.transform(theta) # [B*h*q,N] \n\n        r = r.view(batch_size,self.n_head,qlen,self.attn_num_basis).permute(0,1,3,2) # [B,h,N,q]\n\n        values = values.transpose(-1,-2) # [B,h,d,N]\n        \n        context = torch.matmul(values,r) # [B,h,d,q]\n\n        context = context.permute(3,0,1,2) # [q,B,h,d]\n        context = context.contiguous().view(qlen,batch_size,self.n_head*self.d_head) # [q,B,e]\n\n        context = self.attn_out(context)\n\n        if self.kl_regularizer:\n            return context, kl_reg\n        else:\n            return context\n        ",
    "description": null,
    "url": null
}