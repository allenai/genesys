{
    "acronym": "alibi",
    "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
    "seed_ids": [
        "gpt3",
        "longformer",
        "transformer"
    ],
    "s2id": "9ca329408813d209b1dcb36936f7f9cba82506bd",
    "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",
    "authors": [
        "Ofir Press",
        "Noah A. Smith",
        "M. Lewis"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": "This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi).",
    "citationCount": 461,
    "influentialCitationCount": 62,
    "code": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom fairseq import utils\nfrom fairseq.distributed import fsdp_wrap\nfrom fairseq.models import (\n    FairseqIncrementalDecoder,\n)\nfrom fairseq.modules import (\n    AdaptiveSoftmax,\n    BaseLayer,\n    FairseqDropout,\n    LayerDropModuleList,\n    LayerNorm,\n    PositionalEmbedding,\n    TransformerDecoderLayer,\n)\nfrom fairseq.modules.checkpoint_activations import checkpoint_wrapper\nfrom fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\nfrom torch import Tensor\n\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1024\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\nDEFAULT_MIN_PARAMS_TO_WRAP = int(1e8)\n\n\nclass TransformerDecoder(FairseqIncrementalDecoder):\n    \"\"\"\n    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`TransformerDecoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    \"\"\"\n\n    def __init__(\n        self,\n        args,\n        dictionary,\n        embed_tokens,\n        no_encoder_attn=False,\n        output_projection=None,\n    ):\n        self.args = args\n        super().__init__(dictionary)\n        self.register_buffer(\"version\", torch.Tensor([3]))\n        self._future_mask = torch.empty(0)\n\n        self.dropout_module = FairseqDropout(\n            args.dropout, module_name=self.__class__.__name__\n        )\n        self.decoder_layerdrop = args.decoder_layerdrop\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n\n        input_embed_dim = embed_tokens.embedding_dim\n        embed_dim = args.decoder_embed_dim\n        self.embed_dim = embed_dim\n        self.output_embed_dim = args.decoder_output_dim\n\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n\n        self.embed_tokens = embed_tokens\n\n        self.embed_scale = 1.0 if args.no_scale_embedding else math.sqrt(embed_dim)\n\n        if not args.adaptive_input and args.quant_noise_pq > 0:\n            self.quant_noise = apply_quant_noise_(\n                nn.Linear(embed_dim, embed_dim, bias=False),\n                args.quant_noise_pq,\n                args.quant_noise_pq_block_size,\n            )\n        else:\n            self.quant_noise = None\n\n        self.project_in_dim = (\n            Linear(input_embed_dim, embed_dim, bias=False)\n            if embed_dim != input_embed_dim\n            else None\n        )\n        self.embed_positions = (\n            PositionalEmbedding(\n                self.max_target_positions,\n                embed_dim,\n                self.padding_idx,\n                learned=args.decoder_learned_pos,\n            )\n            if not args.no_token_positional_embeddings\n            else None\n        )\n\n        if getattr(args, \"layernorm_embedding\", False):\n            self.layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n        self.cross_self_attention = getattr(args, \"cross_self_attention\", False)\n\n        if self.decoder_layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.decoder_layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n        self.layers.extend(\n            [\n                self.build_decoder_layer(args, no_encoder_attn)\n                for _ in range(args.decoder_layers)\n            ]\n        )\n        self.num_layers = len(self.layers)\n\n        if args.decoder_normalize_before and not getattr(\n            args, \"no_decoder_final_norm\", False\n        ):\n            self.layer_norm = LayerNorm(embed_dim)\n        else:\n            self.layer_norm = None\n\n        self.project_out_dim = (\n            Linear(embed_dim, self.output_embed_dim, bias=False)\n            if embed_dim != self.output_embed_dim and not args.tie_adaptive_weights\n            else None\n        )\n\n        self.adaptive_softmax = None\n        self.output_projection = output_projection\n        if self.output_projection is None:\n            self.build_output_projection(args, dictionary, embed_tokens)\n        \n        \n        def get_slopes(n):\n            def get_slopes_power_of_2(n):\n                start = (2**(-2**-(math.log2(n)-3)))\n                ratio = start\n                return [start*ratio**i for i in range(n)]\n\n            if math.log2(n).is_integer():\n                return get_slopes_power_of_2(n)                   #In the paper, we only train models that have 2^a heads for some a. This function has\n            else:                                                 #some good properties that only occur when the input is a power of 2. To maintain that even\n                closest_power_of_2 = 2**math.floor(math.log2(n))  #when the number of heads is not a power of 2, we use this workaround. \n                return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2*closest_power_of_2)[0::2][:n-closest_power_of_2]\n \n        maxpos = args.tokens_per_sample\n        attn_heads = args.decoder_attention_heads\n        self.slopes = torch.Tensor(get_slopes(attn_heads))\n        #In the next line, the part after the * is what constructs the diagonal matrix (right matrix in Figure 3 in the paper). \n        #If you run it you'll see that it doesn't exactly print out the same matrix as we have in Figure 3, but one where all rows are identical.\n        #This works because the softmax operation is invariant to translation, and our bias functions are always linear. \n        self.alibi = self.slopes.unsqueeze(1).unsqueeze(1) * torch.arange(maxpos).unsqueeze(0).unsqueeze(0).expand(attn_heads, -1, -1)\n        self.alibi = self.alibi.view(attn_heads, 1, maxpos)\n        self.alibi = self.alibi.repeat(args.max_tokens//maxpos, 1, 1)  # batch_size, 1, 1\n\n\n    def build_output_projection(self, args, dictionary, embed_tokens):\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dictionary),\n                self.output_embed_dim,\n                utils.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.adaptive_softmax_dropout,\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\n                factor=args.adaptive_softmax_factor,\n                tie_proj=args.tie_adaptive_proj,\n            )\n        elif self.share_input_output_embed:\n            self.output_projection = nn.Linear(\n                self.embed_tokens.weight.shape[1],\n                self.embed_tokens.weight.shape[0],\n                bias=False,\n            )\n            self.output_projection.weight = self.embed_tokens.weight\n        else:\n            self.output_projection = nn.Linear(\n                self.output_embed_dim, len(dictionary), bias=False\n            )\n            nn.init.normal_(\n                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n            )\n        num_base_layers = getattr(args, \"base_layers\", 0)\n        for i in range(num_base_layers):\n            self.layers.insert(((i+1) * args.decoder_layers) // (num_base_layers + 1), BaseLayer(args))\n\n\n    def build_decoder_layer(self, args, no_encoder_attn=False):\n        layer = TransformerDecoderLayer(args, no_encoder_attn)\n        checkpoint = getattr(args, \"checkpoint_activations\", False)\n        if checkpoint:\n            offload_to_cpu = getattr(args, \"offload_activations\", False)\n            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)\n        # if we are checkpointing, enforce that FSDP always wraps the\n        # checkpointed layer, regardless of layer size\n        min_params_to_wrap = (\n            getattr(args, \"min_params_to_wrap\", DEFAULT_MIN_PARAMS_TO_WRAP)\n            if not checkpoint else 0\n        )\n        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)\n        return layer\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out: Optional[Dict[str, List[Tensor]]] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        features_only: bool = False,\n        full_context_alignment: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n        src_lengths: Optional[Any] = None,\n        return_all_hiddens: bool = False,\n    ):\n        \"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (optional): output from the encoder, used for\n                encoder-side attention, should be of size T x B x C\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n            features_only (bool, optional): only return features without\n                applying output layer (default: False).\n            full_context_alignment (bool, optional): don't apply\n                auto-regressive mask to self-attention (default: False).\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n\n        x, extra = self.extract_features(\n            prev_output_tokens,\n            encoder_out=encoder_out,\n            incremental_state=incremental_state,\n            full_context_alignment=full_context_alignment,\n            alignment_layer=alignment_layer,\n            alignment_heads=alignment_heads,\n        )\n\n        if not features_only:\n            x = self.output_layer(x)\n        return x, extra\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        encoder_out: Optional[Dict[str, List[Tensor]]],\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        full_context_alignment: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n    ):\n        return self.extract_features_scriptable(\n            prev_output_tokens,\n            encoder_out,\n            incremental_state,\n            full_context_alignment,\n            alignment_layer,\n            alignment_heads,\n        )\n\n    \"\"\"\n    A scriptable subclass of this class has an extract_features method and calls\n    super().extract_features, but super() is not supported in torchscript. A copy of\n    this function is made to be used in the subclass instead.\n    \"\"\"\n\n    def extract_features_scriptable(\n        self,\n        prev_output_tokens,\n        encoder_out: Optional[Dict[str, List[Tensor]]],\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        full_context_alignment: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n    ):\n        \"\"\"\n        Similar to *forward* but only return features.\n\n        Includes several features from \"Jointly Learning to Align and\n        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n\n        Args:\n            full_context_alignment (bool, optional): don't apply\n                auto-regressive mask to self-attention (default: False).\n            alignment_layer (int, optional): return mean alignment over\n                heads at this layer (default: last layer).\n            alignment_heads (int, optional): only average alignment over\n                this many heads (default: all heads).\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n        bs, slen = prev_output_tokens.size()\n        if alignment_layer is None:\n            alignment_layer = self.num_layers - 1\n\n        enc: Optional[Tensor] = None\n        padding_mask: Optional[Tensor] = None\n        if encoder_out is not None and len(encoder_out[\"encoder_out\"]) > 0:\n            enc = encoder_out[\"encoder_out\"][0]\n            assert (\n                enc.size()[1] == bs\n            ), f\"Expected enc.shape == (t, {bs}, c) got {enc.shape}\"\n        if encoder_out is not None and len(encoder_out[\"encoder_padding_mask\"]) > 0:\n            padding_mask = encoder_out[\"encoder_padding_mask\"][0]\n\n        # embed positions\n        positions = None\n        #if self.embed_positions is not None:\n        #    positions = self.embed_positions(\n        #        prev_output_tokens, incremental_state=incremental_state\n        #    )\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n        #    if positions is not None:\n        #        positions = positions[:, -1:]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.quant_noise is not None:\n            x = self.quant_noise(x)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        #if positions is not None:\n        #    x += positions\n\n        if self.layernorm_embedding is not None:\n            x = self.layernorm_embedding(x)\n\n        x = self.dropout_module(x)\n\n        #We move the mask construction here because its slightly more efficient.\n        if incremental_state is None and not full_context_alignment:\n             self_attn_mask = self.buffered_future_mask(x)\n        else:\n             self_attn_mask = None\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        self_attn_padding_mask: Optional[Tensor] = None\n        if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():\n            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)\n\n        # decoder layers\n        attn: Optional[Tensor] = None\n        inner_states: List[Optional[Tensor]] = [x]\n        for idx, layer in enumerate(self.layers):\n            x, layer_attn, _ = layer(\n                x,\n                enc,\n                padding_mask,\n                incremental_state,\n                self_attn_mask=self_attn_mask,\n                self_attn_padding_mask=self_attn_padding_mask,\n                need_attn=bool((idx == alignment_layer)),\n                need_head_weights=bool((idx == alignment_layer)),\n            )\n            inner_states.append(x)\n            if layer_attn is not None and idx == alignment_layer:\n                attn = layer_attn.float().to(x)\n\n        if attn is not None:\n            if alignment_heads is not None:\n                attn = attn[:alignment_heads]\n\n            # average probabilities over heads\n            attn = attn.mean(dim=0)\n\n        if self.layer_norm is not None:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        return x, {\"attn\": [attn], \"inner_states\": inner_states}\n\n    def output_layer(self, features):\n        \"\"\"Project features to the vocabulary size.\"\"\"\n        if self.adaptive_softmax is None:\n            # project back to size of vocabulary\n            return self.output_projection(features)\n        else:\n            return features\n\n    def max_positions(self):\n        \"\"\"Maximum output length supported by the decoder.\"\"\"\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return min(self.max_target_positions, self.embed_positions.max_positions)\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.0)\n    return m\n\n",
    "description": null,
    "url": null
}