{
    "acronym": "matmulfree",
    "title": "Scalable MatMul-free Language Modeling",
    "seed_ids": [
        "griffin",
        "hgrn",
        "rwkv4",
        "resurrectrnn",
        "spikegpt",
        "metaformer",
        "aft"
    ],
    "s2id": "401c4147375b016d4758cf2dd859232a8271fdcd",
    "abstract": "Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.",
    "authors": [
        "Rui-Jie Zhu",
        "Yu Zhang",
        "Ethan Sifferman",
        "Tyler Sheaves",
        "Yiqiao Wang",
        "Dustin Richmond",
        "Peng Zhou",
        "J. Eshraghian"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.",
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": "# -*- coding: utf-8 -*-\n\nfrom __future__ import annotations\n\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (BaseModelOutputWithPast,\n                                           CausalLMOutputWithPast)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import logging\n\nfrom mmfreelm.models.hgrn_bit.configuration_hgrn_bit import HGRNBitConfig\nfrom mmfreelm.models.utils import RecurrentCache\nfrom mmfreelm.modules import FusedCrossEntropyLoss, RMSNorm\nfrom mmfreelm.modules.layernorm import RMSNormLinear\nfrom mmfreelm.modules.activations import swiglu_linear, swiglu\n#from mmfreelm.ops.bitnet import BitLinear_Fuse as BitLinear\nfrom mmfreelm.ops.fusedbitnet import FusedBitLinear as BitLinear\n\nlogger = logging.get_logger(__name__)\n\n\n# -*- coding: utf-8 -*-\n\n# \"HGRN2: Gated Linear RNNs with State Expansion\"[https://arxiv.org/abs/2404.07904]\n\nfrom __future__ import annotations\n\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom transformers.cache_utils import Cache\n\nfrom mmfreelm.modules import FusedRMSNormSwishGate, ShortConvolution\nfrom mmfreelm.modules.activations import swiglu\nfrom mmfreelm.ops.hgrn.recurrent_fuse import fused_recurrent_hgrn\n\n#from mmfreelm.ops.bitnet import BitLinear_Fuse as BitLinear\nfrom mmfreelm.ops.fusedbitnet import FusedBitLinear as BitLinear\n\n\nclass HGRNBitAttention(nn.Module):\n\n    def __init__(\n        self,\n        mode: str = 'fused_recurrent',\n        hidden_size: int = 1024,\n        num_heads: Optional[int] = None,\n        expand_ratio: Optional[int] = 1,\n        use_short_conv: bool = False,\n        conv_size: int = 4,\n        conv_bias: bool = False,\n        share_conv_kernel: bool = True,\n        layernorm_eps: float = 1e-5,\n        layer_idx: int = None\n    ) -> HGRNAttention:\n        super().__init__()\n\n        self.mode = mode\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.expand_ratio = expand_ratio\n        self.input_dim = int(hidden_size * expand_ratio)\n        self.head_dim = self.input_dim // self.num_heads\n\n        self.use_short_conv = use_short_conv\n        self.conv_size = conv_size\n        self.conv_bias = conv_bias\n        self.share_conv_kernel = share_conv_kernel\n\n        self.layer_idx = layer_idx\n\n        assert mode in ['fused_recurrent'], f\"Not suppoerted mode `{mode}`.\"\n        assert self.hidden_size % num_heads == 0, f\"hidden size must be divisible by num_heads of {num_heads}\"\n\n        self.i_proj = BitLinear(hidden_size, self.input_dim, bias=False)\n        self.f_proj = BitLinear(hidden_size, self.input_dim, bias=False)\n        self.g_proj = BitLinear(hidden_size, self.input_dim, bias=False)\n\n        if use_short_conv:\n            self.conv_size = conv_size\n            if share_conv_kernel:\n                self.h_conv1d = ShortConvolution(hidden_size, conv_size, activation='silu')\n            else:\n                self.q_conv1d = ShortConvolution(self.input_dim, conv_size, activation='silu')\n                self.f_conv1d = ShortConvolution(self.input_dim, conv_size, activation='silu')\n                self.i_conv1d = ShortConvolution(self.input_dim, conv_size, activation='silu')\n\n        self.g_norm = FusedRMSNormSwishGate(self.input_dim, layernorm_eps)\n        self.o_proj = BitLinear(self.input_dim, hidden_size, bias=False)\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        if isinstance(module, (nn.Linear, BitLinear)):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[Cache] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        lower_bound: Optional[torch.Tensor] = None,\n        **kwargs\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n        # launching the triton kernel for just one token will actually be slower\n        mode = 'fused_recurrent' if hidden_states.shape[1] == 1 else self.mode\n\n        last_state = past_key_values[self.layer_idx] if use_cache else None\n        if self.use_short_conv:\n            conv_state = last_state[0] if use_cache else None\n            if self.share_conv_kernel:\n                # conv state is updated inplace\n                hidden_states = self.h_conv1d(hidden_states, attention_mask, conv_state)\n                i = self.i_proj(hidden_states)\n                f = self.f_proj(hidden_states)\n            else:\n                conv_state_i = last_state[2] if use_cache else None\n                conv_state_f = last_state[1] if use_cache else None\n                i = self.i_conv1d(self.i_proj(hidden_states), attention_mask, conv_state_i)\n                f = self.f_conv1d(self.f_proj(hidden_states), attention_mask, conv_state_f)\n        else:\n            i = self.i_proj(hidden_states)\n            f = self.f_proj(hidden_states)\n\n        f = f.sigmoid()\n        # the lower bound for the first layer is zero\n        if lower_bound is not None and self.layer_idx > 0:\n            f = lower_bound + (1 - lower_bound) * f\n        i = swiglu(i, 1 - f)\n        # dealing with left-padding\n        if attention_mask is not None:\n            i = i.mul_(attention_mask.unsqueeze(-1))\n        i, f = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=self.num_heads), (i, f))\n\n        recurrent_state = last_state[-1] if use_cache else None\n        if mode == 'fused_recurrent':\n            o, recurrent_state = fused_recurrent_hgrn(i, f, initial_state=recurrent_state, output_final_state=use_cache)\n        else:\n            raise NotImplementedError(f\"Not supported mode `{mode}`.\")\n\n        if past_key_values is not None:\n            if self.use_short_conv:\n                if self.share_conv_kernel:\n                    last_state = (conv_state, recurrent_state)\n                else:\n                    last_state = (conv_state_i, conv_state_f, recurrent_state)\n            else:\n                last_state = (recurrent_state,)\n            past_key_values.update(last_state, self.layer_idx, i.shape[2])\n\n        o = self.g_norm(self.g_proj(hidden_states), rearrange(o, 'b h l d -> b l (h d)'))\n        o = self.o_proj(o)\n\n        return o, None, past_key_values\n\n    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n        param = next(self.parameters())\n        state = tuple()\n        if self.use_short_conv:\n            if self.share_conv_kernel:\n                state += (param.new_zeros(batch_size, self.hidden_size, self.conv_size),)\n            else:\n                state += (param.new_zeros(batch_size, self.hidden_size, self.conv_size),\n                          param.new_zeros(batch_size, self.hidden_size, self.conv_size),\n                          param.new_zeros(batch_size, self.hidden_size, self.conv_size))\n        state += (param.new_zeros(batch_size, self.num_heads, self.head_dim),)\n        return state\n\n    def state_size(self, **kwargs) -> int:\n        state_size = self.hidden_size\n        for module in self.children():\n            if isinstance(module, ShortConvolution):\n                state_size += module.state_size\n        return state_size\n    \n\nclass HGRNBitMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        hidden_ratio: Optional[int] = None,\n        intermediate_size: Optional[int] = None,\n        hidden_act: str = 'swish'\n    ) -> HGRNBitMLP:\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        # the final number of params is `hidden_ratio * hidden_size^2`\n        # `intermediate_size` is chosen to be a multiple of 256 closest to `2/3 * hidden_size * hidden_ratio`\n        if hidden_ratio is None:\n            hidden_ratio = 4\n        if intermediate_size is None:\n            intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n            intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n\n        self.gate_proj = BitLinear(self.hidden_size, self.intermediate_size * 2, bias=False)\n        #self.gate_proj_bit = RMSNormLinear(self.hidden_size)\n        self.down_proj = BitLinear(self.intermediate_size, self.hidden_size, bias=False)\n        #self.gate_proj_bit = RMSNormLinear(self.hidden_size)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        y = self.gate_proj(x)\n        gate, y = y.chunk(2, -1)\n        z = self.down_proj(swiglu(gate, y))\n        return z\n\n\nclass HGRNBitBlock(nn.Module):\n    def __init__(self, config: HGRNBitConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.attn_norm = RMSNorm(hidden_size=config.hidden_size, eps=config.rms_norm_eps)\n        self.attn = HGRNBitAttention(\n            mode=config.attn_mode,\n            hidden_size=config.hidden_size,\n            num_heads=config.num_heads,\n            expand_ratio=config.expand_ratio,\n            use_short_conv=config.use_short_conv,\n            conv_size=config.conv_size,\n            share_conv_kernel=config.share_conv_kernel,\n            layernorm_eps=config.rms_norm_eps,\n            layer_idx=layer_idx\n        )\n        self.mlp_norm = RMSNorm(hidden_size=config.hidden_size, eps=config.rms_norm_eps)\n        self.mlp = HGRNBitMLP(\n            hidden_size=config.hidden_size,\n            hidden_ratio=config.hidden_ratio,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[Tuple[List[torch.Tensor]]] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        lower_bound: Optional[torch.Tensor] = False,\n        **kwargs,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        residual = hidden_states\n        hidden_states = self.attn_norm(hidden_states)\n        hidden_states, attentions, past_key_values = self.attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            lower_bound=lower_bound\n        )\n        hidden_states, residual = self.mlp_norm(hidden_states, residual, True)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states, attentions, past_key_values)\n\n        return outputs\n\n",
    "description": null,
    "url": null
}