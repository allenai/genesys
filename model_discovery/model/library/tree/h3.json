{
    "acronym": "h3",
    "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
    "seed_ids": [
        "gssm",
        "httyh",
        "s4d",
        "flashattn",
        "s4",
        "performer",
        "lineartransformer",
        "reformer",
        "compressivetransformer",
        "transformerxl"
    ],
    "s2id": "5a77b508302771fc083bf24e0bcda8553c9b5421",
    "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
    "authors": [
        "Tri Dao",
        "Daniel Y. Fu",
        "Khaled Kamal Saab",
        "A. Thomas",
        "A. Rudra",
        "Christopher R\u00e9"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": "A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
    "citationCount": 200,
    "influentialCitationCount": 18,
    "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom einops import rearrange\n\ntry:\n    from src.ops.fftconv import fftconv_func\nexcept ImportError:\n    fftconv_func = None\n\n\"\"\"SSM convolution kernels.\nSSKernel wraps different kernels with common options and handles the initialization.\n\"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom opt_einsum import contract\n\nfrom src.models.ssm.ss_kernel_diag import SSKernelDiag, EMAKernel\nfrom src.models.ssm.ss_kernel_shift import SSKernelShift\n\nfrom src.models.ssm import dplr\nfrom src.ops.krylov import power\n\nfrom src.utils.utils import get_logger\n\nlog = get_logger(__name__)\n\n\n_conj = lambda x: torch.cat([x, x.conj()], dim=-1)\n\n\nclass SSKernel(nn.Module):\n    \"\"\"Wrapper around SSKernel parameterizations.\n\n    The SSKernel is expected to support the interface\n    forward()\n    default_state()\n    _setup_step()\n    step()\n    \"\"\"\n\n    def __init__(\n        self,\n        H,\n        N=64,\n        L=None,\n        measure=\"diag-lin\",\n        rank=1,\n        channels=1,\n        dt_min=0.001,\n        dt_max=0.1,\n        deterministic=False,\n        lr=None,\n        mode=\"diag\",\n        n_ssm=None,\n        verbose=False,\n        measure_args={},\n        **kernel_args,\n    ):\n        \"\"\"State Space Kernel which computes the convolution kernel $\\\\bar{K}$\n\n        H: Number of independent SSM copies; controls the size of the model. Also called d_model in the config.\n        N: State size (dimensionality of parameters A, B, C). Also called d_state in the config. Generally shouldn't need to be adjusted and doens't affect speed much.\n        L: Maximum length of convolution kernel, if known. Should work in the majority of cases even if not known.\n        measure: Options for initialization of (A, B). For NPLR mode, recommendations are \"legs\", \"fout\", \"hippo\" (combination of both). For Diag mode, recommendations are \"diag-inv\", \"diag-lin\", \"diag-legs\", and \"diag\" (combination of diag-inv and diag-lin)\n        rank: Rank of low-rank correction for NPLR mode. Needs to be increased for measure \"legt\"\n        channels: C channels turns the SSM from a 1-dim to C-dim map; can think of it having C separate \"heads\" per SSM. This was partly a feature to make it easier to implement bidirectionality; it is recommended to set channels=1 and adjust H to control parameters instead\n        dt_min, dt_max: min and max values for the step size dt (\\Delta)\n        mode: Which kernel algorithm to use. 'nplr' is the full S4 model; 'diag' is the simpler S4D; 'slow' is a dense version for testing\n        n_ssm: Number of independent trainable (A, B) SSMs, e.g. n_ssm=1 means all A/B parameters are tied across the H different instantiations of C. n_ssm=None means all H SSMs are completely independent. Generally, changing this option can save parameters but doesn't affect performance or speed much. This parameter must divide H\n        lr: Passing in a number (e.g. 0.001) sets attributes of SSM parameers (A, B, dt). A custom optimizer hook is needed to configure the optimizer to set the learning rates appropriately for these parameters.\n        \"\"\"\n        super().__init__()\n        self.N = N\n        self.H = H\n        dtype, cdtype = torch.float, torch.cfloat\n        self.channels = channels\n        self.n_ssm = n_ssm if n_ssm is not None else H\n        self.mode = mode\n        self.verbose = verbose\n        self.kernel_args = kernel_args\n\n        # Generate dt\n        if deterministic:\n            log_dt = torch.exp(torch.linspace(math.log(dt_min), math.log(dt_max), H))\n        else:\n            log_dt = torch.rand(self.H, dtype=dtype) * (\n                math.log(dt_max) - math.log(dt_min)\n            ) + math.log(dt_min)\n\n        # Compute the preprocessed representation\n        if mode == \"ema\":\n            self.kernel = EMAKernel(H, N=N, channels=channels, **kernel_args)\n        else:\n            w, P, B, V = dplr.combination(measure, self.N, rank, self.n_ssm, **measure_args)\n\n            # Broadcast C to have H channels\n            if deterministic:\n                C = torch.zeros(channels, self.n_ssm, self.N, dtype=cdtype)\n                C[:, :, :1] = 1.\n                C = contract('hmn, chn -> chm', V.conj().transpose(-1, -2), C) # V^* C\n                C = repeat(C, 'c t n -> c (v t) n', v=self.n_ssm // C.size(-2)).clone().contiguous()\n            else:\n                C = torch.randn(channels, self.H, self.N//2, dtype=cdtype)\n\n            # Broadcast other parameters to have n_ssm copies\n            assert self.n_ssm % B.size(-2) == 0 \\\n                    and self.n_ssm % P.size(-2) == 0 \\\n                    and self.n_ssm % w.size(-2) == 0\n            # Broadcast tensors to n_ssm copies\n            # These will be the parameters, so make sure tensors are materialized and contiguous\n            B = repeat(B, 't n -> (v t) n', v=self.n_ssm // B.size(-2)).clone().contiguous()\n            P = repeat(P, 'r t n -> r (v t) n', v=self.n_ssm // P.size(-2)).clone().contiguous()\n            w = repeat(w, 't n -> (v t) n', v=self.n_ssm // w.size(-2)).clone().contiguous()\n\n            if mode == \"diag\":\n                if not measure.startswith(\"diag\"):\n                    log.warning(\"Diagonal kernel (S4D) activated but initialization is not intended for S4D. Set `measure` to 'diag-lin', 'diag-inv', or 'diag-legs' for the main variants, or 'diag' for a combination of S4D-Lin and S4D-Inv.\")\n                C = C * repeat(B, 't n -> (v t) n', v=H//self.n_ssm)\n                self.kernel = SSKernelDiag(\n                    w, B, C, log_dt, L=L,\n                    lr=lr,\n                    **kernel_args,\n                )\n            elif mode == 'shift':\n                # Initializing B to be e_1\n                B = torch.zeros(self.H, self.N)\n                B[..., 0] = 1.0\n                # Match torch.Conv1d init\n                C = torch.randn(self.H, self.channels, self.N)\n                nn.init.kaiming_uniform_(C, a=math.sqrt(5))\n                C = rearrange(C, 'h c n -> c h n')\n                self.kernel = SSKernelShift(B, C, L=L, lr=lr, **kernel_args)\n            else:\n                raise NotImplementedError(f\"{mode=} is not valid\")\n\n    def forward(self, state=None, L=None, rate=None):\n        return self.kernel(state=state, L=L, rate=rate)\n\n    @torch.no_grad()\n    def forward_state(self, u, state):\n        \"\"\" Forward the state through a sequence, i.e. computes the state after passing chunk through SSM\n\n        state: (B, H, N)\n        u: (B, H, L)\n\n        Returns: (B, H, N)\n        \"\"\"\n\n        if hasattr(self.kernel, \"forward_state\"):\n            return self.kernel.forward_state(u, state)\n\n        dA, dB = self.kernel._setup_state() # Construct dA, dB matrices\n        # dA, dB = self.kernel.dA, self.kernel.dB # (H N N) (H N)\n\n        conj = state.size(-1) != dA.size(-1)\n        if conj: state = _conj(state)\n\n        v = contract('h n, b h l -> b h n l', dB, u.flip(-1)) # dB.unsqueeze(-1) * u.flip(-1).unsqueeze(-2)\n        AL, v = power(u.size(-1), dA, v)\n        next_state = contract(\"h m n, b h n -> b h m\", AL, state)\n        next_state = next_state + v\n\n        if conj: next_state = next_state[..., : next_state.size(-1) // 2]\n        return next_state\n\n    def _setup_step(self, **kwargs):\n        # This method is intended to be private so that setting up an S4 module with\n        # ```\n        # if hasattr(module, 'setup_step'): module.setup_step()\n        # ```\n        # will not trigger this method multiple times\n        self.kernel._setup_step(**kwargs)\n\n    def step(self, u, state, **kwargs):\n        y, state = self.kernel.step(u, state, **kwargs)\n        return y, state\n\n    def default_state(self, *args, **kwargs):\n        return self.kernel.default_state(*args, **kwargs)\n    \n@torch.jit.script\ndef mul_sum(q, y):\n    return (q * y).sum(dim=1)\n\n\nclass H3(nn.Module):\n\n    def __init__(\n            self,\n            d_model,\n            d_state=64,\n            l_max=None,\n            head_dim=1,\n            use_fast_fftconv=False,\n            dropout=0.0,   # Just to absorb the kwarg\n            layer_idx=None,\n            device=None, dtype=None,\n            # SSM Kernel arguments\n            **kernel_args,\n        ):\n        \"\"\"\n        d_state: the dimension of the state, also denoted by N\n        l_max: the maximum kernel length, also denoted by L. Set l_max=None to always use a global kernel\n\n        See the class .kernel.SSKernel for the kernel constructor which accepts kernel_args. Relevant options that are worth considering and tuning include \"mode\" + \"measure\", \"dt_min\", \"dt_max\", \"lr\"\n\n        Other options are all experimental and should not need to be configured\n        \"\"\"\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.d_model = d_model\n        self.head_dim = head_dim\n        assert d_model % head_dim == 0\n        self.H = d_model // head_dim\n        self.N = d_state\n        self.L = l_max\n        self.layer_idx = layer_idx\n        self.use_fast_fftconv = use_fast_fftconv\n        if self.use_fast_fftconv:\n            assert fftconv_func is not None, 'Need to install fftconv'\n\n        self.q_proj = nn.Linear(self.d_model, self.d_model, **factory_kwargs)\n        self.k_proj = nn.Linear(self.d_model, self.d_model, **factory_kwargs)\n        self.v_proj = nn.Linear(self.d_model, self.d_model, **factory_kwargs)\n\n        # TODO: SSKernel doesn't take device argument yet\n        self.ssm_k_kernel = SSKernel(self.d_model, N=d_state, L=self.L, mode='shift',\n                                     lr=kernel_args.get('lr', None))\n        self.ssm_k_D = nn.Parameter(torch.randn(self.d_model))\n        # S4D Kernel\n        self.kernel = SSKernel(self.H, N=self.N, L=self.L, channels=1, **kernel_args)\n        self.D = nn.Parameter(torch.randn(self.H, **factory_kwargs))\n\n        # Pointwise\n        # position-wise output transform to mix features\n        # Don't use FusedDense since the layout is H first\n        self.output_linear = nn.Linear(self.d_model, self.d_model)\n\n    def forward(self, u, inference_params=None):\n        \"\"\"\n        u: (B L H)\n\n        Returns: same shape as u\n        \"\"\"\n        L_og = u.size(-2)\n        if self.use_fast_fftconv and L_og % 2 != 0:\n            u = F.pad(u, (0, 0, 0, 1))\n        L = u.size(-2)\n\n        use_fast_fftconv = self.use_fast_fftconv and inference_params is None\n\n        state_k, state = None, None\n        if inference_params is not None:\n            assert self.layer_idx is not None\n            if self.layer_idx not in inference_params.key_value_memory_dict:\n                batch_shape = (u.shape[0] * self.head_dim * self.head_dim,)\n                state_k = self.ssm_k_kernel.default_state(*batch_shape)\n                state = self.kernel.default_state(*batch_shape)\n                inference_params.key_value_memory_dict[self.layer_idx] = (state_k, state)\n            else:\n                state_k, state = inference_params.key_value_memory_dict[self.layer_idx]\n            if inference_params.sequence_len_offset == 0:\n                self.ssm_k_kernel._setup_step()\n                self.kernel._setup_step()\n\n        if inference_params is not None and inference_params.sequence_len_offset > 0:\n            y, next_state_k, next_state = self.step(u, state_k, state)\n            inference_params.key_value_memory_dict[self.layer_idx][0].copy_(next_state_k)\n            inference_params.key_value_memory_dict[self.layer_idx][1].copy_(next_state)\n            return y\n\n        # Compute SS Kernel\n        L_kernel = L if self.L is None else min(L, self.L )\n        ssm_kernel, k_state = self.kernel(L=L_kernel, state=state, rate=1.0) # (C H L) (B C H L)\n        ssm_kernel = rearrange(ssm_kernel, '1 h l -> h l')\n\n        u = rearrange(u, 'b l h -> (b l) h')\n        dtype = (self.q_proj.weight.dtype if not torch.is_autocast_enabled()\n                 else torch.get_autocast_gpu_dtype())\n        q = self.q_proj.weight @ u.T + self.q_proj.bias.to(dtype).unsqueeze(-1)\n        k = self.k_proj.weight @ u.T + self.k_proj.bias.to(dtype).unsqueeze(-1)\n        v = self.v_proj.weight @ u.T + self.v_proj.bias.to(dtype).unsqueeze(-1)\n        q, k, v = [rearrange(x, 'h (b l) -> b h l', l=L) for x in [q, k, v]]\n\n        k_og = k\n        ssm_k_kernel, _ = self.ssm_k_kernel(L=L_kernel, state=state_k, rate=1.0) # (C H L) (B C H L)\n        ssm_k_kernel = rearrange(ssm_k_kernel, '1 h l -> h l')\n        if not use_fast_fftconv:\n            fft_size = L_kernel + L\n            ssm_k_kernel_f = torch.fft.rfft(ssm_k_kernel, n=fft_size) # (H 2L)\n            k_f = torch.fft.rfft(k.to(ssm_kernel.dtype), n=fft_size) # (B H 2L)\n            shift_k_out = torch.fft.irfft(ssm_k_kernel_f * k_f, n=fft_size)[..., :L]\n            k = shift_k_out + rearrange(self.ssm_k_D, 'h -> h 1') * k\n        else:\n            dropout_mask = None\n            # No GeLU after the SSM\n            # We want output_hbl=True so that k has the same layout as q and v for the next\n            # fftconv\n            k = fftconv_func(k, ssm_k_kernel, self.ssm_k_D, dropout_mask, False, False, True)\n            # This line below looks like it doesn't do anything, but it gets the stride right\n            # for the case batch_size=1. In that case k has stride (L, L, 1), but q and v has\n            # stride (H * L, L, 1). The two strides are equivalent because batch_size=1, but\n            # the C++ code doesn't like that.\n            k = rearrange(rearrange(k, 'b h l -> h b l'), 'h b l -> b h l')\n\n        if not use_fast_fftconv:\n            fft_size = L_kernel + L\n            # kv = k * v\n            kv = (rearrange(k, 'b (h d1) l -> b d1 1 h l', d1=self.head_dim)\n                    * rearrange(v, 'b (h d2) l -> b 1 d2 h l', d2=self.head_dim))  # b d1 d2 h l\n            kv_f = torch.fft.rfft(kv.to(dtype=ssm_kernel.dtype), n=fft_size) / fft_size\n            ssm_kernel_f = torch.fft.rfft(ssm_kernel, n=fft_size)  # h L+1\n            y = torch.fft.irfft(kv_f * ssm_kernel_f, n=fft_size, norm='forward')[..., :L]  # b d1 d2 h l\n            y = y + kv * self.D.unsqueeze(-1)  # b d1 d2 h l\n            q = rearrange(q, 'b (h d1) l -> b d1 1 h l', d1=self.head_dim)\n            # einsum is way slower than multiply and then sum.\n            if self.head_dim > 1:\n                y = mul_sum(y, q)\n                y = rearrange(y, 'b d h l -> b (d h) l')\n            else:\n                y = rearrange(y * q, 'b 1 1 h l -> b h l')\n        else:\n            dropout_mask = None\n            # No GeLU after the SSM\n            # Set output_hbl_layout=True since we'll be doing a matmul right after\n            y = fftconv_func(k, ssm_kernel, self.D,\n                             dropout_mask, False, torch.is_autocast_enabled(), True,\n                             v, self.head_dim, q)\n\n        y = rearrange(y, 'b h l -> b l h')\n\n        if state is not None:\n            assert inference_params is not None\n            # TODO: This doesn't ever happen?\n            # if inference_params.sequence_len_offset > 0:\n            #     y = y + k_state\n            inference_params.key_value_memory_dict[self.layer_idx][0].copy_(\n                self.ssm_k_kernel.forward_state(k_og, state_k)\n            )\n            inference_params.key_value_memory_dict[self.layer_idx][1].copy_(\n                self.kernel.forward_state(rearrange(kv, 'b d1 d2 h l -> (b d1 d2) h l'), state)\n            )\n\n        # y could be in fp32 because of the SSMs\n        if not torch.is_autocast_enabled():\n            y = y.to(dtype=self.output_linear.weight.dtype)\n        y = self.output_linear(y)\n        if L_og < L:\n            y = y[:, :L_og, :]\n\n        return y\n\n    def step(self, u, state_k, state):\n        q, k, v = self.q_proj(u), self.k_proj(u), self.v_proj(u)\n        shift_k, next_state_k = self.ssm_k_kernel.step(rearrange(k, 'b 1 h -> b h'), state_k)\n        k = shift_k + k * self.ssm_k_D\n        # kv = k * v\n        kv = (rearrange(k, 'b 1 (h d1) -> b d1 1 h', d1=self.head_dim)\n                * rearrange(v, 'b 1 (h d2) -> b 1 d2 h', d2=self.head_dim))  # b d1 d2 h\n        y, next_state = self.kernel.step(rearrange(kv, 'b d1 d2 h -> (b d1 d2) h'), state)\n        y = (rearrange(y, '(b d1 d2) 1 h -> b d1 d2 h', d1=self.head_dim, d2=self.head_dim)\n                + kv * self.D)\n        q = rearrange(q, 'b 1 (h d1) -> b d1 1 h', d1=self.head_dim)\n        if self.head_dim > 1:\n            y = mul_sum(y, q)\n            y = rearrange(y, 'b d h l -> b (d h) l')\n        else:\n            y = rearrange(y * q, 'b 1 1 h -> b 1 h')\n        # y could be in fp32 because of the SSMs\n        if not torch.is_autocast_enabled():\n            y = y.to(dtype=self.output_linear.weight.dtype)\n        return self.output_linear(y), next_state_k, next_state",
    "description": null,
    "url": null
}