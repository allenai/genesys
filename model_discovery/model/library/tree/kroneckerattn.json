{
    "acronym": "kroneckerattn",
    "title": "Kronecker Attention Networks",
    "seed_ids": [],
    "s2id": "57ae9f6566f7a9ea849ac63a5d8316855d9c69c8",
    "abstract": "Attention operators have been applied on both 1-D data like texts and higher-order data such as images and videos. Use of attention operators on high-order data requires flattening of the spatial or spatial-temporal dimensions into a vector, which is assumed to follow a multivariate normal distribution. This not only incurs excessive requirements on computational resources, but also fails to preserve structures in data. In this work, we propose to avoid flattening by assuming the data follow matrix-variate normal distributions. Based on this new view, we develop Kronecker attention operators (KAOs) that operate on high-order tensor data directly. More importantly, the proposed KAOs lead to dramatic reductions in computational resources. Experimental results show that our methods reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higher-order data. Results also show that networks with KAOs outperform models without attention, while achieving competitive performance as those with original attention operators.",
    "authors": [
        "Hongyang Gao",
        "Zhengyang Wang",
        "Shuiwang Ji"
    ],
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2020,
    "tldr": "This work develops Kronecker attention operators (KAOs) that operate on high-order tensor data directly that reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higher-order data.",
    "citationCount": 29,
    "influentialCitationCount": 2,
    "code": "import torch\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nimport torch.nn.functional as F\n\nclass KroneckerSelfAttention(nn.Module):\n    def __init__(self, dim, heads, dim_heads = 32):\n        super().__init__()\n        hidden_dim = heads * dim_heads\n\n        self.heads = heads\n        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv1d(hidden_dim, dim, 1)\n\n    def forward(self, x):\n        h = x.shape[-2]\n\n        x = torch.cat((x.mean(dim=-1), x.mean(dim=-2)), dim=-1)\n\n        qkv = self.to_qkv(x)\n        q, k, v = rearrange(qkv, 'b (qkv h d) n -> qkv b h d n', h=self.heads, qkv=3)\n        \n        dots = einsum('bhdi,bhdj->bhij', q, k)\n        attn = dots.softmax(dim=-1)\n        out = einsum('bhij,bhdj->bhdi', attn, v)\n        \n        out = rearrange(out, 'b h d n -> b (h d) n')\n        out = self.to_out(out)\n\n        # outer sum\n        out = rearrange(out[..., :h], 'b c (n 1) -> b c n 1') + rearrange(out[..., h:], 'b c (1 n) -> b c 1 n')\n        return out",
    "description": null,
    "url": null
}