{
    "acronym": "longnet",
    "title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens",
    "seed_ids": [
        "lex",
        "sparsetransformer"
    ],
    "s2id": "c12db2c60e8989f646a29ad4f4d24475e860ad91",
    "abstract": "Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",
    "authors": [
        "Jiayu Ding",
        "Shuming Ma",
        "Li Dong",
        "Xingxing Zhang",
        "Shaohan Huang",
        "Wenhui Wang",
        "Furu Wei"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": "This work introduces LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences, and proposes dilated attention, which expands the attentive field exponentially as the distance grows.",
    "citationCount": 84,
    "influentialCitationCount": 10,
    "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# from long_net.attend import FlashAttention\nfrom zeta.nn.attention.flash_attention import FlashAttention\nfrom long_net.utils import XPOS, RelativePositionBias, LayerNorm, FeedForward, RMSNorm, RotaryEmbedding, SwiGLU\n\n\n# add alibi, qk layer norm, one write head, multihway,\nclass DilatedAttention(nn.Module):\n    \"\"\"\n    Dilated Attention Module.\n\n    Arguments:\n        dim: The dimension of the attention layers.\n        heads: The number of attention heads.\n        dilation_rate: The dilation rate for dilated attention.\n        segment_size: The segment size for dilated attention.\n        dropout (optional): The dropout probability. Default: 0.0\n        causal (optional): If set to True, the attention mechanism is causal. Default: False\n        use_xpos (optional): If set to True, xpos is used for positional encoding. Default: False\n        use_rel_pos_bias (optional): If set to True, relative position bias is used in the attention mechanism. Default: False\n\n    Usage:\n        The `DilatedAttention` class can be used as a module for neural networks and is especially suited for transformer architectures.\n\n        Example:\n            attention = DilatedAttention(dim=512, heads=8, dilation_rate=2, segment_size=64, use_xpos=True, use_rel_pos_bias=True)\n            output = attention(input_tensor)\n\n        This will return the output tensor after applying dilated attention. The `use_xpos` and `use_rel_pos_bias` parameters allow for switching on positional encoding and relative positional bias respectively.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        heads: int,\n        dilation_rate: int,\n        segment_size: int,\n        dropout: float = 0.0,\n        causal: bool = False,\n        use_xpos: bool = False,\n        use_rel_pos_bias: bool = False,\n        qk_norm: bool = False,\n        dtype: torch.dtype = torch.float16,\n        device: str = \"cuda:0\",\n    ) -> None:\n        super(DilatedAttention, self).__init__()\n        self.dim = dim\n        self.heads = heads\n        self.dilation_rate = dilation_rate\n        self.segment_size = segment_size\n        self.dropout = nn.Dropout(dropout)\n        self.causal = causal\n        self.use_xpos = use_xpos\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.qk_norm = qk_norm\n        self.dtype = dtype\n        self.device = device\n\n        self.attention = FlashAttention(causal=self.causal, dropout=dropout).to(\n            device\n        )\n\n        if use_xpos:\n            self.xpos = XPOS(head_dim=dim // heads)\n        if use_rel_pos_bias:\n            self.relative_bias = RelativePositionBias(\n                num_buckets=32, max_distance=128, n_heads=heads\n            )\n\n        self.norm = nn.LayerNorm(dim)\n\n        # head offsets\n        self.head_offsets = nn.Parameter(torch.randn(heads, dim))\n\n        # Linear Projections\n        self.proj_q = nn.Linear(dim, dim)\n        self.proj_k = nn.Linear(dim, dim)\n        self.proj_v = nn.Linear(dim, dim)\n\n    def get_mask(self, i, j):\n        \"\"\"i = row, j=column\"\"\"\n        return torch.ones((i, j), device=self.device, dtype=torch.bool).triu(\n            j - i + 2\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass of the DilatedAttention module.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor.\n        \"\"\"\n        batch_size, seq_len, _ = x.shape\n        padding_len = -seq_len % self.segment_size\n        x = F.pad(x, (0, 0, 0, padding_len))\n        seq_len = seq_len + padding_len\n\n        if self.use_xpos:\n            x = self.xpos(x)\n\n        # Split and sparsify\n        x = x.view(batch_size, -1, self.segment_size, self.dim)\n        x = x[:, :, :: self.dilation_rate, :]\n\n        # qk_norm\n        if self.qk_norm:\n            q, k, v = map(\n                self.norm, (self.proj_q(x), self.proj_k(x), self.proj_v(x))\n            )\n        else:\n            q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n\n        # Perform attention\n        attn_output = self.attention(q, k, v)\n\n        # if use rel pos => apply relative positioning bias\n        if self.use_rel_pos_bias:\n            attn_output += self.relative_bias(\n                batch_size, attn_output.size(1), attn_output.size(1)\n            )\n\n        # if causal create a mask and apply to the output\n        if self.causal:\n            mask = self.get_mask(attn_output.size(1), attn_output.size(1))\n\n            attn_output = attn_output.masked_fill(mask, float(\"-inf\"))\n\n        # apply dropout\n        attn_output = self.dropout(attn_output)\n        # Scatter and concatenate\n        attn_output = attn_output.reshape(batch_size, -1, self.dim)\n        return attn_output\n    \n\n\n# helpers\ndef exists(val):\n    return val is not None\n\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n\n    return inner\n\n\n# top k filtering\n\n\ndef top_k(logits, thres=0.9):\n    k = int((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float(\"-inf\"))\n    probs.scatter_(1, ind, val)\n    return probs\n\n# parallel attention and feedforward with residual\n# discovered by Wang et al + EleutherAI from GPT-J fame\n\n# Assuming necessary imports like RotaryEmbedding, SwiGLU, etc. are present\n\n\n\nclass ParallelTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=64,\n        dilation_rate: int = 2,\n        segment_size: int = 64,\n        heads=8,\n        ff_mult=4,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        self.norm = LayerNorm(dim)\n\n        attn_inner_dim = dim_head * heads\n        ff_inner_dim = dim * ff_mult\n        self.fused_dims = (\n            attn_inner_dim,\n            dim_head,\n            dim_head,\n            (ff_inner_dim * 2),\n        )\n\n        self.heads = heads\n        self.scale = dim_head**-0.5\n        self.rotary_emb = RotaryEmbedding(dim_head)\n\n        self.fused_attn_ff_proj = nn.Linear(\n            dim, sum(self.fused_dims), bias=False\n        )\n        self.attn_out = nn.Linear(attn_inner_dim, dim, bias=False)\n\n        self.attn = DilatedAttention(\n            dim,\n            heads,\n            dilation_rate,\n            segment_size,\n            qk_norm=True,\n            *args,\n            **kwargs,\n        )\n\n        self.ff_out = nn.Sequential(\n            SwiGLU(), nn.Linear(ff_inner_dim, dim, bias=False)\n        )\n\n        # for caching causal mask and rotary embeddings\n\n        self.register_buffer(\"mask\", None, persistent=False)\n        self.register_buffer(\"pos_emb\", None, persistent=False)\n\n        self.proj_q = nn.Linear(dim, dim)\n        self.proj_k = nn.Linear(dim, dim)\n        self.proj_v = nn.Linear(dim, dim)\n\n    def get_mask(self, n, device):\n        if self.mask is not None and self.mask.shape[-1] >= n:\n            return self.mask[:n, :n]\n\n        mask = torch.ones((n, n), device=device, dtype=torch.bool).triu(1)\n        self.register_buffer(\"mask\", mask, persistent=False)\n        return mask\n\n    def get_rotary_embedding(self, n, device):\n        if self.pos_emb is not None and self.pos_emb.shape[-2] >= n:\n            return self.pos_emb[:n]\n\n        pos_emb = self.rotary_emb(n, device=device)\n        self.register_buffer(\"pos_emb\", pos_emb, persistent=False)\n        return pos_emb\n\n    def forward(self, x):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        n, device, h = x.shape[1], x.device, self.heads\n\n        # pre layernorm\n\n        x = self.norm(x)\n\n        # attention queries, keys, values, and feedforward inner\n\n        q = self.proj_q(x)\n        k = self.proj_k(x)\n        v = self.proj_v(x)\n\n        # attention\n\n        attn = self.attn(x)\n\n        # # aggregate values\n\n        # out = einsum(\"b h i j, b j d -> b h i d\", attn, v)\n\n        # # merge heads\n\n        # out = rearrange(out, \"b h n d -> b n (h d)\")\n        return attn\n\n\n# Transformer\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        heads,\n        dim_head,\n        ff_mult=4,\n        dilation_rate: int = 2,\n        segment_size: int = 64,\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n\n        self.feedforward = (FeedForward(dim, dim, dropout=0.1),)\n\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        ParallelTransformerBlock(\n                            dim,\n                            dim_head,\n                            dilation_rate,\n                            segment_size,\n                            heads,\n                            ff_mult,\n                        ),\n                        FeedForward(dim, dim, dropout=0.1),\n                    ]\n                )\n            )\n\n    def forward(self, x):\n        for block, ff in self.layers:\n            x = block(x) + x\n            x = ff(x) + x\n        return x\n\n\n# classes\n\n\nclass LongNetTransformer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        num_tokens,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n        dilation_rate: int = 2,\n        segment_size: int = 64,\n    ):\n        super().__init__()\n        self.emb = nn.Embedding(num_tokens, dim)\n\n        self.transformer = Transformer(\n            dim, depth, heads, dim_head, ff_mult, dilation_rate, segment_size\n        )\n\n        self.to_logits = nn.Sequential(RMSNorm(dim), nn.Linear(dim, num_tokens))\n\n    def forward(self, x):\n        x = self.emb(x)\n        x = self.transformer(x)\n        return self.to_logits(x)\n\n",
    "description": null,
    "url": null
}