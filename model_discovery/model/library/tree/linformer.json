{
    "acronym": "linformer",
    "title": "Linformer: Self-Attention with Linear Complexity",
    "seed_ids": [
        "gpt3",
        "longformer",
        "reformer",
        "blockbert",
        "sparsetransformer",
        "gpt2"
    ],
    "s2id": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
    "abstract": "Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",
    "authors": [
        "Sinong Wang",
        "Belinda Z. Li",
        "Madian Khabsa",
        "Han Fang",
        "Hao Ma"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": "This paper demonstrates that the self-attention mechanism of the Transformer can be approximated by a low-rank matrix, and proposes a new self-Attention mechanism, which reduces the overall self-ATTention complexity from $O(n^2)$ to $O (n)$ in both time and space.",
    "citationCount": 1299,
    "influentialCitationCount": 140,
    "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.checkpoint import checkpoint\n\ndef identity(x, *args, **kwargs):\n    return x\n\ndef get_act(activation):\n    if activation == \"gelu\":\n        return F.gelu\n    if activation == \"relu\":\n        return F.relu\n    return None\n\ndef gen_causal_mask(input_size, dim_k, full_attention=False):\n    \"\"\"\n    Generates a causal mask of size (input_size, dim_k) for linformer\n    Else, it generates (input_size, input_size) for full attention\n    \"\"\"\n    if full_attention:\n        return (torch.triu(torch.ones(input_size, input_size))==1).transpose(0,1)\n    return (torch.triu(torch.ones(dim_k, input_size))==1).transpose(0,1)\n\ndef get_EF(input_size, dim, method=\"learnable\", head_dim=None, bias=True):\n    \"\"\"\n    Retuns the E or F matrix, initialized via xavier initialization.\n    This is the recommended way to do it according to the authors of the paper.\n    Includes a method for convolution, as well as a method for no additional params.\n    \"\"\"\n    assert method == \"learnable\" or method == \"convolution\" or method == \"no_params\", \"The method flag needs to be either 'learnable', 'convolution', or 'no_params'!\"\n    if method == \"convolution\":\n        conv = nn.Conv1d(head_dim, head_dim, kernel_size=int(input_size/dim), stride=int(input_size/dim))\n        return conv\n    if method == \"no_params\":\n        mat = torch.zeros((input_size, dim))\n        torch.nn.init.normal_(mat, mean=0.0, std=1/dim)\n        return mat\n    lin = nn.Linear(input_size, dim, bias)\n    torch.nn.init.xavier_normal_(lin.weight)\n    return lin\n\nclass Residual(nn.Module):\n    \"\"\"\n    Implemenation taken from\n    https://github.com/lucidrains/sinkhorn-transformer/blob/master/sinkhorn_transformer/sinkhorn_transformer.py\n    However, I do postnorm instead of prenorm.\n    \"\"\"\n    def __init__(self, fn, input_channels=0, output_channels=0):\n        super(Residual, self).__init__()\n        self.fn = fn\n        self.resample = nn.Linear(input_channels, output_channels) if input_channels != output_channels else None\n        self.norm = nn.LayerNorm(output_channels)\n\n    def forward(self, tensor, **kwargs):\n        if self.resample is not None:\n            tensor = self.resample(tensor) + self.fn(tensor, **kwargs)\n            tensor = self.norm(tensor)\n            return tensor\n        tensor = tensor + self.fn(tensor, **kwargs)\n        tensor = self.norm(tensor)\n        return tensor\n\nclass PositionalEmbedding(nn.Module):\n    \"\"\"\n    Standard positional embedding.\n    From the paper \"Attention is all you need\".\n    Changed the constant from 10k to 100k, since this may be better for longer sequence lengths.\n    \"\"\"\n    def __init__(self, channels):\n        super(PositionalEmbedding, self).__init__()\n        inv_freq = 1. / (100000 ** (torch.arange(0, channels, 2).float() / channels))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def forward(self, tensor):\n        pos = torch.arange(tensor.shape[1], device=tensor.device).type(self.inv_freq.type())\n        sin_inp = torch.einsum(\"i,j->ij\", pos, self.inv_freq)\n        emb = torch.cat((sin_inp.sin(), sin_inp.cos()), dim=-1)\n        return emb[None,:,:]\n\nclass ProjectInOut(nn.Module):\n    \"\"\"\n    Impelemenation taken from https://github.com/lucidrains/sinkhorn-transformer/blob/73da02958965e1a690cb301292c0a3c549687d44/sinkhorn_transformer/sinkhorn_transformer.py#L218\n    \"\"\"\n    def __init__(self, fn, dim_in, dim_out, project_out=True):\n        super(ProjectInOut, self).__init__()\n        self.fn = fn\n        self.project_in = nn.Linear(dim_in, dim_out)\n        self.project_out = nn.Linear(dim_out, dim_in) if project_out else identity\n\n    def forward(self, tensor, **kwargs):\n        tensor = self.project_in(tensor)\n        tensor = self.fn(tensor, **kwargs)\n        tensor = self.project_out(tensor)\n        return tensor\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    Standard Feed Forward Layer\n    \"\"\"\n    def __init__(self, input_channels, output_channels, ff_dim, dropout, activation=\"gelu\"):\n        super(FeedForward, self).__init__()\n        self.w_1 = nn.Linear(input_channels, ff_dim)\n        self.w_2 = nn.Linear(ff_dim, output_channels)\n        self.activation = get_act(activation)\n        self.dropout = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, tensor, **kwargs):\n        tensor = self.w_1(tensor)\n        if self.activation is not None:\n            tensor = self.activation(tensor)\n        tensor = self.dropout(tensor)\n        tensor = self.w_2(tensor)\n        tensor = self.dropout2(tensor)\n        return tensor\n\nclass LinearAttentionHead(nn.Module):\n    \"\"\"\n    Linear attention, as proposed by the linformer paper\n    \"\"\"\n    def __init__(self, dim, dropout, E_proj, F_proj, causal_mask, full_attention=False):\n        super(LinearAttentionHead, self).__init__()\n        self.E = E_proj\n        self.F = F_proj\n        self.dim = dim\n        self.dropout = nn.Dropout(dropout)\n        self.P_bar = None\n        self.full_attention = full_attention\n        self.causal_mask = causal_mask\n        self.is_proj_tensor = isinstance(E_proj, torch.Tensor)\n\n    def forward(self, Q, K, V, **kwargs):\n        \"\"\"\n        Assume Q, K, V have same dtype\n        E, F are `nn.Linear` modules\n        \"\"\"\n        input_mask = kwargs[\"input_mask\"] if \"input_mask\" in kwargs else None\n        embeddings_mask = kwargs[\"embeddings_mask\"] if \"embeddings_mask\" in kwargs else None\n\n        # Instead of classic masking, we have to do this, because the classic mask is of size nxn\n        if input_mask is not None:\n            # This is for k, v\n            mask = input_mask[:,:,None]\n            K = K.masked_fill_(~mask, 0.0)\n            V = V.masked_fill_(~mask, 0.0)\n            del mask\n\n        if embeddings_mask is not None:\n            mask = embeddings_mask[:,:,None]\n            Q = Q.masked_fill_(~mask, 0.0)\n            del mask\n\n        K = K.transpose(1,2)\n        if not self.full_attention:\n            if self.is_proj_tensor:\n                self.E = self.E.to(K.device)\n                K = torch.matmul(K, self.E)\n            else:\n                K = self.E(K)\n        Q = torch.matmul(Q, K)\n\n        P_bar = Q/torch.sqrt(torch.tensor(self.dim).type(Q.type())).to(Q.device)\n        if self.causal_mask is not None:\n            self.causal_mask = self.causal_mask.to(Q.device)\n            P_bar = P_bar.masked_fill_(~self.causal_mask, float('-inf'))\n        P_bar = P_bar.softmax(dim=-1)\n\n        # Only save this when visualizing\n        if \"visualize\" in kwargs and kwargs[\"visualize\"] == True:\n            self.P_bar = P_bar\n\n        P_bar = self.dropout(P_bar)\n\n        if not self.full_attention:\n            V = V.transpose(1,2)\n            if self.is_proj_tensor:\n                self.F = self.F.to(V.device)\n                V = torch.matmul(V, self.F)\n            else:\n                V = self.F(V)\n            V = V.transpose(1,2)\n        out_tensor = torch.matmul(P_bar, V)\n\n        return out_tensor\n\nclass MHAttention(nn.Module):\n    \"\"\"\n    Multihead attention, with each head being a Linformer Head\n    This feeds directly into a feed forward head\n    \"\"\"\n    def __init__(self, input_size, dim, channels, dim_k, nhead, dropout, checkpoint_level,\n            parameter_sharing, E_proj, F_proj, full_attention, causal_mask, w_o_intermediate_dim=None, decoder_mode=False, method=\"learnable\"):\n        super(MHAttention, self).__init__()\n        self.heads = nn.ModuleList()\n        self.input_size = input_size\n        self.dim_k = dim_k\n        self.channels = channels\n        self.causal_mask = causal_mask\n        self.checkpoint_level = checkpoint_level\n        self.w_o_intermediate_dim = w_o_intermediate_dim\n        if parameter_sharing != \"layerwise\":\n            E_proj = get_EF(input_size, dim_k, method, dim)\n            F_proj = get_EF(input_size, dim_k, method, dim) if parameter_sharing == \"none\" or parameter_sharing == \"headwise\" else E_proj\n\n        self.decoder_mode = decoder_mode\n        self.to_q = nn.ModuleList()\n        self.to_k = nn.ModuleList()\n        self.to_v = nn.ModuleList()\n\n        for _ in range(nhead):\n            if parameter_sharing == \"none\":\n                E_proj = get_EF(input_size, dim_k, method, dim)\n                F_proj = get_EF(input_size, dim_k, method, dim)\n            attn = LinearAttentionHead(dim, dropout, E_proj, F_proj, causal_mask, full_attention)\n            self.heads.append(attn)\n            self.to_q.append(nn.Linear(channels, dim, bias=False))\n            self.to_k.append(nn.Linear(channels, dim, bias=False))\n            self.to_v.append(nn.Linear(channels, dim, bias=False))\n        if w_o_intermediate_dim is None:\n            self.w_o = nn.Linear(dim*nhead, channels)\n        else:\n            self.w_o_1 = nn.Linear(dim*nhead, w_o_intermediate_dim)\n            self.w_o_2 = nn.Linear(w_o_intermediate_dim, channels)\n        self.mh_dropout = nn.Dropout(dropout)\n\n    def forward(self, tensor, **kwargs):\n        batch_size, input_len, channels = tensor.shape\n        assert not (self.decoder_mode and \"embeddings\" not in kwargs), \"Embeddings must be supplied if decoding\"\n        assert not (\"embeddings\" in kwargs and (kwargs[\"embeddings\"].shape[0], kwargs[\"embeddings\"].shape[1], kwargs[\"embeddings\"].shape[2]) != (batch_size, input_len, channels)), \"Embeddings size must be the same as the input tensor\"\n        head_outputs = []\n        for index, head in enumerate(self.heads):\n            Q = self.to_q[index](tensor)\n            K = self.to_k[index](tensor) if not self.decoder_mode else self.to_k[index](kwargs[\"embeddings\"])\n            V = self.to_v[index](tensor) if not self.decoder_mode else self.to_v[index](kwargs[\"embeddings\"])\n            if self.checkpoint_level == \"C2\":\n                head_outputs.append(checkpoint(head,Q,K,V))\n            else:\n                head_outputs.append(head(Q,K,V,**kwargs))\n        out = torch.cat(head_outputs, dim=-1)\n        if self.w_o_intermediate_dim is None:\n            out = self.w_o(out)\n        else:\n            out = self.w_o_1(out)\n            out = self.w_o_2(out)\n        out = self.mh_dropout(out)\n        return out\n\nclass Linformer(nn.Module):\n    \"\"\"\n    My attempt at reproducing the Linformer Paper\n    https://arxiv.org/pdf/2006.04768.pdf\n    \"\"\"\n    def __init__(self, input_size, channels, dim_k, dim_ff=256, dim_d=None, dropout_ff=0.15, nhead=4, depth=1, dropout=0.1, activation=\"gelu\", checkpoint_level=\"C0\", parameter_sharing=\"layerwise\", k_reduce_by_layer=0, full_attention=False, include_ff=True, w_o_intermediate_dim=None, decoder_mode=False, causal=False, method=\"learnable\", ff_intermediate=None):\n        super(Linformer, self).__init__()\n        assert activation == \"gelu\" or activation == \"relu\", \"Only gelu and relu activations supported for now\"\n        assert checkpoint_level == \"C0\" or checkpoint_level == \"C1\" or checkpoint_level == \"C2\", \"Checkpoint level has to be either C0, C1, or C2.\"\n        assert parameter_sharing == \"none\" or parameter_sharing == \"headwise\" or parameter_sharing == \"kv\" or parameter_sharing == \"layerwise\", \"The `parameter_sharing` flag has to be either 'none', 'headwise', 'kv', or 'layerwise'.\"\n        assert channels % nhead == 0 if dim_d is None else True, \"If `dim_d` is not set to a custom value, `channels` must be divisible by `nhead`!\"\n        assert not (ff_intermediate and parameter_sharing==\"layerwise\"), \"Parameter sharing must not be layerwise if ff_intermediate is enabled!\"\n        assert not (ff_intermediate and decoder_mode), \"Raising the dimension in the middle cannot be done in the decoder!\"\n\n        layers = nn.ModuleList()\n        self.decoder_mode = decoder_mode\n        self.input_size = input_size\n        self.channels = channels\n        self.checkpoint_level = checkpoint_level\n        self.depth = depth\n        self.nhead = nhead\n\n        head_dim = channels // nhead if dim_d is None else dim_d\n\n        E_proj = get_EF(input_size, dim_k, method, head_dim)\n        causal_mask = gen_causal_mask(input_size, dim_k, full_attention) if causal else None\n        # If we want causal but only with the encoder\n        causal_enc = gen_causal_mask(input_size, dim_k, full_attention) if (causal and not decoder_mode) else None\n\n        get_attn = lambda attn_channels, curr_dim_k: MHAttention(input_size, head_dim, attn_channels, curr_dim_k, nhead, dropout, checkpoint_level, parameter_sharing, E_proj, E_proj, full_attention, causal_enc, w_o_intermediate_dim, decoder_mode=False, method=method)\n        get_attn_context = lambda attn_channels, curr_dim_k: MHAttention(input_size, head_dim, attn_channels, curr_dim_k, nhead, dropout, checkpoint_level, parameter_sharing, E_proj, E_proj, full_attention, causal_mask, w_o_intermediate_dim, decoder_mode=True, method=method)\n        get_ff = lambda input_channels, output_channels: FeedForward(input_channels, output_channels, dim_ff, dropout_ff, activation)\n\n        for index in range(depth):\n            input_channels = ff_intermediate if (index != 0 and ff_intermediate is not None) and not decoder_mode else channels\n            output_channels = ff_intermediate if (index != depth-1 and ff_intermediate is not None) and not decoder_mode else channels\n            # TODO: Change the input and output channels here\n            attn_layer = get_attn(input_channels, max(1, dim_k - index*k_reduce_by_layer))\n            ff_layer = get_ff(input_channels, output_channels)\n\n            attn_layer, ff_layer = map(lambda res_ch_in, res_ch_out, fn: Residual(fn, res_ch_in, res_ch_out), (input_channels, input_channels), (input_channels, output_channels), (attn_layer, ff_layer))\n\n            if include_ff:\n                layers.extend([attn_layer, ff_layer])\n            else:\n                layers.extend([attn_layer])\n\n            if not self.decoder_mode:\n                continue\n\n            attn_context = get_attn_context(channels, max(1, dim_k - index*k_reduce_by_layer))\n            ff_context = get_ff(channels, channels)\n\n            attn_context, ff_context = map(lambda fn: Residual(fn, channels, channels), (attn_context, ff_context))\n\n            if include_ff:\n                layers.extend([attn_context, ff_context])\n            else:\n                layers.extend([attn_context])\n\n        self.seq = layers\n\n    def forward(self, tensor, **kwargs):\n        \"\"\"\n        Input is (batch_size, seq_len, channels)\n        \"\"\"\n        bt, n, c = tensor.shape\n        assert n == self.input_size, \"This tensor is of the wrong size. Dimension 1 has to match the `input_size` flag\"\n        assert c == self.channels, \"This tensor is of the wrong size. Dimension 2 has to match the `channels` flag\"\n        assert self.checkpoint_level == \"C0\" if kwargs else True, \"Cannot run checkpointing when using kwargs. Please set the checkpoint level to `C0`\"\n        assert \"embeddings\" not in kwargs or self.decoder_mode, \"If decoding, needs to be initialized with `decoder_mode=True`\"\n\n        for layer in self.seq:\n            if self.checkpoint_level != \"C0\":\n                tensor = checkpoint(layer, tensor)\n            else:\n                tensor = layer(tensor, **kwargs)\n        return tensor\n",
    "description": null,
    "url": null
}