{
    "acronym": "multiresconvmem",
    "title": "Sequence Modeling with Multiresolution Convolutional Memory ",
    "seed_ids": [
        "sgconv",
        "s4",
        "hippo",
        "lighdynconv",
        "transformer"
    ],
    "s2id": "6b655b74da908e2e0d7c37996d36d5ff4a28f030",
    "abstract": "Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -- poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. Our MultiresLayer is straightforward to implement, requires significantly fewer parameters, and maintains at most a $\\mathcal{O}(N\\log N)$ memory footprint for a length $N$ sequence. Yet, by stacking such layers, our model yields state-of-the-art performance on a number of sequence classification and autoregressive density estimation tasks using CIFAR-10, ListOps, and PTB-XL datasets.",
    "authors": [
        "Jiaxin Shi",
        "Ke Alexander Wang",
        "E. Fox"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": "The key component of the model is the multiresolution convolution, capturing multiscale trends in the input sequence, and can be implemented with shared filters across a dilated causal convolution tree, garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions.",
    "citationCount": 6,
    "influentialCitationCount": 1,
    "code": "\"\"\" Wavelet memory sequence modeling layer \"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn\nimport pywt\n\n\nclass MultiresLayer(nn.Module):\n    def __init__(self, d_model, kernel_size=None, depth=None, wavelet_init=None, tree_select=\"fading\", \n                 seq_len=None, dropout=0., memory_size=None, indep_res_init=False):\n        super().__init__()\n\n        self.kernel_size = kernel_size\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.tree_select = tree_select\n        if depth is not None:\n            self.depth = depth\n        elif seq_len is not None:\n            self.depth = self.max_depth(seq_len)\n        else:\n            raise ValueError(\"Either depth or seq_len must be provided.\")\n        print(\"depth:\", self.depth)\n\n        if tree_select == \"fading\":\n            self.m = self.depth + 1\n        elif memory_size is not None:\n            self.m = memory_size\n        else:\n            raise ValueError(\"memory_size must be provided when tree_select != 'fading'\")\n\n        with torch.no_grad():\n            if wavelet_init is not None:\n                self.wavelet = pywt.Wavelet(wavelet_init)\n                h0 = torch.tensor(self.wavelet.dec_lo[::-1], dtype=torch.float32)\n                h1 = torch.tensor(self.wavelet.dec_hi[::-1], dtype=torch.float32)\n                self.h0 = nn.Parameter(torch.tile(h0[None, None, :], [d_model, 1, 1]))\n                self.h1 = nn.Parameter(torch.tile(h1[None, None, :], [d_model, 1, 1]))\n            elif kernel_size is not None:\n                self.h0 = nn.Parameter(\n                    torch.empty(d_model, 1, kernel_size).uniform_(-1., 1.) * \n                    math.sqrt(2.0 / (kernel_size * 2))\n                )\n                self.h1 = nn.Parameter(\n                    torch.empty(d_model, 1, kernel_size).uniform_(-1., 1.) * \n                    math.sqrt(2.0 / (kernel_size * 2))\n                )\n            else:\n                raise ValueError(\"kernel_size must be specified for non-wavelet initialization.\")\n\n            w_init = torch.empty(\n                d_model, self.m + 1).uniform_(-1., 1.) * math.sqrt(2.0 / (2*self.m + 2))\n            if indep_res_init:\n                w_init[:, -1] = torch.empty(d_model).uniform_(-1., 1.)\n            self.w = nn.Parameter(w_init)\n\n        self.activation = nn.GELU()\n        dropout_fn = nn.Dropout1d\n        self.dropout = dropout_fn(dropout) if dropout > 0. else nn.Identity()\n\n    def max_depth(self, L):\n        depth = math.ceil(math.log2((L - 1) / (self.kernel_size - 1) + 1))\n        return depth\n\n    def forward(self, x):\n        if self.tree_select == \"fading\":\n            y = forward_fading(x, self.h0, self.h1, self.w, self.depth, self.kernel_size)\n        elif self.tree_select == \"uniform\":\n            y = forward_uniform(x, self.h0, self.h1, self.w, self.depth, self.kernel_size, self.m)\n        else:\n            raise NotImplementedError()\n        y = self.dropout(self.activation(y))\n        return y\n\n\ndef forward_fading(x, h0, h1, w, depth, kernel_size):\n    res_lo = x\n    y = 0.\n    dilation = 1\n    for i in range(depth, 0, -1):\n        padding = dilation * (kernel_size - 1)\n        res_lo_pad = torch.nn.functional.pad(res_lo, (padding, 0), \"constant\", 0)\n        res_hi = torch.nn.functional.conv1d(res_lo_pad, h1, dilation=dilation, groups=x.shape[1])\n        res_lo = torch.nn.functional.conv1d(res_lo_pad, h0, dilation=dilation, groups=x.shape[1])\n        y += w[:, i:i + 1] * res_hi\n        dilation *= 2\n\n    y += w[:, :1] * res_lo\n    y += x * w[:, -1:]\n    return y\n\n\ndef forward_uniform(x, h0, h1, w, depth, kernel_size, memory_size):\n    # x: [bs, d_model, L]\n    coeff_lst = []\n    dilation_lst = [1]\n    dilation = 1\n    res_lo = x\n    for _ in range(depth):\n        padding = dilation * (kernel_size - 1)\n        res_lo_pad = torch.nn.functional.pad(res_lo, (padding, 0), \"constant\", 0)\n        res_hi = torch.nn.functional.conv1d(res_lo_pad, h1, dilation=dilation, groups=x.shape[1])\n        res_lo = torch.nn.functional.conv1d(res_lo_pad, h0, dilation=dilation, groups=x.shape[1])\n        coeff_lst.append(res_hi)\n        dilation *= 2\n        dilation_lst.append(dilation)\n    coeff_lst.append(res_lo)\n    coeff_lst = coeff_lst[::-1]\n    dilation_lst = dilation_lst[::-1]\n\n    # y: [bs, d_model, L]\n    y = uniform_tree_select(coeff_lst, dilation_lst, w, kernel_size, memory_size)\n    y = y + x * w[:, -1:]\n    return y\n\n\ndef uniform_tree_select(coeff_lst, dilation_lst, w, kernel_size, memory_size):\n    latent_dim = 1\n    y_lst = [coeff_lst[0] * w[:, 0, None]]\n    layer_dim = 1\n    dilation_lst[0] = 1\n    for l, coeff_l in enumerate(coeff_lst[1:]):\n        if latent_dim + layer_dim > memory_size:\n            layer_dim = memory_size - latent_dim\n        # layer_w: [d, layer_dim]\n        layer_w = w[:, latent_dim:latent_dim + layer_dim]\n        # coeff_l_pad: [bs, d, L + left_pad]\n        left_pad = (layer_dim - 1) * dilation_lst[l]\n        coeff_l_pad = torch.nn.functional.pad(coeff_l, (left_pad, 0), \"constant\", 0)\n        # y: [bs, d, L]\n        y = torch.nn.functional.conv1d(\n            coeff_l_pad,\n            torch.flip(layer_w[:, None, :], (-1,)),\n            dilation=dilation_lst[l],\n            groups=coeff_l.shape[1],\n        )\n        y_lst.append(y)\n        latent_dim += layer_dim\n        if latent_dim >= memory_size:\n            break\n        layer_dim = 2 * (layer_dim - 1) + kernel_size\n    return sum(y_lst)",
    "description": null,
    "url": null
}