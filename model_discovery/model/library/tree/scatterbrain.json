{
    "acronym": "scatterbrain",
    "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation",
    "seed_ids": [
        "nystromformer",
        "performer",
        "bigbird",
        "lineartransformer",
        "linformer",
        "longformer",
        "reformer",
        "compressivetransformer",
        "sparsetransformer",
        "transformerxl",
        "bert"
    ],
    "s2id": "5f895e84c1fea75de07b4f90da518273c2e57291",
    "abstract": "Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.",
    "authors": [
        "Beidi Chen",
        "Tri Dao",
        "Eric Winsor",
        "Zhao Song",
        "A. Rudra",
        "C. R\u00e9"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": "Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, Scatterbrain is proposed, a novel way to unify sparse and low-rank attention for accurate and efficient approximation and is unbiased with provably low error.",
    "citationCount": 93,
    "influentialCitationCount": 11,
    "code": "import math\nimport torch\nimport torch.nn as nn\n\nfrom einops import rearrange\n\nfrom fast_transformers.local_product import local_dot_product, local_weighted_average\n\nfrom src.models.attention.feature_maps_sb import softmax_kernel,FeatureMap\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom torch.cuda.amp import autocast\n\ntry:\n    from apex import amp\n    APEX_AVAILABLE = True\nexcept:\n    APEX_AVAILABLE = False\n\n\ndef gaussian_orthogonal_random_matrix(nrows, ncols, scaling=0, device=None, dtype=None):\n    factory_kwargs = {'device': device, 'dtype': dtype}\n    nblocks = int(math.ceil(nrows / ncols))\n    # TD [2021-10-28]: Sometimes QR fails on CUDA\n    unstructured_blocks = torch.randn((nblocks, ncols, ncols), device='cpu')\n    q, r = torch.linalg.qr(unstructured_blocks)\n    # To make sure Q is uniform from the Haar distribution https://arxiv.org/pdf/math-ph/0609050.pdf\n    q *= rearrange(torch.diagonal(r, dim1=-2, dim2=-1).sign(), 'b c -> b 1 c')\n    q = q.to(**factory_kwargs)\n    # TD [2021-10-28] Idk why the transpose is necessary. I suspect it isn't.\n    # https://github.com/google-research/google-research/blob/ea313c6e96acce6c863de41615c6cf4079b8ca94/performer/fast_attention/jax/fast_attention.py#L362\n    q = rearrange(q, 'b c c1 -> b c1 c')\n    g_ortho = rearrange(q, 'b c1 c -> (b c1) c')[:nrows]\n\n    if scaling == 0:\n        multiplier = torch.randn((nrows, ncols), **factory_kwargs).norm(dim=1)\n        return rearrange(multiplier, 'r -> r 1') * g_ortho\n    elif scaling == 1:\n        return math.sqrt(ncols) * g_ortho\n    else:\n        raise ValueError(f'Invalid scaling {scaling}')\n\n\n\nclass SBPerformerFeatures(FeatureMap):\n    \"\"\"Random Fourier Features for the RBF kernel according to [1].\n    [1]: \"Weighted Sums of Random Kitchen Sinks: Replacing minimization with\n         randomization in learning\" by A. Rahimi and Benjamin Recht.\n    Arguments\n    ---------\n        query_dims: int, The input query dimensions in order to sample\n                          the noise matrix\n        n_features: int, The size of the feature map (should be divisible by 2)\n                (default: query_dims)\n        softmax_temp: float, The temerature for the Gaussian kernel\n                      approximation exp(-t * |x-y|^2)\n                      (default: 1/sqrt(query_dims))\n        orthogonal: bool, When True the random matrix is initialized for\n                    orthogonal random features to reduce the approximation\n                    variance (default: False)\n        redraw: int, Redraw the random matrix every 'redraw' times\n                (default: 1)\n        deterministic_eval: bool, Only redraw the random matrix during training\n                            (default: False)\n    \"\"\"\n    def __init__(self, query_dims, n_features=None, ortho_scaling=0, softmax_temp=None,\n                 orthogonal=False, cosh=True, redraw=1, deterministic_eval=False, eps=0.0,\n                 device=None, dtype=None):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(query_dims)\n        self.n_features = n_features or int(query_dims * math.log(query_dims))\n        self.ortho_scaling = ortho_scaling\n        # TODO: we're not using @orthogonal atm\n        self.orthogonal = orthogonal\n        self.softmax_temp = 1 / math.sqrt(query_dims) if softmax_temp is None else softmax_temp\n        self.cosh = cosh\n        # self.redraw = redraw\n        # TODO: not redrawing atm, so I'm setting it to an irrational number\n        self.redraw = math.pi\n        self.deterministic_eval = deterministic_eval\n        self.eps = eps  # Stabilizer for softmax kernel\n\n        # Make a buffer for storing the sampled projection_matrix\n        self.register_buffer(\"projection_matrix\", torch.zeros(self.query_dims, self.n_features,\n                                                              **factory_kwargs))\n        self.factory_kwargs = factory_kwargs\n        self._calls = -1\n\n    def new_feature_map(self, device):\n        # If we are not training skip the generation of a new feature map\n        if self.deterministic_eval and not self.training:\n            return\n\n        # Only redraw the new feature map every self.redraw times\n        self._calls += 1\n        if (self._calls % self.redraw) != 0:\n            return\n\n        # We use the cosh feature map so the number of rows is halved\n        nb_rows = self.n_features if not self.cosh else self.n_features // 2\n        projection_matrix = gaussian_orthogonal_random_matrix(nrows=nb_rows,\n                                                              ncols=self.query_dims,\n                                                              scaling=self.ortho_scaling,\n                                                              device=device,\n                                                              dtype=self.factory_kwargs['dtype'])\n        self.register_buffer(\"projection_matrix\", projection_matrix)\n\n    def forward_queries(self, x, return_log=False):\n        return softmax_kernel(x, projection_matrix=self.projection_matrix, is_query=True,\n                              softmax_temp=self.softmax_temp, eps=self.eps, cosh=self.cosh,\n                              return_log=return_log)\n\n    def forward_keys(self, x, return_log=False):\n        return softmax_kernel(x, projection_matrix=self.projection_matrix, is_query=False,\n                              softmax_temp=self.softmax_temp, eps=self.eps, cosh=self.cosh,\n                              return_log=return_log)\n    \n\n@contextmanager\ndef null_context():\n    yield\n\n\ndef linear_attention_normalization(q, k, causal=False):\n    if not causal:\n        return torch.einsum('...nm,...m->...n', q, k.sum(dim=-2))\n    else:\n        return torch.einsum('...nm,...nm->...n', q, k.cumsum(dim=-2))\n\n\n# efficient causal linear attention, created by EPFL\ndef causal_linear_attention(q, k, v, need_weights=False):\n    from fast_transformers.causal_product import causal_dot_product\n    autocast_enabled = torch.is_autocast_enabled()\n    is_half = isinstance(q, torch.cuda.HalfTensor)\n    assert not is_half or APEX_AVAILABLE, 'half tensors can only be used if nvidia apex is available'\n    cuda_context = null_context if not autocast_enabled else partial(autocast, enabled = False)\n    causal_dot_product_fn = amp.float_function(causal_dot_product) if is_half else causal_dot_product\n    with cuda_context():\n        if autocast_enabled:\n            q, k, v = map(lambda t: t.float(), (q, k, v))\n        q_k_v = causal_dot_product_fn(q.unsqueeze(1), k.unsqueeze(1), v.unsqueeze(1)).squeeze(1)\n        if need_weights:\n            attn = torch.einsum('...im,...jm', q, k)\n            causal_mask = torch.triu(torch.ones(q.shape[-2], k.shape[-2], dtype=torch.bool,\n                                                device=k.device), diagonal=1)\n            attn.masked_fill_(causal_mask, 0.0)\n        else:\n            attn = None\n    return q_k_v, attn\n\n\n# non-causal linear attention\ndef linear_attention(q, k, v, need_weights=False):\n    k_v = torch.einsum('...nm,...nd->...md', k, v)\n    q_k_v = torch.einsum('...nm,...md->...nd', q, k_v)\n    attn = None if not need_weights else torch.einsum('...im,...jm->...ij', q, k)\n    return q_k_v, attn\n\n\nclass SBLocalAttention(nn.Module):\n    \"\"\"Implement fast local attention where a query can only attend to\n    neighboring keys.\n    In this attention module the query Q_i can only attend to a key K_j if\n    |i-j| < local_context/2.\n    Arguments\n    ---------\n        local_context: The neighborhood to consider for local attention.\n        softmax_temp: The temperature to use for the softmax attention.\n                      (default: 1/sqrt(d_keys) where d_keys is computed at\n                      runtime)\n        attention_dropout: The dropout rate to apply to the attention\n                           (default: 0.1)\n    \"\"\"\n    def __init__(self, local_context, dim_heads, nb_features=None, ortho_scaling=0,\n                 causal=False, softmax_temp=None, attention_dropout=0.0, softmax_eps=0.0,\n                 device=None, dtype=None):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.feature_map = SBPerformerFeatures(dim_heads, nb_features, ortho_scaling=ortho_scaling,\n                                               softmax_temp=softmax_temp, eps=softmax_eps,\n                                               **factory_kwargs)\n        self.local_context = local_context\n        self.causal = causal\n        self.softmax_temp = softmax_temp\n        self.dropout = nn.Dropout(attention_dropout)\n        self.softmax_eps = softmax_eps\n\n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=False,\n                return_attn_unnormalized=False):\n        \"\"\"Implements the multihead softmax attention.\n        Arguments\n        ---------\n            query: (B, T, H, E) The tensor containing the query\n            key: (B, S, H, E) The tensor containing the key\n            value: (B, S, H, D) The tensor containing the value\n            attn_mask: An implementation of BaseMask that encodes where each\n                       query can attend to\n            key_padding_mask: An implementation of BaseMask that encodes how\n                         many queries each sequence in the batch consists of\n        \"\"\"\n        # Extract some shapes and compute the temperature\n        B, T, H, E = query.shape\n        _, S, _, D = value.shape\n        softmax_temp = self.softmax_temp or 1 / math.sqrt(E)\n\n        # TODO: check causal\n        if attn_mask is None:\n            attn_mask_additive_matrix = torch.zeros(T, S, device=query.device)\n        else:\n            attn_mask_additive_matrix = attn_mask.additive_matrix_finite\n        if key_padding_mask is None:\n            key_padding_mask_lengths = torch.full(size=(B,), fill_value=S, dtype=torch.long,\n                                                  device=key.device)\n        else:\n            key_padding_mask_lengths = key_padding_mask.lengths\n\n        # Permute the dimensions to BHTE instead of BTHE\n        query = rearrange(query, 'b t h e -> b h t e').contiguous()\n        key = rearrange(key, 'b s h e -> b h s e').contiguous()\n        value = rearrange(value, 'b s h d -> b h s d').contiguous()\n\n        self.feature_map.new_feature_map(query.device)\n        q_prime, q_prime_log_scale = self.feature_map.forward_queries(query)\n        k_prime, k_prime_log_scale = self.feature_map.forward_keys(key)\n\n        prime_log_scale = q_prime_log_scale + k_prime_log_scale\n        m = q_prime.shape[-1]\n        if key_padding_mask is not None and not key_padding_mask.all_ones:\n            k_prime = k_prime.masked_fill(~rearrange(key_padding_mask.bool_matrix,\n                                                     'b s -> b 1 s 1'), 0.0)\n        attn_fn = linear_attention if not self.causal else causal_linear_attention\n        q_prime_k_prime_1 = linear_attention_normalization(q_prime, k_prime, causal=self.causal)\n        q_prime_k_prime_v, attn_prime = attn_fn(q_prime, k_prime, value, need_weights=need_weights)\n\n        QK = softmax_temp * local_dot_product(\n            query, key, attn_mask_additive_matrix, key_padding_mask_lengths,\n            self.local_context\n        )\n        dots_prime = local_dot_product(\n            q_prime, k_prime, attn_mask_additive_matrix, key_padding_mask_lengths,\n            self.local_context\n        )\n        # local_dot_product fills in -1e24 for invalid locations. We want to set them to zero.\n        # dots_prime[dots_prime <= -1e24] = 0.0\n        i = rearrange(torch.arange(T, device=query.device), 't -> 1 1 t 1')\n        j = torch.arange(self.local_context, device=query.device)\n        local_idx = i - self.local_context // 2 + j\n        valid_idx_mask = ((local_idx >= 0)\n                          & (local_idx < rearrange(key_padding_mask_lengths, 'b -> b 1 1 1')))\n        dots_prime.masked_fill_(~valid_idx_mask, 0.0)\n        assert torch.all(dots_prime >= 0)\n\n        # Compute the normalization first\n        QK_lse = torch.logsumexp(QK, dim=-1, keepdim=True)\n        dots_prime_sum = dots_prime.sum(dim=-1, keepdim=True)\n        lr_log_normalization = torch.log((rearrange(q_prime_k_prime_1, 'b h s -> b h s 1')\n                                          - dots_prime_sum).clamp_min_(1e-24)) + prime_log_scale\n        log_normalization = torch.logaddexp(QK_lse, lr_log_normalization)\n\n        prime_scale = torch.exp(prime_log_scale - log_normalization)\n        # When we drop out, we want that location in the attn matrix to be zero.\n        # So here we dropout just torch.exp(QK) and leave -dots_prime, so that when we add it back\n        # to attn_prime it's equivalent to setting that location to zero.\n        dots = self.dropout(torch.exp(QK - log_normalization)) - dots_prime * prime_scale\n\n        out_local = local_weighted_average(dots, value)\n        out = out_local + q_prime_k_prime_v * prime_scale\n\n        attn = None\n        if need_weights:\n            attn_local = torch.zeros(B, H, T, S, device=query.device)\n            k = torch.arange(S, device=key.device)\n            idx = k - i\n            local_mask = ((idx >= -(self.local_context // 2))\n                          & (idx < (self.local_context + 1) // 2)\n                          & (k < rearrange(key_padding_mask_lengths, 'b -> b 1 1 1')))\n            attn_local.masked_scatter_(local_mask, dots.masked_select(valid_idx_mask))\n            attn = attn_local + attn_prime * prime_scale\n            if return_attn_unnormalized:  # For testing purpose\n                attn = (attn, attn * torch.exp(log_normalization),\n                        attn_prime * torch.exp(prime_log_scale))\n        return rearrange(out, 'b h t d -> b t h d'), attn",
    "description": null,
    "url": null
}