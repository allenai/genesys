{
    "title": "Linear Transformers Are Secretly Fast Weight Programmers",
    "acronym": "deltanet",
    "s2id": "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
    "abstract": "We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow\"neural net learns by gradient descent to program the ``fast weights\"of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "tldr": "This work infer a memory capacity limitation of recent linearised softmax attention variants, and replaces the purely additive outer products of self-invented activation patterns by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values.",
    "citationCount": 151,
    "influentialCitationCount": 18,
    "seed_ids": [
        "rfa",
        "performer",
        "compressivetransformer",
        "transformerxl"
    ],
    "code": "# -*- coding: utf-8 -*-\n\n# Sect4.2 of Linear Transformers Are Secretly Fast Weight Programmers https://arxiv.org/abs/2102.11174\n\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom torch.nn import functional as F\n\nfrom fla.modules import FusedRMSNormSwishGate, RMSNorm, ShortConvolution\nfrom fla.ops.delta_rule import (chunk_delta_rule, fused_chunk_delta_rule,\n                                fused_recurrent_linear_attn_delta_rule)\n\nif TYPE_CHECKING:\n    from fla.models.utils import Cache\n\n\ndef simple_norm(x):\n    return (F.normalize(x, dim=-1) * x.shape[-1] ** 0.5).to(x)\n\n\n# @torch.jit.script\ndef elu_p1(x):\n    return (F.elu(x, 1., False) + 1.).to(x)\n\n\n# @torch.jit.script\ndef sum_norm(x):\n    return (x / x.sum(-1, keepdim=True)).to(x)\n\n\n# @torch.jit.script\ndef elu_norm(x):\n    dtype = x.dtype\n    x = F.elu(x, 1., False) + 1.\n    return (x / x.sum(-1, keepdim=True)).to(dtype)\n\n\n# https://github.com/IDSIA/recurrent-fwp/blob/master/algorithmic/layers.py#L86C1-L146C1\nclass DeltaNet(nn.Module):\n    def __init__(\n        self,\n        d_model: int = None,\n        hidden_size: int = 1024,\n        expand_k: float = 1.0,\n        expand_v: float = 1.0,\n        num_heads: int = 4,\n        mode: str = 'fused_chunk',\n        chunk_size: int = 16,\n        use_beta: bool = True,\n        use_gate: bool = True,\n        use_output_norm: bool = True,\n        use_elu: bool = False,\n        use_short_conv: bool = True,\n        conv_size: int = 4,\n        conv_bias: bool = False,\n        layer_idx: int = None,\n        qk_activation: str = 'silu',\n        qk_norm: str = None,\n        norm_first: bool = True,\n        norm_eps: float = 1e-5,\n        **kwargs\n    ) -> DeltaNet:\n        super().__init__()\n\n        self.mode = mode\n        self.qk_activation = qk_activation\n        self.qk_norm = qk_norm\n\n        assert self.qk_activation in ['silu', 'relu', 'elu', 'identity']\n        assert self.qk_norm in ['l2', 'sum']\n\n        if d_model is not None:\n            hidden_size = d_model\n        self.hidden_size = hidden_size\n        self.expand_k = expand_k\n        self.expand_v = expand_v\n        self.num_heads = num_heads\n        self.chunk_size = chunk_size\n        self.use_gate = use_gate\n        self.use_output_norm = use_output_norm\n        self.use_short_conv = use_short_conv\n        self.conv_size = conv_size\n        self.conv_bias = conv_bias\n\n        self.key_dim = int(hidden_size * expand_k)\n        self.value_dim = int(hidden_size * expand_v)\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.norm_first = norm_first\n        self.layer_idx = layer_idx\n\n        self.silu = nn.SiLU()\n\n        assert mode in ['chunk', 'fused_chunk', 'fused_recurrent'], f\"Not suppoerted mode `{mode}`.\"\n        assert self.key_dim % num_heads == 0, f\"key dim must be divisible by num_heads of {num_heads}\"\n        assert self.value_dim % num_heads == 0, f\"value dim must be divisible by num_heads of {num_heads}\"\n\n        if norm_first:\n            self.norm = RMSNorm(self.hidden_size, eps=norm_eps)\n\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n\n        self.use_beta = use_beta\n        self.use_elu = use_elu\n        if self.use_beta:\n            self.b_proj = nn.Linear(hidden_size, self.num_heads, bias=False)\n        if use_short_conv:\n            self.conv_size = conv_size\n            self.q_conv1d = ShortConvolution(self.key_dim,\n                                                 conv_size,\n                                                 activation='silu' if qk_activation == 'silu' else None)\n            self.k_conv1d = ShortConvolution(self.key_dim,\n                                                 conv_size,\n                                                 activation='silu' if qk_activation == 'silu' else None)\n            self.v_conv1d = ShortConvolution(self.value_dim, conv_size, activation='silu')\n        if use_gate:\n            self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n            self.o_norm = FusedRMSNormSwishGate(self.head_v_dim, eps=norm_eps)\n        else:\n            self.o_norm = RMSNorm(self.head_v_dim, eps=norm_eps)\n\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False)\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[Cache] = None,\n        use_cache: Optional[bool] = False,\n        output_attentions: Optional[bool] = False,\n        **kwargs\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n        # change to inference mode.\n        mode = 'fused_recurrent' if hidden_states.shape[1] < 64 else self.mode\n\n        if self.norm_first:\n            hidden_states = self.norm(hidden_states)\n\n        last_state = past_key_values[self.layer_idx] if use_cache else None\n\n        if attention_mask is not None:\n            if attention_mask.shape[-1] != hidden_states.shape[-2]:\n                attention_mask = attention_mask[:, -1:]\n\n        if self.use_short_conv:\n            conv_state_q = last_state[0] if use_cache else None\n            conv_state_k = last_state[1] if use_cache else None\n            conv_state_v = last_state[2] if use_cache else None\n            k = self.k_proj(hidden_states)\n            v = self.v_proj(hidden_states)\n            q = self.q_proj(hidden_states)\n            q = self.q_conv1d(q, attention_mask, conv_state_q)\n            k = self.k_conv1d(k, attention_mask, conv_state_k)\n            v = self.v_conv1d(v, attention_mask, conv_state_v)\n        else:\n            q = (self.q_proj(hidden_states))\n            k = (self.k_proj(hidden_states))\n            v = self.silu(self.v_proj(hidden_states))\n\n        # dealing with left-padding\n        if attention_mask is not None:\n            v = v.mul_(attention_mask.unsqueeze(-1))\n\n        q, k, v = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=self.num_heads), (q, k, v))\n\n        if self.qk_activation != 'silu':\n            if self.qk_activation == 'relu':\n                q, k = q.relu(), k.relu()\n            elif self.qk_activation == 'elu':\n                q, k = elu_p1(q), elu_p1(k)\n            elif self.qk_activation == 'identity':\n                pass\n            else:\n                raise NotImplementedError\n\n        if self.qk_norm is not None:\n            if self.qk_norm == 'l2':\n                k = nn.functional.normalize(k, dim=-1, p=2).to(v)  # auto mixed precision type transfer is annoying.\n                q = nn.functional.normalize(q, dim=-1, p=2).to(v)\n            elif self.qk_norm == 'sum':\n                q = sum_norm(q).to(v)\n                k = sum_norm(k).to(v)\n\n        if self.use_beta:\n            beta = rearrange(self.b_proj(hidden_states), 'b l h -> b h l').sigmoid()\n        else:\n            beta = q.new_ones(q.shape[0], q.shape[1], q.shape[2])\n        state = past_key_values[self.layer_idx][-1] if use_cache else None\n        if mode == 'fused_recurrent':\n            o, recurrent_state = fused_recurrent_linear_attn_delta_rule(q, k, v, beta, state, output_final_state=use_cache)\n        elif mode == 'fused_chunk':\n            assert self.chunk_size in [16, 32, 64]\n            o, recurrent_state = fused_chunk_delta_rule(q, k, v, beta, self.chunk_size, state, output_final_state=use_cache)\n        elif mode == 'chunk':\n            assert self.chunk_size in [16, 32, 64]\n            o, recurrent_state = chunk_delta_rule(q, k, v, beta, self.chunk_size, state, output_final_state=use_cache)\n        else:\n            raise NotImplementedError(f\"Not supported mode `{mode}`.\")\n\n        if past_key_values is not None:\n            if self.use_short_conv:\n                state = (conv_state_q, conv_state_k, conv_state_v, recurrent_state)\n            else:\n                state = (recurrent_state,)\n            past_key_values.update(state, self.layer_idx)\n\n        o = rearrange(o, 'b h l d -> b l h d')\n        if self.use_gate:\n            g = rearrange(self.g_proj(hidden_states), 'b l (h d) -> b l h d', h=self.num_heads)\n            o = self.o_norm(o, g)\n        else:\n            o = self.o_norm(o)\n        o = rearrange(o, 'b l h d -> b l (h d)')\n        o = self.o_proj(o)\n\n        return o, None, past_key_values\n\n    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n        param = next(self.parameters())\n        state = tuple()\n        if self.use_short_conv:\n                # for q/k/v each\n            state += (param.new_zeros(batch_size, self.key_dim, self.conv_size),\n                        param.new_zeros(batch_size, self.key_dim, self.conv_size),\n                        param.new_zeros(batch_size, self.value_dim, self.conv_size))\n        state += (param.new_zeros(batch_size, self.num_heads, self.head_qk_dim, self.head_v_dim),)\n        return state"
}