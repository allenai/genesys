{
    "acronym": "liquids4",
    "title": "Liquid Structural State-Space Models",
    "seed_ids": [],
    "s2id": "b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc",
    "abstract": "A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.",
    "authors": [
        "Ramin M. Hasani",
        "Mathias Lechner",
        "Tsun-Hsuan Wang",
        "Makram Chahine",
        "Alexander Amini",
        "Daniela Rus"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": "The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.",
    "citationCount": 55,
    "influentialCitationCount": 8,
    "code": "import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.utils as U\nfrom functools import partial\nfrom einops import rearrange, repeat\nimport opt_einsum as oe\nimport itertools\n\noptimized = True\n\nif optimized:\n    contract = oe.contract\nelse:\n    contract = torch.einsum\n\nfrom src.models.sequence.ss.kernel import SSKernel, _conj\nfrom src.models.nn import LinearActivation, Activation, DropoutNd\n\nclass LeCun(nn.Module):\n    def __init__(self):\n        super(LeCun, self).__init__()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        return 1.7159 * self.tanh(0.666 * x)\n\n\nclass S4(nn.Module):\n    def __init__(\n            self,\n            d_model,\n            d_state=64,\n            l_max=None,\n            channels=1,\n            bidirectional=False,\n            # Arguments for position-wise feedforward components\n            activation='gelu',\n            postact='glu',\n            initializer=None,\n            weight_norm=False,\n            hyper_act=None,\n            dropout=0.0, tie_dropout=False,\n            bottleneck=None,\n            gate=None,\n            transposed=True,\n            verbose=False,\n            shift=False,\n            linear=False,\n            liquid_kernel=None,\n            liquid_degree=2,\n            allcombs=True,\n            lcontract=None,\n            # SSM Kernel arguments\n            **kernel_args,\n        ):\n        \"\"\"\n        d_state: the dimension of the state, also denoted by N\n        l_max: the maximum kernel length, also denoted by L. Set l_max=None to always use a global kernel\n        channels: can be interpreted as a number of \"heads\"; the SSM is a map from a 1-dim to C-dim sequence. It's not recommended to change this unless desperate for things to tune; instead, increase d_model for larger models\n        bidirectional: if True, convolution kernel will be two-sided\n\n        Position-wise feedforward components:\n        --------------------\n        activation: activation in between SS and FF\n        postact: activation after FF\n        initializer: initializer on FF\n        weight_norm: weight normalization on FF\n        hyper_act: use a \"hypernetwork\" multiplication (experimental)\n        dropout: standard dropout argument. tie_dropout=True ties the dropout mask across the sequence length, emulating nn.Dropout1d\n\n        Other arguments:\n        --------------------\n        transposed: choose backbone axis ordering of (B, L, H) (if False) or (B, H, L) (if True) [B=batch size, L=sequence length, H=hidden dimension]\n        gate: add gated activation (GSS)\n        bottleneck: reduce SSM dimension (GSS)\n        shift: experimental option, shouldn't affect results\n        linear: Remove pointwise components so that the entire module is a linear SSM\n\n        See the class .kernel.SSKernel for the kernel constructor which accepts kernel_args. Relevant options that are worth considering and tuning include \"mode\" + \"measure\", \"dt_min\", \"dt_max\", \"lr\"\n\n        Other options are all experimental and should not need to be configured\n        \"\"\"\n\n        super().__init__()\n        import src.utils.train\n        log = src.utils.train.get_logger(__name__)\n        if verbose:\n            log.info(f\"Constructing S4 (H, N, L) = ({d_model}, {d_state}, {l_max})\")\n            log = src.utils.train.get_logger(__name__)\n        if liquid_degree <= 1:\n            raise ValueError(f\"Illegal argument for liquid_degree ({liquid_degree}). Valid options are >= 2\")\n        if liquid_kernel is not None:\n            log.info(f\"Constructing liquid-S4 with liquid kernel '{liquid_kernel}' and degree {liquid_degree} (allcombs={allcombs})\")\n        else:\n            log.info(\n                f\"Using plain S4 (to enable liquid-S4 run with model.layer.liquid_degree='polyb'|'kb')\"\n            )\n        if liquid_kernel not in [None, \"polyb\",\"kb\"]:\n            raise ValueError(f\"Invalid argument for liquid_kernel ('{liquid_kernel}'). Use 'polyb', 'kb'\")\n        self.liquid_kernel = liquid_kernel\n        self.liquid_degree = liquid_degree\n\n        self.d_model = d_model\n        self.H = d_model\n        self.N = d_state\n        self.L = l_max\n        self.bidirectional = bidirectional\n        self.channels = channels\n        self.transposed = transposed\n        self.shift = shift\n        self.linear = linear\n        self.linear = linear\n        if lcontract==\"lecun\":\n            self.lcontract = LeCun()\n        elif lcontract==\"tanh\":\n            self.lcontract = nn.Tanh()\n        else:\n            self.lcontract = nn.Identity()\n\n\n        self.allcombs = allcombs\n\n        self.gate = gate\n        self.bottleneck = bottleneck\n\n        if bottleneck is not None:\n            self.H = self.H // bottleneck\n            self.input_linear = LinearActivation(\n                self.d_model,\n                self.H,\n                transposed=self.transposed,\n                initializer=initializer,\n                activation=activation,\n                activate=True,\n                weight_norm=weight_norm,\n            )\n\n        if gate is not None:\n            self.input_gate = LinearActivation(\n                self.d_model,\n                self.d_model * gate,\n                transposed=self.transposed,\n                initializer=initializer,\n                activation=activation,\n                activate=True,\n                weight_norm=weight_norm,\n            )\n            self.output_gate = LinearActivation(\n                self.d_model * gate,\n                self.d_model,\n                transposed=self.transposed,\n                initializer=initializer,\n                activation=None,\n                activate=False,\n                weight_norm=weight_norm,\n            )\n\n        # optional multiplicative modulation GLU-style\n        # https://arxiv.org/abs/2002.05202\n        self.hyper = hyper_act is not None\n        if self.hyper:\n            channels *= 2\n            self.hyper_activation = Activation(hyper_act)\n\n        self.D = nn.Parameter(torch.randn(channels, self.H))\n\n        if self.bidirectional:\n            channels *= 2\n\n\n        # SSM Kernel\n        self.kernel = SSKernel(self.H, N=self.N, L=self.L, channels=channels, verbose=verbose, **kernel_args)\n        log.info(f\"Using S4 kernel {self.kernel.mode}\")\n        # Pointwise\n        if not self.linear:\n            self.activation = Activation(activation)\n            # dropout_fn = nn.Dropout2d if self.transposed else nn.Dropout # Broken in torch==1.11\n            dropout_fn = DropoutNd if tie_dropout else nn.Dropout\n            self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n        # position-wise output transform to mix features\n        if not self.linear:\n            self.output_linear = LinearActivation(\n                self.H*self.channels,\n                self.d_model*(1 if self.gate is None else self.gate),\n                transposed=self.transposed,\n                initializer=initializer,\n                activation=postact,\n                activate=True,\n                weight_norm=weight_norm,\n            )\n        self._allcombs_index_cache = None\n        self._allcombs_cache_L = None\n\n    def get_combs_cache(self,seq_len, i ):\n        if self._allcombs_cache_L != seq_len:\n            self._allcombs_index_cache = []\n            self._allcombs_cache_L = seq_len\n            for p in range(2, self.liquid_degree + 1):\n                selected_count = 1\n                for n in range(2, seq_len):\n                    count = math.comb(n, p)\n                    if count >= seq_len:\n                        selected_count = n\n                        break\n                indices = range(seq_len - selected_count, seq_len)\n                indices = list(itertools.combinations(indices, p))\n                # print(f\"p={p}, seq_len={seq_len}, selected_count={selected_count}\",)\n                # print(f\"{len(indices)=}\")\n                if len(indices) != seq_len:\n                    # select exactly amount to match sequence length dimension\n                    indices = indices[-seq_len:]\n                indices = torch.LongTensor(indices)\n                self._allcombs_index_cache.append((p, indices))\n        return self._allcombs_index_cache[i]\n\n    def upgrade_degree(self,us,u,i):\n        seq_len = u.size(-1)\n        if self.allcombs:\n            p, indices = self.get_combs_cache(seq_len,i)\n            us = u[..., indices[:, 0]]\n            for j in range(1, p):\n                us = us * u[..., indices[:, j]]\n            if us.size(-1) != u.size(-1):\n                us = F.pad(us, (0, u.size(-1) - us.size(-1)))\n        else:\n            us_shift = torch.nn.functional.pad(us[..., :-1], (1, 0), \"constant\", 0)\n            us = us * us_shift\n        return us\n\n\n    def forward(self, u, state=None, rate=1.0, lengths=None, **kwargs): # absorbs return_output and transformer src mask\n        \"\"\"\n        u: (B H L) if self.transposed else (B L H)\n        state: (H N) never needed unless you know what you're doing\n\n        Returns: same shape as u\n        \"\"\"\n        if not self.transposed: u = u.transpose(-1, -2)\n        L = u.size(-1)\n\n        # Mask out padding tokens\n        # TODO handle option for mask - instead of lengths, which assumes suffix padding\n        if isinstance(lengths, int):\n            if lengths != L:\n                lengths = torch.tensor(lengths, dtype=torch.long, device=u.device)\n            else:\n                lengths = None\n        if lengths is not None:\n            assert isinstance(lengths, torch.Tensor) and lengths.ndim == 1 and lengths.size(0) in [1, u.size(0)]\n            mask = torch.where(torch.arange(L, device=lengths.device) < lengths[:, None, None], 1., 0.)\n            u = u * mask\n\n        if self.gate is not None:\n            v = self.input_gate(u)\n        if self.bottleneck is not None:\n            u = self.input_linear(u)\n\n        # Compute SS Kernel\n        L_kernel = L if self.L is None else min(L, round(self.L / rate))\n        k, k_state = self.kernel(L=L_kernel, rate=rate, state=state) # (C H L) (B C H L)\n\n        if self.bidirectional:\n            k0, k1 = rearrange(k, '(s c) h l -> s c h l', s=2)\n            k = F.pad(k0, (0, L)) \\\n                    + F.pad(k1.flip(-1), (L, 0)) \\\n\n        # Convolution\n        if self.shift:\n            # Try flip and pad to correct for potential off-by-one\n            k_f = torch.fft.rfft(F.pad(k.flip(-1), (L, 0)), n=2*L) # (C H L)\n            u_f = torch.fft.rfft(F.pad(u.flip(-1), (L, 0)), n=2*L) # (B H L)\n            y_f = contract('bhl,chl->bchl', u_f, k_f) # k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)\n            y = torch.fft.irfft(y_f, n=L_kernel+L)[..., L:].flip(-1) # (B C H L)\n            if self.liquid_kernel == \"kb\":\n                raise NotImplementedError()\n        else:\n            k_f = torch.fft.rfft(k, n=L_kernel+L) # (C H L)\n            u_f = torch.fft.rfft(u, n=L_kernel+L) # (B H L)\n            y_f = contract('bhl,chl->bchl', u_f, k_f)\n\n            y_sum = y_f\n            if self.liquid_kernel == \"kb\":\n                # Approximates the liquid kernel in the fourier space by the product of K-bar and B\n                k_b = k\n                us = u\n                dt = torch.exp(self.kernel.log_dt.to(u.device))\n                B = _conj(self.kernel.B).to(u.device)\n                w = _conj(self.kernel.w).to(u.device)\n                dB = torch.diag_embed(1.0 / (1.0 - 0.5 * dt[:, None] * w))  # (256,64,64)\n                dB = dt[:, None] * contract(\"dab,db->da\", dB, B)\n                dB = dB.unsqueeze(0).unsqueeze(-1)\n\n                for i in range(self.liquid_degree-1):\n                    us = self.upgrade_degree(us,u,i)\n                    k_b = k_b.unsqueeze(2)\n                    k_b = contract('abcd,abcd->abd', k_b, dB) # Kbar times B\n                    u_corr = self.lcontract(us)\n                    u_corr = u_corr.flip(dims=[-1])\n                    k_b_f = torch.fft.fft(k_b, n=(L_kernel + L)//2 + 1)  # complex FFT\n                    u_corr_f = torch.fft.rfft(u_corr, n=L_kernel + L)  # real-valued FFT\n                    y_corr_f = contract('bhl,chl->bchl', u_corr_f, k_b_f) # Convolution (Multiplication in FFT domain)\n                    y_sum = y_sum + y_corr_f\n\n            y = torch.fft.irfft(y_sum, n=L_kernel+L)[..., :L] # (B C H L)\n\n        # Compute D term in state space equation - essentially a skip connection\n        y = y + contract('bhl,ch->bchl', u, self.D)\n\n        if self.liquid_kernel == \"polyb\":\n            # Approximates the liquid kernel by computing only the polynomials involving B up to a certain degree\n            dt = torch.exp(self.kernel.log_dt.to(u.device))\n            B = _conj(self.kernel.B).to(u.device)\n            dC = _conj(self.kernel.C).to(u.device)\n            w = _conj(self.kernel.w).to(u.device)\n            dB = torch.diag_embed(1.0 / (1.0 - 0.5 * dt[:, None] * w))  #  (256,64,64)\n\n            dB = dt[:, None] * contract(\"dab,db->da\", dB, B)\n            us = u\n            for i in range(self.liquid_degree-1):\n                # print(f\"[Liquid={self.liquid}] Generating degree {i+1} input polynomial\")\n                us = self.upgrade_degree(us, u, i)\n                u_corr = self.lcontract(us)\n                us_corr = torch.flip(u_corr,[-1])\n                dB1 = dB.unsqueeze(2)\n                dB2 = dB.unsqueeze(1)\n                dB = (dB1 * dB2).sum(2)\n                dCB = contract(\"abc,bc->ab\", dC, dB).unsqueeze(2)\n                if self.bidirectional:\n                    fwd, bwd = dCB.unbind(0)\n                    fwd, bwd = fwd.unsqueeze(0), bwd.unsqueeze(0)\n                    y = (\n                        y\n                        + (us_corr * fwd).unsqueeze(1).float()\n                        + (us_corr.flip(2) * bwd).unsqueeze(1).float()\n                    )\n                else:\n\n                    y = y + (us_corr * dCB).unsqueeze(1).float()\n\n        # Compute state update\n        if state is not None:\n            assert not self.bidirectional, \"Bidirectional not supported with state forwarding\"\n            y = y + k_state #\n            next_state = self.kernel.forward_state(u, state)\n        else:\n            next_state = None\n\n        # Optional hyper-network multiplication\n        if self.hyper:\n            y, yh = rearrange(y, 'b (s c) h l -> s b c h l', s=2)\n            y = self.hyper_activation(yh) * y\n\n        # Reshape to flatten channels\n        y = rearrange(y, '... c h l -> ... (c h) l')\n\n        if not self.linear:\n            y = self.dropout(self.activation(y))\n\n        if not self.transposed: y = y.transpose(-1, -2)\n\n        if not self.linear:\n            y = self.output_linear(y)\n\n        if self.gate is not None:\n            y = self.output_gate(y * v)\n\n        return y, next_state\n\n    def setup_step(self, **kwargs):\n        self.kernel._setup_step(**kwargs)\n\n    def step(self, u, state):\n        \"\"\" Step one time step as a recurrent model. Intended to be used during validation.\n\n        u: (B H)\n        state: (B H N)\n        Returns: output (B H), state (B H N)\n        \"\"\"\n        assert not self.training\n\n        y, next_state = self.kernel.step(u, state) # (B C H)\n        y = y + u.unsqueeze(-2) * self.D\n        y = rearrange(y, 'b c h -> b (c h)')\n        y = self.activation(y)\n        if self.transposed:\n            y = self.output_linear(y.unsqueeze(-1)).squeeze(-1)\n        else:\n            y = self.output_linear(y)\n        return y, next_state\n\n    def default_state(self, *batch_shape, device=None):\n        # kernel is not a SequenceModule so it doesn't need to adhere to same interface\n        # the kernel will know the device of its own parameters\n        return self.kernel.default_state(*batch_shape)\n\n    @property\n    def d_state(self):\n        return self.H * self.N\n\n    @property\n    def d_output(self):\n        return self.d_model\n\n    @property\n    def state_to_tensor(self):\n        return lambda state: rearrange('... h n -> ... (h n)', state)",
    "description": null,
    "url": null
}