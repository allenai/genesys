{
    "acronym": "hopfield",
    "title": "Hopfield Networks is All You Need",
    "seed_ids": [
        "transformer",
        "bert"
    ],
    "s2id": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5",
    "abstract": "We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: this https URL",
    "authors": [
        "Hubert Ramsauer",
        "Bernhard Schafl",
        "Johannes Lehner",
        "Philipp Seidl",
        "Michael Widrich",
        "Lukas Gruber",
        "Markus Holzleitner",
        "Milena Pavlovi'c",
        "G. K. Sandve",
        "Victor Greiff",
        "David P. Kreil",
        "Michael Kopp",
        "G. Klambauer",
        "Johannes Brandstetter",
        "Sepp Hochreiter"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "tldr": "A new PyTorch layer is provided, called \"Hopfield\", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention.",
    "citationCount": 313,
    "influentialCitationCount": 45,
    "code": "import torch\nimport torch.nn as nn\n\nfrom math import sqrt\nfrom torch import Tensor\nfrom typing import Optional, Tuple, Union\n\nfrom torch.nn import Linear, Module, Parameter\n\ntry:\n    from torch.nn.modules.linear import _LinearWithBias\nexcept ImportError:\n    _LinearWithBias = None\n\n\ndef hopfield_core_forward(query,                           # type: Tensor\n                          key,                             # type: Tensor\n                          value,                           # type: Tensor\n                          embed_dim_to_check,              # type: int\n                          num_heads,                       # type: int\n                          in_proj_weight,                  # type: Optional[Tensor]\n                          in_proj_bias,                    # type: Optional[Tensor]\n                          bias_k,                          # type: Optional[Tensor]\n                          bias_v,                          # type: Optional[Tensor]\n                          add_zero_attn,                   # type: bool\n                          dropout_p,                       # type: float\n                          out_proj_weight,                 # type: Tensor\n                          out_proj_bias,                   # type: Tensor\n                          training=True,                   # type: bool\n                          key_padding_mask=None,           # type: Optional[Tensor]\n                          need_weights=True,               # type: bool\n                          attn_mask=None,                  # type: Optional[Tensor]\n                          use_separate_proj_weight=False,  # type: bool\n                          q_proj_weight=None,              # type: Optional[Tensor]\n                          k_proj_weight=None,              # type: Optional[Tensor]\n                          v_proj_weight=None,              # type: Optional[Tensor]\n                          static_k=None,                   # type: Optional[Tensor]\n                          static_v=None,                   # type: Optional[Tensor]\n\n                          key_as_static=False,             # type: bool\n                          query_as_static=False,           # type: bool\n                          value_as_static=False,           # type: bool\n                          value_as_connected=False,        # type: bool\n                          normalize_pattern=False,         # type: bool\n                          normalize_pattern_eps=1e-5,      # type: float\n                          p_norm_weight=None,              # type: Optional[Tensor]\n                          p_norm_bias=None,                # type: Optional[Tensor]\n                          head_dim=None,                   # type: Optional[int]\n                          pattern_dim=None,                # type: Optional[int]\n                          scaling=None,                    # type: Optional[Union[float, Tensor]]\n                          update_steps_max=0,              # type: Optional[Union[int, Tensor]]\n                          update_steps_eps=1e-4,           # type: Union[float, Tensor]\n                          return_raw_associations=False,   # type: bool\n                          return_projected_patterns=False  # type: bool\n                          ):\n    # type: (...) -> Tuple[Tensor, Optional[Tensor]]\n    r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n            See \"Hopfield Networks is All You Need\" for more details in the setting of Hopfield networks.\n        embed_dim_to_check: total dimension of the model (in case of default head dimension).\n        num_heads: parallel attention heads.\n        in_proj_weight, in_proj_bias: input projection weight and bias.\n        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        dropout_p: probability of an element to be zeroed.\n        out_proj_weight, out_proj_bias: the output projection weight and bias.\n        training: apply dropout if is ``True``.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. This is an binary mask. When the value is True,\n            the corresponding value on the attention layer will be filled with -inf.\n        need_weights: output attn_output_weights.\n        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n        use_separate_proj_weight: the function accept the proj. weights for query, key,\n            and value in different forms. If false, in_proj_weight will be used, which is\n            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n        static_k, static_v: static key and value used for attention operators.\n\n        key_as_static: interpret specified key as being static.\n        query_as_static: interpret specified key as being static.\n        value_as_static: interpret specified key as being static.\n        value_as_connected: connect value projection with key projection.\n        normalize_pattern: enable normalization of patterns.\n        normalize_pattern_eps: offset of the denominator for numerical stability.\n        p_norm_weight, p_norm_bias: pattern normalization weight and bias.\n        head_dim: dimensionality of each head.\n        pattern_dim: dimensionality of each projected value input.\n        scaling: scaling of association heads, often represented as beta (one entry per head).\n        update_steps_max: maximum count of association update steps (None equals to infinity).\n        update_steps_eps: minimum difference threshold between two consecutive association update steps.\n        return_raw_associations: return raw association (softmax) values, unmodified.\n        return_projected_patterns: return pattern projection values, unmodified.\n\n    Shape:\n        Inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n          will be unchanged. If a BoolTensor is provided, the positions with the\n          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n          is provided, it will be added to the attention weight.\n        - static_k: :math:`(N*num_heads, S, head_dim)`, where S is the source sequence length, N is the batch size.\n        - static_v: :math:`(N*num_heads, S, head_dim)`, where S is the source sequence length, N is the batch size.\n\n        - scaling: :math:`(num_heads,)`, where num_heads is the amount of heads.\n\n        Outputs:\n        - attn_output: :math:`(L, N, E)`, where L is the target sequence length, N is the batch size,\n          E is the embedding dimension.\n        - attn_output_weights: :math:`(N, L, S)`, where N is the batch size,\n          L is the target sequence length, S is the source sequence length.\n        - attn_raw: :math:``(N, num_heads, L, S)`, where N is the batch size,\n          L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if not torch.jit.is_scripting():\n        tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v,\n                    out_proj_weight, out_proj_bias)\n        if any([type(t) is not Tensor for t in tens_ops]) and nn.functional.has_torch_function(tens_ops):\n            return nn.functional.handle_torch_function(\n                hopfield_core_forward, tens_ops, query, key, value,\n                embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias,\n                bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight,\n                out_proj_bias, training=training, key_padding_mask=key_padding_mask,\n                need_weights=need_weights, attn_mask=attn_mask,\n                use_separate_proj_weight=use_separate_proj_weight,\n                q_proj_weight=q_proj_weight, k_proj_weight=k_proj_weight,\n                v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v,\n                key_as_static=key_as_static, query_as_static=query_as_static,\n                value_as_static=value_as_static, value_as_connected=value_as_connected,\n                normalize_pattern=normalize_pattern, normalize_pattern_eps=normalize_pattern_eps,\n                p_norm_weight=p_norm_weight, p_norm_bias=p_norm_bias,\n                head_dim=head_dim, pattern_dim=pattern_dim, scaling=scaling, update_steps_max=update_steps_max,\n                update_steps_eps=update_steps_eps, return_raw_associations=return_raw_associations)\n    tgt_len, bsz, embed_dim = query.shape[0], value.shape[1], query.shape[2]\n    assert embed_dim == embed_dim_to_check\n    # allow MHA to have different sizes for the feature dimension\n    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n\n    assert (scaling is None) or (type(scaling) in (float, torch.Tensor))\n    if type(scaling) == torch.Tensor:\n        assert scaling.ndimension() == 1 and scaling.shape[0] == num_heads, \"only one entry per head.\"\n\n    assert (update_steps_max is None) or (type(update_steps_max) in (int, torch.Tensor))\n    if type(update_steps_max) == torch.Tensor:\n        assert update_steps_max.ndimension() == 1 and update_steps_max.shape[0] == num_heads, \"only one entry per head.\"\n    elif type(update_steps_max) == int:\n        update_steps_max = torch.tensor([update_steps_max] * num_heads, dtype=torch.int32, device=query.device)\n    elif update_steps_max is None:\n        update_steps_max = -torch.ones(size=(num_heads,), dtype=torch.int32, device=query.device)\n\n    assert type(update_steps_eps) in (float, torch.Tensor)\n    if type(update_steps_eps) == torch.Tensor:\n        assert update_steps_eps.ndimension() == 1 and update_steps_eps.shape[0] == num_heads, \"only one entry per head.\"\n        assert (update_steps_eps <= 0.0).sum() == 0, \"only positive thresholds allowed.\"\n        update_steps_eps = update_steps_eps.to(device=query.device)\n    elif type(update_steps_eps) == float:\n        assert update_steps_eps > 0, \"only positive thresholds allowed.\"\n        update_steps_eps = torch.tensor([update_steps_eps] * num_heads, dtype=query.dtype, device=query.device)\n\n    # Adapt dimensionality of each each.\n    if head_dim is None:\n        head_dim = embed_dim // num_heads\n        assert head_dim * num_heads == embed_dim, r'embed_dim must be divisible by num_heads.'\n    hopfield_dim = num_heads * head_dim\n\n    # Adapt dimensionality of each value projection.\n    if pattern_dim is None:\n        pattern_dim = head_dim\n    assert (not value_as_connected) or (pattern_dim == head_dim)\n\n    q, k, v, xi, src_len = None, None, None, None, 0\n    update_step, xi_old, xi_difference_norm = 0, None, float(r'+inf')\n    update_active_heads = torch.tensor([[[True]]] * num_heads * bsz, device=query.device)\n    assert update_active_heads.any(), \"at least one head needs to be active.\"\n\n    ####################################################################################################################\n    #                                         BEGIN HOPFIELD UPDATE ITERATION                                          #\n    ####################################################################################################################\n\n    while update_active_heads.any():\n\n        # The query is already projected into the \"Hopfield\" space at \"update_step\" equals 0.\n        # No more projection necessary if \"update_step\" greater than 0.\n        if update_step == 0:\n            if not use_separate_proj_weight:\n\n                if torch.equal(query, key) and torch.equal(key, value) and not (\n                        key_as_static or query_as_static or value_as_static):\n                    # self-attention\n                    q, k, v = nn.functional.linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n\n                elif torch.equal(key, value) and not (key_as_static or value_as_static):\n                    # encoder-decoder attention\n                    _start, _end = 0, hopfield_dim\n                    if query_as_static:\n                        q = query.repeat(1, num_heads, 1)\n                    else:\n                        # This is inline in_proj function with in_proj_weight and in_proj_bias\n                        _b = in_proj_bias\n                        _w = in_proj_weight[_start:_end, :]\n                        if _b is not None:\n                            _b = _b[_start:_end]\n                        q = nn.functional.linear(query, _w, _b)\n                        _start = hopfield_dim\n                    _end = None\n\n                    if key is None:\n                        assert value is None\n                        k = None\n                        v = None\n                    else:\n\n                        # This is inline in_proj function with in_proj_weight and in_proj_bias\n                        _b = in_proj_bias\n                        _w = in_proj_weight[_start:_end, :]\n                        if _b is not None:\n                            _b = _b[_start:_end]\n                        k, v = nn.functional.linear(key, _w, _b).chunk(2, dim=-1)\n\n                else:\n                    _start, _end = 0, hopfield_dim\n                    if query_as_static:\n                        q = query.repeat(1, num_heads, 1)\n                    else:\n                        # This is inline in_proj function with in_proj_weight and in_proj_bias\n                        _b = in_proj_bias\n                        _w = in_proj_weight[_start:_end, :]\n                        if _b is not None:\n                            _b = _b[_start:_end]\n                        q = nn.functional.linear(query, _w, _b)\n                        _start += hopfield_dim\n                        _end += hopfield_dim\n\n                    if key_as_static:\n                        k = key.repeat(1, num_heads, 1)\n                    else:\n                        # This is inline in_proj function with in_proj_weight and in_proj_bias\n                        _b = in_proj_bias\n                        _w = in_proj_weight[_start:_end, :]\n                        if _b is not None:\n                            _b = _b[_start:_end]\n                        k = nn.functional.linear(key, _w, _b)\n                        _start += hopfield_dim\n                        _end += hopfield_dim\n\n                    if value_as_static:\n                        v = value.repeat(1, num_heads, 1)\n                    else:\n                        # This is inline in_proj function with in_proj_weight and in_proj_bias\n                        _b = in_proj_bias\n                        _w = in_proj_weight[_start:_end, :]\n                        if _b is not None:\n                            _b = _b[_start:_end]\n                        v = nn.functional.linear(value, _w, _b)\n            else:\n                _start, _end = 0, hopfield_dim\n                if query_as_static:\n                    q = query.repeat(1, num_heads, 1)\n                else:\n                    q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n                    len1, len2 = q_proj_weight_non_opt.size()\n                    assert len1 == hopfield_dim and len2 == query.size(-1)\n                    if in_proj_bias is not None:\n                        q = nn.functional.linear(query, q_proj_weight_non_opt, in_proj_bias[_start:_end])\n                        _start += hopfield_dim\n                        _end += hopfield_dim\n                    else:\n                        q = nn.functional.linear(query, q_proj_weight_non_opt, in_proj_bias)\n\n                v = value\n                if key_as_static:\n                    k = key.repeat(1, num_heads, 1)\n                else:\n                    k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n                    len1, len2 = k_proj_weight_non_opt.size()\n                    assert len1 == hopfield_dim and len2 == key.size(-1)\n\n                    _bias = None if in_proj_bias is None else in_proj_bias[_start:_end]\n                    k = nn.functional.linear(key, k_proj_weight_non_opt, _bias)\n                    if value_as_connected:\n                        v = nn.functional.linear(v, k_proj_weight_non_opt, _bias)\n                    _start += hopfield_dim\n                    _end += num_heads * pattern_dim\n\n                if value_as_static:\n                    if not (value_as_connected or key_as_static):\n                        v = v.repeat(1, num_heads, 1)\n                else:\n                    v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n                    len1, len2 = v_proj_weight_non_opt.size()\n                    assert len1 == (num_heads * pattern_dim) and len2 == v.size(-1)\n                    if in_proj_bias is not None:\n                        v = nn.functional.linear(v, v_proj_weight_non_opt, in_proj_bias[_start:])\n                    else:\n                        v = nn.functional.linear(v, v_proj_weight_non_opt, in_proj_bias)\n\n            if attn_mask is not None:\n                assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n                       attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or \\\n                       attn_mask.dtype == torch.bool, \\\n                       'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n                if attn_mask.dtype == torch.uint8:\n                    warnings.warn(\n                        \"Byte tensor for attn_mask in nn.HopfieldCore is deprecated. Use bool tensor instead.\")\n                    attn_mask = attn_mask.to(torch.bool)\n\n                if attn_mask.dim() == 2:\n                    attn_mask = attn_mask.unsqueeze(0)\n                    if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n                        raise RuntimeError('The size of the 2D attn_mask is not correct.')\n                elif attn_mask.dim() == 3:\n                    if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n                        raise RuntimeError('The size of the 3D attn_mask is not correct.')\n                else:\n                    raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n                # attn_mask's dim is 3 now.\n\n            # Optionally normalize patterns.\n            if normalize_pattern:\n                q = torch.nn.functional.layer_norm(\n                    input=q.reshape(shape=(-1, head_dim)), normalized_shape=(head_dim,),\n                    weight=p_norm_weight, bias=p_norm_bias, eps=normalize_pattern_eps).reshape(shape=q.shape)\n                k = torch.nn.functional.layer_norm(\n                    input=k.reshape(shape=(-1, head_dim)), normalized_shape=(head_dim,),\n                    weight=p_norm_weight, bias=p_norm_bias, eps=normalize_pattern_eps).reshape(shape=k.shape)\n\n        else:\n            active_xi = xi.masked_select(mask=update_active_heads).view(size=(-1, *xi.shape[1:]))\n            active_k = k.masked_select(mask=update_active_heads).view(size=(-1, *k.shape[1:]))\n            q = torch.masked_scatter(input=q, mask=update_active_heads, source=torch.bmm(active_xi, active_k))\n\n        # Optionally scale association heads (each head separately).\n        if type(scaling) == float:\n            q = q * scaling\n        elif type(scaling) == torch.Tensor:\n            q = q * scaling.view(1, 1, -1).repeat(repeats=(1, 1, q.shape[2] // scaling.shape[0]))\n\n        if update_step == 0:\n            # convert ByteTensor key_padding_mask to bool\n            if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n                warnings.warn(\n                    \"Byte tensor for key_padding_mask in nn.HopfieldCore is deprecated. Use bool tensor instead.\")\n                key_padding_mask = key_padding_mask.to(torch.bool)\n\n            if bias_k is not None and bias_v is not None:\n                if static_k is None and static_v is None and key_as_static is None and value_as_static is None:\n                    k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n                    v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n                    if attn_mask is not None:\n                        attn_mask = nn.functional.pad(attn_mask, [0, 1])\n                    if key_padding_mask is not None:\n                        key_padding_mask = nn.functional.pad(key_padding_mask, [0, 1])\n                else:\n                    assert static_k is None, \"bias cannot be added to static key.\"\n                    assert static_v is None, \"bias cannot be added to static value.\"\n                    assert not key_as_static, \"bias cannot be added to static key.\"\n                    assert not value_as_static, \"bias cannot be added to static value.\"\n            else:\n                assert bias_k is None\n                assert bias_v is None\n\n            q = q.contiguous().view(tgt_len, -1, head_dim).transpose(0, 1)\n            if k is not None:\n                k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n            if v is not None:\n                v = v.contiguous().view(v.shape[0], bsz * num_heads, -1).transpose(0, 1)\n\n            if static_k is not None:\n                assert static_k.size(0) == bsz * num_heads\n                assert static_k.size(2) == head_dim\n                k = static_k\n\n            if static_v is not None:\n                assert static_v.size(0) == bsz * num_heads\n                assert static_v.size(2) == pattern_dim\n                v = static_v\n\n            src_len = k.size(1)\n\n            if key_padding_mask is not None:\n                assert key_padding_mask.size(0) == bsz\n                assert key_padding_mask.size(1) == src_len\n\n            if add_zero_attn:\n                src_len += 1\n                k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n                v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n                if attn_mask is not None:\n                    attn_mask = nn.functional.pad(attn_mask, [0, 1])\n                if key_padding_mask is not None:\n                    key_padding_mask = nn.functional.pad(key_padding_mask, [0, 1])\n\n        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n        assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n\n        if attn_mask is not None:\n            if attn_mask.dtype == torch.bool:\n                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n            else:\n                attn_output_weights += attn_mask\n\n        if key_padding_mask is not None:\n            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n            attn_output_weights = attn_output_weights.masked_fill(\n                key_padding_mask.unsqueeze(1).unsqueeze(2),\n                float('-inf'),\n            )\n            attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n\n        # Compute new xi for Hopfield retrieve iterations.\n        if xi is None:\n            xi = nn.functional.softmax(attn_output_weights, dim=-1)\n        else:\n            xi = torch.masked_scatter(input=xi, mask=update_active_heads, source=nn.functional.softmax(\n                attn_output_weights.masked_select(mask=update_active_heads).view(size=(-1, *xi.shape[1:])), dim=-1))\n\n        # Compute threshold-based stopping criterion for Hopfield retrieve iterations.\n        with torch.no_grad():\n            xi_active = xi.view(size=(bsz, num_heads, tgt_len, src_len))\n            update_active_heads = (update_step < update_steps_max) | (update_steps_max < 0)\n            if xi_old is not None:\n                update_active_heads &= ((xi_old - xi_active).norm(p=2, dim=(2, 3)).max(axis=0)[0]) > update_steps_eps\n            update_active_heads = update_active_heads.unsqueeze(dim=1).unsqueeze(dim=2).repeat(repeats=(bsz, 1, 1))\n            xi_old = xi_active\n        update_step += 1\n\n    ####################################################################################################################\n    #                                          END HOPFIELD UPDATE ITERATION                                           #\n    ####################################################################################################################\n\n    attn_output_weights = nn.functional.dropout(xi, p=dropout_p, training=training)\n    attn_output = torch.bmm(attn_output_weights, v)\n    assert list(attn_output.shape[:2]) == [bsz * num_heads, tgt_len]\n    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, -1)\n    if out_proj_weight is not None:\n        assert attn_output.shape[2] == num_heads * pattern_dim\n        attn_output = nn.functional.linear(attn_output, out_proj_weight, out_proj_bias)\n\n    xi = xi.view(bsz, num_heads, tgt_len, src_len) if return_raw_associations else None\n    v = v.view(bsz, num_heads, src_len, -1) if return_projected_patterns else None\n    if need_weights:\n        # average attention weights over heads\n        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n        return attn_output, attn_output_weights.sum(dim=1) / num_heads, xi, v\n    else:\n        return attn_output, None, xi, v\n    \nclass HopfieldCore(Module):\n    r\"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See references: \"Hopfield Networks is All You Need\" and\n                    \"Attention Is All You Need\" (on which this implementation is partly based on).\n\n    .. math::\n        \\text{HopfieldHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\n    Args:\n        embed_dim: total dimension of the model.\n        num_heads: parallel attention heads.\n        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n        bias: add bias as module parameter. Default: True.\n        add_bias_kv: add bias to the key and value sequences at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        kdim: total number of features in key. Default: None.\n        vdim: total number of features in value. Default: None.\n\n        Note: if kdim and vdim are None, they will be set to embed_dim such that\n        query, key, and value have the same number of features.\n\n    Examples::\n\n        >>> hopfield_attn = HopfieldCore(embed_dim, num_heads)\n        >>> attn_output, attn_output_weights, attn_matrix = hopfield_attn(query, key, value)\n    \"\"\"\n    __annotations__ = {\n        'bias_k': torch._jit_internal.Optional[torch.Tensor],\n        'bias_v': torch._jit_internal.Optional[torch.Tensor],\n    }\n\n    def __init__(self,\n                 embed_dim=None,                  # type: Optional[int]\n                 num_heads=1,                     # type: int\n                 dropout=0.0,                     # type: float\n                 bias=True,                       # type: bool\n                 add_bias_kv=False,               # type: bool\n                 add_zero_attn=False,             # type: bool\n                 kdim=None,                       # type: Optional[int]\n                 vdim=None,                       # type: Optional[int]\n\n                 head_dim=None,                   # type: Optional[int]\n                 pattern_dim=None,                # type: Optional[int]\n                 out_dim=None,                    # type: Optional[int]\n                 disable_out_projection=False,    # type: bool\n                 key_as_static=False,             # type: bool\n                 query_as_static=False,           # type: bool\n                 value_as_static=False,           # type: bool\n                 value_as_connected=False,        # type: bool\n                 normalize_pattern=False,         # type: bool\n                 normalize_pattern_affine=False,  # type: bool\n                 normalize_pattern_eps=1e-5       # type: float\n                 ):\n        super(HopfieldCore, self).__init__()\n\n        assert (type(key_as_static) == bool) and (type(query_as_static) == bool) and (type(value_as_static) == bool)\n        self.key_as_static, self.query_as_static, self.value_as_static = key_as_static, query_as_static, value_as_static\n        num_non_static = 3 - (self.key_as_static + self.query_as_static + self.value_as_static)\n        assert 0 <= num_non_static < 4\n\n        self.value_as_connected = value_as_connected\n        self.normalize_pattern, self.normalize_pattern_affine = normalize_pattern, normalize_pattern_affine\n        self.normalize_pattern_eps = normalize_pattern_eps\n        self.disable_out_projection = disable_out_projection\n\n        # In case of a static-only executions, check corresponding projections and normalizations.\n        self.static_execution = self._check_execution_mode()\n        if self.static_execution:\n            embed_dim, kdim, vdim = None, None, None\n        if embed_dim is None:\n            assert self.static_execution, r'static-only execution requires all projections to be deactivated.'\n\n        # Check and set all other properties, conditioned on <static_execution>.\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self._qkv_same_embed_dim = all((\n            self.kdim == embed_dim, self.vdim == embed_dim, pattern_dim is None, not self.value_as_connected))\n        assert (not self.value_as_connected) or (self.kdim == self.vdim), r'key and value need to be of same dimension.'\n\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = None\n        self.pattern_dim = pattern_dim\n        self.virtual_hopfield_dim = None\n        self.virtual_pattern_dim = None\n        if not self.static_execution:\n            if head_dim is None:\n                self.head_dim = embed_dim // num_heads\n                assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads.\"\n            else:\n                assert head_dim > 0, \"dimension of the association space has to be positive.\"\n                self.head_dim = head_dim\n            if self.pattern_dim is None:\n                self.pattern_dim = self.head_dim\n            self.virtual_hopfield_dim = self.num_heads * self.head_dim\n            self.virtual_pattern_dim = self.num_heads * self.pattern_dim\n\n        self.out_dim = embed_dim if out_dim is None else out_dim\n        assert disable_out_projection or (self.out_dim > 0), \"output projection dimension has to be positive.\"\n\n        if normalize_pattern_affine:\n            assert normalize_pattern, \"affine pattern normalization without pattern normalization has no effect.\"\n            self.p_norm_weight = Parameter(torch.Tensor(head_dim))\n            self.p_norm_bias = Parameter(torch.Tensor(head_dim))\n        else:\n            self.register_parameter('p_norm_weight', None)\n            self.register_parameter('p_norm_bias', None)\n\n        if self._qkv_same_embed_dim is False:\n            if query_as_static:\n                self.register_parameter('q_proj_weight', None)\n            else:\n                self.q_proj_weight = Parameter(torch.Tensor(self.virtual_hopfield_dim, embed_dim))\n            if key_as_static:\n                self.register_parameter('k_proj_weight', None)\n            else:\n                self.k_proj_weight = Parameter(torch.Tensor(self.virtual_hopfield_dim, self.kdim))\n            if value_as_static:\n                self.register_parameter('v_proj_weight', None)\n            else:\n                self.v_proj_weight = Parameter(torch.Tensor(\n                    self.virtual_pattern_dim,\n                    self.virtual_hopfield_dim if (value_as_connected and not key_as_static) else self.vdim))\n            self.register_parameter('in_proj_weight', None)\n        else:\n            if num_non_static > 0:\n                self.in_proj_weight = Parameter(torch.empty(\n                    (not query_as_static) * self.virtual_hopfield_dim +\n                    (not key_as_static) * self.virtual_hopfield_dim +\n                    (not value_as_static) * self.virtual_pattern_dim, embed_dim))\n            else:\n                self.register_parameter('in_proj_weight', None)\n            self.register_parameter('q_proj_weight', None)\n            self.register_parameter('k_proj_weight', None)\n            self.register_parameter('v_proj_weight', None)\n\n        if bias and (num_non_static > 0):\n            self.in_proj_bias = Parameter(torch.empty(\n                (not query_as_static) * self.virtual_hopfield_dim +\n                (not key_as_static) * self.virtual_hopfield_dim + self.virtual_pattern_dim))\n        else:\n            self.register_parameter('in_proj_bias', None)\n        if disable_out_projection:\n            self.register_parameter('out_proj', None)\n        else:\n            if bias and _LinearWithBias is not None:\n                self.out_proj = _LinearWithBias(self.virtual_pattern_dim, self.out_dim)\n            else:\n                self.out_proj = Linear(self.virtual_pattern_dim, self.out_dim, bias=bias)\n\n        self.bias_k, self.bias_v = None, None\n        if add_bias_kv:\n            if not key_as_static:\n                self.bias_k = Parameter(torch.empty(1, 1, self.virtual_hopfield_dim))\n            if not value_as_static:\n                self.bias_v = Parameter(torch.empty(1, 1, self.virtual_hopfield_dim))\n            assert not (self.bias_k is None and self.bias_v is None), r'cannot set key/value bias if both are static.'\n\n        self.add_zero_attn = add_zero_attn\n        self.reset_parameters()\n\n    def _check_execution_mode(self) -> bool:\n        return all((\n            self.key_as_static, self.query_as_static, self.value_as_static, not self.value_as_connected,\n            not self.normalize_pattern, not self.normalize_pattern_affine, self.disable_out_projection\n        ))\n\n    def reset_parameters(self):\n        if self.p_norm_weight is not None:\n            nn.init.ones_(self.p_norm_weight)\n            nn.init.zeros_(self.p_norm_bias)\n\n        if self._qkv_same_embed_dim and (self.in_proj_weight is not None):\n            nn.init.normal_(self.in_proj_weight, mean=0.0, std=0.02)\n        else:\n            if self.q_proj_weight is not None:\n                nn.init.normal_(self.q_proj_weight, mean=0.0, std=0.02)\n            if self.k_proj_weight is not None:\n                nn.init.normal_(self.k_proj_weight, mean=0.0, std=0.02)\n            if self.v_proj_weight is not None:\n                nn.init.normal_(self.v_proj_weight, mean=0.0, std=0.02)\n\n        if self.in_proj_bias is not None:\n            nn.init.constant_(self.in_proj_bias, 0.0)\n        if not self.disable_out_projection:\n            nn.init.normal_(self.out_proj.weight, mean=0.0, std=0.02)\n            if self.out_proj.bias is not None:\n                nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.normal_(self.bias_k, mean=0.0, std=0.02)\n        if self.bias_v is not None:\n            nn.init.normal_(self.bias_v, mean=0.0, std=0.02)\n\n    def __setstate__(self, state):\n        super(HopfieldCore, self).__setstate__(state)\n\n    def forward(self,\n                query,                            # type: Tensor\n                key,                              # type: Tensor\n                value,                            # type: Tensor\n                key_padding_mask=None,            # type: Optional[Tensor]\n                need_weights=True,                # type: bool\n                attn_mask=None,                   # type: Optional[Tensor]\n\n                scaling=None,                     # type: Optional[Tensor]\n                update_steps_max=0,               # type: Optional[int]\n                update_steps_eps=1e-4,            # type: float\n                return_raw_associations=False,    # type: bool\n                return_pattern_projections=False  # type: bool\n                ):\n        # type: (...) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n        r\"\"\"\n        Args:\n            query, key, value: map a query and a set of key-value pairs to an output.\n                See \"Attention Is All You Need\" for more details.\n                See \"Hopfield Networks is All You Need\" for more details in the setting of Hopfield networks.\n            key_padding_mask: if provided, specified padding elements in the key will\n                be ignored by the attention. When given a binary mask and a value is True,\n                the corresponding value on the attention layer will be ignored. When given\n                a byte mask and a value is non-zero, the corresponding value on the attention\n                layer will be ignored.\n            need_weights: output attn_output_weights.\n            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n                the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n\n            scaling: scaling of association heads, often represented as beta (one entry per head).\n            update_steps_max: maximum count of association update steps (None equals to infinity).\n            update_steps_eps: minimum difference threshold between two consecutive association update steps.\n            return_raw_associations: return raw association (softmax) values, unmodified.\n            return_pattern_projections: return pattern projection values, unmodified.\n\n        Shape:\n            - Inputs:\n            - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n              the embedding dimension.\n            - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n              the embedding dimension.\n            - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n              the embedding dimension.\n            - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n              If a ByteTensor is provided, the non-zero positions will be ignored while the position\n              with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n              S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n              positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n              while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n              is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n              is provided, it will be added to the attention weight.\n\n            - scaling: :math:`(num_heads,)`, where num_heads is the amount of heads.\n\n            - Outputs:\n            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n              E is the embedding dimension.\n            - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n              L is the target sequence length, S is the source sequence length.\n            - attn_raw: :math:``(N, num_heads, L, S)`, where N is the batch size,\n              L is the target sequence length, S is the source sequence length.\n        \"\"\"\n        if self.query_as_static and self.key_as_static:\n            assert query.shape[2] == key.shape[2], \\\n                f'query shape[2] of {query.shape[2]} and key shape[2] of {key.shape[2]} need to be equal'\n            head_dim, embed_dim_to_check = query.shape[2], query.shape[2]\n        else:\n            assert self.query_as_static or (query.shape[2] == self.embed_dim), \\\n                f'query shape[2] of {query.shape[2]} invalid, needs to be {self.embed_dim}.'\n            assert (not self.query_as_static) or (self.query_as_static and query.shape[2] == self.head_dim), \\\n                f'query shape[2] of {query.shape[2]} invalid, needs to be {self.head_dim}'\n\n            assert self.key_as_static or (key.shape[2] == self.kdim), \\\n                f'key shape[2] of {key.shape[2]} invalid, needs to be {self.kdim}.'\n            assert (not self.key_as_static) or (self.key_as_static and key.shape[2] == self.head_dim), \\\n                f'key shape[2] of {key.shape[2]} invalid, needs to be {self.head_dim}'\n            head_dim, embed_dim_to_check = self.head_dim, self.head_dim if self.query_as_static else self.embed_dim\n\n        assert self.value_as_static or (value.shape[2] == self.vdim), \\\n            f'value shape[2] of {value.shape[2]} invalid, needs to be {self.vdim}.'\n        assert any((\n            not self.value_as_static, self.value_as_static and value.shape[2] == self.pattern_dim,\n            self.disable_out_projection)\n        ), f'value shape[2] of {value.shape[2]} invalid, needs to be {self.pattern_dim}'\n\n        out_weights, out_bias = None, None\n        if not self.disable_out_projection:\n            out_weights, out_bias = self.out_proj.weight, self.out_proj.bias\n\n        if not self._qkv_same_embed_dim:\n            return hopfield_core_forward(\n                query=query, key=key, value=value, embed_dim_to_check=embed_dim_to_check, num_heads=self.num_heads,\n                in_proj_weight=self.in_proj_weight, in_proj_bias=self.in_proj_bias, bias_k=self.bias_k,\n                bias_v=self.bias_v, add_zero_attn=self.add_zero_attn, dropout_p=self.dropout,\n                out_proj_weight=out_weights, out_proj_bias=out_bias, training=self.training,\n                key_padding_mask=key_padding_mask, need_weights=need_weights, attn_mask=attn_mask,\n                use_separate_proj_weight=True, q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n                v_proj_weight=self.v_proj_weight,\n\n                key_as_static=self.key_as_static, query_as_static=self.query_as_static,\n                value_as_static=self.value_as_static, value_as_connected=self.value_as_connected,\n                normalize_pattern=self.normalize_pattern, normalize_pattern_eps=self.normalize_pattern_eps,\n                p_norm_weight=self.p_norm_weight, p_norm_bias=self.p_norm_bias,\n                head_dim=head_dim, pattern_dim=self.pattern_dim, scaling=scaling,\n                update_steps_max=update_steps_max, update_steps_eps=update_steps_eps,\n                return_raw_associations=return_raw_associations, return_projected_patterns=return_pattern_projections)\n        else:\n            return hopfield_core_forward(\n                query=query, key=key, value=value, embed_dim_to_check=embed_dim_to_check, num_heads=self.num_heads,\n                in_proj_weight=self.in_proj_weight, in_proj_bias=self.in_proj_bias, bias_k=self.bias_k,\n                bias_v=self.bias_v, add_zero_attn=self.add_zero_attn, dropout_p=self.dropout,\n                out_proj_weight=out_weights, out_proj_bias=out_bias, training=self.training,\n                key_padding_mask=key_padding_mask, need_weights=need_weights, attn_mask=attn_mask,\n\n                key_as_static=self.key_as_static, query_as_static=self.query_as_static,\n                value_as_static=self.value_as_static, value_as_connected=self.value_as_connected,\n                normalize_pattern=self.normalize_pattern, normalize_pattern_eps=self.normalize_pattern_eps,\n                p_norm_weight=self.p_norm_weight, p_norm_bias=self.p_norm_bias,\n                head_dim=head_dim, pattern_dim=self.pattern_dim, scaling=scaling,\n                update_steps_max=update_steps_max, update_steps_eps=update_steps_eps,\n                return_raw_associations=return_raw_associations, return_projected_patterns=return_pattern_projections)\n        \n\nclass Hopfield(Module):\n    \"\"\"\n    Module with underlying Hopfield association.\n    \"\"\"\n\n    def __init__(self,\n                 input_size: Optional[int] = None,\n                 hidden_size: Optional[int] = None,\n                 output_size: Optional[int] = None,\n                 pattern_size: Optional[int] = None,\n                 num_heads: int = 1,\n                 scaling: Optional[Union[float, Tensor]] = None,\n                 update_steps_max: Optional[Union[int, Tensor]] = 0,\n                 update_steps_eps: Union[float, Tensor] = 1e-4,\n\n                 normalize_stored_pattern: bool = True,\n                 normalize_stored_pattern_affine: bool = True,\n                 normalize_stored_pattern_eps: float = 1e-5,\n                 normalize_state_pattern: bool = True,\n                 normalize_state_pattern_affine: bool = True,\n                 normalize_state_pattern_eps: float = 1e-5,\n                 normalize_pattern_projection: bool = True,\n                 normalize_pattern_projection_affine: bool = True,\n                 normalize_pattern_projection_eps: float = 1e-5,\n                 normalize_hopfield_space: bool = False,\n                 normalize_hopfield_space_affine: bool = False,\n                 normalize_hopfield_space_eps: float = 1e-5,\n                 stored_pattern_as_static: bool = False,\n                 state_pattern_as_static: bool = False,\n                 pattern_projection_as_static: bool = False,\n                 pattern_projection_as_connected: bool = False,\n                 stored_pattern_size: Optional[int] = None,\n                 pattern_projection_size: Optional[int] = None,\n\n                 batch_first: bool = True,\n                 association_activation: Optional[str] = None,\n                 dropout: float = 0.0,\n                 input_bias: bool = True,\n                 concat_bias_pattern: bool = False,\n                 add_zero_association: bool = False,\n                 disable_out_projection: bool = False\n                 ):\n        \"\"\"\n        Initialise new instance of a Hopfield module.\n\n        :param input_size: depth of the input (state pattern)\n        :param hidden_size: depth of the association space\n        :param output_size: depth of the output projection\n        :param pattern_size: depth of patterns to be selected\n        :param num_heads: amount of parallel association heads\n        :param scaling: scaling of association heads, often represented as beta (one entry per head)\n        :param update_steps_max: maximum count of association update steps (None equals to infinity)\n        :param update_steps_eps: minimum difference threshold between two consecutive association update steps\n        :param normalize_stored_pattern: apply normalization on stored patterns\n        :param normalize_stored_pattern_affine: additionally enable affine normalization of stored patterns\n        :param normalize_stored_pattern_eps: offset of the denominator for numerical stability\n        :param normalize_state_pattern: apply normalization on state patterns\n        :param normalize_state_pattern_affine: additionally enable affine normalization of state patterns\n        :param normalize_state_pattern_eps: offset of the denominator for numerical stability\n        :param normalize_pattern_projection: apply normalization on the pattern projection\n        :param normalize_pattern_projection_affine: additionally enable affine normalization of pattern projection\n        :param normalize_pattern_projection_eps: offset of the denominator for numerical stability\n        :param normalize_hopfield_space: enable normalization of patterns in the Hopfield space\n        :param normalize_hopfield_space_affine: additionally enable affine normalization of patterns in Hopfield space\n        :param normalize_hopfield_space_eps: offset of the denominator for numerical stability\n        :param stored_pattern_as_static: interpret specified stored patterns as being static\n        :param state_pattern_as_static: interpret specified state patterns as being static\n        :param pattern_projection_as_static: interpret specified pattern projections as being static\n        :param pattern_projection_as_connected: connect pattern projection with stored pattern\n        :param stored_pattern_size: depth of input (stored pattern)\n        :param pattern_projection_size: depth of input (pattern projection)\n        :param batch_first: flag for specifying if the first dimension of data fed to \"forward\" reflects the batch size\n        :param association_activation: additional activation to be applied on the result of the Hopfield association\n        :param dropout: dropout probability applied on the association matrix\n        :param input_bias: bias to be added to input (state and stored pattern as well as pattern projection)\n        :param concat_bias_pattern: bias to be concatenated to stored pattern as well as pattern projection\n        :param add_zero_association: add a new batch of zeros to stored pattern as well as pattern projection\n        :param disable_out_projection: disable output projection\n        \"\"\"\n        super(Hopfield, self).__init__()\n        assert type(batch_first) == bool, f'\"batch_first\" needs to be a boolean, not {type(batch_first)}.'\n        assert (association_activation is None) or (type(association_activation) == str)\n\n        # Initialise Hopfield association module.\n        self.association_core = HopfieldCore(\n            embed_dim=input_size, num_heads=num_heads, dropout=dropout, bias=input_bias,\n            add_bias_kv=concat_bias_pattern, add_zero_attn=add_zero_association, kdim=stored_pattern_size,\n            vdim=pattern_projection_size, head_dim=hidden_size, pattern_dim=pattern_size, out_dim=output_size,\n            disable_out_projection=disable_out_projection, key_as_static=stored_pattern_as_static,\n            query_as_static=state_pattern_as_static, value_as_static=pattern_projection_as_static,\n            value_as_connected=pattern_projection_as_connected, normalize_pattern=normalize_hopfield_space,\n            normalize_pattern_affine=normalize_hopfield_space_affine,\n            normalize_pattern_eps=normalize_hopfield_space_eps)\n        self.association_activation = None\n        if association_activation is not None:\n            self.association_activation = getattr(torch, association_activation, None)\n\n        # Initialise stored pattern normalization.\n        self.norm_stored_pattern = None\n        if normalize_stored_pattern_affine:\n            assert normalize_stored_pattern, \"affine normalization without normalization has no effect.\"\n        if normalize_stored_pattern:\n            normalized_shape = input_size if stored_pattern_size is None else stored_pattern_size\n            assert normalized_shape is not None, \"stored pattern size required for setting up normalisation\"\n            self.norm_stored_pattern = nn.LayerNorm(\n                normalized_shape=normalized_shape, elementwise_affine=normalize_stored_pattern_affine,\n                eps=normalize_stored_pattern_eps)\n\n        # Initialise state pattern normalization.\n        self.norm_state_pattern = None\n        if normalize_state_pattern_affine:\n            assert normalize_state_pattern, \"affine normalization without normalization has no effect.\"\n        if normalize_state_pattern:\n            assert input_size is not None, \"input size required for setting up normalisation\"\n            self.norm_state_pattern = nn.LayerNorm(\n                normalized_shape=input_size, elementwise_affine=normalize_state_pattern_affine,\n                eps=normalize_state_pattern_eps)\n\n        # Initialise pattern projection normalization.\n        self.norm_pattern_projection = None\n        if normalize_pattern_projection_affine:\n            assert normalize_pattern_projection, \"affine normalization without normalization has no effect.\"\n        if normalize_pattern_projection:\n            normalized_shape = input_size if pattern_projection_size is None else pattern_projection_size\n            assert normalized_shape is not None, \"pattern projection size required for setting up normalisation\"\n            self.norm_pattern_projection = nn.LayerNorm(\n                normalized_shape=normalized_shape, elementwise_affine=normalize_pattern_projection_affine,\n                eps=normalize_pattern_projection_eps)\n\n        # Initialise remaining auxiliary properties.\n        if self.association_core.static_execution:\n            self.__scaling = 1.0 if scaling is None else scaling\n        else:\n            assert self.association_core.head_dim > 0, f'invalid hidden dimension encountered.'\n            self.__scaling = (1.0 / sqrt(self.association_core.head_dim)) if scaling is None else scaling\n        self.__batch_first = batch_first\n        self.__update_steps_max = update_steps_max\n        self.__update_steps_eps = update_steps_eps\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        \"\"\"\n        Reset Hopfield association.\n\n        :return: None\n        \"\"\"\n        for module in (self.association_core, self.norm_stored_pattern,\n                       self.norm_state_pattern, self.norm_pattern_projection):\n            if hasattr(module, r'reset_parameters'):\n                module.reset_parameters()\n\n    def _maybe_transpose(self, *args: Tuple[Tensor, ...]) -> Union[Tensor, Tuple[Tensor, ...]]:\n        \"\"\"\n        Eventually transpose specified data.\n\n        :param args: tensors to eventually transpose (dependent on the state of \"batch_first\")\n        :return: eventually transposed tensors\n        \"\"\"\n        transposed_result = tuple(_.transpose(0, 1) for _ in args) if self.__batch_first else args\n        return transposed_result[0] if len(transposed_result) == 1 else transposed_result\n\n    def _associate(self, data: Union[Tensor, Tuple[Tensor, Tensor, Tensor]],\n                   return_raw_associations: bool = False, return_projected_patterns: bool = False,\n                   stored_pattern_padding_mask: Optional[Tensor] = None,\n                   association_mask: Optional[Tensor] = None) -> Tuple[Optional[Tensor], ...]:\n        \"\"\"\n        Apply Hopfield association module on specified data.\n\n        :param data: data to be processed by Hopfield core module\n        :param return_raw_associations: return raw association (softmax) values, unmodified\n        :param return_projected_patterns: return pattern projection values, unmodified\n        :param stored_pattern_padding_mask: mask to be applied on stored patterns\n        :param association_mask: mask to be applied on inner association matrix\n        :return: Hopfield-processed input data\n        \"\"\"\n        assert (type(data) == Tensor) or ((type(data) == tuple) and (len(data) == 3)), \\\n            r'either one tensor to be used as \"stored pattern\", \"state pattern\" and' \\\n            r' \"pattern_projection\" must be provided, or three separate ones.'\n        if type(data) == Tensor:\n            stored_pattern, state_pattern, pattern_projection = data, data, data\n        else:\n            stored_pattern, state_pattern, pattern_projection = data\n\n        # Optionally transpose data.\n        stored_pattern, state_pattern, pattern_projection = self._maybe_transpose(\n            stored_pattern, state_pattern, pattern_projection)\n\n        # Optionally apply stored pattern normalization.\n        if self.norm_stored_pattern is not None:\n            stored_pattern = self.norm_stored_pattern(input=stored_pattern.reshape(\n                shape=(-1, stored_pattern.shape[2]))).reshape(shape=stored_pattern.shape)\n\n        # Optionally apply state pattern normalization.\n        if self.norm_state_pattern is not None:\n            state_pattern = self.norm_state_pattern(input=state_pattern.reshape(\n                shape=(-1, state_pattern.shape[2]))).reshape(shape=state_pattern.shape)\n\n        # Optionally apply pattern projection normalization.\n        if self.norm_pattern_projection is not None:\n            pattern_projection = self.norm_pattern_projection(input=pattern_projection.reshape(\n                shape=(-1, pattern_projection.shape[2]))).reshape(shape=pattern_projection.shape)\n\n        # Apply Hopfield association and optional activation function.\n        return self.association_core(\n            query=state_pattern, key=stored_pattern, value=pattern_projection,\n            key_padding_mask=stored_pattern_padding_mask, need_weights=False, attn_mask=association_mask,\n            scaling=self.__scaling, update_steps_max=self.__update_steps_max, update_steps_eps=self.__update_steps_eps,\n            return_raw_associations=return_raw_associations, return_pattern_projections=return_projected_patterns)\n\n    def forward(self, input: Union[Tensor, Tuple[Tensor, Tensor, Tensor]],\n                stored_pattern_padding_mask: Optional[Tensor] = None,\n                association_mask: Optional[Tensor] = None) -> Tensor:\n        \"\"\"\n        Apply Hopfield association on specified data.\n\n        :param input: data to be processed by Hopfield association module\n        :param stored_pattern_padding_mask: mask to be applied on stored patterns\n        :param association_mask: mask to be applied on inner association matrix\n        :return: Hopfield-processed input data\n        \"\"\"\n        association_output = self._maybe_transpose(self._associate(\n            data=input, return_raw_associations=False,\n            stored_pattern_padding_mask=stored_pattern_padding_mask,\n            association_mask=association_mask)[0])\n        if self.association_activation is not None:\n            association_output = self.association_activation(association_output)\n        return association_output\n\n\nclass HopfieldPooling(Module):\n    \"\"\"\n    Wrapper class encapsulating a trainable but fixed state pattern and \"Hopfield\" in\n    one combined module to be used as a Hopfield-based pooling layer.\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: Optional[int] = None,\n                 output_size: Optional[int] = None,\n                 pattern_size: Optional[int] = None,\n                 num_heads: int = 1,\n                 scaling: Optional[Union[float, Tensor]] = None,\n                 update_steps_max: Optional[Union[int, Tensor]] = 0,\n                 update_steps_eps: Union[float, Tensor] = 1e-4,\n\n                 normalize_stored_pattern: bool = True,\n                 normalize_stored_pattern_affine: bool = True,\n                 normalize_state_pattern: bool = True,\n                 normalize_state_pattern_affine: bool = True,\n                 normalize_pattern_projection: bool = True,\n                 normalize_pattern_projection_affine: bool = True,\n                 normalize_hopfield_space: bool = False,\n                 normalize_hopfield_space_affine: bool = False,\n                 stored_pattern_as_static: bool = False,\n                 state_pattern_as_static: bool = False,\n                 pattern_projection_as_static: bool = False,\n                 pattern_projection_as_connected: bool = False,\n                 stored_pattern_size: Optional[int] = None,\n                 pattern_projection_size: Optional[int] = None,\n\n                 batch_first: bool = True,\n                 association_activation: Optional[str] = None,\n                 dropout: float = 0.0,\n                 input_bias: bool = True,\n                 concat_bias_pattern: bool = False,\n                 add_zero_association: bool = False,\n                 disable_out_projection: bool = False,\n                 quantity: int = 1,\n                 trainable: bool = True\n                 ):\n        \"\"\"\n        Initialise a new instance of a Hopfield-based pooling layer.\n\n        :param input_size: depth of the input (state pattern)\n        :param hidden_size: depth of the association space\n        :param output_size: depth of the output projection\n        :param pattern_size: depth of patterns to be selected\n        :param num_heads: amount of parallel association heads\n        :param scaling: scaling of association heads, often represented as beta (one entry per head)\n        :param update_steps_max: maximum count of association update steps (None equals to infinity)\n        :param update_steps_eps: minimum difference threshold between two consecutive association update steps\n        :param normalize_stored_pattern: apply normalization on stored patterns\n        :param normalize_stored_pattern_affine: additionally enable affine normalization of stored patterns\n        :param normalize_state_pattern: apply normalization on state patterns\n        :param normalize_state_pattern_affine: additionally enable affine normalization of state patterns\n        :param normalize_pattern_projection: apply normalization on the pattern projection\n        :param normalize_pattern_projection_affine: additionally enable affine normalization of pattern projection\n        :param normalize_hopfield_space: enable normalization of patterns in the Hopfield space\n        :param normalize_hopfield_space_affine: additionally enable affine normalization of patterns in Hopfield space\n        :param stored_pattern_as_static: interpret specified stored patterns as being static\n        :param state_pattern_as_static: interpret specified state patterns as being static\n        :param pattern_projection_as_static: interpret specified pattern projections as being static\n        :param pattern_projection_as_connected: connect pattern projection with stored pattern\n        :param stored_pattern_size: depth of input (stored pattern)\n        :param pattern_projection_size: depth of input (pattern projection)\n        :param batch_first: flag for specifying if the first dimension of data fed to \"forward\" reflects the batch size\n        :param association_activation: additional activation to be applied on the result of the Hopfield association\n        :param dropout: dropout probability applied on the association matrix\n        :param input_bias: bias to be added to input (state and stored pattern as well as pattern projection)\n        :param concat_bias_pattern: bias to be concatenated to stored pattern as well as pattern projection\n        :param add_zero_association: add a new batch of zeros to stored pattern as well as pattern projection\n        :param disable_out_projection: disable output projection\n        :param quantity: amount of state patterns\n        :param trainable: state pattern used for pooling is trainable\n        \"\"\"\n        super(HopfieldPooling, self).__init__()\n        self.hopfield = Hopfield(\n            input_size=input_size, hidden_size=hidden_size, output_size=output_size, pattern_size=pattern_size,\n            num_heads=num_heads, scaling=scaling, update_steps_max=update_steps_max, update_steps_eps=update_steps_eps,\n            normalize_stored_pattern=normalize_stored_pattern,\n            normalize_stored_pattern_affine=normalize_stored_pattern_affine,\n            normalize_state_pattern=normalize_state_pattern,\n            normalize_state_pattern_affine=normalize_state_pattern_affine,\n            normalize_pattern_projection=normalize_pattern_projection,\n            normalize_pattern_projection_affine=normalize_pattern_projection_affine,\n            normalize_hopfield_space=normalize_hopfield_space,\n            normalize_hopfield_space_affine=normalize_hopfield_space_affine,\n            stored_pattern_as_static=stored_pattern_as_static, state_pattern_as_static=state_pattern_as_static,\n            pattern_projection_as_static=pattern_projection_as_static,\n            pattern_projection_as_connected=pattern_projection_as_connected, stored_pattern_size=stored_pattern_size,\n            pattern_projection_size=pattern_projection_size, batch_first=batch_first,\n            association_activation=association_activation, dropout=dropout, input_bias=input_bias,\n            concat_bias_pattern=concat_bias_pattern, add_zero_association=add_zero_association,\n            disable_out_projection=disable_out_projection)\n        self._quantity = quantity\n        pooling_weight_size = self.hopfield.hidden_size if state_pattern_as_static else self.hopfield.input_size\n        self.pooling_weights = nn.Parameter(torch.empty(size=(*(\n            (1, quantity) if batch_first else (quantity, 1)\n        ), input_size if pooling_weight_size is None else pooling_weight_size)), requires_grad=trainable)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        \"\"\"\n        Reset pooling weights and underlying Hopfield association.\n\n        :return: None\n        \"\"\"\n        if hasattr(self.hopfield, r'reset_parameters'):\n            self.hopfield.reset_parameters()\n\n        # Explicitly initialise pooling weights.\n        nn.init.normal_(self.pooling_weights, mean=0.0, std=0.02)\n\n    def _prepare_input(self, input: Union[Tensor, Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"\n        Prepare input for Hopfield association.\n\n        :param input: data to be prepared\n        :return: stored pattern, expanded state pattern as well as pattern projection\n        \"\"\"\n        assert (type(input) == Tensor) or ((type(input) == tuple) and (len(input) == 2)), \\\n            r'either one tensor to be used as \"stored pattern\" and' \\\n            r' \"pattern_projection\" must be provided, or two separate ones.'\n        if type(input) == Tensor:\n            stored_pattern, pattern_projection = input, input\n        else:\n            stored_pattern, pattern_projection = input\n\n        batch_size = stored_pattern.shape[0 if self.batch_first else 1]\n        return stored_pattern, self.pooling_weights.expand(size=(*(\n            (batch_size, self.quantity) if self.batch_first else (self.quantity, batch_size)\n        ), self.pooling_weights.shape[2])), pattern_projection\n\n    def forward(self, input: Union[Tensor, Tuple[Tensor, Tensor]], stored_pattern_padding_mask: Optional[Tensor] = None,\n                association_mask: Optional[Tensor] = None) -> Tensor:\n        \"\"\"\n        Compute Hopfield-based pooling on specified data.\n\n        :param input: data to be pooled\n        :param stored_pattern_padding_mask: mask to be applied on stored patterns\n        :param association_mask: mask to be applied on inner association matrix\n        :return: Hopfield-pooled input data\n        \"\"\"\n        return self.hopfield(\n            input=self._prepare_input(input=input),\n            stored_pattern_padding_mask=stored_pattern_padding_mask,\n            association_mask=association_mask).flatten(start_dim=1)\n\n\n\nclass HopfieldLayer(Module):\n    \"\"\"\n    Wrapper class encapsulating a trainable but fixed stored pattern, pattern projection and \"Hopfield\" in\n    one combined module to be used as a Hopfield-based pooling layer.\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: Optional[int] = None,\n                 output_size: Optional[int] = None,\n                 pattern_size: Optional[int] = None,\n                 num_heads: int = 1,\n                 scaling: Optional[Union[float, Tensor]] = None,\n                 update_steps_max: Optional[Union[int, Tensor]] = 0,\n                 update_steps_eps: Union[float, Tensor] = 1e-4,\n                 lookup_weights_as_separated: bool = False,\n                 lookup_targets_as_trainable: bool = True,\n\n                 normalize_stored_pattern: bool = True,\n                 normalize_stored_pattern_affine: bool = True,\n                 normalize_state_pattern: bool = True,\n                 normalize_state_pattern_affine: bool = True,\n                 normalize_pattern_projection: bool = True,\n                 normalize_pattern_projection_affine: bool = True,\n                 normalize_hopfield_space: bool = False,\n                 normalize_hopfield_space_affine: bool = False,\n                 stored_pattern_as_static: bool = False,\n                 state_pattern_as_static: bool = False,\n                 pattern_projection_as_static: bool = False,\n                 pattern_projection_as_connected: bool = False,\n                 stored_pattern_size: Optional[int] = None,\n                 pattern_projection_size: Optional[int] = None,\n\n                 batch_first: bool = True,\n                 association_activation: Optional[str] = None,\n                 dropout: float = 0.0,\n                 input_bias: bool = True,\n                 concat_bias_pattern: bool = False,\n                 add_zero_association: bool = False,\n                 disable_out_projection: bool = False,\n                 quantity: int = 1,\n                 trainable: bool = True\n                 ):\n        \"\"\"\n        Initialise a new instance of a Hopfield-based lookup layer.\n\n        :param input_size: depth of the input (state pattern)\n        :param hidden_size: depth of the association space\n        :param output_size: depth of the output projection\n        :param pattern_size: depth of patterns to be selected\n        :param num_heads: amount of parallel association heads\n        :param scaling: scaling of association heads, often represented as beta (one entry per head)\n        :param update_steps_max: maximum count of association update steps (None equals to infinity)\n        :param update_steps_eps: minimum difference threshold between two consecutive association update steps\n        :param lookup_weights_as_separated: separate lookup weights from lookup target weights\n        :param lookup_targets_as_trainable: employ trainable lookup target weights (used as pattern projection input)\n        :param normalize_stored_pattern: apply normalization on stored patterns\n        :param normalize_stored_pattern_affine: additionally enable affine normalization of stored patterns\n        :param normalize_state_pattern: apply normalization on state patterns\n        :param normalize_state_pattern_affine: additionally enable affine normalization of state patterns\n        :param normalize_pattern_projection: apply normalization on the pattern projection\n        :param normalize_pattern_projection_affine: additionally enable affine normalization of pattern projection\n        :param normalize_hopfield_space: enable normalization of patterns in the Hopfield space\n        :param normalize_hopfield_space_affine: additionally enable affine normalization of patterns in Hopfield space\n        :param stored_pattern_as_static: interpret specified stored patterns as being static\n        :param state_pattern_as_static: interpret specified state patterns as being static\n        :param pattern_projection_as_static: interpret specified pattern projections as being static\n        :param pattern_projection_as_connected: connect pattern projection with stored pattern\n        :param stored_pattern_size: depth of input (stored pattern)\n        :param pattern_projection_size: depth of input (pattern projection)\n        :param batch_first: flag for specifying if the first dimension of data fed to \"forward\" reflects the batch size\n        :param association_activation: additional activation to be applied on the result of the Hopfield association\n        :param dropout: dropout probability applied on the association matrix\n        :param input_bias: bias to be added to input (state and stored pattern as well as pattern projection)\n        :param concat_bias_pattern: bias to be concatenated to stored pattern as well as pattern projection\n        :param add_zero_association: add a new batch of zeros to stored pattern as well as pattern projection\n        :param disable_out_projection: disable output projection\n        :param quantity: amount of stored patterns\n        :param trainable: stored pattern used for lookup is trainable\n        \"\"\"\n        super(HopfieldLayer, self).__init__()\n        self.hopfield = Hopfield(\n            input_size=input_size, hidden_size=hidden_size, output_size=output_size, pattern_size=pattern_size,\n            num_heads=num_heads, scaling=scaling, update_steps_max=update_steps_max, update_steps_eps=update_steps_eps,\n            normalize_stored_pattern=normalize_stored_pattern,\n            normalize_stored_pattern_affine=normalize_stored_pattern_affine,\n            normalize_state_pattern=normalize_state_pattern,\n            normalize_state_pattern_affine=normalize_state_pattern_affine,\n            normalize_pattern_projection=normalize_pattern_projection,\n            normalize_pattern_projection_affine=normalize_pattern_projection_affine,\n            normalize_hopfield_space=normalize_hopfield_space,\n            normalize_hopfield_space_affine=normalize_hopfield_space_affine,\n            stored_pattern_as_static=stored_pattern_as_static, state_pattern_as_static=state_pattern_as_static,\n            pattern_projection_as_static=pattern_projection_as_static,\n            pattern_projection_as_connected=pattern_projection_as_connected, stored_pattern_size=stored_pattern_size,\n            pattern_projection_size=pattern_projection_size, batch_first=batch_first,\n            association_activation=association_activation, dropout=dropout, input_bias=input_bias,\n            concat_bias_pattern=concat_bias_pattern, add_zero_association=add_zero_association,\n            disable_out_projection=disable_out_projection)\n        self._quantity = quantity\n        lookup_weight_size = self.hopfield.hidden_size if stored_pattern_as_static else self.hopfield.stored_pattern_dim\n        self.lookup_weights = nn.Parameter(torch.empty(size=(*(\n            (1, quantity) if batch_first else (quantity, 1)\n        ), input_size if lookup_weight_size is None else lookup_weight_size)), requires_grad=trainable)\n\n        if lookup_weights_as_separated:\n            target_weight_size = self.lookup_weights.shape[\n                2] if pattern_projection_size is None else pattern_projection_size\n            self.target_weights = nn.Parameter(torch.empty(size=(*(\n                (1, quantity) if batch_first else (quantity, 1)\n            ), target_weight_size)), requires_grad=lookup_targets_as_trainable)\n        else:\n            self.register_parameter(name=r'target_weights', param=None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        \"\"\"\n        Reset lookup and lookup target weights, including underlying Hopfield association.\n\n        :return: None\n        \"\"\"\n        if hasattr(self.hopfield, r'reset_parameters'):\n            self.hopfield.reset_parameters()\n\n        # Explicitly initialise lookup and target weights.\n        nn.init.normal_(self.lookup_weights, mean=0.0, std=0.02)\n        if self.target_weights is not None:\n            nn.init.normal_(self.target_weights, mean=0.0, std=0.02)\n\n    def _prepare_input(self, input: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"\n        Prepare input for Hopfield association.\n\n        :param input: data to be prepared\n        :return: stored pattern, expanded state pattern as well as pattern projection\n        \"\"\"\n        batch_size = input.shape[0 if self.batch_first else 1]\n        stored_pattern = self.lookup_weights.expand(size=(*(\n            (batch_size, self.quantity) if self.batch_first else (self.quantity, batch_size)\n        ), self.lookup_weights.shape[2]))\n        if self.target_weights is None:\n            pattern_projection = stored_pattern\n        else:\n            pattern_projection = self.target_weights.expand(size=(*(\n                (batch_size, self.quantity) if self.batch_first else (self.quantity, batch_size)\n            ), self.target_weights.shape[2]))\n\n        return stored_pattern, input, pattern_projection\n\n    def forward(self, input: Tensor, stored_pattern_padding_mask: Optional[Tensor] = None,\n                association_mask: Optional[Tensor] = None) -> Tensor:\n        \"\"\"\n        Compute Hopfield-based lookup on specified data.\n\n        :param input: data to used in lookup\n        :param stored_pattern_padding_mask: mask to be applied on stored patterns\n        :param association_mask: mask to be applied on inner association matrix\n        :return: result of Hopfield-based lookup on input data\n        \"\"\"\n        return self.hopfield(\n            input=self._prepare_input(input=input),\n            stored_pattern_padding_mask=stored_pattern_padding_mask,\n            association_mask=association_mask)\n\n",
    "description": null,
    "url": null
}