{
    "acronym": "dcpruning",
    "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
    "seed_ids": [
        "flashattn",
        "deltanet",
        "lineartransformer",
        "scattn",
        "transformer",
        "gpt2"
    ],
    "s2id": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6",
    "abstract": "Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.",
    "authors": [
        "Sotiris Anagnostidis",
        "Dario Pavllo",
        "Luca Biggio",
        "Lorenzo Noci",
        "Aur\u00e9lien Lucchi",
        "Thomas Hofmann"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.",
    "citationCount": 22,
    "influentialCitationCount": 1,
    "code": "\"\"\"\nFull definition of a GPT Language Model, all of it in this single file.\nReferences:\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\n2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.nn import functional as F\n\nfrom utils.root_finding import entmax_bisect\n\n\nclass DynamicTensorFast:\n    def __init__(\n        self,\n        batch_size,\n        embedding_dims,\n        capacity=64,\n        resizable=True,\n        reduce_fragmentation=False,\n        compact=True,\n        dtype=torch.float32,\n        device=\"cpu\",\n        debug=False,\n    ):\n        self.batch_size = batch_size\n        self.capacity = capacity\n        self.embedding_dims = embedding_dims\n        self.resizable = resizable\n        self.reduce_fragmentation = reduce_fragmentation\n        self.debug = debug\n        self.compact = compact\n\n        self.tensors = []\n        for i, embedding_dim in enumerate(embedding_dims):\n            if isinstance(embedding_dim, (list, tuple)):\n                # Number of heads is specified\n                assert len(embedding_dim) == 2\n                self.tensors.append(\n                    torch.zeros(\n                        (batch_size, embedding_dim[0], capacity, embedding_dim[1]),\n                        dtype=dtype,\n                        device=device,\n                    )\n                )  # !!!\n            else:\n                self.tensors.append(\n                    torch.zeros(\n                        (batch_size, capacity, embedding_dim),\n                        dtype=dtype,\n                        device=device,\n                    )\n                )\n\n        self.mask = torch.zeros((batch_size, capacity), dtype=torch.bool, device=device)\n        self.max_padded_length = 0\n\n        if self.debug:\n            self.token_ids = torch.full(\n                (batch_size, capacity), dtype=torch.long, device=device, fill_value=-1\n            )\n            self.next_token_id = 0\n\n    def to(self, device=None, dtype=None):\n        for i in range(len(self.tensors)):\n            self.tensors[i] = self.tensors[i].to(device=device, dtype=dtype)\n        self.mask = self.mask.to(device=device)\n        if self.debug:\n            self.token_ids = self.token_ids.to(device=device)\n\n    def _effective_size(self):\n        if self.reduce_fragmentation and self.max_padded_length > 0:\n            return 2 ** int(math.ceil(math.log2(self.max_padded_length)))\n        else:\n            return self.max_padded_length\n\n    def _resize(self, new_capacity):\n        for i, old_tensor in enumerate(self.tensors):\n            if len(old_tensor.shape) == 4:\n                new_tensor = torch.zeros(\n                    (\n                        old_tensor.shape[0],\n                        old_tensor.shape[1],\n                        new_capacity,\n                        old_tensor.shape[3],\n                    ),\n                    dtype=old_tensor.dtype,\n                    device=old_tensor.device,\n                )\n            else:\n                new_tensor = torch.zeros(\n                    (old_tensor.shape[0], new_capacity, old_tensor.shape[2]),\n                    dtype=old_tensor.dtype,\n                    device=old_tensor.device,\n                )\n            new_tensor[..., : self.capacity, :] = old_tensor[..., : self.capacity, :]\n            self.tensors[i] = new_tensor\n\n        new_mask = torch.zeros(\n            (self.mask.shape[0], new_capacity),\n            dtype=self.mask.dtype,\n            device=self.mask.device,\n        )\n        new_mask[:, : self.capacity] = self.mask[:, : self.capacity]\n        self.mask = new_mask\n\n        if self.debug:\n            new_token_ids = torch.full(\n                (self.token_ids.shape[0], new_capacity),\n                dtype=self.token_ids.dtype,\n                device=self.token_ids.device,\n                fill_value=-1,\n            )\n            new_token_ids[:, : self.capacity] = self.token_ids[:, : self.capacity]\n            self.token_ids = new_token_ids\n\n        self.capacity = new_capacity\n\n    def append(self, tensors) -> torch.LongTensor:\n        # Sanity check\n        assert len(tensors) == len(self.embedding_dims)\n        for tensor, embedding_dim in zip(tensors, self.embedding_dims):\n            if isinstance(embedding_dim, (tuple, list)):\n                # Number of heads is specified\n                assert len(tensor.shape) == 3\n                assert tensor.shape[0] == self.batch_size\n                assert tensor.shape[1] == embedding_dim[0]\n                assert tensor.shape[2] == embedding_dim[1]\n            else:\n                assert len(tensor.shape) == 2\n                assert tensor.shape[0] == self.batch_size\n                assert tensor.shape[1] == embedding_dim\n\n        # Find insertion point\n        effective_size = self._effective_size()\n        if effective_size == 0:\n            max_length = 0\n            self.max_padded_length = 1\n            insertion_point = torch.zeros(\n                (self.batch_size,), device=self.mask.device, dtype=torch.long\n            )\n        else:\n            mask = self.mask[:, :effective_size]\n            result = mask.min(dim=1)\n            insertion_point = (\n                result.indices * (~result.values) + mask.shape[1] * result.values\n            )\n            max_length = insertion_point.max().item()\n            self.max_padded_length = max(self.max_padded_length, max_length + 1)\n\n        if max_length == self.capacity:\n            # Needs resizing\n            if not self.resizable:\n                raise RuntimeError(\n                    \"The pre-allocated buffer has been exhausted. \"\n                    \"Increase the capacity or set resizable=True.\"\n                )\n            new_capacity = (self.capacity * 2) if self.capacity > 0 else 1\n            self._resize(new_capacity)\n\n        for i, tensor in enumerate(tensors):\n            if len(tensor.shape) == 3:\n                self.tensors[i].scatter_(\n                    2,\n                    insertion_point[:, None, None, None].expand(\n                        -1, tensor.shape[1], -1, tensor.shape[-1]\n                    ),\n                    tensor[:, :, None],\n                )\n            else:\n                self.tensors[i].scatter_(\n                    1,\n                    insertion_point[:, None, None].expand(-1, -1, tensor.shape[-1]),\n                    tensor[:, None],\n                )\n\n        self.mask.scatter_(1, insertion_point[:, None], True)\n\n        if self.debug:\n            self.token_ids.scatter_(1, insertion_point[:, None], self.next_token_id)\n            self.next_token_id += 1\n\n        return insertion_point\n\n    def remove(self, mask: torch.BoolTensor):\n        expected_size = self._effective_size()\n        assert mask.shape[0] == self.batch_size\n        assert mask.shape[1] == expected_size\n        assert len(mask.shape) == 2\n        inv_mask = ~mask\n        self.mask[:, :expected_size] &= inv_mask\n        if self.debug:\n            self.token_ids[:, :expected_size] *= inv_mask\n            self.token_ids[:, :expected_size] += mask * (-1)\n\n        if self.compact:\n            # Compute load factor\n            mask = self.mask[:, : self.max_padded_length]\n            ratio = mask.sum(dim=1).max().item() / mask.shape[1]\n\n        if self.compact and ratio < 0.9:\n            # Find offset\n            mask = self.mask[:, :expected_size]\n            result = mask.min(dim=1)\n            insertion_point = (\n                result.indices * (~result.values) + mask.shape[1] * result.values\n            )\n            offset = insertion_point.min().item()\n            if self.reduce_fragmentation and offset > 0:\n                offset = 2 ** int(math.floor(math.log2(offset)))\n\n            # Compact data structure\n            indices = torch.argsort(~self.mask[:, offset:expected_size].long()) + offset\n            self.mask[:, offset:expected_size] = self.mask.gather(1, indices)\n            if self.debug:\n                self.token_ids[:, offset:expected_size] = self.token_ids.gather(\n                    1, indices\n                )\n            for i, (tensor, emb_dim) in enumerate(\n                zip(self.tensors, self.embedding_dims)\n            ):\n                if isinstance(emb_dim, (tuple, list)):\n                    indices_ = indices[:, None, :, None].expand(\n                        -1, emb_dim[0], -1, emb_dim[1]\n                    )\n                    self.tensors[i][:, :, offset:expected_size] = tensor.gather(\n                        2, indices_\n                    )\n                else:\n                    indices_ = indices[:, :, None].expand(-1, -1, emb_dim)\n                    self.tensors[i][:, offset:expected_size] = tensor.gather(\n                        1, indices_\n                    )\n\n            # Find new max padded length\n            mask_sum = torch.flip(self.mask[:, offset:expected_size].any(dim=0), (0,))\n            result = mask_sum.max(dim=0)\n            last_value = result.values.item()\n            padded_length = mask_sum.shape[0] - result.indices.item() + offset\n            if last_value:\n                self.max_padded_length = padded_length\n            else:\n                self.max_padded_length = 0\n\n    def values(self, tensor_ids=None):\n        padded_length = self._effective_size()\n        tensors = []\n        for i, (tensor, emb_dim) in enumerate(zip(self.tensors, self.embedding_dims)):\n            if tensor_ids is None or i in tensor_ids:\n                tensors.append(tensor[..., :padded_length, :])\n        return tensors, self.mask[:, :padded_length]\n\n    def get_token_ids(self, compact=False):\n        assert self.debug\n        assert (self.token_ids[:, self.max_padded_length :] == -1).all()\n        if compact:\n            result = []\n            for row in self.token_ids[:, : self._effective_size()]:\n                ids = row[torch.where(row != -1)].sort().values\n                result.append(ids)\n            return result\n        else:\n            return self.token_ids[:, : self._effective_size()]\n\n    def get_dense_mask(self) -> torch.BoolTensor:\n        assert self.debug\n        token_ids = self.token_ids[:, : self.max_padded_length]\n        mask = torch.zeros(\n            (self.batch_size, self.next_token_id + 1),\n            dtype=torch.bool,\n            device=token_ids.device,\n        )\n\n        # Index 0 is a dummy index to deal with gaps (token_id = -1)\n        mask.scatter_(1, token_ids + 1, True)\n        return mask[:, 1:]\n\n    def get_dense_values(self):\n        assert self.debug\n        result = []\n        for row_idx, row in enumerate(self.token_ids[:, : self._effective_size()]):\n            ids = row.argsort()[(row == -1).sum() :]\n            sub_result = []\n            for tensor, emb_dim in zip(self.tensors, self.embedding_dims):\n                if isinstance(emb_dim, (tuple, list)):\n                    # Restore correct shape (number of heads)\n                    tensor = tensor.view(self.batch_size, emb_dim[0], -1, emb_dim[1])\n                sub_result.append(tensor[row_idx, ..., ids, :])\n            result.append(sub_result)\n        return result\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, block_num, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n\n        self.sparse_attention = config.sparse_attention\n        self.block_num = block_num\n        self.config = config\n\n        if self.sparse_attention:\n            # initialize alpha to 1, this will be overwritten\n            # setting alpa to 1 is unstable, so we set it to 1 + eps\n            self.sparsity_alpha = 1.000001\n\n            self.int_n_embd = config.int_n_embd if config.int_n_embd else config.n_embd\n\n            self.q_int = nn.Linear(config.n_embd, self.int_n_embd, bias=False)\n            self.k_int = nn.Linear(config.n_embd, self.int_n_embd, bias=False)\n\n            self.int_bias = nn.Parameter(\n                torch.ones(\n                    1,\n                )\n                * config.sparse_attention_int_bias,\n            )\n\n            torch.nn.init.normal_(\n                self.q_int.weight, mean=0.0, std=1 / math.sqrt(config.n_embd)\n            )\n            torch.nn.init.normal_(\n                self.k_int.weight, mean=0.0, std=1 / math.sqrt(config.n_embd)\n            )\n\n            # bias for the dropping probabilities. Here we assume a token does not drop itself.\n            self.register_buffer(\n                \"bias_int\",\n                torch.tril(\n                    torch.ones(config.block_size, config.block_size), diagonal=-1\n                ).view(1, 1, config.block_size, config.block_size),\n            )\n\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n\n        # We use torch 2.0\n        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n        assert self.flash\n\n        # bias for the attention mask for casual decoding\n        self.register_buffer(\n            \"bias\",\n            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n                1, 1, config.block_size, config.block_size\n            ),\n        )\n\n    def forward(\n        self,\n        x,\n        prev_attn_mask=None,\n        mask=None,\n        store=None,\n        validity_map=None,\n        first_generation=False,\n    ):\n        (\n            B,\n            T,\n            C,\n        ) = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        device = x.device\n\n        q, k, v = self.c_attn.forward(x).split(self.n_embd, dim=2)\n\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n            1, 2\n        )  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n            1, 2\n        )  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n            1, 2\n        )  # (B, nh, T, hs)\n\n        if self.sparse_attention:\n            q_int = self.q_int(x)\n            k_int = self.k_int(x)\n\n        store_mask = None\n        insertion_indices = None\n\n        if store is not None:\n            # we are using caching while generating\n            if first_generation:\n                # if this is the first generation step, we need to insert everything into the store\n                for i in range(T):\n                    # add everything to the cache\n                    if not self.sparse_attention:\n                        store.append([k[:, :, i, :], v[:, :, i, :]])\n                    else:\n                        store.append([k[:, :, i, :], v[:, :, i, :], k_int[:, i, :]])\n\n                # get the correct last insertion indices, due to padding within the prefixes\n                insertion_indices = torch.sum(validity_map, dim=-1) - 1\n\n                # After inseting in the store, remove based on the padding mask\n                _, store_mask = store.values()\n                store_mask = store_mask.clone()\n                store_mask[:, : validity_map.shape[-1]] = validity_map\n\n                store.remove(torch.logical_not(store_mask))\n            else:\n                # add new elements to the store\n                if not self.sparse_attention:\n                    store.append([k[:, :, 0, :], v[:, :, 0, :]])\n\n                    (k, v), store_mask = store.values()\n                else:\n                    insertion_indices = store.append(\n                        [k[:, :, 0, :], v[:, :, 0, :], k_int[:, 0, :]]\n                    )\n\n                    (k, v, k_int), store_mask = store.values()\n\n                validity_map = store_mask\n\n        context_T = k.shape[2]\n\n        if not self.sparse_attention:\n            # regular causal attention\n            attn_mask = torch.zeros(B, 1, T, context_T, device=x.device, dtype=x.dtype)\n\n            if validity_map is not None:\n                # filter out the attention mask to only include the tokens that are not yet processed\n                attn_mask = attn_mask.masked_fill(\n                    validity_map[:, None, None, :] == 0,\n                    float(\"-inf\"),\n                )\n\n            cumprobs = 0  # for compatibility\n        else:\n            p_int_raw = (\n                (\n                    torch.matmul(q_int, k_int.transpose(-1, -2))\n                    / math.sqrt(self.int_n_embd)\n                    + self.int_bias\n                )\n                .unsqueeze(1)\n                .unsqueeze(-1)\n            )\n\n            if self.sparsity_alpha == \"inf\":\n                # in eval mode we replace the alpha-sigmoid with the step function\n                p_int = (p_int_raw > 0)[..., 0]\n            else:\n                # Compare the raw drop scores with the values 0 to get the drop probabilities.\n                p_int_raw = torch.cat([p_int_raw, torch.zeros_like(p_int_raw)], dim=-1)\n\n                # Take only the first value of the entmax_bisect output, which is the probability of dropping.\n                p_int = entmax_bisect(p_int_raw.to(torch.float32), self.sparsity_alpha)[\n                    ..., 0\n                ]\n\n            if store is not None:\n                # here we need to drop from the store\n                if first_generation:\n                    p_int = p_int.float()\n                    p_int = p_int.masked_fill(self.bias_int[:, :, :T, :T] == 0, 1)\n\n                    p_int = p_int.masked_fill(validity_map[:, None, None, :] == 0, 0)\n\n                    # Multiply together probs from the previous tokens.\n                    cumprobs = torch.cumprod(p_int, dim=-2)\n\n                    attn_mask = torch.log(cumprobs)\n\n                    if prev_attn_mask is not None:\n                        attn_mask = attn_mask + prev_attn_mask\n\n                    store_mask[:, : validity_map.shape[-1]] = cumprobs[\n                        torch.arange(B, device=device), 0, insertion_indices, :\n                    ].bool()\n\n                    store.remove(torch.logical_not(store_mask))\n                else:\n                    # specify that we cannot drop ourselves\n                    p_int[\n                        torch.arange(B, device=device), 0, 0, insertion_indices\n                    ] = True\n\n                    p_int = torch.logical_and(p_int, validity_map[:, None, None, :])\n\n                    attn_mask = p_int  # scaled_dot_product_attention can also handle boolean masks\n                    cumprobs = None\n\n                    store.remove(torch.logical_not(p_int[:, 0, 0, :]))\n            else:\n                # training phase\n                p_int = p_int.masked_fill(self.bias_int[:, :, :T, :T] == 0, 1)\n\n                if validity_map is not None:\n                    p_int = p_int.masked_fill(validity_map[:, None, None, :] == 0, 0)\n\n                # Multiply together probs from the previous tokens.\n                cumprobs = torch.cumprod(p_int, dim=-2)\n\n                # Just for stability reasons add an epsilon ...\n                attn_mask = torch.log(cumprobs + (1e-40 if self.training else 0)).to(\n                    p_int_raw.dtype\n                )\n\n                if prev_attn_mask is not None:\n                    attn_mask = attn_mask + prev_attn_mask\n\n        if T == context_T:\n            # Add casual masking, only during training\n            attn_mask = attn_mask.masked_fill(\n                self.bias[:, :, :T, :T] == 0, float(\"-inf\")\n            )\n\n        if mask is not None:  # masking of tokens during training\n            attn_mask = attn_mask.masked_fill(\n                mask[:, None, None, :] == 0, float(\"-inf\")\n            )\n\n        y = torch.nn.functional.scaled_dot_product_attention(\n            q, k, v, attn_mask=attn_mask, dropout_p=self.dropout\n        )\n        y = (\n            y.transpose(1, 2).contiguous().view(B, T, C)\n        )  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n\n        return y, cumprobs, attn_mask\n\n\n",
    "description": null,
    "url": null
}