{
    "acronym": "s4d",
    "title": "On the Parameterization and Initialization of Diagonal State Space Models",
    "seed_ids": [
        "dssm",
        "s4"
    ],
    "s2id": "ca444821352a4bd91884413d8070446e2960715a",
    "abstract": "State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.",
    "authors": [
        "Albert Gu",
        "Ankit Gupta",
        "Karan Goel",
        "Christopher R\u00e9"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": "This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.",
    "citationCount": 166,
    "influentialCitationCount": 33,
    "code": "\"\"\"Minimal version of S4D with extra options and features stripped out, for pedagogical purposes.\"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\nfrom src.models.nn import DropoutNd\n\nclass S4DKernel(nn.Module):\n    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n\n    def __init__(self, d_model, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n        super().__init__()\n        # Generate dt\n        H = d_model\n        log_dt = torch.rand(H) * (\n            math.log(dt_max) - math.log(dt_min)\n        ) + math.log(dt_min)\n\n        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n        self.C = nn.Parameter(torch.view_as_real(C))\n        self.register(\"log_dt\", log_dt, lr)\n\n        log_A_real = torch.log(0.5 * torch.ones(H, N//2))\n        A_imag = math.pi * repeat(torch.arange(N//2), 'n -> h n', h=H)\n        self.register(\"log_A_real\", log_A_real, lr)\n        self.register(\"A_imag\", A_imag, lr)\n\n    def forward(self, L):\n        \"\"\"\n        returns: (..., c, L) where c is number of channels (default 1)\n        \"\"\"\n\n        # Materialize parameters\n        dt = torch.exp(self.log_dt) # (H)\n        C = torch.view_as_complex(self.C) # (H N)\n        A = -torch.exp(self.log_A_real) + 1j * self.A_imag # (H N)\n\n        # Vandermonde multiplication\n        dtA = A * dt.unsqueeze(-1)  # (H N)\n        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device) # (H N L)\n        C = C * (torch.exp(dtA)-1.) / A\n        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n\n        return K\n\n    def register(self, name, tensor, lr=None):\n        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n\n        if lr == 0.0:\n            self.register_buffer(name, tensor)\n        else:\n            self.register_parameter(name, nn.Parameter(tensor))\n\n            optim = {\"weight_decay\": 0.0}\n            if lr is not None: optim[\"lr\"] = lr\n            setattr(getattr(self, name), \"_optim\", optim)\n\n\nclass S4D(nn.Module):\n    def __init__(self, d_model, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n        super().__init__()\n\n        self.h = d_model\n        self.n = d_state\n        self.d_output = self.h\n        self.transposed = transposed\n\n        self.D = nn.Parameter(torch.randn(self.h))\n\n        # SSM Kernel\n        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n\n        # Pointwise\n        self.activation = nn.GELU()\n        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n        dropout_fn = DropoutNd\n        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n\n        # position-wise output transform to mix features\n        self.output_linear = nn.Sequential(\n            nn.Conv1d(self.h, 2*self.h, kernel_size=1),\n            nn.GLU(dim=-2),\n        )\n\n    def forward(self, u, **kwargs): # absorbs return_output and transformer src mask\n        \"\"\" Input and output shape (B, H, L) \"\"\"\n        if not self.transposed: u = u.transpose(-1, -2)\n        L = u.size(-1)\n\n        # Compute SSM Kernel\n        k = self.kernel(L=L) # (H L)\n\n        # Convolution\n        k_f = torch.fft.rfft(k, n=2*L) # (H L)\n        u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n        y = torch.fft.irfft(u_f*k_f, n=2*L)[..., :L] # (B H L)\n\n        # Compute D term in state space equation - essentially a skip connection\n        y = y + u * self.D.unsqueeze(-1)\n\n        y = self.dropout(self.activation(y))\n        y = self.output_linear(y)\n        if not self.transposed: y = y.transpose(-1, -2)\n        return y, None # Return a dummy state to satisfy this repo's interface, but this can be modified",
    "description": null,
    "url": null
}