{
    "acronym": "gssm",
    "title": "Long Range Language Modeling via Gated State Spaces",
    "seed_ids": [
        "dssm",
        "memorizingtrans",
        "flash",
        "cosformer",
        "s4",
        "rfa",
        "performer",
        "lineartransformer",
        "compressivetransformer",
        "transformerxl",
        "transformer",
        "bert",
        "gpt2"
    ],
    "s2id": "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
    "abstract": "State space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further.",
    "authors": [
        "Harsh Mehta",
        "Ankit Gupta",
        "Ashok Cutkosky",
        "Behnam Neyshabur"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": "This work proposes a new layer named Gated State Space (GSS) and shows that it trains significantly faster than the diagonal version of S4 on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement.",
    "citationCount": 134,
    "influentialCitationCount": 17,
    "code": "import torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom torch.fft import rfft, irfft\n\nfrom einops import rearrange\n\n# functions\n\ndef exists(val):\n    return val is not None\n\n# classes\n\nclass DSS(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        kernel_N = 512,\n        dss_kernel_lambda_imag_exp = True\n    ):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n\n        # Lambda\n\n        self.Lambda_real = nn.Parameter(torch.randn(kernel_N))\n        self.Lambda_imag = nn.Parameter(torch.randn(kernel_N))\n\n        # C\n\n        self.C_real = nn.Parameter(torch.randn(dim, kernel_N))\n        self.C_imag = nn.Parameter(torch.randn(dim, kernel_N))\n\n        # params D\n\n        self.param_D = nn.Parameter(torch.randn(dim))\n\n        # whether to exponentiate lambda imag @albertfgu says it is not accurate to s4 original designs (but it is present in the pseudocode)\n\n        self.dss_kernel_lambda_imag_exp = dss_kernel_lambda_imag_exp\n\n    def forward(self, x):\n        \"\"\"\n        einstein notation:\n        b - batch\n        l - sequence length\n        d - dimension\n        \"\"\"\n\n        device, seq_len = x.device, x.shape[1]\n        u = self.norm(x)\n\n        # learned weighted residual\n\n        residual = u * self.param_D\n\n        # derive simple dss kernel\n\n        Lambda_imag = self.Lambda_imag.exp() if self.dss_kernel_lambda_imag_exp else self.Lambda_imag\n\n        Lambda = -self.Lambda_real.exp() + 1j * Lambda_imag\n        C = self.C_real + 1j * self.C_imag\n\n        arange = torch.arange(seq_len, device = device)\n\n        S = (rearrange(Lambda, 'n -> n 1') * rearrange(arange, 'l -> 1 l')).exp()\n        C = C * (Lambda.exp() - 1) / Lambda\n\n        K = einsum('h n, n l -> l h', C, S).real\n\n        # conv1d fft O(nlog(n))\n\n        u_f = rfft(u, n = seq_len * 2, dim = -2)\n        K_f = rfft(K, n = seq_len * 2, dim = -2)\n\n        y = irfft(u_f * K_f, seq_len * 2, dim = -2)[..., :seq_len, :]\n\n        return y + residual\n\nclass GSS(nn.Module):\n    \"\"\" Pseudocode 3.2 \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_expansion_factor = 4,\n        dss_kernel_N = 512,\n        dss_kernel_H = 256,\n        reverse_seq = False,\n        dss_kernel_lambda_imag_exp = True\n    ):\n        super().__init__()\n        self.reverse_seq = reverse_seq\n        self.norm = nn.LayerNorm(dim)\n\n        dim_hidden = int(dim_expansion_factor * dim)\n        self.to_u = nn.Sequential(nn.Linear(dim, dim_hidden, bias = False), nn.GELU())\n        self.to_v = nn.Sequential(nn.Linear(dim, dss_kernel_H, bias = False), nn.GELU())\n\n        self.dss = DSS(dim = dss_kernel_H, kernel_N = dss_kernel_N, dss_kernel_lambda_imag_exp = dss_kernel_lambda_imag_exp)\n\n        self.to_gate = nn.Linear(dss_kernel_H, dim_hidden, bias = False)\n        self.to_out = nn.Linear(dim_hidden, dim)\n\n    def forward(self, x):\n        if self.reverse_seq:\n            x = torch.flip(x, dims = (1,))\n\n        residual, x = x.clone(), self.norm(x)\n\n        u = self.to_u(x)\n        v = self.to_v(x)\n\n        v = self.dss(v)\n\n        uc = self.to_gate(v)\n        out = self.to_out(uc * u)\n\n        out = out + residual\n\n        if self.reverse_seq:\n            out = torch.flip(out, dims = (1,))\n\n        return out\n\n# Gated State Spaces LM\n\nclass GatedStateSpacesLM(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        dim,\n        depth,\n        dim_expansion_factor = 4,\n        dss_kernel_N = 512,\n        dss_kernel_H = 256,\n        dss_kernel_lambda_imag_exp = True\n    ):\n        super().__init__()\n        self.token_emb = nn.Embedding(num_tokens, dim)\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                GSS(\n                    dim = dim,\n                    dss_kernel_H = dss_kernel_H,\n                    dss_kernel_N = dss_kernel_N,\n                    dim_expansion_factor = dim_expansion_factor,\n                    dss_kernel_lambda_imag_exp = dss_kernel_lambda_imag_exp\n                )\n            )\n\n        self.to_logits = nn.Linear(dim, num_tokens, bias = False)\n\n    def forward(self, x, labels = None):\n        x = self.token_emb(x)\n\n        for gss in self.layers:\n            x = gss(x)\n\n        logits = self.to_logits(x)\n\n        if not exists(labels):\n            return logits\n\n        logits = rearrange(logits, 'b n c -> b c n')\n        return F.cross_entropy(logits, labels)",
    "description": null,
    "url": null
}