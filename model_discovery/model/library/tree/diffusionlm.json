{
    "acronym": "diffusionlm",
    "title": "Diffusion-LM Improves Controllable Text Generation ",
    "seed_ids": [
        "transformer",
        "gpt2",
        "gpt"
    ],
    "s2id": "1386b8a11929cf02da291c56aca353e33bbc22ed",
    "abstract": "Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",
    "authors": [
        "Xiang Lisa Li",
        "John Thickstun",
        "Ishaan Gulrajani",
        "Percy Liang",
        "Tatsunori Hashimoto"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": "This work develops a new non-autoregressive language model based on continuous diffusions that is significantly outperforming prior work on controlling the behavior of language models without re-training and demonstrates successful control of Diffusion-LM for six challenging fine-grained control tasks.",
    "citationCount": 507,
    "influentialCitationCount": 73,
    "code": null,
    "description": null,
    "url": null
}