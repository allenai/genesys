{
    "acronym": "contrnn",
    "title": "Modeling Hierarchical Structures with Continuous Recursive Neural Networks ",
    "seed_ids": [],
    "s2id": "db016d2b6d2577c47d62f9de2a7d1ddbf226386a",
    "abstract": "Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases. However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference and ListOps. We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference.",
    "authors": [
        "Jishnu Ray Chowdhury",
        "Cornelia Caragea"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "tldr": "This work proposes Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations of traditional RvNNs by incorporating a continuous relaxation to the induced structure.",
    "citationCount": 11,
    "influentialCitationCount": 1,
    "code": "import torch as T\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.layers import Linear\nfrom models.utils import gelu\nimport math\n\n\nclass FOCN(nn.Module):\n    def __init__(self, config):\n        super(FOCN, self).__init__()\n\n        self.config = config\n        self.hidden_size = config[\"hidden_size\"]\n        self.cell_hidden_size = config[\"cell_hidden_size\"]\n        self.window_size = config[\"window_size\"]\n        self.stop_threshold = config[\"stop_threshold\"]\n        # self.switch_threshold = config[\"switch_threshold\"]\n        self.entropy_gamma = config[\"entropy_gamma\"]\n        self.structure_gamma = 0.01  # config[\"structure_gamma\"]\n        self.speed_gamma = config[\"speed_gamma\"]\n        self.in_dropout = config[\"in_dropout\"]\n        self.hidden_dropout = config[\"hidden_dropout\"]\n        self.recurrent_momentum = config[\"recurrent_momentum\"]\n        self.small_d = config[\"small_d\"]\n\n        self.START = nn.Parameter(T.randn(self.hidden_size))\n        self.END = nn.Parameter(T.randn(self.hidden_size))\n\n        if self.recurrent_momentum:\n            self.past_transition_features = nn.Parameter(T.randn(self.small_d))\n            self.past_non_transition_features = nn.Parameter(T.randn(self.small_d))\n            self.conv_layer = Linear(self.window_size * self.hidden_size + self.small_d, self.hidden_size)\n        else:\n            self.conv_layer = Linear(self.window_size * self.hidden_size, self.hidden_size)\n\n        self.scorer = Linear(self.hidden_size, 1)\n\n        self.initial_transform_layer = Linear(self.hidden_size, self.hidden_size)\n        self.wcell1 = Linear(2 * self.hidden_size, self.cell_hidden_size)\n        self.wcell2 = Linear(self.cell_hidden_size, 4 * self.hidden_size)\n        self.LN = nn.LayerNorm(self.hidden_size)\n\n        self.eps = 1e-8\n\n    # %%\n    def sum_normalize(self, logits, dim=-1):\n        return logits / T.sum(logits + self.eps, keepdim=True, dim=dim)\n\n    # %%\n    def augment_sequence(self, sequence, input_mask):\n        N, S, D = sequence.size()\n        assert input_mask.size() == (N, S, 1)\n\n        \"\"\"\n        AUGMENT SEQUENCE WITH START AND END TOKENS\n        \"\"\"\n        # ADD START TOKEN\n        START = self.START.view(1, 1, D).repeat(N, 1, 1)\n        sequence = T.cat([START, sequence], dim=1)\n        assert sequence.size() == (N, S + 1, D)\n        input_mask = T.cat([T.ones(N, 1, 1).float().to(input_mask.device), input_mask], dim=1)\n        assert input_mask.size() == (N, S + 1, 1)\n\n        # ADD END TOKEN\n        input_mask_no_end = T.cat([input_mask.clone(), T.zeros(N, 1, 1).float().to(input_mask.device)], dim=1)\n        input_mask_yes_end = T.cat([T.ones(N, 1, 1).float().to(input_mask.device), input_mask.clone()], dim=1)\n        END_mask = input_mask_yes_end - input_mask_no_end\n        assert END_mask.size() == (N, S + 2, 1)\n\n        END = self.END.view(1, 1, D).repeat(N, S + 2, 1)\n        sequence = T.cat([sequence, T.zeros(N, 1, D).float().to(sequence.device)], dim=1)\n        sequence = END_mask * END + (1 - END_mask) * sequence\n\n        input_mask = input_mask_yes_end\n        input_mask_no_start = T.cat([T.zeros(N, 1, 1).float().to(input_mask.device),\n                                     input_mask[:, 1:, :]], dim=1)\n\n        return sequence, input_mask, END_mask, input_mask_no_start, input_mask_no_end\n\n    # %%\n    def compute_neighbor_probs(self, active_probs, input_mask):\n        N, S, _ = input_mask.size()\n        assert input_mask.size() == (N, S, 1)\n        input_mask = input_mask.permute(0, 2, 1).contiguous()\n        assert input_mask.size() == (N, 1, S)\n\n        assert active_probs.size() == (N, S, 1)\n        active_probs = active_probs.permute(0, 2, 1).contiguous()\n        assert active_probs.size() == (N, 1, S)\n\n        input_mask_flipped = T.flip(input_mask.clone(), dims=[2])\n        active_probs_flipped = T.flip(active_probs.clone(), dims=[2])\n\n        input_mask = T.stack([input_mask_flipped, input_mask], dim=1)\n        active_probs = T.stack([active_probs_flipped, active_probs], dim=1)\n\n        assert input_mask.size() == (N, 2, 1, S)\n        assert active_probs.size() == (N, 2, 1, S)\n\n        active_probs_matrix = active_probs.repeat(1, 1, S, 1) * input_mask\n        assert active_probs_matrix.size() == (N, 2, S, S)\n        right_probs_matrix = T.triu(active_probs_matrix, diagonal=1)  # mask self and left\n\n        right_probs_matrix_cumsum = T.cumsum(right_probs_matrix, dim=-1)\n        assert right_probs_matrix_cumsum.size() == (N, 2, S, S)\n        remainders = 1.0 - right_probs_matrix_cumsum\n\n        remainders_from_left = T.cat([T.ones(N, 2, S, 1).float().to(remainders.device), remainders[:, :, :, 0:-1]],\n                                     dim=-1)\n        assert remainders_from_left.size() == (N, 2, S, S)\n\n        remainders_from_left = T.max(T.zeros(N, 2, S, 1).float().to(remainders.device), remainders_from_left)\n        assert remainders_from_left.size() == (N, 2, S, S)\n\n        right_neighbor_probs = T.where(right_probs_matrix_cumsum > 1.0,\n                                       remainders_from_left,\n                                       right_probs_matrix)\n\n        right_neighbor_probs = right_neighbor_probs * input_mask\n\n        left_neighbor_probs = right_neighbor_probs[:, 0, :, :]\n        left_neighbor_probs = T.flip(left_neighbor_probs, dims=[1, 2])\n        right_neighbor_probs = right_neighbor_probs[:, 1, :, :]\n\n        return left_neighbor_probs, right_neighbor_probs\n\n    # %%\n    def make_window(self, sequence, left_child_probs, right_child_probs):\n\n        N, S, D = sequence.size()\n\n        left_children_list = []\n        right_children_list = []\n        left_children_k = sequence.clone()\n        right_children_k = sequence.clone()\n\n        for k in range(self.window_size // 2):\n            left_children_k = T.matmul(left_child_probs, left_children_k)\n            left_children_list = [left_children_k.clone()] + left_children_list\n\n            right_children_k = T.matmul(right_child_probs, right_children_k)\n            right_children_list = right_children_list + [right_children_k.clone()]\n\n        windowed_sequence = left_children_list + [sequence] + right_children_list\n        windowed_sequence = T.stack(windowed_sequence, dim=-2)\n\n        assert windowed_sequence.size() == (N, S, self.window_size, D)\n\n        return windowed_sequence\n\n    # %%\n    def initial_transform(self, sequence):\n        sequence = self.LN(self.initial_transform_layer(sequence))\n        return sequence\n\n    # %%\n    def score_fn(self, windowed_sequence, transition_feats):\n        N, S, W, D = windowed_sequence.size()\n        windowed_sequence = windowed_sequence.view(N, S, W * D)\n\n        if self.recurrent_momentum:\n            windowed_sequence = T.cat([windowed_sequence, transition_feats], dim=-1)\n\n        scores = self.scorer(gelu(self.conv_layer(windowed_sequence)))\n\n        transition_scores = scores[:, :, 0].unsqueeze(-1)\n        # reduce_probs = T.sigmoid(scores[:,:,1].unsqueeze(-1))\n        no_op_scores = T.zeros_like(transition_scores).float().to(transition_scores.device)\n        scores = T.cat([transition_scores, no_op_scores], dim=-1)\n        scores = scores / self.temperature\n        max_score = T.max(scores)\n        exp_scores = T.exp(scores - max_score)\n\n        return exp_scores\n\n    # %%\n    def composer(self, child1, child2):\n        N, S, D = child1.size()\n\n        concated = T.cat([child1, child2], dim=-1)\n        assert concated.size() == (N, S, 2 * D)\n\n        # concated = F.dropout(concated, p=self.hidden_dropout, training=self.training)\n        if self.config[\"hidden_activation\"].lower() == \"relu\":\n            intermediate = F.relu(self.wcell1(concated))\n        else:\n            intermediate = gelu(self.wcell1(concated))\n        intermediate = F.dropout(intermediate, p=self.hidden_dropout, training=self.training)\n        contents = self.wcell2(intermediate)\n\n        contents = contents.view(N, S, 4, D)\n        gates = T.sigmoid(contents[:, :, 0:3, :])\n        parent = contents[:, :, 3, :]\n        f1 = gates[..., 0, :]\n        f2 = gates[..., 1, :]\n        i = gates[..., 2, :]\n\n        transition = self.LN(f1 * child1 + f2 * child2 + i * parent)\n\n        return transition\n\n    # %%\n    def compute_entropy_penalty(self, active_probs, last_token_mask):\n        N, S = active_probs.size()\n        active_prob_dist = self.sum_normalize(active_probs, dim=-1)\n        nll_loss = - T.log(T.sum(last_token_mask * active_prob_dist, dim=1) + self.eps)\n        nll_loss = nll_loss.view(N)\n        return nll_loss\n\n    # %%\n    def compute_speed_penalty(self, steps, input_mask):\n        steps = T.max(steps, dim=1)[0]\n        speed_penalty = steps.squeeze(-1) / (T.sum(input_mask.squeeze(-1), dim=1) - 2.0)\n        return speed_penalty\n\n    # %%\n    def encoder_block(self, sequence, input_mask):\n\n        sequence, input_mask, END_mask, \\\n        input_mask_no_start, input_mask_no_end = self.augment_sequence(sequence, input_mask)\n\n        N, S, D = sequence.size()\n\n        \"\"\"\n        Initial Preparations\n        \"\"\"\n        active_probs = T.ones(N, S, 1).float().to(sequence.device) * input_mask\n        steps = T.zeros(N, S, 1).float().to(sequence.device)\n        zeros_sequence = T.zeros(N, 1, 1).float().to(sequence.device)\n        last_token_mask = T.cat([END_mask[:, 1:, :], zeros_sequence], dim=1)\n        START_END_LAST_PAD_mask = input_mask_no_start * input_mask_no_end * (1.0 - last_token_mask)\n        self.START_END_LAST_PAD_mask = START_END_LAST_PAD_mask\n        halt_ones = T.ones(N).float().to(sequence.device)\n        halt_zeros = T.zeros(N).float().to(sequence.device)\n        improperly_terminated_mask = halt_ones.clone()\n        update_mask = T.ones(N).float().to(sequence.device)\n        left_transition_probs = T.zeros(N, S, 1).float().to(sequence.device)\n\n        \"\"\"\n        Initial Transform\n        \"\"\"\n        sequence = self.initial_transform(sequence)\n        sequence = sequence * input_mask\n        \"\"\"\n        Start Recursion\n        \"\"\"\n        t = 0\n        while t < (S - 2):\n            original_active_probs = active_probs.clone()\n            original_sequence = sequence.clone()\n            residual_sequence = sequence.clone()\n            original_steps = steps.clone()\n\n            left_neighbor_probs, right_neighbor_probs \\\n                = self.compute_neighbor_probs(active_probs=active_probs.clone(),\n                                              input_mask=input_mask.clone())\n\n            windowed_sequence = self.make_window(sequence=sequence,\n                                                 left_child_probs=left_neighbor_probs,\n                                                 right_child_probs=right_neighbor_probs)\n\n            if self.recurrent_momentum:\n                transition_feats = left_transition_probs * self.past_transition_features.view(1, 1, -1) \\\n                                   + (1 - left_transition_probs) * self.past_non_transition_features.view(1, 1, -1)\n            else:\n                transition_feats = None\n\n            exp_scores = self.score_fn(windowed_sequence, transition_feats)\n            exp_transition_scores = exp_scores[:, :, 0].unsqueeze(-1)\n            exp_no_op_scores = exp_scores[:, :, 1].unsqueeze(-1)\n\n            exp_transition_scores = exp_transition_scores * START_END_LAST_PAD_mask\n\n            if self.config[\"no_modulation\"] is True:\n                exp_scores = T.cat([exp_transition_scores,\n                                    exp_no_op_scores], dim=-1)\n            else:\n                exp_left_transition_scores = T.matmul(left_neighbor_probs, exp_transition_scores)\n                exp_right_transition_scores = T.matmul(right_neighbor_probs, exp_transition_scores)\n\n                exp_scores = T.cat([exp_transition_scores,\n                                    exp_no_op_scores,\n                                    exp_left_transition_scores,\n                                    exp_right_transition_scores], dim=-1)\n\n            normalized_scores = self.sum_normalize(exp_scores, dim=-1)\n            transition_probs = normalized_scores[:, :, 0].unsqueeze(-1)\n            transition_probs = transition_probs * START_END_LAST_PAD_mask\n\n            left_transition_probs = T.matmul(left_neighbor_probs, transition_probs)\n            left_transition_probs = left_transition_probs * input_mask_no_start * input_mask_no_end\n            left_sequence = windowed_sequence[:, :, self.window_size // 2 - 1, 0:self.hidden_size]\n\n            transition_sequence = self.composer(child1=left_sequence, child2=sequence)\n            transition_sequence = transition_sequence * input_mask\n\n            tp = left_transition_probs\n            sequence = tp * transition_sequence + (1 - tp) * residual_sequence\n            sequence = sequence * input_mask\n            steps = steps + active_probs\n\n            bounded_probs = transition_probs\n            active_probs = active_probs * (1.0 - bounded_probs) * input_mask\n\n            active_probs = T.where(update_mask.view(N, 1, 1).expand(N, S, 1) == 1.0,\n                                   active_probs,\n                                   original_active_probs)\n\n            steps = T.where(update_mask.view(N, 1, 1).expand(N, S, 1) == 1.0,\n                            steps,\n                            original_steps)\n\n            sequence = T.where(update_mask.view(N, 1, 1).expand(N, S, D) == 1.0,\n                               sequence,\n                               original_sequence)\n\n            t += 1\n            discrete_active_status = T.where(active_probs > self.stop_threshold,\n                                             T.ones_like(active_probs).to(active_probs.device),\n                                             T.zeros_like(active_probs).to(active_probs.device))\n\n            halt_condition_component = T.sum(discrete_active_status.squeeze(-1), dim=1) - 2.0\n            update_mask = T.where((halt_condition_component <= 1) | (T.sum(input_mask.squeeze(-1), dim=-1) - 2.0 < t),\n                                  halt_zeros,\n                                  halt_ones)\n\n            proper_termination_condition = T.sum(discrete_active_status * last_token_mask, dim=1).squeeze(-1)\n            improperly_terminated_mask_ = T.where((halt_condition_component == 1) & (proper_termination_condition == 1),\n                                                  halt_zeros,\n                                                  halt_ones)\n\n            improperly_terminated_mask = improperly_terminated_mask * improperly_terminated_mask_\n\n            if self.config[\"early_stopping\"]:\n                if T.sum(update_mask) == 0.0:\n                    break\n\n        steps = steps * START_END_LAST_PAD_mask\n        sequence = sequence * (1 - END_mask)\n        active_probs = active_probs * (1 - END_mask)\n        sequence = sequence[:, 1:-1, :]  # remove START and END\n        active_probs = active_probs[:, 1:-1, :]  # remove START and END\n\n        last_token_mask = END_mask[:, 2:, :]\n        global_state = T.sum(sequence * last_token_mask, dim=1)\n\n        assert active_probs.size(1) == sequence.size(1)\n\n        entropy_penalty = self.compute_entropy_penalty(active_probs.squeeze(-1),\n                                                       last_token_mask.squeeze(-1))\n\n        speed_penalty = self.compute_speed_penalty(steps, input_mask)\n\n        entropy_penalty = entropy_penalty * improperly_terminated_mask\n        penalty = self.entropy_gamma * entropy_penalty + self.speed_gamma * speed_penalty\n\n        return sequence, global_state, penalty\n\n    # %%\n    def forward(self, sequence, input_mask, **kwargs):\n\n        if \"temperature\" in kwargs:\n            self.temperature = kwargs[\"temperature\"]\n        else:\n            self.temperature = 1.0\n\n        self.temperature = 1.0 if self.temperature is None else self.temperature\n\n        input_mask = input_mask.unsqueeze(-1)\n        sequence = sequence * input_mask\n\n        sequence, global_state, penalty = self.encoder_block(sequence, input_mask)\n        sequence = sequence * input_mask\n        return {\"sequence\": sequence, \"penalty\": penalty, \"global_state\": global_state}",
    "description": null,
    "url": null
}