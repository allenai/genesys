{
    "acronym": "rfa",
    "title": "Random Feature Attention",
    "seed_ids": [
        "bigbird",
        "lineartransformer",
        "transformerxl",
        "transformer"
    ],
    "s2id": "9ed25f101f19ea735ca300848948ed64064b97ca",
    "abstract": "Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
    "authors": [
        "Hao Peng",
        "Nikolaos Pappas",
        "Dani Yogatama",
        "Roy Schwartz",
        "Noah A. Smith",
        "Lingpeng Kong"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": "RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, is proposed and explored, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
    "citationCount": 292,
    "influentialCitationCount": 26,
    "code": "\"\"\"RFA cuda.\n\nEinsum notations:\n    b: bsz\n    s: seq_len\n    n: num_layers\n    h: num_heads\n    k: proj_dim\n    d: head_dim\n\"\"\"\n\n\nimport torch\n\n\nEPS = 1.0\n\n\ndef reverse_cumsum(x, dim):\n    return torch.flip(torch.cumsum(torch.flip(x, [dim]), dim), [dim])\n\n\ndef rfa_debug(q, k, v):\n    \"\"\"\n    Args:\n        q: [tgt_len, bsz * num_heads, proj_dim]\n        k: [tgt_len, bsz * num_heads, proj_dim]\n        v: [tgt_len, bsz * num_heads, head_dim]\n\n    Return:\n        attn: [tgt_len, bsz * num_heads, head_dim]\n    \"\"\"\n    s = torch.einsum(\"tbk,tbd->tbkd\", k, v)\n    s = torch.cumsum(s, dim=0)\n    qs = torch.einsum(\"tbkd,tbk->tbd\", s, q)\n\n    z = torch.cumsum(k, dim=0)\n    qz = torch.einsum(\"tbk,tbk->tb\", q, z).clamp_min(EPS)\n    attn = qs / qz.unsqueeze(-1)\n    return attn\n\n\nclass RFA(torch.autograd.Function):\n\n    @staticmethod\n    def forward_torch(q, k, v):\n        \"\"\"\n        Args:\n            q: [tgt_len, bsz * num_heads, proj_dim]\n            k: [tgt_len, bsz * num_heads, proj_dim]\n            v: [tgt_len, bsz * num_heads, head_dim]\n\n        Return:\n            attn: [tgt_len, bsz * num_heads, head_dim]\n        \"\"\"\n        s = torch.einsum(\"tbk,tbd->tbkd\", k, v)\n        s = torch.cumsum(s, dim=0)\n        qs = torch.einsum(\"tbkd,tbk->tbd\", s, q)\n\n        z = torch.cumsum(k, dim=0)\n        qz = torch.einsum(\"tbk,tbk->tb\", q, z).clamp_min(EPS)\n        attn = qs / qz.unsqueeze(-1)\n        return attn\n\n    @staticmethod\n    def backward_torch(q, k, v, grad_attn):\n        \"\"\"\n        Args:\n            grad_attn: [tgt_len, bsz * num_heads, head_dim]\n            q: [tgt_len, bsz * num_heads, proj_dim]\n            k: [tgt_len, bsz * num_heads, proj_dim]\n            v: [tgt_len, bsz * num_heads, head_dim]\n        Return:\n            grad_q: [tgt_len, bsz * num_heads, proj_dim]\n            grad_k: [tgt_len, bsz * num_heads, proj_dim]\n            grad_v: [tgt_len, bsz * num_heads, head_dim]\n        \"\"\"\n        s = torch.einsum(\"tbk,tbd->tbkd\", k, v)\n        s = torch.cumsum(s, dim=0)\n        qs = torch.einsum(\"tbkd,tbk->tbd\", s, q)\n\n        z = torch.cumsum(k, dim=0)\n        qz = torch.einsum(\"tbk,tbk->tb\", q, z).clamp_min(EPS)\n\n        # [bsz, tgt_len, head_dim]\n        grad_qs = grad_attn / qz.unsqueeze(-1)\n\n        grad_qz = torch.einsum(\"tbd,tbd->tb\", grad_attn, qs)\n        grad_qz = -grad_qz / (qz ** 2)\n        grad_qz = grad_qz * (qz > EPS)\n\n        grad_q = torch.einsum(\"tbd,tbkd->tbk\", grad_qs, s) \\\n            + grad_qz.unsqueeze(-1) * z\n\n        grad_s = torch.einsum(\"tbk,tbd->tbkd\", q, grad_qs)\n        grad_s = reverse_cumsum(grad_s, dim=0)\n        grad_k = torch.einsum(\"tbkd,tbd->tbk\", grad_s, v)\n        grad_v = torch.einsum(\"tbkd,tbk->tbd\", grad_s, k)\n\n        grad_k = grad_k + reverse_cumsum(q * grad_qz.unsqueeze(-1), dim=0)\n\n        return grad_q, grad_k, grad_v\n\n    @staticmethod\n    def forward(ctx, q, k, v):\n        \"\"\"\n        Args:\n            q: [tgt_len, bsz * num_heads, proj_dim]\n            k: [tgt_len, bsz * num_heads, proj_dim]\n            v: [tgt_len, bsz * num_heads, head_dim]\n\n        Return:\n            attn: [tgt_len, bsz * num_heads, head_dim]\n        \"\"\"\n        ctx.save_for_backward(q, k, v)\n        attn = RFA.forward_cuda(q, k, v)\n        # attn = RFA.forward_torch(q, k, v)\n        return attn\n\n    @staticmethod\n    def backward(ctx, grad_attn):\n        \"\"\"\n        Args:\n            q: [tgt_len, bsz * num_heads, proj_dim]\n            k: [tgt_len, bsz * num_heads, proj_dim]\n            v: [tgt_len, bsz * num_heads, head_dim]\n            grad_attn: [tgt_len, bsz * num_heads, head_dim]\n        Return:\n            grad_q: [tgt_len, bsz * num_heads, proj_dim]\n            grad_k: [tgt_len, bsz * num_heads, proj_dim]\n            grad_v: [tgt_len, bsz * num_heads, head_dim]\n        \"\"\"\n        q, k, v = ctx.saved_tensors\n        grad_q, grad_k, grad_v = RFA.backward_cuda(q, k, v, grad_attn)\n        # grad_q, grad_k, grad_v = RFA.backward_torch(q, k, v, grad_attn)\n        return grad_q, grad_k, grad_v\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda:0\")\n    dtype = torch.double\n\n    bsz, tgt_len, proj_dim, head_dim = 2, 15, 128, 8\n    q = torch.rand(\n        (tgt_len, bsz, head_dim),\n        device=device, dtype=dtype, requires_grad=True) - 0.5\n    k = torch.rand(\n        (tgt_len, bsz, head_dim),\n        device=device, dtype=dtype, requires_grad=True) - 0.5\n    v = torch.rand(\n        (tgt_len, bsz, head_dim),\n        device=device, dtype=dtype, requires_grad=True)\n\n    res = torch.autograd.gradcheck(\n        RFA.apply,\n        (q, k, v),\n        raise_exception=True)",
    "description": null,
    "url": null
}