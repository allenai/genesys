{
    "acronym": "memformer",
    "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling",
    "seed_ids": [
        "lineartransformer",
        "linformer",
        "compressivetransformer",
        "sparsetransformer"
    ],
    "s2id": "67ee20536c30a225b86902af2f091e28e5e19b40",
    "abstract": "Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.",
    "authors": [
        "Qingyang Wu",
        "Zhenzhong Lan",
        "Kun Qian",
        "Jing Gu",
        "A. Geramifard",
        "Zhou Yu"
    ],
    "venue": "AACL/IJCNLP",
    "year": 2020,
    "tldr": "This work presents Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information, and proposes a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back- Propagation through time with a significantly reduced memory requirement.",
    "citationCount": 36,
    "influentialCitationCount": 2,
    "code": "import math\nimport torch\nfrom torch import nn, einsum\nfrom functools import partial\nimport torch.nn.functional as F\nfrom inspect import isfunction\nfrom einops import rearrange, repeat\nfrom collections import namedtuple\nfrom memformer.autoregressive_wrapper import AutoregressiveWrapper\n\n# constants\n\nResults = namedtuple('Results', ['enc_out', 'mem', 'dec_out'])\nEncOnlyResults = namedtuple('EncOnlyResults', ['enc_out', 'mem'])\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\n\n# keyword argument helpers\n\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key, None), keys))\n    return dict(zip(keys, values))\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\ndef group_by_key_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\n# helper classes\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\n# positional embedding\n\nclass RelativePositionBias(nn.Module):\n    def __init__(self, causal = False, num_buckets = 32, max_distance = 128, heads = 8):\n        super().__init__()\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, causal = True, num_buckets = 32, max_distance = 128):\n        ret = 0\n        n = -relative_position\n        if causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n        ).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def forward(self, qlen, klen):\n        device = self.relative_attention_bias.weight.device\n        q_pos = torch.arange(qlen, dtype = torch.long, device = device)\n        k_pos = torch.arange(klen, dtype = torch.long, device = device)\n        rel_pos = k_pos[None, :] - q_pos[:, None]\n        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets)\n        values = self.relative_attention_bias(rp_bucket)\n        return rearrange(values, 'i j h -> () h i j')\n\n# main classes\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult),\n            nn.GELU(),\n            nn.Linear(dim * mult, dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, causal = False, rel_pos_emb = False):\n        super().__init__()\n        assert (dim % heads) == 0, 'dimension must be divisible by number of heads'\n        dim_head = dim // heads\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        self.causal = causal\n \n        self.to_q = nn.Linear(dim, dim)\n        self.to_kv = nn.Linear(dim, dim * 2)\n        self.to_out = nn.Linear(dim, dim)\n\n    def forward(self, x, context = None, pos_emb = None, mask = None, query_mask = None, kv_mask = None, attend_self = False):\n        b, n, _, h, scale, device = *x.shape, self.heads, self.scale, x.device\n\n        if attend_self:\n            kv_input = torch.cat((x, context), dim = 1)\n        else:\n            kv_input = default(context, x)\n\n        q = self.to_q(x)\n        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n        dots = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n\n        if exists(pos_emb):\n            pos_emb_bias = pos_emb(*dots.shape[-2:])\n            dots += pos_emb_bias\n\n        mask_value = max_neg_value(dots)\n\n        if self.causal:\n            causal_mask = torch.ones((n, n), device = device).triu_(1).bool()\n            dots.masked_fill_(causal_mask, mask_value)\n            del causal_mask\n\n        if any(map(exists, (query_mask, kv_mask))):\n            query_mask = default(query_mask, lambda: torch.ones((b, n), device = device).bool())\n\n            if exists(context):\n                kv_mask = default(kv_mask, lambda: torch.ones((b, context.shape[1]), device = device).bool())\n            else:\n                kv_mask = default(kv_mask, query_mask)\n\n            query_mask = rearrange(query_mask, 'b i -> b () i ()')\n            kv_mask = rearrange(kv_mask, 'b j -> b () () j')\n            seq_mask = query_mask * kv_mask\n            dots.masked_fill_(~seq_mask, mask_value)\n            del seq_mask\n\n        if exists(mask):\n            mask = rearrange(mask, 'b i j -> b () i j')\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        attn = dots.softmax(dim = -1)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Encoder(nn.Module):\n    def __init__(self, dim, depth, heads = 8):\n        super().__init__()\n        self.rel_pos_emb = RelativePositionBias(heads = heads)\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, Attention(dim, heads = heads, rel_pos_emb = True))),\n                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n                Residual(PreNorm(dim, FeedForward(dim)))\n            ]))\n    def forward(self, x, context = None, src_mask = None):\n        for (self_attn, cross_attn, ff) in self.layers:\n            x = self_attn(x, pos_emb = self.rel_pos_emb, query_mask = src_mask)\n            x = cross_attn(x, context = context)\n            x = ff(x)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, dim, depth, heads = 8):\n        super().__init__()\n        self.rel_pos_emb = RelativePositionBias(heads = heads, causal = True)\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, Attention(dim, heads = heads, causal = True, rel_pos_emb = True))),\n                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n                Residual(PreNorm(dim, FeedForward(dim))),\n            ]))\n    def forward(self, x, context = None, src_mask = None, tgt_mask = None):\n        for (self_attn, cross_attn, ff) in self.layers:\n            x = self_attn(x, pos_emb = self.rel_pos_emb, query_mask = src_mask)\n            x = cross_attn(x, context = context, query_mask = src_mask, kv_mask = tgt_mask)\n            x = ff(x)\n        return x\n\nclass TransformerWrapper(nn.Module):\n    def __init__(self, *, num_tokens, max_seq_len, dim, layer_blocks, heads = 8, return_logits = True):\n        super().__init__()\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.max_seq_len = max_seq_len\n        self.layer_blocks = layer_blocks\n        self.norm = nn.LayerNorm(dim)\n        self.to_logits = nn.Linear(dim, num_tokens) if return_logits else nn.Identity()\n\n    def forward(self, x, **kwargs):\n        _, n, device = *x.shape, x.device\n        x = self.token_emb(x)\n        x = self.layer_blocks(x, **kwargs)\n        x = self.norm(x)\n        return self.to_logits(x)\n\nclass Memformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        num_memory_slots,\n        num_mem_updates = 1,\n        encoder_only = False,\n        mem_update_attn_heads = 8,\n        **kwargs):\n        super().__init__()\n        enc_kwargs, kwargs = group_by_key_prefix_and_trim('enc_', kwargs)\n        dec_kwargs, kwargs = group_by_key_prefix_and_trim('dec_', kwargs)\n        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs, 'dimension of either encoder or decoder must be set with `dim` keyword'\n        enc_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], enc_kwargs)\n        dec_transformer_kwargs = pick_and_pop(['num_tokens', 'max_seq_len'], dec_kwargs)\n\n        self.encoder = TransformerWrapper(\n            dim = dim,\n            layer_blocks = Encoder(dim = dim, **enc_kwargs),\n            return_logits = False,\n            **enc_transformer_kwargs\n        )\n\n        self.decoder = TransformerWrapper(\n            dim = dim,\n            layer_blocks = Decoder(dim = dim, **dec_kwargs),\n            return_logits = True,\n            **dec_transformer_kwargs\n        ) if not encoder_only else None\n\n        if exists(self.decoder):\n            self.decoder = AutoregressiveWrapper(self.decoder)\n\n        self.num_mem = num_memory_slots\n        self.memory_slots = nn.Parameter(torch.randn(num_memory_slots, dim))\n\n        self.num_mem_updates = num_mem_updates\n        self.mem_updater = Attention(dim, heads = mem_update_attn_heads)\n        self.gru = nn.GRUCell(dim, dim)\n        self.mem_ff = Residual(PreNorm(dim, FeedForward(dim)))\n\n    def get_initial_mem(self, batch_size):\n        return repeat(self.memory_slots, 'n d -> b n d', b = batch_size)\n\n    def forward(self, src, tgt = None, mems = None, src_mask = None, tgt_mask = None):\n        b, n, num_mem, device = *src.shape, self.num_mem, src.device\n        mems = default(mems, lambda: self.get_initial_mem(b))\n\n        enc = self.encoder(src, context = mems, src_mask = src_mask)\n\n        if exists(self.decoder) and exists(tgt):\n            dec_out = self.decoder(tgt, context = enc, src_mask = tgt_mask, tgt_mask = src_mask, return_loss = True)\n        else:\n            dec_out = torch.tensor(0., requires_grad = True, device = device)\n\n        # update memory with attention\n        mem_mask = torch.eye(num_mem, num_mem, device = device).bool()\n        mem_mask = repeat(mem_mask, 'i j -> b i j', b = b)\n        mem_mask = F.pad(mem_mask, (0, n), value = True)\n\n        if exists(src_mask):\n            src_mask = rearrange(src_mask, 'b j -> b () j')\n            mem_enc_mask = F.pad(src_mask, (num_mem, 0), value = True)\n            mem_mask &= mem_enc_mask\n\n        for _ in range(self.num_mem_updates):\n            prev_mems = mems\n            updated_mems = self.mem_updater(mems, enc, mask = mem_mask, attend_self = True)\n\n            next_mems = self.gru(\n                rearrange(updated_mems, 'b n d -> (b n) d'),\n                rearrange(prev_mems, 'b n d -> (b n) d')\n            )\n\n            mems = rearrange(next_mems, '(b n) d -> b n d', b = b)\n            mems = self.mem_ff(mems)\n\n        if not exists(self.decoder):\n            return EncOnlyResults(enc, mems)\n\n        return Results(enc, mems, dec_out)",
    "description": null,
    "url": null
}