{
    "acronym": "vntransformer",
    "title": "VN-Transformer: Rotation-Equivariant Attention for Vector Neurons",
    "seed_ids": [],
    "s2id": "9628221b1fa271d93968e97a0a5868b657e0a5be",
    "abstract": "Rotation equivariance is a desirable property in many practical applications such as motion forecasting and 3D perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations. Vector Neurons (VN) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional\"vector neurons.\"We introduce a novel\"VN-Transformer\"architecture to address several shortcomings of the current VN models. Our contributions are: $(i)$ we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models; $(ii)$ we extend the VN framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; $(iii)$ we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; $(iv)$ we show that small tradeoffs in equivariance ($\\epsilon$-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models. Finally, we apply our VN-Transformer to 3D shape classification and motion forecasting with compelling results.",
    "authors": [
        "Serge Assaad",
        "Carlton Downey",
        "Rami Al-Rfou",
        "Nigamaa Nayakanti",
        "Benjamin Sapp"
    ],
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2022,
    "tldr": "A rotation-equivariant attention mechanism is derived which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models, and small tradeoffs in equivariance can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware.",
    "citationCount": 12,
    "influentialCitationCount": 1,
    "code": "import torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum, Tensor\n\nfrom einops import rearrange, repeat, reduce\nfrom einops.layers.torch import Rearrange, Reduce\n\nfrom functools import wraps\nfrom packaging import version\nfrom collections import namedtuple\n\n\n# constants\n\nFlashAttentionConfig = namedtuple('FlashAttentionConfig', ['enable_flash', 'enable_math', 'enable_mem_efficient'])\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef once(fn):\n    called = False\n    @wraps(fn)\n    def inner(x):\n        nonlocal called\n        if called:\n            return\n        called = True\n        return fn(x)\n    return inner\n\nprint_once = once(print)\n\n# main class\n\nclass Attend(nn.Module):\n    def __init__(\n        self,\n        dropout = 0.,\n        flash = False,\n        l2_dist = False\n    ):\n        super().__init__()\n        assert not (flash and l2_dist), 'flash attention is not compatible with l2 distance'\n        self.l2_dist = l2_dist\n\n        self.dropout = dropout\n        self.attn_dropout = nn.Dropout(dropout)\n\n        self.flash = flash\n        assert not (flash and version.parse(torch.__version__) < version.parse('2.0.0')), 'in order to use flash attention, you must be using pytorch 2.0 or above'\n\n        # determine efficient attention configs for cuda and cpu\n\n        self.cpu_config = FlashAttentionConfig(True, True, True)\n        self.cuda_config = None\n\n        if not torch.cuda.is_available() or not flash:\n            return\n\n        device_properties = torch.cuda.get_device_properties(torch.device('cuda'))\n\n        if device_properties.major == 8 and device_properties.minor == 0:\n            print_once('A100 GPU detected, using flash attention if input tensor is on cuda')\n            self.cuda_config = FlashAttentionConfig(True, False, False)\n        else:\n            print_once('Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda')\n            self.cuda_config = FlashAttentionConfig(False, True, True)\n\n    def flash_attn(self, q, k, v, mask = None):\n        _, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n\n        # Check if mask exists and expand to compatible shape\n        # The mask is B L, so it would have to be expanded to B H N L\n\n        if exists(mask):\n            mask = mask.expand(-1, heads, q_len, -1)\n\n        # Check if there is a compatible device for flash attention\n\n        config = self.cuda_config if is_cuda else self.cpu_config\n\n        # pytorch 2.0 flash attn: q, k, v, mask, dropout, softmax_scale\n\n        with torch.backends.cuda.sdp_kernel(**config._asdict()):\n            out = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask = mask,\n                dropout_p = self.dropout if self.training else 0.\n            )\n\n        return out\n\n    def forward(self, q, k, v, mask = None):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        q_len, k_len, device = q.shape[-2], k.shape[-2], q.device\n\n        scale = q.shape[-1] ** -0.5\n\n        if exists(mask) and mask.ndim != 4:\n            mask = rearrange(mask, 'b j -> b 1 1 j')\n\n        if self.flash:\n            return self.flash_attn(q, k, v, mask = mask)\n\n        # similarity\n\n        sim = einsum(f\"b h i d, b h j d -> b h i j\", q, k) * scale\n\n        # l2 distance\n\n        if self.l2_dist:\n            # -cdist squared == (-q^2 + 2qk - k^2)\n            # so simply work off the qk above\n            q_squared = reduce(q ** 2, 'b h i d -> b h i 1', 'sum')\n            k_squared = reduce(k ** 2, 'b h j d -> b h 1 j', 'sum')\n            sim = sim * 2 - q_squared - k_squared\n\n        # key padding mask\n\n        if exists(mask):\n            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n\n        # attention\n\n        attn = sim.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n\n        # aggregate values\n\n        out = einsum(f\"b h i j, b h j d -> b h i d\", attn, v)\n\n        return out\n    \n# helper\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef inner_dot_product(x, y, *, dim = -1, keepdim = True):\n    return (x * y).sum(dim = dim, keepdim = keepdim)\n\n# layernorm\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.register_buffer('beta', torch.zeros(dim))\n\n    def forward(self, x):\n        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n\n# equivariant modules\n\nclass VNLinear(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        bias_epsilon = 0.\n    ):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(dim_out, dim_in))\n\n        self.bias = None\n        self.bias_epsilon = bias_epsilon\n\n        # in this paper, they propose going for quasi-equivariance with a small bias, controllable with epsilon, which they claim lead to better stability and results\n\n        if bias_epsilon > 0.:\n            self.bias = nn.Parameter(torch.randn(dim_out))\n\n    def forward(self, x):\n        out = einsum('... i c, o i -> ... o c', x, self.weight)\n\n        if exists(self.bias):\n            bias = F.normalize(self.bias, dim = -1) * self.bias_epsilon\n            out = out + rearrange(bias, '... -> ... 1')\n\n        return out\n\nclass VNReLU(nn.Module):\n    def __init__(self, dim, eps = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.W = nn.Parameter(torch.randn(dim, dim))\n        self.U = nn.Parameter(torch.randn(dim, dim))\n\n    def forward(self, x):\n        q = einsum('... i c, o i -> ... o c', x, self.W)\n        k = einsum('... i c, o i -> ... o c', x, self.U)\n\n        qk = inner_dot_product(q, k)\n\n        k_norm = k.norm(dim = -1, keepdim = True).clamp(min = self.eps)\n        q_projected_on_k = q - inner_dot_product(q, k / k_norm) * k\n\n        out = torch.where(\n            qk >= 0.,\n            q,\n            q_projected_on_k\n        )\n\n        return out\n\nclass VNAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 64,\n        heads = 8,\n        dim_coor = 3,\n        bias_epsilon = 0.,\n        l2_dist_attn = False,\n        flash = False,\n        num_latents = None   # setting this would enable perceiver-like cross attention from latents to sequence, with the latents derived from VNWeightedPool\n    ):\n        super().__init__()\n        assert not (l2_dist_attn and flash), 'l2 distance attention is not compatible with flash attention'\n\n        self.scale = (dim_coor * dim_head) ** -0.5\n        dim_inner = dim_head * heads\n        self.heads = heads\n\n        self.to_q_input = None\n        if exists(num_latents):\n            self.to_q_input = VNWeightedPool(dim, num_pooled_tokens = num_latents, squeeze_out_pooled_dim = False)\n\n        self.to_q = VNLinear(dim, dim_inner, bias_epsilon = bias_epsilon)\n        self.to_k = VNLinear(dim, dim_inner, bias_epsilon = bias_epsilon)\n        self.to_v = VNLinear(dim, dim_inner, bias_epsilon = bias_epsilon)\n        self.to_out = VNLinear(dim_inner, dim, bias_epsilon = bias_epsilon)\n\n        if l2_dist_attn and not exists(num_latents):\n            # tied queries and keys for l2 distance attention, and not perceiver-like attention\n            self.to_k = self.to_q\n\n        self.attend = Attend(flash = flash, l2_dist = l2_dist_attn)\n\n    def forward(self, x, mask = None):\n        \"\"\"\n        einstein notation\n        b - batch\n        n - sequence\n        h - heads\n        d - feature dimension (channels)\n        c - coordinate dimension (3 for 3d space)\n        i - source sequence dimension\n        j - target sequence dimension\n        \"\"\"\n\n        c = x.shape[-1]\n\n        if exists(self.to_q_input):\n            q_input = self.to_q_input(x, mask = mask)\n        else:\n            q_input = x\n\n        q, k, v = self.to_q(q_input), self.to_k(x), self.to_v(x)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) c -> b h n (d c)', h = self.heads), (q, k, v))\n\n        out = self.attend(q, k, v, mask = mask)\n\n        out = rearrange(out, 'b h n (d c) -> b n (h d) c', c = c)\n        return self.to_out(out)\n\ndef VNFeedForward(dim, mult = 4, bias_epsilon = 0.):\n    dim_inner = int(dim * mult)\n    return nn.Sequential(\n        VNLinear(dim, dim_inner, bias_epsilon = bias_epsilon),\n        VNReLU(dim_inner),\n        VNLinear(dim_inner, dim, bias_epsilon = bias_epsilon)\n    )\n\nclass VNLayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.ln = LayerNorm(dim)\n\n    def forward(self, x):\n        norms = x.norm(dim = -1)\n        x = x / rearrange(norms.clamp(min = self.eps), '... -> ... 1')\n        ln_out = self.ln(norms)\n        return x * rearrange(ln_out, '... -> ... 1')\n\nclass VNWeightedPool(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out = None,\n        num_pooled_tokens = 1,\n        squeeze_out_pooled_dim = True\n    ):\n        super().__init__()\n        dim_out = default(dim_out, dim)\n        self.weight = nn.Parameter(torch.randn(num_pooled_tokens, dim, dim_out))\n        self.squeeze_out_pooled_dim = num_pooled_tokens == 1 and squeeze_out_pooled_dim\n\n    def forward(self, x, mask = None):\n        if exists(mask):\n            mask = rearrange(mask, 'b n -> b n 1 1')\n            x = x.masked_fill(~mask, 0.)\n            numer = reduce(x, 'b n d c -> b d c', 'sum')\n            denom = mask.sum(dim = 1)\n            mean_pooled = numer / denom.clamp(min = 1e-6)\n        else:\n            mean_pooled = reduce(x, 'b n d c -> b d c', 'mean')\n\n        out = einsum('b d c, m d e -> b m e c', mean_pooled, self.weight)\n\n        if not self.squeeze_out_pooled_dim:\n            return out\n\n        out = rearrange(out, 'b 1 d c -> b d c')\n        return out\n\n# equivariant VN transformer encoder\n\nclass VNTransformerEncoder(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        depth,\n        dim_head = 64,\n        heads = 8,\n        dim_coor = 3,\n        ff_mult = 4,\n        final_norm = False,\n        bias_epsilon = 0.,\n        l2_dist_attn = False,\n        flash_attn = False\n    ):\n        super().__init__()\n        self.dim = dim\n        self.dim_coor = dim_coor\n\n        self.layers = nn.ModuleList([])\n\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                VNAttention(dim = dim, dim_head = dim_head, heads = heads, bias_epsilon = bias_epsilon, l2_dist_attn = l2_dist_attn, flash = flash_attn),\n                VNLayerNorm(dim),\n                VNFeedForward(dim = dim, mult = ff_mult, bias_epsilon = bias_epsilon),\n                VNLayerNorm(dim)\n            ]))\n\n        self.norm = VNLayerNorm(dim) if final_norm else nn.Identity()\n\n    def forward(\n        self,\n        x,\n        mask = None\n    ):\n        *_, d, c = x.shape\n\n        assert x.ndim == 4 and d == self.dim and c == self.dim_coor, 'input needs to be in the shape of (batch, seq, dim ({self.dim}), coordinate dim ({self.dim_coor}))'\n\n        for attn, attn_post_ln, ff, ff_post_ln in self.layers:\n            x = attn_post_ln(attn(x, mask = mask)) + x\n            x = ff_post_ln(ff(x)) + x\n\n        return self.norm(x)\n\n# invariant layers\n\nclass VNInvariant(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_coor = 3,\n\n    ):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            VNLinear(dim, dim_coor),\n            VNReLU(dim_coor),\n            Rearrange('... d e -> ... e d')\n        )\n\n    def forward(self, x):\n        return einsum('b n d i, b n i o -> b n o', x, self.mlp(x))\n\n# main class\n\nclass VNTransformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        num_tokens = None,\n        dim_feat = None,\n        dim_head = 64,\n        heads = 8,\n        dim_coor = 3,\n        reduce_dim_out = True,\n        bias_epsilon = 0.,\n        l2_dist_attn = False,\n        flash_attn = False,\n        translation_equivariance = False,\n        translation_invariant = False\n    ):\n        super().__init__()\n        self.token_emb = nn.Embedding(num_tokens, dim) if exists(num_tokens) else None\n\n        dim_feat = default(dim_feat, 0)\n        self.dim_feat = dim_feat\n        self.dim_coor_total = dim_coor + dim_feat\n\n        assert (int(translation_equivariance) + int(translation_invariant)) <= 1\n        self.translation_equivariance = translation_equivariance\n        self.translation_invariant = translation_invariant\n\n        self.vn_proj_in = nn.Sequential(\n            Rearrange('... c -> ... 1 c'),\n            VNLinear(1, dim, bias_epsilon = bias_epsilon)\n        )\n\n        self.encoder = VNTransformerEncoder(\n            dim = dim,\n            depth = depth,\n            dim_head = dim_head,\n            heads = heads,\n            bias_epsilon = bias_epsilon,\n            dim_coor = self.dim_coor_total,\n            l2_dist_attn = l2_dist_attn,\n            flash_attn = flash_attn\n        )\n\n        if reduce_dim_out:\n            self.vn_proj_out = nn.Sequential(\n                VNLayerNorm(dim),\n                VNLinear(dim, 1, bias_epsilon = bias_epsilon),\n                Rearrange('... 1 c -> ... c')\n            )\n        else:\n            self.vn_proj_out = nn.Identity()\n\n    def forward(\n        self,\n        coors,\n        *,\n        feats = None,\n        mask = None,\n        return_concatted_coors_and_feats = False\n    ):\n        if self.translation_equivariance or self.translation_invariant:\n            coors_mean = reduce(coors, '... c -> c', 'mean')\n            coors = coors - coors_mean\n\n        x = coors\n\n        if exists(feats):\n            if feats.dtype == torch.long:\n                assert exists(self.token_emb), 'num_tokens must be given to the VNTransformer (to build the Embedding), if the features are to be given as indices'\n                feats = self.token_emb(feats)\n\n            assert feats.shape[-1] == self.dim_feat, f'dim_feat should be set to {feats.shape[-1]}'\n            x = torch.cat((x, feats), dim = -1)\n\n        assert x.shape[-1] == self.dim_coor_total\n\n        x = self.vn_proj_in(x)\n        x = self.encoder(x, mask = mask)\n        x = self.vn_proj_out(x)\n\n        coors_out, feats_out = x[..., :3], x[..., 3:]\n\n        if self.translation_equivariance:\n            coors_out = coors_out + coors_mean\n\n        if not exists(feats):\n            return coors_out\n\n        if return_concatted_coors_and_feats:\n            return torch.cat((coors_out, feats_out), dim = -1)\n\n        return coors_out, feats_out",
    "description": null,
    "url": null
}