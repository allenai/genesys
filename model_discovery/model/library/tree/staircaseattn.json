{
    "acronym": "staircaseattn",
    "title": "Staircase Attention for Recurrent Processing of Sequences",
    "seed_ids": [
        "transformerxl"
    ],
    "s2id": "b50815251c948f00baedccaf5f56c281ffa7650f",
    "abstract": "Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.",
    "authors": [
        "Da Ju",
        "Stephen Roller",
        "Sainbayar Sukhbaatar",
        "J. Weston"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": "Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, and to provide improved modeling power compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.",
    "citationCount": 10,
    "influentialCitationCount": 2,
    "code": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom enum import IntEnum\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom models.transformer_seq import FeedForwardLayer, TransformerOutput\nfrom models.utils import pos_emb, skew, unskew\n\n\n\n\n\nclass AdaptiveMask(nn.Module):\n    def __init__(\n        self,\n        size,\n        ramp_size,\n        init_ratio=0,\n        shape=(1,),\n    ):\n        super(AdaptiveMask, self).__init__()\n        self.size = size\n        self.ramp_size = ramp_size\n        self.size_ratio = nn.Parameter(torch.zeros(*shape) + init_ratio)\n        mask_template = torch.linspace(1 - size, 0, steps=size)\n        self.register_buffer(\"mask_template\", mask_template)\n\n    def prepare_mask(self, span):\n        mask = self.mask_template + self.size_ratio * self.size\n        mask = mask / self.ramp_size + 1\n        mask = mask.clamp(0, 1)\n        if span < self.size:\n            # the input could have been trimmed beforehand to save computation\n            mask = mask.narrow(-1, self.size - span, span)\n        self.mask_prepared = mask\n\n    def forward(self, x):\n        if hasattr(self, \"mask_prepared\"):\n            return x * self.mask_prepared\n\n        mask = self.mask_template + self.size_ratio * self.size\n        mask = mask / self.ramp_size + 1\n        mask = mask.clamp(0, 1)\n        if x.size(-1) < self.size:\n            # the input could have been trimmed beforehand to save computation\n            mask = mask.narrow(-1, self.size - x.size(-1), x.size(-1))\n        x = x * mask\n        return x\n\n    def get_max_size(self, include_ramp=True):\n        max_size = self.size_ratio.max().item()\n        max_size = max_size * self.size\n        if include_ramp:\n            max_size += self.ramp_size\n        max_size = max(0, min(self.size, math.ceil(max_size)))\n        return max_size\n\n    def get_avg_size(self, include_ramp=True):\n        avg_size = self.size_ratio.mean().item()\n        avg_size = avg_size * self.size\n        if include_ramp:\n            avg_size += self.ramp_size\n        avg_size = max(0, min(self.size, math.ceil(avg_size)))\n        return avg_size\n\n    def param_clamp(self):\n        self.size_ratio.data.clamp_(0, 1)\n\n\nclass AdaptiveSpan(nn.Module):\n    def __init__(self, args, size, loss_coeff, ramp_size, init_ratio):\n        super(AdaptiveSpan, self).__init__()\n        self.size = size\n        self.loss_coeff = loss_coeff\n        self.args = args\n        if self.args.adapt_span_layer:\n            self.mask = AdaptiveMask(self.size, ramp_size, init_ratio=init_ratio)\n        else:\n            self.mask = AdaptiveMask(\n                self.size, ramp_size, init_ratio=init_ratio, shape=(args.nheads, 1, 1),\n            )\n\n    def forward(self, attn):\n        if self.args.adapt_span_layer:\n            attn = self.mask(attn)\n        elif self.args.feedback:\n            B = attn.size(0)\n            attn = attn.reshape(B // self.args.nheads, self.args.nheads, 1, -1)\n            attn = self.mask(attn)\n            attn = attn.view(B, -1)\n        else:\n            B, M = attn.size(0), attn.size(1)\n            attn = attn.reshape(B // self.args.nheads, self.args.nheads, M, -1)\n            attn = self.mask(attn)\n            attn = attn.view(B, M, -1)\n        return attn\n\n    # how many steps can be skipped\n    def get_trim_len(self):\n        L = self.size\n        trim_len = min(L - 1, L - self.mask.get_max_size())\n        trim_len = (\n            math.floor(trim_len / self.args.adapt_span_trim_step)\n            * self.args.adapt_span_trim_step\n        )  # for better memory caching\n        return trim_len\n\n    # determine how long the cache should be\n    def get_cache_size(self):\n        trim_len = self.get_trim_len()\n        # give a buffer of 64 steps as spans can increase during training\n        return min(self.size, self.size - trim_len + 64)\n\n    # trim out unnecessary memory computation\n    def trim_memory(self, key, value, key_pe, val_pe):\n        trim_len = self.get_trim_len()\n        if key is not None:\n            if self.args.feedback:\n                cache_size = key.size(1)\n            else:\n                cache_size = key.size(1) - self.args.mem_sz\n            trim_len_cache = trim_len - (self.size - cache_size)\n            if self.args.feedback:\n                # keys and values must have cut to the right sizes beforehand.\n                # Also adapt_span_cache=False, so cache can't be shorter.\n                assert trim_len_cache == 0\n            if trim_len_cache > 0:\n                key = key[:, trim_len_cache:, :]\n                value = value[:, trim_len_cache:, :]\n            elif trim_len_cache < 0:\n                print(\n                    \"warning: cache is too short. cache_size={} trim_len={}\".format(\n                        cache_size, trim_len\n                    )\n                )\n                key = F.pad(key, [0, 0, -trim_len_cache, 0])\n                value = F.pad(value, [0, 0, -trim_len_cache, 0])\n        if trim_len > 0:\n            if key_pe is not None:\n                key_pe = key_pe[:, :, trim_len:]\n            if val_pe is not None:\n                val_pe = val_pe[:, trim_len:, :]\n        return key, value, key_pe, val_pe\n\n    # compute the loss\n    def get_loss(self):\n        return self.mask.size_ratio.mean() * self.loss_coeff * self.size\n\n    def param_clamp(self):\n        self.mask.param_clamp()\n\nclass SeqAttention(nn.Module):\n    \"\"\"\n    Sequential self-attention layer.\n\n    Each position only attends to its previous L positions (doesn't include the current\n    position) using relative position embeddings.\n    \"\"\"\n\n    def __init__(self, args):\n        super(SeqAttention, self).__init__()\n        self.args = args\n        self.dropout = nn.Dropout(args.dropout)\n\n        self.key_pe, self.val_pe = None, None\n        self.key_pe = pos_emb(args, (1, args.head_dim, args.attn_lim))\n\n        if self.args.adapt_span:\n            self.adaptive_span = AdaptiveSpan(\n                args,\n                args.attn_lim,\n                args.adapt_span_loss,\n                args.adapt_span_len,\n                args.adapt_span_init,\n            )\n\n    def forward(self, query, key, value):\n        # query = B x M x H\n        # key, value = B x (M+L) x H\n        aux_loss = 0\n\n        key_pe, val_pe = self.key_pe, self.val_pe\n        if self.args.adapt_span:\n            key, value, key_pe, val_pe = self.adaptive_span.trim_memory(\n                key, value, key_pe, val_pe\n            )\n\n        attn = 0\n\n        # compute attention from context\n        attn = torch.matmul(\n            query, key.transpose(-1, -2)\n        )  # B x M (dest) x (M+L) (src)\n        attn = unskew(attn)  # B x M x L\n\n        # compute the effect of position embedding\n        attn = attn + torch.matmul(query, key_pe)  # B x M x L\n\n        attn = attn / math.sqrt(self.args.head_dim)  # B x M X L\n        attn = F.softmax(attn, dim=-1)\n        if self.args.adapt_span:\n            attn = self.adaptive_span(attn)\n            attn = attn / (attn.sum(-1, keepdim=True) + 1e-8)\n        attn = self.dropout(attn)  # B x M X L\n\n        out = 0\n        attn_cont = skew(attn, 0)  # B x M X (L+M)\n        out = out + torch.matmul(attn_cont, value)  # B x M x H\n\n        return out, aux_loss\n\n\nclass MultiHeadSeqAttention(nn.Module):\n    def __init__(self, args):\n        super(MultiHeadSeqAttention, self).__init__()\n        self.args = args\n        self.attn = SeqAttention(args)\n\n        self.proj_query = nn.Linear(\n            args.hid_sz, args.head_dim * args.nheads, bias=False\n        )\n        self.proj_out = nn.Linear(args.head_dim * args.nheads, args.hid_sz, bias=False)\n        if self.args.pre_norm:\n            self.proj_out.weight.data.div_(math.sqrt(self.args.nlayers * 2))\n        self.proj_val = nn.Linear(\n            args.hid_sz, args.head_dim * args.nheads, bias=False\n        )\n        self.proj_key = nn.Linear(\n            args.hid_sz, args.head_dim * args.nheads, bias=False\n        )\n\n    def head_reshape(self, x):\n        K = self.args.nheads\n        D = self.args.head_dim\n        sz = x.size()\n        sz = sz[:-1] + (K, D)  # B x (M+L) x K x D\n        x = x.view(sz)  # B x (M+L) x K x D\n        x = x.transpose(1, 2).contiguous()  # B x K x (M+L) x D\n        x = x.view(-1, x.size(-2), x.size(-1))  # B_K x (M+L) x D\n        return x\n\n    def forward(self, query, key, value):\n        B = query.size(0)\n        M = query.size(1)\n\n        query = self.proj_query(query)\n        query = self.head_reshape(query)\n        value = self.proj_val(value)\n        value = self.head_reshape(value)\n        key = self.proj_key(key)\n        key = self.head_reshape(key)\n\n        out, aux_loss = self.attn(query, key, value)  # B_K x M x D\n        out = out.view(B, self.args.nheads, M, self.args.head_dim)  # B x K x M x D\n        out = out.transpose(1, 2).contiguous()  # B x M x K x D\n        out = out.view(B, M, -1)  # B x M x K_D\n        out = self.proj_out(out)  # B x M x H\n        return out, aux_loss\n\n\n\ndef add_args(parser):\n    parser.add_argument(\n        \"--staircase-size\",\n        type=int,\n        default=64,\n        help=\"number of tokens in each transformer forward\",\n    )\n    parser.add_argument(\n        \"--max-staircase-size-forward\",\n        type=int,\n        default=63,\n        help=\"max number of fresh tokens considered\",\n    )\n    parser.add_argument(\n        \"--fix-staircase-size-forward\",\n        type=int,\n        default=-1,\n        help=\"max number of fresh tokens considered\",\n    )\n    parser.add_argument(\n        \"--validation-staircase-size-forward\",\n        type=int,\n        default=32,\n        help=\"max number of fresh tokens considered during validation\",\n    )\n    parser.add_argument(\n        \"--staircase-module-fixed-length\",\n        action=\"store_true\",\n        default=False,\n        help=\"init h_prev with 0s to ensure the transformer module has fixed forward length\",\n    )\n    parser.add_argument(\n        \"--out-drop\",\n        type=float,\n        default=0,\n        help=\"insert a dropout before the last linear layer\",\n    )\n    parser.add_argument(\n        \"--emb-drop\", type=float, default=0, help=\"dropout on input embedding\"\n    )\n\n\nclass StaircaseSeqAttention(nn.Module):\n    \"\"\"\n    Sequential self-attention layer.\n\n    Each position only attends to its previous L positions (doesn't include the current\n    position) using relative position embeddings.\n    \"\"\"\n\n    def __init__(self, args):\n        super(StaircaseSeqAttention, self).__init__()\n        self.args = args\n        self.dropout = nn.Dropout(args.dropout)\n\n        self.key_pe, self.val_pe = None, None\n        self.key_pe = pos_emb(args, (1, args.head_dim, args.attn_lim))\n\n    def forward(self, query, key, value):\n        # query = B x M x H\n        # key, value = B x (M+L) x H\n        # mask_causal M * L\n        mask_causal = query.new_zeros(\n            key.size(1), key.size(1)).fill_(float(\"-inf\"))\n        mask_causal = mask_causal.triu(diagonal=1)\n        mask_causal = mask_causal[-query.size(1):, ]\n        aux_loss = 0\n\n        key_pe, val_pe = self.key_pe, self.val_pe\n\n        attn = 0\n\n        attn = torch.matmul(query, key.transpose(-1, -2))\n\n        L_size = attn.size(-1)\n        attn_pos = torch.matmul(query, key_pe[:, :, -L_size:])  # B x M x L\n        attn_pos = skew(attn_pos, 0)  # B x M x (N + L)\n        attn_pos = attn_pos[:, :, -L_size - 1: -1]  # B x M x L\n        attn = attn + attn_pos\n        attn = attn + mask_causal\n\n        attn = attn / math.sqrt(self.args.head_dim)  # B x M X L\n        attn = F.softmax(attn, dim=-1)\n\n        attn = self.dropout(attn)  # B x M X L\n\n        out = 0\n\n        out = out + torch.matmul(attn, value)  # B x S x H\n\n        return out, aux_loss\n\n\nclass StaircaseMultiHeadSeqAttention(MultiHeadSeqAttention):\n    def __init__(self, args):\n        super(StaircaseMultiHeadSeqAttention, self).__init__(args)\n        self.args = args\n        self.attn = StaircaseSeqAttention(args)\n\n\nclass TransformerModLayer(nn.Module):\n    def __init__(self, args, layer_ind):\n        super(TransformerModLayer, self).__init__()\n        self.args = args\n        self.attn = StaircaseMultiHeadSeqAttention(args)\n        self.ff = FeedForwardLayer(args)\n        self.norm1 = nn.LayerNorm(args.hid_sz)\n        self.norm2 = nn.LayerNorm(args.hid_sz)\n\n    def attention(self, query, key, value):\n        return self.attn(query, key, value)[0]\n\n    def forward(self, h, context, **kargs):\n        # h = B x S x H\n        if self.args.pre_norm:\n            # add layer norm on context as well\n            context = self.norm1(context)\n            attn_out = self.attention(self.norm1(h), context, context)\n        else:\n            attn_out = self.attention(h, context, context)\n\n        # FF\n        if self.args.pre_norm:\n            h2 = h + attn_out  # B x S x H\n            ff_out = self.ff(self.norm2(h2))\n            out = h2 + ff_out  # B x S x H\n        else:\n            h2 = self.norm1(h + attn_out)  # B x S x H\n            ff_out = self.ff(h2)\n            out = self.norm2(h2 + ff_out)  # B x S x H\n\n        return out\n\n    def get_cache_size(self):\n        return 0\n\n\nclass TransformerMod(nn.Module):\n    def __init__(self, args):\n        super(TransformerMod, self).__init__()\n        self.args = args\n        self.build_layers()\n        for l in range(1, len(self.layers)):\n            self.layers[l].attn.attn.key_pe = self.layers[0].attn.attn.key_pe\n            self.layers[l].attn.attn.val_pe = self.layers[0].attn.attn.val_pe\n\n    def build_layers(self):\n        self.layers = nn.ModuleList()\n        for l in range(self.args.nlayers):\n            self.layers.append(TransformerModLayer(self.args, l))\n\n    def get_layer(self, layer_ind):\n        return self.layers[layer_ind]\n\n    def forward(self, h, context):\n        # h : B x S x H\n        for l in range(self.args.nlayers):\n            # only forward size get updated from the context as well\n            h = self.get_layer(l)(h, context)  # B x S x H\n            forward_size = h.size(1)\n            if h.size(1) == context.size(1):\n                # self-attention\n                context = h\n            else:\n                context = torch.cat([context[:, :-forward_size, :], h], dim=1)\n        return h\n\n\nclass StaircaseModel(nn.Module):\n    def __init__(self, args):\n        super(StaircaseModel, self).__init__()\n        self.args = args\n        self.transformer = TransformerMod(args)\n        self.fix_staircase_size_forward = self.args.fix_staircase_size_forward\n        self.staircase_size = self.args.staircase_size\n        self.mem_size = self.args.mem_sz\n        self.hidden_size = self.args.hid_sz\n\n        assert self.mem_size % self.fix_staircase_size_forward == 0\n        assert self.staircase_size % self.fix_staircase_size_forward == 0\n        self.validation_staircase_size_forward = (\n            self.args.validation_staircase_size_forward\n        )\n\n        self.in_emb = nn.Embedding(args.vocab_sz, args.hid_sz)\n\n        self.out = TransformerOutput(args)\n        if args.emb_drop > 0:\n            self.emb_dropout = nn.Dropout(args.emb_drop)\n        if args.out_drop > 0:\n            self.out_dropout = nn.Dropout(args.out_drop)\n        if self.args.pre_norm:\n            self.out_norm = nn.LayerNorm(args.hid_sz)\n\n    def init_hid_cache(self, batch_sz):\n        # creates a cache of # steps\n        # 256 / 64 = 4\n        # cache size\n        # 192\n        # 128\n        # 64\n        # 0\n        steps = self.staircase_size // self.fix_staircase_size_forward\n        hid = []\n        for i in range(steps):\n            cache_size = self.staircase_size - \\\n                (i + 1) * self.fix_staircase_size_forward\n            if cache_size > 0:\n                hid.append(\n                    [\n                        torch.zeros((batch_sz, cache_size, self.hidden_size)).to(\n                            self.args.device\n                        )\n                        for i in range(self.args.nlayers)\n                    ]\n                )\n        return hid\n\n    def get_cache(self, h_prev, idx):\n        if idx >= len(h_prev):\n            return [None]\n        return h_prev[idx]\n\n    def assemble_context(self, cache, prev_outputs, new_tokens):\n        context = [cache, prev_outputs, new_tokens]\n        context = [i for i in context if i is not None]\n        context = torch.cat(context, dim=1)\n        return context\n\n    def assemble_query(self, prev_outputs, new_tokens):\n        query = [prev_outputs, new_tokens]\n        query = [i for i in query if i is not None]\n        query = torch.cat(query, dim=1)\n        return query\n\n    def get_new_tokens(self, h, start_idx, end_idx):\n        if end_idx > h.size(1):\n            return None\n        return h[:, start_idx:end_idx, :]\n\n    def forward(self, x, h_prev, target=None, **kargs):\n        # input h B x M\n        # assume h_prev [B, staircase_size], and will init with 0s\n        # create output placeholder\n        # output [B, mem_size, hidden_size]\n        hid_after_embed = self.in_emb(x)  # B x M x H\n        if self.args.emb_drop > 0:\n            hid_after_embed = self.emb_dropout(hid_after_embed)\n        # no cache between forwards\n\n        output = hid_after_embed.new_zeros(\n            (hid_after_embed.size(0), self.mem_size, self.hidden_size)\n        )\n        start_idx = 0\n        # generate scheduling for staircases, randomness for training purpose\n        total_steps = (\n            self.mem_size + self.staircase_size\n        ) // self.fix_staircase_size_forward - 1\n        prev_output = None\n        for step_idx in range(total_steps):\n            end_idx = start_idx + self.fix_staircase_size_forward\n            cache = self.get_cache(h_prev, step_idx)\n            new_tokens = self.get_new_tokens(\n                hid_after_embed, start_idx, end_idx)\n            # should put into cache:\n            cache_id = step_idx - (\n                total_steps\n                - (self.staircase_size // self.fix_staircase_size_forward)\n                + 1\n            )\n            # consumed all forward steps\n            # when should we move prev_output forwards\n            if step_idx >= self.staircase_size // self.fix_staircase_size_forward:\n                prev_output = prev_output[:,\n                                          self.fix_staircase_size_forward:, :]\n            # input to the first layer of the model\n            context = self.assemble_context(\n                cache[0], prev_output, new_tokens)\n            h = self.assemble_query(prev_output, new_tokens)\n            assert context.size(1) <= self.staircase_size\n            # forward into layers\n            cache_for_next = []\n            for layer in range(self.args.nlayers):\n                # only forward size get updated from the context as well\n                h = self.transformer.get_layer(layer)(h, context)\n                if h.size(1) == context.size(1):\n                    # self-attention\n                    context = h\n                elif layer + 1 < self.args.nlayers:\n                    context = torch.cat([cache[layer + 1], h], dim=1)\n                # put into cache\n                if cache_id >= 0:\n                    cache_for_next.append(h)\n            # the output from the last layer\n            prev_output = h\n            # put into cache\n            if len(cache_for_next) > 0:\n                h_prev[cache_id] = cache_for_next\n\n            start_idx = end_idx\n            # put into output\n            if step_idx - self.staircase_size // self.fix_staircase_size_forward >= -1:\n                offset = (\n                    step_idx\n                    - self.staircase_size // self.fix_staircase_size_forward\n                    + 1\n                )\n                output[\n                    :,\n                    offset\n                    * self.fix_staircase_size_forward: offset\n                    * self.fix_staircase_size_forward\n                    + prev_output.size(1),\n                    :,\n                ] = prev_output\n        out = output\n        if self.args.pre_norm:\n            out = self.out_norm(out)\n        if self.args.out_drop > 0:\n            out = self.out_dropout(out)\n        out = self.out(out, target)\n        # feeding to the next transformer step.\n        return out, h_prev, 0.0",
    "description": null,
    "url": null
}