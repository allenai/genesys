{
    "acronym": "m2",
    "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture",
    "seed_ids": [],
    "s2id": "c85268696fe1435605ae66a18653cfdcf8153753",
    "abstract": "Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.",
    "authors": [
        "Daniel Y. Fu",
        "Simran Arora",
        "Jessica Grogan",
        "Isys Johnson",
        "Sabri Eyuboglu",
        "Armin W. Thomas",
        "Benjamin Spector",
        "Michael Poli",
        "A. Rudra",
        "Christopher R'e"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.",
    "citationCount": 22,
    "influentialCitationCount": 2,
    "code": "# Copyright (c) 2023, Dan Fu and Simran Arora.\n# Adapted from https://github.com/HazyResearch/safari/blob/main/src/models/sequence/hyena.py\n\nimport torch.nn as nn\nfrom einops import rearrange\nimport opt_einsum as oe\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\ncontract = oe.contract\n\nfrom src.utils.train import OptimModule\n\n\ndef fftconv_ref(u_variable, k, D_variable, dropout_mask, gelu=True, k_rev=None, flashfft=None):\n    # u.shape:   B H L\n    seqlen = u_variable.shape[-1]\n\n    if flashfft is not None:\n        y = flashfft(u_variable.to(dtype=torch.bfloat16).contiguous(), k)\n    else:\n        fft_size = 2 * seqlen\n        k_f = torch.fft.rfft(k, n=fft_size) / fft_size\n        if k_rev is not None:\n            k_rev_f = torch.fft.rfft(k_rev, n=fft_size) / fft_size\n            k_f = k_f + k_rev_f.conj()\n        u_f = torch.fft.rfft(u_variable.to(dtype=k.dtype), n=fft_size)\n\n        if len(u_variable.shape) > 3:\n            k_f = k_f.unsqueeze(1)\n\n        y = torch.fft.irfft(u_f * k_f, n=fft_size, norm=\"forward\")[..., :seqlen]\n\n    out = y + u_variable * D_variable\n\n    if gelu:\n        out = F.gelu(out)\n    if dropout_mask is not None:\n        return (out * rearrange(dropout_mask, \"b H -> b H 1\")).to(dtype=u_variable.dtype)\n    else:\n        return out.to(dtype=u_variable.dtype)\n\n\n@torch.jit.script\ndef mul_sum(q, y):\n    return (q * y).sum(dim=1)\n\n\nclass Sin(nn.Module):\n    def __init__(self, dim, w=10, w_mod=1, train_freq=True):\n        super().__init__()\n\n        init_tensor = torch.ones(1, dim)\n        self.freq = (\n            nn.Parameter(w * init_tensor)\n            if train_freq\n            else w * torch.ones(1, dim)\n        )\n        self.w_mod = w_mod\n\n    def forward(self, x):\n        return torch.sin(self.w_mod * self.freq * x)\n\n\nclass PositionalEmbedding(OptimModule):\n    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float = 1e-5, **kwargs):\n        \"\"\"Complex exponential positional embeddings for Hyena filters.\"\"\"\n        super().__init__()\n\n        self.seq_len = seq_len\n        # The time embedding fed to the filteres is normalized so that t_f = 1\n        t = torch.linspace(0, 1, self.seq_len)[None, :, None]  # 1, L, 1\n\n        if emb_dim > 1:\n            bands = (emb_dim - 1) // 2\n        # To compute the right embeddings we use the \"proper\" linspace\n        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None]\n        w = 2 * math.pi * t_rescaled / seq_len  # 1, L, 1\n\n        f = torch.linspace(1e-4, bands - 1, bands)[None, None]\n        z = torch.exp(-1j * f * w)\n        z = torch.cat([t, z.real, z.imag], dim=-1)\n        self.register(\"z\", z, lr=lr_pos_emb)\n        self.register(\"t\", t, lr=0.0)\n\n    def forward(self, L):\n        return self.z[:, :L], self.t[:, :L]\n\n\nclass ExponentialModulation(OptimModule):\n    def __init__(\n        self,\n        d_model,\n        fast_decay_pct=0.3,\n        slow_decay_pct=1.5,\n        target=1e-2,\n        modulation_lr=0.0,\n        shift: float = 0.0,\n        **kwargs,\n    ):\n        super().__init__()\n        self.shift = shift\n        max_decay = math.log(target) / fast_decay_pct\n        min_decay = math.log(target) / slow_decay_pct\n        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None]\n        self.register(\"deltas\", deltas, lr=modulation_lr)\n\n    def forward(self, t, x):\n        decay = torch.exp(-t * self.deltas.abs())\n        x = x * (decay + self.shift)\n        return x\n\n\nclass HyenaFilter(OptimModule):\n    def __init__(\n        self,\n        d_model,\n        emb_dim=3,  # dim of input to MLP, augments with positional encoding\n        order=16,  # width of the implicit MLP\n        seq_len=1024,\n        lr=1e-3,\n        lr_pos_emb=1e-5,\n        dropout=0.0,\n        w=1,  # frequency of periodic activations\n        w_mod=1, # non-learnable modification of w\n        wd=0,  # weight decay of kernel parameters\n        bias=True,\n        num_inner_mlps=2,\n        linear_mixer=False,\n        modulate: bool = True,\n        normalized=False,\n        bidirectional=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Implicit long filter with modulation.\n\n        Args:\n            d_model: number of channels in the input\n            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands\n            order: width of the FFN\n            num_inner_mlps: number of inner linear layers inside filter MLP\n\n        Note:\n            filter_dropout is not implemented\n        \"\"\"\n        super().__init__()\n        \n        self.d_model=d_model\n        self.emb_dim=emb_dim\n        self.seq_len=seq_len\n        self.modulate=modulate\n        self.use_bias = bias\n        self.bidirectional = bidirectional\n\n        self.bias = nn.Parameter(torch.randn(self.d_model))\n        self.dropout = nn.Dropout(dropout)\n\n        act = Sin(dim=order, w=w, w_mod=w_mod)\n        assert (\n            emb_dim % 2 != 0 and emb_dim >= 3\n        ), \"emb_dim must be odd and greater or equal to 3 (time, sine and cosine)\"\n        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)\n\n        # uses a variable number of inner linear layers\n        if linear_mixer is False:\n            self.implicit_filter = nn.Sequential(\n                nn.Linear(emb_dim, order),\n                act,\n            )\n            for i in range(num_inner_mlps):\n                self.implicit_filter.append(nn.Linear(order, order))\n                self.implicit_filter.append(act)\n            self.implicit_filter.append(nn.Linear(order, d_model, bias=False))\n        else:\n            self.implicit_filter = nn.Sequential(\n                nn.Linear(emb_dim, d_model, bias=False),\n            )\n\n        if self.bidirectional:\n            self.implicit_filter_rev = nn.Sequential(\n                nn.Linear(emb_dim, order),\n                act,\n            )\n            for i in range(num_inner_mlps):\n                self.implicit_filter_rev.append(nn.Linear(order, order))\n                self.implicit_filter_rev.append(act)\n            self.implicit_filter_rev.append(nn.Linear(order, d_model, bias=False))\n\n        self.modulation = ExponentialModulation(d_model, **kwargs)\n\n        self.normalized = normalized\n        for c in self.implicit_filter.children():\n            for name, v in c.state_dict().items():\n                optim = {\"weight_decay\": wd, \"lr\": lr}\n                setattr(getattr(c, name), \"_optim\", optim)\n        \n        self.flashfft = None\n\n    def filter(self, L, *args, **kwargs):\n        z, t = self.pos_emb(L)\n        h = self.implicit_filter(z)\n        if self.modulate:\n            h = self.modulation(t, h)\n        if self.normalized:\n            h = h / torch.norm(h, dim=-1, p=1, keepdim=True)\n        return h\n    \n    def filter_rev(self, L, *args, **kwargs):\n        z, t = self.pos_emb(L)\n        h = self.implicit_filter_rev(z)\n        if self.modulate:\n            h = self.modulation(t, h)\n        if self.normalized:\n            h = h / torch.norm(h, dim=-1, p=1, keepdim=True)\n        return h\n\n    def forward(self, x, L, k_fwd=None, k_rev=None, bias=None, *args, **kwargs):\n        if k_fwd is None:\n            k_fwd = self.filter(L)\n            if self.bidirectional and k_rev is None:\n                k_rev = self.filter_rev(L)\n\n        # Ensure compatibility with filters that return a tuple\n        k_fwd = k_fwd[0] if type(k_fwd) is tuple else k_fwd\n        if bias is None:\n            bias = self.bias\n        bias = bias if self.use_bias else 0 * bias\n\n        if self.bidirectional:\n            k_rev = k_rev[0] if type(k_rev) is tuple else k_rev\n            k = F.pad(k_fwd, (0, L)) \\\n                      + F.pad(k_rev.flip(-1), (L, 0))\n        else:\n            k = k_fwd\n\n        \n        y = fftconv_ref(\n            x, \n            k, \n            bias, \n            dropout_mask=None,\n            gelu=False,\n            flashfft=self.flashfft,\n        )\n\n        return y.to(dtype=x.dtype)\n\nclass MonarchMixerSequenceMixing(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        l_max=128,\n        dropout=0.0,\n        hyena_kernel_lr=None,\n        bidirectional=False,\n        hyena_lr_pos_emb=1e-5,\n        hyena_w=10,\n        hyena_w_mod=1,\n        hyena_wd=0.1,\n        hyena_emb_dim=3,\n        hyena_filter_dropout=0.0,\n        hyena_filter_order=16,\n        residual_long_conv=False,\n        hyena_training_additions=False,\n    ):\n        super().__init__()\n\n        self.d_model = d_model\n        self.l_max = l_max\n        self.kernel_lr = hyena_kernel_lr\n        self.channels = 1\n        self.bidirectional = bidirectional\n        self.residual_long_conv = residual_long_conv\n        self.NUM_PROJECTIONS = 3\n\n        print('-- Bidirectional:', self.bidirectional)\n        print(\"-- Using Long Conv Residual:\", self.residual_long_conv)\n        print('-- Hyena w:', hyena_w)\n        print('-- Hyena w mod:', hyena_w_mod)\n        print(f\"-- Hyena filter order: {hyena_filter_order}\")\n        print(f\"-- Hyena filter dropout: {hyena_filter_dropout}\")\n        print(f\"-- Hyena filter wd: {hyena_wd}\")\n        print(f\"-- Hyena filter emb dim: {hyena_emb_dim}\")\n        print(f\"-- Hyena filter lr: {hyena_kernel_lr}\")\n        print(f\"-- Hyena filter lr pos emb: {hyena_lr_pos_emb}\")\n\n        self.filter_fn = HyenaFilter(\n            self.d_model,\n            order=hyena_filter_order,\n            seq_len=self.l_max,\n            dropout=hyena_filter_dropout,\n            bidirectional=self.bidirectional,\n            lr=hyena_kernel_lr,\n            lr_pos_emb=hyena_lr_pos_emb,\n            w=hyena_w,  # frequency of periodic activations\n            w_mod=hyena_w_mod,\n            wd=hyena_wd,  # weight decay of kernel parameters\n            emb_dim=hyena_emb_dim,\n        )\n        \n        if self.residual_long_conv:\n            self.filter_fn2 = HyenaFilter(\n                self.d_model,\n                order=hyena_filter_order,\n                seq_len=self.l_max,\n                dropout=hyena_filter_dropout,\n                bidirectional=self.bidirectional,\n                lr=hyena_kernel_lr,\n                lr_pos_emb=hyena_lr_pos_emb,\n                w=hyena_w,  # frequency of periodic activations\n                w_mod=hyena_w_mod,\n                wd=hyena_wd,  # weight decay of kernel parameters\n                emb_dim=hyena_emb_dim,\n            )\n        \n        # setup projections\n        self.in_linear = nn.Linear(d_model, 3 * d_model)\n        self.out_linear = nn.Linear(d_model, d_model)\n        self.hyena_training_additions = hyena_training_additions\n        if self.hyena_training_additions:\n            self.act = nn.Identity()\n            self.drop = nn.Dropout(dropout)\n            self.layernorm = nn.LayerNorm(d_model)\n        \n        # setup short conv\n        total_width = self.d_model * self.NUM_PROJECTIONS\n        self.short_filter = nn.Conv1d(\n            in_channels=total_width,\n            out_channels=total_width,\n            kernel_size=3,\n            groups=total_width,\n            padding=2,\n        )\n\n\n    def forward(self, u, **kwargs):\n        # u is B L H\n        if self.hyena_training_additions:\n            u = self.layernorm(u)\n        L = u.size(-2)\n\n        # in projection\n        u_orig = u\n        u = self.in_linear(u)\n        u = rearrange(u, \"b l d -> b d l\")\n        \n        # short filter\n        uc = self.short_filter(u)[..., :L]\n\n        x1, x2, v = uc.split(self.d_model, dim=1)\n        \n        v = v * x1\n        if self.hyena_training_additions:\n            v = self.drop(v)\n\n        k = self.filter_fn.filter(L, device=u.device)\n        k = rearrange(k, \"c l d -> c d l\")[0] # `c` is always 1 by default\n\n        if self.bidirectional:\n            k_rev = self.filter_fn.filter_rev(L, device=u.device)\n            k_rev = rearrange(k_rev, \"c l d -> c d l\")[0] # `c` is always 1 by default\n        else:\n            k_rev = None\n\n        y = self.filter_fn(v, L, k_fwd=k, k_rev=k_rev, bias= self.filter_fn.bias[None, :, None])\n\n        if self.residual_long_conv:\n            k2 = self.filter_fn2.filter(L, device=u.device)\n            k2 = rearrange(k2, \"c l d -> c d l\")[0]\n\n            if self.bidirectional:\n                k2_rev = self.filter_fn2.filter_rev(L, device=u.device)\n                k2_rev = rearrange(k2_rev, \"c l d -> c d l\")[0] # `c` is always 1 by default\n            else:\n                k2_rev = None                \n\n            yu = self.filter_fn2(u_orig.transpose(-1, -2), L, k_fwd=k2, k_rev=k2_rev, bias= self.filter_fn2.bias[None, :, None])\n        \n        # post gating\n        y = y * x2\n\n        if self.residual_long_conv:\n            y = y + yu\n\n        y = y.transpose(-1, -2)\n        if self.hyena_training_additions:\n            y = self.drop(self.act(y))\n        y = self.out_linear(y)\n\n        return y, None\n\n ",
    "description": null,
    "url": null
}