{
    "acronym": "landmarkattn",
    "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
    "seed_ids": [
        "lex",
        "rmt",
        "flashattn",
        "memorizingtrans",
        "roformer",
        "memformer",
        "performer",
        "bigbird",
        "linformer",
        "reformer",
        "transformerxl",
        "gpt2"
    ],
    "s2id": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
    "abstract": "While Transformers have shown remarkable success in natural language processing, their attention mechanism's large memory requirements have limited their ability to handle longer contexts. Prior approaches, such as recurrent memory or retrieval-based augmentation, have either compromised the random-access flexibility of attention (i.e., the capability to select any token in the entire context) or relied on separate mechanisms for relevant context retrieval, which may not be compatible with the model's attention. In this paper, we present a novel approach that allows access to the complete context while retaining random-access flexibility, closely resembling running attention on the entire context. Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism. Our approach seamlessly integrates with specialized data structures and the system's memory hierarchy, enabling processing of arbitrarily long context lengths. We demonstrate that our method can obtain comparable performance with Transformer-XL while significantly reducing the number of retrieved tokens in each step. Finally, we show that fine-tuning LLaMA 7B with our method successfully extends its context length capacity to over 32k tokens, allowing for inference at the context lengths of GPT-4. We release the implementation of landmark attention and the code to reproduce our experiments at https://github.com/epfml/landmark-attention/.",
    "authors": [
        "Amirkeivan Mohtashami",
        "Martin Jaggi"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": "This paper uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism.",
    "citationCount": 86,
    "influentialCitationCount": 13,
    "code": "import math\nimport triton\nimport torch\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(#debug, sdz, sdh, sdm, sdn,\n    Q, K, V, sm_scale, \n    Out,\n    sqz, sqh, sqm, sqd, # shape = (Z,H,N_CTX_Q,D)\n    skz, skh, skn, skd, # shape = (Z,H,N_CTX_KV,D)\n    svz, svh, svn, svd, # shape = (Z,H,N_CTX_KV,D)\n    soz, soh, som, sod, # shape = (Z,H,N_CTX_Q,D)\n    L, M,\n    Z, H, N_CTX_Q, N_CTX_KV, \n    BLOCK: tl.constexpr, # will load BLOCK_M queries, and compute self attention by blocks of BLOCK_N keys\n    BLOCK_DMODEL: tl.constexpr, # dimensionality of heads: D\n    N_PREFIX_Q: tl.constexpr,\n):\n    \n    start_m = tl.program_id(0) # idx of sequence length chunk of size 128 (BLOCK_N)\n    off_hz = tl.program_id(1) # idx of head_batch (unique idx for each head in each batch)\n\n    BLOCK_M: tl.constexpr = BLOCK\n    BLOCK_N: tl.constexpr = BLOCK\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M) # indices of queries we want to process\n    offs_m_real = (start_m + N_PREFIX_Q) * BLOCK_M + tl.arange(0, BLOCK_M)  # indices of queries we want to process\n    offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)\n    offs_n = tl.arange(0, BLOCK_N) # indices of keys we want to process, we start from [0, BLOCK_N-1] and update in the loop\n    offs_d = tl.arange(0, BLOCK_DMODEL) # we want to process all the dimensions of a given head\n\n    offs_q = off_hz * sqh + offs_m[:, None] * sqm + offs_d[None, :] * sqd # Q.view(Z*H,N_CTX_Q,D)[off_hz, start_m*BLOCK_M:(start_m+1)*BLOCK_M, :].squeeze() that's a BLOCK_M*D matrix\n    offs_k = off_hz * skh + offs_n[None, :] * skn + offs_d[:, None] * skd # K.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].transpose(1,2).squeeze() that's a D*BLOCK_N matrix\n    offs_v = off_hz * svh + offs_n[:, None] * svn + offs_d[None, :] * svd # V.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].squeeze() that's a BLOCK_N*D matrix\n\n    # pointers to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    # Load values\n    q_vals = tl.load(Q + offs_q, mask=offs_m[:, None] < N_CTX_Q, other=0) \n    \n\n    for start_n in range(0, (N_PREFIX_Q + start_m)):\n        # Load values for K and K_idx\n        k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n\n        # compute qk\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n        qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n        qk *= sm_scale\n        # causal masking\n        qk = tl.where(offs_m_real[:,None] >= offs_n[None,:], qk, float(\"-inf\"))\n        landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, qk, float(\"-inf\")), 1)\n        normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, float(\"-inf\"), qk)\n        normal_m = tl.max(normal_qk, 1)\n        normal_p = tl.exp(normal_qk - normal_m[:, None])\n        normal_denom = tl.sum(normal_p, 1)\n\n        # compute attention weights\n        m_curr = tl.maximum(landmark_qk, m_prev) # compute new m\n        m_curr_ = m_curr # tl.where(m_curr != float('-inf'), m_curr, float(0.0))  # ADDITIONAL CHECK IF YOU GET NaNs\n        l_prev *= tl.exp(m_prev - m_curr_) # correct old l\n        landmark_p = tl.exp(landmark_qk - m_curr_)\n        l_curr = landmark_p + l_prev \n        l_rcp = 1. / l_curr # rescale operands of matmuls\n        # l_rcp = tl.where((l_rcp == float('inf')), 0, l_rcp)  # ADDITIONAL CHECK IF YOU GET NaNs\n        landmark_p *= l_rcp\n\n        acc *= (l_prev * l_rcp)[:, None] # weight for each value vector\n        # update acc\n        v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n        acc += tl.dot((landmark_p[:, None] * normal_p / normal_denom[:, None]).to(Q.dtype.element_ty), v_vals, allow_tf32=False) \n\n\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n\n        # update offsets\n        offs_n += BLOCK_N\n        offs_k += BLOCK_N * skn\n        offs_v += BLOCK_N * svn\n\n    k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n    # compute qk\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n    qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n    qk *= sm_scale\n    # causal masking\n    qk = tl.where(offs_m_real[:,None] >= offs_n[None,:], qk, float(\"-inf\"))\n\n    m_curr = tl.maximum(tl.max(qk, 1), m_prev) # compute new m\n    m_curr_ = m_curr#m_curr_ = tl.where(m_curr != float('-inf'), m_curr, float(0.0))\n\n    l_prev *= tl.exp(m_prev - m_curr_) # correct old l\n    p = tl.exp(qk - m_curr_[:, None])\n    l_curr = tl.sum(p, 1) + l_prev \n\n    l_rcp = 1. / l_curr # rescale operands of matmuls\n    # l_rcp = tl.where((l_rcp == float('inf')), 0, l_rcp)  # ADDITIONAL CHECK IF YOU GET NaNs\n    p *= l_rcp[:, None]\n    acc *= (l_prev * l_rcp)[:, None] # weight for each value vector\n    # update acc\n    p = p.to(Q.dtype.element_ty)\n    v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n    acc += tl.dot(p, v_vals, allow_tf32=False) \n\n    l_prev = l_curr\n    m_prev = m_curr\n \n    # store L and M\n    offs_L = off_hz * N_CTX_Q + offs_m # L is of shape (Z*H, N_CTX_Q), here we point to L[off_hz, start_m*Block_M:(start_m+1)*Block_M]\n    offs_M = off_hz * N_CTX_Q + offs_m\n    tl.store(L + offs_L, l_prev, mask=offs_m < N_CTX_Q)\n    tl.store(M + offs_M, m_prev, mask=offs_m < N_CTX_Q)\n    # store results to output\n    offs_o = off_hz * soh + offs_m[:, None] * som + offs_d[None, :] * sod\n    tl.store(Out + offs_o, acc, mask=offs_m[:, None] < N_CTX_Q)\n\n\n@triton.jit\ndef _bwd_preprocess(\n    Out, soz, soh, som, sod,\n    DO, L, slzh, slm,\n    NewDO, Delta, N_CTX_Q,\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_d = tl.arange(0, D_HEAD)\n    # load\n    off_o = off_hz * soh + off_m[:, None] * som + off_d[None, :] * sod\n    off_l = off_hz * slzh + off_m * slm\n    o = tl.load(Out + off_o).to(tl.float32)#, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    do = tl.load(DO + off_o).to(tl.float32)#, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    denom = tl.load(L + off_l).to(tl.float32)#, mask=off_m < N_CTX_Q, other=1.0).to(tl.float32)\n    # denom = tl.where(denom == 0, 1.0, denom)  # ADDITIONAL CHECK IF YOU GET NaNs\n    # compute\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    # write-back\n    tl.store(NewDO + off_o, do)#, mask=off_m[:, None] < N_CTX_Q)\n    tl.store(Delta + off_l, delta)#, mask=off_m < N_CTX_Q)\n\n\n@triton.jit\ndef _bwd_kernel(\n    Q, K, V, sm_scale, Out, DO,\n    DQ, DK, DV,\n    L, M,\n    D,\n    sqz, sqh, sqm, sqd,\n    skz, skh, skn, skd,\n    svz, svh, svn, svd,\n    Z, H, N_CTX_Q, N_CTX_KV,\n    BLOCK: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    N_PREFIX_Q: tl.constexpr,\n):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n\n    BLOCK_M: tl.constexpr = BLOCK\n    BLOCK_N: tl.constexpr = BLOCK\n\n    # offset pointers for batch/head\n    Q += off_z * sqz + off_h * sqh\n    K += off_z * skz + off_h * skh\n    V += off_z * svz + off_h * svh\n    DO += off_z * sqz + off_h * sqh\n    DQ += off_z * sqz + off_h * sqh\n    DK += off_z * skz + off_h * skh\n    DV += off_z * svz + off_h * svh\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    \n    # pointer to row-wise quantities in value-like data\n    D_ptrs = D + off_hz * N_CTX_Q # pointer to D.view(Z*H,N_CTX_Q)[off_hz]\n    m_ptrs = M + off_hz * N_CTX_Q # pointer to m.view(Z*H,N_CTX_Q)[off_hz]\n\n    for start_n in range(0, N_CTX_KV, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + tl.arange(0, BLOCK_N)\n        # pointers for keys and values\n        k_ptrs = K + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        v_ptrs = V + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n\n        # initialize dv amd dk\n        dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n\n        k = tl.load(k_ptrs)#, mask=offs_n[:, None] < N_CTX_KV)\n        v = tl.load(v_ptrs)#, mask=offs_n[:, None] < N_CTX_KV)\n\n        if start_n < N_PREFIX_Q * BLOCK_M:\n            start_q_index = 0\n        elif N_CTX_Q <= start_n - N_PREFIX_Q * BLOCK_M:\n            start_q_index = start_n - N_PREFIX_Q * BLOCK_M\n        else:\n            first_start_m = start_n - N_PREFIX_Q * BLOCK_M\n            first_start_m = tl.multiple_of(first_start_m, BLOCK_M)\n            offs_m = (first_start_m + tl.arange(0, BLOCK_M))\n            offs_m_real = offs_m + N_PREFIX_Q * BLOCK_M # indices of queries we want to process\n            offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)    \n\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            \n            q = tl.load(q_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk = tl.where(offs_m_real[:,None] >= (offs_n[None,:]), qk, float(\"-inf\"))\n\n            m = tl.load(m_ptrs + offs_m) #, mask=offs_m < N_CTX_Q)\n            m_ = m # tl.where(m != float('-inf'), m, 0.0)\n\n            last_p = tl.exp(qk * sm_scale - m_[:, None])\n\n            do = tl.load(do_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dv\n            dv += tl.dot(tl.trans(last_p.to(Q.dtype.element_ty)), do, allow_tf32=False)\n\n\n            Di = tl.load(D_ptrs + offs_m) #, mask=offs_m < N_CTX_Q)\n            # compute dp = dot(v, do)\n            last_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            last_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            # compute ds = p * (dp - delta[:, None])\n            ds = last_p * last_dp * sm_scale\n            \n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q, allow_tf32=False)\n\n            dq = tl.load(dq_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dq\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k, allow_tf32=False)\n            tl.store(dq_ptrs, dq) #, mask=offs_m[:, None] < N_CTX_Q)\n            start_q_index = first_start_m + BLOCK_M\n\n        for start_m in range(start_q_index, N_CTX_Q, BLOCK_M):\n            start_m = tl.multiple_of(start_m, BLOCK_M)\n            offs_m = (start_m + tl.arange(0, BLOCK_M))\n            # offs_m_real = offs_m + N_PREFIX_Q * BLOCK_M # indices of queries we want to process\n            # offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)    \n\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            \n            q = tl.load(q_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk *= sm_scale\n            # qk = tl.where(offs_m_real[:,None] >= (offs_n[None,:]), qk, float(\"-inf\"))  # This should not be necessary anymore since we separate the first step\n\n            landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, qk, float(\"-inf\")), 1)\n            normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, float(\"-inf\"), qk)\n\n            m = tl.load(m_ptrs + offs_m)#, mask=offs_m < N_CTX_Q)\n            m_ = m # tl.where(m != float('-inf'), m, 0.0)  # ADDITIONAL CHECK IF YOU GET NaNs\n\n            p = tl.exp(landmark_qk - m_) # BLOCK_M\n\n            do = tl.load(do_ptrs)#, mask=offs_m[:,None] < N_CTX_Q) # BLOCK_M x H\n\n            normal_m = tl.max(normal_qk, 1)\n            normal_p = tl.exp(normal_qk - normal_m[:, None])\n            normal_p_normalized = normal_p / tl.sum(normal_p, 1)[:, None] # BLOCK_M x (BLOCK_N - 1)\n            normal_kv = tl.dot(normal_p_normalized.to(Q.dtype.element_ty), v, allow_tf32=False) # BLOCK_M x H\n\n            normal_D = tl.sum(do * normal_kv, 1)\n\n            # compute dv\n            dv += tl.dot(tl.trans((p[:, None] * normal_p_normalized).to(Q.dtype.element_ty)), do, allow_tf32=False)\n\n            Di = tl.load(D_ptrs + offs_m)#, mask=offs_m < N_CTX_Q)\n            # compute dp and ds for landmark\n            dp = tl.zeros([BLOCK_M], dtype=tl.float32) - Di\n            dp += normal_D \n            landmark_ds = p * dp\n            # compute dp and ds for others\n            normal_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - normal_D[:, None]\n            normal_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            normal_ds = p[:, None] * normal_p_normalized * normal_dp \n            # merge\n            ds = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, landmark_ds[:, None], normal_ds)\n            ds *= sm_scale\n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q, allow_tf32=False)\n\n            dq = tl.load(dq_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dq\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k, allow_tf32=False)\n            tl.store(dq_ptrs, dq) #, mask=offs_m[:, None] < N_CTX_Q)\n         \n        # write-back\n        dv_ptrs = DV + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dk_ptrs = DK + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        tl.store(dv_ptrs, dv) #, mask=offs_n[:, None] < N_CTX_KV)\n        tl.store(dk_ptrs, dk) #, mask=offs_n[:, None] < N_CTX_KV)\n\n\nclass FusedLandmarkAttention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, n_prefix_q, sm_scale, block_size):\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n\n        # shape constraints\n        batch, nheads, seqlen_q, d = q.shape\n        _, _, seqlen_k, _ = k.shape\n        assert k.shape == (batch, nheads, seqlen_k, d)\n        assert v.shape == (batch, nheads, seqlen_k, d)\n        assert d <= 128, 'FlashAttention only support head dimensions up to 128'\n        assert q.dtype == k.dtype == v.dtype, 'All tensors must have the same type'\n        #assert q.dtype in [torch.float16, torch.bfloat16], 'Only support fp16 and bf16'\n        assert q.is_cuda and k.is_cuda and v.is_cuda\n        \n        BLOCK = block_size\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        num_warps = 4 if d <= 64 else 8\n\n        _fwd_kernel[grid](\n            q, k, v, sm_scale,\n            o,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n            L, m,\n            q.shape[0], q.shape[1], q.shape[2], k.shape[2],\n            BLOCK=BLOCK, BLOCK_DMODEL=d,\n            N_PREFIX_Q=n_prefix_q,\n            num_warps=num_warps, num_stages=2\n        )\n\n        ctx.save_for_backward(q, k, v, o, L, m)\n        ctx.grid = grid\n        ctx.sm_scale = sm_scale\n        ctx.BLOCK_DMODEL = d\n        ctx.N_PREFIX_Q = n_prefix_q\n        ctx.BLOCK = BLOCK\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        BLOCK = ctx.BLOCK\n        q, k, v, o, l, m = ctx.saved_tensors\n        assert q.shape[2] % BLOCK == 0, \"Backward supported only for full blocks\"\n        assert k.shape[2] % BLOCK == 0, \"Backward supported only for full blocks\"\n\n        do = do.contiguous()\n        dq = torch.zeros_like(q, dtype=torch.float32)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n        do_scaled = torch.empty_like(do)\n        delta = torch.empty_like(l)\n        _bwd_preprocess[(ctx.grid[0], ctx.grid[1])](\n            o, o.stride(0), o.stride(1), o.stride(2), o.stride(3), do, l, l.stride(0), l.stride(1),\n            do_scaled, delta, q.shape[2],\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n        )\n        _bwd_kernel[(ctx.grid[1],)](\n            q, k, v, ctx.sm_scale,\n            o, do_scaled,\n            dq, dk, dv,\n            l, m,\n            delta,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], k.shape[2],\n            BLOCK=BLOCK,\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, \n            N_PREFIX_Q=ctx.N_PREFIX_Q,\n            num_warps=8,\n            num_stages=1,\n        )\n        return dq, dk, dv, None, None, None\n\ndef fused_landmark_attention(q, k, v, is_mem, sm_scale=None, block_size=64):\n\n    expected_is_mem = torch.arange(0, is_mem.shape[-1], device=is_mem.device) % block_size == (block_size - 1)\n    assert (is_mem == expected_is_mem).all()\n\n    n_history_kv = k.shape[-2] - q.shape[-2]\n    assert n_history_kv % block_size == 0\n    n_history_blocks = n_history_kv // block_size\n\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(q.size(-1))\n\n    return FusedLandmarkAttention.apply(q, k, v, n_history_blocks, sm_scale, block_size)",
    "description": null,
    "url": null
}