{
    "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
    "acronym": "samba",
    "s2id": "28eb18717cfa257f0fc49fb9512c48279cafa031",
    "abstract": "Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.",
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "Samba is presented, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA), which selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism.",
    "citationCount": 5,
    "influentialCitationCount": 1,
    "seed_ids": [
        "based",
        "selfextend",
        "gla",
        "mamba",
        "retnet",
        "pi",
        "seqboat",
        "landmarkattn",
        "gqa",
        "mega",
        "s4d",
        "flashattn",
        "alibi",
        "longformer",
        "mqa",
        "transformer",
        "gpt2"
    ],
    "code": "# -*- coding: utf-8 -*-\n\nfrom __future__ import annotations\n\nimport math\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import ModelOutput, logging\n\nfrom fla.layers.attn import Attention\nfrom fla.models.mamba.modeling_mamba import MambaCache, MambaMixer\nfrom fla.models.samba.configuration_samba import SambaConfig\nfrom fla.modules import FusedCrossEntropyLoss, RMSNorm\nfrom fla.modules.activations import swiglu_linear\n\nlogger = logging.get_logger(__name__)\n\n\n\"\"\"\nclass Mamba(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        d_state=16,\n        d_conv=4,\n        expand=2,\n        dt_rank=\"auto\",\n        dt_min=0.001,\n        dt_max=0.1,\n        dt_init=\"random\",\n        dt_scale=1.0,\n        dt_init_floor=1e-4,\n        conv_bias=True,\n        bias=False,\n        use_fast_path=True,  # Fused kernel options\n        layer_idx=None,\n        device=None,\n        dtype=None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = int(self.expand * self.d_model)\n        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n        self.use_fast_path = use_fast_path\n        self.layer_idx = layer_idx\n\n        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n\n        self.conv1d = nn.Conv1d(\n            in_channels=self.d_inner,\n            out_channels=self.d_inner,\n            bias=conv_bias,\n            kernel_size=d_conv,\n            groups=self.d_inner,\n            padding=d_conv - 1,\n            **factory_kwargs,\n        )\n\n        self.activation = \"silu\"\n        self.act = nn.SiLU()\n\n        self.x_proj = nn.Linear(\n            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs\n        )\n        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n\n        # Initialize special dt projection to preserve variance at initialization\n        dt_init_std = self.dt_rank**-0.5 * dt_scale\n        if dt_init == \"constant\":\n            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n        elif dt_init == \"random\":\n            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n        else:\n            raise NotImplementedError\n\n        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n        dt = torch.exp(\n            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n            + math.log(dt_min)\n        ).clamp(min=dt_init_floor)\n        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        with torch.no_grad():\n            self.dt_proj.bias.copy_(inv_dt)\n        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n        self.dt_proj.bias._no_reinit = True\n\n        # S4D real initialization\n        A = repeat(\n            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n            \"n -> d n\",\n            d=self.d_inner,\n        ).contiguous()\n        A_log = torch.log(A)  # Keep A_log in fp32\n        self.A_log = nn.Parameter(A_log)\n        self.A_log._no_weight_decay = True\n\n        # D \"skip\" parameter\n        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32\n        self.D._no_weight_decay = True\n\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n\n    def forward(self, hidden_states, inference_params=None):\n        batch, seqlen, dim = hidden_states.shape\n\n        conv_state, ssm_state = None, None\n        if inference_params is not None:\n            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n            if inference_params.seqlen_offset > 0:\n                # The states are updated inplace\n                out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n                return out\n\n        # We do matmul and transpose BLH -> HBL at the same time\n        xz = rearrange(\n            self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n            \"d (b l) -> b d l\",\n            l=seqlen,\n        )\n        if self.in_proj.bias is not None:\n            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\")\n\n        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n        # In the backward pass we write dx and dz next to each other to avoid torch.cat\n        if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:  # Doesn't support outputting the states\n            out = mamba_inner_fn(\n                xz,\n                self.conv1d.weight,\n                self.conv1d.bias,\n                self.x_proj.weight,\n                self.dt_proj.weight,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                A,\n                None,  # input-dependent B\n                None,  # input-dependent C\n                self.D.float(),\n                delta_bias=self.dt_proj.bias.float(),\n                delta_softplus=True,\n            )\n        else:\n            x, z = xz.chunk(2, dim=1)\n            # Compute short convolution\n            if conv_state is not None:\n                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n                conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))  # Update state (B D W)\n            if causal_conv1d_fn is None:\n                x = self.act(self.conv1d(x)[..., :seqlen])\n            else:\n                assert self.activation in [\"silu\", \"swish\"]\n                x = causal_conv1d_fn(\n                    x=x,\n                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n                    bias=self.conv1d.bias,\n                    activation=self.activation,\n                )\n\n            # We're careful here about the layout, to avoid extra transposes.\n            # We want dt to have d as the slowest moving dimension\n            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n            x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))  # (bl d)\n            dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n            dt = self.dt_proj.weight @ dt.t()\n            dt = rearrange(dt, \"d (b l) -> b d l\", l=seqlen)\n            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n            assert self.activation in [\"silu\", \"swish\"]\n            y = selective_scan_fn(\n                x,\n                dt,\n                A,\n                B,\n                C,\n                self.D.float(),\n                z=z,\n                delta_bias=self.dt_proj.bias.float(),\n                delta_softplus=True,\n                return_last_state=ssm_state is not None,\n            )\n            if ssm_state is not None:\n                y, last_state = y\n                ssm_state.copy_(last_state)\n            y = rearrange(y, \"b d l -> b l d\")\n            out = self.out_proj(y)\n        return out\n\n    def step(self, hidden_states, conv_state, ssm_state):\n        dtype = hidden_states.dtype\n        assert hidden_states.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)\n        x, z = xz.chunk(2, dim=-1)  # (B D)\n\n        # Conv step\n        if causal_conv1d_update is None:\n            conv_state.copy_(torch.roll(conv_state, shifts=-1, dims=-1))  # Update state (B D W)\n            conv_state[:, :, -1] = x\n            x = torch.sum(conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n            if self.conv1d.bias is not None:\n                x = x + self.conv1d.bias\n            x = self.act(x).to(dtype=dtype)\n        else:\n            x = causal_conv1d_update(\n                x,\n                conv_state,\n                rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n                self.conv1d.bias,\n                self.activation,\n            )\n\n        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)\n        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n        # Don't add dt_bias here\n        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)\n        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n\n        # SSM step\n        if selective_state_update is None:\n            # Discretize A and B\n            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))\n            dA = torch.exp(torch.einsum(\"bd,dn->bdn\", dt, A))\n            dB = torch.einsum(\"bd,bn->bdn\", dt, B)\n            ssm_state.copy_(ssm_state * dA + rearrange(x, \"b d -> b d 1\") * dB)\n            y = torch.einsum(\"bdn,bn->bd\", ssm_state.to(dtype), C)\n            y = y + self.D.to(dtype) * x\n            y = y * self.act(z)  # (B D)\n        else:\n            y = selective_state_update(\n                ssm_state, x, dt, A, B, C, self.D, z=z, dt_bias=self.dt_proj.bias, dt_softplus=True\n            )\n\n        out = self.out_proj(y)\n        return out.unsqueeze(1), conv_state, ssm_state\n\n    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n        device = self.out_proj.weight.device\n        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype\n        conv_state = torch.zeros(\n            batch_size, self.d_model * self.expand, self.d_conv, device=device, dtype=conv_dtype\n        )\n        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype\n        # ssm_dtype = torch.float32\n        ssm_state = torch.zeros(\n            batch_size, self.d_model * self.expand, self.d_state, device=device, dtype=ssm_dtype\n        )\n        return conv_state, ssm_state\n\n    def _get_states_from_cache(self, inference_params, batch_size, initialize_states=False):\n        assert self.layer_idx is not None\n        if self.layer_idx not in inference_params.key_value_memory_dict:\n            batch_shape = (batch_size,)\n            conv_state = torch.zeros(\n                batch_size,\n                self.d_model * self.expand,\n                self.d_conv,\n                device=self.conv1d.weight.device,\n                dtype=self.conv1d.weight.dtype,\n            )\n            ssm_state = torch.zeros(\n                batch_size,\n                self.d_model * self.expand,\n                self.d_state,\n                device=self.dt_proj.weight.device,\n                dtype=self.dt_proj.weight.dtype,\n                # dtype=torch.float32,\n            )\n            inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)\n        else:\n            conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]\n            # TODO: What if batch size changes between generation, and we reuse the same states?\n            if initialize_states:\n                conv_state.zero_()\n                ssm_state.zero_()\n        return conv_state, ssm_state\n\n\"\"\"\n\n\n\nclass SambaMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        hidden_ratio: Optional[int] = None,\n        hidden_act: str = 'swish'\n    ) -> SambaMLP:\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        # the final number of params is `hidden_ratio * hidden_size^2`\n        # `intermediate_size` is chosen to be a multiple of 256 closest to `2/3 * hidden_size * hidden_ratio`\n        if hidden_ratio is None:\n            hidden_ratio = 4\n        self.hidden_ratio = hidden_ratio\n\n        self.intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        self.intermediate_size = 256 * ((self.intermediate_size + 256 - 1) // 256)\n\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        y = self.gate_proj(x)\n        gate, y = y.chunk(2, -1)\n        return swiglu_linear(gate, y, self.down_proj.weight, self.down_proj.bias)\n\n\nclass SambaBlock(nn.Module):\n    def __init__(self, config, layer_idx):\n        super().__init__()\n\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.layer_idx = layer_idx\n\n        self.mixer_norm = RMSNorm(hidden_size=config.hidden_size, eps=config.norm_eps)\n        if self.layer_idx % 2 == 0:\n            self.mixer = MambaMixer(config, layer_idx=layer_idx)\n        else:\n            self.mixer = Attention(\n                hidden_size=config.hidden_size,\n                num_heads=config.num_heads,\n                num_kv_heads=config.num_kv_heads,\n                window_size=config.window_size,\n                max_position_embeddings=config.max_position_embeddings,\n                layer_idx=layer_idx\n            )\n        self.mlp_norm = RMSNorm(hidden_size=config.hidden_size, eps=config.norm_eps)\n        self.mlp = SambaMLP(\n            hidden_size=config.hidden_size,\n            hidden_ratio=config.hidden_ratio,\n            hidden_act=config.hidden_act\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        cache_params: Optional[Tuple[torch.Tensor]] = None,\n        **kwargs,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\n        residual = hidden_states\n        hidden_states = self.mixer_norm(hidden_states)\n        if isinstance(self.mixer, MambaMixer):\n            hidden_states = self.mixer(hidden_states, cache_params=cache_params)\n        else:\n            hidden_states, _, cache_params = self.mixer(hidden_states=hidden_states, past_key_values=cache_params)\n        hidden_states, residual = self.mlp_norm(hidden_states, residual, True)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.backbone = SambaModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.backbone.get_input_embeddings()\n\n    def set_input_embeddings(self, new_embeddings):\n        return self.backbone.set_input_embeddings(new_embeddings)\n\n    def _update_model_kwargs_for_generation(\n        self, outputs: ModelOutput, model_kwargs: Dict[str, Any], **kwargs\n    ) -> Dict[str, Any]:\n        model_kwargs[\"cache_params\"] = outputs.get(\"cache_params\", None)\n        return model_kwargs\n\n    def prepare_inputs_for_generation(\n        self, input_ids, cache_params: Optional[MambaCache] = None, inputs_embeds=None, attention_mask=None, **kwargs\n    ):\n        # only last token for inputs_ids if the state is passed along.\n        if cache_params is not None:\n            input_ids = input_ids[:, -1].unsqueeze(-1)\n\n        if inputs_embeds is not None and cache_params is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs[\"cache_params\"] = cache_params\n        return model_inputs\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,  # noqa\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        cache_params: Optional[MambaCache] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        use_cache: Optional[bool] = None,\n        **kwargs,  # for now we need this for generation\n    ) -> Union[Tuple, SambaCausalLMOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        samba_outputs = self.backbone(\n            input_ids,\n            cache_params=cache_params,\n            inputs_embeds=inputs_embeds,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            use_cache=use_cache,\n        )\n        hidden_states = samba_outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            if self.config.fuse_cross_entropy:\n                loss_fct = FusedCrossEntropyLoss(inplace_backward=True)\n            else:\n                loss_fct = nn.CrossEntropyLoss()\n            # Enable model parallelism\n            labels = labels.to(logits.device)\n            labels = torch.cat((labels[..., 1:], torch.full_like(labels[:, :1], loss_fct.ignore_index)), 1)\n            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + samba_outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return SambaCausalLMOutput(\n            loss=loss,\n            logits=logits,\n            cache_params=samba_outputs.cache_params,\n            hidden_states=samba_outputs.hidden_states,\n        )"
}