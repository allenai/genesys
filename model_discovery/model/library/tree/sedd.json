{
    "acronym": "sedd",
    "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution ",
    "seed_ids": [
        "likelihooddifflm",
        "diffusionbert",
        "ssdlm",
        "classfreediffu",
        "diffusionlm",
        "flashattn",
        "d3pms",
        "gpt2"
    ],
    "s2id": "ce806f8d32f6fb1eaa821248a1bc4fa2cd949fbb",
    "abstract": "Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
    "authors": [
        "Aaron Lou",
        "Chenlin Meng",
        "Stefano Ermon"
    ],
    "venue": "",
    "year": 2023,
    "tldr": "This work proposes Score Entropy Discrete Diffusion models (SEDD), a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance.",
    "citationCount": 4,
    "influentialCitationCount": 3,
    "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\nfrom einops import rearrange\nfrom flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func\n# from flash_attn.ops.fused_dense import FusedMLP, FusedDense\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom omegaconf import OmegaConf\n\nfrom . import rotary\nfrom .fused_add_dropout_scale import (\n    bias_dropout_add_scale_fused_train, \n    bias_dropout_add_scale_fused_inference, \n    get_bias_dropout_add_scale, \n    modulate_fused,\n)\n\n\ndef modulate(x, shift, scale):\n    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n\n\n\n#################################################################################\n#                                  Layers                                       #\n#################################################################################\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones([dim]))\n        self.dim = dim\n    def forward(self, x):\n        with torch.cuda.amp.autocast(enabled=False):\n            x = F.layer_norm(x.float(), [self.dim])\n        return x * self.weight[None,None,:]\n\n\ndef residual_linear(x, W, x_skip, residual_scale):\n    \"\"\"x_skip + residual_scale * W @ x\"\"\"\n    dim_out, dim_in = W.shape[0], W.shape[1]\n    return torch.addmm(\n        x_skip.view(-1, dim_out),\n        x.view(-1, dim_in),\n        W.T,\n        alpha=residual_scale\n    ).view(*x.shape[:-1], dim_out)\n\n\n#################################################################################\n#               Embedding Layers for Timesteps and Class Labels                 #\n#################################################################################\n\nclass TimestepEmbedder(nn.Module):\n    \"\"\"\n    Embeds scalar timesteps into vector representations.\n    \"\"\"\n    def __init__(self, hidden_size, frequency_embedding_size=256, silu=True):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=t.device)\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n        return embedding\n\n    def forward(self, t):\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n\n\nclass LabelEmbedder(nn.Module):\n    \"\"\"\n    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n    \"\"\"\n    def __init__(self, num_classes, cond_size):\n        super().__init__()\n        self.embedding_table = nn.Embedding(num_classes + 1, cond_size)\n        self.num_classes = num_classes\n\n        # TODO think of initializing with 0.02 std deviation like in original DiT paper\n\n    def forward(self, labels):\n        embeddings = self.embedding_table(labels)\n        return embeddings\n    \n\n#################################################################################\n#                                 Core Model                                    #\n#################################################################################\n\n\nclass DDiTBlock(nn.Module):\n\n    def __init__(self, dim, n_heads, cond_dim, mlp_ratio=4, dropout=0.1):\n        super().__init__()\n        self.n_heads = n_heads\n\n        self.norm1 = LayerNorm(dim)\n        self.attn_qkv = nn.Linear(dim, 3 * dim, bias=False)\n        self.attn_out = nn.Linear(dim, dim, bias=False)\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.norm2 = LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, mlp_ratio * dim, bias=True),\n            nn.GELU(approximate=\"tanh\"),\n            nn.Linear(mlp_ratio * dim, dim, bias=True)\n        )\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.dropout = dropout\n        \n\n        self.adaLN_modulation = nn.Linear(cond_dim, 6 * dim, bias=True)\n        self.adaLN_modulation.weight.data.zero_()\n        self.adaLN_modulation.bias.data.zero_()\n\n\n    def _get_bias_dropout_scale(self):\n        return (\n            bias_dropout_add_scale_fused_train\n            if self.training\n            else bias_dropout_add_scale_fused_inference\n        )\n\n\n    def forward(self, x, rotary_cos_sin, c, seqlens=None):\n        batch_size, seq_len = x.shape[0], x.shape[1]\n\n        bias_dropout_scale_fn = self._get_bias_dropout_scale()\n\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c)[:, None].chunk(6, dim=2)\n\n        # attention operation\n        x_skip = x\n        x = modulate_fused(self.norm1(x), shift_msa, scale_msa)\n        # dtype0 = x.dtype\n\n        qkv = self.attn_qkv(x)\n        qkv = rearrange(qkv, 'b s (three h d) -> b s three h d', three=3, h=self.n_heads)\n        with torch.cuda.amp.autocast(enabled=False):\n            cos, sin = rotary_cos_sin\n            qkv = rotary.apply_rotary_pos_emb(\n                qkv, cos.to(qkv.dtype), sin.to(qkv.dtype)\n            )\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        if seqlens is None:\n            cu_seqlens = torch.arange(\n                0, (batch_size + 1) * seq_len, step=seq_len,\n                dtype=torch.int32, device=qkv.device\n            )\n        else:\n            cu_seqlens = seqlens.cumsum(-1)\n        x = flash_attn_varlen_qkvpacked_func(\n            qkv, cu_seqlens, seq_len, 0., causal=False)\n        \n        x = rearrange(x, '(b s) h d -> b s (h d)', b=batch_size)\n\n        x = bias_dropout_scale_fn(self.attn_out(x), None, gate_msa, x_skip, self.dropout)\n\n        # mlp operation\n        x = bias_dropout_scale_fn(self.mlp(modulate_fused(self.norm2(x), shift_mlp, scale_mlp)), None, gate_mlp, x, self.dropout)\n        return x\n\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, dim, vocab_dim):\n        \"\"\"\n        Mode arg: 0 -> use a learned layer, 1 -> use eigenvectors, \n        2-> add in eigenvectors, 3 -> use pretrained embedding matrix\n        \"\"\"\n        super().__init__()\n        self.embedding = nn.Parameter(torch.empty((vocab_dim, dim)))\n        torch.nn.init.kaiming_uniform_(self.embedding, a=math.sqrt(5))\n\n    def forward(self, x):\n        return self.embedding[x]\n\n\nclass DDitFinalLayer(nn.Module):\n    def __init__(self, hidden_size, out_channels, cond_dim):\n        super().__init__()\n        self.norm_final = LayerNorm(hidden_size)\n        self.linear = nn.Linear(hidden_size, out_channels)\n        self.linear.weight.data.zero_()\n        self.linear.bias.data.zero_()\n\n        self.adaLN_modulation = nn.Linear(cond_dim, 2 * hidden_size, bias=True)\n        self.adaLN_modulation.weight.data.zero_()\n        self.adaLN_modulation.bias.data.zero_()\n\n\n    def forward(self, x, c):\n        shift, scale = self.adaLN_modulation(c)[:, None].chunk(2, dim=2)\n        x = modulate_fused(self.norm_final(x), shift, scale)\n        x = self.linear(x)\n        return x\n\n\nclass SEDD(nn.Module, PyTorchModelHubMixin):\n    def __init__(self, config):\n        super().__init__()\n\n        # hack to make loading in configs easier\n        if type(config) == dict:\n            config = OmegaConf.create(config)\n\n        self.config = config\n\n        self.absorb = config.graph.type == \"absorb\"\n        vocab_size = config.tokens + (1 if self.absorb else 0)\n\n        self.vocab_embed = EmbeddingLayer(config.model.hidden_size, vocab_size)\n        self.sigma_map = TimestepEmbedder(config.model.cond_dim)\n        self.rotary_emb = rotary.Rotary(config.model.hidden_size // config.model.n_heads)\n\n        self.blocks = nn.ModuleList([\n            DDiTBlock(config.model.hidden_size, config.model.n_heads, config.model.cond_dim, dropout=config.model.dropout) for _ in range(config.model.n_blocks)\n        ])\n\n        self.output_layer = DDitFinalLayer(config.model.hidden_size, vocab_size, config.model.cond_dim)\n        self.scale_by_sigma = config.model.scale_by_sigma\n\n    \n    def _get_bias_dropout_scale(self):\n        return (\n            bias_dropout_add_scale_fused_train\n            if self.training\n            else bias_dropout_add_scale_fused_inference\n        )\n\n\n    def forward(self, indices, sigma):\n\n        x = self.vocab_embed(indices)\n        c = F.silu(self.sigma_map(sigma))\n\n        rotary_cos_sin = self.rotary_emb(x)\n\n        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n            for i in range(len(self.blocks)):\n                x = self.blocks[i](x, rotary_cos_sin, c, seqlens=None)\n\n            x = self.output_layer(x, c)\n\n\n        if self.scale_by_sigma:\n            assert self.absorb, \"Haven't configured this to work.\"\n            esigm1_log = torch.where(sigma < 0.5, torch.expm1(sigma), sigma.exp() - 1).log().to(x.dtype)[:, None, None]\n            x = x - esigm1_log - np.log(x.shape[-1] - 1)# this will be approximately averaged at 0\n            \n        x = torch.scatter(x, -1, indices[..., None], torch.zeros_like(x[..., :1]))\n\n        return x",
    "description": null,
    "url": null
}