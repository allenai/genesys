{
    "acronym": "longrope",
    "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
    "seed_ids": [
        "streamingllm",
        "yarn",
        "landmarkattn",
        "roformer",
        "compressivetransformer"
    ],
    "s2id": "c9603ec967879c24973b5bd48861df2e5555932e",
    "abstract": "Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.",
    "authors": [
        "Yiran Ding",
        "L. Zhang",
        "Chengruidong Zhang",
        "Yuanyuan Xu",
        "Ning Shang",
        "Jiahang Xu",
        "Fan Yang",
        "Mao Yang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window.",
    "citationCount": 36,
    "influentialCitationCount": 1,
    "code": "# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\nimport math\nimport torch\nimport transformers\n\n\nclass LongRoPEScaledRotaryEmbedding(torch.nn.Module):\n    \"\"\"\n    LongRoPE Scaled Rotary Positional Encoding class for Llama-like model.\n\n    Args:\n        dim (int): Head dimension.\n        rescale_factors (list): List of rescale factors for each dimension.\n        scale (float, optional): Length scale for code compatibility.\n        max_position_embeddings (int, optional): Maximum number of position embeddings (after scaled).\n        original_max_position_embeddings (int, optional): Original maximum number of position embeddings (before scaled).\n        base (int, optional): Base value for the positional encoding. Defaults to 10000.\n        magnitude_scaling_policy (str, optional): Attention temperature scaling function. Defaults to \"su\".\n        device (torch.device, optional): Device on which to create the embedding. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim, \n        rescale_factors,\n        scale=1.0,\n        max_position_embeddings=4096,\n        original_max_position_embeddings=4096,\n        base=10000,\n        magnitude_scaling_policy=\"su\",\n        model_type=\"llama\",\n        device=None,\n    ):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.original_max_position_embeddings = original_max_position_embeddings\n        self.base = base\n\n        if magnitude_scaling_policy == \"su\":\n            calc_mscale = self._calc_mscale_su\n        elif magnitude_scaling_policy == \"yarn\":\n            calc_mscale = self._calc_mscale_yarn\n        else:\n            calc_mscale = lambda scale: float(magnitude_scaling_policy)\n        self.mscale = calc_mscale(self.max_position_embeddings / self.original_max_position_embeddings)\n\n        self.rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32, device=device)\n        assert self.rescale_factors.shape == (self.dim // 2, ), \\\n            f\"misaligned shape for LongRoPE rescale factors: {self.rescale_factors.shape}\"\n\n        if model_type == \"llama\":\n            self.forward = self._forward_llama\n        elif model_type == \"mistral\":\n            self.forward = self._forward_mistral\n            self.register_buffer(\"inv_freq\", self._calc_inv_freq(max_position_embeddings, device))\n        else:\n            raise ValueError(f\"Unsupported model type for LongRoPE: {model_type}\")\n\n    def _calc_mscale_su(self, scale):\n        if scale <= 1.0:\n            return 1.0\n        return math.sqrt(1 + math.log(scale) / math.log(self.original_max_position_embeddings))\n\n    def _calc_mscale_yarn(self, scale):\n        if scale <= 1.0:\n            return 1.0\n        return 0.1 * math.log(scale) + 1.0\n\n    def _calc_inv_freq(self, seq_len, device):\n        rescale_factors = self.rescale_factors.to(device)\n        exponent = torch.arange(0, self.dim, 2, dtype=torch.float32, device=device) / self.dim\n        return 1.0 / (rescale_factors * (self.base ** exponent))\n\n    @torch.no_grad()\n    def _forward_mistral(self, x, seq_len=None):\n        seq_len = x.shape[-2] if seq_len is None else seq_len\n        t = torch.arange(seq_len, device=x.device, dtype=torch.float32)\n        inv_freq = self.inv_freq.to(x.device)\n        freqs = torch.outer(t, inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        return (emb.cos() * self.mscale).to(x.dtype), (emb.sin() * self.mscale).to(x.dtype)\n\n    @torch.no_grad()\n    def _forward_llama(self, x, position_ids, seq_len=None):\n        seq_len = x.shape[-2] if seq_len is None else seq_len\n        inv_freq = self._calc_inv_freq(seq_len, x.device)\n        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos() * self.mscale\n            sin = emb.sin() * self.mscale\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass DynamicLongRoPEScaledRotaryEmbedding(LongRoPEScaledRotaryEmbedding):\n\n    def _calc_inv_freq(self, seq_len, device):\n        rescale_factors = self.rescale_factors.to(device)\n        current_scale = seq_len / self.original_max_position_embeddings\n        original_scale = self.max_position_embeddings / self.original_max_position_embeddings\n        dynamic_scale = (current_scale - 1.0) / (original_scale - 1.0)\n        rescale_factors = 1.0 + (self.rescale_factors - 1.0) * dynamic_scale\n        exponent = torch.arange(0, self.dim, 2, dtype=torch.float32, device=device) / self.dim\n        return 1.0 / (rescale_factors * (self.base ** exponent))\n\n\nclass MixedLongRoPEScaledRotaryEmbedding(LongRoPEScaledRotaryEmbedding):\n\n    def __init__(\n        self,\n        dim, \n        rescale_factors,\n        start_token_idx,\n        original_embeddings,\n        scale=1.0,\n        max_position_embeddings=4096,\n        original_max_position_embeddings=4096,\n        base=10000,\n        magnitude_scaling_policy=\"su\",\n        model_type=\"llama\",\n        device=None,\n    ):\n        self.start_token_idx = start_token_idx\n        self.original_embeddings = (x.to(device) for x in original_embeddings)\n        super().__init__(\n            dim, rescale_factors, scale, max_position_embeddings, original_max_position_embeddings,\n            base, magnitude_scaling_policy, model_type, device,\n        )\n        self._longrope_forward = self.forward\n        self.forward = lambda *inputs: self._add_original_embeddings(self._longrope_forward(*inputs))\n\n    def _add_original_embeddings(self, emb_cos, emb_sin):\n        if self.start_token_idx > 0:\n            assert self.original_embeddings is not None, \\\n                'need input original embeddings for start token index > 0'\n            emb_cos_origin, emb_sin_origin = self.original_embeddings\n            assert emb_cos_origin.shape == emb_cos.shape and emb_sin_origin.shape == emb_cos.shape, \\\n                'original embeddings shape should be the same with current embeddings'\n            emb_cos[..., :self.start_token_idx, :] = emb_cos_origin[..., :self.start_token_idx, :]\n            emb_sin[..., :self.start_token_idx, :] = emb_sin_origin[..., :self.start_token_idx, :]\n        return emb_cos, emb_sin",
    "description": null,
    "url": null
}