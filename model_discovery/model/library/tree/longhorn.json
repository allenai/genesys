{
    "acronym": "longhorn",
    "title": "Longhorn: State Space Models are Amortized Online Learners",
    "seed_ids": [
        "ttt",
        "samba",
        "based",
        "gla",
        "mamba",
        "retnet",
        "rwkv4",
        "hyena",
        "ptwoattn",
        "sgconv",
        "s5",
        "gssm",
        "flashattn",
        "dssm",
        "s4",
        "deltanet",
        "performer",
        "lineartransformer",
        "linformer",
        "transformer"
    ],
    "s2id": "af4e8a4bf3effaf04a141e1048276c77ea8585a9",
    "abstract": "The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.",
    "authors": [
        "Bo Liu",
        "Rui Wang",
        "Lemeng Wu",
        "Yihao Feng",
        "Peter Stone",
        "Qian Liu"
    ],
    "venue": "",
    "year": 2024,
    "tldr": "This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.",
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": "import json\nimport math\nimport os\nfrom collections import namedtuple\nfrom dataclasses import dataclass, field\nfrom functools import partial\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom torch import Tensor\n\nfrom mamba_ssm.ops.selective_scan_interface import longhorn_inner_fn\nfrom mamba_ssm.utils.generation import GenerationMixin\nfrom mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept ImportError:\n    causal_conv1d_fn, causal_conv1d_update = None, None\n\ntry:\n    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn\nexcept ImportError:\n    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n\n\n@dataclass\nclass LonghornConfig:\n\n    d_model: int = 2560\n    d_intermediate: int = 0\n    n_layer: int = 64\n    vocab_size: int = 50277\n    ssm_cfg: dict = field(default_factory=dict)\n    attn_layer_idx: list = field(default_factory=list)\n    attn_cfg: dict = field(default_factory=dict)\n    rms_norm: bool = True\n    residual_in_fp32: bool = True\n    fused_add_norm: bool = True\n    pad_vocab_size_multiple: int = 8\n    tie_embeddings: bool = True\n\n\nclass Longhorn(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        d_state=16,\n        d_conv=4,\n        expand=2,\n        dt_rank=\"auto\",\n        dt_min=0.001,\n        dt_max=0.1,\n        dt_init=\"random\",\n        dt_scale=1.0,\n        dt_init_floor=1e-4,\n        conv_bias=True,\n        bias=False,\n        use_fast_path=False,  # Fused kernel options\n        layer_idx=None,\n        device=None,\n        dtype=None,\n    ):\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = int(self.expand * self.d_model)\n        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n        self.use_fast_path = use_fast_path\n        self.layer_idx = layer_idx\n\n        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n\n        self.conv1d = nn.Conv1d(\n            in_channels=self.d_inner,\n            out_channels=self.d_inner,\n            bias=conv_bias,\n            kernel_size=d_conv,\n            groups=self.d_inner,\n            padding=d_conv - 1,\n            **factory_kwargs,\n        )\n\n        self.activation = \"silu\"\n        self.act = nn.SiLU()\n\n        self.x_proj = nn.Linear(\n            self.d_inner, self.dt_rank + self.d_state*2, bias=False, **factory_kwargs\n        )\n\n        self.dt_head = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n\n        # D \"skip\" parameter\n        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32\n        self.D._no_weight_decay = True\n\n    def forward(self, hidden_states, inference_params=None):\n        \"\"\"\n        hidden_states: (B, L, D)\n        Returns: same shape as hidden_states\n        \"\"\"\n        batch, seqlen, dim = hidden_states.shape\n\n        conv_state, ssm_state = None, None\n        if inference_params is not None:\n            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n            if inference_params.seqlen_offset > 0:\n                # The states are updated inplace\n                out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n                return out\n\n        # We do matmul and transpose BLH -> HBL at the same time\n        xz = rearrange(\n            self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n            \"d (b l) -> b d l\",\n            l=seqlen,\n        )\n\n        y = longhorn_inner_fn(\n            xz,\n            self.conv1d.weight,\n            self.conv1d.bias,\n            self.x_proj.weight,\n            self.dt_head.weight,\n            None,\n            D=self.D,\n            delta_bias=self.dt_head.bias.float(),\n            out_proj_weight=self.out_proj.weight,\n        )\n        return y\n\n\nclass Block(nn.Module):\n    def __init__(\n        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False\n    ):\n        \"\"\"\n        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n\n        This Block has a slightly different structure compared to a regular\n        prenorm Transformer block.\n        The standard block is: LN -> MHA/MLP -> Add.\n        [Ref: https://arxiv.org/abs/2002.04745]\n        Here we have: Add -> LN -> Mixer, returning both\n        the hidden_states (output of the mixer) and the residual.\n        This is purely for performance reasons, as we can fuse add and LayerNorm.\n        The residual needs to be provided (except for the very first block).\n        \"\"\"\n        super().__init__()\n        self.residual_in_fp32 = residual_in_fp32\n        self.fused_add_norm = fused_add_norm\n        self.mixer = mixer_cls(dim)\n        self.norm = norm_cls(dim)\n        if self.fused_add_norm:\n            assert RMSNorm is not None, \"RMSNorm import fails\"\n            assert isinstance(\n                self.norm, (nn.LayerNorm, RMSNorm)\n            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n\n    def forward(\n        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None\n    ):\n        r\"\"\"Pass the input through the encoder layer.\n\n        Args:\n            hidden_states: the sequence to the encoder layer (required).\n            residual: hidden_states = Mixer(LN(residual))\n        \"\"\"\n        if not self.fused_add_norm:\n            residual = (hidden_states + residual) if residual is not None else hidden_states\n            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n            if self.residual_in_fp32:\n                residual = residual.to(torch.float32)\n        else:\n            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n            hidden_states, residual = fused_add_norm_fn(\n                hidden_states,\n                self.norm.weight,\n                self.norm.bias,\n                residual=residual,\n                prenorm=True,\n                residual_in_fp32=self.residual_in_fp32,\n                eps=self.norm.eps,\n            )\n        hidden_states = self.mixer(hidden_states, inference_params=inference_params)\n        return hidden_states, residual\n\n    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n\n",
    "description": null,
    "url": null
}