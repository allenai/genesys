{
    "acronym": "fastr2d2",
    "title": "Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation",
    "seed_ids": [
        "contrnn",
        "transformer",
        "bert"
    ],
    "s2id": "dc85180153687539724d20a5927b2fbdf5f8e2a4",
    "abstract": "Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n\u00b3) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.However, its rule-based pruning process suffers from local optima and slow inference. In this paper, we propose a unified R2D2 method that overcomes these issues. We use a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference. Our parser casts parsing as a split point scoring task by first scoring all split points for a given sentence and then using the highest-scoring one to recursively split a span into two parts. The reverse order of the splits is considered as the order of pruning in the encoder. We optimize the unsupervised parser by minimizing the Kullback\u2013Leibler distance between tree probabilities from the parser and the R2D2 model.Our experiments show that our Fast-R2D2 significantly improves the grammar induction quality and achieves competitive results in downstream tasks.",
    "authors": [
        "Xiang Hu",
        "Haitao Mi",
        "Liang Li",
        "Gerard de Melo"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": "This paper uses a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference and significantly improves the grammar induction quality and achieves competitive results in downstream tasks.",
    "citationCount": 10,
    "influentialCitationCount": 0,
    "code": "# coding=utf-8\n# Copyright (c) 2024 Ant Group\n# Author: Xiang Hu\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom utils.model_loader import load_model\nfrom datetime import datetime\nfrom model.modeling_outputs import R2D2GenOutput\nfrom concurrent.futures import ThreadPoolExecutor\n\n\nclass FastGenerativeR2D2(nn.Module):\n    def __init__(self, r2d2, action_layers, generation_layers, vocab_size, \n                 r2d2_input_dim, embedding_dim, dropout_rate=0.2, ext_vocab_size=0, \n                 fix_embeddings=False):\n        # embedding dim is used to feed to r2d2\n        # input dim is sued to feed to GPT\n        super().__init__()\n        self.embedding_dim = embedding_dim  # embedding_dim > r2d2_input_dim\n        self.r2d2_input_dim = r2d2_input_dim\n        self.r2d2 = r2d2\n\n        self.vocab_size = vocab_size\n\n        # self.action_ln = nn.Linear(self.embedding_dim, 2)  # judge reduce or predict next token\n\n        self.enable_gpt = False\n        if action_layers is not None and generation_layers is not None:\n            self.action_layers = action_layers\n            self.generation_layers = generation_layers\n            self.bos_embedding = nn.Parameter(torch.rand(self.embedding_dim))\n            self.up_scale = nn.Linear(self.r2d2_input_dim, self.embedding_dim)\n            self.dense = nn.Sequential(nn.Linear(self.embedding_dim, 4 * self.embedding_dim),\n                                        nn.GELU(),\n                                        nn.Dropout(dropout_rate),\n                                        nn.Linear(4 * self.embedding_dim, self.embedding_dim))\n            self.action_mlp = nn.Sequential(nn.LayerNorm(self.embedding_dim),\n                                nn.Linear(self.embedding_dim, self.embedding_dim),\n                                nn.GELU(),\n                                nn.Dropout(dropout_rate),\n                                nn.Linear(self.embedding_dim, 2))\n            self.enable_gpt = True\n        \n        self.classifier = nn.Linear(self.embedding_dim, vocab_size, bias=False)\n        self.embeddings = nn.Embedding(vocab_size, self.embedding_dim)\n        self.embeddings.requires_grad = not fix_embeddings\n        self.down_scale = nn.Linear(self.embedding_dim, self.r2d2_input_dim)\n\n        self.insideoutside_dense = nn.Sequential(\n            nn.Linear(r2d2_input_dim, 4 * r2d2_input_dim),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(4 * r2d2_input_dim, self.embedding_dim)\n        )\n\n        # self.parallel_stream = torch.cuda.Stream()\n\n        self._init_weights()\n        self._tie_weights()\n\n    def _init_weights(self):\n        if self.enable_gpt:\n            self.bos_embedding.data.normal_(mean=0, std=0.02)\n        self.embeddings.weight.data.normal_(mean=0, std=0.02)\n\n    def _tie_weights(self):\n        self.classifier.weight = self.embeddings.weight\n\n    def get_parser(self):\n        return self.r2d2.parser\n        \n    def from_pretrain(self, model_path, strict=True):\n        load_model(self, model_path, strict=strict)\n        self._tie_weights()\n\n    def _append_eos_label(self, eos_labels, chunk_input_ids, chunk_masks, next_token_indices, max_input_len):\n        chunk_masks = (chunk_masks.sum(dim=1) > 0).to(int)\n        seq_lens = chunk_masks.sum(dim=1)  # (N)\n        temp_ids = torch.zeros((chunk_input_ids.shape[0], chunk_input_ids.shape[1] + 1), dtype=chunk_input_ids.dtype, device=chunk_input_ids.device)\n        temp_ids.fill_(-100)\n        temp_ids[:, :-1] = chunk_input_ids\n        # comment this line to support discriminant way\n        temp_ids.scatter_(1, seq_lens.unsqueeze(1), torch.tensor(eos_labels, device=chunk_input_ids.device).unsqueeze(1))\n        chunk_input_ids = temp_ids\n        next_token_indices = next_token_indices[:, :max_input_len + 1]\n        return next_token_indices, chunk_input_ids\n\n    def forward(self, chunk_input_ids= None, chunk_masks=None, input_ids=None, masks=None, eos_labels=None, group_ids=None, \n                atom_spans=None, span_ids=None, external_vocab_ids=None, \n                coeff=1.0, temperature=1.0, past_key_values=None):\n        batch_size = max(group_ids) + 1\n        r2d2_input_ids = torch.where(chunk_input_ids == -100, 0, chunk_input_ids)\n        input_embeddings = self.embeddings(r2d2_input_ids)\n        r2d2_embeddings = self.down_scale(input_embeddings)\n        # max_input_len = chunk_input_ids.shape[1]\n        max_input_len = (chunk_masks != 0).sum(dim=1).max().to('cpu', non_blocking=True)\n        \n        ctx, outside_tgt, ldr_repr, position_ids, tgt_ids, token_indices, ext_ids, split_targets, l_height = \\\n            self.r2d2(r2d2_input_ids, chunk_masks, input_ids, masks, r2d2_embeddings, group_ids, \n                      max_input_len, atom_spans=atom_spans, coeff=coeff, temperature=temperature, span_ids=span_ids,\n                      eos_labels=eos_labels, external_vocab_ids=external_vocab_ids)\n\n\n        if self.training:\n            # with torch.cuda.stream(self.parallel_stream):\n            parser_loss = self.r2d2.parser_loss(ctx)\n            outside_embeddings = self.r2d2.outside_embeddings(ctx)\n            outside_logits = self.classifier(self.insideoutside_dense(outside_embeddings))\n            insideoutside_loss = F.cross_entropy(outside_logits, outside_tgt)\n        else:\n            parser_loss = insideoutside_loss = 0\n        \n        logits = action_logits = None\n        gpt_loss = action_loss = 0\n        past_kv = None\n        hidden_states = None\n\n        if self.enable_gpt:\n            if past_key_values is not None:\n                action_past_kv, gen_past_kv = past_key_values\n            else:\n                action_past_kv = gen_past_kv = None\n            gpt_input = self.up_scale(ldr_repr)\n            gpt_input.scatter_(1, token_indices.unsqueeze(2).repeat(1, 1, input_embeddings.shape[-1]), \n                                input_embeddings.to(gpt_input.dtype))\n            \n            # ext_embedding = self.ext_embeds(ext_ids)\n            # gpt_input = gpt_input + ext_embedding\n            bos_emb = self.bos_embedding.unsqueeze(0).repeat(batch_size, 1)\n            # position ids already considered <bos>\n            cat_input = torch.cat([bos_emb.unsqueeze(1), gpt_input], dim=1)  # (group_size, L + 1, dim)\n            # cat_input = self.layer_norm(cat_input)\n            # cat_input = self.norm(cat_input)\n            outputs = self.action_layers(inputs_embeds=cat_input, position_ids=position_ids, past_key_values=action_past_kv)  # (B, L, dim)\n            action_logits = self.action_mlp(outputs.last_hidden_state)  # (B, L, 2)\n            action_tgt = torch.where(tgt_ids == self.r2d2.reduce_id, 1, 0)  # REDUCE: 1, SHIFT:0\n            action_tgt = torch.where(tgt_ids != -1, action_tgt, -1)\n            # print(action_tgt)\n\n            next_token_indices = (tgt_ids != self.r2d2.reduce_id).int().argsort(dim=-1, descending=True, stable=True)  # (B, L)\n            if eos_labels is None:\n                truncated_len = max_input_len if self.training else max_input_len + 1\n                next_token_indices = next_token_indices[:, :truncated_len]\n            else:\n                next_token_indices, chunk_input_ids = self._append_eos_label(eos_labels, chunk_input_ids, chunk_masks, next_token_indices, max_input_len)\n            token_outputs = outputs.last_hidden_state.gather(1, next_token_indices.unsqueeze(2).repeat(1, 1, self.embedding_dim))\n            # token_pos_ids = position_ids.gather(1, next_token_indices)\n            # gather outputs to predict the next token\n            token_outputs = self.generation_layers(inputs_embeds=token_outputs, past_key_values=gen_past_kv)\n\n            hidden_states = token_outputs.last_hidden_state\n            logits = self.classifier(self.dense(hidden_states))  # (group_size, L + 1, vocab)\n            # predict token loss + action loss\n            # print(\"chunk_input_ids: \", chunk_input_ids)\n            if self.training:\n                gpt_loss = F.cross_entropy(logits.permute(0, 2, 1), chunk_input_ids, ignore_index=-100)\n                action_loss = F.cross_entropy(action_logits.permute(0, 2, 1), action_tgt, ignore_index=-1)\n            past_kv = (outputs.past_key_values, token_outputs.past_key_values)\n\n        # torch.cuda.synchronize()\n        # return loss + lm_loss + parser_loss, split_targets\n        return R2D2GenOutput(struct_loss=insideoutside_loss + l_height, \n                             non_struct_loss=0.5 * gpt_loss + action_loss + parser_loss,\n                             logits=logits,\n                             action_logits=action_logits,\n                             hidden_states=hidden_states, \n                             tgt_ids=chunk_input_ids, \n                             gpt_loss=gpt_loss,\n                             action_loss=action_loss,\n                             inside_outside_loss=insideoutside_loss,\n                             parser_loss=parser_loss,\n                             past_kv=past_kv,\n                             splits=split_targets)\n\n\nclass FastGenerativeR2D2_discriminant_glue(FastGenerativeR2D2):\n    \n    def _append_eos_label(self, eos_labels, chunk_input_ids, chunk_masks, next_token_indices, max_input_len):\n        chunk_masks = (chunk_masks.sum(dim=1) > 0).to(int)\n        seq_lens = chunk_masks.sum(dim=1)  # (N)\n        temp_ids = torch.zeros((chunk_input_ids.shape[0], chunk_input_ids.shape[1] + 1), dtype=chunk_input_ids.dtype, device=chunk_input_ids.device)\n        temp_ids.fill_(-100)\n        temp_ids[:, :-1] = chunk_input_ids\n        # temp_ids.scatter_(1, seq_lens.unsqueeze(1), torch.tensor(eos_labels, device=chunk_input_ids.device).unsqueeze(1))\n        chunk_input_ids = temp_ids\n        next_token_indices = next_token_indices[:, :max_input_len + 1]\n        return next_token_indices, chunk_input_ids",
    "description": null,
    "url": null
}