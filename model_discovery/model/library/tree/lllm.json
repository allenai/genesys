{
    "acronym": "lllm",
    "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models",
    "seed_ids": [
        "transnormer",
        "flash",
        "soft",
        "lstransformer",
        "nystromformer",
        "performer",
        "lineartransformer",
        "compressivetransformer",
        "transformer",
        "gpt2"
    ],
    "s2id": "b72a8884a7a2f93d60d44930dd77d0af85dd32b8",
    "abstract": "Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.",
    "authors": [
        "Haoran You",
        "Yichao Fu",
        "Zheng Wang",
        "Amir Yazdanbakhsh",
        "Y. Lin"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.",
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": "import math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\n\nfrom einops import rearrange\nfrom rotary_embedding_torch import RotaryEmbedding\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef padding_to_multiple_of(n, mult):\n    remainder = n % mult\n    if remainder == 0:\n        return 0\n    return mult - remainder\n\n# scalenorm\n\nclass ScaleNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5):\n        super().__init__()\n        self.scale = dim ** -0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1))\n\n    def forward(self, x):\n        norm = torch.norm(x, dim = -1, keepdim = True) * self.scale\n        return x / norm.clamp(min = self.eps) * self.g\n\n# absolute positional encodings\n\nclass ScaledSinuEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(1,))\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def forward(self, x):\n        n, device = x.shape[1], x.device\n        t = torch.arange(n, device = device).type_as(self.inv_freq)\n        sinu = einsum('i , j -> i j', t, self.inv_freq)\n        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n        return emb * self.scale\n\n# T5 relative positional bias\n\nclass T5RelativePositionBias(nn.Module):\n    def __init__(\n        self,\n        scale,\n        causal = False,\n        num_buckets = 32,\n        max_distance = 128\n    ):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, 1)\n\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position,\n        causal = True,\n        num_buckets = 32,\n        max_distance = 128\n    ):\n        ret = 0\n        n = -relative_position\n        if not causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n        ).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def forward(self, x):\n        i, j, device = *x.shape[-2:], x.device\n        q_pos = torch.arange(i, dtype = torch.long, device = device)\n        k_pos = torch.arange(j, dtype = torch.long, device = device)\n        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n        rp_bucket = self._relative_position_bucket(rel_pos, causal = self.causal, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        bias = rearrange(values, 'i j 1 -> i j')\n        return bias * self.scale\n\n# class\n\nclass OffsetScale(nn.Module):\n    def __init__(self, dim, heads = 1):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(heads, dim))\n        self.beta = nn.Parameter(torch.zeros(heads, dim))\n        nn.init.normal_(self.gamma, std = 0.02)\n\n    def forward(self, x):\n        out = einsum('... d, h d -> ... h d', x, self.gamma) + self.beta\n        return out.unbind(dim = -2)\n\n# activation functions\n\nclass ReLUSquared(nn.Module):\n    def forward(self, x):\n        return F.relu(x) ** 2\n\nclass LaplacianAttnFn(nn.Module):\n    \"\"\" https://arxiv.org/abs/2209.10655 claims this is more stable than Relu squared \"\"\"\n\n    def forward(self, x):\n        mu = math.sqrt(0.5)\n        std = math.sqrt((4 * math.pi) ** -1)\n        return (1 + torch.special.erf((x - mu) / (std * math.sqrt(2)))) * 0.5\n\n# FLASH\n\nclass FLASH(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        group_size = 256,\n        query_key_dim = 128,\n        expansion_factor = 2.,\n        causal = False,\n        dropout = 0.,\n        rotary_pos_emb = None,\n        norm_klass = nn.LayerNorm,\n        shift_tokens = False,\n        laplace_attn_fn = False,\n        reduce_group_non_causal_attn = True,\n        global_ = True,\n        local_ = True,\n        conv = False,\n        kernel = 63,\n        square=False,\n        castling = False,\n        softmax = False,\n        quad = False,\n    ):\n        super().__init__()\n        hidden_dim = int(dim * expansion_factor)\n        self.group_size = group_size\n        self.causal = causal\n        self.shift_tokens = shift_tokens\n\n        self.attn_fn = ReLUSquared() if not laplace_attn_fn else LaplacianAttnFn()\n\n        # positional embeddings\n\n        self.rotary_pos_emb = rotary_pos_emb\n        self.rel_pos_bias = T5RelativePositionBias(query_key_dim ** 0.5, causal = causal)\n\n        # norm\n\n        self.norm = norm_klass(dim)\n        self.dropout = nn.Dropout(dropout)\n\n        # whether to reduce groups in non causal linear attention\n\n        self.reduce_group_non_causal_attn = reduce_group_non_causal_attn\n\n        # projections\n\n        self.to_hidden = nn.Sequential(\n            nn.Linear(dim, hidden_dim * 2),\n            nn.SiLU()\n        )\n\n        self.to_qk = nn.Sequential(\n            nn.Linear(dim, query_key_dim),\n            nn.SiLU()\n        )\n        if softmax or square:\n            self.qk_offset_scale = OffsetScale(query_key_dim, heads = 2)\n        else:\n            self.qk_offset_scale = OffsetScale(query_key_dim, heads = 4)\n        self.to_out = nn.Linear(hidden_dim, dim)\n\n        self.global_ = global_\n        self.local_ = local_ \n        self.square = square\n\n        self.conv = conv\n        if conv:\n            res_kernel_size = kernel\n            self.dwconv = torch.nn.Conv2d(\n            in_channels=1,\n            out_channels=1,\n            kernel_size=(res_kernel_size, 1),\n            padding=(res_kernel_size // 2, 0),\n            bias=False,\n            groups=1,\n            )\n            self.conv_mask = torch.ones((1, 1, res_kernel_size, 1), dtype=torch.float32)\n            self.conv_mask[:, :, res_kernel_size // 2 + 1 :, :] = 0.0\n        self.castling = castling\n        self.softmax = softmax\n        self.quad = quad\n    def forward(\n        self,\n        x,\n        *,\n        mask = None\n    ):\n        \"\"\"\n        b - batch\n        n - sequence length (within groups)\n        g - group dimension\n        d - feature dimension (keys)\n        e - feature dimension (values)\n        i - sequence dimension (source)\n        j - sequence dimension (target)\n        \"\"\"\n\n        b, n, device, g = x.shape[0], x.shape[-2], x.device, self.group_size\n\n        # prenorm\n\n        normed_x = self.norm(x)\n\n        # do token shift - a great, costless trick from an independent AI researcher in Shenzhen\n\n        if self.shift_tokens:\n            x_shift, x_pass = normed_x.chunk(2, dim = -1)\n            x_shift = F.pad(x_shift, (0, 0, 1, -1), value = 0.)\n            normed_x = torch.cat((x_shift, x_pass), dim = -1)\n\n        # initial projections\n\n        v, gate = self.to_hidden(normed_x).chunk(2, dim = -1)\n        qk = self.to_qk(normed_x)\n\n        # offset and scale\n        v0 = v\n        if self.square:\n\n            q, k = self.qk_offset_scale(qk)\n\n            if exists(mask):\n                l_mask = rearrange(mask, '... -> ... 1')\n                k = k.masked_fill(~l_mask, 0.)\n            if exists(self.rotary_pos_emb):\n                q, k = map(self.rotary_pos_emb.rotate_queries_or_keys, (q, k))\n\n            qsize = q.size(1)\n            padding = padding_to_multiple_of(n, qsize)\n\n            if padding > 0:\n                q, k, v = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (q, k, v))\n\n                mask = default(mask, torch.ones((b, n), device = device, dtype = torch.bool))\n                mask = F.pad(mask, (0, padding), value = False)\n\n            sim = einsum('... i d, ... j d -> ... i j', q, k) / qsize\n\n            sim = sim + self.rel_pos_bias(sim)\n\n            attn = self.attn_fn(sim / qsize)\n            attn = self.dropout(attn)\n            \n            if self.causal:\n                causal_mask = torch.ones((qsize, qsize), dtype = torch.bool, device = device).triu(1)\n                attn = attn.masked_fill(causal_mask, 0)\n\n\n            if exists(mask):\n                attn = attn.masked_fill(~mask[:,None,:], 0.)\n\n            softmax_out = einsum('... i j, ... j d -> ... i d', attn, v)\n\n            out = gate * (softmax_out)\n\n            # projection out and residual\n\n            return self.to_out(out) + x\n\n        if self.softmax:\n            q, k = self.qk_offset_scale(qk)\n\n            if exists(mask):\n                l_mask = rearrange(mask, '... -> ... 1')\n                k = k.masked_fill(~l_mask, 0.)\n            if exists(self.rotary_pos_emb):\n                q, k = map(self.rotary_pos_emb.rotate_queries_or_keys, (q, k))\n\n            qsize = q.size(1)\n            padding = padding_to_multiple_of(n, qsize)\n\n            if padding > 0:\n                q, k, v = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (q, k, v))\n\n                mask = default(mask, torch.ones((b, n), device = device, dtype = torch.bool))\n                mask = F.pad(mask, (0, padding), value = False)\n\n\n            sim = einsum('... i d, ... j d -> ... i j', q, k) / qsize\n\n            sim = sim + self.rel_pos_bias(sim)\n            if self.causal:\n                causal_mask = torch.ones((qsize, qsize), dtype = torch.bool, device = device).triu(1)\n                sim = sim.masked_fill(causal_mask, torch.finfo(sim.dtype).min)\n\n            attn = torch.nn.functional.softmax(sim, dim=-1)\n\n            attn = self.dropout(attn)\n\n            if exists(mask):\n                attn = attn.masked_fill(~mask[:,None,:], 0.)\n\n\n\n            softmax_out = einsum('... i j, ... j d -> ... i d', attn, v)\n\n            out = gate * (softmax_out)\n\n\n            return self.to_out(out) + x\n\n        else:\n            quad_q, lin_q, quad_k, lin_k = self.qk_offset_scale(qk)\n        # mask out linear attention keys\n\n        if exists(mask):\n            lin_mask = rearrange(mask, '... -> ... 1')\n            lin_k = lin_k.masked_fill(~lin_mask, 0.)\n\n        # rotate queries and keys\n\n        if exists(self.rotary_pos_emb):\n            quad_q, lin_q, quad_k, lin_k = map(self.rotary_pos_emb.rotate_queries_or_keys, (quad_q, lin_q, quad_k, lin_k))\n\n        # padding for groups\n\n        padding = padding_to_multiple_of(n, g)\n\n        if padding > 0:\n            quad_q, quad_k, lin_q, lin_k, v = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (quad_q, quad_k, lin_q, lin_k, v))\n\n            mask = default(mask, torch.ones((b, n), device = device, dtype = torch.bool))\n            mask = F.pad(mask, (0, padding), value = False)\n\n        # group along sequence\n\n        quad_q, quad_k, lin_q, lin_k, v = map(lambda t: rearrange(t, 'b (n g) d -> b n g d', g = self.group_size), (quad_q, quad_k, lin_q, lin_k, v))\n\n        if exists(mask):\n            mask = rearrange(mask, 'b (g j) -> b g 1 j', j = g)\n\n        # calculate quadratic attention output\n\n        sim = einsum('... i d, ... j d -> ... i j', quad_q, quad_k) / g\n\n        sim = sim + self.rel_pos_bias(sim)\n\n        attn = self.attn_fn(sim)\n        attn = self.dropout(attn)\n\n        if exists(mask):\n            attn = attn.masked_fill(~mask, 0.)\n\n        if self.causal:\n            causal_mask = torch.ones((g, g), dtype = torch.bool, device = device).triu(1)\n            attn = attn.masked_fill(causal_mask, 0.)\n\n        quad_out = einsum('... i j, ... j d -> ... i d', attn, v)\n\n        # calculate linear attention output\n\n        if self.causal:\n            lin_kv = einsum('b g n d, b g n e -> b g d e', lin_k, v) / g\n\n            # exclusive cumulative sum along group dimension\n\n            lin_kv = lin_kv.cumsum(dim = 1)\n            lin_kv = F.pad(lin_kv, (0, 0, 0, 0, 1, -1), value = 0.)\n\n            lin_out = einsum('b g d e, b g n d -> b g n e', lin_kv, lin_q)\n        else:\n            context_einsum_eq = 'b d e' if self.reduce_group_non_causal_attn else 'b g d e'\n            lin_kv = einsum(f'b g n d, b g n e -> {context_einsum_eq}', lin_k, v) / n\n            lin_out = einsum(f'b g n d, {context_einsum_eq} -> b g n e', lin_q, lin_kv)\n\n        # fold back groups into full sequence, and excise out padding\n\n        quad_attn_out, lin_attn_out = map(lambda t: rearrange(t, 'b g n d -> b (g n) d')[:, :n], (quad_out, lin_out))\n\n        # gate\n        out = None \n        if self.global_:\n            out = lin_attn_out\n        if self.local_:\n            out = quad_attn_out if out is None else quad_attn_out + lin_attn_out\n        \n        if self.conv:\n            if self.conv_mask.device != out.device:\n                self.conv_mask = self.conv_mask.to(out.device)\n            self.dwconv.weight.data *= self.conv_mask\n            conv_out = self.dwconv(v0.unsqueeze(1)) \n            if self.castling:\n                out = 0.5 * v0 + 1.0 / math.pi * out \n                out = out / out.norm(dim=-1, keepdim=True)\n                out += conv_out.squeeze(1)\n            else:\n                out = out + conv_out.squeeze(1)\n\n        out = gate * (out)\n\n        # projection out and residual\n\n        return self.to_out(out) + x\n",
    "description": null,
    "url": null
}