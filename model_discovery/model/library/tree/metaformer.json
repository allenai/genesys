{
    "acronym": "metaformer",
    "title": "MetaFormer Is Actually What You Need for Vision",
    "seed_ids": [
        "gpt3"
    ],
    "s2id": "57150ca7d793d6f784cf82da1c349edf7beb6bc2",
    "abstract": "Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1 % top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 49%/61% fewer MACs. The effectiveness of Pool-Former verifies our hypothesis and urges us to initiate the concept of \u201cMetaFormer\u201d, a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design.",
    "authors": [
        "Weihao Yu",
        "Mi Luo",
        "Pan Zhou",
        "Chenyang Si",
        "Yichen Zhou",
        "Xinchao Wang",
        "Jiashi Feng",
        "Shuicheng Yan"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "tldr": "It is argued that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks, and calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules.",
    "citationCount": 608,
    "influentialCitationCount": 90,
    "code": "# Copyright 2021 Garena Online Private Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMetaFormer implementation with hybrid stages\n\"\"\"\nfrom typing import Sequence\nfrom functools import partial, reduce\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom timm.models.registry import register_model\n\n\nfrom .poolformer import PatchEmbed, LayerNormChannel, GroupNorm, Mlp\n\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .95, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, \n        'classifier': 'head',\n        **kwargs\n    }\n\nclass AddPositionEmb(nn.Module):\n    \"\"\"Module to add position embedding to input features\n    \"\"\"\n    def __init__(\n        self, dim=384, spatial_shape=[14, 14],\n        ):\n        super().__init__()\n        if isinstance(spatial_shape, int):\n            spatial_shape = [spatial_shape]\n        assert isinstance(spatial_shape, Sequence), \\\n            f'\"spatial_shape\" must by a sequence or int, ' \\\n            f'get {type(spatial_shape)} instead.'\n        if len(spatial_shape) == 1:\n            embed_shape = list(spatial_shape) + [dim]\n        else:\n            embed_shape = [dim] + list(spatial_shape)\n        self.pos_embed = nn.Parameter(torch.zeros(1, *embed_shape))\n    def forward(self, x):\n        return x+self.pos_embed\n\n\nclass Pooling(nn.Module):\n    \"\"\"\n    Implementation of pooling for PoolFormer\n    --pool_size: pooling size\n    \"\"\"\n    def __init__(self, pool_size=3, **kwargs):\n        super().__init__()\n        self.pool = nn.AvgPool2d(\n            pool_size, stride=1, padding=pool_size//2, count_include_pad=False)\n\n    def forward(self, x):\n        return self.pool(x) - x\n\n\nclass Attention(nn.Module):\n    \"\"\"Attention module that can take tensor with [B, N, C] or [B, C, H, W] as input.\n    Modified from: \n    https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    \"\"\"\n    def __init__(self, dim, head_dim=32, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        assert dim % head_dim == 0, 'dim should be divisible by head_dim'\n        self.head_dim = head_dim\n        self.num_heads = dim // head_dim\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        shape = x.shape\n        if len(shape) == 4:\n            B, C, H, W = shape\n            N = H * W\n            x = torch.flatten(x, start_dim=2).transpose(-2, -1) # (B, N, C)\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n\n        # trick here to make q@k.t more stable\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        # attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        if len(shape) == 4:\n            x = x.transpose(-2, -1).reshape(B, C, H, W)\n\n        return x\n\n\nclass SpatialFc(nn.Module):\n    \"\"\"SpatialFc module that take features with shape of (B,C,*) as input.\n    \"\"\"\n    def __init__(\n        self, spatial_shape=[14, 14], **kwargs, \n        ):\n        super().__init__()\n        if isinstance(spatial_shape, int):\n            spatial_shape = [spatial_shape]\n        assert isinstance(spatial_shape, Sequence), \\\n            f'\"spatial_shape\" must by a sequence or int, ' \\\n            f'get {type(spatial_shape)} instead.'\n        N = reduce(lambda x, y: x * y, spatial_shape)\n        self.fc = nn.Linear(N, N, bias=False)\n\n    def forward(self, x):\n        # input shape like [B, C, H, W]\n        shape = x.shape\n        x = torch.flatten(x, start_dim=2) # [B, C, H*W]\n        x = self.fc(x) # [B, C, H*W]\n        x = x.reshape(*shape) # [B, C, H, W]\n        return x\n\n\nclass MetaFormerBlock(nn.Module):\n    \"\"\"\n    Implementation of one MetaFormer block.\n    --dim: embedding dim\n    --token_mixer: token mixer module\n    --mlp_ratio: mlp expansion ratio\n    --act_layer: activation\n    --norm_layer: normalization\n    --drop: dropout rate\n    --drop path: Stochastic Depth, \n        refer to https://arxiv.org/abs/1603.09382\n    --use_layer_scale, --layer_scale_init_value: LayerScale, \n        refer to https://arxiv.org/abs/2103.17239\n    \"\"\"\n    def __init__(self, dim, \n                 token_mixer=nn.Identity, \n                 mlp_ratio=4., \n                 act_layer=nn.GELU, norm_layer=LayerNormChannel, \n                 drop=0., drop_path=0., \n                 use_layer_scale=True, layer_scale_init_value=1e-5):\n\n        super().__init__()\n\n        self.norm1 = norm_layer(dim)\n        self.token_mixer = token_mixer(dim=dim)\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, \n                       act_layer=act_layer, drop=drop)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. \\\n            else nn.Identity()\n        self.use_layer_scale = use_layer_scale\n        if use_layer_scale:\n            self.layer_scale_1 = nn.Parameter(\n                layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n            self.layer_scale_2 = nn.Parameter(\n                layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n\n    def forward(self, x):\n        if self.use_layer_scale:\n            x = x + self.drop_path(\n                self.layer_scale_1.unsqueeze(-1).unsqueeze(-1)\n                * self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(\n                self.layer_scale_2.unsqueeze(-1).unsqueeze(-1)\n                * self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\ndef basic_blocks(dim, index, layers, token_mixer=nn.Identity, \n                 mlp_ratio=4., \n                 act_layer=nn.GELU, norm_layer=LayerNormChannel, \n                 drop_rate=.0, drop_path_rate=0., \n                 use_layer_scale=True, layer_scale_init_value=1e-5):\n    \"\"\"\n    generate PoolFormer blocks for a stage\n    return: PoolFormer blocks \n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * (\n            block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(MetaFormerBlock(\n            dim, token_mixer=token_mixer, mlp_ratio=mlp_ratio, \n            act_layer=act_layer, norm_layer=norm_layer, \n            drop=drop_rate, drop_path=block_dpr, \n            use_layer_scale=use_layer_scale, \n            layer_scale_init_value=layer_scale_init_value, \n            ))\n    blocks = nn.Sequential(*blocks)\n\n    return blocks\n\n\nclass MetaFormer(nn.Module):\n    \"\"\"\n    MetaFormer, the main class of our model\n    --layers: [x,x,x,x], number of blocks for the 4 stages\n    --embed_dims, --mlp_ratios: the embedding dims and mlp ratios for the 4 stages\n    --token_mixers: token mixers of different stages\n    --norm_layer, --act_layer: define the types of normalization and activation\n    --num_classes: number of classes for the image classification\n    --in_patch_size, --in_stride, --in_pad: specify the patch embedding\n        for the input image\n    --down_patch_size --down_stride --down_pad: \n        specify the downsample (patch embed.)\n    --add_pos_embs: position embedding modules of different stages\n    \"\"\"\n    def __init__(self, layers, embed_dims=None, \n                 token_mixers=None, mlp_ratios=None, \n                 norm_layer=LayerNormChannel, act_layer=nn.GELU, \n                 num_classes=1000,\n                 in_patch_size=7, in_stride=4, in_pad=2, \n                 downsamples=None, down_patch_size=3, down_stride=2, down_pad=1, \n                 add_pos_embs=None, \n                 drop_rate=0., drop_path_rate=0.,\n                 use_layer_scale=True, layer_scale_init_value=1e-5, \n                 **kwargs):\n\n        super().__init__()\n\n\n        self.num_classes = num_classes\n\n        self.patch_embed = PatchEmbed(\n            patch_size=in_patch_size, stride=in_stride, padding=in_pad, \n            in_chans=3, embed_dim=embed_dims[0])\n        if add_pos_embs is None:\n            add_pos_embs = [None] * len(layers)\n        if token_mixers is None:\n            token_mixers = [nn.Identity] * len(layers)\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            if add_pos_embs[i] is not None:\n                network.append(add_pos_embs[i](embed_dims[i]))\n            stage = basic_blocks(embed_dims[i], i, layers, \n                                 token_mixer=token_mixers[i], mlp_ratio=mlp_ratios[i],\n                                 act_layer=act_layer, norm_layer=norm_layer, \n                                 drop_rate=drop_rate, \n                                 drop_path_rate=drop_path_rate,\n                                 use_layer_scale=use_layer_scale, \n                                 layer_scale_init_value=layer_scale_init_value)\n            network.append(stage)\n            if i >= len(layers) - 1:\n                break\n            if downsamples[i] or embed_dims[i] != embed_dims[i+1]:\n                # downsampling between two stages\n                network.append(\n                    PatchEmbed(\n                        patch_size=down_patch_size, stride=down_stride, \n                        padding=down_pad, \n                        in_chans=embed_dims[i], embed_dim=embed_dims[i+1]\n                        )\n                    )\n\n        self.network = nn.ModuleList(network)\n        self.norm = norm_layer(embed_dims[-1])\n        self.head = nn.Linear(\n            embed_dims[-1], num_classes) if num_classes > 0 \\\n            else nn.Identity()\n\n        self.apply(self.cls_init_weights)\n\n    # init for classification\n    def cls_init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes):\n        self.num_classes = num_classes\n        self.head = nn.Linear(\n            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_embeddings(self, x):\n        x = self.patch_embed(x)\n        return x\n\n    def forward_tokens(self, x):\n        for idx, block in enumerate(self.network):\n            x = block(x)\n        return x\n\n    def forward(self, x):\n        # input embedding\n        x = self.forward_embeddings(x)\n        # through backbone\n        x = self.forward_tokens(x)\n        x = self.norm(x)\n        # for image classification\n        cls_out = self.head(x.mean([-2, -1]))\n        return cls_out\n\nmodel_urls = {\n    \"metaformer_id_s12\": \"https://github.com/sail-sg/poolformer/releases/download/v1.0/metaformer_id_s12.pth.tar\",\n    \"metaformer_pppa_s12_224\": \"https://github.com/sail-sg/poolformer/releases/download/v1.0/metaformer_pppa_s12_224.pth.tar\",\n    \"metaformer_ppaa_s12_224\": \"https://github.com/sail-sg/poolformer/releases/download/v1.0/metaformer_ppaa_s12_224.pth.tar\",\n    \"metaformer_pppf_s12_224\": \"https://github.com/sail-sg/poolformer/releases/download/v1.0/metaformer_pppf_s12_224.pth.tar\",\n    \"metaformer_ppff_s12_224\": \"https://github.com/sail-sg/poolformer/releases/download/v1.0/metaformer_ppff_s12_224.pth.tar\",\n}\n\n\n@register_model\ndef metaformer_id_s12(pretrained=False, **kwargs):\n    layers = [2, 2, 6, 2]\n    embed_dims = [64, 128, 320, 512]\n    token_mixers = [nn.Identity] * len(layers)\n    mlp_ratios = [4, 4, 4, 4]\n    downsamples = [True, True, True, True]\n    model = MetaFormer(\n        layers, embed_dims=embed_dims,\n        token_mixers=token_mixers,\n        mlp_ratios=mlp_ratios,\n        norm_layer=GroupNorm,\n        downsamples=downsamples,\n        **kwargs)\n    model.default_cfg = _cfg(crop_pct=0.9)\n    if pretrained:\n        url = model_urls['metaformer_id_s12']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint)\n    return model\n\n\n@register_model\ndef metaformer_pppa_s12_224(pretrained=False, **kwargs):\n    layers = [2, 2, 6, 2]\n    embed_dims = [64, 128, 320, 512]\n    add_pos_embs = [None, None, None,\n        partial(AddPositionEmb, spatial_shape=[7, 7])]\n    token_mixers = [Pooling, Pooling, Pooling, Attention]\n    mlp_ratios = [4, 4, 4, 4]\n    downsamples = [True, True, True, True]\n    model = MetaFormer(\n        layers, embed_dims=embed_dims,\n        token_mixers=token_mixers,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        add_pos_embs=add_pos_embs,\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        url = model_urls['metaformer_pppa_s12_224']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint)\n    return model\n\n\n@register_model\ndef metaformer_ppaa_s12_224(pretrained=False, **kwargs):\n    layers = [2, 2, 6, 2]\n    embed_dims = [64, 128, 320, 512]\n    add_pos_embs = [None, None, \n        partial(AddPositionEmb, spatial_shape=[14, 14]), None]\n    token_mixers = [Pooling, Pooling, Attention, Attention]\n    mlp_ratios = [4, 4, 4, 4]\n    downsamples = [True, True, True, True]\n    model = MetaFormer(\n        layers, embed_dims=embed_dims,\n        token_mixers=token_mixers,\n        mlp_ratios=mlp_ratios,\n        downsamples=downsamples,\n        add_pos_embs=add_pos_embs,\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        url = model_urls['metaformer_ppaa_s12_224']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint)\n    return model\n\n\n@register_model\ndef metaformer_pppf_s12_224(pretrained=False, **kwargs):\n    layers = [2, 2, 6, 2]\n    embed_dims = [64, 128, 320, 512]\n    token_mixers = [Pooling, Pooling, Pooling,\n        partial(SpatialFc, spatial_shape=[7, 7]),\n        ]\n    mlp_ratios = [4, 4, 4, 4]\n    downsamples = [True, True, True, True]\n    model = MetaFormer(\n        layers, embed_dims=embed_dims,\n        token_mixers=token_mixers,\n        mlp_ratios=mlp_ratios,\n        norm_layer=GroupNorm,\n        downsamples=downsamples,\n        **kwargs)\n    model.default_cfg = _cfg(crop_pct=0.9)\n    if pretrained:\n        url = model_urls['metaformer_pppf_s12_224']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint)\n    return model\n\n\n@register_model\ndef metaformer_ppff_s12_224(pretrained=False, **kwargs):\n    layers = [2, 2, 6, 2]\n    embed_dims = [64, 128, 320, 512]\n    token_mixers = [Pooling, Pooling, \n        partial(SpatialFc, spatial_shape=[14, 14]), \n        partial(SpatialFc, spatial_shape=[7, 7]),\n        ]\n    mlp_ratios = [4, 4, 4, 4]\n    downsamples = [True, True, True, True]\n    model = MetaFormer(\n        layers, embed_dims=embed_dims,\n        token_mixers=token_mixers,\n        mlp_ratios=mlp_ratios,\n        norm_layer=GroupNorm,\n        downsamples=downsamples,\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        url = model_urls['metaformer_ppff_s12_224']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model.load_state_dict(checkpoint)\n    return model\n\n\n\n\n",
    "description": null,
    "url": null
}