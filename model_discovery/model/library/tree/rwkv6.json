{
    "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
    "acronym": "rwkv6",
    "s2id": "157ed5647da39a7f5d33a84a90414b2a9e97e301",
    "abstract": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer",
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.",
    "citationCount": 16,
    "influentialCitationCount": 1,
    "seed_ids": [
        "griffin",
        "hgrn",
        "rwkv4",
        "aft",
        "nystromformer",
        "performer",
        "bigbird",
        "linformer",
        "longformer",
        "sinkhorn",
        "reformer",
        "compressivetransformer",
        "sparsetransformer"
    ],
    "code": "# gab.py\n\nimport torch\nimport torch.nn as nn\n\nfrom model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n\nfrom einops import rearrange\n\nfrom transformers.activations import ACT2FN\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\n# YOU CAN IMPORT MORE MODULES HERE #\n\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n\n\ndef naive_recurrent_rwkv6(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    u: torch.Tensor,\n    scale: Optional[float] = None,\n):\n    orig_dtype = q.dtype\n    B, H, T, K, V = *q.shape, v.shape[-1]\n    q, k, v, w, u = map(lambda x: x.float(), (q, k, v, w, u))\n    h = torch.zeros(B, H, K, V, dtype=torch.float32, device=q.device)\n    o = torch.zeros_like(v)\n\n    if scale is None:\n        scale = K ** -0.5\n\n    for i in range(T):\n        q_i = q[:, :, i, :] * scale\n        k_i = k[:, :, i]\n        v_i = v[:, :, i, :]\n        w_i = w[:, :, i].exp()\n        kv_i = k_i[..., None] * v_i[..., None, :]\n        o_i = (h + u[None, ..., None] * kv_i) * q_i[..., None]\n        o[:, :, i] = o_i.sum(-2)\n        h = h * w_i[..., None] + kv_i\n    return o.to(orig_dtype)\n\nclass RWKV6Attention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int = 1024,\n        num_heads: int = 4,\n        gate_fn: str = 'swish',\n        proj_low_rank_dim: int = 32,\n        gate_low_rank_dim: int = 64,\n        elementwise_affine: Optional[bool] = True,\n        norm_eps: float = 1e-5,\n        device=None,\n        dtype=None,\n        **kwargs\n    ):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.proj_low_rank_dim = proj_low_rank_dim\n        self.gate_low_rank_dim = gate_low_rank_dim\n\n        self.key_dim = hidden_size // 2\n        self.value_dim = hidden_size\n\n        assert self.key_dim % num_heads == 0, f\"key dim must be divisible by num_heads of {num_heads}\"\n        assert self.value_dim % num_heads == 0, f\"value dim must be divisible by num_heads of {num_heads}\"\n\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.x_proj = nn.Sequential(\n            LerpLinear(hidden_size, proj_low_rank_dim * 5, device=device, dtype=dtype),\n            nn.Tanh(),\n            nn.Linear(proj_low_rank_dim * 5, hidden_size, bias=False, device=device, dtype=dtype)\n        )\n        self.x_bias = nn.Parameter(torch.zeros(5, hidden_size, device=device, dtype=dtype))\n\n        self.r_proj = DDLerpLinear(hidden_size, self.key_dim, device=device, dtype=dtype)\n        self.w_proj = DDLerpLinear(hidden_size, self.key_dim, low_rank_dim=gate_low_rank_dim, device=device, dtype=dtype)\n        self.k_proj = DDLerpLinear(hidden_size, self.key_dim, device=device, dtype=dtype)\n        self.v_proj = DDLerpLinear(hidden_size, self.value_dim, device=device, dtype=dtype)\n        self.g_proj = DDLerpLinear(hidden_size, self.value_dim, low_rank_dim=gate_low_rank_dim, device=device, dtype=dtype)\n        self.bonus = nn.Parameter(torch.zeros(num_heads, self.head_qk_dim, device=device, dtype=dtype))\n\n        # self.g_norm = nn.GroupNorm(self.num_heads, self.value_dim, affine=elementwise_affine, eps=norm_eps, device=device, dtype=dtype) # buggy now\n        self.g_norm = nn.LayerNorm(self.value_dim, elementwise_affine=elementwise_affine, eps=norm_eps, device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False, device=device, dtype=dtype)\n        self.gate_fn = ACT2FN[gate_fn]\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        if isinstance(module, nn.Parameter):\n            nn.init.xavier_uniform_(module, gain=2 ** -2.5)\n        module._is_hf_initialized = True\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        **kwargs\n    ) -> Tuple[torch.Tensor]:\n        batch_size, seq_len, hidden_size = hidden_states.shape\n        # launching the triton kernel for just one token will actually be slower\n        last_state = None\n\n        if hidden_states.shape[1] == 1 and last_state is not None:\n            shifted = last_state[0].unsqueeze(1)\n        else:\n            shifted = self.time_shift(hidden_states)\n            if last_state is not None:\n                shifted[:, 0] = last_state[0]\n\n        delta = shifted - hidden_states\n        x = self.x_proj[0](hidden_states, delta).view(batch_size, seq_len, -1, self.proj_low_rank_dim)\n        x = torch.einsum('b l n r, h n r-> b l n h', self.x_proj[1](x), self.x_proj[2].weight.view(hidden_size, 5, -1))\n\n        r, w, k, v, g = x.add_(self.x_bias).unbind(-2)\n        r = self.r_proj(hidden_states, r, delta)\n        w = self.w_proj(hidden_states, w, delta)\n        k = self.k_proj(hidden_states, k, delta)\n        v = self.v_proj(hidden_states, v, delta)\n        g = self.g_proj(hidden_states, g, delta)\n\n        r, w, k, v = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=self.num_heads), (r, w, k, v))\n        w = -torch.exp(w)\n        u = self.bonus\n\n        o = naive_recurrent_rwkv6(r, k, v, w, u, scale=1.0)\n\n        o = rearrange(o, 'b h l d -> b l (h d)')\n        o = self.g_norm(o)\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n\n        # o = o[:, :_seqlen]\n        return o\n\n    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n        param = next(self.parameters())\n        state = [param.new_zeros(batch_size, self.hidden_size),\n                 param.new_zeros(batch_size, self.num_heads, self.head_qk_dim, self.head_v_dim)]\n        return state\n\n    def state_size(self, **kwargs) -> int:\n        state_size = self.key_dim * self.head_v_dim\n        return state_size\n\n\nclass LoRA(nn.Module):\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        low_rank_dim: int,\n        bias: Optional[bool] = True,\n        device=None,\n        dtype=None,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.bias = bias\n\n        self.lora = nn.Sequential(\n            nn.Linear(input_dim, low_rank_dim, bias=False, device=device, dtype=dtype),\n            nn.Tanh(),\n            nn.Linear(low_rank_dim, output_dim, bias=bias, device=device, dtype=dtype)\n        )\n\n    def __repr__(self) -> str:\n        s = f\"{self.__class__.__name__}(\"\n        s += f\"input_dim={self.input_dim}, low_rank_dim={self.low_rank_dim}, output_dim={self.output_dim}\"\n        if not self.bias:\n            s += f\", bias={self.bias}\"\n        s += \")\"\n        return s\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.lora(x)\n\n\nclass LerpLinear(nn.Module):\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        low_rank_dim: Optional[int] = None,\n        device=None,\n        dtype=None,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if low_rank_dim is None:\n            self.linear = nn.Linear(input_dim, output_dim, bias=False, device=device, dtype=dtype)\n        else:\n            self.linear = LoRA(input_dim, output_dim, low_rank_dim, device=device, dtype=dtype)\n        self.mu = nn.Parameter(torch.zeros(input_dim, device=device, dtype=dtype))\n\n    def __repr__(self) -> str:\n        s = f\"{self.__class__.__name__}({self.input_dim}, {self.output_dim}\"\n        if self.low_rank_dim is not None:\n            s += f\", low_rank_dim={self.low_rank_dim}\"\n        s += \")\"\n        return s\n\n    def forward(self, x: torch.Tensor, delta: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if delta is None:\n            shifted = self.time_shift(x)\n            if len(shifted.shape) == 2:\n                shifted = shifted.unsqueeze(1)\n            delta = shifted - x\n        return self.linear(x + delta * self.mu)\n\n\nclass DDLerpLinear(nn.Module):\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        low_rank_dim: Optional[int] = None,\n        device=None,\n        dtype=None,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if low_rank_dim is None:\n            self.linear = nn.Linear(input_dim, output_dim, bias=False, device=device, dtype=dtype)\n        else:\n            self.linear = LoRA(input_dim, output_dim, low_rank_dim, device=device, dtype=dtype)\n\n    def __repr__(self) -> str:\n        s = f\"{self.__class__.__name__}({self.input_dim}, {self.output_dim}\"\n        if self.low_rank_dim is not None:\n            s += f\", low_rank_dim={self.low_rank_dim}\"\n        s += \")\"\n        return s\n\n    def forward(self, x: torch.Tensor, mu: torch.Tensor, delta: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if delta is None:\n            shifted = self.time_shift(x)\n            if len(shifted.shape) == 2:\n                shifted = shifted.unsqueeze(1)\n            delta = shifted - x\n        return self.linear(x + delta * mu)\n    \n\nclass RWKV6FeedForward(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        device=None,\n        dtype=None,\n    ):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        hidden_ratio = 3.5\n        intermediate_size = int(hidden_size * hidden_ratio)\n        intermediate_size = 32 * ((intermediate_size + 32 - 1) // 32)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n\n        self.key = LerpLinear(hidden_size, intermediate_size, device=device, dtype=dtype)\n        self.value = nn.Linear(intermediate_size, hidden_size, bias=False, device=device, dtype=dtype)\n        self.receptance = LerpLinear(hidden_size, hidden_size, device=device, dtype=dtype)\n        self.relu=nn.ReLU()\n\n    def forward(\n        self,\n        x: torch.Tensor,\n    ) -> torch.Tensor:\n        shifted = self.time_shift(x)\n        delta = shifted - x\n        # key = self.act_fn(self.key(x, delta))\n        _key=self.key(x,delta)\n        r=self.relu(_key)\n        key=r*r\n        value = self.value(key)\n        receptance = self.receptance(x, delta)\n\n        return receptance.sigmoid() * value\n\n\nclass GAB(GABBase):\n    \"\"\"Generalized Autoregressive Block\n        Input:        X: (batch, seqlen, embed_dim)\n        Output:       Y: (batch, seqlen, embed_dim)\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n    \"\"\"\n    def __init__(\n            self,\n            embed_dim: int, \n            device=None,\n            dtype=None,\n            num_heads: int = 4,\n            proj_low_rank_dim: int = 32,\n            gate_low_rank_dim: int = 64,\n            norm_eps: float = 1e-5,\n            **kwargs,\n        ): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n        # argv: list of hyperparameters\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\n        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n        \n        # COMPLETING THE CODE HERE #\n        self.hidden_size = embed_dim\n\n        self.attn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=norm_eps, **factory_kwargs)\n        self.attn = RWKV6Attention(\n            hidden_size=self.hidden_size,\n            num_heads=num_heads,\n            proj_low_rank_dim=proj_low_rank_dim,\n            gate_low_rank_dim=gate_low_rank_dim,\n            norm_eps=norm_eps,\n            **factory_kwargs\n        )\n        self.ffn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=norm_eps, **factory_kwargs)\n        self.ffn = RWKV6FeedForward(\n            hidden_size=self.hidden_size,\n            **factory_kwargs\n        )\n\n\n    # YOU CAN ADD MORE FUNCTIONS HERE #\n\n\n    def _forward(self,X,**kwargs): # type hints are optional but recommended\n        # THE CODE HERE MUST BE COMPLETED #\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states) +X\n        hidden_states = self.ffn_norm(X)\n        X = self.ffn(hidden_states) + X\n        return X\n    \n    \n\"\"\" The dictionary of hyperparameters for constructing a GAB layer\n    embed_dim, device, dtype should NOT be included in gab_config\n\"\"\"\ngab_config = {\n    # THE HYPERPARAMETERS OF ADDITIONAL ARGUMENTS IN GAB CLASS #\n    'num_heads': 4,\n    'proj_low_rank_dim': 32,\n    'gate_low_rank_dim': 64,\n    'norm_eps': 1e-5,\n}\n"
}