{
    "acronym": "dssm",
    "title": "Diagonal State Spaces are as Effective as Structured State Spaces",
    "seed_ids": [
        "s4"
    ],
    "s2id": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87",
    "abstract": "Modeling long range dependencies in sequential data is a fundamental step towards attaining human-level performance in many modalities such as text, vision, audio and video. While attention-based models are a popular and effective choice in modeling short-range interactions, their performance on tasks requiring long range reasoning has been largely inadequate. In an exciting result, Gu et al. (ICLR 2022) proposed the $\\textit{Structured State Space}$ (S4) architecture delivering large gains over state-of-the-art models on several long-range tasks across various modalities. The core proposition of S4 is the parameterization of state matrices via a diagonal plus low rank structure, allowing efficient computation. In this work, we show that one can match the performance of S4 even without the low rank correction and thus assuming the state matrices to be diagonal. Our $\\textit{Diagonal State Space}$ (DSS) model matches the performance of S4 on Long Range Arena tasks, speech classification on Speech Commands dataset, while being conceptually simpler and straightforward to implement.",
    "authors": [
        "Ankit Gupta",
        "Jonathan Berant"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": "This work shows that one can match the performance of S4 even without the low rank correction and thus assuming the state matrices to be diagonal, and proposes a new diagonal state space model (DSS) that is conceptually simpler and straightforward to implement.",
    "citationCount": 164,
    "influentialCitationCount": 21,
    "code": "\"\"\"SSM convolution kernels.\n\nSSMKernelDPLR is the S4 kernel, implementing the 'diagonal plus low-rank' algorithm from the original S4 paper. This stores parameters A, B, C, dt, and calling it creates the SSM convolution kernel bar{K}.\n\nSSMKernelDiag is the S4D kernel, a simpler algorithm for computing the kernel for the case of diagonal state matrices A.\n\nSSMKernel wraps these with common options and handles the initialization.\n\"\"\"\n\nfrom typing import Optional, Mapping, Tuple, Union\nfrom collections import defaultdict\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor # For type hints\nimport numpy as np\nfrom einops import rearrange, repeat\n\nimport src.models.hippo.hippo as hippo\nimport src.models.sequence.kernels.dplr as dplr\nfrom src.models.functional.krylov import krylov, power\nimport src.utils.train\n\nlog = src.utils.train.get_logger(__name__)\n\n# Try CUDA extension\ntry:\n    from extensions.kernels.vandermonde import log_vandermonde_cuda\n    has_cuda_extension = True\n    log.info(\"CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) found.\")\nexcept:\n    log.warning(\n        \"CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\"\n    )\n    has_cuda_extension = False\n\ntry:\n    import pykeops\n    from src.models.functional.vandermonde import log_vandermonde as log_vandermonde_keops, log_vandermonde_transpose as log_vandermonde_transpose_keops\n\n    has_pykeops = True\n    log.info(\"Pykeops installation found.\")\nexcept ImportError:\n    has_pykeops = False\n    if not has_cuda_extension:\n        log.warning(\n            \"Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.\"\n        )\n\n# Fallback versions\nfrom src.models.functional.vandermonde import log_vandermonde_naive\nfrom src.models.functional.vandermonde import log_vandermonde_transpose_naive\n\n# Base Kernel class\nfrom src.models.sequence.kernels.kernel import Kernel\n\n# Alias torch.einsum; can easily swap to opt_einsum if desired\ncontract = torch.einsum\n\n_conj = lambda x: torch.cat([x, x.conj()], dim=-1)\n_c2r = torch.view_as_real\n_r2c = torch.view_as_complex\n\nif tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 10):\n    _resolve_conj = lambda x: x.conj().resolve_conj()\nelse:\n    _resolve_conj = lambda x: x.conj()\n\ndef inv_transform(param, transform='none'):\n    \"\"\"Initialize a (positive) parameter under a transform.\"\"\"\n    param = torch.clamp(param, min=1e-4)\n    if transform == 'none':\n        return param\n    elif transform == 'exp':\n        return torch.log(param) # Some of the HiPPO methods have real part 0\n    elif transform == 'relu':\n        return param\n    elif transform == 'sigmoid':\n        return torch.logit(param)\n    elif transform == 'softplus':\n        return torch.log(torch.exp(param)-1)\n    else: raise NotImplementedError\n\ndef param_transform(param, transform='none'):\n    \"\"\"Get a (positive) parameter under a transform.\"\"\"\n    if transform == 'none':\n        p = param\n    elif transform == 'exp':\n        p = torch.exp(param)\n    elif transform == 'relu':\n        # JAX version seems to NaN if you allow 0's, although this code was fine without it\n        p = F.relu(param)+1e-4\n    elif transform == 'sigmoid':\n        p = F.sigmoid(param)\n    elif transform == 'softplus':\n        p = F.softplus(param)\n    else: raise NotImplementedError\n    return p\n\n\nclass SSMKernel(Kernel):\n    \"\"\"Parent class for different SSM parameterizations.\n\n    This class is abstract and only defines some initializations and flags that are common to all SSM variants.\n    It is instantiated by subclasses SSMKernel{Dense,Real,Diag,DPLR}.\n\n    Options:\n    d_state (N): State size (dimensionality of parameters A, B, C). Generally shouldn't need to be adjusted and doens't affect speed much for most kernels (e.g. S4, S4D).\n    deterministic: Use a deterministic initialization for dt, A, B, C.\n        Useful for debugging as well as constructing a simple exponential decay kernel (e.g. used in S4ND image->video inflation).\n\n    dt_min, dt_max: min and max values for the step size dt\n    dt_tie: Keep dt tied across the N dimensions of the state. Although this theoretically makes more sense, models such as S5 and Mega have found slightly improvements by setting it to False.\n    dt_transform: Transform function for parameterization of dt (default 'softplus', used to be 'exp')\n\n    rank: Rank of low-rank correction for DPLR mode. Needs to be increased for init \"legt\".\n    n_ssm: Number of independent trainable (A, B) SSMs, e.g.\n        `n_ssm=1` means all A/B parameters are tied across the H different instantiations of C.\n        `n_ssm=None` means all H SSMs are completely independent.\n        Generally, changing this option can save parameters but doesn't affect performance or speed much.\n        This parameter must divide H.\n    init: Options for initialization of (A, B). For DPLR mode, recommendations are \"legs\", \"fout\", \"hippo\" (combination of both). For Diag mode, recommendations are \"diag-inv\", \"diag-lin\", \"diag-legs\", and \"diag\" (combination of diag-inv and diag-lin).\n    init_args: Extra arguments passed into initialization function (see dplr.py for options).\n    \"\"\"\n\n    def init_dt(self):\n        # Generate dt\n        if self.deterministic:  # Meant for debugging\n            assert self.dt_tie, \"Deterministic dt initialization is tied\"\n            assert self.dt_transform == 'exp', \"Deterministic dt transform should be 'exp' for simplicity\"\n            inv_dt = torch.exp(torch.linspace(math.log(self.dt_min), math.log(self.dt_max), self.H)).unsqueeze(-1) # (H 1)\n        else:\n            shape = (self.H, 1) if self.dt_tie else (self.H, self.N//2)\n            # Initialize log dt\n            inv_dt = torch.rand(*shape, dtype=self.dtype) * (\n                math.log(self.dt_max) - math.log(self.dt_min)\n            ) + math.log(self.dt_min)\n            if self.dt_transform != 'exp':\n                inv_dt = inv_transform(torch.exp(inv_dt), self.dt_transform)\n\n        return inv_dt\n\n    def init_ssm_real(self):\n        \"\"\"Returns (dense, real) (A, B, C) parameters for init options.\"\"\"\n        # Generate A, B\n        A, B = hippo.transition(self.init, self.N)\n        A = torch.as_tensor(A, dtype=self.dtype)\n        B = torch.as_tensor(B, dtype=self.dtype)[:, 0]\n        B = repeat(B, 'n -> v n', v=self.n_ssm).clone().contiguous()\n        A = repeat(A, 'n m -> v n m', v=self.n_ssm).clone().contiguous()\n\n        # Generate C\n        if self.deterministic:\n            C = torch.zeros(self.channels, self.H, self.N, dtype=self.dtype)\n            C[..., :1] = 1.0\n        else:\n            C = torch.randn(self.channels, self.H, self.N, dtype=self.dtype)\n\n        return A, B, C\n\n    def init_ssm_dplr(self):\n        \"\"\"Returns DPLR (A, P, B, C) parameters for init options.\"\"\"\n        A, P, B, V = dplr.combination(self.init, self.N, self.rank, self.n_ssm, **self.init_args)\n\n        # Broadcast C to have H channels\n        if self.deterministic:\n            C = torch.zeros(self.channels, self.n_ssm, self.N, dtype=self.cdtype)\n            C[:, :, :1] = 1.\n            C = contract('hmn, chn -> chm', V.conj().transpose(-1, -2), C) # V^* C\n            C = repeat(C, 'c t n -> c (v t) n', v=self.H // C.size(-2)).clone().contiguous()\n        else:\n            C = torch.randn(self.channels, self.H, self.N//2, dtype=self.cdtype)\n\n        # Broadcast other parameters to have n_ssm copies\n        assert self.n_ssm % B.size(-2) == 0 \\\n                and self.n_ssm % P.size(-2) == 0 \\\n                and self.n_ssm % A.size(-2) == 0\n\n        # Broadcast tensors to n_ssm copies\n        # These will be the parameters, so make sure tensors are materialized and contiguous\n        B = repeat(B, 't n -> (v t) n', v=self.n_ssm // B.size(-2)).clone().contiguous()\n        P = repeat(P, 'r t n -> r (v t) n', v=self.n_ssm // P.size(-2)).clone().contiguous()\n        A = repeat(A, 't n -> (v t) n', v=self.n_ssm // A.size(-2)).clone().contiguous()\n\n        # Because these complex parameterizations assume conjugate symmetry,\n        # halve the value of self.N for convenience\n        self.N //= 2\n\n        return A, P, B, C\n\n    def __init__(\n        self,\n        # General Kernel arguments for parent class\n        d_model: int = 0,\n        channels: int = 1,\n        l_max: Optional[int] = None,\n        lr: Union[float, Optional[Mapping]] = None,\n        wd: Union[float, Optional[Mapping]] = 0.0,\n        verbose: bool = True,\n        # SSM arguments\n        d_state: int = 64,\n        deterministic: bool = False,\n        # dt options\n        dt_min: float = 0.001,\n        dt_max: float = 0.1,\n        dt_tie: bool = True,\n        dt_transform: str = 'exp',\n        # (A, B, C) options\n        rank: int = 1,\n        n_ssm: Optional[int] = None,\n        measure: Optional[str] = None,\n        init: Optional[str] = \"legs\",\n        # Extra hyperparameters for initialization\n        **init_args,\n    ):\n        super().__init__(d_model=d_model, channels=channels, l_max=l_max, lr=lr, wd=wd, verbose=verbose)\n        self.N = d_state\n        self.dtype, self.cdtype = torch.float, torch.cfloat\n        self.deterministic = deterministic\n        # dt options\n        self.dt_min = dt_min\n        self.dt_max = dt_max\n        self.dt_tie = dt_tie\n        self.dt_transform = dt_transform\n        # SSM options (A, B, C)\n        self.rank = rank\n        self.n_ssm = n_ssm if n_ssm is not None else self.H\n        if measure is not None:\n            log.warning(\"Warning: 'measure' option changed to 'init' and will be removed in a future version.\")\n            assert init is None, \"'measure' and 'init' cannot both be passed into SSMKernel\"\n            init, measure = measure, init\n        self.init = init\n        self.init_args = init_args\n\n    @torch.no_grad()\n    def forward_state(self, u, state):\n        \"\"\"Forward the state through a sequence, i.e. computes the state after passing chunk through SSM\n\n        This is a generic version of this functionality that works for SSMs.\n        It is currently used by SSMKernelDense and SSMKernelDPLR.\n        This is a suboptimal implementation; it is recommended to use SSMKernelDiag\n        if this functionality is desired.\n\n        state: (B, H, N)\n        u: (B, H, L)\n\n        Returns: (B, H, N)\n        \"\"\"\n\n        # Construct dA, dB matrices\n        dA, dB = self._setup_state() # (H N N) (H N)\n\n        conj = state.size(-1) != dA.size(-1)\n        if conj: state = _conj(state)\n\n        v = contract('h n, b h l -> b h n l', dB, u.flip(-1))\n        AL, v = power(u.size(-1), dA, v)\n        next_state = contract(\"h m n, b h n -> b h m\", AL, state)\n        next_state = next_state + v\n\n        if conj: next_state = next_state[..., : next_state.size(-1) // 2]\n        return next_state\n\n    def _setup_state(self):\n        \"\"\"Register dA and dB to module.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def d_state(self):\n        \"\"\"d_state and state_to_tensor are used by specific decoders.\n\n        These were used in earlier versions and should not be needed in general.\n        \"\"\"\n        return self.H * self.N\n\n    @property\n    def state_to_tensor(self):\n        return lambda state: rearrange('... h n -> ... (h n)', state)\n\n\nclass SSMKernelDiag(SSMKernel):\n    \"\"\"SSM kernel using diagonal state matrix (S4D model).\n\n    Options:\n    dt_fast:  (experimental) Parameterize inv_dt under sinh function.\n        (Ohno et al. \"Fast Saturating Gate for Learning Long Time Scales with RNNs\")\n    real_transform, imag_transform: ['none' | 'exp' | 'relu' | 'sigmoid' | 'softplus']\n        Parameterize the real/imag parts of the diagonal of A under this function.\n    bandlimit: Mask high frequencies of the kernel (indices corresponding to\n        diagonal elements with large imaginary part). Introduced in S4ND paper.\n    backend: ['cuda' | 'keops' | 'naive'] Options for Vandermonde/Cauchy kernel (in order of efficiency).\n    is_real : Real-valued SSM; can be interpreted as EMA.\n    \"\"\"\n\n    def __init__(\n        self,\n        dt_fast: bool = False,\n        real_transform: str = 'exp',\n        imag_transform: str = 'none',\n        bandlimit: Optional[float] = None,\n        backend: str = 'cuda',\n        is_real: bool = False,\n        **kwargs,\n    ):\n        # Special case: for real-valued, d_state semantics change\n        if is_real and 'd_state' in kwargs:\n            kwargs['d_state'] = kwargs['d_state'] * 2\n        super().__init__(**kwargs)\n        self.dt_fast = dt_fast\n        self.real_transform = real_transform\n        self.imag_transform = imag_transform\n        self.bandlimit = bandlimit\n        self.backend = backend\n        self.is_real = is_real\n\n        # Initialize dt, A, B, C\n        inv_dt = self.init_dt()\n        A, P, B, C = self.init_ssm_dplr()\n        # Note that in the Diag case, P will be ignored\n        # The DPLR case subclasses this and uses P\n        self.register_params(A, B, C, inv_dt, P)\n\n    def register_params(self, A, B, C, inv_dt, P):\n        \"\"\"Process the initialization into form of trainable parameters.\n\n        A: (S, N) diagonal matrix\n        B: (S, N)\n        C: (C, H, N)\n        dt: (H) timescale per feature\n\n        Dimensions:\n        N (or d_state): state size\n        H (or d_model): total SSM copies\n        S (or n_ssm): number of trainable copies of (A, B, dt); must divide H\n        C (or channels): system is 1-dim to C-dim\n\n        The forward pass of this Module returns a tensor of shape (C, H, L)\n\n        Note: tensor shape N here denotes half the true state size, because of conjugate symmetry\n        \"\"\"\n\n        assert self.backend in ['cuda', 'keops', 'naive']\n\n        if self.dt_fast: inv_dt = torch.asinh(inv_dt)\n\n        # Rank of low-rank correction\n        assert self.H == inv_dt.size(0)\n        assert self.N == A.size(-1) == B.size(-1) == C.size(-1)\n        assert self.n_ssm == A.size(-2) == B.size(-2) # Number of independent SSMs trained\n        self.repeat = self.H // A.size(0)\n\n        # Check that diagonal part has negative real and imag part\n        # (allow some tolerance for numerical precision on real part\n        # since it may be constructed by a diagonalization)\n        assert torch.all(A.real < 1e-4) and torch.all(A.imag <= 0.0)\n\n        # Broadcast everything to correct shapes\n        C = C.expand(torch.broadcast_shapes(C.shape, (1, self.H, self.N))) # (C, H, N)  # TODO originally this was only in DPLR, check safe for Diag\n        B = B.unsqueeze(0) # (1, H, N)\n        assert self.channels == C.shape[0]\n\n        # Register dt\n        self.register(\"inv_dt\", inv_dt, self.lr_dict['dt'], self.wd_dict['dt'])\n        # Register ABC\n        if self.is_real:\n            self.register(\"C\", C.real, self.lr_dict['C'], None)\n            self.register(\"B\", B.real, self.lr_dict['B'], self.wd_dict['B'])\n            self.register(\"A_real\", inv_transform(-A.real, self.real_transform), self.lr_dict['A'], self.wd_dict['A'])\n        else:\n            self.register(\"C\", _c2r(_resolve_conj(C)), self.lr_dict['C'], None)\n            self.register(\"B\", _c2r(B), self.lr_dict['B'], self.wd_dict['B'])\n            self.register(\"A_real\", inv_transform(-A.real, self.real_transform), self.lr_dict['A'], self.wd_dict['A'])\n            self.register(\"A_imag\", inv_transform(-A.imag, self.imag_transform), self.lr_dict['A'], self.wd_dict['A'])\n\n    def _get_params(self, rate=1.0):\n        \"\"\"Process the internal parameters.\"\"\"\n\n        # (S N) where S=n_ssm\n        if self.is_real:\n            A = -param_transform(self.A_real, self.real_transform)\n            B = self.B # (1 S N)\n            C = self.C # (C H N)\n        else:\n            A = -param_transform(self.A_real, self.real_transform) - 1j * param_transform(self.A_imag, self.imag_transform)\n            B = _r2c(self.B) # (1 S N)\n            C = _r2c(self.C) # (C H N)\n\n        if self.dt_fast: inv_dt = torch.sinh(self.inv_dt)\n        else: inv_dt = self.inv_dt\n        dt = param_transform(inv_dt, self.dt_transform) * rate # (H N)\n\n        if self.bandlimit is not None:\n            freqs = dt / rate * A.imag.abs() / (2*math.pi) # (H N)\n            mask = torch.where(freqs < self.bandlimit * .5, 1, 0)\n            C = C * mask\n\n        # Incorporate dt into A and B\n        A = repeat(A, 't n -> (v t) n', v=self.repeat)  # (H N)\n        B = repeat(B, 'b t n -> b (v t) n', v=self.repeat)  # (1 H N)\n\n        # TODO: The downstream algorithm should only need to access dt*A\n        # However the current DPLR kernel still uses dt and A separately\n        # Once that is fixed, this should return dtA instead of dt and A\n        dtA = dt * A  # (H N)\n\n        return dt, A, B, C\n\n    def forward(self, L, state=None, rate=1.0):\n        \"\"\"See Kernel.forward() for argument documentation.\"\"\"\n\n        dt, A, B, C = self._get_params(rate)\n        dtA = dt * A\n\n        # Augment B with state\n        if state is not None:\n            s = state / dt\n            B = torch.cat([s, B], dim=-3) # (1+B H N)\n\n\n        # Combine B and C\n        C = (B[:, None, :, :] * C).view(-1, self.H, self.N)\n\n        # Dispatch which Vandermonde kernel to use\n        if has_cuda_extension and C.dtype == torch.cfloat and C.device.type == 'cuda' and self.backend == 'cuda':\n            log_vandermonde = log_vandermonde_cuda\n        elif has_pykeops and self.backend in ['cuda', 'keops']:\n            log_vandermonde = log_vandermonde_keops\n        else:\n            log_vandermonde = log_vandermonde_naive\n\n        # Main kernel\n        # Implementation from DSS meant for case when real eigenvalues can be positive\n        P = dtA.unsqueeze(-1) * torch.arange(L, device=C.device) # [H N L]\n        A_gt_0 = A.real > 0                                      # [N]\n        if A_gt_0.any():\n            with torch.no_grad():\n                P_max = dtA * (A_gt_0 * (L-1))                   # [H N]\n            P = P - P_max.unsqueeze(-1)                          # [H N L]\n        S = P.exp()                                              # [H N L]\n\n        dtA_neg = dtA * (1 - 2*A_gt_0)                           # [H N]\n        num = dtA_neg.exp() - 1                                  # [H N]\n        den = (dtA_neg * L).exp() - 1                            # [H N]\n\n        # Inline reciprocal function for DSS logic\n        x = den * A\n        x_conj = _resolve_conj(x)\n        r = x_conj / (x*x_conj + 1e-7)\n\n        C = C * num * r             # [C H N]\n        K = contract('chn,hnl->chl', C, S).float()\n\n        K = K.view(-1, self.channels, self.H, L) # (1+B C H L)\n\n        if state is not None:\n            K_state = K[:-1, :, :, :] # (B C H L)\n        else:\n            K_state = None\n        K = K[-1, :, :, :] # (C H L)\n\n        return K, K_state\n\n    def _setup_step(self):\n        \"\"\"Set up dA, dB, dC discretized parameters for stepping.\"\"\"\n\n        dt, A, B, C, = self._get_params()\n        # Incorporate dt into A\n        dtA = dt * A  # (H N)\n        \n        self.dB = rearrange(self.dB, '1 h n -> h n')\n        self.dC = C\n\n    def default_state(self, *batch_shape):\n        C = _r2c(self.C)\n        state = torch.zeros(*batch_shape, self.H, self.N, dtype=C.dtype, device=C.device)\n        return state\n\n    def step(self, u, state):\n        next_state = contract(\"h n, b h n -> b h n\", self.dA, state) \\\n                + contract(\"h n, b h -> b h n\", self.dB, u)\n        y = contract(\"c h n, b h n -> b c h\", self.dC, next_state)\n        return 2*y.real, next_state\n\n    def forward_state(self, u, state):\n        \"\"\"Pass the state forward through an entire sequence.\"\"\"\n        self._setup_step()\n        AL = self.dA ** u.size(-1)\n        u = u.flip(-1).to(self.dA).contiguous() # (B H L)\n        # Dispatch which Vandermonde kernel to use\n        if has_pykeops and self.backend in ['cuda', 'keops']:\n            log_vandermonde_transpose = log_vandermonde_transpose_keops\n        else:\n            log_vandermonde_transpose = log_vandermonde_transpose_naive\n        v = log_vandermonde_transpose(u, self.dB, self.dA.log(), u.size(-1))\n        next_state = AL * state + v\n        return next_state\n",
    "description": null,
    "url": null
}