{
    "acronym": "lightningattn2",
    "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
    "seed_ids": [
        "hgrn",
        "streamingllm",
        "transnormerllm",
        "pi",
        "tnn",
        "transnormer",
        "roformer",
        "rfa",
        "performer",
        "lineartransformer",
        "gpt3",
        "transformer"
    ],
    "s2id": "7294c426b8a95975ca932eaf8f700acdd3d950b2",
    "abstract": "Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.",
    "authors": [
        "Zhen Qin",
        "Weigao Sun",
        "Dong Li",
        "Xuyang Shen",
        "Weixuan Sun",
        "Yiran Zhong"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": "Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.",
    "citationCount": 9,
    "influentialCitationCount": 1,
    "code": "# Copyright (c) 2024 Doraemonzzz\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    Out,\n    S,  # log lambda\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    BLOCK_MODEL: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_h = off_bh % h\n    off_e = tl.program_id(1)\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    # channel offset\n    e_offset = off_e * BLOCK_MODEL\n\n    ##### get block ptr\n    Q_block_ptr = Q + qk_offset + tl.arange(0, d)[None, :]\n    K_trans_block_ptr = K + qk_offset + tl.arange(0, d)[:, None]\n    V_block_ptr = V + v_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n    O_block_ptr = Out + o_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n    S_block_ptr = S + off_h\n\n    ##### init diag decay(Lambda); q, k decay; kv\n    s = tl.load(S_block_ptr)\n    # q, k decay\n    off_block = tl.arange(\n        0, BLOCK\n    )  # Not bug, this is a bit different from algorithm 1, but is mathematically equivalent\n    q_decay = tl.exp(-s.to(tl.float32) * off_block[:, None])\n    k_trans_decay = tl.exp(-s.to(tl.float32) * (BLOCK - off_block[None, :]))\n    block_decay = tl.exp(-s.to(tl.float32) * BLOCK)\n    # diag decay\n    index = off_block[:, None] - off_block[None, :]\n    s_index = s * index\n    s_index = tl.where(index >= 0, -s_index, float(\"-inf\"))\n    diag_decay = tl.exp(s_index)\n    kv = tl.zeros([d, BLOCK_MODEL], dtype=tl.float32)\n\n    ##### compute\n    for i in range(NUM_BLOCK):\n        # load\n        q = tl.load(\n            Q_block_ptr + off_block[:, None] * d, mask=off_block[:, None] < n, other=0.0\n        ).to(tl.float32)\n        k_trans = tl.load(\n            K_trans_block_ptr + off_block[None, :] * d,\n            mask=off_block[None, :] < n,\n            other=0.0,\n        ).to(tl.float32)\n        v = tl.load(\n            V_block_ptr + off_block[:, None] * e, mask=off_block[:, None] < n, other=0.0\n        ).to(tl.float32)\n\n        # compute\n        qk = tl.dot(q, k_trans) * diag_decay\n        o_intra = tl.dot(qk, v)\n        o_inter = tl.dot(q, kv) * q_decay\n        o = o_intra + o_inter\n\n        # save and update\n        tl.store(\n            O_block_ptr + off_block[:, None] * e,\n            o.to(O_block_ptr.dtype.element_ty),\n            mask=off_block[:, None] < n,\n        )\n        kv = block_decay * kv + tl.dot(k_trans * k_trans_decay, v)\n        off_block += BLOCK\n\n\n@triton.jit\ndef _bwd_intra_kernel(\n    Q,\n    K,\n    V,\n    S,\n    DO,\n    DQ,\n    DK,\n    DV,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    CBLOCK: tl.constexpr,\n    NUM_CBLOCK: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_h = off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK + tl.arange(0, BLOCK)\n\n    ##### get block ptr\n    Q_trans_block_ptr = (\n        Q + qk_offset + block_offset[None, :] * d + tl.arange(0, d)[:, None]\n    )\n    K_block_ptr = K + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = (\n        V + v_offset + block_offset[None, :] * e + tl.arange(0, e)[:, None]\n    )\n\n    DQ_block_ptr = DQ + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[None, :]\n    DK_trans_block_ptr = (\n        DK + qk_offset + block_offset[None, :] * d + tl.arange(0, d)[:, None]\n    )\n    DV_block_ptr = DV + v_offset + block_offset[:, None] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + block_offset[:, None] * e + tl.arange(0, e)[None, :]\n\n    S_block_ptr = S + off_h\n\n    ##### init diag decay(Lambda)\n    s = tl.load(S_block_ptr)\n    array = tl.arange(0, BLOCK).to(tl.float32)\n    # diag\n    index = array[:, None] - array[None, :]\n    s_index = s * index\n    s_index = tl.where(index >= 0, -s_index, float(\"-inf\"))\n    diag_decay = tl.exp(s_index)\n    diag_decay_trans = tl.trans(diag_decay)\n\n    ##### load block\n    k = tl.load(K_block_ptr, mask=block_offset[:, None] < n, other=0.0).to(tl.float32)\n    v_trans = tl.load(V_trans_block_ptr, mask=block_offset[None, :] < n, other=0.0).to(\n        tl.float32\n    )\n    do = tl.load(DO_block_ptr, mask=block_offset[:, None] < n, other=0.0).to(tl.float32)\n    q_trans = tl.load(Q_trans_block_ptr, mask=block_offset[None, :] < n, other=0.0).to(\n        tl.float32\n    )\n\n    ##### compute\n    dqk = tl.dot(do, v_trans) * diag_decay\n    dq_intra = tl.dot(dqk, k)\n\n    dk_intra_trans = tl.dot(q_trans, dqk)\n\n    qk_trans = tl.dot(k, q_trans) * diag_decay_trans\n    dv_intra = tl.dot(qk_trans, do)\n\n    dq = dq_intra\n    dk_trans = dk_intra_trans\n    dv = dv_intra\n\n    # save\n    tl.store(\n        DQ_block_ptr,\n        dq.to(DQ_block_ptr.dtype.element_ty),\n        mask=block_offset[:, None] < n,\n    )\n    tl.store(\n        DK_trans_block_ptr,\n        dk_trans.to(DK_trans_block_ptr.dtype.element_ty),\n        mask=block_offset[None, :] < n,\n    )\n    tl.store(\n        DV_block_ptr,\n        dv.to(DV_block_ptr.dtype.element_ty),\n        mask=block_offset[:, None] < n,\n    )\n\n\n@triton.jit\ndef _bwd_inter_kernel(\n    Q,\n    K,\n    V,\n    S,\n    DO,\n    DQ,\n    DK,\n    DV,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    CBLOCK: tl.constexpr,\n    NUM_CBLOCK: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_h = off_bh % h\n\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    S_block_ptr = S + off_h\n\n    ##### get block ptr\n    DQ_block_ptr = (\n        DQ + qk_offset + tl.arange(0, CBLOCK)[:, None] * d + tl.arange(0, d)[None, :]\n    )\n    K_block_ptr = (\n        K + qk_offset + tl.arange(0, CBLOCK)[:, None] * d + tl.arange(0, d)[None, :]\n    )\n    V_trans_block_ptr = (\n        V + v_offset + tl.arange(0, CBLOCK)[None, :] * e + tl.arange(0, e)[:, None]\n    )\n    DO_block_ptr = (\n        DO + o_offset + tl.arange(0, CBLOCK)[:, None] * e + tl.arange(0, e)[None, :]\n    )\n    # mask\n    off_block1 = tl.arange(0, CBLOCK)\n    off_block2 = tl.arange(0, CBLOCK)\n    # compute block array\n    c_array = tl.arange(0, CBLOCK)\n\n    ##### init lambda; kv\n    s = tl.load(S_block_ptr)\n    block_decay = tl.exp(-s.to(tl.float32) * BLOCK)\n    kv_trans = tl.zeros([e, d], dtype=tl.float32)\n\n    ##### compute dq inter\n    for i in range(NUM_BLOCK):\n        # compute in subblock\n        for j in range(NUM_CBLOCK):\n            if i > 0:  # if not add this, may have bug\n                q_decay = tl.exp(-s.to(tl.float32) * (j * CBLOCK + c_array[:, None]))\n                do = tl.load(DO_block_ptr, mask=off_block1[:, None] < n, other=0.0).to(\n                    tl.float32\n                )\n                dq_inter = tl.dot(do, kv_trans) * q_decay\n                dq = dq_inter + tl.load(\n                    DQ_block_ptr, mask=off_block1[:, None] < n, other=0.0\n                )\n                tl.store(\n                    DQ_block_ptr,\n                    dq.to(DQ_block_ptr.dtype.element_ty),\n                    mask=off_block1[:, None] < n,\n                )\n\n            DQ_block_ptr += CBLOCK * d\n            DO_block_ptr += CBLOCK * e\n            off_block1 += CBLOCK\n\n        # update kv in subblock\n        kv_trans_current = tl.zeros([e, d], dtype=tl.float32)\n        for j in range(NUM_CBLOCK):\n            v_trans = tl.load(\n                V_trans_block_ptr, mask=off_block2[None, :] < n, other=0.0\n            ).to(tl.float32)\n            k = tl.load(K_block_ptr, mask=off_block2[:, None] < n, other=0.0).to(\n                tl.float32\n            )\n            k_decay = tl.exp(\n                -s.to(tl.float32) * (BLOCK - (j * CBLOCK + c_array[:, None]))\n            )\n            kv_trans_current += tl.dot(v_trans, k * k_decay)\n\n            K_block_ptr += CBLOCK * d\n            V_trans_block_ptr += CBLOCK * e\n            off_block2 += CBLOCK\n\n        kv_trans = block_decay * kv_trans + kv_trans_current\n\n    ##### get block ptr\n    m = NUM_BLOCK * BLOCK\n    off_block1 = m + tl.arange(0, CBLOCK)\n    off_block2 = m + tl.arange(0, CBLOCK)\n\n    Q_trans_block_ptr = (\n        Q\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[None, :] * d\n        + tl.arange(0, d)[:, None]\n    )\n    K_block_ptr = (\n        K\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[:, None] * d\n        + tl.arange(0, d)[None, :]\n    )\n    V_trans_block_ptr = (\n        V\n        + v_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[None, :] * e\n        + tl.arange(0, e)[:, None]\n    )\n\n    DK_trans_block_ptr = (\n        DK\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[None, :] * d\n        + tl.arange(0, d)[:, None]\n    )\n    DV_block_ptr = (\n        DV\n        + v_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[:, None] * e\n        + tl.arange(0, e)[None, :]\n    )\n    DO_block_ptr = (\n        DO\n        + o_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[:, None] * e\n        + tl.arange(0, e)[None, :]\n    )\n\n    ##### init dkv\n    dkv = tl.zeros([d, e], dtype=tl.float32)\n\n    ##### compute dk, dv inter\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        # compute in subblock\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            K_block_ptr -= CBLOCK * d\n            V_trans_block_ptr -= CBLOCK * e\n            DK_trans_block_ptr -= CBLOCK * d\n            DV_block_ptr -= CBLOCK * e\n            off_block1 -= CBLOCK\n\n            if i < NUM_BLOCK - 1:  # if not add this, may have bug\n                k = tl.load(K_block_ptr, mask=off_block1[:, None] < n, other=0.0).to(\n                    tl.float32\n                )\n                v_trans = tl.load(\n                    V_trans_block_ptr, mask=off_block1[None, :] < n, other=0.0\n                ).to(tl.float32)\n\n                k_decay_trans = tl.exp(\n                    -s.to(tl.float32) * (BLOCK - (j * CBLOCK + c_array[None, :]))\n                )\n                k_decay = tl.exp(\n                    -s.to(tl.float32) * (BLOCK - (j * CBLOCK + c_array[:, None]))\n                )\n                dk_inter_trans = tl.dot(dkv, v_trans) * k_decay_trans\n                dv_inter = tl.dot(k, dkv) * k_decay\n\n                dk_trans = dk_inter_trans + tl.load(\n                    DK_trans_block_ptr, mask=off_block1[None, :] < n, other=0.0\n                )\n                dv = dv_inter + tl.load(\n                    DV_block_ptr, mask=off_block1[:, None] < n, other=0.0\n                )\n\n                tl.store(\n                    DK_trans_block_ptr,\n                    dk_trans.to(DK_trans_block_ptr.dtype.element_ty),\n                    mask=off_block1[None, :] < n,\n                )\n                tl.store(\n                    DV_block_ptr,\n                    dv.to(DV_block_ptr.dtype.element_ty),\n                    mask=off_block1[:, None] < n,\n                )\n\n        # update dkv in subblock\n        dkv_current = tl.zeros([d, e], dtype=tl.float32)\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            DO_block_ptr -= CBLOCK * e\n            Q_trans_block_ptr -= CBLOCK * d\n            off_block2 -= CBLOCK\n\n            do = tl.load(DO_block_ptr, mask=off_block2[:, None] < n, other=0.0).to(\n                tl.float32\n            )\n            q_trans = tl.load(\n                Q_trans_block_ptr, mask=off_block2[None, :] < n, other=0.0\n            ).to(tl.float32)\n            q_decay_trans = tl.exp(-s.to(tl.float32) * (j * CBLOCK + c_array[None, :]))\n            dkv_current += tl.dot(q_trans * q_decay_trans, do)\n\n        dkv = block_decay * dkv + dkv_current\n\n\nclass LightningAttention2(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, s):\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n        s = s.contiguous()\n\n        b, h, n, d = q.shape\n        e = v.shape[-1]\n        o = torch.empty((b, h, n, e), dtype=q.dtype, device=q.device)\n\n        BLOCK = 64\n        NUM_BLOCK = triton.cdiv(q.shape[2], BLOCK)\n        # parallel over channel\n        BLOCK_MODEL = min(triton.next_power_of_2(e), 32)\n        grid = (b * h, triton.cdiv(e, BLOCK_MODEL))\n\n        _fwd_kernel[grid](\n            q,\n            k,\n            v,\n            o,\n            s,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            BLOCK_MODEL=BLOCK_MODEL,\n        )\n\n        ctx.save_for_backward(q, k, v, s)\n\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        q, k, v, s = ctx.saved_tensors\n\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n        s = s.contiguous()\n        do = do.contiguous()\n\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n\n        b, h, n, d = q.shape\n        e = v.shape[-1]\n\n        # block size\n        BLOCK = 64\n        NUM_BLOCK = triton.cdiv(n, BLOCK)\n        # compute block size\n        CBLOCK = 32\n        NUM_CBLOCK = BLOCK // CBLOCK\n\n        # for intra part, compute in parallel\n        grid = (b * h, NUM_BLOCK)\n        _bwd_intra_kernel[grid](\n            q,\n            k,\n            v,\n            s,\n            do,\n            dq,\n            dk,\n            dv,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            CBLOCK=CBLOCK,\n            NUM_CBLOCK=NUM_CBLOCK,\n        )\n\n        # for inter part, compute in sequencial\n        grid = (b * h,)\n        _bwd_inter_kernel[grid](\n            q,\n            k,\n            v,\n            s,\n            do,\n            dq,\n            dk,\n            dv,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            CBLOCK=CBLOCK,\n            NUM_CBLOCK=NUM_CBLOCK,\n        )\n\n        return dq, dk, dv, None, None\n\n\nlightning_attn2 = LightningAttention2.apply\n\n",
    "description": null,
    "url": null
}