{
    "acronym": "eva",
    "title": "Efficient Attention via Control Variates",
    "seed_ids": [
        "flashattn",
        "lara",
        "abc",
        "lstransformer",
        "luna",
        "rfa",
        "deltanet",
        "nystromformer",
        "performer",
        "lineartransformer",
        "linformer",
        "funneltransformer",
        "reformer",
        "compressivetransformer",
        "sparsetransformer",
        "transformer"
    ],
    "s2id": "ac608a4a6b19b3208e560eee5daadb3cc18638a2",
    "abstract": "Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.",
    "authors": [
        "Lin Zheng",
        "Jianbo Yuan",
        "Chong Wang",
        "Lingpeng Kong"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "tldr": "This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.",
    "citationCount": 15,
    "influentialCitationCount": 1,
    "code": "import math\nfrom typing import Dict, Optional, Tuple, List\nimport logging\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nimport uuid\nfrom einops import rearrange\nfrom efficient_attention.attn_utils import pad_to_multiple,FairseqDropout,T5RelativePositionBias\n\nlogger = logging.getLogger(__name__)\n\n\ndef prm_projection(\n    data: torch.Tensor,\n    projection_matrix: torch.Tensor,\n    normalize: bool=True\n    ):\n    \"\"\"\n    Constructs nonnegative kernel features for fast softmax attention.\n    Args:\n    data: input for which features are computes\n    projection_matrix: random matrix used to compute features\n    eps: numerical stabilizer.\n    Returns:\n    Random features for fast softmax attention.\n    \"\"\"\n    # data : [b, h, lk, d]\n    # proj : [b, h, lc, d]\n    data_normalizer = (data.shape[-1] ** -0.5)\n    data_dash = torch.einsum('...nd,...md->...nm', \n                            projection_matrix,\n                            (data_normalizer * data),\n                            ) # [b, h, lq, lk]\n    # norm = (data_normalizer ** 2) * torch.sum(data ** 2, dim=-1).unsqueeze(-2) / 2.0# [b, h, 1, lk]\n    norm = data_normalizer * torch.sum(data ** 2, dim=-1).unsqueeze(-2) / 2.0# [b, h, 1, lk]\n    if normalize:\n        proj_data = F.softmax(data_dash - norm, dim=-1)  # [b, h, l_c, l_k]      \n    else:\n        proj_data = data_dash - norm\n    return proj_data\n\ndef window_1d_merge(x):\n    return rearrange(x, '... g w d ->... (g w) d')\n\ndef causal_window_1d_partition(x, window_size, ext_window_size=0, pad_val=0):\n    b, h, n, d = x.shape\n    n_groups = n // window_size\n    if ext_window_size > 0:\n        ext_len = ext_window_size\n        x = F.pad(x, (0, 0, ext_len, 0), value=pad_val)\n        out_shape = (b, h, n_groups, ext_len + window_size, d)\n        strides = x.stride()\n        out_stride = (strides[0], strides[1], window_size * strides[2], strides[2], strides[3])\n        return torch.as_strided(x, size=out_shape, stride=out_stride)\n    else:\n        return rearrange(x, '... (g w) d -> ... g w d', w=window_size)\n\ndef default(val, d):\n    return val if val is not None else d\n\ndef quant_noise(module, p, block_size):\n    \"\"\"\n    Wraps modules and applies quantization noise to the weights for\n    subsequent quantization with Iterative Product Quantization as\n    described in \"Training with Quantization Noise for Extreme Model Compression\"\n\n    Args:\n        - module: nn.Module\n        - p: amount of Quantization Noise\n        - block_size: size of the blocks for subsequent quantization with iPQ\n\n    Remarks:\n        - Module weights must have the right sizes wrt the block size\n        - Only Linear, Embedding and Conv2d modules are supported for the moment\n        - For more detail on how to quantize by blocks with convolutional weights,\n          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n        - We implement the simplest form of noise here as stated in the paper\n          which consists in randomly dropping blocks\n    \"\"\"\n\n    # if no quantization noise, don't register hook\n    if p <= 0:\n        return module\n\n    # supported modules\n    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n\n    # test whether module.weight has the right sizes wrt block_size\n    is_conv = module.weight.ndim == 4\n\n    # 2D matrix\n    if not is_conv:\n        assert (\n            module.weight.size(1) % block_size == 0\n        ), \"Input features must be a multiple of block sizes\"\n\n    # 4D matrix\n    else:\n        # 1x1 convolutions\n        if module.kernel_size == (1, 1):\n            assert (\n                module.in_channels % block_size == 0\n            ), \"Input channels must be a multiple of block sizes\"\n        # regular convolutions\n        else:\n            k = module.kernel_size[0] * module.kernel_size[1]\n            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n\n    def _forward_pre_hook(mod, input):\n        # no noise for evaluation\n        if mod.training:\n            if not is_conv:\n                # gather weight and sizes\n                weight = mod.weight\n                in_features = weight.size(1)\n                out_features = weight.size(0)\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                mask = torch.zeros(\n                    in_features // block_size * out_features, device=weight.device\n                )\n                mask.bernoulli_(p)\n                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n\n            else:\n                # gather weight and sizes\n                weight = mod.weight\n                in_channels = mod.in_channels\n                out_channels = mod.out_channels\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                if mod.kernel_size == (1, 1):\n                    mask = torch.zeros(\n                        int(in_channels // block_size * out_channels),\n                        device=weight.device,\n                    )\n                    mask.bernoulli_(p)\n                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n                else:\n                    mask = torch.zeros(\n                        weight.size(0), weight.size(1), device=weight.device\n                    )\n                    mask.bernoulli_(p)\n                    mask = (\n                        mask.unsqueeze(2)\n                        .unsqueeze(3)\n                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n                    )\n\n            # scale weights and apply mask\n            mask = mask.to(\n                torch.bool\n            )  # x.bool() is not currently supported in TorchScript\n            s = 1 / (1 - p)\n            mod.weight.data = s * weight.masked_fill(mask, 0)\n\n    module.register_forward_pre_hook(_forward_pre_hook)\n    return module\n\n\nclass CausalEVAttention(nn.Module):\n    \"\"\"Multi-headed attention.\n\n    See \"Attention Is All You Need\" for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        self_attention=False,\n        q_noise=0.0,\n        qn_block_size=8,\n        attn_args=None\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n\n        self.self_attention = self_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            \"Self-attention requires query, key and \" \"value to be of the same size\"\n        )\n\n        self.k_proj = quant_noise(\n            nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.v_proj = quant_noise(\n            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.q_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        self.out_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        self.window_size = attn_args.window_size\n        if attn_args.overlap_window:\n            self.ext_size = max(1, self.window_size)\n        else:\n            self.ext_size = 0\n        \n        self.causal = attn_args.causal\n        self.num_chunks = attn_args.num_chunks\n        self.chunk_size = attn_args.chunk_size\n        if self.chunk_size is not None:\n            assert self.window_size >= self.chunk_size and self.window_size % self.chunk_size == 0\n            # chunk_size overrides the number of landmarks\n            self.num_chunks = None\n\n        self.use_t5_rpe = (attn_args.use_t5_rpe) if attn_args.window_size > 0 else False\n\n        if self.use_t5_rpe:\n            self.rel_pos_bias = T5RelativePositionBias(\n                self.scaling, \n                causal = self.causal, \n                num_buckets=max(min(int((self.window_size + self.ext_size) / 2), 64), 16), \n                max_distance=attn_args.window_size + self.ext_size\n            )\n        else:\n            self.rel_pos_bias = None\n\n        self.adaptive_proj = attn_args.adaptive_proj\n        if self.adaptive_proj in ['qk']:\n            self.adaptive_mu_q = nn.Sequential(\n                nn.Linear(self.head_dim, self.head_dim),\n                nn.LayerNorm(self.head_dim),\n            )\n            self.adaptive_mu_k = nn.Sequential(\n                nn.Linear(self.head_dim, self.head_dim),\n                nn.LayerNorm(self.head_dim),\n            )\n        elif self.adaptive_proj in ['no-ln']:\n            self.adaptive_mu_q = nn.Sequential(\n                nn.Linear(self.head_dim, self.head_dim),\n            )\n            self.adaptive_mu_k = nn.Sequential(\n                nn.Linear(self.head_dim, self.head_dim),\n            )    \n\n    def _process_input(self, x, key_padding_mask):\n        # this function re-implements the parent method.\n        B, N, C = x.shape\n        if self.window_size > 0:\n            if key_padding_mask is None:\n                x, key_padding_mask = pad_to_multiple(x, self.window_size, dim=-2, create_mask=True)\n            else:\n                x = pad_to_multiple(x, self.window_size, dim=-2)\n                key_padding_mask = pad_to_multiple(key_padding_mask, self.window_size, dim=-1, value=True)\n            N = x.shape[-2]\n        return x, key_padding_mask\n\n\n    def window_partition(self, x, shape, ext_window_size, pad_val=0, window_size=None):\n        window_size = default(window_size, self.window_size)\n        return causal_window_1d_partition(\n            x, \n            window_size=window_size, \n            ext_window_size=ext_window_size, \n            pad_val=pad_val\n            )\n    \n    def window_merge(self, x, shape, window_size=None):\n        window_size = default(window_size, self.window_size)\n        return window_1d_merge(x)\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = True,\n        # static_kv: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        # before_softmax: bool = False,\n        # need_head_weights: bool = False,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        mask_val = -5e4\n        query = query.transpose(0, 1)\n        \n        bsz, tgt_len, embed_dim = query.size()\n        src_len = tgt_len\n        assert embed_dim == self.embed_dim, f\"query dim {embed_dim} != {self.embed_dim}\"\n        assert list(query.size()) == [bsz, tgt_len, embed_dim]\n        if key is not None:\n            key = key.transpose(0, 1)\n            value = value.transpose(0, 1)\n            key_bsz, src_len , _ = key.size()\n            if not torch.jit.is_scripting():\n                assert key_bsz == bsz\n                assert value is not None\n                assert bsz, src_len == value.shape[:2]\n\n        if incremental_state is None:\n            # pad the whole seq only when incremental_state is None.\n            B, tgt_len, C = query.shape\n            query, key_padding_mask = self._process_input(query, key_padding_mask)\n            B, N, C = query.shape\n        seq_shape = (N,)\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q = (\n            q.contiguous()\n            .view(bsz, N, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                .view(bsz, -1, self.num_heads, self.head_dim)\n                .transpose(1, 2)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                .view(bsz, -1, self.num_heads, self.head_dim)\n                .transpose(1, 2)\n            )\n\n        # Training & evaluation only. No incremental state is used.\n        if key_padding_mask is None:\n            key_padding_mask = torch.zeros(B, N, dtype=k.dtype, device=k.device)\n        key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(-1).to(torch.bool) # [b, 1, n, 1]\n    \n        w_q = self.window_partition(q, seq_shape, ext_window_size=0) # [b, h, w, i, d]\n        w_k = self.window_partition(k, seq_shape, ext_window_size=self.ext_size)\n        w_v = self.window_partition(v, seq_shape, ext_window_size=self.ext_size) # [b, h, w, j, d]\n\n        if self.chunk_size is not None:\n            rf_chunk_size = self.chunk_size\n        else:\n            rf_chunk_size = int(N // self.num_chunks)\n        if rf_chunk_size >= N:\n            rf_w_q = q\n            rf_w_k = k\n            rf_w_v = v\n        else:\n            # [b, h, c, j, d]\n            rf_w_q = self.window_partition(q, seq_shape, window_size=rf_chunk_size, ext_window_size=0)\n            # [b, h, c, j, d]\n            rf_w_k = self.window_partition(k, seq_shape, window_size=rf_chunk_size, ext_window_size=0)\n            # [b, h, c, j, d]\n            rf_w_v = self.window_partition(v, seq_shape, window_size=rf_chunk_size, ext_window_size=0)\n            # compute local attention\n            # [b, 1, c, j, 1]\n            rf_w_mask = self.window_partition(\n                key_padding_mask, \n                seq_shape, \n                window_size=rf_chunk_size,\n                ext_window_size=0,\n                pad_val=1\n                ).to(torch.bool)\n            # print(rf_w_mask)\n            rf_w_q = rf_w_q.masked_fill(rf_w_mask, 0.)\n            rf_w_k = rf_w_k.masked_fill(rf_w_mask, 0.)\n            rf_w_v = rf_w_v.masked_fill(rf_w_mask, 0.)\n\n            rf_q_bar = self.adaptive_mu_q(rf_w_q.mean(dim=-2))\n            rf_k_bar = self.adaptive_mu_k(rf_w_k.mean(dim=-2))\n            # [b, h, c, d]\n            mu = rf_q_bar + rf_k_bar\n            \n            ######################## Sampling from proposal ###############################\n            if self.training:\n                weights = mu + torch.randn_like(mu)\n            else:\n                weights = mu    \n            # [b, h, c, j, d], [b, h, c, 1, d] -> [b, h, c, j]\n            log_proj_w_k = prm_projection(rf_w_k, weights.unsqueeze(-2), normalize=False).squeeze(-2)\n            log_proj_w_k = log_proj_w_k.masked_fill(rf_w_mask.squeeze(-1), mask_val)\n\n            # [b, h, c, j] [b, h, c, j, d] -> [b, h, c, d]\n            beta = torch.einsum('...cj,...cjd->...cd', torch.softmax(log_proj_w_k, dim=-1), rf_w_v)\n        \n            # compute approx. expectation of CVs.\n            # [b, h, c, d]\n            approx_expected_cv = torch.einsum('...wid,...cd->...wic', w_q, self.scaling * rf_k_bar)\n            if self.causal:\n                # [b, h, j, c, c]\n                b, h, j, c = q.shape[0], q.shape[1], rf_w_k.shape[-2], rf_w_k.shape[-3]\n                if self.adaptive_proj in ['no-ln', 'qk']:\n                    causal_mask = torch.ones(b, h, j, c, c, dtype=q.dtype, device=q.device).triu(0).transpose(-2, -3) # [b, h, c, j, c]\n                    # NOTE: .triu(0) is used to remove the context of the current chunk from localized RFA.\n                    # since we compute `rf_q_bar` for each chunk for random features, \n                    # it requires the future information if we compute it on the current chunk.\n                    # however, note that the current chunk's information is still retained through\n                    # the local attention module.\n                else:\n                    raise NotImplementedError(\"Other adaptive projection methods are not implemented yet.\")\n                causal_mask = self.window_merge(causal_mask, seq_shape) # [b, h, n, c]\n                causal_mask = self.window_partition(causal_mask, seq_shape, ext_window_size=0).to(torch.bool) # [b, h, w, i, c]\n                approx_expected_cv = approx_expected_cv.masked_fill(causal_mask, mask_val)\n\n            # compute local attention\n            mask_q = self.window_partition(\n                key_padding_mask, \n                seq_shape, \n                ext_window_size=0,\n                pad_val=1\n                ).to(torch.bool) # [b, 1, w, i, 1]\n            mask_k = self.window_partition(\n                key_padding_mask, \n                seq_shape, \n                ext_window_size=self.ext_size,\n                pad_val=1\n                ).to(torch.bool).transpose(-1, -2) # [b, 1, w, 1, j] \n            local_dots_mask = torch.logical_or(mask_q, mask_k)\n            log_qk_local_dot = torch.einsum('bhwie,bhwje->bhwij', w_q, w_k) * self.scaling # [b, h, w, i, j]\n            # if self.use_headed_t5_rpe:\n                # here the t5-rpe-bias has already been scaled by \\sqrt{d}\n                # log_qk_local_dot = log_qk_local_dot + self.headed_rel_pos_bias(log_qk_local_dot)\n            if self.use_t5_rpe:\n                # here the t5-rpe-bias has already been scaled by \\sqrt{d}\n                log_qk_local_dot = log_qk_local_dot + self.rel_pos_bias(log_qk_local_dot)\n\n            log_qk_local_dot = log_qk_local_dot.masked_fill(local_dots_mask, mask_val)\n\n            if self.causal:\n                # e.g., if window_size = 3 and ext_size = 3, then it creates a causal_mask as follows:\n                # [0 0 0 0 1 1]\n                # [0 0 0 0 0 1]\n                # [0 0 0 0 0 0]\n                causal_mask = torch.ones_like(log_qk_local_dot).triu(1 + self.ext_size).to(torch.bool)\n                log_qk_local_dot = log_qk_local_dot.masked_fill(causal_mask, mask_val)\n\n            local_len = log_qk_local_dot.shape[-1]\n            num_rfa_chunks = approx_expected_cv.shape[-1]\n\n            # compute attention weights along with normalizing constant.\n            attn = torch.softmax(torch.cat([log_qk_local_dot, approx_expected_cv], dim=-1), dim=-1)\n            attn = self.dropout_module(attn)\n            local_attn, ra_attn = torch.split(attn, [local_len, num_rfa_chunks], dim=-1)\n            output_local = torch.einsum('bhwij,bhwjd->bhwid', local_attn, w_v)\n            output_snis = torch.einsum('bhwic,bhcd->bhwid', ra_attn, beta) \n            ######################## Combine them together ############################\n            output = self.window_merge(output_snis + output_local, seq_shape) # [b, h, n, d]\n            x = output.permute(0, 2, 1, 3).reshape((B,) + tuple(seq_shape) + (C,))\n            x = self.out_proj(x)\n            if tgt_len is not None and tgt_len != N:\n                x = x[..., :tgt_len, :]\n            return x.transpose(0, 1).contiguous(), None\n\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n        key_padding_mask: Optional[Tensor],\n        prev_key_padding_mask: Optional[Tensor],\n        batch_size: int,\n        src_len: int,\n        static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            if src_len > prev_key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n                    device=prev_key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [prev_key_padding_mask.float(), filler.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = prev_key_padding_mask.float()\n        elif key_padding_mask is not None:\n            if src_len > key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - key_padding_mask.size(1)),\n                    device=key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [filler.float(), key_padding_mask.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = key_padding_mask.float()\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights\n",
    "description": null,
    "url": null
}