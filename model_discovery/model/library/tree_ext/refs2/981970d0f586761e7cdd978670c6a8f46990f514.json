{
    "paperId": "981970d0f586761e7cdd978670c6a8f46990f514",
    "externalIds": {
        "ArXiv": "2309.01430",
        "DBLP": "journals/corr/abs-2309-01430",
        "DOI": "10.48550/arXiv.2309.01430",
        "CorpusId": 261531557
    },
    "title": "DAT++: Spatially Dynamic Vision Transformer with Deformable Attention",
    "abstract": "Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 135,
    "citationCount": 12,
    "influentialCitationCount": 1,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2309.01430",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way, which enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -1.6211957931518555,
            -2.825838565826416,
            3.39920711517334,
            5.977352619171143,
            -0.63770592212677,
            0.3339731693267822,
            4.270639896392822,
            0.7870086431503296,
            0.23699559271335602,
            -0.15065616369247437,
            0.004808366298675537,
            6.605834484100342,
            -2.9155824184417725,
            3.369126796722412,
            -5.090505599975586,
            -3.807277202606201,
            -1.2384471893310547,
            2.951267957687378,
            5.610324859619141,
            1.3271218538284302,
            -0.5674418210983276,
            1.188136100769043,
            -0.5123964548110962,
            -1.8450796604156494,
            -3.447263240814209,
            -0.2759917378425598,
            2.2395431995391846,
            1.622125267982483,
            -3.0640060901641846,
            -1.5968295335769653,
            -2.0278801918029785,
            -4.322216987609863,
            4.472934722900391,
            0.10235732793807983,
            1.0385921001434326,
            -1.102366328239441,
            0.614700436592102,
            3.4459786415100098,
            -4.2954864501953125,
            -3.0518577098846436,
            3.584420680999756,
            2.9067749977111816,
            0.5721052289009094,
            1.6148505210876465,
            -1.4980665445327759,
            0.8188267946243286,
            -3.0660126209259033,
            2.9386777877807617,
            2.7240383625030518,
            0.5467390418052673,
            1.3511550426483154,
            2.555373191833496,
            -1.0500078201293945,
            1.561311960220337,
            -0.6989590525627136,
            -1.277435541152954,
            3.119006633758545,
            3.591935396194458,
            0.5114578604698181,
            1.6146284341812134,
            3.9506893157958984,
            4.593936920166016,
            1.670150876045227,
            -0.22394192218780518,
            2.9907989501953125,
            -2.1170220375061035,
            0.5131229162216187,
            4.151790618896484,
            1.7256112098693848,
            -0.9839548468589783,
            -3.0912396907806396,
            -5.727931022644043,
            -1.5989530086517334,
            -2.0243148803710938,
            -1.8618650436401367,
            -0.7548773884773254,
            2.51908540725708,
            -3.799062490463257,
            -1.8650033473968506,
            0.11656337976455688,
            -0.006714612245559692,
            2.8552262783050537,
            -1.199664831161499,
            0.20486712455749512,
            2.5829927921295166,
            -2.4252052307128906,
            0.11630766838788986,
            0.3853207230567932,
            -0.48353397846221924,
            -3.0615601539611816,
            -4.641009330749512,
            1.0243539810180664,
            4.666670799255371,
            0.46930432319641113,
            -5.469146728515625,
            1.4634051322937012,
            1.8277695178985596,
            -2.2240724563598633,
            -3.7743380069732666,
            2.453458786010742,
            0.07958778738975525,
            -0.9834010601043701,
            -0.11609485745429993,
            0.4225231409072876,
            1.7647968530654907,
            -5.36669921875,
            -1.1384365558624268,
            0.1051815003156662,
            0.37851107120513916,
            -3.605103015899658,
            -3.751417875289917,
            3.0516459941864014,
            -0.886910617351532,
            2.557525873184204,
            -2.8495936393737793,
            1.3096189498901367,
            1.6985479593276978,
            3.2256152629852295,
            1.8189010620117188,
            3.530428886413574,
            -0.7360346913337708,
            -3.691117763519287,
            -5.178186416625977,
            3.112706184387207,
            0.2991957664489746,
            5.080175399780273,
            -0.1699894666671753,
            4.7307538986206055,
            2.9343812465667725,
            -4.705549716949463,
            3.4389803409576416,
            -1.1240869760513306,
            1.8983515501022339,
            0.4229948818683624,
            0.8586705327033997,
            0.8215749859809875,
            -1.0971994400024414,
            1.31834077835083,
            -0.7602596282958984,
            0.7884701490402222,
            -0.3291324973106384,
            0.7599999308586121,
            -0.0155259370803833,
            2.592546224594116,
            1.2950818538665771,
            2.6952128410339355,
            -0.9964514374732971,
            5.811358451843262,
            4.625479698181152,
            5.888285160064697,
            2.4505059719085693,
            -2.6498208045959473,
            0.08700627088546753,
            4.258162021636963,
            1.385430932044983,
            0.26350584626197815,
            -7.99664306640625,
            4.2719244956970215,
            -3.3168230056762695,
            3.3987581729888916,
            -0.7068442106246948,
            1.2254458665847778,
            -10.517943382263184,
            0.7167860865592957,
            0.43816816806793213,
            -5.280849456787109,
            -0.09100973606109619,
            2.105494976043701,
            1.9829705953598022,
            3.560720443725586,
            -0.03859002888202667,
            1.3224104642868042,
            1.3394606113433838,
            4.753150463104248,
            2.627396583557129,
            6.014239311218262,
            2.2709767818450928,
            -2.193794012069702,
            -3.347555637359619,
            -0.9541784524917603,
            -1.691429853439331,
            -0.6270040273666382,
            -6.8758721351623535,
            -0.6889710426330566,
            -4.678465366363525,
            -4.218014717102051,
            -0.1568375527858734,
            0.960584282875061,
            -1.533015489578247,
            -2.3079352378845215,
            2.4902851581573486,
            -0.2630562484264374,
            6.97353458404541,
            6.2046074867248535,
            1.9213354587554932,
            1.6937413215637207,
            0.13175705075263977,
            2.4899113178253174,
            -1.072281837463379,
            1.0218422412872314,
            2.40083384513855,
            -1.9954932928085327,
            0.5489606261253357,
            -1.632900357246399,
            1.5673322677612305,
            -1.5224430561065674,
            -2.617774486541748,
            1.5742647647857666,
            3.807224750518799,
            2.1527585983276367,
            0.10805374383926392,
            -3.723726749420166,
            0.7117813229560852,
            3.6145830154418945,
            -3.341376781463623,
            -1.7318446636199951,
            -6.9074177742004395,
            -0.147752046585083,
            1.1452889442443848,
            0.7987586259841919,
            2.7591261863708496,
            1.6240519285202026,
            -2.5891642570495605,
            -1.0478891134262085,
            1.340978980064392,
            -3.808518409729004,
            1.473182201385498,
            4.223334312438965,
            0.660094678401947,
            2.6405763626098633,
            -2.7334446907043457,
            -3.89047908782959,
            4.06365442276001,
            -2.3999810218811035,
            -6.70656156539917,
            -3.2378077507019043,
            -2.4837379455566406,
            -0.9881453514099121,
            0.09758216142654419,
            -2.675118923187256,
            6.226824760437012,
            3.4073667526245117,
            5.085635662078857,
            4.0431952476501465,
            -0.6771962642669678,
            -4.489884853363037,
            -1.4924180507659912,
            2.35202693939209,
            4.2806854248046875,
            -0.23307308554649353,
            1.9867671728134155,
            -1.2977346181869507,
            3.978726863861084,
            -3.2435076236724854,
            -0.7128657102584839,
            0.5774871110916138,
            4.2173542976379395,
            1.438547134399414,
            -1.088308334350586,
            -1.0918999910354614,
            0.330679714679718,
            3.6741080284118652,
            1.8257088661193848,
            6.4862847328186035,
            -2.987152338027954,
            2.764519453048706,
            -0.524631679058075,
            1.270708441734314,
            -0.2854817509651184,
            5.176967620849609,
            1.5764234066009521,
            1.98423433303833,
            -0.9496297240257263,
            -4.715987205505371,
            -4.206864833831787,
            -1.2630475759506226,
            -5.384527206420898,
            -0.308642715215683,
            3.1271684169769287,
            1.7728718519210815,
            2.7562928199768066,
            0.5029275417327881,
            -2.1968495845794678,
            -0.3874388039112091,
            -0.0878228098154068,
            -3.9437882900238037,
            -4.446521759033203,
            0.49746787548065186,
            3.2404110431671143,
            -0.5965555310249329,
            -4.531857490539551,
            2.1268303394317627,
            -3.236149311065674,
            3.472489595413208,
            -1.0655285120010376,
            3.9692742824554443,
            3.8950042724609375,
            -0.26322048902511597,
            0.44509249925613403,
            -2.9150643348693848,
            -3.999330997467041,
            2.265261173248291,
            5.086996078491211,
            -2.8690309524536133,
            1.1043431758880615,
            3.3185431957244873,
            -0.2065446972846985,
            -2.253993272781372,
            -0.1806783676147461,
            -4.436359405517578,
            -2.131681203842163,
            -0.26004496216773987,
            6.291560649871826,
            -3.4757590293884277,
            -2.656059741973877,
            -0.7468706965446472,
            0.09816896915435791,
            -1.4589060544967651,
            -0.7600412368774414,
            2.341841697692871,
            -0.2745339870452881,
            1.222525715827942,
            -2.505714178085327,
            -2.628605365753174,
            -2.6416900157928467,
            -3.590620994567871,
            0.17751821875572205,
            0.1684083640575409,
            -2.7171411514282227,
            3.8022897243499756,
            1.1430943012237549,
            3.2353012561798096,
            1.4772100448608398,
            1.005932092666626,
            0.3985016345977783,
            -3.1089465618133545,
            -0.39871278405189514,
            1.1717537641525269,
            1.615502119064331,
            3.27780818939209,
            -0.5798047780990601,
            3.4496848583221436,
            -0.3262752890586853,
            1.308402180671692,
            5.323076248168945,
            0.2717673182487488,
            1.4124813079833984,
            -2.2212421894073486,
            0.7136456966400146,
            -3.358548879623413,
            -1.8915631771087646,
            1.795800805091858,
            1.3294036388397217,
            -5.401701927185059,
            -0.07283288240432739,
            3.059398889541626,
            3.792712450027466,
            1.2766984701156616,
            3.833984136581421,
            -0.3045928478240967,
            -3.2840378284454346,
            -0.16977906227111816,
            -0.07242673635482788,
            -3.267472267150879,
            -0.14810840785503387,
            -1.1417882442474365,
            9.08491039276123,
            0.39981240034103394,
            -2.4720749855041504,
            -3.0745062828063965,
            -4.40959358215332,
            -1.7169276475906372,
            0.26086461544036865,
            5.6144514083862305,
            -0.8028688430786133,
            -0.21670064330101013,
            -0.9619156718254089,
            -4.796102523803711,
            0.9498822093009949,
            1.3804041147232056,
            -1.5803306102752686,
            2.9721903800964355,
            1.4309003353118896,
            3.6588032245635986,
            -0.9138783812522888,
            -0.7795170545578003,
            -2.842947483062744,
            0.4526495337486267,
            3.1322290897369385,
            2.7072432041168213,
            -0.7845072150230408,
            3.2077035903930664,
            2.9713311195373535,
            -1.0602021217346191,
            -3.0020909309387207,
            -6.646989345550537,
            -4.569414138793945,
            -2.8091511726379395,
            2.2436702251434326,
            -3.267261028289795,
            -3.518688201904297,
            1.6026830673217773,
            3.72440242767334,
            1.0374677181243896,
            -0.8734591007232666,
            -0.7596227526664734,
            4.356256484985352,
            -0.8191355466842651,
            2.9605655670166016,
            1.2665371894836426,
            -0.21292570233345032,
            -2.2759528160095215,
            0.6979390978813171,
            -1.631538987159729,
            -0.0922679603099823,
            0.720828652381897,
            -0.07885316014289856,
            1.8071449995040894,
            2.606295347213745,
            1.3240678310394287,
            -1.251917839050293,
            3.5951285362243652,
            3.307213068008423,
            1.2672014236450195,
            -0.7517784833908081,
            3.9948348999023438,
            0.04913365840911865,
            2.499358654022217,
            -3.7234275341033936,
            -0.6826332211494446,
            -2.1678669452667236,
            1.9242308139801025,
            -3.9063920974731445,
            -2.390352487564087,
            -3.228410482406616,
            1.781886339187622,
            -0.24269616603851318,
            0.12318754196166992,
            4.505211353302002,
            -1.1740652322769165,
            0.7666451930999756,
            5.507479667663574,
            -4.028496265411377,
            4.472566604614258,
            -2.96459698677063,
            -2.447567939758301,
            -0.8142436146736145,
            -2.5638089179992676,
            -0.7285431623458862,
            -4.776666164398193,
            1.493490219116211,
            -3.8458807468414307,
            -2.7365636825561523,
            -1.9717872142791748,
            1.2936714887619019,
            1.280189871788025,
            -2.304778575897217,
            -0.7902583479881287,
            2.33219051361084,
            -2.418335199356079,
            -4.798763275146484,
            3.328418731689453,
            1.0883761644363403,
            2.683806896209717,
            0.2696899175643921,
            4.851365566253662,
            1.3937344551086426,
            -0.33367764949798584,
            -1.6592934131622314,
            -0.0935213565826416,
            3.9460537433624268,
            3.5744307041168213,
            -1.725522756576538,
            -0.7348207235336304,
            1.7530615329742432,
            -0.9340509176254272,
            1.395485281944275,
            0.8423073291778564,
            1.1122958660125732,
            -2.8245506286621094,
            0.1390160322189331,
            0.6529049873352051,
            5.328032970428467,
            -0.7637054920196533,
            -3.1096351146698,
            3.637943983078003,
            5.902128219604492,
            3.6109633445739746,
            4.602255821228027,
            3.695423126220703,
            0.00030305981636047363,
            -2.010373592376709,
            4.178317070007324,
            -2.611612319946289,
            1.334179401397705,
            -0.9341003894805908,
            -1.0219833850860596,
            2.3928334712982178,
            2.655914545059204,
            -2.8706393241882324,
            -0.6440986394882202,
            -4.32333517074585,
            -1.605670690536499,
            -1.4499824047088623,
            -2.0467112064361572,
            4.856784820556641,
            1.2486381530761719,
            0.019225411117076874,
            -1.0330365896224976,
            -2.4645042419433594,
            -3.0322985649108887,
            1.6235820055007935,
            -3.583888292312622,
            0.8947474956512451,
            -0.5941369533538818,
            1.0357252359390259,
            3.1533162593841553,
            -2.7031822204589844,
            -0.7956205606460571,
            1.0548577308654785,
            1.535079002380371,
            2.7721352577209473,
            -0.14546620845794678,
            2.962731122970581,
            2.2749087810516357,
            0.36849507689476013,
            -7.734938621520996,
            2.6134626865386963,
            -2.779268980026245,
            3.5418832302093506,
            2.2592926025390625,
            2.2412984371185303,
            1.3364967107772827,
            -3.0184102058410645,
            -3.6676690578460693,
            -5.824655055999756,
            -0.046524882316589355,
            2.266989231109619,
            1.352256417274475,
            -0.9366654753684998,
            -2.078575611114502,
            3.1303277015686035,
            -1.0656863451004028,
            0.549973726272583,
            -0.3420173227787018,
            2.314560651779175,
            -5.050248622894287,
            0.3209930956363678,
            1.065091848373413,
            1.1562896966934204,
            0.7638034820556641,
            1.7079910039901733,
            -1.1358495950698853,
            -4.988178730010986,
            -3.0597870349884033,
            3.9870705604553223,
            -4.775284767150879,
            -3.514279842376709,
            0.7087258100509644,
            0.7299655675888062,
            1.3349390029907227,
            -3.1205503940582275,
            0.04990560933947563,
            5.7172088623046875,
            -4.733887672424316,
            -2.695708990097046,
            3.583003520965576,
            3.7205286026000977,
            -4.572530269622803,
            -2.5175371170043945,
            -1.555294156074524,
            -4.787739276885986,
            -4.734292030334473,
            3.2710132598876953,
            1.640808343887329,
            5.038839340209961,
            3.398984909057617,
            4.4499382972717285,
            -3.6384804248809814,
            1.6098170280456543,
            -3.376359701156616,
            -1.9175841808319092,
            -3.096005916595459,
            -4.899355888366699,
            -0.00043827295303344727,
            -3.286032199859619,
            -3.2161192893981934,
            -1.359006643295288,
            -1.4478799104690552,
            1.027893304824829,
            1.53517484664917,
            -1.4087638854980469,
            2.3526337146759033,
            -6.51617431640625,
            -3.673851490020752,
            -1.5053433179855347,
            5.08457612991333,
            -0.4313402771949768,
            0.09615826606750488,
            3.6391024589538574,
            5.371943950653076,
            2.377607822418213,
            2.5888209342956543,
            1.4903829097747803,
            -3.8523740768432617,
            2.8708336353302,
            0.42304906249046326,
            1.9072561264038086,
            -2.772087812423706,
            0.48884642124176025,
            0.6398941874504089,
            2.856931209564209,
            17.720252990722656,
            0.5442182421684265,
            -0.9109959602355957,
            -4.505826950073242,
            -1.754502534866333,
            -5.599742889404297,
            1.90264892578125,
            0.05362415313720703,
            2.5391347408294678,
            2.432713508605957,
            -0.7134034037590027,
            -5.2119951248168945,
            1.6839187145233154,
            0.21221542358398438,
            -4.78212308883667,
            -2.2666778564453125,
            -3.7235255241394043,
            2.0535190105438232,
            -4.11979866027832,
            -2.0332486629486084,
            -2.552278995513916,
            1.1459193229675293,
            -0.9520440101623535,
            -0.945987343788147,
            -2.035428762435913,
            5.303385257720947,
            1.4740760326385498,
            -0.4664824604988098,
            -6.179836750030518,
            3.5252819061279297,
            3.411815643310547,
            1.1224615573883057,
            4.955468654632568,
            -3.3070995807647705,
            -2.157550811767578,
            4.281122207641602,
            5.246963024139404,
            0.05794277787208557,
            1.9008666276931763,
            -0.23407115042209625,
            -0.9257054924964905,
            2.774050712585449,
            -7.25712251663208,
            -0.6690009832382202,
            0.41269534826278687,
            1.9898707866668701,
            -2.101304054260254,
            -3.2099475860595703,
            -1.9308875799179077,
            4.089897155761719,
            3.081617832183838,
            1.1423314809799194,
            -0.06678730249404907,
            6.170838356018066,
            3.2848057746887207,
            5.8790717124938965,
            0.01159106194972992,
            0.2189139574766159,
            1.374428391456604,
            -1.5026041269302368,
            -1.5117441415786743,
            0.7492297887802124,
            -0.5240926146507263,
            -4.637022018432617,
            -0.030771315097808838,
            1.9427211284637451,
            -5.345818996429443,
            5.258447170257568,
            -0.09776508808135986,
            0.42320090532302856,
            1.9615063667297363,
            1.7391371726989746,
            -0.49291419982910156,
            -3.5849411487579346,
            -2.3016579151153564,
            -5.350887298583984,
            -0.44676870107650757,
            0.08126595616340637,
            2.2861833572387695,
            4.471027374267578,
            -7.441888809204102,
            2.7634177207946777,
            -2.987725019454956,
            0.6160024404525757,
            5.443967819213867,
            -1.6079045534133911,
            2.036078453063965,
            0.648044764995575,
            -0.03997659683227539,
            3.5597047805786133,
            -0.5997610688209534,
            -0.37122368812561035,
            2.672717571258545,
            -1.4786425828933716,
            5.1464762687683105,
            -7.459306716918945,
            -3.1575963497161865,
            -0.9323959350585938,
            -3.7851405143737793,
            -3.563837766647339,
            4.5376176834106445,
            2.149643898010254,
            2.9753243923187256,
            -1.982429027557373,
            -0.6215612888336182,
            -2.3824033737182617,
            0.6197609901428223,
            -1.893821358680725,
            -3.6262760162353516,
            -2.7574315071105957,
            4.545940399169922,
            -1.81643807888031,
            1.326911449432373,
            0.7335958480834961,
            -1.8950786590576172,
            -2.364360809326172,
            -2.694689989089966,
            5.156076431274414,
            2.479527235031128,
            2.5443334579467773,
            1.3078727722167969,
            -0.6995730400085449,
            -4.013427734375,
            -3.7470762729644775,
            -2.572380542755127,
            -0.14324912428855896,
            0.13768243789672852,
            4.592001914978027,
            0.35685890913009644,
            -4.107883930206299,
            -0.8270497918128967,
            -1.789454698562622,
            -2.352592945098877,
            -0.8808144927024841,
            -3.449732780456543,
            -1.3833448886871338,
            0.8490434885025024,
            3.147353172302246,
            -3.002516031265259,
            3.7890284061431885,
            3.501969814300537,
            -1.8686869144439697,
            -4.074389457702637,
            8.169145584106445,
            -2.6219029426574707,
            -0.8115780353546143,
            -0.8269150257110596,
            -5.1328630447387695,
            -5.106683254241943,
            -1.0155940055847168,
            -0.05976587533950806,
            -0.6447042226791382,
            5.503507614135742,
            -0.9093168377876282,
            -1.1031969785690308,
            -5.0688796043396
        ]
    },
    "authors": [
        {
            "authorId": "2039921875",
            "name": "Zhuofan Xia"
        },
        {
            "authorId": "51170295",
            "name": "Xuran Pan"
        },
        {
            "authorId": "2235964292",
            "name": "Shiji Song"
        },
        {
            "authorId": "2238181631",
            "name": "Li Erran Li"
        },
        {
            "authorId": "143983679",
            "name": "Gao Huang"
        }
    ],
    "references": [
        {
            "paperId": "e12561022ff0945afd8ea764965f7a6e106ef26a",
            "title": "Dynamic Perceiver for Efficient Visual Recognition"
        },
        {
            "paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
            "title": "DINOv2: Learning Robust Visual Features without Supervision"
        },
        {
            "paperId": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
            "title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention"
        },
        {
            "paperId": "a117d3a69370d7fc9db77e2c8ee0cd4573dfdb20",
            "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens"
        },
        {
            "paperId": "261c5f6d599d07a24c12efe8b0ed0ec77db8b2fc",
            "title": "Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention"
        },
        {
            "paperId": "28a840d36aeabb165804c293f9b91a47d118f3b9",
            "title": "GFNet: Global Filter Networks for Visual Recognition"
        },
        {
            "paperId": "af21d7b29a8b48967d0151a0b86f15d755eba02b",
            "title": "Vision Transformer With Quadrangle Attention"
        },
        {
            "paperId": "2f4d8f3c016ec53380b376ae7ac516f9c0f07a0d",
            "title": "BiFormer: Vision Transformer with Bi-Level Routing Attention"
        },
        {
            "paperId": "bc04606ad7fb32dbb06daaecc66c61304bb11ec5",
            "title": "Adaptive Rotated Convolution for Rotated Object Detection"
        },
        {
            "paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95",
            "title": "Scaling Vision Transformers to 22 Billion Parameters"
        },
        {
            "paperId": "ee57e4d7a125f4ca8916284a857c3760d7d378d3",
            "title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"
        },
        {
            "paperId": "79e07495df577219ec343d68a23e91f9bb4f2e2c",
            "title": "Dynamic Grained Encoder for Vision Transformers"
        },
        {
            "paperId": "7d1fa55b32aa5d81cba98d3feb5e45ddfd87acd2",
            "title": "A Close Look at Spatial Modeling: From Attention to Convolution"
        },
        {
            "paperId": "cfca7eedc6ede9d363d1662280a74d78dcdc9d4a",
            "title": "Scaling Language-Image Pre-Training via Masking"
        },
        {
            "paperId": "8d6520112cf35d84cf680de38411ba84dfc4a4da",
            "title": "Vision Transformer with Super Token Sampling"
        },
        {
            "paperId": "ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c",
            "title": "Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information"
        },
        {
            "paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f",
            "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"
        },
        {
            "paperId": "26c80bd65baa90f5b18157de4951f4eb0b62ab69",
            "title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"
        },
        {
            "paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f",
            "title": "Token Merging: Your ViT But Faster"
        },
        {
            "paperId": "a883336e5c2e9f46f5012343227a6be4671c9ca0",
            "title": "Dilated Neighborhood Attention Transformer"
        },
        {
            "paperId": "e79e2b2f998e7e05c10cc8c50c1e331bc137b766",
            "title": "ClusTR: Exploring Efficient Self-attention via Clustering for Vision Transformers"
        },
        {
            "paperId": "3e448df5aa191f7a3945d0fd609c8bc5966a2333",
            "title": "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"
        },
        {
            "paperId": "2fe71acc2c3f1e75b6149dea72838f0b594ad013",
            "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"
        },
        {
            "paperId": "d1869155960e4b1b882b39171dbecd25a7eda3cd",
            "title": "More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity"
        },
        {
            "paperId": "968f628c3d42dbfd16fd4516e61cfedc16612310",
            "title": "Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks"
        },
        {
            "paperId": "86609b3567c1f039aecd87cc87ef8b8a995215bc",
            "title": "Global Context Vision Transformers"
        },
        {
            "paperId": "6fb05e3483d05f79e59e5ce957708d8ab8932fdc",
            "title": "Peripheral Vision Transformer"
        },
        {
            "paperId": "bf6ce546c589fa8054b3972b266532664914bd21",
            "title": "Fast Vision Transformers with HiLo Attention"
        },
        {
            "paperId": "04715db23b5f63cf8d8e2e04c2798a74d16cdf6b",
            "title": "Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer"
        },
        {
            "paperId": "ba609e5c83ad9b32723e547b9d3c89d97373f755",
            "title": "VSA: Learning Varied-Size Window Attention in Vision Transformers"
        },
        {
            "paperId": "ad7bcec33f5206d4f28687a6a5a950de67010651",
            "title": "Neighborhood Attention Transformer"
        },
        {
            "paperId": "d2f63b56fc6bc373f5c023454c2b253326962865",
            "title": "DeiT III: Revenge of the ViT"
        },
        {
            "paperId": "259c681c76335540e13081efad584efdf9101868",
            "title": "DaViT: Dual Attention Vision Transformers"
        },
        {
            "paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc",
            "title": "MaxViT: Multi-Axis Vision Transformer"
        },
        {
            "paperId": "fa717a2e31f0cef4e26921f3b147a98644d2e64c",
            "title": "Focal Modulation Networks"
        },
        {
            "paperId": "01e2e629189b498482eb4e3ac6959addc4451b03",
            "title": "ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer"
        },
        {
            "paperId": "75c642ebdcfbd4c16d6c3161130b72ff9af5c311",
            "title": "Three things everyone should know about Vision Transformers"
        },
        {
            "paperId": "9f1b0e4c42a5a85d4c023030557ade4419f82ecf",
            "title": "Scaling Up Your Kernels to 31\u00d731: Revisiting Large Kernel Design in CNNs"
        },
        {
            "paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84",
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
        },
        {
            "paperId": "6c22336873706b1cf5205ac6bd2432aa69d97821",
            "title": "ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond"
        },
        {
            "paperId": "177e957f5cd93229c9794ea652c646d2557b4a69",
            "title": "A ConvNet for the 2020s"
        },
        {
            "paperId": "7fafe86c53404cdc1c2c61adc413f2309a097ed8",
            "title": "Glance and Focus Networks for Dynamic Visual Recognition"
        },
        {
            "paperId": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5",
            "title": "QuadTree Attention for Vision Transformers"
        },
        {
            "paperId": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
            "title": "Vision Transformer with Deformable Attention"
        },
        {
            "paperId": "0a0c204919ec72c6c335296ebf639ebc379d3ac5",
            "title": "Learned Queries for Efficient Local Attention"
        },
        {
            "paperId": "008a428e049003fe768068a0f1fa1416af5c4982",
            "title": "Masked Feature Prediction for Self-Supervised Visual Pre-Training"
        },
        {
            "paperId": "c2a0c18e810535db52e5ebaf180c64ce70356748",
            "title": "A-ViT: Adaptive Tokens for Efficient Vision Transformer"
        },
        {
            "paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e",
            "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"
        },
        {
            "paperId": "38212997a6e8c55141574c329bb58d2eadcb0db5",
            "title": "AdaViT: Adaptive Vision Transformers for Efficient Image Recognition"
        },
        {
            "paperId": "a0023d03985f94dddef12f762bda45948f144460",
            "title": "On the Integration of Self-Attention and Convolution"
        },
        {
            "paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5",
            "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"
        },
        {
            "paperId": "9c4753ef43d2928866dc5bf6cec53d03373ec2fa",
            "title": "SimMIM: a Simple Framework for Masked Image Modeling"
        },
        {
            "paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
            "title": "Masked Autoencoders Are Scalable Vision Learners"
        },
        {
            "paperId": "ecb85de81dd77dd6d7466a9839023000ddc3ce17",
            "title": "Spatially Adaptive Feature Refinement for Efficient Inference"
        },
        {
            "paperId": "d780914102ecef35a0722d713ee521082854b6a8",
            "title": "Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation"
        },
        {
            "paperId": "d045133e6e022684329ff944d67f91888be1bc3b",
            "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer"
        },
        {
            "paperId": "c945efdeefaacb8ca679298720f4b0b054dc84bd",
            "title": "Vision Transformer with Progressive Sampling"
        },
        {
            "paperId": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede",
            "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"
        },
        {
            "paperId": "57e81d6545dc0279af6d63018bf82a6b9e363fec",
            "title": "DPT: Deformable Patch-based Transformer for Visual Recognition"
        },
        {
            "paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65",
            "title": "CMT: Convolutional Neural Networks Meet Vision Transformers"
        },
        {
            "paperId": "a143725bccbef904b10348625f5b0dd1eafd7f32",
            "title": "Visual Parser: Representing Part-whole Hierarchies with Transformers"
        },
        {
            "paperId": "66775d9f16b3f4ca43dba2b31c7c42ca6dcba72b",
            "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer"
        },
        {
            "paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798",
            "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"
        },
        {
            "paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1",
            "title": "Early Convolutions Help Transformers See Better"
        },
        {
            "paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a",
            "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"
        },
        {
            "paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe",
            "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"
        },
        {
            "paperId": "722ad6ac92286507437b31486f47987d6ece05c9",
            "title": "BEiT: BERT Pre-Training of Image Transformers"
        },
        {
            "paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0",
            "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"
        },
        {
            "paperId": "6b6ffb94626e672caffafc77097491d9ee7a8682",
            "title": "On the Connection between Local Attention and Dynamic Depth-wise Convolution"
        },
        {
            "paperId": "576c462dbc1f3d732b919ef1daac37a817123e52",
            "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias"
        },
        {
            "paperId": "2e8149dafb864ec3675087c99bf5572fcf4eb170",
            "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"
        },
        {
            "paperId": "07e987364bf0be1949e379f976f8dea675977337",
            "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens"
        },
        {
            "paperId": "14b97585f136671742f6ce4151081e487b1fc1fe",
            "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition"
        },
        {
            "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
            "title": "Emerging Properties in Self-Supervised Vision Transformers"
        },
        {
            "paperId": "6709d5583f658f589ae6a2184805933aceb18849",
            "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"
        },
        {
            "paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6",
            "title": "Multiscale Vision Transformers"
        },
        {
            "paperId": "5b68522f58b61e7235b852677337ef3725075fd9",
            "title": "Co-Scale Conv-Attentional Image Transformers"
        },
        {
            "paperId": "4f7e800d16cbe969b7f4b0797988e61e0059d003",
            "title": "CondenseNet V2: Sparse Feature Reactivation for Deep Networks"
        },
        {
            "paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae",
            "title": "An Empirical Study of Training Self-Supervised Vision Transformers"
        },
        {
            "paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325",
            "title": "Going deeper with Image Transformers"
        },
        {
            "paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2",
            "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"
        },
        {
            "paperId": "e775e649d815a02373eac840cf5e33a04ff85c95",
            "title": "CvT: Introducing Convolutions to Vision Transformers"
        },
        {
            "paperId": "96da196d6f8c947db03d13759f030642f8234abf",
            "title": "DeepViT: Towards Deeper Vision Transformer"
        },
        {
            "paperId": "610b302950a19acef1c45456111dcd495f638c18",
            "title": "ConViT: improving vision transformers with soft convolutional inductive biases"
        },
        {
            "paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
            "title": "Perceiver: General Perception with Iterative Attention"
        },
        {
            "paperId": "0ae67202f0584afccefa770865d14a46655d2975",
            "title": "Transformer in Transformer"
        },
        {
            "paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881",
            "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"
        },
        {
            "paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a",
            "title": "Conditional Positional Encodings for Vision Transformers"
        },
        {
            "paperId": "837ac4ed6825502f0460caec45e12e734c85b113",
            "title": "Dynamic Neural Networks: A Survey"
        },
        {
            "paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
            "title": "Bottleneck Transformers for Visual Recognition"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "e374193760d53abd55be0846665dde9e028cc42b",
            "title": "3D Object Detection with Pointformer"
        },
        {
            "paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504",
            "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"
        },
        {
            "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "title": "End-to-End Object Detection with Transformers"
        },
        {
            "paperId": "54c7445f319823c7dcc948c830e75e2fa7460b33",
            "title": "Exploring Self-Attention for Image Recognition"
        },
        {
            "paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582",
            "title": "Designing Network Design Spaces"
        },
        {
            "paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39",
            "title": "Randaugment: Practical automated data augmentation with a reduced search space"
        },
        {
            "paperId": "441555b5cd09703e55c03e70bd2c9f82c0ffcf9b",
            "title": "Deep High-Resolution Representation Learning for Visual Recognition"
        },
        {
            "paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d",
            "title": "Stand-Alone Self-Attention in Vision Models"
        },
        {
            "paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
        },
        {
            "paperId": "6f42a3843b72155bd375eac2c66516bd19b530ec",
            "title": "Convolutional Networks with Dense Connectivity"
        },
        {
            "paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c",
            "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"
        },
        {
            "paperId": "5e19eba1e6644f7c83f607383d256deea71f87ae",
            "title": "Searching for MobileNetV3"
        },
        {
            "paperId": "66143960c0325c70329a3869cc8052f0416b87aa",
            "title": "GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond"
        },
        {
            "paperId": "a84906dbd4d6640f918d0b6ed2a7313dda0d55f1",
            "title": "Panoptic Feature Pyramid Networks"
        },
        {
            "paperId": "987b2db58fbe0bda771f11a046cd23de1ce92b39",
            "title": "Deformable ConvNets V2: More Deformable, Better Results"
        },
        {
            "paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1",
            "title": "Unified Perceptual Parsing for Scene Understanding"
        },
        {
            "paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4",
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
        },
        {
            "paperId": "04957e40d47ca89d38653e97f728883c0ad26e5d",
            "title": "Cascade R-CNN: Delving Into High Quality Object Detection"
        },
        {
            "paperId": "efbac99adf8628aae7f070e5b4388a295956f9d2",
            "title": "CondenseNet: An Efficient DenseNet Using Learned Group Convolutions"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "4feef0fd284feb1233399b400eb897f59ec92755",
            "title": "mixup: Beyond Empirical Risk Minimization"
        },
        {
            "paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341",
            "title": "Random Erasing Data Augmentation"
        },
        {
            "paperId": "1a857da1a8ce47b2aa185b91b5cb215ddef24de7",
            "title": "Focal Loss for Dense Object Detection"
        },
        {
            "paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
            "title": "Focal Loss for Dense Object Detection"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
        },
        {
            "paperId": "1a0912bb76777469295bb2c059faee907e7f3258",
            "title": "Mask R-CNN"
        },
        {
            "paperId": "4a73a1840945e87583d89ca0216a2c449d50a4a3",
            "title": "Deformable Convolutional Networks"
        },
        {
            "paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878",
            "title": "Feature Pyramid Networks for Object Detection"
        },
        {
            "paperId": "88512be44744615f4baa8e14f600f036db4c2433",
            "title": "Semantic Understanding of Scenes Through the ADE20K Dataset"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
            "title": "Deep Networks with Stochastic Depth"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "title": "Microsoft COCO: Common Objects in Context"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "6dc61f37ecc552413606d8c89ffbc46ec98ed887",
            "title": "Acceleration of stochastic approximation by averaging"
        },
        {
            "paperId": "293535c2b0ef674e1ed9a7ba227e37cca35e5e4b",
            "title": "EViT: Expediting Vision Transformers via Token Reorganizations"
        },
        {
            "paperId": "ff169d09a933756e8798021dbf9e24a0bbfd9b38",
            "title": "Image BERT Pre-training with Online Tokenizer"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": "2a23ffef0b4f5f8689ffffb4cd9f515cb28336bd",
            "title": "HRFormer: High-Resolution Vision Transformer for Dense Predict"
        },
        {
            "paperId": "5a9bc55f6332e38f62eb509b684147a1d4f10fd9",
            "title": "Focal Attention for Long-Range Interactions in Vision Transformers"
        },
        {
            "paperId": "279696f90d13b1327ec7adb73e711e7d8f5db761",
            "title": "Token Labeling: Training a 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        }
    ]
}