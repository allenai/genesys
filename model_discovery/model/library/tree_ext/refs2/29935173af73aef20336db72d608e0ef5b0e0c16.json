{
    "paperId": "29935173af73aef20336db72d608e0ef5b0e0c16",
    "externalIds": {
        "DBLP": "conf/icml/BergstraYC13",
        "MAG": "2113207845",
        "CorpusId": 3356163
    },
    "title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures",
    "abstract": "Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned. \n \nIn this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.",
    "venue": "International Conference on Machine Learning",
    "year": 2013,
    "referenceCount": 30,
    "citationCount": 2051,
    "influentialCitationCount": 125,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "32837403",
            "name": "J. Bergstra"
        },
        {
            "authorId": "2292273",
            "name": "Daniel Yamins"
        },
        {
            "authorId": "2042941",
            "name": "David D. Cox"
        }
    ],
    "references": [
        {
            "paperId": "0e652a99761d2664f28f8931fee5b1d6b78c2a82",
            "title": "Making a Science of Model Search"
        },
        {
            "paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e",
            "title": "Practical Bayesian Optimization of Machine Learning Algorithms"
        },
        {
            "paperId": "8f731c693587f25f93f22e2ed88bdaac843a4a05",
            "title": "Machine learning for predictive auto-tuning with boosted regression trees"
        },
        {
            "paperId": "0d77e82e45bea1ab04703373c37c9aead7481303",
            "title": "How Does the Brain Solve Visual Object Recognition?"
        },
        {
            "paperId": "188e247506ad992b8bc62d6c74789e89891a984f",
            "title": "Random Search for Hyper-Parameter Optimization"
        },
        {
            "paperId": "03911c85305d42aa2eeb02be82ef6fb7da644dd0",
            "title": "Algorithms for Hyper-Parameter Optimization"
        },
        {
            "paperId": "6f568d757d2c1ab42f2006faa25690b74c3d2d44",
            "title": "The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization"
        },
        {
            "paperId": "9c23859ec7313f2e756a3e85575735e0c52249f4",
            "title": "Scaling up biologically-inspired computer vision: A case study in unconstrained face recognition on facebook"
        },
        {
            "paperId": "1128c5607a19af6022be723d10dbf8fad3ca26ab",
            "title": "Beyond simple features: A large-scale feature search approach to unconstrained face recognition"
        },
        {
            "paperId": "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74",
            "title": "Scikit-learn: Machine Learning in Python"
        },
        {
            "paperId": "728744423ff0fb7e327664ed4e6352a95bb6c893",
            "title": "Sequential Model-Based Optimization for General Algorithm Configuration"
        },
        {
            "paperId": "560ac582a82dc0ce9765fc942f07f94908eaeeb3",
            "title": "Surrogating the surrogate: accelerating Gaussian-process-based global optimization with a mixture cross-entropy algorithm"
        },
        {
            "paperId": "d46fd54609e09bcd135fd28750003185a5ee4125",
            "title": "A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation"
        },
        {
            "paperId": "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3",
            "title": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"
        },
        {
            "paperId": "268a4f8da15a42f3e0e71691f760ff5edbf9cec8",
            "title": "LIBLINEAR: A Library for Large Linear Classification"
        },
        {
            "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "title": "A Fast Learning Algorithm for Deep Belief Nets"
        },
        {
            "paperId": "554894f70b28dba58b396c2d84080ac01051261b",
            "title": "Gaussian Processes For Machine Learning"
        },
        {
            "paperId": "a3db48fdc9aaf6921f269817ba4ed16b9b198394",
            "title": "A Taxonomy of Global Optimization Methods Based on Response Surfaces"
        },
        {
            "paperId": "577d19a115f9ef6f002483fcf88adbb3b5479556",
            "title": "Independent component analysis: algorithms and applications"
        },
        {
            "paperId": "85abadb689897997f1e37baa7b5fc6f7d497518b",
            "title": "Hierarchical models of object recognition in cortex"
        },
        {
            "paperId": "f9f836d28f52ad260213d32224a6d227f8e8849a",
            "title": "Object recognition from local scale-invariant features"
        },
        {
            "paperId": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "title": "Backpropagation Applied to Handwritten Zip Code Recognition"
        },
        {
            "paperId": "69e68bfaadf2dccff800158749f5a50fe82d173b",
            "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"
        },
        {
            "paperId": null,
            "title": "Hyperopt: Distributed asynchronous hyperparameter optimization in Python"
        },
        {
            "paperId": null,
            "title": "Hyperpa-rameter optimization for convolutional vision architectures"
        },
        {
            "paperId": null,
            "title": "Theano: a CPU and GPU math expression compiler"
        },
        {
            "paperId": null,
            "title": "Interactive Bayesian Optimization: Learning Parameters for Graphics and Animation"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": "08c4fdd974d874c87ea87faa6b404a7b8eb72c73",
            "title": "Automated configuration of algorithms for solving hard computational problems"
        },
        {
            "paperId": null,
            "title": "The application of Bayesian methods for seeking the extremum"
        }
    ]
}