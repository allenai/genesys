{
    "paperId": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc",
    "externalIds": {
        "ArXiv": "2306.07629",
        "DBLP": "journals/corr/abs-2306-07629",
        "DOI": "10.48550/arXiv.2306.07629",
        "CorpusId": 259144954
    },
    "title": "SqueezeLLM: Dense-and-Sparse Quantization",
    "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 103,
    "citationCount": 91,
    "influentialCitationCount": 14,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2306.07629",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint, demonstrates that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.8007140159606934,
            -0.026445284485816956,
            0.6120021343231201,
            5.407167434692383,
            -0.4983574450016022,
            1.4874114990234375,
            5.129055023193359,
            -2.0538105964660645,
            -0.528588056564331,
            0.05750012397766113,
            -1.6347545385360718,
            3.4851620197296143,
            0.048114895820617676,
            -1.0606412887573242,
            -5.718479156494141,
            -1.6124588251113892,
            -1.7644259929656982,
            1.0445020198822021,
            6.06916618347168,
            3.6006600856781006,
            -3.321406602859497,
            1.6893573999404907,
            -4.190424919128418,
            4.190303802490234,
            -3.1069610118865967,
            -0.22367358207702637,
            -1.7483201026916504,
            2.8910088539123535,
            -0.5249388217926025,
            -1.0991328954696655,
            1.3258979320526123,
            -2.9132628440856934,
            5.12001895904541,
            -4.215456485748291,
            4.742818832397461,
            -0.8539467453956604,
            -0.2598173916339874,
            6.9317145347595215,
            -2.6589853763580322,
            -1.1214210987091064,
            0.029039442539215088,
            1.6366372108459473,
            1.677816390991211,
            -1.9004454612731934,
            -0.9644521474838257,
            0.6450982093811035,
            0.8227916359901428,
            0.17328183352947235,
            -0.7225711941719055,
            0.02903151512145996,
            2.5810234546661377,
            1.8181092739105225,
            1.9393662214279175,
            0.7980916500091553,
            -2.9628052711486816,
            -3.5772299766540527,
            0.07000155746936798,
            1.6745879650115967,
            2.3371996879577637,
            -3.380852699279785,
            2.700507879257202,
            2.3308773040771484,
            0.06513988971710205,
            1.1229026317596436,
            2.1451714038848877,
            -3.161710739135742,
            1.3216718435287476,
            6.1760101318359375,
            0.32891348004341125,
            -1.850907325744629,
            0.4483879506587982,
            -4.771359443664551,
            2.9943079948425293,
            0.2936583459377289,
            -6.489690780639648,
            0.588761568069458,
            -1.0657382011413574,
            -6.620555400848389,
            -0.2320229709148407,
            -3.1948461532592773,
            -0.3261476457118988,
            4.237903118133545,
            -2.3626253604888916,
            2.694859266281128,
            3.19368314743042,
            -1.5365346670150757,
            -3.4964981079101562,
            -0.3962448239326477,
            0.9078760147094727,
            -1.05855131149292,
            3.3125829696655273,
            0.46592721343040466,
            -0.7170534133911133,
            2.0999631881713867,
            -4.734238624572754,
            -1.1880664825439453,
            0.08976475894451141,
            2.4604315757751465,
            0.7797016501426697,
            1.3622791767120361,
            3.193145275115967,
            0.6207463145256042,
            0.49125322699546814,
            0.28677231073379517,
            3.4972665309906006,
            -4.281227111816406,
            -2.1020100116729736,
            0.5101949572563171,
            -0.45996493101119995,
            -3.030655860900879,
            -3.4579195976257324,
            1.923262119293213,
            -1.3252201080322266,
            -0.28358036279678345,
            -3.7939066886901855,
            -1.689683198928833,
            -0.4231673777103424,
            -1.0012091398239136,
            -2.360814094543457,
            3.5576019287109375,
            -1.076716423034668,
            -4.109126091003418,
            -2.2707529067993164,
            0.4298899173736572,
            3.5767154693603516,
            2.316718101501465,
            -1.3900463581085205,
            -0.9175609350204468,
            -2.8508949279785156,
            -2.495293140411377,
            0.47675183415412903,
            -0.5775217413902283,
            2.841287136077881,
            -0.46270257234573364,
            2.4660091400146484,
            0.8614333868026733,
            -3.662790298461914,
            1.789078712463379,
            -4.337377548217773,
            -0.5230504870414734,
            3.874906063079834,
            1.2525601387023926,
            -0.8017879724502563,
            4.097320556640625,
            1.1708617210388184,
            2.286316156387329,
            0.8048704862594604,
            -0.8379095792770386,
            0.9395602941513062,
            6.524086952209473,
            5.0377678871154785,
            -4.925742149353027,
            -0.05215898156166077,
            0.7327801585197449,
            0.09510660171508789,
            3.262104034423828,
            -3.166888952255249,
            2.0665132999420166,
            -0.846054196357727,
            0.24377767741680145,
            1.2827999591827393,
            0.48547226190567017,
            -10.66943359375,
            -0.03103935718536377,
            3.9143173694610596,
            -3.0069947242736816,
            -2.052083969116211,
            1.3251006603240967,
            -2.5053272247314453,
            3.763824462890625,
            -0.6060537099838257,
            2.7945733070373535,
            2.6788086891174316,
            3.978991985321045,
            2.759835720062256,
            4.8228559494018555,
            4.642278671264648,
            -3.395927906036377,
            -2.008089542388916,
            0.7685507535934448,
            0.460067480802536,
            -0.9348670244216919,
            -6.890542984008789,
            0.14075864851474762,
            -5.48807430267334,
            -0.3507786989212036,
            -1.61783766746521,
            -1.7366840839385986,
            -1.86372709274292,
            -0.2375032752752304,
            0.5192947387695312,
            -0.38748520612716675,
            4.998702049255371,
            4.980061054229736,
            2.2969703674316406,
            -2.716139316558838,
            2.0401511192321777,
            2.4473137855529785,
            -0.9233434200286865,
            1.748542308807373,
            5.217162132263184,
            0.47360795736312866,
            -1.497106909751892,
            -2.234009027481079,
            6.418554782867432,
            3.0648069381713867,
            -2.928867816925049,
            3.1121528148651123,
            0.7896146774291992,
            0.9660353064537048,
            0.3498583436012268,
            -1.0957282781600952,
            -1.7798242568969727,
            1.6187562942504883,
            -3.1154441833496094,
            -3.991564989089966,
            -6.184184551239014,
            3.198890447616577,
            4.9315409660339355,
            0.7225174903869629,
            1.1730469465255737,
            -1.454799771308899,
            0.29329216480255127,
            -4.027469158172607,
            1.1416831016540527,
            -2.5389089584350586,
            3.2151448726654053,
            0.7063132524490356,
            -0.26611143350601196,
            2.1898810863494873,
            -0.17265045642852783,
            -4.684606552124023,
            -0.272114634513855,
            -0.002332240343093872,
            -8.498849868774414,
            0.465681254863739,
            -3.027977228164673,
            1.628730058670044,
            -3.5471997261047363,
            -1.0112720727920532,
            5.169097900390625,
            1.6526243686676025,
            1.7015340328216553,
            4.483835220336914,
            3.433410167694092,
            -2.202681064605713,
            -1.735347032546997,
            1.6492524147033691,
            1.5185962915420532,
            -2.3053274154663086,
            -0.8932167887687683,
            -1.796349048614502,
            0.522074818611145,
            -3.549215316772461,
            2.810863971710205,
            2.93650221824646,
            0.4281916618347168,
            -0.6810237169265747,
            1.6792248487472534,
            2.0728187561035156,
            -1.3929288387298584,
            3.8340671062469482,
            3.6411588191986084,
            6.030848503112793,
            -1.8295644521713257,
            -2.5064666271209717,
            -3.5330734252929688,
            0.1043778657913208,
            1.155761957168579,
            2.840993881225586,
            2.628126382827759,
            1.1670989990234375,
            -0.2689633369445801,
            -5.253597259521484,
            -3.5112009048461914,
            -5.325778007507324,
            -2.104188919067383,
            0.457370787858963,
            -0.15390145778656006,
            4.705357551574707,
            3.5465002059936523,
            -2.6728672981262207,
            -0.7694720029830933,
            -0.3772163987159729,
            -1.3893005847930908,
            -2.1403937339782715,
            -0.6461907625198364,
            0.5613755583763123,
            -2.83961820602417,
            -2.307776927947998,
            -4.040943622589111,
            3.8067004680633545,
            -4.475969314575195,
            -1.0203949213027954,
            -4.334027290344238,
            3.2459402084350586,
            5.302375793457031,
            -0.12388306856155396,
            -1.6090757846832275,
            -0.3662080764770508,
            -1.4063634872436523,
            4.244091033935547,
            3.7825236320495605,
            -0.2941816449165344,
            0.42890679836273193,
            3.017971992492676,
            3.157975196838379,
            -3.558195114135742,
            3.4871320724487305,
            -1.5976974964141846,
            -2.522645950317383,
            -0.26523610949516296,
            4.600568771362305,
            -4.211256980895996,
            1.9727487564086914,
            -0.0641026645898819,
            4.717678070068359,
            -0.46613091230392456,
            -3.160741090774536,
            2.549093246459961,
            -0.33063167333602905,
            0.8011062145233154,
            -6.983133792877197,
            -2.0080456733703613,
            -2.876514196395874,
            -0.6772773265838623,
            4.6844162940979,
            2.6660726070404053,
            -3.3716235160827637,
            2.074092388153076,
            2.215963125228882,
            5.662444114685059,
            4.152468681335449,
            2.9145631790161133,
            0.44011399149894714,
            -4.977193355560303,
            0.3780561685562134,
            -2.1645331382751465,
            -0.9510740637779236,
            -1.8319873809814453,
            -1.6431094408035278,
            5.210842132568359,
            -2.521761894226074,
            3.5202198028564453,
            -0.5366122126579285,
            -2.979466199874878,
            3.619476079940796,
            -2.807250499725342,
            1.2872202396392822,
            -0.940353512763977,
            0.28138303756713867,
            -0.7136340737342834,
            3.135810613632202,
            -2.1578712463378906,
            1.667546272277832,
            2.6240267753601074,
            2.307209014892578,
            -0.5015665292739868,
            3.893639087677002,
            2.7711539268493652,
            0.08322840929031372,
            3.2911553382873535,
            1.8357824087142944,
            -0.7936424612998962,
            0.971616268157959,
            -2.7227954864501953,
            9.377660751342773,
            0.09263521432876587,
            1.4907599687576294,
            -3.0883660316467285,
            -2.2434096336364746,
            -3.526710033416748,
            -0.7772895097732544,
            -0.2974972128868103,
            -3.158573865890503,
            -1.9234195947647095,
            0.6838377118110657,
            -5.015989303588867,
            1.5977210998535156,
            -0.3104660212993622,
            1.2672090530395508,
            3.4688706398010254,
            -0.5538692474365234,
            3.61259126663208,
            -0.14955154061317444,
            1.6300349235534668,
            -0.7729222774505615,
            2.58608341217041,
            1.7276400327682495,
            -2.5156617164611816,
            -3.9424147605895996,
            2.5774686336517334,
            1.4373347759246826,
            2.357360363006592,
            -2.9565622806549072,
            -2.2688493728637695,
            -5.119738578796387,
            -1.8979530334472656,
            -1.7425780296325684,
            1.1519291400909424,
            2.581371545791626,
            -0.09022489190101624,
            5.723694801330566,
            3.5279908180236816,
            -4.655412197113037,
            -1.7400915622711182,
            4.658599853515625,
            0.5275746583938599,
            -2.1468019485473633,
            1.2298334836959839,
            -4.952762603759766,
            -3.340914249420166,
            -0.8240872621536255,
            -2.642941951751709,
            0.7283503413200378,
            -0.1033041775226593,
            3.021230697631836,
            4.894770622253418,
            1.1666978597640991,
            0.593682050704956,
            0.15213990211486816,
            5.353826522827148,
            3.825157880783081,
            4.770778656005859,
            -0.36912113428115845,
            1.4307122230529785,
            2.189945697784424,
            0.06769254803657532,
            -2.1007747650146484,
            2.562887668609619,
            -3.348005533218384,
            5.649369239807129,
            -1.851630687713623,
            1.6875821352005005,
            -1.0686521530151367,
            1.1363273859024048,
            0.345673143863678,
            -0.3840782344341278,
            1.9114264249801636,
            1.0827763080596924,
            1.6581882238388062,
            4.699687957763672,
            -3.8689627647399902,
            3.579923629760742,
            1.631335735321045,
            2.9961445331573486,
            0.9778894782066345,
            1.860925316810608,
            -3.227010488510132,
            -3.3894853591918945,
            1.8656692504882812,
            -6.165990829467773,
            -1.7084147930145264,
            0.38871997594833374,
            2.9624269008636475,
            1.3237996101379395,
            -1.5224502086639404,
            -2.822331428527832,
            1.3610039949417114,
            -2.61289119720459,
            -2.7265453338623047,
            2.8706860542297363,
            -0.0472109317779541,
            -0.08767002820968628,
            1.2310175895690918,
            2.0018234252929688,
            -0.9571644067764282,
            -3.481510877609253,
            0.2558433413505554,
            -2.0821595191955566,
            2.0721018314361572,
            -0.8872908353805542,
            0.8071264624595642,
            0.8289369344711304,
            -2.5560145378112793,
            0.11575008928775787,
            3.9493184089660645,
            0.7991562485694885,
            1.4477943181991577,
            -5.609111309051514,
            -2.1503517627716064,
            -2.8954734802246094,
            5.0015549659729,
            -1.603848934173584,
            -1.955819845199585,
            5.896492958068848,
            0.25294363498687744,
            2.541602611541748,
            2.591109037399292,
            3.091201066970825,
            -1.3903722763061523,
            -0.6729199290275574,
            4.253255844116211,
            -0.11481112241744995,
            2.826871871948242,
            -0.7754018902778625,
            -3.4871602058410645,
            1.5377336740493774,
            3.574174404144287,
            0.7752050757408142,
            -1.9729373455047607,
            -4.962192058563232,
            -3.592959403991699,
            -1.9449753761291504,
            -3.665593147277832,
            3.1392323970794678,
            2.104069471359253,
            1.6938259601593018,
            -3.750814437866211,
            1.045528531074524,
            3.1343424320220947,
            3.347442626953125,
            -6.720111846923828,
            0.6735687851905823,
            0.9521068334579468,
            3.91340708732605,
            3.622243881225586,
            -2.5576748847961426,
            -0.3589809536933899,
            3.7125117778778076,
            -5.307210922241211,
            2.0321078300476074,
            -0.4229373335838318,
            1.0873730182647705,
            0.5760666131973267,
            2.469817638397217,
            -4.966843605041504,
            2.328784942626953,
            1.105257272720337,
            8.652518272399902,
            6.012092113494873,
            5.945772171020508,
            0.5433343648910522,
            -3.570680618286133,
            -2.087376356124878,
            -2.4079995155334473,
            3.076387643814087,
            3.045358657836914,
            -1.91092848777771,
            -2.3851962089538574,
            -2.6933670043945312,
            0.4445831775665283,
            0.3625103831291199,
            2.6582388877868652,
            -5.078534126281738,
            3.4577081203460693,
            -4.79332160949707,
            -2.6568922996520996,
            -3.5905542373657227,
            2.479006767272949,
            -0.6692865490913391,
            -1.2986788749694824,
            1.2709400653839111,
            -1.0507065057754517,
            -5.742525577545166,
            -1.9291341304779053,
            -2.4148483276367188,
            -1.018450379371643,
            -0.7117547988891602,
            4.404412269592285,
            1.2132480144500732,
            0.1688627004623413,
            -0.2560664415359497,
            2.688039779663086,
            -2.9720749855041504,
            -1.1504669189453125,
            -2.247124671936035,
            2.5480785369873047,
            0.7793991565704346,
            -1.7034066915512085,
            -0.3611757755279541,
            -1.9428746700286865,
            0.694320797920227,
            2.4151995182037354,
            2.7344164848327637,
            1.422638177871704,
            3.9745919704437256,
            0.10488912463188171,
            -0.6762399673461914,
            -3.5284652709960938,
            -2.645021438598633,
            -2.9429872035980225,
            -2.586500406265259,
            -4.930798053741455,
            0.6775297522544861,
            -2.5891189575195312,
            -1.6331143379211426,
            3.997847557067871,
            -3.7406063079833984,
            2.0709786415100098,
            -0.2421143651008606,
            -4.274044513702393,
            -2.2077322006225586,
            -3.227510452270508,
            -0.7860053777694702,
            -2.6466503143310547,
            4.1266069412231445,
            -3.1861538887023926,
            0.29891499876976013,
            0.9706352949142456,
            1.2910219430923462,
            6.247452735900879,
            5.262426376342773,
            1.0440125465393066,
            -5.528840065002441,
            -0.25094762444496155,
            1.8021690845489502,
            0.21245905756950378,
            -0.17251993715763092,
            -0.327819287776947,
            2.3418562412261963,
            1.5465543270111084,
            15.07718276977539,
            -0.04522264003753662,
            -0.5652159452438354,
            -1.920163631439209,
            0.5315127372741699,
            -4.236932754516602,
            -0.4230915904045105,
            3.08556866645813,
            1.4353728294372559,
            3.3408710956573486,
            -0.6282883882522583,
            -3.5767765045166016,
            1.3823752403259277,
            3.781636953353882,
            -3.1676273345947266,
            -0.605329155921936,
            -2.534097194671631,
            0.7281794548034668,
            -2.6385138034820557,
            1.8977454900741577,
            -1.8182452917099,
            1.4267061948776245,
            -0.8806744813919067,
            -1.6378812789916992,
            -1.1097646951675415,
            1.5746586322784424,
            3.882012128829956,
            2.0514116287231445,
            -4.287786483764648,
            1.005907654762268,
            1.0593626499176025,
            3.035109043121338,
            0.9490182399749756,
            -0.20925158262252808,
            -3.3808412551879883,
            1.226820707321167,
            5.927042007446289,
            1.0402871370315552,
            1.5204079151153564,
            1.250125765800476,
            -3.2080907821655273,
            1.5604279041290283,
            -2.6856842041015625,
            1.0259490013122559,
            -1.674297571182251,
            1.26412832736969,
            2.329092502593994,
            -3.0816445350646973,
            -1.735727071762085,
            4.4232563972473145,
            -2.83147931098938,
            0.6662474870681763,
            -3.244320869445801,
            -1.747327208518982,
            0.3277926743030548,
            3.103757381439209,
            -2.376373052597046,
            -2.6264190673828125,
            3.3660378456115723,
            -1.2983484268188477,
            2.4268088340759277,
            0.6083747148513794,
            -3.4727835655212402,
            -2.890113115310669,
            -2.775289297103882,
            0.2074052095413208,
            -3.5819010734558105,
            2.579212188720703,
            3.4733026027679443,
            2.077653646469116,
            5.844571113586426,
            0.6739754676818848,
            0.9363219141960144,
            -3.433539390563965,
            -1.3094321489334106,
            -3.3324661254882812,
            2.907144546508789,
            0.0409853458404541,
            -1.664391040802002,
            8.929120063781738,
            -3.816305160522461,
            2.836683511734009,
            -2.5659613609313965,
            -1.1621043682098389,
            6.135197639465332,
            -2.775460958480835,
            3.4593684673309326,
            -0.8792086243629456,
            -0.664685845375061,
            4.785181045532227,
            -0.810491144657135,
            -1.4658560752868652,
            4.029383659362793,
            2.55080509185791,
            5.077401638031006,
            -4.125373363494873,
            -3.121548652648926,
            -4.439907550811768,
            -3.7809109687805176,
            -4.25150728225708,
            6.459117889404297,
            4.4222731590271,
            1.2174890041351318,
            -2.918912887573242,
            0.06133157014846802,
            -0.16423387825489044,
            -0.7045932412147522,
            -5.334384918212891,
            -5.340801239013672,
            -2.195599317550659,
            2.9142513275146484,
            -3.585015296936035,
            -0.5297198295593262,
            1.0629674196243286,
            -0.4957202672958374,
            -2.4172825813293457,
            0.8762935996055603,
            -0.19126227498054504,
            0.222966268658638,
            2.965148448944092,
            -1.184609055519104,
            1.358595371246338,
            -2.266353130340576,
            -2.698603630065918,
            -0.8610821962356567,
            2.4402716159820557,
            0.148212730884552,
            -0.6850476264953613,
            -1.7107634544372559,
            -1.1223211288452148,
            -0.46906381845474243,
            -2.0817959308624268,
            -0.07445108890533447,
            2.54135799407959,
            -2.35837721824646,
            -3.0110116004943848,
            -0.1813984513282776,
            0.2768552899360657,
            -2.990645408630371,
            2.3345251083374023,
            3.8722243309020996,
            -2.6921892166137695,
            -2.8272972106933594,
            9.537233352661133,
            -0.23407509922981262,
            1.3247736692428589,
            -2.4687604904174805,
            -3.620145320892334,
            -3.133087635040283,
            -1.3995281457901,
            -1.1674858331680298,
            -1.8077359199523926,
            2.650238037109375,
            -0.7163133025169373,
            -0.5218284726142883,
            -3.5300025939941406
        ]
    },
    "authors": [
        {
            "authorId": "2109586102",
            "name": "Sehoon Kim"
        },
        {
            "authorId": "2029486869",
            "name": "Coleman Hooper"
        },
        {
            "authorId": "10419477",
            "name": "A. Gholami"
        },
        {
            "authorId": "143879884",
            "name": "Zhen Dong"
        },
        {
            "authorId": "2141625113",
            "name": "Xiuyu Li"
        },
        {
            "authorId": "2191455",
            "name": "Sheng Shen"
        },
        {
            "authorId": "1717098",
            "name": "Michael W. Mahoney"
        },
        {
            "authorId": "1732330",
            "name": "K. Keutzer"
        }
    ],
    "references": [
        {
            "paperId": "4c14b1c41cb0aaa68f5d3f4a432f55e7199657ea",
            "title": "AI and Memory Wall"
        },
        {
            "paperId": "2209dd35db8098b6c80caeda705f75339f141e22",
            "title": "Extreme Compression of Large Language Models via Additive Quantization"
        },
        {
            "paperId": "eb2c2330177f765038a2b17e2ee3498965865797",
            "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
        },
        {
            "paperId": "08b45a6920fd5ba5701a42b9e3441758365b699c",
            "title": "QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection"
        },
        {
            "paperId": "56b828717f32251a5e0f0be9c0113077f23c8429",
            "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df",
            "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"
        },
        {
            "paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9",
            "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
        },
        {
            "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
        },
        {
            "paperId": "2a44c6b7f291f625314a82ba3131e605009fd533",
            "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models"
        },
        {
            "paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e",
            "title": "Full Stack Optimization of Transformer Inference: a Survey"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "489ab1945feb21f17b3efbcf40726c8cbb52bb75",
            "title": "Q-Diffusion: Quantizing Diffusion Models"
        },
        {
            "paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c",
            "title": "Accelerating Large Language Model Decoding with Speculative Sampling"
        },
        {
            "paperId": "8b87d39baf53d982bad7df8ab6c5c8e67c124c67",
            "title": "NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "9b069ba5259d229bfd4fe3ac3768148e2d1092f8",
            "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention"
        },
        {
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
        },
        {
            "paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6",
            "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
        },
        {
            "paperId": "3f6243097a58e386aea1215fed4f372dee07a100",
            "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586",
            "title": "Confident Adaptive Language Modeling"
        },
        {
            "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
            "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
        },
        {
            "paperId": "c7d549ca504090312344a6b11274210a349d0464",
            "title": "Mr.BiQ: Post-Training Non-Uniform Quantization based on Minimizing the Reconstruction Error"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0",
            "title": "A Fast Post-Training Pruning Framework for Transformers"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77",
            "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"
        },
        {
            "paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19",
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
        },
        {
            "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
            "title": "LaMDA: Language Models for Dialog Applications"
        },
        {
            "paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14",
            "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"
        },
        {
            "paperId": "5f895e84c1fea75de07b4f90da518273c2e57291",
            "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"
        },
        {
            "paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
            "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
        },
        {
            "paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
            "title": "Primer: Searching for Efficient Transformers for Language Modeling"
        },
        {
            "paperId": "ef18db2a18ac61e72783a613328842ce86ef00bf",
            "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"
        },
        {
            "paperId": "774295eec4466aff34247e6ba0e06682817d7ded",
            "title": "Post-Training Sparsity-Aware Quantization"
        },
        {
            "paperId": "5a09edeb26f9f116f2c0503cd020f38fb943f79b",
            "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers"
        },
        {
            "paperId": "dd0a27aa2285bc64798fa76944400ab6d9ce3025",
            "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"
        },
        {
            "paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e",
            "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
        },
        {
            "paperId": "de983239063bf87fafc549e651ea133088b1831f",
            "title": "Hessian-Aware Pruning and Optimal Neural Implant"
        },
        {
            "paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8",
            "title": "I-BERT: Integer-only BERT Quantization"
        },
        {
            "paperId": "c375e121926db9551f224ff235018ea38bb159b7",
            "title": "BinaryBERT: Pushing the Limit of BERT Quantization"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "6dbe18f41615ec60b10e4698a0c82fd8c0b05f5a",
            "title": "HAWQV3: Dyadic Neural Network Quantization"
        },
        {
            "paperId": "097210dc65924f8ce59523faf444e635523dc714",
            "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"
        },
        {
            "paperId": "755f16fdbbfb0b128adb3cda9c8c2799aea34290",
            "title": "Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation"
        },
        {
            "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
            "title": "Measuring Massive Multitask Language Understanding"
        },
        {
            "paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69",
            "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"
        },
        {
            "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
            "title": "Linformer: Self-Attention with Linear Complexity"
        },
        {
            "paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
            "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"
        },
        {
            "paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e",
            "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"
        },
        {
            "paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa",
            "title": "Lite Transformer with Long-Short Range Attention"
        },
        {
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
        },
        {
            "paperId": "54d4ff8d536b292149a4fa017c22349cf4e54ce4",
            "title": "AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search"
        },
        {
            "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "title": "Reformer: The Efficient Transformer"
        },
        {
            "paperId": "0acc25f3993bd9431418ae275aa12a536b740b77",
            "title": "ZeroQ: A Novel Zero Shot Quantization Framework"
        },
        {
            "paperId": "6e5d89c2b3b5ead2c3ab389534de62a28c1e8e6e",
            "title": "PyHessian: Neural Networks Through the Lens of the Hessian"
        },
        {
            "paperId": "0a5d987ddb5463062babceca90ba974db0cf96e7",
            "title": "HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583",
            "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
        },
        {
            "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
            "title": "Are Sixteen Heads Really Better than One?"
        },
        {
            "paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d",
            "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"
        },
        {
            "paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217",
            "title": "The State of Sparsity in Deep Neural Networks"
        },
        {
            "paperId": "a9a4a12900d0ac1063505bb28d5e0180d78a39a0",
            "title": "Non-Monotonic Sequential Text Generation"
        },
        {
            "paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
            "title": "The Evolved Transformer"
        },
        {
            "paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235",
            "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"
        },
        {
            "paperId": "3cd3f1585ced02cbb56a9e1428176a6c2b211da2",
            "title": "Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss"
        },
        {
            "paperId": "a8e1b91b0940a539aca302fb4e5c1f098e4e3860",
            "title": "LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks"
        },
        {
            "paperId": "09f8f3d6a721f1d0e4e9f01831aaafc05cf79461",
            "title": "Deep Neural Network Compression with Single and Multiple Level Quantization"
        },
        {
            "paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454",
            "title": "Non-Autoregressive Neural Machine Translation"
        },
        {
            "paperId": "e41c96b0d36c89f6dd1f21ccb80c7cc19af6a1cf",
            "title": "Balanced CSR Sparse Matrix-Vector Product on Graphics Processors"
        },
        {
            "paperId": "6d749712ef4f02a87f4e362058469a1da46c8bcc",
            "title": "Towards the Limit of Network Quantization"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "d3cb9bad655197b52932978dd8186b36c512bf92",
            "title": "Quantized Convolutional Neural Networks for Mobile Devices"
        },
        {
            "paperId": "642d0f49b7826adcf986616f4af77e736229990f",
            "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
        },
        {
            "paperId": "e7bf9803705f2eb608db1e59e5c7636a3f171916",
            "title": "Compressing Deep Convolutional Networks using Vector Quantization"
        },
        {
            "paperId": "843a1567b056c8a1d0deddc8b699e1725194f85c",
            "title": "Latency lags bandwith"
        },
        {
            "paperId": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724",
            "title": "Optimal Brain Surgeon and general network pruning"
        },
        {
            "paperId": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516",
            "title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"
        },
        {
            "paperId": "81051b830a4f5606106765902a51ba281c9230f9",
            "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"
        },
        {
            "paperId": "ccf15b75d3ed3287c0ac524666578ed785bff1a3",
            "title": "Big Little Transformer Decoder"
        },
        {
            "paperId": null,
            "title": "Post-Training Quantization (PTQ) does not involve retraining Zhao et al"
        },
        {
            "paperId": null,
            "title": "Stanford alpaca: An instruction-following llama model"
        },
        {
            "paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312",
            "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "d9a67e3aa1fbd5f07f8f6f8554f193ba36c5abe7",
            "title": "Non-uniform Step Size Quantization for Accurate Post-training Quantization"
        },
        {
            "paperId": null,
            "title": "KevinWang,andAndyZou"
        },
        {
            "paperId": null,
            "title": "A framework for few-shot language model evaluation"
        },
        {
            "paperId": null,
            "title": "Sparse Matrix-Vector Multiplication with CUDA"
        },
        {
            "paperId": "f8b1b43f284f1246ca015cc002ac949bb67c5645",
            "title": "Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        },
        {
            "paperId": null,
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
        },
        {
            "paperId": "84d35c48c1ee6a00b78b5a07daba47a3dc9d0e51",
            "title": "Output Sensitivity-Aware DETR Quantization"
        },
        {
            "paperId": null,
            "title": "arXiv preprint"
        },
        {
            "paperId": null,
            "title": "SqueezeLLM (0.45%) 3.24 (4.94) 5.73 3.63 4.26 (3.76) 5.57 3.39 encoder-only or encoder-decoder architectures,"
        },
        {
            "paperId": null,
            "title": "Grouping Dense-and-Sparse + Grouping Dense-and-Sparse"
        },
        {
            "paperId": null,
            "title": "Koala: A dialogue model for academic research. Blog post"
        }
    ]
}