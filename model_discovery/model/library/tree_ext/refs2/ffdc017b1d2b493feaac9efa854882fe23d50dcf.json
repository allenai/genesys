{
    "paperId": "ffdc017b1d2b493feaac9efa854882fe23d50dcf",
    "externalIds": {
        "ArXiv": "2310.08041",
        "DBLP": "conf/iclr/LiuGWD0Z24",
        "DOI": "10.48550/arXiv.2310.08041",
        "CorpusId": 263908852
    },
    "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models",
    "abstract": "Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 72,
    "citationCount": 30,
    "influentialCitationCount": 2,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2310.08041",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "QLLM is proposed, an accurate and efficient low-bitwidth PTQ method designed for LLMs that introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.8897180557250977,
            1.1465368270874023,
            -3.2875282764434814,
            6.508429050445557,
            -1.678896188735962,
            0.8857605457305908,
            2.8625874519348145,
            0.84219890832901,
            -1.9022047519683838,
            -0.5118234157562256,
            -3.611032724380493,
            2.3339333534240723,
            -0.5899724364280701,
            -0.49906203150749207,
            -4.277569770812988,
            -1.5492799282073975,
            -1.8683781623840332,
            -0.19353856146335602,
            6.589329719543457,
            2.629974126815796,
            -3.900531768798828,
            0.503292441368103,
            -2.5731124877929688,
            1.5813555717468262,
            -0.7496864199638367,
            -0.20846304297447205,
            -1.144385576248169,
            1.3376250267028809,
            -2.791301727294922,
            -1.7616863250732422,
            -2.206303358078003,
            -3.641826629638672,
            6.4653143882751465,
            -3.5904855728149414,
            4.563538551330566,
            -2.206878423690796,
            -0.014408886432647705,
            7.531844139099121,
            -2.7082455158233643,
            -0.6970934867858887,
            1.045806646347046,
            -3.540025472640991,
            -0.0008347108960151672,
            -1.2777798175811768,
            -0.9771323204040527,
            2.4148435592651367,
            3.1025619506835938,
            3.445512056350708,
            -0.3408060669898987,
            2.3484129905700684,
            3.236776351928711,
            1.9137459993362427,
            1.892730474472046,
            1.661512851715088,
            -2.518005609512329,
            -3.374335289001465,
            0.16352280974388123,
            1.876581072807312,
            1.5525219440460205,
            -3.6945958137512207,
            3.1540517807006836,
            2.3467488288879395,
            -1.1872259378433228,
            3.5373897552490234,
            1.0409387350082397,
            -2.1591548919677734,
            0.33335059881210327,
            4.070187091827393,
            1.216494083404541,
            -1.1063460111618042,
            1.2398091554641724,
            -3.66086483001709,
            2.77380108833313,
            0.18796733021736145,
            -3.5767605304718018,
            -1.6811925172805786,
            -0.022194862365722656,
            -6.174875736236572,
            -0.8510778546333313,
            -3.3783841133117676,
            0.8769461512565613,
            1.9969137907028198,
            2.130704641342163,
            -1.0069942474365234,
            3.1854374408721924,
            -2.5682382583618164,
            -3.2019400596618652,
            1.4755260944366455,
            3.0656285285949707,
            -1.3279939889907837,
            1.8694084882736206,
            4.4643940925598145,
            -2.3828306198120117,
            2.4946837425231934,
            -0.9741306304931641,
            0.5600968599319458,
            -0.2573825716972351,
            -0.5760449171066284,
            -1.0209672451019287,
            1.1153650283813477,
            -0.7173203229904175,
            -2.5031580924987793,
            0.9516217708587646,
            1.0927300453186035,
            1.6261473894119263,
            -5.305031776428223,
            -0.8601901531219482,
            0.718722939491272,
            -0.3850058913230896,
            -2.9943885803222656,
            -0.10127118229866028,
            1.2913707494735718,
            -1.9023587703704834,
            -4.004909992218018,
            -3.5549325942993164,
            -3.5720393657684326,
            -1.626908779144287,
            -1.9235340356826782,
            0.12084926664829254,
            5.607006549835205,
            -1.4700027704238892,
            -2.9674277305603027,
            -3.188075542449951,
            0.257086843252182,
            3.7066802978515625,
            4.915025234222412,
            0.9340097308158875,
            -0.9606378078460693,
            -2.6534135341644287,
            -2.8013393878936768,
            -0.08047369122505188,
            -0.8347660303115845,
            3.4959511756896973,
            -1.5132224559783936,
            7.990041255950928,
            0.9827827215194702,
            -5.576119899749756,
            1.6233415603637695,
            -0.9615530967712402,
            -2.032681465148926,
            3.37520432472229,
            3.1709861755371094,
            1.5047030448913574,
            2.897223472595215,
            2.507936477661133,
            -0.4375762939453125,
            0.4813421368598938,
            0.3668351173400879,
            0.4106302857398987,
            6.838616847991943,
            4.518556594848633,
            -3.2180709838867188,
            -0.34085574746131897,
            0.25995752215385437,
            -0.3800733685493469,
            5.5401225090026855,
            -2.4338598251342773,
            2.839205741882324,
            0.32984426617622375,
            1.1029484272003174,
            -1.202638864517212,
            -0.012488996610045433,
            -6.915717124938965,
            -3.5976099967956543,
            3.6943812370300293,
            -2.225527286529541,
            -1.6666563749313354,
            1.8637733459472656,
            -2.045492649078369,
            3.198636054992676,
            0.3187493085861206,
            3.497558355331421,
            1.296642541885376,
            2.044860363006592,
            1.7390093803405762,
            5.01966667175293,
            4.252809524536133,
            -3.8192288875579834,
            -3.0786211490631104,
            1.3933730125427246,
            -0.1823466718196869,
            1.2322801351547241,
            -4.881443023681641,
            1.7224891185760498,
            -3.668300151824951,
            -0.5632538199424744,
            -3.1744658946990967,
            1.5828883647918701,
            -3.5527520179748535,
            0.9688659906387329,
            -2.3981800079345703,
            -0.067055344581604,
            3.570115327835083,
            4.8717041015625,
            1.9379100799560547,
            -0.586217999458313,
            2.860856771469116,
            3.3987905979156494,
            -2.8539693355560303,
            1.7683098316192627,
            4.813042640686035,
            0.7755732536315918,
            -3.3255906105041504,
            -2.837045669555664,
            3.646959066390991,
            1.8888217210769653,
            -5.38117790222168,
            2.799464702606201,
            0.006518453359603882,
            0.5287096500396729,
            0.6737240552902222,
            0.04868173599243164,
            -1.1991233825683594,
            -0.8162267208099365,
            -2.5690269470214844,
            -2.5724408626556396,
            -5.742773056030273,
            2.8857924938201904,
            3.6086912155151367,
            3.253488063812256,
            -0.3808854818344116,
            1.280788540840149,
            -1.4233572483062744,
            -6.669866561889648,
            3.4527738094329834,
            -0.5042257308959961,
            1.3633265495300293,
            -0.30988597869873047,
            1.6334530115127563,
            3.353304862976074,
            1.5528512001037598,
            -3.025426149368286,
            -0.8414077162742615,
            -0.5715106129646301,
            -4.387038707733154,
            -0.7143896818161011,
            -4.759067058563232,
            2.2500104904174805,
            -3.890465259552002,
            0.38668709993362427,
            5.173569202423096,
            1.5553510189056396,
            0.36433473229408264,
            0.7540470957756042,
            1.2076270580291748,
            -0.5522684454917908,
            -2.0523681640625,
            0.056678250432014465,
            -2.007382392883301,
            -3.291142463684082,
            -1.4699492454528809,
            -1.1535885334014893,
            1.6788533926010132,
            -4.621527671813965,
            1.7768901586532593,
            1.7251875400543213,
            0.13696573674678802,
            1.848622441291809,
            1.269033432006836,
            3.644927501678467,
            -0.36418741941452026,
            2.2293307781219482,
            1.3174545764923096,
            2.5658178329467773,
            -0.3782539963722229,
            -4.000445365905762,
            -4.931694984436035,
            -0.5211227536201477,
            -0.5651413202285767,
            0.5733788013458252,
            3.4620375633239746,
            1.602359652519226,
            0.758086621761322,
            -4.893645763397217,
            -3.8009135723114014,
            -7.186945915222168,
            0.7309303879737854,
            0.47201746702194214,
            0.8489347100257874,
            2.7254867553710938,
            3.1529672145843506,
            -3.8128914833068848,
            -1.9720369577407837,
            -1.7326929569244385,
            -2.1210227012634277,
            -1.5463881492614746,
            0.9408147931098938,
            -1.1257505416870117,
            -2.4870285987854004,
            -3.5857434272766113,
            -3.345672607421875,
            1.2359721660614014,
            -2.1666483879089355,
            -1.6194825172424316,
            -4.000286102294922,
            -0.23685744404792786,
            5.206716537475586,
            -0.8496618866920471,
            -1.3489503860473633,
            1.0047223567962646,
            0.2303382158279419,
            3.2311978340148926,
            4.793996810913086,
            -0.5415934920310974,
            -0.07349938154220581,
            1.923314094543457,
            0.5367202162742615,
            -4.0807719230651855,
            -0.01235400140285492,
            -3.8722012042999268,
            -0.20954203605651855,
            -1.3186452388763428,
            4.656148910522461,
            -2.418424367904663,
            -1.0114589929580688,
            -2.0755374431610107,
            5.143850803375244,
            1.3869099617004395,
            -3.7432138919830322,
            2.6604886054992676,
            1.6850727796554565,
            2.4199624061584473,
            -7.472406387329102,
            -0.40624672174453735,
            -4.329188346862793,
            -2.545532464981079,
            1.5476871728897095,
            4.315899848937988,
            -3.745487689971924,
            1.6070153713226318,
            -2.3063223361968994,
            6.6018805503845215,
            5.951969146728516,
            3.0391311645507812,
            -2.803703784942627,
            -4.62398624420166,
            -0.6622798442840576,
            -1.620079517364502,
            1.407997965812683,
            -3.5087194442749023,
            0.06093865633010864,
            3.36323618888855,
            0.6757687926292419,
            3.3239083290100098,
            -2.490410566329956,
            -0.04833388328552246,
            3.7775659561157227,
            -0.04871596395969391,
            -0.1179165244102478,
            -1.351860761642456,
            1.2555370330810547,
            -0.2920600473880768,
            3.221312999725342,
            -0.029333055019378662,
            1.3839755058288574,
            2.075697898864746,
            1.3988909721374512,
            -0.3596099615097046,
            3.9528067111968994,
            2.067619562149048,
            -0.48641929030418396,
            1.2280113697052002,
            2.6956756114959717,
            -3.330578327178955,
            1.3096027374267578,
            -3.4937219619750977,
            11.33049488067627,
            -1.9776813983917236,
            2.9807772636413574,
            -3.4919044971466064,
            -1.8484351634979248,
            -0.539635181427002,
            -4.30905294418335,
            0.4802740812301636,
            -2.1946210861206055,
            -0.27811694145202637,
            -0.9389244914054871,
            -5.274593353271484,
            -0.4647679030895233,
            1.7856518030166626,
            2.5623598098754883,
            1.7012507915496826,
            1.2419453859329224,
            1.475294828414917,
            -0.6297697424888611,
            1.3469116687774658,
            1.6934692859649658,
            1.1323796510696411,
            2.3013579845428467,
            -1.6980195045471191,
            -2.807664394378662,
            1.594894528388977,
            1.444805383682251,
            6.00067138671875,
            -2.415626287460327,
            -2.0151147842407227,
            -5.833654403686523,
            -4.646230220794678,
            1.3562355041503906,
            2.2866146564483643,
            2.561610221862793,
            -0.5188803672790527,
            5.122845649719238,
            4.373799800872803,
            -3.5155138969421387,
            1.8015207052230835,
            2.343790292739868,
            -1.350724697113037,
            0.23789191246032715,
            1.811692476272583,
            -5.175036907196045,
            -2.736724376678467,
            -2.8777387142181396,
            -5.855342864990234,
            1.2236518859863281,
            -1.634061574935913,
            2.222344398498535,
            3.010699987411499,
            4.162935256958008,
            3.6952404975891113,
            -1.985500693321228,
            2.347233533859253,
            3.4082889556884766,
            2.5852813720703125,
            -0.7149698734283447,
            0.43477606773376465,
            5.055840015411377,
            2.0265910625457764,
            0.6971160173416138,
            1.2302825450897217,
            -0.8272304534912109,
            3.334428548812866,
            -1.1573688983917236,
            -0.33754217624664307,
            -2.537050724029541,
            3.864131212234497,
            2.5404891967773438,
            0.18457436561584473,
            0.7061269283294678,
            0.9174326658248901,
            0.7499911189079285,
            3.4264895915985107,
            0.4307519495487213,
            2.4868316650390625,
            -0.5844348669052124,
            2.783355712890625,
            2.077336072921753,
            0.8780398368835449,
            -1.0549534559249878,
            -4.37501335144043,
            2.2735800743103027,
            -3.648062229156494,
            1.254774570465088,
            -1.0456442832946777,
            1.2332717180252075,
            0.5931684970855713,
            -2.5637645721435547,
            0.6227812767028809,
            0.4341698884963989,
            -3.7453999519348145,
            -3.6825337409973145,
            2.136340618133545,
            0.053877830505371094,
            -0.7982911467552185,
            0.5629556775093079,
            2.8508920669555664,
            -1.7687726020812988,
            -4.303799152374268,
            -0.7422298789024353,
            1.5048021078109741,
            1.4916303157806396,
            -0.7422699332237244,
            -2.7480690479278564,
            2.882248878479004,
            -3.9285056591033936,
            -0.9743630886077881,
            4.1801981925964355,
            2.6087639331817627,
            0.2750607430934906,
            -5.707716464996338,
            -3.0238311290740967,
            0.39796093106269836,
            7.809236526489258,
            -2.6717238426208496,
            -0.7529842257499695,
            4.1252641677856445,
            1.38760244846344,
            3.74320125579834,
            3.153932571411133,
            1.1065518856048584,
            -1.494266152381897,
            -0.14784157276153564,
            4.133713722229004,
            -1.4577895402908325,
            2.698674440383911,
            0.24386203289031982,
            -5.979403495788574,
            1.9789659976959229,
            1.5693550109863281,
            -0.26400870084762573,
            -4.153824806213379,
            -5.465328216552734,
            -2.439110040664673,
            -0.01529867947101593,
            -3.736314058303833,
            2.723532199859619,
            2.588149070739746,
            1.7065049409866333,
            -1.0132098197937012,
            0.6155292987823486,
            4.120523452758789,
            2.431532382965088,
            -6.457871913909912,
            2.6314103603363037,
            0.27430060505867004,
            1.877572774887085,
            0.48558562994003296,
            0.2689790725708008,
            -1.068543791770935,
            3.2692902088165283,
            -3.5573906898498535,
            -1.563906192779541,
            -3.907813310623169,
            2.6341209411621094,
            0.7451077699661255,
            1.4157614707946777,
            -2.528550148010254,
            0.1512763649225235,
            2.2163169384002686,
            4.9682512283325195,
            8.148658752441406,
            4.826044082641602,
            -0.5360224843025208,
            -2.797852039337158,
            0.21360230445861816,
            0.1911698579788208,
            3.108060836791992,
            3.7313711643218994,
            -4.627696990966797,
            -0.780082643032074,
            -2.2257251739501953,
            0.6503810882568359,
            1.2355883121490479,
            3.2352890968322754,
            -0.34753888845443726,
            1.8858795166015625,
            -3.961310386657715,
            -2.15562105178833,
            -4.337326526641846,
            1.3841044902801514,
            0.2721118628978729,
            -0.37069976329803467,
            1.3495326042175293,
            -1.6067616939544678,
            -1.2653878927230835,
            -3.5541746616363525,
            -0.1588519811630249,
            -0.9076789617538452,
            -1.521489143371582,
            6.599411964416504,
            1.1117427349090576,
            -0.7488527894020081,
            0.8063153624534607,
            1.5982091426849365,
            -1.7328840494155884,
            -2.320000410079956,
            -2.140983819961548,
            0.8618106842041016,
            -0.09253552556037903,
            -0.7110015749931335,
            0.7322182655334473,
            -4.772488117218018,
            3.275726318359375,
            1.5618181228637695,
            0.9907642602920532,
            0.7157707214355469,
            3.8975534439086914,
            -1.3457837104797363,
            -0.9115036725997925,
            -2.5285158157348633,
            -1.7318699359893799,
            -2.689378261566162,
            -1.0972093343734741,
            -5.0245361328125,
            1.2844908237457275,
            -2.4297752380371094,
            -2.2525484561920166,
            2.4067304134368896,
            -1.9672009944915771,
            4.1236162185668945,
            1.5435800552368164,
            -3.3269214630126953,
            -3.2799320220947266,
            -0.6821197867393494,
            1.1276620626449585,
            -2.034848690032959,
            2.2169554233551025,
            -1.7601184844970703,
            -0.5630995631217957,
            0.3539160490036011,
            -0.473486065864563,
            5.497639179229736,
            4.746289253234863,
            1.4747717380523682,
            -4.186263561248779,
            -0.013277880847454071,
            1.4711997509002686,
            0.12564188241958618,
            -1.4022716283798218,
            -0.6107461452484131,
            1.5195822715759277,
            3.6990771293640137,
            16.340087890625,
            -1.5809383392333984,
            -2.3417062759399414,
            -0.6379573345184326,
            1.1210529804229736,
            -1.3702776432037354,
            -2.9045674800872803,
            1.9934654235839844,
            -1.3896024227142334,
            1.155832052230835,
            -2.252699375152588,
            0.5367460250854492,
            0.6551096439361572,
            1.2136446237564087,
            -4.993131160736084,
            1.83567214012146,
            -4.272296905517578,
            3.2837653160095215,
            -2.517939805984497,
            1.0228296518325806,
            2.67425537109375,
            3.4845213890075684,
            -1.8130767345428467,
            -3.090991497039795,
            -2.145575523376465,
            2.0884790420532227,
            1.0220472812652588,
            1.2949292659759521,
            -2.9758834838867188,
            -1.6602627038955688,
            0.03431081771850586,
            0.28510546684265137,
            0.7705740332603455,
            -1.0396649837493896,
            0.06915853172540665,
            2.169344186782837,
            2.8514652252197266,
            -1.130859375,
            3.6440601348876953,
            3.503150224685669,
            -1.5286586284637451,
            2.584085464477539,
            -3.565093517303467,
            0.33181411027908325,
            -2.674997091293335,
            0.1496604084968567,
            2.6667380332946777,
            -5.5248894691467285,
            -3.0952398777008057,
            -0.5467521548271179,
            -0.9366697669029236,
            1.714958906173706,
            -1.3219135999679565,
            -1.036380410194397,
            -0.26221054792404175,
            2.6935431957244873,
            0.24508732557296753,
            -1.4020755290985107,
            3.0920112133026123,
            0.9605650305747986,
            -0.5808663964271545,
            -0.011874139308929443,
            -1.7452882528305054,
            -1.7843208312988281,
            0.24019795656204224,
            2.115356922149658,
            -1.5076216459274292,
            3.652665615081787,
            1.5621024370193481,
            1.2866251468658447,
            2.8433141708374023,
            0.9622489213943481,
            1.9629710912704468,
            -0.5746461153030396,
            -2.2645654678344727,
            -2.865184783935547,
            2.564793348312378,
            -0.935684323310852,
            -3.3890113830566406,
            6.663672924041748,
            -0.621606707572937,
            1.1716687679290771,
            1.9090492725372314,
            -1.6955361366271973,
            6.672982215881348,
            -3.239870071411133,
            5.1013503074646,
            -0.929325520992279,
            -0.7558254599571228,
            2.833847761154175,
            -1.7830629348754883,
            -1.7629024982452393,
            2.795271396636963,
            3.8920164108276367,
            2.3246419429779053,
            -2.744138717651367,
            -3.954113006591797,
            -5.531096458435059,
            -3.4245781898498535,
            -2.603592872619629,
            4.732850551605225,
            4.121607780456543,
            2.192093849182129,
            -0.7296406030654907,
            -0.8460121154785156,
            -0.23796352744102478,
            -1.5672225952148438,
            -1.8047165870666504,
            -2.564157009124756,
            -0.5012962818145752,
            2.9775781631469727,
            -4.26859188079834,
            -1.2693836688995361,
            -0.8255985975265503,
            3.2724721431732178,
            -2.004024028778076,
            -0.37755435705184937,
            2.4001035690307617,
            -0.43221908807754517,
            4.111703395843506,
            -1.5793683528900146,
            -2.246368646621704,
            -1.8304184675216675,
            -1.9552947282791138,
            0.5964394807815552,
            1.3136167526245117,
            -1.7242201566696167,
            -1.4457151889801025,
            -1.3742315769195557,
            -0.17519253492355347,
            0.478674054145813,
            -1.8756210803985596,
            -0.7767500877380371,
            2.1973612308502197,
            -0.23160362243652344,
            -1.5815880298614502,
            1.3082222938537598,
            1.2014448642730713,
            -2.7217187881469727,
            -0.207035094499588,
            4.292892932891846,
            -3.692772388458252,
            -3.960155487060547,
            8.618697166442871,
            -1.436107873916626,
            -2.684342622756958,
            -2.304896116256714,
            -1.5794897079467773,
            -1.36423659324646,
            1.0803894996643066,
            -1.8063479661941528,
            1.0999131202697754,
            3.1777141094207764,
            0.11366456747055054,
            4.206745624542236,
            -3.383183002471924
        ]
    },
    "authors": [
        {
            "authorId": "49270464",
            "name": "Jing Liu"
        },
        {
            "authorId": "152217579",
            "name": "Ruihao Gong"
        },
        {
            "authorId": "14457026",
            "name": "Xiuying Wei"
        },
        {
            "authorId": "2257809681",
            "name": "Zhiwei Dong"
        },
        {
            "authorId": "2249720327",
            "name": "Jianfei Cai"
        },
        {
            "authorId": "3194022",
            "name": "Bohan Zhuang"
        }
    ],
    "references": [
        {
            "paperId": "3cf409c5dc261c3c495c4cca091e674adcaa5081",
            "title": "Towards End-to-end 4-Bit Inference on Generative Large Language Models"
        },
        {
            "paperId": "633e3fe49fe9c314f7245f77401c2e4a95e925a9",
            "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"
        },
        {
            "paperId": "eb2c2330177f765038a2b17e2ee3498965865797",
            "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
        },
        {
            "paperId": "56b828717f32251a5e0f0be9c0113077f23c8429",
            "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"
        },
        {
            "paperId": "aeb9454987c3f85563cf7a5d2cb7f3d502d3398d",
            "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6",
            "title": "A Simple and Effective Pruning Approach for Large Language Models"
        },
        {
            "paperId": "f7fb316c78dc4a4e413803daf6d4d80bb40140dd",
            "title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation"
        },
        {
            "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
            "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
        },
        {
            "paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df",
            "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"
        },
        {
            "paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9",
            "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
        },
        {
            "paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2",
            "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"
        },
        {
            "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
        },
        {
            "paperId": "a10843d1349fff8d2a7d9722f800802187fef67f",
            "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"
        },
        {
            "paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200",
            "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"
        },
        {
            "paperId": "017010b941d902a467f6d329ae5e74fd67e67912",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"
        },
        {
            "paperId": "2a44c6b7f291f625314a82ba3131e605009fd533",
            "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models"
        },
        {
            "paperId": "6518091d7f2af10629b836df8c7a53ce4104c4fb",
            "title": "Token Merging for Fast Stable Diffusion"
        },
        {
            "paperId": "8f48c75e1354c88a84a67abb60789083c12e5037",
            "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f",
            "title": "Token Merging: Your ViT But Faster"
        },
        {
            "paperId": "dfdb2894d50e095ce97f994ed6cee38554c4c84f",
            "title": "Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer"
        },
        {
            "paperId": "3f6243097a58e386aea1215fed4f372dee07a100",
            "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "5eeb828685e44ca5b8ebafb34a9fa4d51c9186df",
            "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models"
        },
        {
            "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
            "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
        },
        {
            "paperId": "edfea69d9cd1a4d40f4d879aa36f93ad7d26a659",
            "title": "QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization"
        },
        {
            "paperId": "60bc501b98ff5bfcc464c3b1e19e5459e5f6feff",
            "title": "Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation"
        },
        {
            "paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
            "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
        },
        {
            "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "title": "LoRA: Low-Rank Adaptation of Large Language Models"
        },
        {
            "paperId": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3",
            "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"
        },
        {
            "paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8",
            "title": "I-BERT: Integer-only BERT Quantization"
        },
        {
            "paperId": "0df7b7c74df6a041e068e0021816b8a911e4e6e2",
            "title": "Differentiable Joint Pruning and Quantization for Hardware Efficiency"
        },
        {
            "paperId": "1f58885f410b20e1e9dc850c8e967a960ccafee4",
            "title": "EasyQuant: Post-training Quantization via Scale Optimization"
        },
        {
            "paperId": "31a68965ea4af87295cb83d8fe72e31f651a4ee1",
            "title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "0c0dfe47afcec2e229015f3c8f213d4c88e86b28",
            "title": "Up or Down? Adaptive Rounding for Post-Training Quantization"
        },
        {
            "paperId": "8cc855854384755336aa05c5376ea455137bfbce",
            "title": "LSQ+: Improving low-bit quantization through learnable offsets and better initialization"
        },
        {
            "paperId": "e9fd2dd3852cd9f628ada316bc6c134800576fff",
            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Inference"
        },
        {
            "paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
            "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f",
            "title": "Triton: an intermediate language and compiler for tiled neural network computations"
        },
        {
            "paperId": "d77123b54dcc8014949584ab624e97298617bcad",
            "title": "Data-Free Quantization Through Weight Equalization and Bias Correction"
        },
        {
            "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
            "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
        },
        {
            "paperId": "79e523beb1e1411a241edde0464b07c2ebc231d1",
            "title": "Single Path One-Shot Neural Architecture Search with Uniform Sampling"
        },
        {
            "paperId": "dc160709bbe528b506a37ead334f60d258413357",
            "title": "Learned Step Size Quantization"
        },
        {
            "paperId": "47a1edfb88f5b4a7ba1e9f6aed327f67f942f6d6",
            "title": "Low-bit Quantization of Neural Networks for Efficient Inference"
        },
        {
            "paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235",
            "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"
        },
        {
            "paperId": "f789425a7af1d012675118d7d10cd50afad09074",
            "title": "Post training 4-bit quantization of convolutional networks for rapid-deployment"
        },
        {
            "paperId": "3cd3f1585ced02cbb56a9e1428176a6c2b211da2",
            "title": "Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss"
        },
        {
            "paperId": "a8e1b91b0940a539aca302fb4e5c1f098e4e3860",
            "title": "LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks"
        },
        {
            "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
            "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
        },
        {
            "paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294",
            "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "ee53c9480132fc0d09b1192226cb2c460462fd6d",
            "title": "Channel Pruning for Accelerating Very Deep Neural Networks"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7",
            "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"
        },
        {
            "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
            "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
        },
        {
            "paperId": "6757659aeba247db2a35691ee3b4c029e1a2dcf4",
            "title": "Kernel Fusion: An Effective Method for Better Power Efficiency on Multithreaded GPU"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "81051b830a4f5606106765902a51ba281c9230f9",
            "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"
        },
        {
            "paperId": "363668677c459ebc0ff494655f993a93a0251009",
            "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"
        },
        {
            "paperId": "343d24c4dcfaff2132373d218561a23fbd53e934",
            "title": "OWQ: Lessons learned from activation outliers for weight quantization in large language models"
        },
        {
            "paperId": null,
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
        },
        {
            "paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312",
            "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": null,
            "title": "A framework for few-shot language model evaluation"
        },
        {
            "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
            "title": "An Adversarial Winograd Schema Challenge at Scale"
        },
        {
            "paperId": "c3cb27f9ef7176658f37b607e75cc2c37f5e0ea8",
            "title": "Accurate and Efficient 2-bit Quantized Neural Networks"
        },
        {
            "paperId": "015ca32bca81dbda1e2e432445eef798582236e1",
            "title": "Conference Paper"
        }
    ]
}