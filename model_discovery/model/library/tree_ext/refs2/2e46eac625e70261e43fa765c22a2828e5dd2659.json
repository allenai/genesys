{
    "paperId": "2e46eac625e70261e43fa765c22a2828e5dd2659",
    "externalIds": {
        "MAG": "2969262604",
        "DBLP": "conf/intellisys/Hu19",
        "ArXiv": "1811.05544",
        "DOI": "10.1007/978-3-030-29513-4_31",
        "CorpusId": 53305352
    },
    "title": "An Introductory Survey on Attention Mechanisms in NLP Problems",
    "abstract": null,
    "venue": "Intelligent Systems with Applications",
    "year": 2018,
    "referenceCount": 36,
    "citationCount": 222,
    "influentialCitationCount": 4,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An introductory summary of the attention mechanism in different NLP problems is conducted, aiming to provide basic knowledge on this widely used method, to discuss its different variants for different tasks, explore its association with other techniques in machine learning, and examine methods for evaluating its performance."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "49592393",
            "name": "Dichao Hu"
        }
    ],
    "references": [
        {
            "paperId": "c0507e8d32d408c99af4f2bb0e658d06efa0fd48",
            "title": "Topic Memory Networks for Short Text Classification"
        },
        {
            "paperId": "d330aea1ec2a8de80e0e8670dabb68bbecfe1bb6",
            "title": "Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction"
        },
        {
            "paperId": "b959b2314927973f74de996c00ca987bddc4b53b",
            "title": "MARS: Memory Attention-Aware Recommender System"
        },
        {
            "paperId": "d44fdde76605fddd1c411f4aa13126b65cd98bb5",
            "title": "Dynamic Meta-Embeddings for Improved Sentence Representations"
        },
        {
            "paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
            "title": "Self-Attention with Relative Position Representations"
        },
        {
            "paperId": "adc276e6eae7051a027a4c269fb21dae43cadfed",
            "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding"
        },
        {
            "paperId": "b0e404fd1adaa0fa7b1ef12b4b828db3d497ab1c",
            "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling"
        },
        {
            "paperId": "f804f268b214241ccc35efb6928058ce13271c50",
            "title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction"
        },
        {
            "paperId": "dd12bf052dcc2d0d397120723864498b0c3f31e9",
            "title": "Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms"
        },
        {
            "paperId": "c624c38e53f321a6df2d16bd707499ce744ca114",
            "title": "Abstractive Document Summarization with a Graph-Based Attentional Neural Model"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
            "title": "A Structured Self-attentive Sentence Embedding"
        },
        {
            "paperId": "786d1ca11bfd95d8c845ef4a90b2d55092c2fb2f",
            "title": "Coupled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms"
        },
        {
            "paperId": "13d9323a8716131911bfda048a40e2cde1a76a46",
            "title": "Structured Attention Networks"
        },
        {
            "paperId": "e2dba792360873aef125572812f3673b1a85d850",
            "title": "Enriching Word Vectors with Subword Information"
        },
        {
            "paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2",
            "title": "Hierarchical Attention Networks for Document Classification"
        },
        {
            "paperId": "bba5f2852b1db8a18004eb7328efa5e1d57cc62a",
            "title": "Key-Value Memory Networks for Directly Reading Documents"
        },
        {
            "paperId": "0f2ea810c16275dc74e880296e20dbd83b1bae1c",
            "title": "Gated-Attention Readers for Text Comprehension"
        },
        {
            "paperId": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194",
            "title": "Dynamic Memory Networks for Visual and Textual Question Answering"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
            "title": "Effective Approaches to Attention-based Neural Machine Translation"
        },
        {
            "paperId": "452059171226626718eb677358836328f884298e",
            "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"
        },
        {
            "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "title": "End-To-End Memory Networks"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de",
            "title": "Neural Turing Machines"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7",
            "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "944a1cfd79dbfb6fef460360a0765ba790f4027a",
            "title": "Recurrent Continuous Translation Models"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "eb82d3035849cd23578096462ba419b53198a556",
            "title": "The PageRank Citation Ranking : Bringing Order to the Web"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": null,
            "title": "End-toend memory networks. In Advances in neural information processing"
        },
        {
            "paperId": null,
            "title": "Explore how attention is associated with other concepts or techniques in machine learning, such as pre-training and ensemble"
        },
        {
            "paperId": null,
            "title": "Discuss different variants based on the special task they are dealing with"
        }
    ]
}