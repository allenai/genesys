{
    "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
    "externalIds": {
        "DBLP": "journals/corr/abs-2206-01861",
        "ArXiv": "2206.01861",
        "DOI": "10.48550/arXiv.2206.01861",
        "CorpusId": 249395624
    },
    "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers",
    "abstract": "How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "referenceCount": 85,
    "citationCount": 280,
    "influentialCitationCount": 34,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2206.01861",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work is able to show that ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.7032670974731445,
            -0.5794639587402344,
            -0.759428083896637,
            7.0904998779296875,
            -2.4431354999542236,
            2.5767152309417725,
            3.2275476455688477,
            -0.9790171980857849,
            0.10236763954162598,
            -1.3081836700439453,
            -3.4013407230377197,
            2.076221227645874,
            1.058916449546814,
            0.4370400011539459,
            -5.810959815979004,
            -3.3777780532836914,
            0.40932518243789673,
            3.4984865188598633,
            6.907993793487549,
            2.1230251789093018,
            -3.663130760192871,
            -0.12800246477127075,
            -3.8887789249420166,
            3.9884958267211914,
            -2.284698009490967,
            0.31263071298599243,
            -1.587199330329895,
            -0.8120332360267639,
            -1.9190913438796997,
            -0.6769657135009766,
            0.8450835943222046,
            -2.848123550415039,
            3.378744602203369,
            -3.087625026702881,
            4.987955570220947,
            -2.503443479537964,
            -1.5421916246414185,
            7.496359825134277,
            -5.7744269371032715,
            -0.029222339391708374,
            2.3042330741882324,
            -2.114637613296509,
            0.5088696479797363,
            0.5899420380592346,
            -2.273224353790283,
            -0.02599397301673889,
            3.3633780479431152,
            -0.41166800260543823,
            -1.6283082962036133,
            -0.48198702931404114,
            1.6737459897994995,
            2.89827036857605,
            0.47531527280807495,
            -0.46548357605934143,
            -2.0607895851135254,
            -2.8847506046295166,
            -0.7197738885879517,
            2.356816291809082,
            0.5392276048660278,
            -3.9535272121429443,
            2.181948184967041,
            2.2993812561035156,
            1.1373666524887085,
            1.3159151077270508,
            2.567518711090088,
            -1.7745777368545532,
            -0.8518275618553162,
            3.6601245403289795,
            -0.8035562038421631,
            -1.64736807346344,
            -1.1896957159042358,
            -3.5985546112060547,
            -0.06463044881820679,
            0.13166099786758423,
            -4.465788841247559,
            -1.1347450017929077,
            1.9578660726547241,
            -3.7361223697662354,
            0.7890363931655884,
            -4.262465953826904,
            -1.0563278198242188,
            4.103658676147461,
            -1.3991336822509766,
            0.2947869896888733,
            4.592842102050781,
            -0.493596613407135,
            -3.432492971420288,
            -0.8277082443237305,
            0.7888680696487427,
            -2.775583267211914,
            4.12190055847168,
            -0.6027615070343018,
            -0.06953603029251099,
            1.3338812589645386,
            -2.4709219932556152,
            -0.5006024837493896,
            0.4320933222770691,
            -0.9273629188537598,
            -2.4136199951171875,
            3.7291312217712402,
            0.0240786075592041,
            0.07246343791484833,
            -0.06398147344589233,
            -0.0789867639541626,
            2.4283199310302734,
            -4.122319221496582,
            -4.235014915466309,
            1.2819335460662842,
            -0.6724433898925781,
            -3.133504867553711,
            -2.3326985836029053,
            3.9235939979553223,
            -0.6635011434555054,
            -2.190185070037842,
            -4.2547454833984375,
            -2.0391030311584473,
            -0.4154002070426941,
            -1.0779149532318115,
            -2.808833122253418,
            3.4859540462493896,
            0.8656798005104065,
            -0.19218730926513672,
            -5.274195194244385,
            -0.9536570906639099,
            2.1887705326080322,
            0.8000251650810242,
            -0.9047442674636841,
            0.179155170917511,
            -1.0626722574234009,
            -2.569200038909912,
            -0.40497273206710815,
            -1.1403037309646606,
            4.642674922943115,
            -0.8458113074302673,
            4.972960948944092,
            0.02871537208557129,
            -2.779632806777954,
            1.3366292715072632,
            -4.25465726852417,
            0.6046352386474609,
            2.521522283554077,
            1.783752202987671,
            0.8474022150039673,
            3.062995672225952,
            3.4648568630218506,
            5.598992347717285,
            0.720291793346405,
            4.0471649169921875,
            -1.5694282054901123,
            6.03731632232666,
            6.862936019897461,
            -6.218971252441406,
            0.47453540563583374,
            2.234159469604492,
            0.8481680154800415,
            4.877188682556152,
            -5.879932403564453,
            2.7244629859924316,
            1.7843711376190186,
            0.6097346544265747,
            3.2369801998138428,
            0.43086934089660645,
            -8.434438705444336,
            0.3933297097682953,
            6.054786682128906,
            -1.0197635889053345,
            -2.1562936305999756,
            0.36242228746414185,
            -1.9216400384902954,
            1.755139708518982,
            0.11550116539001465,
            4.318454742431641,
            4.95054292678833,
            1.9710516929626465,
            5.500794887542725,
            7.002345561981201,
            4.354009628295898,
            -0.6183009147644043,
            0.40814948081970215,
            -1.520200490951538,
            -0.8817192316055298,
            1.3056515455245972,
            -4.236576080322266,
            2.2045609951019287,
            -5.999052047729492,
            -1.6274125576019287,
            -4.02913236618042,
            -0.9801109433174133,
            -2.6030826568603516,
            1.4041858911514282,
            0.231905996799469,
            -1.9216350317001343,
            2.4564619064331055,
            3.9281206130981445,
            0.016575157642364502,
            -0.7099765539169312,
            1.4615192413330078,
            4.919411659240723,
            -3.85976505279541,
            2.594144582748413,
            6.370856761932373,
            -0.18407395482063293,
            -5.194018363952637,
            -1.426360845565796,
            4.946592807769775,
            2.215714693069458,
            -2.9619016647338867,
            3.7738871574401855,
            1.8484933376312256,
            1.0412943363189697,
            -1.683485746383667,
            -1.2949962615966797,
            0.8126137852668762,
            1.2621784210205078,
            -2.3586783409118652,
            -2.1437535285949707,
            -4.293474197387695,
            2.8988046646118164,
            5.454133987426758,
            2.875993251800537,
            0.7397407293319702,
            -1.0448639392852783,
            -1.4838297367095947,
            -4.294028282165527,
            -0.36446595191955566,
            -3.616739511489868,
            2.2073678970336914,
            0.9877449870109558,
            0.7197219133377075,
            2.2305240631103516,
            -0.997516393661499,
            -7.33113956451416,
            1.5443270206451416,
            -0.8845741152763367,
            -6.713807106018066,
            -2.4973647594451904,
            -1.982957363128662,
            2.1045031547546387,
            -0.3756152391433716,
            0.6281856894493103,
            6.356688022613525,
            0.9457657337188721,
            1.905164122581482,
            1.6649397611618042,
            2.6231677532196045,
            -1.9802769422531128,
            -3.7388951778411865,
            0.162053644657135,
            -1.050727367401123,
            -3.426673412322998,
            -2.472651958465576,
            -2.4794161319732666,
            2.5506181716918945,
            -5.412677764892578,
            1.8619704246520996,
            0.9586074352264404,
            3.6551239490509033,
            -4.81132698059082,
            1.4109082221984863,
            1.0182831287384033,
            -3.0250587463378906,
            4.109050750732422,
            4.276748180389404,
            5.189244747161865,
            -1.966174602508545,
            -1.9626131057739258,
            -5.41786527633667,
            -1.9578932523727417,
            -0.5534849762916565,
            0.3627637028694153,
            2.463456153869629,
            -0.4035504162311554,
            -1.3384723663330078,
            -1.0589019060134888,
            -4.341294288635254,
            -6.433897018432617,
            -0.8146393299102783,
            0.5435466766357422,
            1.9489498138427734,
            6.141820430755615,
            0.8609347343444824,
            -3.0388731956481934,
            0.36343950033187866,
            -1.37859046459198,
            -0.7133033275604248,
            1.0681071281433105,
            0.08444041013717651,
            -2.141368865966797,
            -1.823669672012329,
            0.7006958723068237,
            -5.311483383178711,
            2.338221311569214,
            -3.3650827407836914,
            -2.482543468475342,
            -4.107980251312256,
            2.8440842628479004,
            5.5231170654296875,
            -1.0389130115509033,
            -0.7087942361831665,
            1.351902723312378,
            0.2914595901966095,
            3.46399188041687,
            4.428731441497803,
            -1.6078495979309082,
            3.5987465381622314,
            4.16412353515625,
            4.26090145111084,
            -2.7792270183563232,
            2.0930230617523193,
            -3.5566258430480957,
            -0.7847826480865479,
            -1.2349886894226074,
            2.787686824798584,
            -4.005504608154297,
            2.2197978496551514,
            1.0718188285827637,
            3.8726038932800293,
            -0.6797187328338623,
            -1.5495487451553345,
            3.664769172668457,
            0.5409767031669617,
            1.1893935203552246,
            -6.495580673217773,
            -1.7895301580429077,
            -2.5606331825256348,
            -3.2187204360961914,
            4.892114162445068,
            1.587618350982666,
            -4.825253486633301,
            2.0708696842193604,
            -0.8674359321594238,
            4.380511283874512,
            7.553991317749023,
            2.88564395904541,
            1.026432752609253,
            -4.483631610870361,
            2.0764167308807373,
            -1.193597674369812,
            -1.862805962562561,
            -1.9806386232376099,
            -2.4957852363586426,
            4.324080467224121,
            -3.497565269470215,
            5.309985637664795,
            -1.360396146774292,
            0.3314909338951111,
            3.232466697692871,
            -2.5018343925476074,
            -0.12106180191040039,
            -1.3388376235961914,
            -1.4793702363967896,
            -0.013501167297363281,
            2.3027822971343994,
            -0.43548017740249634,
            -0.600017786026001,
            2.5971004962921143,
            1.9720252752304077,
            0.6046102046966553,
            3.6190547943115234,
            0.2747996747493744,
            1.9464491605758667,
            0.5839941501617432,
            1.3367657661437988,
            -1.7244261503219604,
            -0.18386277556419373,
            -0.07673592865467072,
            10.428717613220215,
            0.6856381893157959,
            -1.211769700050354,
            -2.9177260398864746,
            -1.545727014541626,
            -4.971646308898926,
            -0.2903262674808502,
            0.2574790120124817,
            0.19325388967990875,
            0.2463679313659668,
            3.545301675796509,
            -4.241473197937012,
            -0.7256565690040588,
            -0.11392617225646973,
            0.9377552270889282,
            4.023707866668701,
            0.7126585245132446,
            3.833798885345459,
            -1.181382656097412,
            -0.4991857409477234,
            -1.4253767728805542,
            1.6092718839645386,
            1.1155518293380737,
            -2.554990530014038,
            -4.344861030578613,
            2.2152702808380127,
            3.343214511871338,
            2.7138161659240723,
            -4.9894022941589355,
            -4.254444122314453,
            -4.208648204803467,
            -5.720921516418457,
            -1.4142361879348755,
            -0.10487763583660126,
            2.2231717109680176,
            0.9507940411567688,
            3.7994003295898438,
            3.7103331089019775,
            -3.562896966934204,
            -3.000243663787842,
            5.23676872253418,
            0.2787103056907654,
            -1.1801273822784424,
            0.10465839505195618,
            -7.748531341552734,
            -1.387511134147644,
            -2.5363965034484863,
            -4.408262252807617,
            -0.5158035755157471,
            1.4686498641967773,
            1.3434885740280151,
            3.2653632164001465,
            3.907536029815674,
            2.798323392868042,
            -0.14849986135959625,
            1.3584259748458862,
            2.2309350967407227,
            4.050081729888916,
            -1.7121970653533936,
            -0.21400529146194458,
            4.72017240524292,
            2.260490655899048,
            -1.6799087524414062,
            1.245711326599121,
            -2.7604212760925293,
            3.8575210571289062,
            -1.7900186777114868,
            1.7559148073196411,
            -0.10612183809280396,
            2.2951786518096924,
            1.3045682907104492,
            0.5228530168533325,
            0.8882136344909668,
            1.3135572671890259,
            0.1878933608531952,
            4.119679927825928,
            -0.5635251402854919,
            5.0219621658325195,
            0.738760769367218,
            2.696509838104248,
            2.075590133666992,
            0.5041499733924866,
            -2.3161873817443848,
            -2.8496055603027344,
            2.889416456222534,
            -5.514654159545898,
            -2.784297227859497,
            0.5572323799133301,
            3.2542171478271484,
            -0.22290480136871338,
            -2.677544593811035,
            -1.5123674869537354,
            1.1591296195983887,
            -2.1551947593688965,
            -2.7615840435028076,
            3.638741970062256,
            1.907995343208313,
            0.9880555868148804,
            0.44249263405799866,
            0.0851476788520813,
            -1.4057328701019287,
            -4.0383148193359375,
            -1.6215194463729858,
            -0.9942537546157837,
            4.167937755584717,
            -2.26938796043396,
            0.27131569385528564,
            -0.15008336305618286,
            -2.4549617767333984,
            -0.49969372153282166,
            5.050510406494141,
            2.8601105213165283,
            1.1511340141296387,
            -5.820281028747559,
            -0.9392979145050049,
            -0.07176131010055542,
            4.009094715118408,
            -4.260552406311035,
            -2.955148220062256,
            2.5261378288269043,
            4.236663341522217,
            3.47941255569458,
            1.694610595703125,
            4.527453422546387,
            -3.1187572479248047,
            -0.5455795526504517,
            4.593489646911621,
            -1.2725858688354492,
            5.044164657592773,
            0.7404709458351135,
            -3.4098777770996094,
            -0.22813954949378967,
            1.707263469696045,
            -1.4503296613693237,
            -1.3349665403366089,
            -4.196216583251953,
            -1.096015214920044,
            -2.988863945007324,
            -1.7242745161056519,
            2.707688808441162,
            -0.661616325378418,
            3.1619582176208496,
            -4.249260902404785,
            -2.658829689025879,
            2.228846311569214,
            0.6627191305160522,
            -6.210934162139893,
            0.36000192165374756,
            0.9605133533477783,
            4.98295783996582,
            0.9520372748374939,
            -1.6445728540420532,
            1.262133002281189,
            3.8166635036468506,
            -3.1491072177886963,
            1.3691489696502686,
            -0.45802736282348633,
            0.42920544743537903,
            -1.1959023475646973,
            2.8937361240386963,
            -1.8899648189544678,
            4.239840507507324,
            2.4641504287719727,
            4.899974346160889,
            6.235212326049805,
            3.969308853149414,
            -0.8792304992675781,
            -4.2173614501953125,
            -1.8950475454330444,
            -2.94773006439209,
            3.6063694953918457,
            2.4410157203674316,
            -4.9807939529418945,
            -0.005861453711986542,
            -0.6086753606796265,
            0.40556687116622925,
            -0.4774928689002991,
            1.525648832321167,
            -5.455207347869873,
            2.4267327785491943,
            -1.052939534187317,
            1.0693395137786865,
            -4.963382720947266,
            1.33077073097229,
            0.8557674884796143,
            0.06409099698066711,
            2.538804531097412,
            -1.5321385860443115,
            -2.8383431434631348,
            -2.4867117404937744,
            -4.088281154632568,
            0.2208072543144226,
            -1.9301652908325195,
            5.535618782043457,
            -0.20657935738563538,
            1.8054497241973877,
            1.201420783996582,
            3.0935773849487305,
            -4.716453552246094,
            -2.0642006397247314,
            -3.2734124660491943,
            3.0714423656463623,
            1.0063457489013672,
            -2.9809582233428955,
            -0.8991703987121582,
            -3.262150764465332,
            2.244776487350464,
            -0.18412792682647705,
            1.1394346952438354,
            4.2025861740112305,
            4.776663780212402,
            -1.456141471862793,
            -1.7570676803588867,
            -0.4171449542045593,
            -2.473468780517578,
            -4.31427526473999,
            -4.441737174987793,
            -3.53454327583313,
            1.1633169651031494,
            -4.334371566772461,
            -2.341332197189331,
            4.478994846343994,
            -0.7558326721191406,
            0.570591926574707,
            -0.6511638164520264,
            -3.667757511138916,
            -4.271145343780518,
            -2.861344575881958,
            -0.8569908738136292,
            -0.5957481861114502,
            1.844646692276001,
            -1.1411428451538086,
            0.8521102666854858,
            2.2806713581085205,
            3.647616147994995,
            6.966979026794434,
            5.353672981262207,
            2.003652572631836,
            -5.347646236419678,
            0.5472419857978821,
            3.7440850734710693,
            0.5909974575042725,
            -2.1639671325683594,
            -2.6625003814697266,
            -1.577751636505127,
            3.0695502758026123,
            14.337410926818848,
            0.37456363439559937,
            -2.6149256229400635,
            -4.901689052581787,
            1.5778218507766724,
            -2.3687381744384766,
            -2.270371913909912,
            0.37233397364616394,
            -0.2483459711074829,
            0.5928994417190552,
            -0.7488450407981873,
            -0.5485866665840149,
            0.6152597665786743,
            -0.19250519573688507,
            -2.433201789855957,
            0.34145545959472656,
            -1.3016166687011719,
            1.0892573595046997,
            -0.09883460402488708,
            -0.1662713885307312,
            -0.2168877124786377,
            0.3394491672515869,
            0.04155644774436951,
            -0.41964036226272583,
            -2.9300379753112793,
            4.034181594848633,
            1.8797669410705566,
            3.0100488662719727,
            -2.6940183639526367,
            2.6097285747528076,
            -0.003290697932243347,
            1.932936429977417,
            0.5755484700202942,
            1.6364080905914307,
            -2.227720260620117,
            0.5978308320045471,
            5.303133964538574,
            -1.4933233261108398,
            2.3637547492980957,
            3.14968204498291,
            -1.2822370529174805,
            0.9029712677001953,
            -1.1770424842834473,
            -0.9758822917938232,
            -4.375079154968262,
            0.37071800231933594,
            3.7246227264404297,
            -4.971638202667236,
            0.6859135031700134,
            2.2125120162963867,
            -0.10369922965765,
            -0.8348435163497925,
            -2.0206098556518555,
            -1.7194733619689941,
            1.3315048217773438,
            3.3503081798553467,
            -0.4580591917037964,
            -2.1636669635772705,
            4.281764507293701,
            -0.9099903702735901,
            1.2370867729187012,
            -0.25856393575668335,
            -2.8034770488739014,
            -2.924461841583252,
            -0.8891609311103821,
            1.4803330898284912,
            -0.9831373691558838,
            3.0223212242126465,
            3.3077645301818848,
            2.1018080711364746,
            2.8851184844970703,
            -0.20570264756679535,
            1.3546379804611206,
            -3.5811917781829834,
            0.3988211750984192,
            -1.1124589443206787,
            2.8663902282714844,
            -0.058888353407382965,
            -0.6941248178482056,
            8.776338577270508,
            -1.7516685724258423,
            0.5701829195022583,
            1.2402385473251343,
            -2.7929203510284424,
            5.914758205413818,
            -4.730555534362793,
            3.931245803833008,
            0.9150677919387817,
            -1.2275567054748535,
            5.575263023376465,
            -2.2328803539276123,
            -1.6176819801330566,
            2.894469738006592,
            5.215634346008301,
            4.123182773590088,
            -3.8965344429016113,
            -2.5620620250701904,
            -2.9864463806152344,
            -3.3727047443389893,
            -3.2938010692596436,
            5.1906890869140625,
            4.374057769775391,
            -0.44748151302337646,
            -2.0247483253479004,
            -0.374839186668396,
            -0.8230823874473572,
            -2.132033348083496,
            -1.5643017292022705,
            -1.9101076126098633,
            -0.9423229694366455,
            4.450432777404785,
            -3.6954710483551025,
            -0.9991505146026611,
            1.221430778503418,
            -0.006534755229949951,
            -1.9858602285385132,
            0.818764328956604,
            2.386392831802368,
            1.0360065698623657,
            2.7070531845092773,
            -2.4763119220733643,
            -3.092782497406006,
            -2.6872076988220215,
            -2.5522775650024414,
            -0.5651650428771973,
            3.2611000537872314,
            0.9434157609939575,
            -2.1613152027130127,
            1.0688486099243164,
            0.3598540425300598,
            0.5162420272827148,
            -1.681739091873169,
            -1.4001884460449219,
            0.4056735038757324,
            -1.3354434967041016,
            -2.0536389350891113,
            0.9285460710525513,
            2.3200974464416504,
            -3.129138708114624,
            0.29261085391044617,
            3.343600034713745,
            -4.6566267013549805,
            -4.07712984085083,
            5.028357028961182,
            -0.34516459703445435,
            1.5485197305679321,
            -1.8342187404632568,
            -2.5343692302703857,
            -0.5783314108848572,
            -0.5163871645927429,
            -3.617952823638916,
            -0.35896164178848267,
            4.852431297302246,
            -1.3786674737930298,
            2.5693306922912598,
            -0.7403509616851807
        ]
    },
    "authors": [
        {
            "authorId": "9088433",
            "name": "Z. Yao"
        },
        {
            "authorId": "3394222",
            "name": "Reza Yazdani Aminabadi"
        },
        {
            "authorId": "67016465",
            "name": "Minjia Zhang"
        },
        {
            "authorId": "2129511744",
            "name": "Xiaoxia Wu"
        },
        {
            "authorId": "2609325",
            "name": "Conglong Li"
        },
        {
            "authorId": "2145020341",
            "name": "Yuxiong He"
        }
    ],
    "references": [
        {
            "paperId": "4247f45a5730e3bda5836e2bc7941e30f5b91cb7",
            "title": "Board"
        },
        {
            "paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1",
            "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af",
            "title": "Compression of Generative Pre-trained Language Models via Quantization"
        },
        {
            "paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19",
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
        },
        {
            "paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
            "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
        },
        {
            "paperId": "c295391129426d89ec58cebb049d1cd2e976deec",
            "title": "Post-Training Quantization for Vision Transformer"
        },
        {
            "paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e",
            "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
        },
        {
            "paperId": "2a0ae7182b13789056e13dc1887904c923a92675",
            "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization"
        },
        {
            "paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8",
            "title": "I-BERT: Integer-only BERT Quantization"
        },
        {
            "paperId": "c375e121926db9551f224ff235018ea38bb159b7",
            "title": "BinaryBERT: Pushing the Limit of BERT Quantization"
        },
        {
            "paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
        },
        {
            "paperId": "10e824b010c2125b13decef30cf1dceec6196817",
            "title": "ANLIzing the Adversarial Natural Language Inference Dataset"
        },
        {
            "paperId": "097210dc65924f8ce59523faf444e635523dc714",
            "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"
        },
        {
            "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
            "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e",
            "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"
        },
        {
            "paperId": "0c0dfe47afcec2e229015f3c8f213d4c88e86b28",
            "title": "Up or Down? Adaptive Rounding for Post-Training Quantization"
        },
        {
            "paperId": "c9d3c181d999b0e11c6e4c51b3f9aefd01489e0f",
            "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation"
        },
        {
            "paperId": "37127a02d129cb733377458d2f155997e3fd622f",
            "title": "Training with Quantization Noise for Extreme Fixed-Point Compression"
        },
        {
            "paperId": "159dc82a5ee901716b0154051988b5408acfc861",
            "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression"
        },
        {
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
        },
        {
            "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
            "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
        },
        {
            "paperId": "57f123c95ecf9d901be3a53291f53302740451e2",
            "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"
        },
        {
            "paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d",
            "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"
        },
        {
            "paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5",
            "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"
        },
        {
            "paperId": "0acc25f3993bd9431418ae275aa12a536b740b77",
            "title": "ZeroQ: A Novel Zero Shot Quantization Framework"
        },
        {
            "paperId": "547334370d2b03d51a39a1509fe7e164cd30e550",
            "title": "Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "c95383f251a62c63217586059c67f63507c3e839",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression"
        },
        {
            "paperId": "d77123b54dcc8014949584ab624e97298617bcad",
            "title": "Data-Free Quantization Through Weight Equalization and Bias Correction"
        },
        {
            "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
            "title": "BERT Rediscovers the Classical NLP Pipeline"
        },
        {
            "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
            "title": "Are Sixteen Heads Really Better than One?"
        },
        {
            "paperId": "9770fff7379a7ab9006b48939462354dda9a2053",
            "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"
        },
        {
            "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
            "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
        },
        {
            "paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d",
            "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"
        },
        {
            "paperId": "dc160709bbe528b506a37ead334f60d258413357",
            "title": "Learned Step Size Quantization"
        },
        {
            "paperId": "a5b66ee341cb990f7f70a124b5fab3316d3b7e27",
            "title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"
        },
        {
            "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
            "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
        },
        {
            "paperId": "2344cca985dd4e2e2519838b2353b5c295e73036",
            "title": "A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset"
        },
        {
            "paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb",
            "title": "Neural Network Acceptability Judgments"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "48cfb687fa140be30e6c61775e83f77ea391fa3f",
            "title": "COPA: Constrained PARAFAC2 for Sparse & Large Datasets"
        },
        {
            "paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096",
            "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "6a821cb17b30c26218e3eb5c20d609dc04a47bcb",
            "title": "Exploring the Regularity of Sparse Structure in Convolutional Neural Networks"
        },
        {
            "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
            "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
        },
        {
            "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
        },
        {
            "paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8",
            "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
            "title": "Pruning Filters for Efficient ConvNets"
        },
        {
            "paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a",
            "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"
        },
        {
            "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
            "paperId": "10a2482088e469dd40f49bdc9978b292b3f7bb1f",
            "title": "Ternary Weight Networks"
        },
        {
            "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "title": "Learning both Weights and Connections for Efficient Neural Network"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "71b4ca7440a06094f58a4dace1a455ad6430bead",
            "title": "Recognizing Textual Entailment: Models and Applications Ido Dagan1, Dan Roth2, Mark Sammons2, and Fabio Massimo Zanzotto3 (1Bar-Ilan University, Israel, 2University of Illinois, Urbana, IL, and 3University of Rome \u201cTor Vergata,\u201d Italy) Morgan & Claypool (Synthesis Lectures on Human Language Technolo"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3",
            "title": "Semantic Parsing on Freebase from Question-Answer Pairs"
        },
        {
            "paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f",
            "title": "The Winograd Schema Challenge"
        },
        {
            "paperId": "6757659aeba247db2a35691ee3b4c029e1a2dcf4",
            "title": "Kernel Fusion: An Effective Method for Better Power Efficiency on Multithreaded GPU"
        },
        {
            "paperId": "dd01291bcb89b3332e6fae969f9a15256325f004",
            "title": "PiQA: an algebra for querying protein data sets"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": null,
            "title": "Using tensor cores in cuda fortran"
        },
        {
            "paperId": "a74b9cc3b08d2a088d6f0b0c037188ccfd1ceaf4",
            "title": "MLPruning: A Multilevel Structured Pruning Framework for Transformer-based Models"
        },
        {
            "paperId": null,
            "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021"
        },
        {
            "paperId": null,
            "title": "GPT-NeoX: Large scale autoregressive language modeling in pytorch, 2021"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
            "title": "An Adversarial Winograd Schema Challenge at Scale"
        },
        {
            "paperId": null,
            "title": "CUTLASS: Fast Linear Algebra in CUDA C++"
        },
        {
            "paperId": null,
            "title": "URL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs"
        },
        {
            "paperId": "475354f10798f110d34792b6d88f31d6d5cb099e",
            "title": "Automatically Constructing a Corpus of Sentential Paraphrases"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        },
        {
            "paperId": null,
            "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]"
        },
        {
            "paperId": null,
            "title": "b) Did you describe any potential participant risks, with links to Institutional Review"
        },
        {
            "paperId": null,
            "title": "b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes"
        },
        {
            "paperId": null,
            "title": "Full Zero-shot Evaluation of GPT-3-style Models includes all zero-shot evaluation results in this section for all GPT-3-style models"
        }
    ]
}