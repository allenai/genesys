{
    "paperId": "40c115c43adee6bc7a00ffd444ee9c045360d97d",
    "externalIds": {
        "ArXiv": "2110.06914",
        "DBLP": "conf/iclr/00050A22",
        "CorpusId": 238743865
    },
    "title": "What Happens after SGD Reaches Zero Loss? -A Mathematical Framework",
    "abstract": "Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function $L$ can form a manifold. Intuitively, with a sufficiently small learning rate $\\eta$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such a regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, $\\mathrm{tr}[\\nabla^2 L]$. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold -- i.e., the\"implicit bias\"-- using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a global analysis of the implicit bias valid for $\\eta^{-2}$ steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for $\\eta^{-1.6}$ steps and (2) allowing arbitrary noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires $O(\\kappa\\ln d)$ samples for learning an $\\kappa$-sparse overparametrized linear model in $\\mathbb{R}^d$ (Woodworth et al., 2020), while GD initialized in the kernel regime requires $\\Omega(d)$ samples. This upper bound is minimax optimal and improves the previous $\\tilde{O}(\\kappa^2)$ upper bound (HaoChen et al., 2020).",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "referenceCount": 85,
    "citationCount": 82,
    "influentialCitationCount": 15,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper gives a general framework for analysis of the implicit bias of Stochastic Gradient Descent using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -4.06296443939209,
            -0.7513248920440674,
            -0.4410116672515869,
            2.6691126823425293,
            -2.7093405723571777,
            0.2835998833179474,
            4.330431938171387,
            -5.239507675170898,
            -0.28974002599716187,
            -3.1233644485473633,
            -3.8569562435150146,
            2.792393207550049,
            -0.21682456135749817,
            0.27410653233528137,
            -3.6090850830078125,
            2.1064376831054688,
            -3.0723390579223633,
            -0.4942162334918976,
            2.5845279693603516,
            0.8956359624862671,
            -3.085421323776245,
            2.428710460662842,
            -3.7081894874572754,
            -2.434685707092285,
            -3.203486919403076,
            0.915755569934845,
            1.5221168994903564,
            -1.0668296813964844,
            -2.689382553100586,
            -3.009636402130127,
            -0.7749789953231812,
            -2.8549232482910156,
            5.291165351867676,
            -2.1153640747070312,
            2.255831003189087,
            -0.8957285284996033,
            3.4271397590637207,
            8.132954597473145,
            -3.057929515838623,
            1.5156714916229248,
            -1.2131459712982178,
            1.7865957021713257,
            0.546011745929718,
            0.6744422316551208,
            1.2058824300765991,
            -2.891263484954834,
            2.784464120864868,
            1.4969544410705566,
            -1.612154483795166,
            2.588507652282715,
            1.0602028369903564,
            -0.18064069747924805,
            1.4981108903884888,
            5.241960525512695,
            2.010612964630127,
            -0.5744078755378723,
            -0.16961674392223358,
            1.1961555480957031,
            0.8218638896942139,
            -4.016097068786621,
            1.65934419631958,
            3.5837512016296387,
            0.00833401083946228,
            0.005993133410811424,
            -0.21073591709136963,
            -3.728905439376831,
            -2.0035271644592285,
            5.336440563201904,
            2.5254569053649902,
            2.218958616256714,
            -0.7418811321258545,
            -2.856372594833374,
            1.3185374736785889,
            -0.25324928760528564,
            -2.3577780723571777,
            3.04805326461792,
            0.06603431701660156,
            -5.554323196411133,
            -2.053375720977783,
            0.3946864604949951,
            0.018964260816574097,
            3.692875385284424,
            0.08445033431053162,
            -0.57572340965271,
            2.826610565185547,
            -0.1581491231918335,
            -2.1026687622070312,
            -1.4261263608932495,
            1.1819323301315308,
            -1.5828032493591309,
            1.169729232788086,
            2.4408700466156006,
            1.497620940208435,
            1.587533950805664,
            -5.369575500488281,
            2.3223745822906494,
            0.6944637894630432,
            1.0733517408370972,
            -1.6286530494689941,
            1.3843460083007812,
            3.3788695335388184,
            0.6420915126800537,
            3.3419747352600098,
            -1.510524034500122,
            2.0093045234680176,
            -2.294445753097534,
            0.17839038372039795,
            0.3194844126701355,
            2.272170066833496,
            -2.918212890625,
            1.1041197776794434,
            1.82112455368042,
            -1.9040279388427734,
            -2.1058568954467773,
            0.20681172609329224,
            2.791745662689209,
            -1.4700000286102295,
            -1.665938377380371,
            -0.42958593368530273,
            4.119579315185547,
            -0.46058934926986694,
            -2.708552837371826,
            -3.750671863555908,
            2.3108558654785156,
            -0.8256053924560547,
            3.0767579078674316,
            -1.3691837787628174,
            1.526354193687439,
            -3.278207540512085,
            -3.887540340423584,
            0.6869503259658813,
            -2.7625932693481445,
            2.78220534324646,
            -1.536123514175415,
            6.562039375305176,
            3.103257179260254,
            -1.5025991201400757,
            -0.29967382550239563,
            -2.167078971862793,
            -1.8918612003326416,
            0.41372960805892944,
            1.7006721496582031,
            -0.8222561478614807,
            0.4761037826538086,
            3.186955690383911,
            3.5405242443084717,
            -0.9543548226356506,
            4.720560550689697,
            1.4235007762908936,
            5.4085869789123535,
            4.347604274749756,
            -3.501049041748047,
            4.025779724121094,
            -1.0635321140289307,
            3.591165542602539,
            2.748539447784424,
            -6.875160217285156,
            1.4973090887069702,
            0.5445088148117065,
            0.49814772605895996,
            0.4809441566467285,
            -1.942443609237671,
            -10.599424362182617,
            0.10922831296920776,
            3.1459310054779053,
            -1.1011240482330322,
            -0.9413518905639648,
            2.7976481914520264,
            -0.37380409240722656,
            0.12511610984802246,
            1.586772084236145,
            1.5162864923477173,
            0.9949783086776733,
            2.3947837352752686,
            2.28132963180542,
            3.1138858795166016,
            3.7716782093048096,
            -2.3934078216552734,
            2.015038251876831,
            1.5844192504882812,
            -1.5189529657363892,
            -1.3714370727539062,
            -6.749112129211426,
            -0.02787870168685913,
            -2.160479784011841,
            -2.8226213455200195,
            -2.194869041442871,
            -0.16129615902900696,
            -0.06364947557449341,
            0.08257284760475159,
            0.6322552561759949,
            -1.9009761810302734,
            5.0138349533081055,
            4.565946578979492,
            3.536449432373047,
            -0.6902495622634888,
            1.0591411590576172,
            0.6047887802124023,
            -0.02242279052734375,
            0.11659741401672363,
            7.029462814331055,
            2.883882999420166,
            -2.732785701751709,
            -1.4573227167129517,
            1.4364651441574097,
            -0.65421462059021,
            -2.9053311347961426,
            0.2759181261062622,
            3.6717445850372314,
            -0.5257623195648193,
            -0.8874412178993225,
            -0.39684781432151794,
            -3.0775346755981445,
            -0.3003321886062622,
            -1.7242636680603027,
            -1.9161227941513062,
            -3.577216148376465,
            0.8125451803207397,
            3.4754233360290527,
            -0.27145203948020935,
            -1.28128981590271,
            1.2188361883163452,
            -3.50620436668396,
            -1.600811243057251,
            1.4373775720596313,
            -0.08042164146900177,
            2.690138816833496,
            3.2814459800720215,
            0.4988815188407898,
            1.4378912448883057,
            3.320373773574829,
            -2.259045124053955,
            3.9592947959899902,
            2.412226676940918,
            -5.477132797241211,
            -0.329891562461853,
            -1.1934969425201416,
            0.02493155002593994,
            -1.1646870374679565,
            1.7638139724731445,
            2.7439966201782227,
            -1.7397065162658691,
            -0.9554895162582397,
            4.404938697814941,
            0.6239964962005615,
            1.374516248703003,
            -3.841916799545288,
            1.4240002632141113,
            -2.435455322265625,
            0.2645096182823181,
            -2.0884618759155273,
            0.050842225551605225,
            0.0071223825216293335,
            -2.242441177368164,
            1.4274685382843018,
            2.2004663944244385,
            -0.10456901788711548,
            2.4584133625030518,
            0.8516046404838562,
            -0.7986666560173035,
            3.0662155151367188,
            2.204709768295288,
            -1.2413098812103271,
            0.8617895841598511,
            0.4190499484539032,
            -0.27427804470062256,
            -3.722555637359619,
            -4.906055450439453,
            -1.1136125326156616,
            -0.6528885364532471,
            1.451348066329956,
            -0.43627458810806274,
            -0.6813552379608154,
            -8.367725372314453,
            -2.8814799785614014,
            -3.791193962097168,
            -0.7801117300987244,
            -0.36428871750831604,
            5.3068647384643555,
            4.415477752685547,
            0.19106057286262512,
            -1.8978443145751953,
            1.2898998260498047,
            -2.101824998855591,
            -5.946030616760254,
            -3.0037283897399902,
            -1.1027439832687378,
            0.9256962537765503,
            1.7698878049850464,
            -1.924896478652954,
            -3.1966471672058105,
            3.2997214794158936,
            -2.8126120567321777,
            1.6890711784362793,
            -3.3070802688598633,
            3.8018553256988525,
            2.312793731689453,
            -1.2359071969985962,
            -1.8472793102264404,
            2.1112070083618164,
            1.9681365489959717,
            2.197262763977051,
            3.5940518379211426,
            -0.07586175203323364,
            1.0547484159469604,
            3.9806389808654785,
            -1.166179895401001,
            -1.5199382305145264,
            3.3553247451782227,
            -0.8560171127319336,
            0.5835694670677185,
            -1.2867794036865234,
            3.4071197509765625,
            -0.955528736114502,
            -2.72021484375,
            -1.6599494218826294,
            0.03698629140853882,
            1.5628371238708496,
            -2.5472183227539062,
            2.5796360969543457,
            -0.02910393476486206,
            2.1659326553344727,
            -4.860011577606201,
            -0.6290409564971924,
            -1.9391956329345703,
            1.482204556465149,
            3.076725482940674,
            2.0363621711730957,
            -3.118957996368408,
            2.4716813564300537,
            1.6504991054534912,
            3.4204912185668945,
            3.892758846282959,
            6.343461990356445,
            -0.7383880615234375,
            -4.635217666625977,
            -0.657655656337738,
            2.5106005668640137,
            -0.41531193256378174,
            1.8296831846237183,
            -0.276009738445282,
            4.830666542053223,
            -0.2760225832462311,
            0.8192883729934692,
            -2.7377099990844727,
            -2.40903377532959,
            1.6833665370941162,
            -3.029736280441284,
            -0.15347017347812653,
            -4.101572036743164,
            1.6192818880081177,
            -0.16023868322372437,
            1.5230388641357422,
            -0.8764944672584534,
            1.1251031160354614,
            1.0516124963760376,
            4.896117210388184,
            3.3419291973114014,
            2.7751545906066895,
            1.9465689659118652,
            1.087787389755249,
            -2.6231813430786133,
            2.0479378700256348,
            -1.912779688835144,
            -0.17791509628295898,
            -1.624732255935669,
            7.559260368347168,
            -0.23721474409103394,
            -0.2846498489379883,
            -4.311033248901367,
            2.103621482849121,
            -1.3817673921585083,
            0.7642519474029541,
            1.8930295705795288,
            -2.747525215148926,
            -2.544886350631714,
            3.9861531257629395,
            -3.955904245376587,
            3.2563352584838867,
            -0.15100973844528198,
            -0.03493189811706543,
            4.496107578277588,
            0.35267508029937744,
            5.049752235412598,
            0.42795348167419434,
            0.2762817442417145,
            -2.8166823387145996,
            1.505079984664917,
            2.0598742961883545,
            -2.0073630809783936,
            -1.414820671081543,
            3.569375514984131,
            0.694732666015625,
            1.614054799079895,
            -2.0868656635284424,
            -3.853623390197754,
            -6.236457824707031,
            -3.3206188678741455,
            -0.8577226996421814,
            -0.7816725373268127,
            -0.7543766498565674,
            4.067080497741699,
            3.1985411643981934,
            1.4427698850631714,
            -1.8124898672103882,
            -3.681276321411133,
            5.084586143493652,
            1.2306525707244873,
            0.24709150195121765,
            0.6472138166427612,
            -1.63037109375,
            -3.4429688453674316,
            -1.4734658002853394,
            -3.6354918479919434,
            1.002078652381897,
            0.5716955661773682,
            1.4913318157196045,
            2.829728126525879,
            2.7894105911254883,
            1.0639851093292236,
            -1.410724401473999,
            3.555522918701172,
            3.5579400062561035,
            2.1871325969696045,
            -1.6721453666687012,
            2.554626226425171,
            3.7390503883361816,
            4.773042678833008,
            -3.2010085582733154,
            -0.15036773681640625,
            -2.6080474853515625,
            4.547739028930664,
            -3.685422897338867,
            1.0453581809997559,
            -0.2956446707248688,
            4.677366256713867,
            3.3045785427093506,
            3.5505857467651367,
            1.6190996170043945,
            -0.06020480394363403,
            -3.12740421295166,
            0.9950881004333496,
            -2.150362730026245,
            2.955425500869751,
            0.49290502071380615,
            -0.37369126081466675,
            -1.5562281608581543,
            0.9160343408584595,
            0.5973082184791565,
            -3.6230316162109375,
            4.22208309173584,
            -3.1191601753234863,
            -2.58742618560791,
            -0.5755840539932251,
            -1.8857237100601196,
            0.7238041758537292,
            -0.27159973978996277,
            -1.2356147766113281,
            0.4626495838165283,
            -2.3130273818969727,
            -3.212984085083008,
            3.826967716217041,
            0.7108615636825562,
            3.2975006103515625,
            1.130673885345459,
            3.217543601989746,
            -0.29735422134399414,
            -3.546830177307129,
            -0.9087681770324707,
            -0.20574724674224854,
            0.9192991256713867,
            -1.5984375476837158,
            -0.6020530462265015,
            -4.72677755355835,
            2.153496026992798,
            1.4612889289855957,
            2.9410598278045654,
            1.357390284538269,
            -0.9171821475028992,
            -2.0641448497772217,
            -3.5335583686828613,
            -1.9925312995910645,
            0.6798356771469116,
            0.7619563341140747,
            -5.386131286621094,
            4.569214820861816,
            0.5844766497612,
            1.2846219539642334,
            2.990976095199585,
            1.8523056507110596,
            -1.872326374053955,
            0.5661886930465698,
            3.2175698280334473,
            1.1324739456176758,
            3.5261800289154053,
            3.348630905151367,
            -5.104363918304443,
            1.1982814073562622,
            -0.939894437789917,
            -3.3747920989990234,
            1.8817132711410522,
            -2.6820342540740967,
            0.05874526500701904,
            -1.56787109375,
            -3.5784053802490234,
            1.5475770235061646,
            0.6223746538162231,
            -0.1031370460987091,
            -1.90785551071167,
            -2.952951192855835,
            -3.372709035873413,
            1.0590298175811768,
            -3.580327033996582,
            0.8278798460960388,
            -3.2631609439849854,
            0.7417691349983215,
            1.5394097566604614,
            -2.968388557434082,
            -3.0863380432128906,
            -0.17637473344802856,
            1.651271104812622,
            0.5256401896476746,
            0.10779955983161926,
            3.7780954837799072,
            -0.2282065749168396,
            0.02755051851272583,
            -4.574352264404297,
            2.5723304748535156,
            3.7699971199035645,
            4.827272891998291,
            4.747553825378418,
            5.671525001525879,
            0.7520621418952942,
            -3.144935131072998,
            -1.216412901878357,
            -0.9340078830718994,
            4.291379928588867,
            3.473942279815674,
            -2.8002471923828125,
            1.0716382265090942,
            -0.6089481115341187,
            -1.223061203956604,
            -0.11602774262428284,
            -0.35382646322250366,
            -0.36763978004455566,
            1.550176978111267,
            -1.7245852947235107,
            -1.6365934610366821,
            -0.2575455904006958,
            -0.22283494472503662,
            -0.3969561457633972,
            -1.7750681638717651,
            3.156003952026367,
            -2.0654690265655518,
            -4.366373538970947,
            -1.808856725692749,
            -0.6599763631820679,
            -3.5031418800354004,
            -1.9612748622894287,
            0.2493503987789154,
            -2.479572296142578,
            -1.1631994247436523,
            0.48558831214904785,
            -0.8695075511932373,
            2.379647731781006,
            -1.56308114528656,
            0.5404654741287231,
            2.5980515480041504,
            -3.605760097503662,
            -1.4119600057601929,
            0.9194421172142029,
            -2.770509719848633,
            -2.7613470554351807,
            0.22990503907203674,
            -0.4519638419151306,
            0.9663287997245789,
            6.9066314697265625,
            1.4897918701171875,
            -1.9848604202270508,
            -0.11449694633483887,
            0.6019979119300842,
            -4.444122314453125,
            -3.455777883529663,
            -1.9182157516479492,
            1.678124189376831,
            -3.446718215942383,
            0.06609523296356201,
            2.7366392612457275,
            -1.0664135217666626,
            -0.8830041289329529,
            -1.0113781690597534,
            -3.9012691974639893,
            -0.2668105959892273,
            -3.985125780105591,
            1.1581265926361084,
            -2.930959939956665,
            2.177387237548828,
            2.941204071044922,
            -1.231308937072754,
            -1.983384370803833,
            3.4545812606811523,
            3.0627944469451904,
            4.616552829742432,
            0.3478511571884155,
            -1.1415895223617554,
            1.5826141834259033,
            1.7293434143066406,
            0.5141396522521973,
            -5.364446640014648,
            -0.19376899302005768,
            -0.40602412819862366,
            2.8316526412963867,
            19.38357162475586,
            0.847446084022522,
            -0.31031620502471924,
            -0.22561150789260864,
            0.8461334705352783,
            -4.145567893981934,
            -1.3334763050079346,
            4.851379871368408,
            2.565889358520508,
            4.191532135009766,
            -3.3644039630889893,
            -3.3932785987854004,
            -0.04280957579612732,
            3.015908718109131,
            -5.7276153564453125,
            -0.6149848103523254,
            -2.817998170852661,
            1.445145845413208,
            -0.7775595188140869,
            0.5813316106796265,
            -2.738750457763672,
            -1.0177249908447266,
            -0.892189085483551,
            -1.6781895160675049,
            1.2028170824050903,
            4.441169261932373,
            -1.085784912109375,
            1.694706678390503,
            -7.250240802764893,
            2.3229048252105713,
            2.862509250640869,
            0.2567472755908966,
            0.5243239402770996,
            -1.5703647136688232,
            -1.5525779724121094,
            1.5644521713256836,
            7.8035807609558105,
            0.48673248291015625,
            3.6075305938720703,
            -0.41588282585144043,
            0.44841158390045166,
            2.6626510620117188,
            -4.033702850341797,
            2.157024621963501,
            -1.0085971355438232,
            1.4887113571166992,
            4.160941123962402,
            -1.7224273681640625,
            -0.5104184150695801,
            3.9168100357055664,
            5.546393871307373,
            -0.013141423463821411,
            -2.884211540222168,
            -1.0580930709838867,
            4.068317890167236,
            -0.24461603164672852,
            -0.5059269070625305,
            1.119976282119751,
            1.137871265411377,
            3.2301416397094727,
            -0.3265461325645447,
            -1.8561959266662598,
            -2.8932714462280273,
            -4.8304548263549805,
            -0.1695396900177002,
            -1.8927263021469116,
            -2.325049638748169,
            2.3807919025421143,
            1.0126808881759644,
            -0.35633933544158936,
            6.306468486785889,
            0.42897820472717285,
            2.3218133449554443,
            -1.987296223640442,
            -2.6334900856018066,
            -6.008617877960205,
            -0.8045490384101868,
            1.804917812347412,
            1.9393672943115234,
            4.858857154846191,
            -3.5107293128967285,
            0.9475667476654053,
            -4.527403354644775,
            -0.9159946441650391,
            3.2149133682250977,
            -3.2869009971618652,
            2.935197114944458,
            -3.3983664512634277,
            -0.4294004440307617,
            1.1066899299621582,
            -1.3660210371017456,
            -0.2502182722091675,
            0.4323158860206604,
            0.8261841535568237,
            0.6863234639167786,
            -4.623748302459717,
            -1.6560999155044556,
            -1.6763763427734375,
            -9.470523834228516,
            -3.9114341735839844,
            4.993499755859375,
            -0.3275339603424072,
            2.9525160789489746,
            0.3095450699329376,
            3.1152679920196533,
            -0.13401156663894653,
            -1.2288844585418701,
            -3.5468828678131104,
            -6.0430097579956055,
            -0.8999730944633484,
            2.427672863006592,
            -2.6758651733398438,
            -0.41343894600868225,
            0.09182995557785034,
            3.0124120712280273,
            -1.0633560419082642,
            -2.651745319366455,
            1.0991718769073486,
            0.9787647724151611,
            2.326866865158081,
            -1.5782884359359741,
            -3.02366304397583,
            -0.8728067278862,
            0.073799729347229,
            -0.21164849400520325,
            2.770721912384033,
            -0.9166920185089111,
            -0.8951126337051392,
            -1.7812429666519165,
            -4.316619873046875,
            -0.21581867337226868,
            -0.3742355406284332,
            -0.010571837425231934,
            0.8493170738220215,
            -1.2568155527114868,
            -1.7441189289093018,
            1.9645055532455444,
            -0.28450441360473633,
            -3.557903289794922,
            6.770772933959961,
            1.877880334854126,
            -1.3824732303619385,
            -2.800220251083374,
            8.254514694213867,
            -1.8787195682525635,
            1.6359658241271973,
            -0.029591098427772522,
            -0.2954624593257904,
            -2.9137887954711914,
            -1.3806415796279907,
            -2.3156991004943848,
            4.391634941101074,
            5.883369445800781,
            -0.08842992782592773,
            -2.167117118835449,
            -2.853440761566162
        ]
    },
    "authors": [
        {
            "authorId": "46947755",
            "name": "Zhiyuan Li"
        },
        {
            "authorId": "2118915359",
            "name": "Tianhao Wang"
        },
        {
            "authorId": "145563465",
            "name": "Sanjeev Arora"
        }
    ],
    "references": [
        {
            "paperId": "e9e556d8e56c5f830d74695e19f7474f5ae2e5d9",
            "title": "Regularizing Deep Neural Networks with Stochastic Estimators of Hessian Trace"
        },
        {
            "paperId": "74a3721cb5bdafc3c9671ace6cc8d94510e5449f",
            "title": "Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity"
        },
        {
            "paperId": "0f900b9876d03cc2b09b8c113121e749ccddd698",
            "title": "Label Noise SGD Provably Prefers Flat Global Minimizers"
        },
        {
            "paperId": "ececb57bb831d8168638aca987f70628b441d6e0",
            "title": "Stochastic Gradient Descent with Noise of Machine Learning Type Part II: Continuous Time Analysis"
        },
        {
            "paperId": "55fac1b87b453d40f6f997a51a9eb9dd009b4ee8",
            "title": "On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)"
        },
        {
            "paperId": "683e00d7c0c8e31260f9867093509518bcbdbfc8",
            "title": "On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent"
        },
        {
            "paperId": "4fa32fec61c50f8339a05e097dacebe71cf9ab8e",
            "title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent"
        },
        {
            "paperId": "27558603527494688876cbd0cf5af53af5127f4a",
            "title": "Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning"
        },
        {
            "paperId": "54f105cd0ccd999b9aa01636edb736489a24718c",
            "title": "Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?"
        },
        {
            "paperId": "7dfac3bf747fefc62f5bdaa124a609992a18b974",
            "title": "Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate"
        },
        {
            "paperId": "87371d7f222c906c7dda4bd2428a6cca607ca9ce",
            "title": "Understanding Implicit Regularization in Over-Parameterized Nonlinear Statistical Model"
        },
        {
            "paperId": "82b20ed50126e106091dd16aaeb538cbb3bfddb9",
            "title": "Shape Matters: Understanding the Implicit Bias of the Noise Covariance"
        },
        {
            "paperId": "9896efaaa661d5d1a0213fb0d92d4253eaaa6808",
            "title": "The critical locus of overparameterized neural networks"
        },
        {
            "paperId": "a916be79ecedb98fb22943df1af8ce216eb52f43",
            "title": "A Diffusion Theory for Deep Learning Dynamics: Stochastic Gradient Descent Escapes From Sharp Minima Exponentially Fast"
        },
        {
            "paperId": "437fbb0cc305b7d3f0b2462a316cef0327099b26",
            "title": "Gradient descent optimizes over-parameterized deep ReLU networks"
        },
        {
            "paperId": "ee8a9a6246ef6fa3ecdd2716560b4cff932e298e",
            "title": "Implicit Regularization for Optimal Sparse Recovery"
        },
        {
            "paperId": "36451fe94b7b1f978b8301cf3f7305566bd3b454",
            "title": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"
        },
        {
            "paperId": "855f943165013e0897b9f2bc54f6748ba08aef2a",
            "title": "Stochastic Gradient and Langevin Processes"
        },
        {
            "paperId": "fd47a15447706fca16d24cf8cac575e4a5909e9d",
            "title": "On the Noisy Gradient Descent that Generalizes as SGD"
        },
        {
            "paperId": "3a982f7e82bedb82bd2b53cf2da1d08012ed5179",
            "title": "Kernel and Rich Regimes in Overparametrized Models"
        },
        {
            "paperId": "b0a4b1f67632f3abd6e27a397be2d004020859f2",
            "title": "Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets"
        },
        {
            "paperId": "3f46ac38812f9f0f99ff1edddd85d71a84da4497",
            "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks"
        },
        {
            "paperId": "217a85f667778567d7ffc8b56060783caf5803b0",
            "title": "Implicit Regularization in Deep Matrix Factorization"
        },
        {
            "paperId": "bcf397d57dbcb5d423a341bfadef8dfc09d0cfb8",
            "title": "Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process"
        },
        {
            "paperId": "e7f728bb85def743320f20df1aa366cb657d803a",
            "title": "Convergence rates for the stochastic gradient descent method for non-convex objective functions"
        },
        {
            "paperId": "a60e77b8f9c6ca6ce12b5a1ccda897d00f9feeb4",
            "title": "Implicit regularization via hadamard product over-parametrization in high-dimensional linear regression"
        },
        {
            "paperId": "1ed23d68ddead971ecd655988bb16bda181d4b10",
            "title": "Interplay Between Optimization and Generalization of Stochastic Gradient Descent with Covariance Noise"
        },
        {
            "paperId": "9b15a6f2434b9274cd1228eed4288b98cd316394",
            "title": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"
        },
        {
            "paperId": "014e1751da7cec01119a9ea579e3f7c20c1dbad2",
            "title": "On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning"
        },
        {
            "paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443",
            "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"
        },
        {
            "paperId": "12c873cc792dd18c8d414d9d229a7887d3d9e724",
            "title": "On Connected Sublevel Sets in Deep Learning"
        },
        {
            "paperId": "62a0bafd54099a79d78a013611a0c7d38e237032",
            "title": "On Lazy Training in Differentiable Programming"
        },
        {
            "paperId": "611fe6e34df07ea1b2104899e49642b4531b53e9",
            "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"
        },
        {
            "paperId": "03e7e8663c69e691be6b6403b1eb1bbf593d31f2",
            "title": "Gradient Descent Finds Global Minima of Deep Neural Networks"
        },
        {
            "paperId": "c0d770fa10b6510828afdde2040e5c63fb61c457",
            "title": "Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations"
        },
        {
            "paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
            "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"
        },
        {
            "paperId": "e3300e3800fae17aa4a4e1e684359cbd272421d0",
            "title": "On the loss landscape of a class of deep neural networks with no bad local valleys"
        },
        {
            "paperId": "42ec3db12a2e4628885451b13035c2e975220a25",
            "title": "A Convergence Theory for Deep Learning via Over-Parameterization"
        },
        {
            "paperId": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6",
            "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"
        },
        {
            "paperId": "7a84a692327534fd227fa1e07fcb3816b633c591",
            "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
        },
        {
            "paperId": "33c6e984c1fe9edfb645ed410e20273c8af6906a",
            "title": "The loss landscape of overparameterized neural networks"
        },
        {
            "paperId": "2c90d366126a3ccd3c43e47891730650003059da",
            "title": "Essentially No Barriers in Neural Network Energy Landscape"
        },
        {
            "paperId": "1cb0c425f57f1b052b82ce03e26433b98bcba2d2",
            "title": "The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects"
        },
        {
            "paperId": "f6195d8dc6aad8231e97b563246f2585842bc68b",
            "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs"
        },
        {
            "paperId": "33416f2dc49db24cca520a3b234f02463a4e833e",
            "title": "Characterizing Implicit Bias in Terms of Optimization Geometry"
        },
        {
            "paperId": "6ea8cbf0cc4cda3d981348a279b464524a8485cc",
            "title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization"
        },
        {
            "paperId": "36cd4f761b9911b8509ac90c2831fe8449152107",
            "title": "Understanding the Loss Surface of Neural Networks for Binary Classification"
        },
        {
            "paperId": "7996894ff576c389441d036a0ad51176f0624c87",
            "title": "Spurious Valleys in Two-layer Neural Network Optimization Landscapes"
        },
        {
            "paperId": "14fafd5adc1714a2ce3a309ed4c5b441ba7004ca",
            "title": "Differentiating the pseudo determinant"
        },
        {
            "paperId": "8b4b861583f698e89c8cd9e198aad86809a71de7",
            "title": "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations"
        },
        {
            "paperId": "f9e6af73d33e7aac3f349bef927fcd666e8e00db",
            "title": "Three Factors Influencing Minima in SGD"
        },
        {
            "paperId": "11adc8bd35bd897502f9b5452ab7ac668ec9b0fb",
            "title": "The Implicit Bias of Gradient Descent on Separable Data"
        },
        {
            "paperId": "ec025c3cdab31159026bafe16b99a4042ccbd19c",
            "title": "First-order Methods Almost Always Avoid Saddle Points"
        },
        {
            "paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5",
            "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"
        },
        {
            "paperId": "4e8917e73e02c76d55ded62e43541d44684a4c8a",
            "title": "Implicit Regularization in Matrix Factorization"
        },
        {
            "paperId": "8501e330d78391f4e690886a8eb8fac867704ea6",
            "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks"
        },
        {
            "paperId": "6783b727e7de7d6d826e765e48406aacf103e63d",
            "title": "SGD Learns the Conjugate Kernel Class of the Network"
        },
        {
            "paperId": "0f94591cc05e6f75c21749d507ef58d204f63b7d",
            "title": "Topology and Geometry of Half-Rectified Network Optimization"
        },
        {
            "paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81",
            "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"
        },
        {
            "paperId": "9dd705a9974c6ac7bb8a32d89ce2841bb1ac66af",
            "title": "Gradient Descent Only Converges to Minimizers"
        },
        {
            "paperId": "2f8eb618406e5ae3fe73b9b4ffe2b346107febaa",
            "title": "Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms"
        },
        {
            "paperId": "31d55d6dbe628c24d6959e01fce09dfc7ba118e6",
            "title": "Convex recovery of a structured signal from independent random linear measurements"
        },
        {
            "paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889",
            "title": "One weird trick for parallelizing convolutional neural networks"
        },
        {
            "paperId": "c562a2479094768fcf76bcc992cdbf12227a98fe",
            "title": "Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming"
        },
        {
            "paperId": "bd13e0f8adcea360f12cc02c18fd00266db7a93d",
            "title": "Lectures on Morse Homology"
        },
        {
            "paperId": "ee6275a84962a0ffd6212585e4f7ee7ffb2b068a",
            "title": "Feature selection, L1 vs. L2 regularization, and rotational invariance"
        },
        {
            "paperId": "3a5d995064c2066597c7f49ddda81c3605e08ce9",
            "title": "Solutions of a Stochastic Differential Equation Forced Onto a Manifold by a Large Drift"
        },
        {
            "paperId": "8088df81f7d2207886f7ff43210a9d6c833db01a",
            "title": "Differentiation of the Limit Mapping in a Dynamical System"
        },
        {
            "paperId": "2136bdc847fd6123e0253acbec69accfdf70e996",
            "title": "ON CONVERGENCE OF STOCHASTIC PROCESSES"
        },
        {
            "paperId": null,
            "title": "2017), one approach to approximate SGD (1) by SDE is to consider the following SDE"
        },
        {
            "paperId": null,
            "title": "\u039e-dimensional Brownian motion and that \u03c3(X) : RD \u2192 RD\u00d7\u039e is a deterministic noise function"
        },
        {
            "paperId": null,
            "title": "Riemannian geometry"
        },
        {
            "paperId": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
            "title": "Efficient BackProp"
        },
        {
            "paperId": "b2bbb969c41b75d3a46bbdc632123712dbc95246",
            "title": "Differential Equations and Dynamical Systems"
        },
        {
            "paperId": "74f31c3e0310489d904064d4856b2ad4635fe349",
            "title": "Stochastic Analysis on Manifolds"
        },
        {
            "paperId": "99e37b4b689339da03493d01a853c00afc67f63d",
            "title": "Brownian Motion and Stochastic Calculus"
        },
        {
            "paperId": "57f1100197a13a7b9da77069bee3824e5557df2d",
            "title": "An Introduction to Stochastic-Process Limits and their Application to Queues"
        },
        {
            "paperId": "2159cea7acd3f9b392c1f61a71e24c13bc675eb2",
            "title": "On the probability that a random \u00b11-matrix is singular"
        },
        {
            "paperId": "bc55639dcb2d8cb372643c2336ed2dc85ddcaaed",
            "title": "Review: P. Billingsley, Convergence of probability measures"
        },
        {
            "paperId": "ed53839649c48f220cc7703ebf27c3a67297f2b8",
            "title": "Gradient methods for solving equations and inequalities"
        },
        {
            "paperId": null,
            "title": "A topological property of real analytic subsets"
        },
        {
            "paperId": null,
            "title": "far only analysis for simple diagonal linear nets"
        },
        {
            "paperId": null,
            "title": "Limiting diffusion for adaptive gradient methods, like momentum-SGD"
        },
        {
            "paperId": null,
            "title": "Note x(T ) \u2208 span{\u2207f x (x(0))}, which is at most an n-dimensional space spanned by the gradients of model output at x(0)"
        },
        {
            "paperId": null,
            "title": "Since R(w) is a convex function, it follows that R(w(\u221e) + s(w * \u2212 w(\u221e))) \u2264 s R(w * ) + (1 \u2212 s) R(\u221e) < R(w(\u221e)) for all 0 < s \u2264 1"
        }
    ]
}