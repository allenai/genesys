{
    "paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a",
    "externalIds": {
        "DBLP": "journals/corr/abs-2003-08271",
        "MAG": "3088409176",
        "ArXiv": "2003.08271",
        "DOI": "10.1007/s11431-020-1647-3",
        "CorpusId": 212747830
    },
    "title": "Pre-trained models for natural language processing: A survey",
    "abstract": null,
    "venue": "Science China Technological Sciences",
    "year": 2020,
    "referenceCount": 264,
    "citationCount": 1275,
    "influentialCitationCount": 46,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.2479257583618164,
            -2.896864891052246,
            1.3853341341018677,
            3.845830202102661,
            2.5304741859436035,
            0.9663446545600891,
            6.280016899108887,
            1.0249824523925781,
            0.4610501527786255,
            1.1412177085876465,
            -0.44562196731567383,
            3.07417368888855,
            0.38710731267929077,
            1.1308469772338867,
            -2.0127112865448,
            -0.48739659786224365,
            -0.5452610850334167,
            -4.573002815246582,
            3.272134780883789,
            0.5287556052207947,
            -1.0780202150344849,
            1.0827162265777588,
            -0.9764049053192139,
            -0.1075349897146225,
            0.10380114614963531,
            -1.4677724838256836,
            3.1892075538635254,
            1.1517513990402222,
            -3.7232069969177246,
            -1.8327280282974243,
            0.5945178866386414,
            -4.073879241943359,
            1.5552480220794678,
            -6.494756698608398,
            -0.8289375901222229,
            -2.693333148956299,
            -3.6373138427734375,
            7.491675853729248,
            -2.9172253608703613,
            -1.8874202966690063,
            -3.2052505016326904,
            -2.5206823348999023,
            1.4704458713531494,
            6.2681965827941895,
            2.1580448150634766,
            -0.4095975160598755,
            2.053053617477417,
            2.123831272125244,
            -0.5793257355690002,
            1.0829015970230103,
            3.4874114990234375,
            -0.8761327862739563,
            0.7060836553573608,
            1.9747192859649658,
            2.685058832168579,
            -2.0841760635375977,
            0.15311536192893982,
            4.071728706359863,
            0.8434593081474304,
            -2.4788780212402344,
            3.4038851261138916,
            5.791031360626221,
            0.5689258575439453,
            -0.7782256007194519,
            4.020698547363281,
            -4.1960344314575195,
            -5.6895670890808105,
            2.9347403049468994,
            2.9490902423858643,
            0.04710391163825989,
            -1.528324842453003,
            -4.512762069702148,
            -0.29738959670066833,
            -1.1893932819366455,
            0.8385770320892334,
            -2.0234603881835938,
            -0.967603325843811,
            -6.952947616577148,
            -0.42463669180870056,
            0.14993827044963837,
            0.4564540386199951,
            -3.581528663635254,
            0.9556260108947754,
            2.023662805557251,
            1.4678049087524414,
            0.47359955310821533,
            -7.075707912445068,
            0.9334223866462708,
            0.8930554986000061,
            -2.910191059112549,
            -0.19704082608222961,
            0.15990221500396729,
            1.1850991249084473,
            5.022241592407227,
            -3.641010284423828,
            -3.197385787963867,
            1.0854440927505493,
            -3.435026168823242,
            -4.541274070739746,
            -1.9520741701126099,
            5.333564758300781,
            -1.3022935390472412,
            1.9695301055908203,
            2.8303987979888916,
            3.84659743309021,
            -3.3605704307556152,
            1.190691351890564,
            -0.4503597021102905,
            1.4507830142974854,
            -1.1290853023529053,
            -1.4966832399368286,
            1.9501092433929443,
            -0.4414530396461487,
            -2.2354421615600586,
            -0.5307926535606384,
            -3.7564830780029297,
            0.9963228106498718,
            -0.4811908006668091,
            -2.4632680416107178,
            2.4399142265319824,
            -0.6198613047599792,
            -1.8772878646850586,
            -2.2041690349578857,
            -1.7482210397720337,
            -0.5473566055297852,
            1.8676812648773193,
            -0.41540277004241943,
            0.4759798049926758,
            1.8239049911499023,
            -1.6467443704605103,
            6.0899858474731445,
            0.12674272060394287,
            4.109255790710449,
            0.672859787940979,
            5.214718818664551,
            2.2465529441833496,
            -3.227267265319824,
            0.5599117875099182,
            0.36267322301864624,
            -2.0541679859161377,
            -0.032706886529922485,
            5.728097438812256,
            -2.018458127975464,
            1.4294075965881348,
            -3.5191409587860107,
            0.8626253604888916,
            -0.3565518260002136,
            -2.610288619995117,
            1.1250920295715332,
            5.04506778717041,
            3.3936495780944824,
            -5.447873592376709,
            -0.7336423397064209,
            0.7108497619628906,
            -2.099203586578369,
            3.860037088394165,
            -4.970212936401367,
            -0.598542332649231,
            -0.48757174611091614,
            0.7607841491699219,
            -1.4944161176681519,
            -1.3078759908676147,
            -5.9486236572265625,
            -0.10858356952667236,
            2.3291006088256836,
            -5.785781383514404,
            -0.5166845321655273,
            3.2442550659179688,
            -3.309999465942383,
            2.876917839050293,
            0.1734846979379654,
            2.1182141304016113,
            -1.8140335083007812,
            2.169283151626587,
            0.9230183362960815,
            3.4808766841888428,
            0.6157268285751343,
            -1.6118661165237427,
            -3.321162700653076,
            -1.1297248601913452,
            0.5357940793037415,
            -1.9240232706069946,
            -5.56829833984375,
            0.2600029706954956,
            -5.904572486877441,
            -0.6825831532478333,
            -1.8655407428741455,
            -4.359649658203125,
            2.5793075561523438,
            -2.490217924118042,
            -2.1419286727905273,
            1.4743146896362305,
            6.136253356933594,
            8.311978340148926,
            2.281809091567993,
            1.9383866786956787,
            0.5172026753425598,
            7.560535430908203,
            -2.6477439403533936,
            1.6910374164581299,
            1.019594669342041,
            -0.9016030430793762,
            0.9839178323745728,
            -3.1216912269592285,
            3.311575412750244,
            5.721982479095459,
            -1.465302586555481,
            4.303491115570068,
            2.4329216480255127,
            -1.3087236881256104,
            2.578355312347412,
            -0.21438118815422058,
            -1.7615300416946411,
            3.423124313354492,
            -0.48223453760147095,
            -1.8660690784454346,
            -5.313035011291504,
            -0.49081990122795105,
            5.406985282897949,
            -0.8349549174308777,
            -3.044877290725708,
            1.2448742389678955,
            0.17246825993061066,
            -1.2244865894317627,
            3.3539462089538574,
            -4.415052890777588,
            1.4924956560134888,
            -0.6945534348487854,
            -3.459744453430176,
            -3.5109846591949463,
            -2.9289660453796387,
            -1.4639421701431274,
            0.07028841972351074,
            -0.25390565395355225,
            -2.5490777492523193,
            -0.5563056468963623,
            -4.802411079406738,
            0.7816489338874817,
            -1.1542012691497803,
            -0.4991071820259094,
            5.554337024688721,
            1.8869035243988037,
            -0.01150749996304512,
            5.069766998291016,
            2.8526690006256104,
            0.9502066373825073,
            -4.059187889099121,
            2.2070906162261963,
            -0.1391180157661438,
            -1.490692377090454,
            -2.0343551635742188,
            -1.7322005033493042,
            1.1611392498016357,
            4.257904052734375,
            -0.4809141755104065,
            0.8764524459838867,
            1.666616678237915,
            0.17371299862861633,
            -2.2371890544891357,
            3.653384208679199,
            0.8542810678482056,
            1.476137638092041,
            -2.0886754989624023,
            3.0054574012756348,
            -3.027510404586792,
            -1.4925992488861084,
            -2.8495287895202637,
            -1.3100439310073853,
            -5.1903910636901855,
            4.4242844581604,
            6.235163688659668,
            -0.3514597415924072,
            0.49408188462257385,
            -1.1695575714111328,
            -0.6762290596961975,
            -4.450277328491211,
            -4.000375270843506,
            -2.2559187412261963,
            4.0049028396606445,
            -0.07316415011882782,
            0.5620423555374146,
            -2.7413322925567627,
            1.8688318729400635,
            0.03446832299232483,
            -0.8310814499855042,
            -3.260178565979004,
            -1.6291391849517822,
            -0.6995677947998047,
            -1.8984191417694092,
            -4.465686798095703,
            -1.0997989177703857,
            4.4443535804748535,
            -1.2131012678146362,
            -2.0569448471069336,
            0.34546026587486267,
            0.4456161558628082,
            3.324497699737549,
            1.162969708442688,
            1.800980806350708,
            1.071201205253601,
            1.6946706771850586,
            4.1853108406066895,
            2.754554510116577,
            -1.0076016187667847,
            2.376495599746704,
            2.8767199516296387,
            2.817689895629883,
            -3.46433162689209,
            1.1814563274383545,
            -1.2713040113449097,
            -2.7529149055480957,
            0.43321096897125244,
            1.888131022453308,
            -6.5582146644592285,
            1.0216054916381836,
            1.1880040168762207,
            -0.6110090017318726,
            -0.3252553343772888,
            -2.1131691932678223,
            -1.1413586139678955,
            -1.3357648849487305,
            2.4026124477386475,
            -4.9251580238342285,
            -1.1090459823608398,
            -3.887921094894409,
            2.0686495304107666,
            2.564079761505127,
            0.1233900785446167,
            -2.537202835083008,
            3.6616506576538086,
            -1.8787548542022705,
            4.247352600097656,
            2.692783832550049,
            1.657958745956421,
            -2.8784546852111816,
            -3.7291297912597656,
            -1.009596347808838,
            -1.4593002796173096,
            1.8779995441436768,
            0.3151342272758484,
            0.6506253480911255,
            4.702969074249268,
            -2.8841753005981445,
            1.8878521919250488,
            0.8306429386138916,
            2.354137420654297,
            1.1409153938293457,
            -1.0426021814346313,
            0.34292468428611755,
            -1.4975574016571045,
            -0.37446659803390503,
            2.481860637664795,
            5.142332553863525,
            1.9590153694152832,
            2.357323408126831,
            3.244199275970459,
            0.21907785534858704,
            0.3953932523727417,
            -3.234318494796753,
            -0.694888710975647,
            2.3579955101013184,
            -1.7060351371765137,
            4.455862045288086,
            1.5633893013000488,
            5.33759069442749,
            -3.091742992401123,
            7.104104042053223,
            -2.4105522632598877,
            0.3182319104671478,
            -4.100164413452148,
            -3.5805928707122803,
            -2.2059690952301025,
            -3.50527286529541,
            5.242898941040039,
            -4.016295433044434,
            -3.7264294624328613,
            1.5697402954101562,
            -3.3450872898101807,
            0.3255373239517212,
            0.7477598786354065,
            -0.49695223569869995,
            1.9915101528167725,
            -4.393166542053223,
            -0.36797356605529785,
            -0.33645331859588623,
            3.030022621154785,
            -2.3558623790740967,
            2.548799753189087,
            -0.12358906865119934,
            0.39791327714920044,
            -0.5879690647125244,
            1.7912044525146484,
            0.29042211174964905,
            -1.3432848453521729,
            -5.864232540130615,
            -0.1922437846660614,
            -1.367087960243225,
            -2.046046018600464,
            1.3635191917419434,
            2.254141330718994,
            1.2002460956573486,
            1.8466780185699463,
            0.06337153911590576,
            2.4269988536834717,
            -3.4716925621032715,
            2.475001335144043,
            3.923220634460449,
            -3.5927670001983643,
            -3.033234119415283,
            -0.7631036639213562,
            -7.282766819000244,
            -2.3663899898529053,
            -1.9386646747589111,
            -2.8858253955841064,
            0.46754953265190125,
            1.3749561309814453,
            3.268991708755493,
            4.809444427490234,
            1.634652018547058,
            -0.307638019323349,
            -2.6621646881103516,
            0.6140445470809937,
            7.720918655395508,
            -0.3905284106731415,
            1.5858407020568848,
            0.27974799275398254,
            2.9034080505371094,
            2.1730690002441406,
            -1.935120701789856,
            1.3438271284103394,
            -1.0152490139007568,
            0.510358452796936,
            -1.3927713632583618,
            -3.6489977836608887,
            0.49872279167175293,
            1.362060308456421,
            2.4723567962646484,
            4.833526611328125,
            0.9114265441894531,
            -5.3830060958862305,
            -0.5689747333526611,
            1.0175939798355103,
            2.2455782890319824,
            2.932504653930664,
            0.47010183334350586,
            3.910557270050049,
            2.4670779705047607,
            1.5697119235992432,
            0.9244274497032166,
            0.1678440421819687,
            2.80350923538208,
            -1.5456148386001587,
            -3.741340160369873,
            -0.7445846796035767,
            -1.6357545852661133,
            0.03175868093967438,
            -3.1978044509887695,
            1.108480453491211,
            -0.6620173454284668,
            1.7921717166900635,
            -2.7920122146606445,
            6.3448686599731445,
            -1.9234724044799805,
            0.8142696022987366,
            0.27910831570625305,
            0.13479453325271606,
            -1.9753618240356445,
            -1.8379926681518555,
            -3.088165283203125,
            4.165132522583008,
            1.5383384227752686,
            0.12513458728790283,
            -3.1988062858581543,
            4.275667667388916,
            -2.355968952178955,
            1.398226261138916,
            3.7740707397460938,
            0.6973823308944702,
            3.4021689891815186,
            -4.697400093078613,
            -3.527768135070801,
            -0.09424890577793121,
            4.310821533203125,
            -4.12799596786499,
            1.1383895874023438,
            1.2720507383346558,
            1.6490237712860107,
            -0.9980616569519043,
            1.4234293699264526,
            0.8441040515899658,
            2.6180520057678223,
            5.957148551940918,
            2.7626147270202637,
            1.7903270721435547,
            -1.422004222869873,
            0.675527811050415,
            -3.973982095718384,
            -0.5741380453109741,
            1.053274393081665,
            -1.5692880153656006,
            0.1552663892507553,
            -6.67311954498291,
            3.3817315101623535,
            0.6858875751495361,
            -2.685215473175049,
            7.382019996643066,
            5.405290126800537,
            0.007568061351776123,
            -0.2665420174598694,
            -1.5226476192474365,
            -0.09201975166797638,
            1.2094248533248901,
            -4.432539939880371,
            1.0141459703445435,
            -2.7016897201538086,
            0.5675550103187561,
            4.360161781311035,
            -0.6753078699111938,
            0.8324059844017029,
            -2.2674953937530518,
            0.6087911128997803,
            -1.871980905532837,
            -4.430901527404785,
            2.371062755584717,
            -0.4649260640144348,
            1.3553131818771362,
            1.5436652898788452,
            -3.156744956970215,
            1.3719937801361084,
            3.5145325660705566,
            4.941234588623047,
            2.122807741165161,
            3.2972185611724854,
            -4.761178016662598,
            0.5550164580345154,
            -3.2511472702026367,
            -1.1180646419525146,
            2.517070770263672,
            -2.281266450881958,
            2.304381847381592,
            0.965321958065033,
            1.2874327898025513,
            3.5919432640075684,
            5.345736026763916,
            2.168264150619507,
            0.5200136303901672,
            -3.276608467102051,
            -2.208632469177246,
            0.4497598111629486,
            -0.8651748895645142,
            -0.6598799228668213,
            0.44683659076690674,
            4.56868839263916,
            3.596776008605957,
            -3.6193323135375977,
            -1.601269245147705,
            -0.8874614238739014,
            0.007038511335849762,
            -1.8329617977142334,
            5.26389741897583,
            3.5581235885620117,
            1.1741726398468018,
            3.4845919609069824,
            0.5657228231430054,
            -0.27353158593177795,
            -1.5362200736999512,
            0.8782868385314941,
            2.5671682357788086,
            1.6765803098678589,
            -6.15493631362915,
            -0.4328916370868683,
            -2.2541205883026123,
            0.1544715166091919,
            2.9597487449645996,
            2.822147846221924,
            2.485086679458618,
            2.9907219409942627,
            0.6870785355567932,
            -2.249025583267212,
            -1.3015308380126953,
            1.0274399518966675,
            1.289584994316101,
            -1.6389983892440796,
            -2.2689969539642334,
            -0.20190514624118805,
            -1.0763263702392578,
            -1.8853434324264526,
            -0.006487607955932617,
            -5.20413064956665,
            -1.3781466484069824,
            5.139679431915283,
            0.40539538860321045,
            0.9603725671768188,
            -0.6557689905166626,
            2.2873358726501465,
            -3.873633861541748,
            4.300165176391602,
            1.4683570861816406,
            0.47374972701072693,
            5.752758026123047,
            -0.4171910583972931,
            2.023742437362671,
            3.413194179534912,
            2.381654739379883,
            0.7429772615432739,
            -0.620476484298706,
            4.617107391357422,
            0.7541272044181824,
            -1.203941822052002,
            2.6802737712860107,
            -2.838984489440918,
            3.7108421325683594,
            12.842288970947266,
            2.758281707763672,
            -0.2631000876426697,
            0.9930993914604187,
            -4.747237682342529,
            -2.7370948791503906,
            -1.3207802772521973,
            0.5542646050453186,
            -0.07250213623046875,
            -0.47627753019332886,
            -1.1016356945037842,
            -3.5788447856903076,
            -4.3466291427612305,
            0.9447603821754456,
            -5.1108527183532715,
            0.7495357990264893,
            -0.30552005767822266,
            4.4645562171936035,
            -1.2442524433135986,
            0.18463654816150665,
            0.4955112934112549,
            1.8145487308502197,
            -5.401262283325195,
            -3.9747915267944336,
            -1.1243972778320312,
            1.540909767150879,
            4.167491436004639,
            2.8966407775878906,
            -5.743730545043945,
            0.20098555088043213,
            -2.8177666664123535,
            0.6110880374908447,
            1.8936643600463867,
            0.20833426713943481,
            -1.4524989128112793,
            2.823638916015625,
            2.2675254344940186,
            -4.604090690612793,
            1.0406112670898438,
            -0.8748109936714172,
            -1.480623483657837,
            3.988252878189087,
            -2.324533700942993,
            2.983825206756592,
            -3.277935028076172,
            1.1966843605041504,
            1.1743566989898682,
            -0.8593470454216003,
            1.20890474319458,
            5.543198108673096,
            1.3317601680755615,
            -0.37297341227531433,
            -0.673893392086029,
            -0.26674729585647583,
            1.3398032188415527,
            -3.332857847213745,
            0.693110466003418,
            -6.930314064025879,
            0.25705862045288086,
            -4.009200096130371,
            0.004274914041161537,
            -0.9969851970672607,
            2.230100393295288,
            -3.3769640922546387,
            -1.5668652057647705,
            -3.768916130065918,
            -5.218461036682129,
            3.2885665893554688,
            2.2237589359283447,
            -0.37252628803253174,
            2.1906967163085938,
            3.1304380893707275,
            -2.4453349113464355,
            -3.412977933883667,
            0.10927760601043701,
            -5.045069694519043,
            2.069397211074829,
            -1.8495206832885742,
            -1.1466575860977173,
            3.789605140686035,
            -0.6263809204101562,
            1.7135907411575317,
            -3.048417806625366,
            -2.5436646938323975,
            3.637723207473755,
            -1.6989860534667969,
            3.3938143253326416,
            2.6619532108306885,
            -1.2937051057815552,
            2.9741036891937256,
            -1.899793028831482,
            0.4648416042327881,
            5.21828556060791,
            6.058080673217773,
            -2.2706451416015625,
            -4.931391716003418,
            -1.2078959941864014,
            -1.9045708179473877,
            -2.687730073928833,
            -1.5769290924072266,
            4.727540493011475,
            1.46665620803833,
            -1.6870932579040527,
            -2.733161449432373,
            -0.09104377031326294,
            -2.9821267127990723,
            -2.238382577896118,
            -3.287046194076538,
            -1.7701119184494019,
            -2.318429708480835,
            5.30214262008667,
            -2.6987876892089844,
            -2.8059136867523193,
            -0.07121467590332031,
            4.049993515014648,
            -4.508936405181885,
            2.332179546356201,
            2.327446937561035,
            3.1397016048431396,
            4.267168998718262,
            2.173537492752075,
            -3.5620005130767822,
            -3.593592405319214,
            -5.648162841796875,
            -1.2956889867782593,
            -1.50096595287323,
            1.0250083208084106,
            -4.7465596199035645,
            -1.8970658779144287,
            -0.9758291840553284,
            4.072359085083008,
            -3.4270713329315186,
            -0.5484409332275391,
            -0.1690026819705963,
            -1.1698048114776611,
            -1.1816329956054688,
            0.12173625826835632,
            1.4118584394454956,
            -1.5878679752349854,
            4.914167404174805,
            -0.25288182497024536,
            -3.769918441772461,
            -1.0691478252410889,
            7.963449001312256,
            0.1255236715078354,
            -2.8154659271240234,
            -2.581045150756836,
            1.428821325302124,
            -0.7552823424339294,
            0.9675300717353821,
            3.3608272075653076,
            -0.10256333649158478,
            1.0633647441864014,
            0.8188002109527588,
            -3.855890989303589,
            -2.979870080947876
        ]
    },
    "authors": [
        {
            "authorId": "1767521",
            "name": "Xipeng Qiu"
        },
        {
            "authorId": "153345698",
            "name": "Tianxiang Sun"
        },
        {
            "authorId": "26339093",
            "name": "Yige Xu"
        },
        {
            "authorId": "95329799",
            "name": "Yunfan Shao"
        },
        {
            "authorId": "145493218",
            "name": "Ning Dai"
        },
        {
            "authorId": "1790227",
            "name": "Xuanjing Huang"
        }
    ],
    "references": [
        {
            "paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530",
            "title": "A Survey of Transformers"
        },
        {
            "paperId": "d73d8edbc804b7848b4a5a1a11b6d33f9f42fa94",
            "title": "Elbert: Fast Albert with Confidence-Window Based Early Exit"
        },
        {
            "paperId": "03662672662f49e6b06148e94b407b60b0bb72f3",
            "title": "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models"
        },
        {
            "paperId": "9c053552dfa6184f7dc56d620bcb1e8f22c729a3",
            "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit"
        },
        {
            "paperId": "9631f5bc3e6d345db42425824f1e7d21d35efa0c",
            "title": "Early Exiting with Ensemble Internal Classifiers"
        },
        {
            "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
        },
        {
            "paperId": "209f9bde2dee7cf1677801586562ffe56d435d38",
            "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"
        },
        {
            "paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d",
            "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"
        },
        {
            "paperId": "a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9",
            "title": "How many data points is a prompt worth?"
        },
        {
            "paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
            "title": "Making Pre-trained Language Models Better Few-shot Learners"
        },
        {
            "paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "title": "WARP: Word-level Adversarial ReProgramming"
        },
        {
            "paperId": "7eda139d737eea10fc1d95364327a41ec0cee4a4",
            "title": "CoLAKE: Contextualized Language and Knowledge Embedding"
        },
        {
            "paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9",
            "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"
        },
        {
            "paperId": "e34fdd75a78d08e1ff901ed466e6b1aaf4915750",
            "title": "FlauBERT : des mod\u00e8les de langue contextualis\u00e9s pr\u00e9-entra\u00een\u00e9s pour le fran\u00e7ais (FlauBERT : Unsupervised Language Model Pre-training for French)"
        },
        {
            "paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8",
            "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "8747f028acccde9ee7c35c858da8091613d3e574",
            "title": "Faster Depth-Adaptive Transformers"
        },
        {
            "paperId": "90a1491ac32e732c93773354e4e665794ed4d490",
            "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"
        },
        {
            "paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271",
            "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"
        },
        {
            "paperId": "2ffcf8352223c95ae8cef4daaec995525ecc926b",
            "title": "Adversarial Training for Large Neural Language Models"
        },
        {
            "paperId": "929b4775b6896634e11a8feb0ca4ca64ef7b3e24",
            "title": "Extractive Summarization as Text Matching"
        },
        {
            "paperId": "c5cc2340766d68ece08bb1520d357bcf8c03ad48",
            "title": "The Right Tool for the Job: Matching Model and Instance Complexities"
        },
        {
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
        },
        {
            "paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa",
            "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"
        },
        {
            "paperId": "dc0ce66f5ab4c5173cdef951649044e4c4c05076",
            "title": "BERT-ATTACK: Adversarial Attack against BERT Using BERT"
        },
        {
            "paperId": "e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286",
            "title": "A Survey on Contextual Embeddings"
        },
        {
            "paperId": "33496cb3a5623925267528fa6b726f015e4dcda2",
            "title": "Data Augmentation using Pre-trained Transformer Models"
        },
        {
            "paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e",
            "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"
        },
        {
            "paperId": "501a8b86428563539667e8117cd8409674ef97c3",
            "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"
        },
        {
            "paperId": "738215a396f6eee1709c6b521a6199769f0ce674",
            "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"
        },
        {
            "paperId": "efe638a32c6bd9ad24a233784008bfe5b33cfc83",
            "title": "Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"
        },
        {
            "paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0",
            "title": "A Primer in BERTology: What We Know About How BERT Works"
        },
        {
            "paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d",
            "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"
        },
        {
            "paperId": "12aa5c0c9fa3077f41f153cdccfa2aacf8940be3",
            "title": "From static to dynamic word representations: a survey"
        },
        {
            "paperId": "dc373d5e108a90a70f55285a852a32706adbeb45",
            "title": "Incorporating BERT into Neural Machine Translation"
        },
        {
            "paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5",
            "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"
        },
        {
            "paperId": "4243555758433880a67b15b50f752b1e2a8c4609",
            "title": "UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation"
        },
        {
            "paperId": "704a1a4ff7b6fed65b0c49ef87b6845d60755fa7",
            "title": "TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval"
        },
        {
            "paperId": "01ff40e32d810c09535bcfd21f315f3bc9248784",
            "title": "Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference"
        },
        {
            "paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7",
            "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"
        },
        {
            "paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86",
            "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"
        },
        {
            "paperId": "1359d2ef45f1550941e22bf046026c89f6edf315",
            "title": "AraBERT: Transformer-based Model for Arabic Language Understanding"
        },
        {
            "paperId": "157cdb1634f3f2c797e1c75d9eda259826424982",
            "title": "Adversarial Training for Aspect-Based Sentiment Analysis with BERT"
        },
        {
            "paperId": "7cf8510d5905bd8a63f1e098e05ab591d689e0fd",
            "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction"
        },
        {
            "paperId": "8d00049c345b9c8cc76ea2ea2565f8bb69f6b683",
            "title": "Retrospective Reader for Machine Reading Comprehension"
        },
        {
            "paperId": "495da6f19baa09c6db3697d839e10432cdc25934",
            "title": "Multilingual Denoising Pre-training for Neural Machine Translation"
        },
        {
            "paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d",
            "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"
        },
        {
            "paperId": "634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b",
            "title": "RobBERT: a Dutch RoBERTa-based Language Model"
        },
        {
            "paperId": "c6a84615bc36486cd0170f8a3e1b7e5ec8f5344e",
            "title": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation"
        },
        {
            "paperId": "c7fc1cac162c0e2a934704184c7554fd6b6253f0",
            "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model"
        },
        {
            "paperId": "a4d5e425cac0bf84c86c0c9f720b6339d6288ffa",
            "title": "BERTje: A Dutch BERT Model"
        },
        {
            "paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"
        },
        {
            "paperId": "3b2538f84812f434c740115c185be3e5e216c526",
            "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study"
        },
        {
            "paperId": "477d66dcd2c08243dcc69822d6da7ec06393773a",
            "title": "Multilingual is not enough: BERT for Finnish"
        },
        {
            "paperId": "f1957038e9ded19108d3c71340d7462152b70f25",
            "title": "Integrating Graph Contextualized Knowledge into Pre-trained Language Models"
        },
        {
            "paperId": "a75649771901a4881b44c0ceafa469fcc6e6f968",
            "title": "How Can We Know What Language Models Know?"
        },
        {
            "paperId": "f67fcbb1aec92ae293998ddfd904f61a31bef334",
            "title": "Inducing Relational Knowledge from BERT"
        },
        {
            "paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
            "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"
        },
        {
            "paperId": "c12e6c65e1de5d3993c5b65d0e234ae1f60c85ae",
            "title": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection"
        },
        {
            "paperId": "b61c6405f4de381758e8b52a20313554d68a9d85",
            "title": "CamemBERT: a Tasty French Language Model"
        },
        {
            "paperId": "9df6cc3bf35b70613abe95ad269ac74f169c9080",
            "title": "BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA"
        },
        {
            "paperId": "68c1bf884f0fc0e86641466a1f1fa67e79f16a17",
            "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"
        },
        {
            "paperId": "849b74ecce69108d36bf87ea9b87219cbfa10562",
            "title": "Negated LAMA: Birds cannot fly"
        },
        {
            "paperId": "348be8e64565a3df80d743f0580b63d3fbb49f35",
            "title": "SentiLARE: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis"
        },
        {
            "paperId": "497b1a8711d5f5f0a27254fd4a1bba16ad0d9625",
            "title": "SentiLR: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis"
        },
        {
            "paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
            "title": "Unsupervised Cross-lingual Representation Learning at Scale"
        },
        {
            "paperId": "6007bd2a34385132a7885b934d90b519a1f65bba",
            "title": "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"
        },
        {
            "paperId": "5534d774a06039e13b72876c21d39949132b512b",
            "title": "Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents"
        },
        {
            "paperId": "20b847537d3b7b9661733c1770c5faab3c0e2215",
            "title": "Biomedical Named Entity Recognition with Multilingual BERT"
        },
        {
            "paperId": "80e949e0e58e06c6f4f75ac4a2d7216b0fa1cfb8",
            "title": "Recycling a Pre-trained BERT Encoder for Neural Machine Translation"
        },
        {
            "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
            "paperId": "2be2f27c079663d3e3a769bcb04b0d341e76a707",
            "title": "SpeechBERT: Cross-Modal Pre-trained Language Model for End-to-end Spoken Question Answering"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "530a059cb48477ad1e3d4f8f4b153274c8997332",
            "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI"
        },
        {
            "paperId": "4585611042d2be0d997ee135e3fe219d668db9ec",
            "title": "Depth-Adaptive Transformer"
        },
        {
            "paperId": "b85d339e49399966d629973c889e8edfca56517c",
            "title": "A Mutual Information Maximization Perspective of Language Representation Learning"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "22b6e7e7063f596c213d7094ca991a29426be241",
            "title": "Progress Notes Classification and Keyword Extraction using Attention-based Deep Learning Models with BERT"
        },
        {
            "paperId": "327d7e55d64cb34d55bd3a3fe58233c238a312cd",
            "title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models"
        },
        {
            "paperId": "a23975683b044a871e39afb7781662f5eccaa4ba",
            "title": "Exploiting BERT for End-to-End Aspect-based Sentiment Analysis"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "d01fa0311e8e15b8b874b376123530c815f52852",
            "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding"
        },
        {
            "paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290",
            "title": "UNITER: UNiversal Image-TExt Representation Learning"
        },
        {
            "paperId": "740e4599b0e3113ad804cee4394c7fa7c0e96ca5",
            "title": "Extreme Language Model Compression with Optimal Subwords and Shared Projections"
        },
        {
            "paperId": "54416048772b921720f19869ed11c2a360589d03",
            "title": "UNITER: Learning UNiversal Image-TExt Representations"
        },
        {
            "paperId": "cc3be86706e0aff342e67b6ab84293fddc98423d",
            "title": "MobileBERT: Task-Agnostic Compression of BERT by Progressive Knowledge Transfer"
        },
        {
            "paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97",
            "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"
        },
        {
            "paperId": "025a0dc4a2a98742f1b410b6318a46de2c854b22",
            "title": "Learning Video Representations using Contrastive Bidirectional Transformer"
        },
        {
            "paperId": "663f4cc30a69aee07e291299d196806ead12d520",
            "title": "Technical report on Conversational Question Answering"
        },
        {
            "paperId": "aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47",
            "title": "Cross-Lingual Natural Language Generation via Pre-Training"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "06a73ad09664435f8b3cd90293f4e05a047cf375",
            "title": "K-BERT: Enabling Language Representation with Knowledge Graph"
        },
        {
            "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8",
            "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"
        },
        {
            "paperId": "3f9df96b26c42dea6dd6cad64557a3b7d698ea90",
            "title": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning"
        },
        {
            "paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09",
            "title": "Knowledge Enhanced Contextual Word Representations"
        },
        {
            "paperId": "0fc85e11928eb15d3c3a2fa737490ffc7b3986e2",
            "title": "Transformer to CNN: Label-scarce distillation for efficient text classification"
        },
        {
            "paperId": "e14c93e69cbf9645abb70cef09391f21f644d6d8",
            "title": "Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity"
        },
        {
            "paperId": "faaaf0a3c6e8283465bb719f1ee4999479a2624e",
            "title": "Informing Unsupervised Pretraining with External Linguistic Knowledge"
        },
        {
            "paperId": "65f788fb964901e3f1149a0a53317535ca85ed7d",
            "title": "Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"
        },
        {
            "paperId": "f98e135986414cccf29aec593d547c0656e4d82c",
            "title": "Commonsense Knowledge Mining from Pretrained Models"
        },
        {
            "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
            "title": "Language Models as Knowledge Bases?"
        },
        {
            "paperId": "4d6ca284c20a886919e517017a5f8dca97d1d50c",
            "title": "Specializing Word Embeddings (for Parsing) by Information Bottleneck"
        },
        {
            "paperId": "c93b2d64fce8737506757bbce51e17b533f9285b",
            "title": "On the use of BERT for Neural Machine Translation"
        },
        {
            "paperId": "2f9d4887d0022400fc40c774c4c78350c3bc5390",
            "title": "Small and Practical BERT Models for Sequence Labeling"
        },
        {
            "paperId": "6c1beae31b92c70b42ebeb99e5598d73bff6eea5",
            "title": "NEZHA: Neural Contextualized Representation for Chinese Language Understanding"
        },
        {
            "paperId": "4f2841cf0ed8edd5f9d2b7f76b95ec2a8674afb1",
            "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification"
        },
        {
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression"
        },
        {
            "paperId": "93ad19fbc85360043988fa9ea7932b7fdf1fa948",
            "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"
        },
        {
            "paperId": "4aa6298b606941a282d735fa3143da293199d2ca",
            "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"
        },
        {
            "paperId": "88051a6dce3b67541d8096647da2f6d31daa9e9a",
            "title": "Latent Relation Language Models"
        },
        {
            "paperId": "79c93274429d6355959f1e4374c2147bb81ea649",
            "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"
        },
        {
            "paperId": "3caf34532597683c980134579b156cd0d7db2f40",
            "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP"
        },
        {
            "paperId": "2bc1c8bd00bbf7401afcb5460277840fd8bab029",
            "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"
        },
        {
            "paperId": "3012f85c312412a6ac665cb1cc5180ad194332c8",
            "title": "Multi-Task Self-Supervised Learning for Disfluency Detection"
        },
        {
            "paperId": "772717eb2e369cd68c11b7da7aa779450dced9d0",
            "title": "SenseBERT: Driving Some Sense into BERT"
        },
        {
            "paperId": "8492269d2bb474d57d6def97efcf86c42735554a",
            "title": "BERT-based Ranking for Biomedical Entity Normalization"
        },
        {
            "paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
            "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"
        },
        {
            "paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
            "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"
        },
        {
            "paperId": "d56c1fc337fb07ec004dc846f80582c327af717c",
            "title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"
        },
        {
            "paperId": "b82153bf85d5d1edd3f170aace830e5328ca9ed0",
            "title": "Fusion of Detected Objects in Text for Visual Question Answering"
        },
        {
            "paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb",
            "title": "Text Summarization with Pretrained Encoders"
        },
        {
            "paperId": "fedea6d90747397cd144d1a419edf234110895d8",
            "title": "Mask and Infill: Applying Masked Language Model for Sentiment Transfer"
        },
        {
            "paperId": "a0e49f65b6847437f262c59d0d399255101d0b75",
            "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"
        },
        {
            "paperId": "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
            "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"
        },
        {
            "paperId": "335613303ebc5eac98de757ed02a56377d99e03a",
            "title": "What Does BERT Learn about the Structure of Language?"
        },
        {
            "paperId": "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2",
            "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc",
            "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"
        },
        {
            "paperId": "367c41f623f86e75d3154f6cab5b749cb7eb06b5",
            "title": "Searching for Effective Neural Extractive Summarization: What Works and What\u2019s Next"
        },
        {
            "paperId": "e7046bf945ad6326537a1ac78a96fd2f45acc900",
            "title": "Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension"
        },
        {
            "paperId": "2ff41a463a374b138bb5a012e5a32bc4beefec20",
            "title": "Pre-Training with Whole Word Masking for Chinese BERT"
        },
        {
            "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
            "paperId": "96901acc92d68350443774596fa2b38bc522a0ce",
            "title": "Barack\u2019s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling"
        },
        {
            "paperId": "f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c",
            "title": "Contrastive Bidirectional Transformer for Temporal Representation Learning"
        },
        {
            "paperId": "0de0a44b859a3719d11834479112314b4caba669",
            "title": "A Multiscale Visualization of Attention in the Transformer Model"
        },
        {
            "paperId": "afd110eace912c2b273e64851c6b4df2658622eb",
            "title": "Visualizing and Measuring the Geometry of BERT"
        },
        {
            "paperId": "809cc93921e4698bde891475254ad6dfba33d03b",
            "title": "How Multilingual is Multilingual BERT?"
        },
        {
            "paperId": "135112c7ba1762d65f39b1a61777f26ae4dfd8ad",
            "title": "Is Attention Interpretable?"
        },
        {
            "paperId": "455a8838cde44f288d456d01c76ede95b56dc675",
            "title": "A Structural Probe for Finding Syntax in Word Representations"
        },
        {
            "paperId": "82d40215de43fbc2aa3b0f8c6ebba73f35e64c9b",
            "title": "An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese"
        },
        {
            "paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583",
            "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
        },
        {
            "paperId": "cc94d15ba408c260c8fe4fa4f1cb6797a996dd21",
            "title": "Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"
        },
        {
            "paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde",
            "title": "ERNIE: Enhanced Language Representation with Informative Entities"
        },
        {
            "paperId": "07c53193b50aa0b108b14b9edfbef64ea1e9119b",
            "title": "Story Ending Prediction by Transferable BERT"
        },
        {
            "paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
            "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"
        },
        {
            "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
            "title": "BERT Rediscovers the Classical NLP Pipeline"
        },
        {
            "paperId": "a022bda79947d1f656a1164003c1b3ae9a843df9",
            "title": "How to Fine-Tune BERT for Text Classification?"
        },
        {
            "paperId": "dca404a59d66f196f55789e28033eaec85da6a91",
            "title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model"
        },
        {
            "paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88",
            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"
        },
        {
            "paperId": "145b8b5d99a2beba6029418ca043585b90138d12",
            "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"
        },
        {
            "paperId": "d9f6ada77448664b71128bb19df15765336974a6",
            "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
        },
        {
            "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
            "title": "Are Sixteen Heads Really Better than One?"
        },
        {
            "paperId": "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
            "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"
        },
        {
            "paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d",
            "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"
        },
        {
            "paperId": "7ebed46b7f3ec913e508e6468304fcaea832eda1",
            "title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"
        },
        {
            "paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
            "title": "ERNIE: Enhanced Representation through Knowledge Integration"
        },
        {
            "paperId": "b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb",
            "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"
        },
        {
            "paperId": "2a567ebd78939d0861d788f0fedff8d40ae62bf2",
            "title": "Publicly Available Clinical BERT Embeddings"
        },
        {
            "paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a",
            "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"
        },
        {
            "paperId": "a4bc4b98a917174ac2ab14bd5e66d64306079ab5",
            "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis"
        },
        {
            "paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89",
            "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"
        },
        {
            "paperId": "b5aa927c906101b3f8854a29f374551e3ea64474",
            "title": "Pre-trained language model representations for language generation"
        },
        {
            "paperId": "0de47f354468283efc7765ec0b3588b2ae483c77",
            "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence"
        },
        {
            "paperId": "9f1c5777a193b2c3bb2b25e248a156348e5ba56d",
            "title": "Cloze-driven Pretraining of Self-attention Networks"
        },
        {
            "paperId": "8659bf379ca8756755125a487c43cfe8611ce842",
            "title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks"
        },
        {
            "paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028",
            "title": "SciBERT: A Pretrained Language Model for Scientific Text"
        },
        {
            "paperId": "f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
            "title": "Linguistic Knowledge and Transferability of Contextual Representations"
        },
        {
            "paperId": "578e050d6e007797d032a07e712142035f2666dc",
            "title": "An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models"
        },
        {
            "paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f",
            "title": "Attention is not Explanation"
        },
        {
            "paperId": "403227333329b36183004f04db72362b604adef3",
            "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning"
        },
        {
            "paperId": "2a31319e73d4486716168b65cdf7559baeda18ce",
            "title": "Star-Transformer"
        },
        {
            "paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c",
            "title": "Parameter-Efficient Transfer Learning for NLP"
        },
        {
            "paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac",
            "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"
        },
        {
            "paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"
        },
        {
            "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
            "title": "Cross-lingual Language Model Pretraining"
        },
        {
            "paperId": "efeab0dcdb4c1cce5e537e57745d84774be99b9a",
            "title": "Assessing BERT's Syntactic Abilities"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "188024469a2443f262b3cbb5c5d4a96851949d68",
            "title": "Conditional BERT Contextual Augmentation"
        },
        {
            "paperId": "b47381e04739ea3f392ba6c8faaf64105493c196",
            "title": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"
        },
        {
            "paperId": "a3143eaa68040d366848a9c324b29d3f56f97a5d",
            "title": "Shallow-Deep Networks: Understanding and Mitigating Network Overthinking"
        },
        {
            "paperId": "22655979df781d222eaf812b0d325fa9adf11594",
            "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
        },
        {
            "paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094",
            "title": "CoQA: A Conversational Question Answering Challenge"
        },
        {
            "paperId": "af3825437b627db1a99f946f7aa773ba8b03befd",
            "title": "Learning deep representations by mutual information estimation and maximization"
        },
        {
            "paperId": "421fc2556836a6b441de806d7b393a35b6eaea58",
            "title": "Contextual String Embeddings for Sequence Labeling"
        },
        {
            "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
            "title": "Universal Transformers"
        },
        {
            "paperId": "0b43f66d99f43017db3e2975d38f1d86d1bde1ca",
            "title": "A Multi-task Approach to Learning Multilingual Representations"
        },
        {
            "paperId": "6411da05a0e6f3e38bcac0ce57c28038ff08081c",
            "title": "Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "d393943a873ead524069d0f7f55acef05cc9ba45",
            "title": "Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling"
        },
        {
            "paperId": "93b4cc549a1bc4bc112189da36c318193d05d806",
            "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform"
        },
        {
            "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "title": "Deep Contextualized Word Representations"
        },
        {
            "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "title": "Universal Language Model Fine-tuning for Text Classification"
        },
        {
            "paperId": "8dd85e38445a5ddb5dd71cabc3c4246de30c014f",
            "title": "A Survey of Model Compression and Acceleration for Deep Neural Networks"
        },
        {
            "paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7",
            "title": "Learned in Translation: Contextualized Word Vectors"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning"
        },
        {
            "paperId": "263210f256603e3b62476ffb5b9bbbbc6403b646",
            "title": "What do Neural Machine Translation Models Learn about Morphology?"
        },
        {
            "paperId": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38",
            "title": "Semi-supervised sequence tagging with bidirectional language models"
        },
        {
            "paperId": "896de8418884f4aab1ae4a60027500c9e8baffc3",
            "title": "BranchyNet: Fast inference via early exiting from deep neural networks"
        },
        {
            "paperId": "bd345877856dc83c2c10c125dbf0f41e2bde38b1",
            "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding"
        },
        {
            "paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc",
            "title": "Unsupervised Pretraining for Sequence to Sequence Learning"
        },
        {
            "paperId": "67d968c7450878190e45ac7886746de867bf673d",
            "title": "Neural Architecture Search with Reinforcement Learning"
        },
        {
            "paperId": "36eff562f65125511b5dfab68ce7f7a943c27478",
            "title": "Semi-Supervised Classification with Graph Convolutional Networks"
        },
        {
            "paperId": "59761abc736397539bdd01ad7f9d91c8607c0457",
            "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"
        },
        {
            "paperId": "e2dba792360873aef125572812f3673b1a85d850",
            "title": "Enriching Word Vectors with Subword Information"
        },
        {
            "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
            "paperId": "e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b",
            "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning"
        },
        {
            "paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01",
            "title": "Adaptive Computation Time for Recurrent Neural Networks"
        },
        {
            "paperId": "96acb1c882ad655c6b8459c2cd331803801446ca",
            "title": "Representation Learning of Knowledge Graphs with Entity Descriptions"
        },
        {
            "paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
            "title": "Semi-supervised Sequence Learning"
        },
        {
            "paperId": "318b558717ff9a4a996e45368b26a1233f03d1d7",
            "title": "Aligning Knowledge and Text Embeddings by Entity Descriptions"
        },
        {
            "paperId": "adcfebbe2a1960e5a23243d1a5f3837832109ff1",
            "title": "Distributional vectors encode referential attributes"
        },
        {
            "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
            "title": "Neural Machine Translation of Rare Words with Subword Units"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "1f600f213dbbd70f06093438855f39022957b4bf",
            "title": "Long Short-Term Memory Over Recursive Structures"
        },
        {
            "paperId": "311d7757c7820ff4b28958403ae077eb9458d91f",
            "title": "How Well Do Distributional Models Capture Different Types of Semantic Knowledge?"
        },
        {
            "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "title": "Skip-Thought Vectors"
        },
        {
            "paperId": "c7188396767227c0d9ed087a7b077f22fccd7372",
            "title": "Bilingual Word Representations with Monolingual Quality in Mind"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "f0efb4f8e1e5957bb252d9d530202b1cef9b0494",
            "title": "Knowledge Graph and Text Jointly Embedding"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
            "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
            "title": "Distributed Representations of Sentences and Documents"
        },
        {
            "paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
            "title": "A Convolutional Neural Network for Modelling Sentences"
        },
        {
            "paperId": "9d3aaa919c78c06f24588d97ed1028d51860b321",
            "title": "Improving Vector Space Word Representations Using Multilingual Correlation"
        },
        {
            "paperId": "53ca064b9f1b92951c1997e90b776e95b0880e52",
            "title": "Learning word embeddings efficiently with noise-contrastive estimation"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
            "title": "Linguistic Regularities in Continuous Space Word Representations"
        },
        {
            "paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "title": "Representation Learning: A Review and New Perspectives"
        },
        {
            "paperId": "bc1022b031dc6c7019696492e8116598097a8c12",
            "title": "Natural Language Processing (Almost) from Scratch"
        },
        {
            "paperId": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972",
            "title": "A Survey on Transfer Learning"
        },
        {
            "paperId": "268b8f10a45e71f63daab6403bb453da31ae28a7",
            "title": "Melting of Peridotite to 140 Gigapascals"
        },
        {
            "paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
        },
        {
            "paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
            "title": "Why Does Unsupervised Pre-training Help Deep Learning?"
        },
        {
            "paperId": "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9",
            "title": "Model compression"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "3106e66537a0c8f53278e553bcb38f0b0992ec0e",
            "title": "Distributed Representations"
        },
        {
            "paperId": "4bb08f30bb2c83334b21fbbc68c3f2622d4fb04b",
            "title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
        },
        {
            "paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd",
            "title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"
        },
        {
            "paperId": "53d8b356551a2361020a948f64454a6d599af69f",
            "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
        },
        {
            "paperId": null,
            "title": "Pre-trained Models for Natural Language Advances"
        },
        {
            "paperId": "08ee34a64247c0fe3c22b9f3c0848eb921041a8d",
            "title": "Supplementary Material: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
        },
        {
            "paperId": null,
            "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"
        },
        {
            "paperId": null,
            "title": "Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are fewshot learners"
        },
        {
            "paperId": null,
            "title": "ELECTRA: Pre-training text encoders as discriminators rather than generators"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "a3347bbd82938788ec085772813c095de17a0b37",
            "title": "Is BERT Really Robust? Natural Language Attack on Text Classi\ufb01cation and Entailment"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "\u00c9ric Villemonte de la Clergerie"
        },
        {
            "paperId": null,
            "title": "BERT and PALs: Projected attention layers for e \ufb03 cient adaptation in multi-task learning"
        },
        {
            "paperId": null,
            "title": "MobileBERT: Task-agnostic com- 26 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) pression of BERT by progressive knowledge transfer"
        },
        {
            "paperId": null,
            "title": "Pre-trained Models for Natural Language Processing: A Survey March (2020) 23 gual is multilingual BERT"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": "c50dca78e97e335d362d6b991ae0e1448914e9a3",
            "title": "Reducing the Dimensionality of Data with Neural"
        },
        {
            "paperId": null,
            "title": "BART: denoising sequence-to-24"
        }
    ]
}