{
    "paperId": "9af2264799bdc3490e4650e2f5d126762caf420f",
    "externalIds": {
        "DBLP": "conf/icassp/KimHW17",
        "ArXiv": "1609.06773",
        "MAG": "2952470929",
        "DOI": "10.1109/ICASSP.2017.7953075",
        "CorpusId": 206742834
    },
    "title": "Joint CTC-attention based end-to-end speech recognition using multi-task learning",
    "abstract": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4\u201314.6% relative improvements in Character Error Rate (CER).",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2016,
    "referenceCount": 24,
    "citationCount": 863,
    "influentialCitationCount": 70,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1609.06773",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2678842",
            "name": "Suyoun Kim"
        },
        {
            "authorId": "145443186",
            "name": "Takaaki Hori"
        },
        {
            "authorId": "1746678",
            "name": "Shinji Watanabe"
        }
    ],
    "references": [
        {
            "paperId": "1f7fb2c16e51f207eb1816f4f3fc3e1649c364f0",
            "title": "An analysis of environment, microphone and data simulation mismatches in robust speech recognition"
        },
        {
            "paperId": "39a8ce943d0b4171e7fa6e4cab2d80cdb23cdbf1",
            "title": "On Online Attention-Based Speech Recognition and Joint Mandarin Character-Pinyin Training"
        },
        {
            "paperId": "77854e1a86835065b77b7b15ffabb34f3853f4a2",
            "title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition"
        },
        {
            "paperId": "878ba5458e9e51f0b341fd9117fa0b43ef4096d3",
            "title": "End-to-end attention-based large vocabulary speech recognition"
        },
        {
            "paperId": "97acdfb3d247f8250d865ef8a9169f06e40f138b",
            "title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding"
        },
        {
            "paperId": "b624504240fa52ab76167acfe3156150ca01cf3b",
            "title": "Attention-Based Models for Speech Recognition"
        },
        {
            "paperId": "24741d280869ad9c60321f5ab6e5f01b7852507d",
            "title": "Deep Speech: Scaling up end-to-end speech recognition"
        },
        {
            "paperId": "47d2dc34e1d02a8109f5c04bb6939725de23716d",
            "title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f",
            "title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks"
        },
        {
            "paperId": "1149888d75af4ed5dffc25731b875651c3ccdeb2",
            "title": "Hybrid speech recognition with Deep Bidirectional LSTM"
        },
        {
            "paperId": "8729441d734782c3ed532a7d2d9611b438c0a09a",
            "title": "ADADELTA: An Adaptive Learning Rate Method"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "67156902beca9bc90b728c8d5dd4ac9d8b27d3a3",
            "title": "Chainer : a Next-Generation Open Source Framework for Deep Learning"
        },
        {
            "paperId": null,
            "title": "Listen, attend and spell"
        },
        {
            "paperId": "d2b62f77cb2864e465aa60bca6c26bb1d2f84963",
            "title": "Acoustic Modeling Using Deep Belief Networks"
        },
        {
            "paperId": "31868290adf1c000c611dfc966b514d5a34e8d23",
            "title": "FUNDAMENTAL TECHNOLOGIES IN MODERN SPEECH RECOGNITION Digital Object Identifier 10.1109/MSP.2012.2205597"
        },
        {
            "paperId": null,
            "title": "CSR-I (wsj0) complete, \" Linguistic Data Consortium"
        },
        {
            "paperId": null,
            "title": "\u201cCSR-II (wsj1) complete,\u201d"
        },
        {
            "paperId": "261a056f8b21918e8616a429b2df6e1d5d33be41",
            "title": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
        },
        {
            "paperId": "845ee9838c1f5bf63b7db2c95ec5d27af14a4e02",
            "title": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequences with Recurrent Neural Networks"
        },
        {
            "paperId": null,
            "title": "Preferred Networks"
        }
    ]
}