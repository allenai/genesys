{
    "paperId": "adf3b591281688b7e71b254ab931b2aa39b4b59f",
    "externalIds": {
        "ArXiv": "1505.00853",
        "DBLP": "journals/corr/XuWCL15",
        "MAG": "1921523184",
        "CorpusId": 14083350
    },
    "title": "Empirical Evaluation of Rectified Activations in Convolutional Network",
    "abstract": "In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.",
    "venue": "arXiv.org",
    "year": 2015,
    "referenceCount": 17,
    "citationCount": 2731,
    "influentialCitationCount": 273,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results, and are negative on the common belief that sparsity is the key of good performance in ReLU."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2113742783",
            "name": "Bing Xu"
        },
        {
            "authorId": "48246959",
            "name": "Naiyan Wang"
        },
        {
            "authorId": "1913774",
            "name": "Tianqi Chen"
        },
        {
            "authorId": "2112144126",
            "name": "Mu Li"
        }
    ],
    "references": [
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
        },
        {
            "paperId": "e3c433ab9608d7329f944552ba1721e277a42d74",
            "title": "Transferring Rich Feature Hierarchies for Robust Visual Tracking"
        },
        {
            "paperId": "e70d75454de81b05fed9b63a405bcd1229f20229",
            "title": "Deeply learned face representations are sparse, selective, and robust"
        },
        {
            "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "title": "Going deeper with convolutions"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "cbb19236820a96038d000dc629225d36e0b6294a",
            "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"
        },
        {
            "paperId": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "title": "Network In Network"
        },
        {
            "paperId": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "367f2c63a6f6a10b3b64b8729d601e69337ee3cc",
            "title": "Rectifier Nonlinearities Improve Neural Network Acoustic Models"
        },
        {
            "paperId": null,
            "title": "Deep sparse rectifier networks"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": null,
            "title": "Convergence curves for training and test sets of different activations on CIFAR-100 Network in Network. Figure 4: Convergence curves for training and test sets of different activations on NDSB Net"
        },
        {
            "paperId": null,
            "title": "The superiority of RReLU is more signi\ufb01cant than that on CIFAR-10/CIFAR-100"
        }
    ]
}