{
    "paperId": "30c0cdc414f68211d5d0514df027cec22e005174",
    "externalIds": {
        "ArXiv": "2301.00234",
        "CorpusId": 255372865
    },
    "title": "A Survey on In-context Learning",
    "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
    "venue": "",
    "year": 2022,
    "referenceCount": 180,
    "citationCount": 215,
    "influentialCitationCount": 14,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents a formal definition of ICL and clarify its correlation to related studies, and organizes and discusses advanced techniques, including training strategies, prompt designing strategies, and related analysis."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.8527350425720215,
            -0.7546036243438721,
            -0.512082576751709,
            4.899658679962158,
            4.477057456970215,
            0.26989006996154785,
            4.266345977783203,
            0.3924616277217865,
            -1.5710296630859375,
            3.522994041442871,
            -0.6975679993629456,
            4.2106499671936035,
            -1.2159050703048706,
            0.7539839744567871,
            -1.4783117771148682,
            -0.49085503816604614,
            0.24175062775611877,
            -3.888371467590332,
            3.417485475540161,
            0.3502362370491028,
            -1.111421823501587,
            1.8171961307525635,
            -2.3051023483276367,
            -1.853011131286621,
            0.7765157222747803,
            -1.0645027160644531,
            6.283775329589844,
            0.3346548080444336,
            -3.3660669326782227,
            1.0861196517944336,
            0.14849162101745605,
            -4.42243766784668,
            2.97383975982666,
            -6.47312068939209,
            -0.03635752201080322,
            -4.9629716873168945,
            -2.3364288806915283,
            7.895944118499756,
            -2.1301116943359375,
            -2.9263384342193604,
            -3.725749969482422,
            -0.3520156443119049,
            -0.8472473621368408,
            3.637561798095703,
            2.7280023097991943,
            0.03197270631790161,
            3.5274739265441895,
            1.541506290435791,
            0.08436325192451477,
            -0.2797013521194458,
            0.987734317779541,
            -1.654282808303833,
            -0.9400058388710022,
            3.652585029602051,
            2.8994452953338623,
            0.15467986464500427,
            1.6036185026168823,
            1.9066524505615234,
            5.291553020477295,
            -0.28315049409866333,
            4.010716438293457,
            6.965943336486816,
            -2.2320337295532227,
            3.3580691814422607,
            2.6179404258728027,
            -3.7121682167053223,
            -3.0880799293518066,
            2.3335318565368652,
            1.5214471817016602,
            4.966485977172852,
            -0.3309498429298401,
            -4.823667526245117,
            0.7379775047302246,
            2.0942463874816895,
            -0.613760232925415,
            -0.687959611415863,
            0.9071400761604309,
            -7.12239933013916,
            -0.4495893120765686,
            -1.5078859329223633,
            0.8518692255020142,
            1.7128431797027588,
            2.5461926460266113,
            2.956364870071411,
            2.6642024517059326,
            -1.688735008239746,
            -7.680617809295654,
            4.386794090270996,
            2.607759952545166,
            -4.910983085632324,
            -1.9106199741363525,
            -1.3243122100830078,
            0.5613409280776978,
            2.957669734954834,
            -2.986968517303467,
            -2.1596689224243164,
            3.534363269805908,
            -2.5001487731933594,
            -3.1478376388549805,
            -3.3282968997955322,
            5.491055011749268,
            -0.4645775854587555,
            1.00956130027771,
            3.1792540550231934,
            3.97746205329895,
            -7.2652812004089355,
            1.762300968170166,
            -0.6942384839057922,
            1.3824048042297363,
            0.37202203273773193,
            0.5483981966972351,
            3.2877585887908936,
            -0.8057073950767517,
            -4.126528739929199,
            0.8843905925750732,
            -2.246412992477417,
            -2.1567277908325195,
            0.1930854618549347,
            -1.282209873199463,
            5.824666976928711,
            -0.9138437509536743,
            2.0633325576782227,
            -1.436502456665039,
            -0.5846929550170898,
            -0.30206039547920227,
            2.1865744590759277,
            0.31484976410865784,
            -3.451082706451416,
            -0.026439428329467773,
            -3.8097333908081055,
            2.68989896774292,
            1.163663387298584,
            4.7545599937438965,
            -0.9128850698471069,
            4.66523551940918,
            3.2314913272857666,
            -3.0259251594543457,
            -0.867234468460083,
            -0.6015045642852783,
            -1.7686470746994019,
            -1.6066040992736816,
            4.89641809463501,
            -1.411543369293213,
            0.46978843212127686,
            0.11178961396217346,
            -1.4025232791900635,
            1.151771903038025,
            -5.823831558227539,
            0.9220241904258728,
            5.506736755371094,
            3.0401155948638916,
            -3.235658645629883,
            -1.1500829458236694,
            -0.8374494910240173,
            -0.3587891459465027,
            5.440636157989502,
            -0.5497795939445496,
            -0.6608877778053284,
            -1.9226431846618652,
            -0.8179104328155518,
            2.5588316917419434,
            3.5780839920043945,
            -8.117203712463379,
            -1.5021450519561768,
            2.872596502304077,
            -5.799769401550293,
            -0.3785577714443207,
            5.561375617980957,
            -0.7219455242156982,
            1.2510244846343994,
            -1.3558520078659058,
            1.3123681545257568,
            -0.3907402455806732,
            2.7750821113586426,
            0.6448867321014404,
            0.2665221095085144,
            0.5006243586540222,
            -3.982851982116699,
            -0.6923898458480835,
            -0.7250000238418579,
            -3.1067328453063965,
            0.30129164457321167,
            -7.164987087249756,
            -1.0526093244552612,
            -1.5475714206695557,
            -1.3007211685180664,
            -0.5665596723556519,
            -6.157016277313232,
            3.780625820159912,
            -2.6292314529418945,
            -1.6524972915649414,
            0.1393427848815918,
            7.097524166107178,
            6.032270431518555,
            5.587544918060303,
            1.3071632385253906,
            0.6415549516677856,
            4.158721923828125,
            -2.1509010791778564,
            1.598193645477295,
            3.114089012145996,
            0.3483806848526001,
            -0.7634931802749634,
            -2.0075719356536865,
            2.6552696228027344,
            2.994357109069824,
            -3.483726978302002,
            5.126012802124023,
            3.113497734069824,
            0.6228510737419128,
            0.8269225358963013,
            1.6628270149230957,
            -1.3257622718811035,
            3.905247688293457,
            -1.1143699884414673,
            -0.7584583759307861,
            -5.235795974731445,
            2.8945670127868652,
            3.1340537071228027,
            1.2157317399978638,
            -2.6653714179992676,
            0.6702333092689514,
            2.368992328643799,
            -1.9128789901733398,
            2.348461866378784,
            -4.309146881103516,
            2.3288354873657227,
            -2.228360176086426,
            -2.204742431640625,
            -1.5975420475006104,
            -5.33629035949707,
            -0.675821840763092,
            -1.5461931228637695,
            -1.6908042430877686,
            -5.983082294464111,
            -1.1831436157226562,
            -7.721385955810547,
            0.5516541004180908,
            -0.11167159676551819,
            -4.362125396728516,
            3.948197841644287,
            4.2951483726501465,
            2.0447964668273926,
            6.580845832824707,
            1.2817720174789429,
            2.539154052734375,
            -1.9979171752929688,
            3.720768451690674,
            -1.3411405086517334,
            -1.6932713985443115,
            -1.7572935819625854,
            -1.8210867643356323,
            5.110043525695801,
            0.5703147649765015,
            0.7821959853172302,
            1.9395922422409058,
            -1.3172494173049927,
            1.3529558181762695,
            0.30250218510627747,
            2.0282161235809326,
            2.715778350830078,
            5.374463081359863,
            -0.362440288066864,
            1.3435429334640503,
            -2.016901731491089,
            0.3508461117744446,
            -3.165832042694092,
            -2.4422569274902344,
            -2.848276138305664,
            4.519071578979492,
            2.7201008796691895,
            0.9217418432235718,
            -1.4023175239562988,
            -2.981635093688965,
            -1.6190805435180664,
            -7.739907264709473,
            -0.46473440527915955,
            -1.3990027904510498,
            1.710495114326477,
            0.2536503076553345,
            1.3179206848144531,
            -0.9846851825714111,
            -1.9963512420654297,
            0.5623540282249451,
            1.7788395881652832,
            -1.7008006572723389,
            0.003840148448944092,
            -0.3511722683906555,
            -2.117227554321289,
            -5.729506969451904,
            -0.8643732666969299,
            4.160028457641602,
            -2.3735623359680176,
            -1.7858116626739502,
            -0.41818514466285706,
            1.1379839181900024,
            2.392061233520508,
            0.8509479761123657,
            -0.02084759622812271,
            -1.9206798076629639,
            -1.066606044769287,
            3.099658250808716,
            2.7320878505706787,
            0.6901866793632507,
            0.006881386041641235,
            1.7652032375335693,
            -0.39344415068626404,
            0.7137261629104614,
            0.17561107873916626,
            -1.1816534996032715,
            3.1292195320129395,
            0.006207793951034546,
            4.604205131530762,
            -7.102889537811279,
            -1.3435804843902588,
            1.3403501510620117,
            -1.3312166929244995,
            0.8811602592468262,
            -3.856133460998535,
            1.2059305906295776,
            -0.814420223236084,
            -0.8847228288650513,
            -3.1490259170532227,
            -3.7936882972717285,
            -3.415529251098633,
            2.6155333518981934,
            -0.8248171806335449,
            -0.1511777639389038,
            -5.79362678527832,
            0.914326012134552,
            0.3864186108112335,
            3.660435438156128,
            2.396455764770508,
            1.7584905624389648,
            -5.1637725830078125,
            -3.8538239002227783,
            -0.5284106731414795,
            -2.8368701934814453,
            2.9383492469787598,
            -0.15485666692256927,
            2.296018123626709,
            5.372269630432129,
            -0.2509465217590332,
            0.020980358123779297,
            -0.46673470735549927,
            2.3254241943359375,
            1.0329837799072266,
            -1.5615208148956299,
            -2.822007179260254,
            -2.9061522483825684,
            0.7441514134407043,
            2.4261085987091064,
            1.8335528373718262,
            0.7187438607215881,
            3.2854349613189697,
            2.262424945831299,
            3.516540050506592,
            1.245734453201294,
            -2.0250539779663086,
            0.7846344709396362,
            -2.3639042377471924,
            -1.3311935663223267,
            4.587536334991455,
            2.8751792907714844,
            1.3752131462097168,
            -0.7087458372116089,
            10.30007553100586,
            -4.676962852478027,
            0.3528864085674286,
            -6.814830780029297,
            -3.6480026245117188,
            -4.567077159881592,
            -4.491034030914307,
            4.377444267272949,
            -4.081338405609131,
            -4.748433589935303,
            -2.3025941848754883,
            -1.954262375831604,
            2.0682005882263184,
            2.9858133792877197,
            -0.9083095192909241,
            3.2170958518981934,
            -2.731858015060425,
            0.28990456461906433,
            -0.32875198125839233,
            0.828434407711029,
            -2.0941057205200195,
            1.669154405593872,
            -0.9698404669761658,
            1.2182109355926514,
            1.2937114238739014,
            -0.25229036808013916,
            0.3876115083694458,
            0.26475998759269714,
            -2.2997031211853027,
            1.4685781002044678,
            0.5238747596740723,
            -3.838923931121826,
            0.3779163360595703,
            1.4126609563827515,
            -0.07768476009368896,
            -0.10882794857025146,
            0.8935995101928711,
            3.6554856300354004,
            -3.31015944480896,
            2.8659112453460693,
            0.21597683429718018,
            -0.2862822115421295,
            -0.8073481321334839,
            -0.8352178335189819,
            -4.714669227600098,
            -2.5083351135253906,
            -1.5866539478302002,
            -3.208092212677002,
            -0.814301609992981,
            0.45986488461494446,
            1.5305583477020264,
            3.0679657459259033,
            -0.031166493892669678,
            1.7221379280090332,
            -5.078159809112549,
            2.9455349445343018,
            8.056748390197754,
            0.48902395367622375,
            -1.3460092544555664,
            0.7441113591194153,
            2.973924160003662,
            -0.7967709302902222,
            0.29379868507385254,
            3.8620049953460693,
            0.24801582098007202,
            3.395263910293579,
            -1.1903574466705322,
            -1.9191491603851318,
            0.3735496699810028,
            2.0858163833618164,
            3.0702807903289795,
            3.5168631076812744,
            4.436532974243164,
            -4.799369812011719,
            -1.6630427837371826,
            2.8314685821533203,
            -2.222759962081909,
            2.2715065479278564,
            1.7204811573028564,
            3.0681204795837402,
            1.3242021799087524,
            0.5025288462638855,
            -3.0961146354675293,
            0.0064574480056762695,
            4.515087127685547,
            -0.10785573720932007,
            -3.436026096343994,
            -2.5094518661499023,
            -0.8565800189971924,
            1.4604575634002686,
            -3.6624932289123535,
            -1.2387785911560059,
            -1.0814106464385986,
            1.0406758785247803,
            -2.3903563022613525,
            1.4104886054992676,
            -1.1263529062271118,
            1.3927390575408936,
            0.3070733845233917,
            -0.43662145733833313,
            -0.7264810800552368,
            -5.107277870178223,
            -2.2314045429229736,
            4.150320053100586,
            3.108435869216919,
            -1.2679948806762695,
            -3.9936931133270264,
            0.5027709007263184,
            -1.53515625,
            3.137053966522217,
            3.9388723373413086,
            -0.8745497465133667,
            2.695242404937744,
            -5.4242753982543945,
            -4.523980140686035,
            1.717132568359375,
            3.6726183891296387,
            -1.5969849824905396,
            -0.9348087310791016,
            3.7024712562561035,
            0.3345072269439697,
            0.4744616746902466,
            0.49690163135528564,
            -1.8496241569519043,
            0.27407026290893555,
            3.7947354316711426,
            1.941798448562622,
            1.3836357593536377,
            -1.1236783266067505,
            0.34280145168304443,
            -4.164931297302246,
            0.9022414088249207,
            2.4970703125,
            3.415010452270508,
            3.419159173965454,
            -4.53664493560791,
            1.1322096586227417,
            -0.918854832649231,
            -1.9144989252090454,
            8.154111862182617,
            5.445615291595459,
            -1.0568022727966309,
            -0.7645632028579712,
            -0.4600982666015625,
            -0.4186936914920807,
            -0.2918679118156433,
            -6.6207685470581055,
            2.606891632080078,
            -0.6832505464553833,
            -1.7218742370605469,
            2.523184299468994,
            -0.9955016374588013,
            -0.6161161065101624,
            0.3773000240325928,
            -0.23439151048660278,
            -2.136340856552124,
            -0.6127526760101318,
            0.9451242685317993,
            1.431962251663208,
            -0.11579263210296631,
            -2.2170231342315674,
            -0.16905295848846436,
            1.381657600402832,
            4.833199501037598,
            6.791133880615234,
            1.912508487701416,
            2.9113945960998535,
            -1.8981064558029175,
            0.3194708526134491,
            -4.524813652038574,
            2.887911796569824,
            4.728070259094238,
            -3.7393527030944824,
            0.962198793888092,
            -0.06885045766830444,
            2.3400063514709473,
            2.1934542655944824,
            5.492881774902344,
            1.9194889068603516,
            1.5914331674575806,
            -3.914414644241333,
            0.9226563572883606,
            0.9785594940185547,
            0.26344406604766846,
            1.3872439861297607,
            -1.7217583656311035,
            5.19788932800293,
            3.251009225845337,
            -2.8681225776672363,
            -1.8594844341278076,
            -1.303627848625183,
            -0.8331217169761658,
            -2.23801851272583,
            2.2929553985595703,
            2.578925132751465,
            -1.0823673009872437,
            -0.3119220733642578,
            2.2441303730010986,
            -1.2096130847930908,
            0.7673647403717041,
            -3.073113441467285,
            2.137446880340576,
            1.6170475482940674,
            -2.4494681358337402,
            -1.4999263286590576,
            -3.30788254737854,
            0.8873711824417114,
            1.24452543258667,
            0.3927006721496582,
            3.905224323272705,
            2.060204029083252,
            0.686800479888916,
            1.0891990661621094,
            -1.8692383766174316,
            1.4620089530944824,
            -2.1150758266448975,
            -0.3498826324939728,
            -2.0630173683166504,
            1.110878586769104,
            -0.7204346060752869,
            -3.7295408248901367,
            3.3295609951019287,
            -4.326488971710205,
            0.2129138708114624,
            5.496034145355225,
            1.2566320896148682,
            2.4320805072784424,
            0.23601305484771729,
            3.100320816040039,
            -4.634291648864746,
            2.078110933303833,
            0.14541447162628174,
            0.8750104308128357,
            3.543585777282715,
            -0.7951024770736694,
            4.937921524047852,
            2.246138095855713,
            1.1912671327590942,
            -0.7178955078125,
            -2.474527359008789,
            3.448000431060791,
            -1.9754602909088135,
            -0.474710613489151,
            -0.6737234592437744,
            -4.2625885009765625,
            1.0452040433883667,
            15.572938919067383,
            -0.4168529212474823,
            1.005359411239624,
            0.5839888453483582,
            -1.7603024244308472,
            -3.895829916000366,
            -1.5242516994476318,
            1.5962762832641602,
            1.1376875638961792,
            -0.4231853485107422,
            -3.333686590194702,
            -5.135342597961426,
            -1.920207142829895,
            0.12473702430725098,
            -2.93906831741333,
            -1.6051238775253296,
            -2.054642677307129,
            2.215449333190918,
            -1.7205144166946411,
            -0.25053369998931885,
            -1.2147901058197021,
            3.0073776245117188,
            -4.237616062164307,
            -3.7850863933563232,
            -1.5677794218063354,
            -0.4290740191936493,
            2.0610623359680176,
            1.7247636318206787,
            -4.54649543762207,
            -0.3662787675857544,
            0.6434557437896729,
            2.2044739723205566,
            -0.053119659423828125,
            0.5859130620956421,
            -1.272071123123169,
            2.407299518585205,
            2.0979907512664795,
            -3.6921749114990234,
            1.8628737926483154,
            0.9609911441802979,
            -0.5172718167304993,
            1.004892349243164,
            -5.110366344451904,
            2.4002420902252197,
            -2.5637359619140625,
            0.5877366065979004,
            4.016232490539551,
            0.25348204374313354,
            -0.38572239875793457,
            5.356852054595947,
            -2.3050808906555176,
            -2.5511465072631836,
            -1.0449875593185425,
            -0.390003502368927,
            1.7204442024230957,
            -4.289185523986816,
            0.6391244530677795,
            -3.2296791076660156,
            -1.8742971420288086,
            -0.9852900505065918,
            1.5261121988296509,
            -0.6705683469772339,
            1.234668493270874,
            -2.225841522216797,
            -1.8630993366241455,
            -0.997340977191925,
            -3.7712697982788086,
            1.9598324298858643,
            0.5702730417251587,
            -0.3584446609020233,
            3.4337539672851562,
            2.045926094055176,
            -1.5080313682556152,
            -2.8297696113586426,
            -0.8715429306030273,
            -1.6521878242492676,
            1.9108600616455078,
            -0.7128004431724548,
            -1.002392053604126,
            4.200014114379883,
            1.0177699327468872,
            3.487126588821411,
            -1.9329466819763184,
            -2.5625059604644775,
            2.143714427947998,
            -0.5272455215454102,
            1.3201736211776733,
            2.6877708435058594,
            -4.246222496032715,
            1.6488370895385742,
            0.045207679271698,
            2.550182580947876,
            4.3531174659729,
            2.5733838081359863,
            2.1214234828948975,
            -4.791167736053467,
            -1.6615056991577148,
            -1.294159173965454,
            -3.9680590629577637,
            -0.6890215873718262,
            4.678427696228027,
            2.2156872749328613,
            -0.329220175743103,
            -1.7957143783569336,
            0.20935668051242828,
            0.16378004848957062,
            -3.079270362854004,
            -6.5679426193237305,
            -3.8285341262817383,
            -1.9430338144302368,
            3.3954830169677734,
            -2.0037412643432617,
            -0.3550165891647339,
            0.9779202938079834,
            3.2461259365081787,
            -1.6985266208648682,
            1.9993853569030762,
            -0.9136406779289246,
            0.2512992024421692,
            6.539231300354004,
            0.1832006722688675,
            -2.0496459007263184,
            -0.8090384006500244,
            -4.307929515838623,
            -2.097827911376953,
            0.6718111634254456,
            0.643305242061615,
            -4.335667610168457,
            -2.649137496948242,
            -0.6619939804077148,
            4.2471489906311035,
            -5.0492024421691895,
            -0.5431088209152222,
            -0.34734970331192017,
            0.8786135315895081,
            -0.4530167579650879,
            2.7429161071777344,
            1.8140075206756592,
            0.009771570563316345,
            6.2727861404418945,
            -1.7981241941452026,
            -2.6599783897399902,
            -0.4783751964569092,
            8.808181762695312,
            2.108243942260742,
            -0.7262203693389893,
            -2.1820428371429443,
            0.5924553871154785,
            2.2174272537231445,
            0.8344676494598389,
            3.422983169555664,
            0.5056295394897461,
            -0.029806464910507202,
            1.3956440687179565,
            -4.774432182312012,
            -3.39504075050354
        ]
    },
    "authors": [
        {
            "authorId": "2047143813",
            "name": "Qingxiu Dong"
        },
        {
            "authorId": "49192881",
            "name": "Lei Li"
        },
        {
            "authorId": "10780897",
            "name": "Damai Dai"
        },
        {
            "authorId": "2113919886",
            "name": "Ce Zheng"
        },
        {
            "authorId": "150358371",
            "name": "Zhiyong Wu"
        },
        {
            "authorId": "7267809",
            "name": "Baobao Chang"
        },
        {
            "authorId": "2116530295",
            "name": "Xu Sun"
        },
        {
            "authorId": "47883405",
            "name": "Jingjing Xu"
        },
        {
            "authorId": "3335836",
            "name": "Zhifang Sui"
        }
    ],
    "references": [
        {
            "paperId": "e1cde20b294d99e7335c9410026d76bb51c3e537",
            "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers"
        },
        {
            "paperId": "093f9b0cd66a2356f18ecac15cf4209edae1ca4c",
            "title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks"
        },
        {
            "paperId": "e88a6a1b385d4ce14e0eea162bd821ae8320291f",
            "title": "Feature-Adaptive and Data-Scalable In-Context Learning"
        },
        {
            "paperId": "8cfb990489950eb5734198ef163410a387081c20",
            "title": "In-Context Learning with Long-Context Models: An In-Depth Exploration"
        },
        {
            "paperId": "2717e5c7384ec12cfd6cf9c34897c6adad3230ed",
            "title": "Long-context LLMs Struggle with Long In-context Learning"
        },
        {
            "paperId": "63914b58bb944edbccba8304fd3e5bccb232a964",
            "title": "MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning"
        },
        {
            "paperId": "29eadc8a20594dcf62d505cf478149c4d4ec647e",
            "title": "Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning"
        },
        {
            "paperId": "1eddc4e36632342309885131a63ebd0bac1d837f",
            "title": "ICL Markup: Structuring In-Context Learning using Soft-Token Tags"
        },
        {
            "paperId": "600c842b141a4c92b4e1306ff1cdba04ab61e2ff",
            "title": "Auto-ICL: In-Context Learning without Human Supervision"
        },
        {
            "paperId": "267ef391ebaa4fc3c8ec62cb92107e8efe82a5db",
            "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering"
        },
        {
            "paperId": "419a7e9df13a3a03234aba0f05f48291261b8e59",
            "title": "Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models"
        },
        {
            "paperId": "03613effe356d2a8815f899027d6a5868822fd93",
            "title": "Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection"
        },
        {
            "paperId": "95240dda409e28acccdc5cf619ad0c036cf4292d",
            "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
        },
        {
            "paperId": "5128025d3fa915e08c1b33aa5228ee1f3e58884a",
            "title": "Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models"
        },
        {
            "paperId": "d395c771f6537259610497ba218cce5b9bfc2c50",
            "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries"
        },
        {
            "paperId": "b217b6bc340af9a10bebbf8acc36ea30871769bd",
            "title": "In-Context Learning with Iterative Demonstration Selection"
        },
        {
            "paperId": "380ba3def262f92e44c95ea9ff0eac755db34c59",
            "title": "Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning"
        },
        {
            "paperId": "f30ddf0c7455f89f016c540564e235b191c503db",
            "title": "Do pretrained Transformers Learn In-Context by Gradient Descent?"
        },
        {
            "paperId": "0a9030dd6cc2d438509f7568c1081d58c2523d5c",
            "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning"
        },
        {
            "paperId": "2ccac575a4899144a875a817b46e4423192a7ac5",
            "title": "Exploring Demonstration Ensembling for In-context Learning"
        },
        {
            "paperId": "3c8444cc4e96bdbe6853b886caf032afd1ee1d20",
            "title": "CausalLM is not optimal for in-context learning"
        },
        {
            "paperId": "cbec8bf16a459b0ae38856f604a6a14cd1343477",
            "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention"
        },
        {
            "paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5",
            "title": "Lost in the Middle: How Language Models Use Long Contexts"
        },
        {
            "paperId": "602cea8acd2a25dee3c2970d4f05174cd6f91982",
            "title": "In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick"
        },
        {
            "paperId": "4c60ce3e5116037390b3b92866f43df83f3e9c6f",
            "title": "Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression"
        },
        {
            "paperId": "f4d543ff431359947bf41152ac01233b8062221f",
            "title": "In-Context Learning through the Bayesian Prism"
        },
        {
            "paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2",
            "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
        },
        {
            "paperId": "11ae58636a5daf0ea1297f1c4ee94042fcebefa8",
            "title": "Birth of a Transformer: A Memory Viewpoint"
        },
        {
            "paperId": "4487bdcf1eb42bdec83709ba0df5b32dcf388976",
            "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization"
        },
        {
            "paperId": "8f936af93fb2b52b9678ff8f17c1ebe8de236a88",
            "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning"
        },
        {
            "paperId": "7bd4ca8706a79983d31ab74e6c79bfdfd949602e",
            "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning"
        },
        {
            "paperId": "43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0",
            "title": "Iterative Forward Tuning Boosts In-context Learning in Language Models"
        },
        {
            "paperId": "ff2a0fb125e7f03428420230c6ecbeafd4cf07a8",
            "title": "Can We Edit Factual Knowledge by In-Context Learning?"
        },
        {
            "paperId": "dd889342b0de45f7434cdfa7543e3bd46ec824cb",
            "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations"
        },
        {
            "paperId": "f84c0d902efefb18a941a0a01cbf8d70c019a4cf",
            "title": "Explaining Emergent In-Context Learning as Kernel Regression"
        },
        {
            "paperId": "0088c9f4d50706c7ab71efa13bcb4b42cf2058e2",
            "title": "PRODIGY: Enabling In-context Learning Over Graphs"
        },
        {
            "paperId": "8ce6ad6d8a73757309d3b9f525cf15cb68e32397",
            "title": "Efficient Prompting via Dynamic In-Context Learning"
        },
        {
            "paperId": "3fb0731538c59f8520a309996a0567b58965f0fe",
            "title": "Pre-Training to Learn in Context"
        },
        {
            "paperId": "7fa85f9c0fe44f1bf9e58a55f0f009296578c2f0",
            "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning"
        },
        {
            "paperId": "dd7f28d93dc2ec3cbc1cc66c3443c1c17105f1b3",
            "title": "Small Models are Valuable Plug-ins for Large Language Models"
        },
        {
            "paperId": "3db1219429c3f04e88347d41269bdc83c457fbf9",
            "title": "Symbol tuning improves in-context learning in language models"
        },
        {
            "paperId": "ca0c955699f552e1c2fbda747bd41faf8a2513ce",
            "title": "Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text"
        },
        {
            "paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd",
            "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
        },
        {
            "paperId": "1fb5a5298747b8c7d60f98640a543f20d42ab053",
            "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment"
        },
        {
            "paperId": "500cfab9345de174644d7534bb39695f17920601",
            "title": "How Do In-Context Examples Affect Compositional Generalization?"
        },
        {
            "paperId": "6a2d96d2a7adde6349f15c1e680b67d114e7b67c",
            "title": "Unified Demonstration Retriever for In-Context Learning"
        },
        {
            "paperId": "d6d3604f369bb0415cbe814e43ca3131323b03e2",
            "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning"
        },
        {
            "paperId": "f2cd02c03d0169374442d9bc227c9aed178f4b20",
            "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models"
        },
        {
            "paperId": "57be0448d168e8d6d0b6e0d1a4405fb5fbaa1b56",
            "title": "In-Context Learning Unlocked for Diffusion Models"
        },
        {
            "paperId": "a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3",
            "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression"
        },
        {
            "paperId": "cc0f0cb09a73f82ed44d900f5ca710bec784acc1",
            "title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction"
        },
        {
            "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b",
            "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
        },
        {
            "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
            "title": "Visual Instruction Tuning"
        },
        {
            "paperId": "dfd8944d39b378489b878d6e105d040fa0e524db",
            "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis"
        },
        {
            "paperId": "9cef5a098486aeab6ed3700c5e3d29488488d16f",
            "title": "Exploring Effective Factors for Improving Visual In-Context Learning"
        },
        {
            "paperId": "90af7c6cdf4b3359f6d275afb436f54f60082364",
            "title": "SegGPT: Segmenting Everything In Context"
        },
        {
            "paperId": "0ea7fc93d4947d9024ccaa202987a2070683bc1f",
            "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction"
        },
        {
            "paperId": "da3aca9d7b50da823f669c983edeb60445720fe0",
            "title": "The Learnability of In-Context Learning"
        },
        {
            "paperId": "197022486b2e2584302bd9b6442e44d15bf3e351",
            "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"
        },
        {
            "paperId": "154493f69d7db3d49da0e51df0192c6ad5f1724a",
            "title": "Larger language models do in-context learning differently"
        },
        {
            "paperId": "c40ec51ddd4145402bd95eeb3ce6977778d87881",
            "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling"
        },
        {
            "paperId": "a55dfc1482c9fa32859d1e8e8c5813f5a22982cc",
            "title": "OpenICL: An Open-Source Framework for In-context Learning"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "dc385646887a3669ae0ee506a263d592f4f7c7a6",
            "title": "Finding Support Examples for In-Context Learning"
        },
        {
            "paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",
            "title": "Language Is Not All You Need: Aligning Perception with Language Models"
        },
        {
            "paperId": "b6207fe49e29c77402f8dbab052e949990949609",
            "title": "In-context Example Selection with Influences"
        },
        {
            "paperId": "48abfc41a0abf023d2037ebb2f274835e0d322d0",
            "title": "Compositional Exemplars for In-context Learning"
        },
        {
            "paperId": "2e6fa3095df1d1ed041dfb4f5a18e31d4b7bd7bb",
            "title": "In-Context Learning with Many Demonstration Examples"
        },
        {
            "paperId": "1d75f8de31bf47ec46fa5586056420ec8bc97e86",
            "title": "Using In-Context Learning to Improve Dialogue Safety"
        },
        {
            "paperId": "465471bb5bf1a945549d6291c2d23367966b4957",
            "title": "In-Context Retrieval-Augmented Language Models"
        },
        {
            "paperId": "5f61ddc37476acf3741b0bfe5fcb59639cadbb86",
            "title": "What Makes Good Examples for Visual In-Context Learning?"
        },
        {
            "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
        },
        {
            "paperId": "c2f91f35df893714418cc29096083dce0b441229",
            "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"
        },
        {
            "paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
            "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"
        },
        {
            "paperId": "79f47aaef765d3452327588c37ad37826da760c7",
            "title": "Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners"
        },
        {
            "paperId": "0c0300f53c01ae609c97395c98de4c9d85d92876",
            "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"
        },
        {
            "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
            "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
        },
        {
            "paperId": "70b98d90767345b15e0569082c0e4ac661279b5d",
            "title": "Is GPT-3 a Good Data Annotator?"
        },
        {
            "paperId": "3d5922d71a370f32b7f232a596def914f67eebd1",
            "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering"
        },
        {
            "paperId": "6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",
            "title": "Reasoning with Language Model Prompting: A Survey"
        },
        {
            "paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e",
            "title": "Teaching Small Language Models to Reason"
        },
        {
            "paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
            "title": "Transformers learn in-context by gradient descent"
        },
        {
            "paperId": "34ff1da13770908ef0bf389365cdde743d3c9db1",
            "title": "Diverse Demonstrations Improve In-context Compositional Generalization"
        },
        {
            "paperId": "eecb45aa040064cbc0b37fd100706c02e7dc880e",
            "title": "Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"
        },
        {
            "paperId": "d03a9b2a0e090cc9fd2ba0a457ecea35372f1018",
            "title": "Demystifying Prompts in Language Models via Perplexity Estimation"
        },
        {
            "paperId": "9ceaeff7117965832f4c05fd6355d021862d0a82",
            "title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning"
        },
        {
            "paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
            "title": "What learning algorithm is in-context learning? Investigations with linear models"
        },
        {
            "paperId": "4d17732d90440682b0500f4e209c6cc4fac20e0e",
            "title": "Teaching Algorithmic Reasoning via In-context Learning"
        },
        {
            "paperId": "b8bd29a6104d26a16687400049a4e7e026ae6258",
            "title": "Active Example Selection for In-Context Learning"
        },
        {
            "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
            "title": "Large Language Models Are Human-Level Prompt Engineers"
        },
        {
            "paperId": "c8d594f09413b1555970f43e68847c211235d60f",
            "title": "Prompting GPT-3 To Be Reliable"
        },
        {
            "paperId": "663a41c866d49ce052801fbc88947d39764cad29",
            "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"
        },
        {
            "paperId": "e070ff286709db28312e08b52b05539debe88146",
            "title": "Measuring and Narrowing the Compositionality Gap in Language Models"
        },
        {
            "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2",
            "title": "Automatic Chain of Thought Prompting in Large Language Models"
        },
        {
            "paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57",
            "title": "Language Models are Multilingual Chain-of-Thought Reasoners"
        },
        {
            "paperId": "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
            "title": "Complexity-Based Prompting for Multi-Step Reasoning"
        },
        {
            "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
            "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought"
        },
        {
            "paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f",
            "title": "In-context Learning and Induction Heads"
        },
        {
            "paperId": "46d64d0c1dd240f5035b1af57e738b3f70850ca2",
            "title": "On the Relation between Sensitivity and Accuracy in In-context Learning"
        },
        {
            "paperId": "86d0d3855f94105e25d81cab9f3d269c6062a9c4",
            "title": "Selective Annotation Makes Language Models Better Few-Shot Learners"
        },
        {
            "paperId": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "title": "Visual Prompting via Image Inpainting"
        },
        {
            "paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9",
            "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"
        },
        {
            "paperId": "dcbf62f17dad0f4554f91c822d141fb92f78429a",
            "title": "Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator"
        },
        {
            "paperId": "a8fd9c1625011741f74401ff9bdc1c584e25c86d",
            "title": "Language Models are General-Purpose Interfaces"
        },
        {
            "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
            "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
        },
        {
            "paperId": "316206a2f89eb94ce02a81fba1dc304586f21b39",
            "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations"
        },
        {
            "paperId": "4e5f7cd537a1bbcd090f9887b1b59f39a3715dba",
            "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions"
        },
        {
            "paperId": "5437e8adab596d7294124c0e798708e050e25321",
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
        },
        {
            "paperId": "81986b8a3d3fe6c5be06fc4527953fb514ad12e8",
            "title": "Improving In-Context Few-Shot Learning via Self-Supervised Training"
        },
        {
            "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
            "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
        },
        {
            "paperId": "1fafaccebc4a74898a74c606f846318c4c2c7536",
            "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"
        },
        {
            "paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd",
            "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"
        },
        {
            "paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
            "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "3f4d11971f2c64be9125a7fe99c019588bbebf16",
            "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought"
        },
        {
            "paperId": "f4df78183261538e718066331898ee5cad7cad05",
            "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
        },
        {
            "paperId": "663662fbd9be73f99c764a7b85982bf825acfe8a",
            "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention"
        },
        {
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
            "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
            "title": "LaMDA: Language Models for Dialog Applications"
        },
        {
            "paperId": "002c58077a1f1b296468b117230a1199e91f35c2",
            "title": "Black-Box Tuning for Language-Model-as-a-Service"
        },
        {
            "paperId": "f9838a3be5c94bb2674a0e224de349b50e18f3c4",
            "title": "Learning To Retrieve Prompts for In-Context Learning"
        },
        {
            "paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
            "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"
        },
        {
            "paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7",
            "title": "MetaICL: Learning to Learn In Context"
        },
        {
            "paperId": "6bd91a3183ddb844641acb9f3fe9faec6a9ff617",
            "title": "Meta-learning via Language Model In-context Tuning"
        },
        {
            "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
            "title": "Finetuned Language Models Are Zero-Shot Learners"
        },
        {
            "paperId": "4e263b4cd6998bff2501dd143e685f413179b12d",
            "title": "Want To Reduce Labeling Cost? GPT-3 Can Help"
        },
        {
            "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
            "title": "On the Opportunities and Risks of Foundation Models"
        },
        {
            "paperId": "eea7bca03bda3ee2448cd012bbcb2b33822861d8",
            "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification"
        },
        {
            "paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
            "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
        },
        {
            "paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d",
            "title": "Multimodal Few-Shot Learning with Frozen Language Models"
        },
        {
            "paperId": "0adec918885dff698acf359988ed79a543157f80",
            "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"
        },
        {
            "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
            "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
        },
        {
            "paperId": "240b0caabb415578bdea4da7d0a32bdff2e8163f",
            "title": "Editing Factual Knowledge in Language Models"
        },
        {
            "paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69",
            "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"
        },
        {
            "paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74",
            "title": "What Makes Good In-Context Examples for GPT-3?"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86",
            "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"
        },
        {
            "paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09",
            "title": "Knowledge Enhanced Contextual Word Representations"
        },
        {
            "paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde",
            "title": "ERNIE: Enhanced Language Representation with Informative Entities"
        },
        {
            "paperId": "d9f6ada77448664b71128bb19df15765336974a6",
            "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
        },
        {
            "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
            "title": "The Curious Case of Neural Text Degeneration"
        },
        {
            "paperId": "3b5505eaec8583a2dd98d72a84d95b9eff475a81",
            "title": "Few-shot Learning: A Survey"
        },
        {
            "paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
            "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
        },
        {
            "paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e",
            "title": "Character-level Convolutional Networks for Text Classification"
        },
        {
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference"
        },
        {
            "paperId": "b7fc1c390c9c6d22ac4bf7dce589af3174ac13fc",
            "title": "Beyond the imitation game"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "7066d011f1eaa7b0ab3e97f683d2bda9d978131f",
            "title": "Learning and reasoning by analogy"
        },
        {
            "paperId": "bff20fb30adad8d1c173963089df5fc9664304f0",
            "title": "A Markovian Decision Process"
        },
        {
            "paperId": "9070c05c70c3b16b2405b04255d12d2b0584014b",
            "title": "Empirical Methods"
        },
        {
            "paperId": "a131c44951b7ace0892dd830dd0a040b99ed0803",
            "title": "Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning"
        },
        {
            "paperId": "3b01b5b497e1f359b11da45af029281ee6f64c2c",
            "title": "Towards Enhancing In-Context Learning for Code Generation"
        },
        {
            "paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725",
            "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"
        },
        {
            "paperId": "227dcfb8f289b9629997da8572cfa84a3a016e2e",
            "title": "Differentially Private In-Context Learning"
        },
        {
            "paperId": "ca0dcc09e69732b7fc582d8a610488f86ad07e36",
            "title": "In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"
        },
        {
            "paperId": null,
            "title": "Incontext learning as maintaining coherency: A study of on-the-fly machine translation using large language models"
        },
        {
            "paperId": null,
            "title": "Mot: Prethinking and recalling enable chatgpt to selfimprove with memory-of-thoughts"
        },
        {
            "paperId": null,
            "title": ". An information-theoretic approach to prompt engineering without ground truth labels"
        },
        {
            "paperId": null,
            "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
            "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
        },
        {
            "paperId": "8eb180e37164d43029f627b1388279bdac47acce",
            "title": "Human Language Technologies"
        },
        {
            "paperId": null,
            "title": "2022b. Automatic chain of thought"
        },
        {
            "paperId": null,
            "title": "2022. Large language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change)"
        },
        {
            "paperId": null,
            "title": "2024. Self-demos: Eliciting out-of-demonstration generalizability in large language models"
        },
        {
            "paperId": null,
            "title": "Methods in Natural"
        },
        {
            "paperId": null,
            "title": "2023b. Transformers as statisticians: Provable in-context learning with in-context algo-rithm selection"
        },
        {
            "paperId": null,
            "title": "2024. Grimoire is all you need for enhancing large language models"
        },
        {
            "paperId": null,
            "title": "2023b. Understanding in-context learning via supportive pre-training data"
        },
        {
            "paperId": null,
            "title": "only need a small amount of data to adapt to learn from the context during warmup"
        },
        {
            "paperId": null,
            "title": "The performance of ICL strongly relies on the demonstration surface, including the selection, for-matting, and ordering of demonstration examples."
        },
        {
            "paperId": null,
            "title": "AI@Meta. 2024. Llama 3 model card. Technical report, Meta"
        },
        {
            "paperId": null,
            "title": "the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
        },
        {
            "paperId": null,
            "title": "distribution of LLMs plays an important role in instance selecting. (3) For k demonstrations, the size of search space of permutations is k!"
        },
        {
            "paperId": null,
            "title": "2023. Statistical knowledge assessment for large language models"
        },
        {
            "paperId": null,
            "title": "1631\u20131642, Seattle, Washington, USA"
        },
        {
            "paperId": null,
            "title": "Qiaoqiao She, and Yongdong Zhang. 2023a. k nn prompting: Learning beyond the context with nearest neighbor inference"
        },
        {
            "paperId": null,
            "title": "2023. GPT-4 technical report"
        },
        {
            "paperId": null,
            "title": "2022. Scaling instruction-finetuned language models"
        }
    ]
}