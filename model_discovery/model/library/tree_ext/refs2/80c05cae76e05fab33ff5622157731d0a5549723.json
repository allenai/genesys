{
    "paperId": "80c05cae76e05fab33ff5622157731d0a5549723",
    "externalIds": {
        "DBLP": "journals/corr/LeZ16",
        "ArXiv": "1603.00423",
        "ACL": "W16-1610",
        "MAG": "2950099263",
        "DOI": "10.18653/v1/W16-1610",
        "CorpusId": 6672908
    },
    "title": "Quantifying the Vanishing Gradient and Long Distance Dependency Problem in Recursive Neural Networks and Recursive LSTMs",
    "abstract": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.",
    "venue": "Rep4NLP@ACL",
    "year": 2016,
    "referenceCount": 14,
    "citationCount": 46,
    "influentialCitationCount": 2,
    "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W16-1610.pdf",
        "status": "HYBRID"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is demonstrated that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems, and it is suggested that the benefits of including hierarchical structure and of including LSTM-style gating are complementary."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "144106794",
            "name": "Phong Le"
        },
        {
            "authorId": "1787819",
            "name": "Willem H. Zuidema"
        }
    ],
    "references": [
        {
            "paperId": "1f600f213dbbd70f06093438855f39022957b4bf",
            "title": "Long Short-Term Memory Over Recursive Structures"
        },
        {
            "paperId": "1492ddfd4f4b152b83f11db8c9ecdfd0d2543294",
            "title": "Compositional Distributional Semantics with Long Short Term Memory"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "5d43224147a5bb8b17b6a6fc77bf86490e86991a",
            "title": "A Recursive Recurrent Neural Network for Statistical Machine Translation"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "acc4e56c44771ebf69302a06af51498aeb0a6ac8",
            "title": "Parsing with Compositional Vector Grammars"
        },
        {
            "paperId": "53ab89807caead278d3deb7b6a4180b277d3cb77",
            "title": "Better Word Representations with Recursive Neural Networks for Morphology"
        },
        {
            "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
        },
        {
            "paperId": "1be8778de4c6eb623871fe08d0998016bd60936f",
            "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages"
        },
        {
            "paperId": "ba6e3b28090d935205ed0e1d398206906b5b8905",
            "title": "Linguistics"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "title": "Learning long-term dependencies with gradient descent is difficult"
        },
        {
            "paperId": "81b3b3fe994a9eda6d3f9d2149aa4492d1933975",
            "title": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks"
        },
        {
            "paperId": null,
            "title": "Deep Learning and Unsupervised Feature Learning Workshop"
        }
    ]
}