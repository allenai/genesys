{
    "paperId": "5e04881e91bff952d102d967c4ffb498ec30d4af",
    "externalIds": {
        "DBLP": "journals/corr/abs-1811-03115",
        "MAG": "2890152612",
        "ArXiv": "1811.03115",
        "CorpusId": 53208380
    },
    "title": "Blockwise Parallel Decoding for Deep Autoregressive Models",
    "abstract": "Deep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While common architecture classes such as recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, generation still remains an inherently sequential process. To overcome this limitation, we propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model. This allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel. We verify our approach empirically through a series of experiments using state-of-the-art self-attention models for machine translation and image super-resolution, achieving iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. In terms of wall-clock time, our fastest models exhibit real-time speedups of up to 4x over standard greedy decoding.",
    "venue": "Neural Information Processing Systems",
    "year": 2018,
    "referenceCount": 15,
    "citationCount": 146,
    "influentialCitationCount": 14,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a novel blockwise parallel decoding scheme in which it makes predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model, which allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "144872294",
            "name": "Mitchell Stern"
        },
        {
            "authorId": "1846258",
            "name": "Noam M. Shazeer"
        },
        {
            "authorId": "39328010",
            "name": "Jakob Uszkoreit"
        }
    ],
    "references": [
        {
            "paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1",
            "title": "Born Again Neural Networks"
        },
        {
            "paperId": "642c1b4a9da95ea4239708afc5929a5007a1870d",
            "title": "Tensor2Tensor for Neural Machine Translation"
        },
        {
            "paperId": "2d08ed53491053d84b6de89aedbf2178b9c8cf84",
            "title": "Fast Decoding in Sequence Models using Discrete Latent Variables"
        },
        {
            "paperId": "f2c882fd290d616ff96c1c5d6af4578682e26556",
            "title": "Efficient Neural Audio Synthesis"
        },
        {
            "paperId": "9c5c89199114858eafbe50b46d77d38ffd03b28a",
            "title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement"
        },
        {
            "paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329",
            "title": "Image Transformer"
        },
        {
            "paperId": "f6cbf83e1ce3b099d656d2346b261d5ef7f2b62e",
            "title": "Parallel WaveNet: Fast High-Fidelity Speech Synthesis"
        },
        {
            "paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454",
            "title": "Non-Autoregressive Neural Machine Translation"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning"
        },
        {
            "paperId": "df0402517a7338ae28bc54acaac400de6b456a46",
            "title": "WaveNet: A Generative Model for Raw Audio"
        },
        {
            "paperId": "57a10537978600fd33dcdd48922c791609a4851a",
            "title": "Sequence-Level Knowledge Distillation"
        },
        {
            "paperId": "1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52",
            "title": "A Neural Attention Model for Abstractive Sentence Summarization"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
            "title": "Deep Learning Face Attributes in the Wild"
        }
    ]
}