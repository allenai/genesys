{
    "paperId": "97ad7960bdd20442093bc8e31391e235b943eecf",
    "externalIds": {
        "DBLP": "conf/iclr/TianW0CD24",
        "ArXiv": "2310.00535",
        "DOI": "10.48550/arXiv.2310.00535",
        "CorpusId": 263334266
    },
    "title": "JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention",
    "abstract": "We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 70,
    "citationCount": 22,
    "influentialCitationCount": 1,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2310.00535",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.48276686668396,
            1.1976782083511353,
            2.8336310386657715,
            3.708413600921631,
            2.789599895477295,
            -1.6985923051834106,
            2.9964680671691895,
            -4.840566635131836,
            3.3198540210723877,
            -1.5488672256469727,
            0.9815018177032471,
            5.030303955078125,
            0.5474197864532471,
            2.9247565269470215,
            -2.875410556793213,
            0.8152238726615906,
            -1.54973304271698,
            2.3818159103393555,
            0.3676055669784546,
            1.9589208364486694,
            -2.0979466438293457,
            1.4959285259246826,
            -2.1589250564575195,
            -3.649935007095337,
            -2.2005627155303955,
            0.1646159291267395,
            4.206325531005859,
            0.9638987183570862,
            -3.3901801109313965,
            3.0790562629699707,
            -3.173384189605713,
            -7.051858425140381,
            5.144521713256836,
            -4.609451770782471,
            1.6797304153442383,
            -1.238114833831787,
            -2.224052906036377,
            6.064879417419434,
            -4.180634498596191,
            1.8512741327285767,
            -1.5002362728118896,
            -1.02298903465271,
            2.5190625190734863,
            -0.9410123825073242,
            -0.1571376919746399,
            -0.5519145727157593,
            0.3694031834602356,
            2.1518023014068604,
            -1.14808988571167,
            3.6892504692077637,
            2.569981575012207,
            -0.1014135479927063,
            -2.0614492893218994,
            4.1198954582214355,
            2.898313522338867,
            -2.6426448822021484,
            2.8202104568481445,
            2.048520565032959,
            1.1829520463943481,
            0.29623734951019287,
            4.48969841003418,
            2.020265579223633,
            0.05726146697998047,
            0.7520207166671753,
            4.204469680786133,
            -4.027441501617432,
            -3.5263772010803223,
            3.8881564140319824,
            2.2298779487609863,
            6.212991237640381,
            -0.6228897571563721,
            -8.389242172241211,
            -0.9627314805984497,
            1.070360541343689,
            -3.6519010066986084,
            2.865816593170166,
            -5.273093223571777,
            -9.070510864257812,
            -1.7285633087158203,
            -0.23903316259384155,
            -0.6083533763885498,
            0.39940959215164185,
            3.059777021408081,
            1.5934133529663086,
            1.0344171524047852,
            -2.411761522293091,
            -3.1758110523223877,
            1.3376826047897339,
            1.793480396270752,
            -5.067564487457275,
            1.466984510421753,
            1.5246927738189697,
            -1.3905062675476074,
            2.161508083343506,
            -4.430138111114502,
            -0.6344119310379028,
            -0.9277383089065552,
            0.14459002017974854,
            -5.3765130043029785,
            -1.9252617359161377,
            3.508608818054199,
            -3.1153037548065186,
            0.22768503427505493,
            1.2868365049362183,
            4.725533485412598,
            -1.0163573026657104,
            -2.5802388191223145,
            0.07320697605609894,
            1.8951213359832764,
            -2.1646361351013184,
            -5.404949188232422,
            2.7212510108947754,
            -3.0152840614318848,
            -0.18306785821914673,
            0.21235451102256775,
            -1.7225404977798462,
            -3.0002596378326416,
            0.3945484757423401,
            2.338225841522217,
            5.719743251800537,
            -0.34137263894081116,
            2.2408084869384766,
            -4.498195648193359,
            0.8511014580726624,
            0.28054025769233704,
            -0.05029331147670746,
            1.2488747835159302,
            1.3771626949310303,
            1.327387809753418,
            -4.3713274002075195,
            2.8150243759155273,
            -2.4356637001037598,
            4.167627334594727,
            -2.5636820793151855,
            3.2961864471435547,
            2.4976389408111572,
            -3.822195053100586,
            1.6480478048324585,
            -3.158623218536377,
            -2.24456787109375,
            -1.40165114402771,
            1.4048575162887573,
            0.0943945050239563,
            -3.812225341796875,
            2.6909091472625732,
            2.2693350315093994,
            -0.6952800750732422,
            4.386570453643799,
            2.381761074066162,
            6.114640235900879,
            3.0261688232421875,
            -3.774115562438965,
            0.24522006511688232,
            2.2678539752960205,
            -1.5506672859191895,
            2.5370349884033203,
            -8.017687797546387,
            2.7745628356933594,
            -5.208495140075684,
            2.4585723876953125,
            0.6773555874824524,
            -0.6886247396469116,
            -9.422436714172363,
            -2.101375102996826,
            1.5612461566925049,
            -4.293049335479736,
            -0.9084423780441284,
            0.29040539264678955,
            -0.3454200327396393,
            3.0250983238220215,
            3.1016340255737305,
            3.819432497024536,
            1.2517374753952026,
            1.1949797868728638,
            5.348484039306641,
            3.785656213760376,
            3.14201021194458,
            -1.1347122192382812,
            -0.1536673903465271,
            -0.02359098196029663,
            -2.1797847747802734,
            -4.583524703979492,
            -4.923564910888672,
            3.297267436981201,
            -1.1731878519058228,
            -1.3941128253936768,
            -1.3034063577651978,
            -1.8249297142028809,
            -5.881817817687988,
            -0.6815245747566223,
            0.8439538478851318,
            -0.23043441772460938,
            4.8833160400390625,
            4.738827705383301,
            3.454782247543335,
            1.7290098667144775,
            2.244320869445801,
            0.42154109477996826,
            -0.38180679082870483,
            0.4902639091014862,
            5.813022136688232,
            0.5301655530929565,
            -3.225355863571167,
            -2.8554437160491943,
            2.0810675621032715,
            0.31771308183670044,
            -3.5813355445861816,
            4.435429573059082,
            -0.34298938512802124,
            -2.1309900283813477,
            -0.828750491142273,
            0.9568555951118469,
            0.089387446641922,
            1.9404749870300293,
            -2.780864953994751,
            -1.1242409944534302,
            -5.161258697509766,
            2.410841941833496,
            5.528438091278076,
            3.745539665222168,
            -1.8241941928863525,
            1.4914770126342773,
            -0.27625274658203125,
            -2.824915885925293,
            -0.9929744005203247,
            -1.8560307025909424,
            3.9348816871643066,
            -0.9281985759735107,
            2.369019031524658,
            2.3927371501922607,
            0.5951507091522217,
            -1.6615099906921387,
            0.6853611469268799,
            -2.266864538192749,
            -6.14324426651001,
            -0.6302204132080078,
            -5.077862739562988,
            1.4302598237991333,
            -3.538715362548828,
            -2.7689690589904785,
            5.164594650268555,
            0.8408758640289307,
            0.2532517910003662,
            5.022235870361328,
            4.368556499481201,
            2.1892099380493164,
            -2.7621009349823,
            3.599945545196533,
            -4.934277534484863,
            -0.7966764569282532,
            -1.3800303936004639,
            0.38584378361701965,
            4.0649824142456055,
            -1.8149572610855103,
            -0.7001752853393555,
            1.8860490322113037,
            0.4449314773082733,
            0.25073230266571045,
            0.5983922481536865,
            2.2041354179382324,
            2.4504590034484863,
            2.523911952972412,
            3.6923539638519287,
            1.9868322610855103,
            -1.5921061038970947,
            3.854583740234375,
            -1.509799599647522,
            -1.1118048429489136,
            -1.7223365306854248,
            4.9389543533325195,
            -0.3702968657016754,
            1.948161005973816,
            -2.2129745483398438,
            -5.095929145812988,
            0.9324281215667725,
            -2.324065685272217,
            -4.7763166427612305,
            -1.3269327878952026,
            1.3861147165298462,
            5.125326156616211,
            -0.904944658279419,
            -4.023688793182373,
            -1.3180999755859375,
            -1.737365484237671,
            -0.797527551651001,
            -3.584567070007324,
            -2.0245087146759033,
            1.4143755435943604,
            0.0976724624633789,
            -1.2289413213729858,
            -4.5921173095703125,
            3.139387607574463,
            -6.438440322875977,
            0.7471356391906738,
            -0.7704410552978516,
            4.8183698654174805,
            2.2744979858398438,
            -1.9789695739746094,
            1.5000247955322266,
            -2.280923366546631,
            -3.094031572341919,
            -0.7855663299560547,
            2.2836837768554688,
            -0.8640602827072144,
            0.5809526443481445,
            8.304006576538086,
            1.5757439136505127,
            -2.1955020427703857,
            -4.915067195892334,
            -5.138838291168213,
            -1.6974883079528809,
            1.1807277202606201,
            3.698274612426758,
            -1.1672769784927368,
            0.1442255675792694,
            1.3975244760513306,
            2.6594650745391846,
            -3.5752687454223633,
            -1.5349671840667725,
            1.071895956993103,
            -3.909315347671509,
            -0.17291782796382904,
            -6.485724925994873,
            -4.85500431060791,
            -3.233787775039673,
            -0.7287936210632324,
            5.221578598022461,
            3.310638427734375,
            -4.783158302307129,
            3.0831918716430664,
            -1.6674683094024658,
            3.3351166248321533,
            2.2165260314941406,
            3.58126163482666,
            -0.03711484372615814,
            -1.4795809984207153,
            -1.3497669696807861,
            -0.010253071784973145,
            -1.2234208583831787,
            6.66053581237793,
            1.7502477169036865,
            6.275305271148682,
            -0.6285122036933899,
            2.147397994995117,
            1.8813416957855225,
            -0.3105706572532654,
            -2.268038511276245,
            -3.417416572570801,
            -2.2029080390930176,
            -3.55558705329895,
            -0.7339884638786316,
            1.8069961071014404,
            2.2176928520202637,
            -4.133161544799805,
            2.3888356685638428,
            1.7627683877944946,
            2.394439935684204,
            1.8095862865447998,
            1.6618282794952393,
            4.025033950805664,
            2.7312159538269043,
            -3.9999516010284424,
            -0.03826946020126343,
            -1.5925354957580566,
            -0.42069441080093384,
            -0.5386663675308228,
            10.11186408996582,
            2.5540354251861572,
            2.0160579681396484,
            -5.840117931365967,
            -1.9392993450164795,
            0.004602760076522827,
            -2.401632785797119,
            5.131682395935059,
            -4.697272777557373,
            -3.7974960803985596,
            3.414462089538574,
            -2.3473222255706787,
            3.069425582885742,
            0.3048577308654785,
            -0.9731172323226929,
            4.436163902282715,
            2.2771568298339844,
            5.402205467224121,
            -1.0585180521011353,
            0.21454334259033203,
            -2.9836714267730713,
            3.178727626800537,
            2.8613786697387695,
            -0.6779294610023499,
            -4.006190776824951,
            2.0212905406951904,
            1.5724490880966187,
            2.1302971839904785,
            -3.040424108505249,
            -3.768871545791626,
            -4.113443851470947,
            -4.992614269256592,
            0.8002998232841492,
            -0.06865811347961426,
            -0.04249390959739685,
            2.8972761631011963,
            3.0675482749938965,
            6.4064741134643555,
            -3.1967830657958984,
            -1.3365049362182617,
            4.946942329406738,
            -3.2478113174438477,
            -2.716742753982544,
            0.838046669960022,
            -0.8175376653671265,
            -2.6953282356262207,
            -0.7110694646835327,
            -4.8036088943481445,
            -0.56763756275177,
            -2.563544273376465,
            0.7214397192001343,
            4.702251434326172,
            3.075371742248535,
            -2.4412147998809814,
            0.39946064352989197,
            3.3362808227539062,
            1.607156753540039,
            2.5581114292144775,
            -0.25537267327308655,
            5.698247909545898,
            3.063833475112915,
            4.7225751876831055,
            -1.0253397226333618,
            1.8917019367218018,
            -0.5342634916305542,
            4.607052326202393,
            -5.4865007400512695,
            0.4621129035949707,
            -0.3941000699996948,
            3.6456050872802734,
            4.998130798339844,
            1.848157525062561,
            -0.25541752576828003,
            -0.9476063251495361,
            1.379814863204956,
            3.858045816421509,
            -0.19681859016418457,
            6.503921031951904,
            -0.6663130521774292,
            -0.22711943089962006,
            0.7704927921295166,
            0.9375939965248108,
            -1.0430221557617188,
            -3.7852988243103027,
            2.1552114486694336,
            -3.396273136138916,
            -4.344514846801758,
            0.7008223533630371,
            -1.8624440431594849,
            2.079846143722534,
            -4.392792701721191,
            -2.3568994998931885,
            1.7166469097137451,
            -2.0830819606781006,
            -4.886899948120117,
            2.886124610900879,
            -1.2817285060882568,
            3.9653656482696533,
            2.3046326637268066,
            6.297868728637695,
            -3.8769278526306152,
            -0.36899226903915405,
            -0.4169897437095642,
            0.8023525476455688,
            4.441164016723633,
            -0.43653497099876404,
            0.4874952435493469,
            -0.4052787125110626,
            2.340576171875,
            0.36658841371536255,
            3.2174668312072754,
            -0.5412349700927734,
            -1.1278573274612427,
            -1.6106826066970825,
            -5.279850959777832,
            -1.681757926940918,
            1.552593469619751,
            -2.5207438468933105,
            -2.300215721130371,
            2.4842288494110107,
            4.789335250854492,
            0.49428194761276245,
            4.033144474029541,
            3.661459445953369,
            0.3557337820529938,
            3.5449230670928955,
            3.1640100479125977,
            2.002594232559204,
            2.003769636154175,
            -0.4067588448524475,
            -3.337277889251709,
            1.4146755933761597,
            2.9039292335510254,
            -0.617786169052124,
            3.2603650093078613,
            -0.4463801383972168,
            2.5793228149414062,
            -0.04609644412994385,
            -4.680270195007324,
            2.8981173038482666,
            4.239645004272461,
            2.9168930053710938,
            -1.736854910850525,
            -1.4516162872314453,
            -6.179865837097168,
            1.8014323711395264,
            -7.326610565185547,
            4.045570373535156,
            -0.2509021759033203,
            0.18914401531219482,
            2.105587959289551,
            0.06339195370674133,
            1.7780522108078003,
            3.656531572341919,
            -1.1833248138427734,
            -3.073342800140381,
            0.42072373628616333,
            2.1876649856567383,
            -0.6697474718093872,
            2.739074230194092,
            -6.342264175415039,
            -4.2069854736328125,
            0.4625389575958252,
            4.399449348449707,
            5.518770217895508,
            5.861728191375732,
            3.127488136291504,
            -2.1423287391662598,
            -1.9123790264129639,
            -0.8811310529708862,
            2.8056180477142334,
            4.402063846588135,
            -5.586104869842529,
            -1.2714896202087402,
            3.5416207313537598,
            2.084347724914551,
            -0.6222563982009888,
            0.2998446822166443,
            -0.6013178825378418,
            3.0878610610961914,
            -4.164369106292725,
            -0.6491607427597046,
            -2.441110372543335,
            -2.559854030609131,
            -1.0986332893371582,
            -0.8720829486846924,
            1.6239168643951416,
            -4.450547218322754,
            -3.8635194301605225,
            -1.1590063571929932,
            -0.26462268829345703,
            -0.923635721206665,
            -3.2025327682495117,
            3.929697036743164,
            1.2884423732757568,
            0.7910737991333008,
            1.9744038581848145,
            2.749601125717163,
            -1.9796441793441772,
            -3.5433497428894043,
            -2.161543607711792,
            3.635014295578003,
            -0.7831083536148071,
            -1.9728734493255615,
            0.7136127352714539,
            -3.8465476036071777,
            -2.8569555282592773,
            1.3497133255004883,
            0.9941529035568237,
            2.674372673034668,
            0.7239680290222168,
            3.891489028930664,
            -1.425321340560913,
            1.9244433641433716,
            0.28119802474975586,
            -3.3080689907073975,
            -1.540132999420166,
            -4.532478332519531,
            2.266822099685669,
            -5.65499210357666,
            -4.582159519195557,
            1.964350938796997,
            -4.191074371337891,
            0.04537500441074371,
            -0.15584489703178406,
            -1.4733541011810303,
            2.7284257411956787,
            -4.807328224182129,
            -2.0621681213378906,
            -4.34244441986084,
            2.768230676651001,
            2.6284866333007812,
            -1.295194149017334,
            0.962267279624939,
            -0.15041713416576385,
            3.7655410766601562,
            7.026946067810059,
            3.145634412765503,
            -1.2810345888137817,
            -0.27270621061325073,
            1.7519762516021729,
            0.2806565463542938,
            -3.3452537059783936,
            0.6412901878356934,
            -0.7221283912658691,
            -0.7566540241241455,
            17.865135192871094,
            0.6345715522766113,
            2.0108935832977295,
            -1.444709062576294,
            -3.140956401824951,
            -3.6737024784088135,
            0.09615364670753479,
            2.4274213314056396,
            1.6940678358078003,
            1.1588913202285767,
            -2.0598723888397217,
            -2.356719970703125,
            0.7913479804992676,
            1.1257283687591553,
            -1.153018593788147,
            -2.0791378021240234,
            -1.832066297531128,
            -0.9604678153991699,
            -2.3616793155670166,
            -1.0663787126541138,
            -2.0692877769470215,
            5.5390825271606445,
            -2.6301140785217285,
            2.147202730178833,
            2.3385744094848633,
            1.9046236276626587,
            -2.0987119674682617,
            2.755218029022217,
            -8.927865982055664,
            -1.8990488052368164,
            1.4224742650985718,
            2.104343891143799,
            -0.5405793786048889,
            1.0324795246124268,
            -1.9323440790176392,
            5.108025550842285,
            6.07047700881958,
            -0.85204017162323,
            7.085383415222168,
            -0.6311659216880798,
            -0.4264887571334839,
            1.7567312717437744,
            -3.4478700160980225,
            2.9152307510375977,
            0.6455288529396057,
            1.5954253673553467,
            0.9495127201080322,
            -2.0540294647216797,
            -3.507709503173828,
            4.864375114440918,
            4.3577070236206055,
            0.969208300113678,
            -1.4596328735351562,
            0.1459357738494873,
            3.4901318550109863,
            1.808559775352478,
            2.211993455886841,
            -1.7123432159423828,
            3.8080291748046875,
            1.3156378269195557,
            -0.31406816840171814,
            -1.1894205808639526,
            -3.0665249824523926,
            -1.832585096359253,
            -1.0732108354568481,
            -3.904752731323242,
            -4.169903755187988,
            3.6492486000061035,
            1.4412755966186523,
            2.104792833328247,
            3.4096171855926514,
            2.4319708347320557,
            0.6497882604598999,
            0.6930134296417236,
            -3.574037790298462,
            -7.287583351135254,
            0.868586003780365,
            0.14978930354118347,
            -0.36878281831741333,
            8.254297256469727,
            -6.1828765869140625,
            2.159139394760132,
            -1.1898014545440674,
            1.1057028770446777,
            3.737156391143799,
            -1.5978939533233643,
            6.023404121398926,
            -0.4558015465736389,
            -4.8233537673950195,
            3.763411045074463,
            2.709078788757324,
            -3.256063461303711,
            0.19873401522636414,
            1.3791711330413818,
            5.706540107727051,
            -5.6441240310668945,
            -3.93957781791687,
            -2.525801181793213,
            -4.177171230316162,
            -4.636924743652344,
            4.569751262664795,
            1.9412486553192139,
            2.1346843242645264,
            -4.4061970710754395,
            -2.2933285236358643,
            1.2621583938598633,
            -3.453986167907715,
            -9.125055313110352,
            -5.958078861236572,
            -4.483694553375244,
            6.422252655029297,
            -2.546967029571533,
            -0.6236507892608643,
            -1.9212709665298462,
            -0.5398564338684082,
            -0.3334488868713379,
            -1.4353463649749756,
            6.483986854553223,
            -0.00847192108631134,
            4.3629984855651855,
            3.1278743743896484,
            -1.3310496807098389,
            -3.779850482940674,
            -2.4005725383758545,
            -5.904687404632568,
            1.6271709203720093,
            1.4756646156311035,
            -1.8493163585662842,
            -2.548022985458374,
            -0.10474586486816406,
            3.093419075012207,
            -5.330775260925293,
            -1.5031733512878418,
            -2.5984177589416504,
            -0.5362193584442139,
            -2.8791511058807373,
            0.04220752418041229,
            3.540675163269043,
            -5.226740837097168,
            6.001460075378418,
            4.156840801239014,
            -3.2549352645874023,
            -3.3547723293304443,
            8.268468856811523,
            -0.59870845079422,
            2.3989980220794678,
            -1.6227468252182007,
            -0.7588128447532654,
            -2.239732265472412,
            -2.951277256011963,
            0.30840110778808594,
            5.629437446594238,
            5.487161636352539,
            -1.2259432077407837,
            -1.3562005758285522,
            -1.1329830884933472
        ]
    },
    "authors": [
        {
            "authorId": "2249538771",
            "name": "Yuandong Tian"
        },
        {
            "authorId": "2249900684",
            "name": "Yiping Wang"
        },
        {
            "authorId": "2109338656",
            "name": "Zhenyu (Allen) Zhang"
        },
        {
            "authorId": "2249538643",
            "name": "Beidi Chen"
        },
        {
            "authorId": "2249763843",
            "name": "Simon S. Du"
        }
    ],
    "references": [
        {
            "paperId": "f9a4ed62ea6da274c1c81748b2bca240655b7c29",
            "title": "Transformers as Support Vector Machines"
        },
        {
            "paperId": "a87f40a49da377c0d00bebe711e417fc3b1d8969",
            "title": "Max-Margin Token Selection in Attention Mechanism"
        },
        {
            "paperId": "4a7530bbaee7563ee244f3ffed6b706bd96f08a8",
            "title": "Trained Transformers Learn Linear Models In-Context"
        },
        {
            "paperId": "194a048814acc5cd5a9ee08102df3dcb61b2dfc9",
            "title": "Transformers learn through gradual rank increase"
        },
        {
            "paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3",
            "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"
        },
        {
            "paperId": "3f16d91bdfca925df761d27fd3b11af7d68c63d8",
            "title": "On the Role of Attention in Prompt-tuning"
        },
        {
            "paperId": "11ae58636a5daf0ea1297f1c4ee94042fcebefa8",
            "title": "Birth of a Transformer: A Memory Viewpoint"
        },
        {
            "paperId": "50eb97f832ffcd2114f79957c977215176384e3d",
            "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer"
        },
        {
            "paperId": "a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3",
            "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression"
        },
        {
            "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
            "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
        },
        {
            "paperId": "15288293edeae26dad6e37218cc1c0fc96316635",
            "title": "Do Transformers Parse while Predicting the Masked Word?"
        },
        {
            "paperId": "f3fde8a09b757ab356da1314d7a938504edf8314",
            "title": "How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding"
        },
        {
            "paperId": "971091fd7e80ffb9e96c04fa00c4f6d1fb0c8213",
            "title": "Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron"
        },
        {
            "paperId": "91166a75f0e32b782a57028f1501aba6335ac550",
            "title": "A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity"
        },
        {
            "paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
            "title": "Transformers learn in-context by gradient descent"
        },
        {
            "paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
            "title": "What learning algorithm is in-context learning? Investigations with linear models"
        },
        {
            "paperId": "13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0",
            "title": "Vision Transformers provably learn spatial structure"
        },
        {
            "paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f",
            "title": "In-context Learning and Induction Heads"
        },
        {
            "paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9",
            "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"
        },
        {
            "paperId": "f5f5616f39493566a9d502f611adcc8f1ceb394e",
            "title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit"
        },
        {
            "paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",
            "title": "Exploring Length Generalization in Large Language Models"
        },
        {
            "paperId": "f5db3b0a99e9ab7777b2fecf8b5d237715a3464d",
            "title": "Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "0b0d7d87c58d41b92d907347b778032be5966f60",
            "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"
        },
        {
            "paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737",
            "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"
        },
        {
            "paperId": "148011adfae37b821407aae84fcbbf7fb4619eb6",
            "title": "On the Expressive Power of Self-Attention Matrices"
        },
        {
            "paperId": "b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea",
            "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"
        },
        {
            "paperId": "4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506",
            "title": "Approximating How Single Head Attention Learns"
        },
        {
            "paperId": "901b546ae60d1e3b6cfe80f19f0786321e701bf4",
            "title": "Why are Adaptive Methods Good for Attention Models?"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "5d50c955e4db594983167572f3cb033d7b8d98ef",
            "title": "Understanding Self-supervised Learning with Dual Deep Networks"
        },
        {
            "paperId": "10c86505de83647c7b4157595ab10f64e97c94ef",
            "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"
        },
        {
            "paperId": "dd0cbd365304eddec5ee961ced19291e1463bad1",
            "title": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks"
        },
        {
            "paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510",
            "title": "Infinite attention: NNGP and NTK for deep attention networks"
        },
        {
            "paperId": "75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1",
            "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling"
        },
        {
            "paperId": "19e0fa37631c7588651ef8c335928a9a2d4b2e2c",
            "title": "A Mean-field Analysis of Deep ResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth"
        },
        {
            "paperId": "42b0d32a7e644b657e34ce056c84d215d9f62187",
            "title": "Universal"
        },
        {
            "paperId": "0f1afdbbc806e191af4d294cd3e84787bd8de376",
            "title": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks"
        },
        {
            "paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d",
            "title": "Are Transformers universal approximators of sequence-to-sequence functions?"
        },
        {
            "paperId": "437fbb0cc305b7d3f0b2462a316cef0327099b26",
            "title": "Gradient descent optimizes over-parameterized deep ReLU networks"
        },
        {
            "paperId": "8215da38ec3b3ae08deb818ba3fc821377a120df",
            "title": "Towards Understanding the Importance of Shortcut Connections in Residual Networks"
        },
        {
            "paperId": "7cf64265882f7129b127ce0e27ff7bca9173aa58",
            "title": "The Context"
        },
        {
            "paperId": "9b81a4df6fbc2702f335ff984381a1634d1be23d",
            "title": "Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks"
        },
        {
            "paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443",
            "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"
        },
        {
            "paperId": "62a0bafd54099a79d78a013611a0c7d38e237032",
            "title": "On Lazy Training in Differentiable Programming"
        },
        {
            "paperId": "03e7e8663c69e691be6b6403b1eb1bbf593d31f2",
            "title": "Gradient Descent Finds Global Minima of Deep Neural Networks"
        },
        {
            "paperId": "6398cb8f2af1c988a097ed1e1cefb380195edfb8",
            "title": "(Preprint)"
        },
        {
            "paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
            "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"
        },
        {
            "paperId": "134c165953e23a6dc7d4f0d86989e92362ca4335",
            "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks"
        },
        {
            "paperId": "42ec3db12a2e4628885451b13035c2e975220a25",
            "title": "A Convergence Theory for Deep Learning via Over-Parameterization"
        },
        {
            "paperId": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6",
            "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"
        },
        {
            "paperId": "7a84a692327534fd227fa1e07fcb3816b633c591",
            "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
        },
        {
            "paperId": "9c7de616d16e5643e9e29dfdf2d7d6001c548132",
            "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"
        },
        {
            "paperId": "f1d48ad5a04360bf65e793b84298d8e0570bf1cc",
            "title": "A mean field view of the landscape of two-layer neural networks"
        },
        {
            "paperId": "63ae7e196430fe250e65b65fdef261d357921d28",
            "title": "Gradient Descent with Identity Initialization Efficiently Learns Positive-Definite Linear Transformations by Deep Residual Networks"
        },
        {
            "paperId": "181350e78a5c9e1f44e9cc2ace2137b7180856ba",
            "title": "Learning One Convolutional Layer with Overlapping Patches"
        },
        {
            "paperId": "f91248a4f587f89f1d1d8e557cee08b8114686d9",
            "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima"
        },
        {
            "paperId": "fbe1d29737b66840d2bb9b74cd093858ef1805dd",
            "title": "When is a Convolutional Filter Easy To Learn?"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "97db7860df3ee3624f8b55d5820b00f893bc4f9a",
            "title": "Learning ReLUs via Gradient Descent"
        },
        {
            "paperId": "c6f2f35169abb6bfeb4dd2deec15d38587910168",
            "title": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"
        },
        {
            "paperId": "fc756b45678ef7ffc1a796de62365013011b659e",
            "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "c4cb90a67f45e7cbacb5286e934b309e89843922",
            "title": "Attention is Turing-Complete"
        },
        {
            "paperId": null,
            "title": "A mathematical framework for transformer circuits"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "c43b41816a62f8aa260130b6fe8c1d3cb4463628",
            "title": "Toward Understanding the Importance of Noise in Training Neural Networks"
        },
        {
            "paperId": null,
            "title": "(cid:32)Lukasz"
        },
        {
            "paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607",
            "title": "A Survey on Context Learning"
        },
        {
            "paperId": "015ca32bca81dbda1e2e432445eef798582236e1",
            "title": "Conference Paper"
        }
    ]
}