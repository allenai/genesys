{
    "paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
    "externalIds": {
        "MAG": "2952158118",
        "ArXiv": "1607.03474",
        "DBLP": "journals/corr/ZillySKS16",
        "CorpusId": 1101453
    },
    "title": "Recurrent Highway Networks",
    "abstract": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.",
    "venue": "International Conference on Machine Learning",
    "year": 2016,
    "referenceCount": 60,
    "citationCount": 407,
    "influentialCitationCount": 62,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem is introduced that illuminates several modeling and optimization issues and improves the understanding of the LSTM cell."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2360589",
            "name": "J. Zilly"
        },
        {
            "authorId": "2100612",
            "name": "R. Srivastava"
        },
        {
            "authorId": "2865775",
            "name": "J. Koutn\u00edk"
        },
        {
            "authorId": "145341374",
            "name": "J. Schmidhuber"
        }
    ],
    "references": [
        {
            "paperId": "535347e07b2ce5ac229349156db1e7bed0486b91",
            "title": "Highway and Residual Networks learn Unrolled Iterative Estimation"
        },
        {
            "paperId": "67d968c7450878190e45ac7886746de867bf673d",
            "title": "Neural Architecture Search with Reinforcement Learning"
        },
        {
            "paperId": "424aef7340ee618132cc3314669400e23ad910ba",
            "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"
        },
        {
            "paperId": "55cf59bfbb25d6363cab87cb747648ebe8a096e5",
            "title": "Multiplicative LSTM for sequence modelling"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
            "title": "Hierarchical Multiscale Recurrent Neural Networks"
        },
        {
            "paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559",
            "title": "Using the Output Embedding to Improve Language Models"
        },
        {
            "paperId": "136cf66392f1d6bf42da4cc070888996dc472b91",
            "title": "On Multiplicative Integration with Recurrent Neural Networks"
        },
        {
            "paperId": "952454718139dba3aafc6b3b67c4f514ac3964af",
            "title": "Recurrent Batch Normalization"
        },
        {
            "paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01",
            "title": "Adaptive Computation Time for Recurrent Neural Networks"
        },
        {
            "paperId": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "title": "Exploring the Limits of Language Modeling"
        },
        {
            "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
            "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "9cee45ef1212ebbc7d468f9b1d7df24f5005e64d",
            "title": "Highway long short-term memory RNNS for distant speech recognition"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "b92aa7024b87f50737b372e5df31ef091ab54e62",
            "title": "Training Very Deep Networks"
        },
        {
            "paperId": "5b791cd374c7109693aaddee2c12d659ae4e3ec0",
            "title": "Grid Long Short-Term Memory"
        },
        {
            "paperId": "5b8364c21155d3d2cd38ea4c8b8580beba9a3250",
            "title": "An Empirical Exploration of Recurrent Network Architectures"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "d46b81707786d18499f911b4ab72bb10c65406ba",
            "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units"
        },
        {
            "paperId": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081",
            "title": "LSTM: A Search Space Odyssey"
        },
        {
            "paperId": "d14c7e5f5cace4c925abc74c88baa474e9f31a28",
            "title": "Gated Feedback Recurrent Neural Networks"
        },
        {
            "paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97",
            "title": "Recurrent Neural Network Regularization"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "193edd20cae92c6759c18ce93eeea96afd9528eb",
            "title": "Deep learning in neural networks: An overview"
        },
        {
            "paperId": "085d57b7f44e97ba6fa54543170b884a7461fc08",
            "title": "On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures"
        },
        {
            "paperId": "533ee188324b833e059cb59b654e6160776d5812",
            "title": "How to Construct Deep Recurrent Neural Networks"
        },
        {
            "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
            "title": "Generating Sequences With Recurrent Neural Networks"
        },
        {
            "paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4",
            "title": "Context dependent recurrent neural network language model"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "934735587a2a899f4619e35331f302b220961183",
            "title": "First Experiments with PowerPlay"
        },
        {
            "paperId": "152d82025f02916019e4cfcc943dceecc159cda4",
            "title": "Self-Delimiting Neural Networks"
        },
        {
            "paperId": "07c43a3ff15f2104022f2b1ca8ec4128a930b414",
            "title": "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription"
        },
        {
            "paperId": "5a9ef216bf11f222438fff130c778267d39a9564",
            "title": "Practical Variational Inference for Neural Networks"
        },
        {
            "paperId": "12496bf48ebdb5ab3c92bc911d6ee42369fa70bc",
            "title": "Sequence Labelling in Structured Domains with Hierarchical Recurrent Neural Networks"
        },
        {
            "paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "title": "Learning to Forget: Continual Prediction with LSTM"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "title": "Learning long-term dependencies with gradient descent is difficult"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "50c770b425a5bb25c77387f687a9910a9d130722",
            "title": "Learning Complex, Extended Sequences Using the Principle of History Compression"
        },
        {
            "paperId": "f49101078fb25f714a9813259ece149581017782",
            "title": "Taylor expansion of the accumulated rounding error"
        },
        {
            "paperId": "5762b7deff7e95febe193196d548379ff34b34f1",
            "title": "Improved Learning through Augmenting the Loss"
        },
        {
            "paperId": null,
            "title": "Brainstorm: Fast, Flexible and Fun Neural Networks, Version 0.5"
        },
        {
            "paperId": null,
            "title": "The human knowledge compression"
        },
        {
            "paperId": "3449b65008b27f6e60a73d80c1fd990f0481126b",
            "title": "Torch7: A Matlab-like Environment for Machine Learning"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
            "title": "Scaling learning algorithms towards AI"
        },
        {
            "paperId": "2e5f2b57f4c476dd69dc22ccdf547e48f40a994c",
            "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"
        },
        {
            "paperId": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "title": "Untersuchungen zu dynamischen neuronalen Netzen"
        },
        {
            "paperId": "66965187b7cb949ea07f15630535d2aea5bde505",
            "title": "Reinforcement Learning in Markovian and Non-markovian Environments"
        },
        {
            "paperId": null,
            "title": "Complexity of exact gradient computation algorithms for recurrent neural networks. Technical Report NU-CCS-89-27, Boston"
        },
        {
            "paperId": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "title": "Generalization of backpropagation with application to a recurrent gas market model"
        },
        {
            "paperId": "fa277dfe3645463a25432282563fca4891d846ea",
            "title": "Applications of advances in nonlinear sensitivity analysis"
        },
        {
            "paperId": "32657fbf5859d3ed16d8664d1fc6f154dcefa976",
            "title": "System Modeling and Optimization"
        },
        {
            "paperId": null,
            "title": "The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors"
        },
        {
            "paperId": null,
            "title": "\u00dcber die Abgrenzung der Eigenwerte einer Matrix"
        },
        {
            "paperId": null,
            "title": "Bulletin de l'Acad\u00e8mie des Sciences de l'URSS. Classe des sciences math\u00e8matiques"
        },
        {
            "paperId": null,
            "title": "Regarding variational dropout, a rate of 0.25 was applied to both the embedding and hidden units and a rate of 0.7 was applied to both input to gates and output of the network"
        },
        {
            "paperId": null,
            "title": "Srivastava , Rupesh Kumar , Steunebrink , Bas R , and Schmidhuber , J\u00fcrgen . First experiments with power - play"
        },
        {
            "paperId": "02f38b2d72d7b3243b5ba4005f814f71b80eec00",
            "title": "The Utility Driven Dynamic Error Propagation Network"
        }
    ]
}