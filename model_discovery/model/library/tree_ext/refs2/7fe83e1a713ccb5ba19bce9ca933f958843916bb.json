{
    "paperId": "7fe83e1a713ccb5ba19bce9ca933f958843916bb",
    "externalIds": {
        "ArXiv": "1511.04868",
        "MAG": "2491408735",
        "CorpusId": 17160640
    },
    "title": "A Neural Transducer",
    "abstract": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.",
    "venue": "",
    "year": 2015,
    "referenceCount": 20,
    "citationCount": 47,
    "influentialCitationCount": 1,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation, and performs well for long sequences even when attention mechanisms are not used."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "3111912",
            "name": "N. Jaitly"
        },
        {
            "authorId": "3089810",
            "name": "David Sussillo"
        },
        {
            "authorId": "2827616",
            "name": "Quoc V. Le"
        },
        {
            "authorId": "1689108",
            "name": "O. Vinyals"
        },
        {
            "authorId": "1701686",
            "name": "I. Sutskever"
        },
        {
            "authorId": "1751569",
            "name": "Samy Bengio"
        }
    ],
    "references": [
        {
            "paperId": "b59d91e0699d4e1896a15bae13fd180bdaf77ea5",
            "title": "Neural Programmer-Interpreters"
        },
        {
            "paperId": "bf85a0cd645ad68919c0706741ab568a60a58af2",
            "title": "Neural Programmer: Inducing Latent Programs with Gradient Descent"
        },
        {
            "paperId": "878ba5458e9e51f0b341fd9117fa0b43ef4096d3",
            "title": "End-to-end attention-based large vocabulary speech recognition"
        },
        {
            "paperId": "b624504240fa52ab76167acfe3156150ca01cf3b",
            "title": "Attention-Based Models for Speech Recognition"
        },
        {
            "paperId": "5247a6e3a60ff0381355e66bfc313bf27512ae0c",
            "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses"
        },
        {
            "paperId": "86311b182786bfde19446f6ded0854de973d4060",
            "title": "A Neural Conversational Model"
        },
        {
            "paperId": "c9d7afd65b2127e6f0651bc7e13eeec1897ac8dd",
            "title": "Reinforcement Learning Neural Turing Machines"
        },
        {
            "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "title": "End-To-End Memory Networks"
        },
        {
            "paperId": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40",
            "title": "Grammar as a Foreign Language"
        },
        {
            "paperId": "47d2dc34e1d02a8109f5c04bb6939725de23716d",
            "title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results"
        },
        {
            "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "title": "Show and tell: A neural image caption generator"
        },
        {
            "paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de",
            "title": "Neural Turing Machines"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "2319a491378867c7049b3da055c5df60e1671158",
            "title": "Playing Atari with Deep Reinforcement Learning"
        },
        {
            "paperId": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "title": "Speech recognition with deep recurrent neural networks"
        },
        {
            "paperId": "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f",
            "title": "Sequence Transduction with Recurrent Neural Networks"
        },
        {
            "paperId": null,
            "title": "Listen, attend and spell"
        },
        {
            "paperId": "31868290adf1c000c611dfc966b514d5a34e8d23",
            "title": "FUNDAMENTAL TECHNOLOGIES IN MODERN SPEECH RECOGNITION Digital Object Identifier 10.1109/MSP.2012.2205597"
        }
    ]
}