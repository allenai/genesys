{
    "paperId": "98a88f6b4d11f1e07a021ce1346b1454a66ab488",
    "externalIds": {
        "DBLP": "journals/corr/abs-1805-00912",
        "MAG": "2798547456",
        "CorpusId": 19936690
    },
    "title": "Fast Directional Self-Attention Mechanism",
    "abstract": "In this paper, we propose a self-attention mechanism, dubbed \"fast directional self-attention (Fast-DiSA)\", which is a fast and light extension of \"directional self-attention (DiSA)\". The proposed Fast-DiSA performs as expressively as the original DiSA but only uses much less computation time and memory, in which 1) both token2token and source2token dependencies are modeled by a joint compatibility function designed for a hybrid of both dot-product and multi-dim ways; 2) both multi-head and multi-dim attention combined with bi-directional temporal information captured by multiple positional masks are in consideration without heavy time and memory consumption appearing in the DiSA. The experiment results show that the proposed Fast-DiSA can achieve state-of-the-art performance as fast and memory-friendly as CNNs. The code for Fast-DiSA is released at \\url{this https URL}.",
    "venue": "arXiv.org",
    "year": 2018,
    "referenceCount": 38,
    "citationCount": 5,
    "influentialCitationCount": 1,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The experiment results show that the proposed Fast-DiSA can achieve state-of-the-art performance as fast and memory-friendly as CNNs."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": null,
            "name": "Tao Shen"
        },
        {
            "authorId": "1805655",
            "name": "Tianyi Zhou"
        },
        {
            "authorId": "2062835",
            "name": "Guodong Long"
        },
        {
            "authorId": "1746594",
            "name": "Jing Jiang"
        },
        {
            "authorId": "48934799",
            "name": "Chengqi Zhang"
        }
    ],
    "references": [
        {
            "paperId": "5f38a07add820c82a2b13a17d891f08cdcaef500",
            "title": "Enhancing Sentence Embedding with Generalized Pooling"
        },
        {
            "paperId": "060ff1aad5619a7d6d6cdfaf8be5da29bff3808c",
            "title": "Linguistically-Informed Self-Attention for Semantic Role Labeling"
        },
        {
            "paperId": "0ef460c47377c3b9482d8177cbcafad1730a91a5",
            "title": "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling"
        },
        {
            "paperId": "5d727286a59d7e2681b6fac5fa269e782849f007",
            "title": "Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling"
        },
        {
            "paperId": "d12ae90771d14555a64aa48b8a3f638e7d14426d",
            "title": "Distance-based Self-Attention Network for Natural Language Inference"
        },
        {
            "paperId": "6ed376a26045ff0048ec2b216785d396960d6ed1",
            "title": "Deep Semantic Role Labeling with Self-Attention"
        },
        {
            "paperId": "adc276e6eae7051a027a4c269fb21dae43cadfed",
            "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding"
        },
        {
            "paperId": "ad45b1291067120bf9e55ac7424eb627e0aab149",
            "title": "Training RNNs as Fast as CNNs"
        },
        {
            "paperId": "26fe009b958e8728382d9d764bd7153632f0b869",
            "title": "Shortcut-Stacked Sentence Encoders for Multi-Domain Inference"
        },
        {
            "paperId": "ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc",
            "title": "Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference"
        },
        {
            "paperId": "027f9695189355d18ec6be8e48f3d23ea25db35d",
            "title": "Learning to Compose Task-Specific Tree Structures"
        },
        {
            "paperId": "a4dd3beea286a20c4e4f66436875932d597190bc",
            "title": "Deep Semantic Role Labeling: What Works and What\u2019s Next"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning"
        },
        {
            "paperId": "42d2338a8c2e44154e10ff4d68a3c389aeca3913",
            "title": "Reinforced Mnemonic Reader for Machine Comprehension"
        },
        {
            "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
            "title": "Bidirectional Attention Flow for Machine Comprehension"
        },
        {
            "paperId": "f93a0a3e8a3e6001b4482430254595cf737697fa",
            "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention"
        },
        {
            "paperId": "36c097a225a95735271960e2b63a2cb9e98bff83",
            "title": "A Fast Unified Model for Parsing and Sentence Understanding"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference"
        },
        {
            "paperId": "c34e41312b47f60986458759d5cc546c2b53f748",
            "title": "End-to-end learning of semantic role labeling using recurrent neural networks"
        },
        {
            "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "title": "Skip-Thought Vectors"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "af9b9235a68307c782d14d4bf12cb80d662d247f",
            "title": "Efficient Inference and Structured Learning for Semantic Role Labeling"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
            "paperId": "0ca7d208ff8d81377e0eaa9723820aeae7a7322d",
            "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences"
        },
        {
            "paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
            "title": "A Convolutional Neural Network for Modelling Sentences"
        },
        {
            "paperId": "1149888d75af4ed5dffc25731b875651c3ccdeb2",
            "title": "Hybrid speech recognition with Deep Bidirectional LSTM"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "cdcf7cb29f37ac0546961ea8a076075b9cc1f992",
            "title": "Mining and summarizing customer reviews"
        },
        {
            "paperId": "167e1359943b96b9e92ee73db1df69a1f65d731d",
            "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts"
        },
        {
            "paperId": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7",
            "title": "Learning Question Classifiers"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": "1cff7cc15555c38607016aaba24059e76b160adb",
            "title": "Annotating Expressions of Opinions and Emotions in Language"
        }
    ]
}