{
    "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
    "externalIds": {
        "MAG": "2953084091",
        "DBLP": "conf/emnlp/BowmanAPM15",
        "ACL": "D15-1075",
        "ArXiv": "1508.05326",
        "DOI": "10.18653/v1/D15-1075",
        "CorpusId": 14604520
    },
    "title": "A large annotated corpus for learning natural language inference",
    "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2015,
    "referenceCount": 40,
    "citationCount": 3978,
    "influentialCitationCount": 845,
    "openAccessPdf": {
        "url": "https://doi.org/10.18653/v1/d15-1075",
        "status": "BRONZE"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Stanford Natural Language Inference corpus is introduced, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning, which allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "3644767",
            "name": "Samuel R. Bowman"
        },
        {
            "authorId": "32301760",
            "name": "Gabor Angeli"
        },
        {
            "authorId": "144922861",
            "name": "Christopher Potts"
        },
        {
            "authorId": "144783904",
            "name": "Christopher D. Manning"
        }
    ],
    "references": [
        {
            "paperId": "a47a0b0859547f327760d27bf8ce09c6e6f21fa2",
            "title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification"
        },
        {
            "paperId": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
            "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"
        },
        {
            "paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39",
            "title": "Memory Networks"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "11ec56898a9e7f401a2affe776b5297bd4e25025",
            "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment"
        },
        {
            "paperId": "f529dc492b7f3d1b22db64bc7ad36b1f13641a84",
            "title": "Illinois-LH: A Denotational and Distributional Approach to Semantics"
        },
        {
            "paperId": "6d334cb557f5addd3fc080134483125eea0b7a90",
            "title": "On our best behaviour"
        },
        {
            "paperId": "7c05a4ffee7e159e34b2efea7e44d994333ec628",
            "title": "Recursive Neural Networks Can Learn Logical Semantics"
        },
        {
            "paperId": "a53815cf0c9cbb54a23b11f73c7d532910afa750",
            "title": "The Excitement Open Platform for Textual Inferences"
        },
        {
            "paperId": "8cef20c98d41018c44c78772424112c5b5679144",
            "title": "Focused Entailment Graphs for Open IE Propositions"
        },
        {
            "paperId": "c333778104f648c385b4631f7b4a859787e9d3d3",
            "title": "A SICK cure for the evaluation of compositional distributional semantic models"
        },
        {
            "paperId": "44040913380206991b1991daf1192942e038fe31",
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"
        },
        {
            "paperId": "b5f55466136bde834232f5641619bcc488c070bc",
            "title": "Design and realization of a modular architecture for textual entailment"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "8729441d734782c3ed532a7d2d9611b438c0a09a",
            "title": "ADADELTA: An Adaptive Learning Rate Method"
        },
        {
            "paperId": "5e9fa46f231c59e6573f9a116f77f53703347659",
            "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification"
        },
        {
            "paperId": "5feb2c61b04532869e44d1ca4e48c7108aee5fd3",
            "title": "An extended model of natural logic"
        },
        {
            "paperId": "287d5571dbf255a7ccbdc2bcfe9211fd8f0b2a7c",
            "title": "Finding Contradictions in Text"
        },
        {
            "paperId": "d5b4fbd5d2d6e3083d8fa20b113261a2fc9817f2",
            "title": "Information Structure and Non\u2010canonical Syntax"
        },
        {
            "paperId": "40e7e3d009f83f17b6aa892eeae67d225056c98a",
            "title": "Recognizing Textual Entailment Using Sentence Similarity based on Dependency Tree Skeletons"
        },
        {
            "paperId": "ea2563467c1c472a346d165b7f97c86317d63ca4",
            "title": "Recognising Textual Entailment with Logical Inference"
        },
        {
            "paperId": "0c739b915d633cc3c162e4ef1e57b796c2dc2217",
            "title": "VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations"
        },
        {
            "paperId": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "title": "Accurate Unlexicalized Parsing"
        },
        {
            "paperId": "5ffa3aea748533186b6638d97eafe80d86b208b2",
            "title": "Entailment, intensionality and text understanding"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "68c03788224000794d5491ab459be0b2a2c38677",
            "title": "WordNet: A Lexical Database for English"
        },
        {
            "paperId": "607e20aa228f9f8f0f9e96e841f2b0ec75726728",
            "title": "Direct Transfer of Learned Information Among Neural Networks"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "24b391a74ec61209b86a9c4ad37387f9ca261f12",
            "title": "A brief history of natural logic"
        },
        {
            "paperId": "de794d50713ea5f91a7c9da3d72041e2f5ef8452",
            "title": "The Third PASCAL Recognizing Textual Entailment Challenge"
        },
        {
            "paperId": "e03d300581e16f6664157d2c1c6ceec33ec528ce",
            "title": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment"
        },
        {
            "paperId": "e808f28d411a958c5db81ceb111beb2638698f47",
            "title": "The PASCAL Recognising Textual Entailment Challenge"
        },
        {
            "paperId": "51a3f127ce3047295d9967ccff19fd006ebc8841",
            "title": "A Natural Logic Inference System"
        },
        {
            "paperId": null,
            "title": "Brown corpus manual"
        },
        {
            "paperId": "de4961a6431b9553d9f13319236570a9f033fcab",
            "title": "Understanding natural language"
        },
        {
            "paperId": null,
            "title": "Semantic Theory"
        },
        {
            "paperId": null,
            "title": "specifically on the strengths of these models at producing informative sentence representations, we use sentence embedding as an intermediate step in the"
        },
        {
            "paperId": null,
            "title": "The overlap between words in the premise and hypothesis, both as an absolute count and a percentage of possible overlap, and both over all words and over just nouns, verbs, adjectives, and adverbs"
        },
        {
            "paperId": null,
            "title": "An indicator for every unigram and bigram in the hypothesis"
        },
        {
            "paperId": null,
            "title": "Cross-bigrams: for every pair of bigrams across the premise and hypothesis which share a POS tag on the second word, an indicator feature over the two bigrams"
        }
    ]
}