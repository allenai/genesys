{
    "paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39",
    "externalIds": {
        "MAG": "3017022649",
        "DBLP": "journals/corr/abs-2004-07320",
        "ArXiv": "2004.07320",
        "CorpusId": 215814169
    },
    "title": "Training with Quantization Noise for Extreme Model Compression",
    "abstract": "We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work beyond int8 fixed-point quantization with extreme compression methods where the approximations introduced by STE are severe, such as Product Quantization. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14MB and 80.0 top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3MB.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "referenceCount": 88,
    "citationCount": 213,
    "influentialCitationCount": 16,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights, establishing new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.13822078704834,
            -0.570040225982666,
            -2.062753677368164,
            2.406648874282837,
            -4.830872058868408,
            3.8626749515533447,
            4.057941436767578,
            -1.1162424087524414,
            -1.778608798980713,
            -2.2455663681030273,
            -2.350499391555786,
            3.9369640350341797,
            0.19988366961479187,
            -2.5530166625976562,
            -4.688765048980713,
            -0.09038335829973221,
            -0.6045185923576355,
            2.800078868865967,
            3.0311028957366943,
            4.977707386016846,
            -2.7340874671936035,
            1.4374629259109497,
            -5.7473015785217285,
            1.7164121866226196,
            -0.9694490432739258,
            0.30711838603019714,
            0.3797520399093628,
            0.4268396496772766,
            -1.0963521003723145,
            -1.7347357273101807,
            -0.7119919657707214,
            -3.7830164432525635,
            4.695767879486084,
            -1.9651520252227783,
            1.8762011528015137,
            0.7266621589660645,
            0.04530033469200134,
            6.548108100891113,
            -2.0373148918151855,
            -2.9316601753234863,
            -1.7706186771392822,
            -0.7694122791290283,
            2.2834272384643555,
            -0.8280808925628662,
            -0.18706673383712769,
            -0.9337595105171204,
            0.22136029601097107,
            1.6792497634887695,
            0.7308360934257507,
            0.452245831489563,
            2.046144962310791,
            2.048396348953247,
            1.7422975301742554,
            0.4414910674095154,
            -0.43125754594802856,
            -0.7171949148178101,
            -0.3546226918697357,
            0.7485160231590271,
            -0.5749901533126831,
            -0.7972585558891296,
            -0.8541004061698914,
            2.7173190116882324,
            -0.2779588997364044,
            1.4395723342895508,
            1.8879307508468628,
            -0.8249523043632507,
            -0.832851767539978,
            4.732473373413086,
            0.06470689177513123,
            -1.2124109268188477,
            -0.638198971748352,
            -4.185184478759766,
            1.636631965637207,
            -0.48153015971183777,
            -5.8176679611206055,
            -1.3272727727890015,
            0.8035144805908203,
            -4.796943664550781,
            0.21008186042308807,
            -0.540364146232605,
            -0.7993131875991821,
            4.0419087409973145,
            -2.4531240463256836,
            -0.1455954909324646,
            6.9221649169921875,
            -0.7741693258285522,
            -1.3673423528671265,
            -3.2808332443237305,
            -0.343136191368103,
            0.4541351795196533,
            2.136801242828369,
            0.2895033359527588,
            3.0216357707977295,
            2.0724921226501465,
            -2.806065082550049,
            0.09553590416908264,
            0.4109283685684204,
            0.9765263199806213,
            0.8861610293388367,
            1.6598803997039795,
            0.5477451086044312,
            0.3950659930706024,
            1.0834698677062988,
            -0.7516026496887207,
            4.212381362915039,
            -3.3450231552124023,
            0.7392288446426392,
            1.4120382070541382,
            1.6126211881637573,
            -5.402317047119141,
            -0.25157541036605835,
            2.287334442138672,
            -1.5297186374664307,
            -0.137568861246109,
            -3.7138822078704834,
            -1.6213595867156982,
            1.3524292707443237,
            0.24188578128814697,
            -1.8631950616836548,
            1.9526700973510742,
            0.3884297311306,
            -5.550525665283203,
            -3.2221028804779053,
            1.04405677318573,
            1.3757166862487793,
            4.0992536544799805,
            0.34100115299224854,
            3.5787272453308105,
            -3.694629192352295,
            -5.988651752471924,
            0.8400760293006897,
            -3.0948781967163086,
            2.307169198989868,
            -0.6475228071212769,
            4.06326150894165,
            1.687283992767334,
            -2.094820737838745,
            0.9285021424293518,
            -1.2118003368377686,
            0.03174307942390442,
            3.7765350341796875,
            2.8559136390686035,
            -0.7343160510063171,
            3.1054067611694336,
            4.365596771240234,
            4.502639293670654,
            -0.6876498460769653,
            -0.5282055139541626,
            -1.8280822038650513,
            5.299348831176758,
            4.154760837554932,
            -2.4256088733673096,
            0.9750871062278748,
            0.49566781520843506,
            2.246488094329834,
            3.5881805419921875,
            -1.6052829027175903,
            2.4007019996643066,
            -0.5576142072677612,
            -0.6932977437973022,
            -2.708012580871582,
            -1.7004214525222778,
            -8.974441528320312,
            -1.9056077003479004,
            4.948892116546631,
            -2.880570411682129,
            0.22458451986312866,
            1.4278329610824585,
            -1.1960569620132446,
            3.5961334705352783,
            1.649111032485962,
            1.3299680948257446,
            0.8895830512046814,
            5.132558345794678,
            4.004698276519775,
            5.898642539978027,
            3.939743995666504,
            -3.637232780456543,
            0.04281225800514221,
            2.222412109375,
            -1.7348130941390991,
            -1.430369257926941,
            -2.6914591789245605,
            0.7901065349578857,
            -7.054941177368164,
            -1.2152509689331055,
            -2.6861298084259033,
            0.2450130581855774,
            -1.7013638019561768,
            -0.7698038220405579,
            1.2925453186035156,
            -0.9212079048156738,
            4.130724906921387,
            3.9718408584594727,
            3.6349034309387207,
            -0.059088870882987976,
            -0.02024930715560913,
            2.7843947410583496,
            -1.9499884843826294,
            0.6021376848220825,
            6.305177211761475,
            0.3958556652069092,
            -1.808525562286377,
            -4.032085418701172,
            3.8025145530700684,
            0.927457332611084,
            -3.1729745864868164,
            4.046823501586914,
            0.19114893674850464,
            -1.1281776428222656,
            -2.1811912059783936,
            -3.8107335567474365,
            -1.4634101390838623,
            0.921049177646637,
            -2.9646191596984863,
            -1.279242753982544,
            -4.795535564422607,
            1.9006524085998535,
            4.489762306213379,
            -0.24579380452632904,
            -0.4432283341884613,
            1.710536241531372,
            -2.965193748474121,
            -3.700693130493164,
            2.792189836502075,
            -2.2772672176361084,
            1.6707806587219238,
            2.679410934448242,
            0.01970762014389038,
            -0.39796265959739685,
            2.30031418800354,
            -3.7647414207458496,
            0.9068329334259033,
            1.30325448513031,
            -4.77065896987915,
            2.9762630462646484,
            -5.487248420715332,
            1.1999796628952026,
            -0.4226594865322113,
            0.2500503659248352,
            6.6134796142578125,
            -0.7169328927993774,
            2.0771453380584717,
            3.5953588485717773,
            0.14040982723236084,
            -2.545724391937256,
            -3.1698853969573975,
            -0.7423849105834961,
            0.30031123757362366,
            -3.723439931869507,
            -1.385809302330017,
            1.1718196868896484,
            -2.279533624649048,
            -4.339864730834961,
            1.666593074798584,
            1.5064703226089478,
            2.2572154998779297,
            -0.18221759796142578,
            1.5760835409164429,
            1.0585651397705078,
            -1.7215664386749268,
            8.379061698913574,
            -2.590881586074829,
            1.3157449960708618,
            -0.7848027348518372,
            -3.9167113304138184,
            -5.844510078430176,
            -3.1734795570373535,
            0.447292685508728,
            3.3886337280273438,
            0.3773842751979828,
            -1.0054917335510254,
            -1.850813865661621,
            -1.27504301071167,
            -2.9938557147979736,
            -2.0573368072509766,
            0.7897480726242065,
            1.3240653276443481,
            3.6403322219848633,
            4.5130720138549805,
            0.17425096035003662,
            -5.029693603515625,
            -0.5604403614997864,
            -3.0956339836120605,
            -3.19822359085083,
            -3.739013195037842,
            -0.18415498733520508,
            1.2684764862060547,
            -4.529736042022705,
            -0.2874860167503357,
            -6.211305618286133,
            1.0955963134765625,
            -3.170228958129883,
            2.3098256587982178,
            -4.179189682006836,
            0.9355507493019104,
            4.083804130554199,
            2.654132604598999,
            -0.7878689765930176,
            3.564044952392578,
            0.6733675003051758,
            1.2062575817108154,
            4.0863847732543945,
            -0.7650478482246399,
            2.080352783203125,
            0.9602683782577515,
            0.4111025929450989,
            -3.3952813148498535,
            3.1272153854370117,
            -2.575115442276001,
            -1.3319023847579956,
            0.0387037992477417,
            2.662991762161255,
            -4.182298183441162,
            2.3624820709228516,
            -1.3315932750701904,
            2.9694454669952393,
            -0.20266062021255493,
            -3.039414405822754,
            2.5170607566833496,
            2.044682502746582,
            3.4710922241210938,
            -2.8023900985717773,
            -5.473306655883789,
            -2.225287437438965,
            -1.6319221258163452,
            5.017020225524902,
            0.2353067845106125,
            -2.5202994346618652,
            3.1546237468719482,
            4.336522102355957,
            1.5476340055465698,
            4.813236713409424,
            2.7373554706573486,
            -1.8400810956954956,
            -4.594215393066406,
            -0.07429784536361694,
            -1.9061802625656128,
            0.9784311056137085,
            0.04035408794879913,
            -1.996254801750183,
            4.128695487976074,
            -0.38851362466812134,
            2.9601826667785645,
            -4.109270095825195,
            -1.6770479679107666,
            5.736482620239258,
            -0.8713175058364868,
            1.061307430267334,
            0.1783239245414734,
            -1.4433103799819946,
            1.4846711158752441,
            3.523379325866699,
            -2.076301336288452,
            -0.19278639554977417,
            1.180403470993042,
            3.459167957305908,
            -1.7736684083938599,
            3.3583967685699463,
            2.5423312187194824,
            0.5444446802139282,
            0.41223305463790894,
            5.681146144866943,
            -1.165196180343628,
            0.39065903425216675,
            -1.442136287689209,
            9.605228424072266,
            -1.702075481414795,
            -2.510108232498169,
            -3.9174087047576904,
            -2.371643543243408,
            -3.1993706226348877,
            -0.2911338210105896,
            0.07801806926727295,
            -0.05441206693649292,
            -2.3539390563964844,
            2.641902446746826,
            -4.550734043121338,
            2.4389808177948,
            0.5967959761619568,
            3.788917064666748,
            1.6741504669189453,
            -0.63137286901474,
            4.27526330947876,
            0.501829206943512,
            -1.0165733098983765,
            0.395557165145874,
            -0.3646181523799896,
            0.5617752075195312,
            -1.9898998737335205,
            -0.9232155680656433,
            0.5147141218185425,
            1.0678929090499878,
            2.3686723709106445,
            -3.8129279613494873,
            -3.18257474899292,
            -5.630475044250488,
            -4.092535018920898,
            -2.363621473312378,
            -0.5554656982421875,
            -0.015775665640830994,
            0.254419207572937,
            5.137605667114258,
            0.49009108543395996,
            -2.9850573539733887,
            -2.7761354446411133,
            3.3841946125030518,
            -1.3972971439361572,
            -2.343149423599243,
            1.8957313299179077,
            -4.308465957641602,
            -0.45560556650161743,
            -2.1475374698638916,
            -0.34367573261260986,
            2.2985715866088867,
            -0.8043836355209351,
            1.239417314529419,
            3.425082206726074,
            1.3881490230560303,
            3.5234484672546387,
            -1.0993235111236572,
            2.924448013305664,
            4.033320903778076,
            4.792199611663818,
            -1.7962534427642822,
            3.719076156616211,
            1.6420966386795044,
            0.543732762336731,
            2.0649359226226807,
            0.18590480089187622,
            -2.3086953163146973,
            5.219718933105469,
            -2.0579466819763184,
            2.9163804054260254,
            -0.7101067304611206,
            0.7288447618484497,
            0.8997798562049866,
            -0.09890618920326233,
            3.063730239868164,
            -1.6972285509109497,
            0.547735869884491,
            3.6389670372009277,
            -1.5635042190551758,
            3.46549129486084,
            1.777044653892517,
            1.4691970348358154,
            2.6394643783569336,
            -0.13863950967788696,
            -0.621384859085083,
            -3.2523951530456543,
            4.130941390991211,
            -5.640283584594727,
            -2.888185501098633,
            0.5843202471733093,
            1.544068694114685,
            1.4382739067077637,
            0.6436139345169067,
            -1.8319703340530396,
            -0.81379234790802,
            0.1983286440372467,
            -1.9675469398498535,
            -0.13475428521633148,
            0.6039702296257019,
            1.0499199628829956,
            -0.35871991515159607,
            1.966439962387085,
            2.5493102073669434,
            0.646205723285675,
            -0.13663452863693237,
            0.9193464517593384,
            3.9440064430236816,
            1.2402032613754272,
            -2.065218687057495,
            -1.1077587604522705,
            0.681328535079956,
            1.1231212615966797,
            4.486660003662109,
            3.462411403656006,
            0.9377486705780029,
            -7.812773704528809,
            -4.481544494628906,
            -1.815502643585205,
            3.403412342071533,
            0.5781179666519165,
            -1.0419163703918457,
            3.9333410263061523,
            2.5336453914642334,
            3.0335049629211426,
            5.339562892913818,
            -0.21031004190444946,
            -2.1505606174468994,
            -0.7187687754631042,
            4.582555770874023,
            -0.40704628825187683,
            3.672161817550659,
            0.22845135629177094,
            -2.9372079372406006,
            1.6467427015304565,
            2.3637170791625977,
            -1.2349681854248047,
            -1.0634641647338867,
            -2.4849255084991455,
            -2.5182743072509766,
            0.15142011642456055,
            0.16227805614471436,
            1.98093581199646,
            1.037320852279663,
            0.47653287649154663,
            0.4974619746208191,
            -0.12204557657241821,
            0.4029442369937897,
            0.14087030291557312,
            -2.9212071895599365,
            0.8798114061355591,
            1.3863354921340942,
            -0.8850758075714111,
            1.3490530252456665,
            -0.7014023661613464,
            -1.512711524963379,
            -0.5683369636535645,
            -1.3623396158218384,
            1.2480474710464478,
            0.9767054319381714,
            3.030113458633423,
            -1.0090148448944092,
            1.5586804151535034,
            -2.578001022338867,
            1.4606140851974487,
            1.8666561841964722,
            3.068000555038452,
            7.104759693145752,
            2.8627781867980957,
            -0.11011219769716263,
            -3.8278861045837402,
            0.5606893301010132,
            -2.0034685134887695,
            2.451655626296997,
            1.950886845588684,
            -5.023583889007568,
            -1.289716124534607,
            -1.5637798309326172,
            1.6276116371154785,
            -1.5060287714004517,
            2.3472704887390137,
            -3.0993738174438477,
            2.5378878116607666,
            -1.1500074863433838,
            -4.074122905731201,
            -1.188030481338501,
            3.462082862854004,
            1.3382383584976196,
            1.1019597053527832,
            3.0647482872009277,
            -1.4458062648773193,
            -5.936122894287109,
            -2.844700813293457,
            -2.4615349769592285,
            0.43779438734054565,
            -0.15479588508605957,
            2.7720296382904053,
            -0.5910748243331909,
            0.7904616594314575,
            -1.7240488529205322,
            0.36115407943725586,
            -3.4485132694244385,
            -4.344412803649902,
            -0.5841215252876282,
            1.7015433311462402,
            -0.8886744976043701,
            -1.2081643342971802,
            -1.3294169902801514,
            -4.315701484680176,
            1.5214067697525024,
            -0.4242754578590393,
            0.4846135973930359,
            0.10207490622997284,
            5.119089126586914,
            -0.1569340080022812,
            -2.234846591949463,
            -0.6638765931129456,
            -2.7027134895324707,
            -3.1574177742004395,
            -2.8121416568756104,
            -3.1239609718322754,
            3.189603328704834,
            -7.300629615783691,
            -2.692847490310669,
            4.353665828704834,
            -1.3850088119506836,
            1.4958401918411255,
            -1.7951250076293945,
            -4.312180519104004,
            -4.131647109985352,
            -2.5642497539520264,
            -1.3389918804168701,
            -3.162388801574707,
            1.2778503894805908,
            0.6451037526130676,
            -0.08367855101823807,
            -0.11910085380077362,
            2.7708916664123535,
            6.933138847351074,
            4.922726154327393,
            3.987391948699951,
            -2.231626272201538,
            3.298426628112793,
            2.4769413471221924,
            1.1240832805633545,
            -0.44136178493499756,
            -3.5717382431030273,
            1.4214190244674683,
            2.6892647743225098,
            16.037010192871094,
            1.3534181118011475,
            -0.997223973274231,
            -3.4441769123077393,
            1.6307045221328735,
            -2.9294328689575195,
            -3.3719258308410645,
            3.8484585285186768,
            1.5590606927871704,
            2.641874313354492,
            2.0702898502349854,
            -1.2220532894134521,
            0.7016134262084961,
            1.4812825918197632,
            -4.086951732635498,
            0.10012035071849823,
            -1.7808870077133179,
            1.3363553285598755,
            -0.3115187883377075,
            -0.9996621012687683,
            -1.7455896139144897,
            -0.1877197027206421,
            3.0505857467651367,
            -1.8819613456726074,
            -1.588856816291809,
            2.5648858547210693,
            1.510391116142273,
            1.2409244775772095,
            -2.1070573329925537,
            -0.5923894047737122,
            1.6600204706192017,
            1.6322927474975586,
            1.2667524814605713,
            3.9410579204559326,
            -2.4063498973846436,
            -0.0031856000423431396,
            6.6997504234313965,
            -0.1356087177991867,
            2.7011265754699707,
            2.022117853164673,
            -2.2529330253601074,
            1.1363985538482666,
            -4.067896366119385,
            -1.3850785493850708,
            -0.0373062789440155,
            -0.0156378336250782,
            3.273402214050293,
            -5.978793621063232,
            -0.3823836147785187,
            5.904736518859863,
            -0.935082197189331,
            1.4116461277008057,
            -1.1628444194793701,
            -2.0304155349731445,
            -0.38604962825775146,
            1.9067744016647339,
            -1.5656547546386719,
            -1.3667850494384766,
            1.4566234350204468,
            -0.8444237112998962,
            1.829033374786377,
            0.2014225423336029,
            -5.027730941772461,
            -3.1725826263427734,
            -0.3881794512271881,
            1.5905016660690308,
            -3.527949333190918,
            4.8376078605651855,
            3.3464035987854004,
            1.4623578786849976,
            3.905107021331787,
            1.6753435134887695,
            3.9083523750305176,
            -4.998305320739746,
            -1.4210686683654785,
            -3.819916248321533,
            -1.6815729141235352,
            -2.0002880096435547,
            0.2369564324617386,
            6.864268779754639,
            -1.4459776878356934,
            1.7689728736877441,
            -1.4783579111099243,
            -1.4794055223464966,
            6.979030132293701,
            -3.2346689701080322,
            3.5172958374023438,
            -0.46029195189476013,
            -1.1632059812545776,
            1.4922090768814087,
            -2.1116793155670166,
            -1.4122705459594727,
            2.0813968181610107,
            0.7703753113746643,
            4.493664264678955,
            -2.3918955326080322,
            -2.8299338817596436,
            -0.5279929041862488,
            -2.4391207695007324,
            -4.128645896911621,
            4.542834281921387,
            1.8892593383789062,
            1.9065766334533691,
            -0.5763366222381592,
            1.3090907335281372,
            -0.3071545660495758,
            2.311570882797241,
            -3.966247081756592,
            -7.593428611755371,
            -0.9638708233833313,
            1.8572049140930176,
            -4.428222179412842,
            -2.9825680255889893,
            1.8824009895324707,
            0.21196338534355164,
            -1.0761383771896362,
            -1.324147343635559,
            0.15238748490810394,
            0.518136739730835,
            1.4059431552886963,
            -0.303749144077301,
            -1.0194518566131592,
            -1.2934160232543945,
            -3.0299971103668213,
            -2.717963695526123,
            0.6151850819587708,
            -1.673444151878357,
            -1.0385321378707886,
            1.7696183919906616,
            -0.7157703638076782,
            -0.6104464530944824,
            0.9770537614822388,
            -3.306501865386963,
            -1.7340668439865112,
            -2.113809823989868,
            -0.9475915431976318,
            2.9183149337768555,
            0.37212806940078735,
            -2.191040515899658,
            0.9453344345092773,
            5.787967681884766,
            -1.3750367164611816,
            -2.483806610107422,
            6.068289279937744,
            -3.2992959022521973,
            0.16523703932762146,
            -0.6375508904457092,
            -2.1986966133117676,
            0.13074350357055664,
            1.1861330270767212,
            -2.758765935897827,
            1.9795479774475098,
            3.3765554428100586,
            -0.46568724513053894,
            -0.5057363510131836,
            -0.2801557183265686
        ]
    },
    "authors": [
        {
            "authorId": "144270981",
            "name": "Angela Fan"
        },
        {
            "authorId": "37502184",
            "name": "Pierre Stock"
        },
        {
            "authorId": "143853801",
            "name": "Benjamin Graham"
        },
        {
            "authorId": "3024698",
            "name": "Edouard Grave"
        },
        {
            "authorId": "1731535",
            "name": "R. Gribonval"
        },
        {
            "authorId": "1681054",
            "name": "H. J\u00e9gou"
        },
        {
            "authorId": "2319608",
            "name": "Armand Joulin"
        }
    ],
    "references": [
        {
            "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
            "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "54d4ff8d536b292149a4fa017c22349cf4e54ce4",
            "title": "AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search"
        },
        {
            "paperId": "f51497f463566581874c941353dd9d80069c5b77",
            "title": "Compressive Transformers for Long-Range Sequence Modelling"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "cc34e129335325010c99af288a0b9812c7a7685e",
            "title": "Additive Powers-of-Two Quantization: A Non-uniform Discretization for Neural Networks"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout"
        },
        {
            "paperId": "973404c13a5d50469182b76f299860bf71c12fb3",
            "title": "Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks"
        },
        {
            "paperId": "740e4599b0e3113ad804cee4394c7fa7c0e96ca5",
            "title": "Extreme Language Model Compression with Optimal Subwords and Shared Projections"
        },
        {
            "paperId": "c7b9ac7b5e7eab390561284b1f47777a4c4c023a",
            "title": "Faster and Just As Accurate: A Simple Decomposition for Transformer Models"
        },
        {
            "paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97",
            "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression"
        },
        {
            "paperId": "93ad19fbc85360043988fa9ea7932b7fdf1fa948",
            "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "adce96bdd3f3093a90933f288106b67f20e094f6",
            "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks"
        },
        {
            "paperId": "830995ef17cc291c13f42dfd9f462137de1d2179",
            "title": "Augmenting Self-attention with Persistent Memory"
        },
        {
            "paperId": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25",
            "title": "A Tensorized Transformer for Language Modeling"
        },
        {
            "paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
        },
        {
            "paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13",
            "title": "Adaptive Attention Span in Transformers"
        },
        {
            "paperId": "5e19eba1e6644f7c83f607383d256deea71f87ae",
            "title": "Searching for MobileNetV3"
        },
        {
            "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
            "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"
        },
        {
            "paperId": "e20ef9c95e5580678a89448c69ded5ea17f98edb",
            "title": "Stochastic Quantization for Learning Accurate Low-Bit Deep Neural Networks"
        },
        {
            "paperId": "fea820b7d953d32069e189af2961c28fd213470b",
            "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"
        },
        {
            "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
            "title": "Cross-lingual Language Model Pretraining"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "df71a17df5350b0dbf8e5e084ae56a65cee9aaf8",
            "title": "HAQ: Hardware-Aware Automated Quantization"
        },
        {
            "paperId": "4a1004ecd34118116344633c7cdcc34493c423ee",
            "title": "Rethinking the Value of Network Pruning"
        },
        {
            "paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
            "title": "Adaptive Input Representations for Neural Language Modeling"
        },
        {
            "paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e",
            "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"
        },
        {
            "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
            "title": "Universal Transformers"
        },
        {
            "paperId": "3d8b62c060f8444907e7c975c6ae590373b51ed4",
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper"
        },
        {
            "paperId": "6e45251b16cd423f3c025f004959c6d2b26efab0",
            "title": "Accelerating Neural Transformer via an Average Attention Network"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "d5a8c9bef7020e800d3b02573a983ff1eddf285e",
            "title": "Training wide residual networks for deployment using a single bit for each weight"
        },
        {
            "paperId": "49e60f82d6ae835c56473464f67ca5c11d3e95ec",
            "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks"
        },
        {
            "paperId": "d3f6b3ce8f7b67c1e112a79b3fe9764242c655f5",
            "title": "Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks"
        },
        {
            "paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4",
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
        },
        {
            "paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294",
            "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"
        },
        {
            "paperId": "2ec7156913117949ab933f27f492d0149bc0031f",
            "title": "Learning Sparse Neural Networks through L0 Regularization"
        },
        {
            "paperId": "efbac99adf8628aae7f070e5b4388a295956f9d2",
            "title": "CondenseNet: An Efficient DenseNet Using Learned Group Convolutions"
        },
        {
            "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
            "title": "Automatic differentiation in PyTorch"
        },
        {
            "paperId": "049fd80f52c0b1fa4d532945d95a24734b62bdf3",
            "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression"
        },
        {
            "paperId": "a13878623a884c10caade384327a505c83024126",
            "title": "Model compression as constrained optimization, with application to neural nets. Part II: quantization"
        },
        {
            "paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89",
            "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
        },
        {
            "paperId": "34cc3ceae5c3f7c8acbb89f2bff63f9d452b00d5",
            "title": "Variational Dropout Sparsifies Deep Neural Networks"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "5feb32a73dd1bd9e13f84a7b3344497a5545106b",
            "title": "FastText.zip: Compressing text classification models"
        },
        {
            "paperId": "2d876ed1dd2c58058d7197b734a8e4d349b8f231",
            "title": "Quasi-Recurrent Neural Networks"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a",
            "title": "Efficient softmax approximation for GPUs"
        },
        {
            "paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
            "title": "Pruning Filters for Efficient ConvNets"
        },
        {
            "paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86",
            "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"
        },
        {
            "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
            "title": "Deep Networks with Stochastic Depth"
        },
        {
            "paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7",
            "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"
        },
        {
            "paperId": "123ae35aa7d6838c817072032ce5615bb891652d",
            "title": "BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec",
            "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"
        },
        {
            "paperId": "642d0f49b7826adcf986616f4af77e736229990f",
            "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
        },
        {
            "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "title": "Learning both Weights and Connections for Efficient Neural Network"
        },
        {
            "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "b7cf49e30355633af2db19f35189410c8515e91f",
            "title": "Deep Learning with Limited Numerical Precision"
        },
        {
            "paperId": "e7bf9803705f2eb608db1e59e5c7636a3f171916",
            "title": "Compressing Deep Convolutional Networks using Vector Quantization"
        },
        {
            "paperId": "533ee188324b833e059cb59b654e6160776d5812",
            "title": "How to Construct Deep Recurrent Neural Networks"
        },
        {
            "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
            "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
        },
        {
            "paperId": "38f35dd624cd1cf827416e31ac5e0e0454028eca",
            "title": "Regularization of Neural Networks using DropConnect"
        },
        {
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "We report accuracy and size in megabytes. * indicates distillation using BERT Large. \u2020 indicates training with data augmentation"
        },
        {
            "paperId": null,
            "title": "Classy vision"
        },
        {
            "paperId": null,
            "title": "2018) on the BooksCorpus + Wiki dataset with a LayerDrop rate of 0.2. We finetune the pre-trained models on the MNLI task"
        },
        {
            "paperId": null,
            "title": "2016) that contains 100M tokens and a vocabulary of 260k words"
        },
        {
            "paperId": "ca1f32117321aca3c205012e65f24bceebebc135",
            "title": "Model compression as constrained optimization, with application to neural nets"
        },
        {
            "paperId": null,
            "title": "We consider Transformers for language modeling, RoBERTa for pre-training sentence representations, and EfficientNet for image classification. Our models are implemented in PyTorch"
        },
        {
            "paperId": null,
            "title": "At test time, we respect sentence boundaries. We set LayerDrop to 0.2. We set Quant-Noise value to 0.05. During training time"
        },
        {
            "paperId": "fbeaa499e10e98515f7e1c4ad89165e8c0677427",
            "title": "Improving the speed of neural networks on CPUs"
        },
        {
            "paperId": "4748d22348e72e6e06c2476486afddbc76e5eca7",
            "title": "Product Quantization for Nearest Neighbor Search"
        },
        {
            "paperId": "015ca32bca81dbda1e2e432445eef798582236e1",
            "title": "Conference Paper"
        },
        {
            "paperId": null,
            "title": "The EfficientNet-B3 of Classy Vision achieves a Top-1 accuracy of 81.5%, which is slightly below than the performance of 81"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        },
        {
            "paperId": null,
            "title": "Mobile-bert: Task-agnostic compression of bert for resource limited devices"
        },
        {
            "paperId": null,
            "title": "We quantize with 256 centroids which represents a balance between size and representation capacity. The effect of the number of centroids on performance and size is shown in Figure 4 (a)"
        }
    ]
}