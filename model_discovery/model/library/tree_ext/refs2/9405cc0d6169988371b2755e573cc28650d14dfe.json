{
    "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
    "externalIds": {
        "MAG": "2955855238",
        "CorpusId": 160025533
    },
    "title": "Language Models are Unsupervised Multitask Learners",
    "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
    "venue": "",
    "year": 2019,
    "referenceCount": 75,
    "citationCount": 18198,
    "influentialCitationCount": 3024,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "38909097",
            "name": "Alec Radford"
        },
        {
            "authorId": "49387725",
            "name": "Jeff Wu"
        },
        {
            "authorId": "48422824",
            "name": "R. Child"
        },
        {
            "authorId": "150970919",
            "name": "D. Luan"
        },
        {
            "authorId": "2698777",
            "name": "Dario Amodei"
        },
        {
            "authorId": "1701686",
            "name": "I. Sutskever"
        }
    ],
    "references": [
        {
            "paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0",
            "title": "Natural Questions: A Benchmark for Question Answering Research"
        },
        {
            "paperId": "fc7c428f604d13604a1d62e8a3e1b393c730791a",
            "title": "Do We Train on Test Data? Purging CIFAR of Near-Duplicates"
        },
        {
            "paperId": "aecddd82840323e5bd43f9c73a32fed88ee93c8c",
            "title": "An Effective Approach to Unsupervised Machine Translation"
        },
        {
            "paperId": "8742db12ca9c5ac37d54436443a4a946e1ac9e42",
            "title": "The advantages and challenges of \u201cbig data\u201d: Insights from the 14 billion word iWeb corpus"
        },
        {
            "paperId": "19281b9ecdb5c07a93423a506627ab9d9b0cf039",
            "title": "Learning and Evaluating General Linguistic Intelligence"
        },
        {
            "paperId": "7a8f8109e65ed9a6048859681a825eb5655e5dd2",
            "title": "No Training Required: Exploring Random Encoders for Sentence Classification"
        },
        {
            "paperId": "b496b11fb2091678cc2d299cc778046d9a64b0a4",
            "title": "A BERT Baseline for the Natural Questions"
        },
        {
            "paperId": "e81b50f68265b84d55d03dab3c296b9fd4516857",
            "title": "TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "59851fe73f811eec5e44a12d2b39934db9174a3d",
            "title": "Strike (With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects"
        },
        {
            "paperId": "80b583413b6e4ce0ccef908612634ef9e0d06531",
            "title": "On the Evaluation of Common-Sense Reasoning in Natural Language Understanding"
        },
        {
            "paperId": "757a4e5fd848885e9c8d85450fe15c85b86d5ffb",
            "title": "Entity Tracking Improves Cloze-style Reading Comprehension"
        },
        {
            "paperId": "227458886343b86bd15adf58c769be326b4b058a",
            "title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents"
        },
        {
            "paperId": "256623ff025f36d343588bcd0b966c1fd26afcf8",
            "title": "Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling"
        },
        {
            "paperId": "a9a5d671271fff45429084e184a788f611b6f194",
            "title": "FRAGE: Frequency-Agnostic Word Representation"
        },
        {
            "paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094",
            "title": "CoQA: A Conversational Question Answering Challenge"
        },
        {
            "paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d",
            "title": "Character-Level Language Modeling with Deeper Self-Attention"
        },
        {
            "paperId": "7af89df3691d8c33aaf1858f7cc51da1bc9549a9",
            "title": "Bottom-Up Abstractive Summarization"
        },
        {
            "paperId": "9784fbf77295860b2e412137b86356d70b25e3c0",
            "title": "The Natural Language Decathlon: Multitask Learning as Question Answering"
        },
        {
            "paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535",
            "title": "A Simple Method for Commonsense Reasoning"
        },
        {
            "paperId": "f445493badf53febbaeab340a4fca98d9e4ab7f7",
            "title": "Do CIFAR-10 Classifiers Generalize to CIFAR-10?"
        },
        {
            "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
            "title": "Hierarchical Neural Story Generation"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "title": "Deep Contextualized Word Representations"
        },
        {
            "paperId": "afc2850945a871e72c245818f9bc141bd659b453",
            "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning"
        },
        {
            "paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9",
            "title": "Generating Wikipedia by Summarizing Long Sequences"
        },
        {
            "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "title": "Universal Language Model Fine-tuning for Text Classification"
        },
        {
            "paperId": "a1c922be467d1c0c64b963e65dae41778b81b2a0",
            "title": "Deep Learning Scaling is Predictable, Empirically"
        },
        {
            "paperId": "e3d772986d176057aca2f5e3eb783da53b559134",
            "title": "Unsupervised Machine Translation Using Monolingual Corpora Only"
        },
        {
            "paperId": "c2a7afbb5609a723f8eea91bfde4b02579b048d6",
            "title": "Unsupervised Neural Machine Translation"
        },
        {
            "paperId": "562c09c112df56c5696c010d90a815d6018a86c8",
            "title": "Word Translation Without Parallel Data"
        },
        {
            "paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7",
            "title": "Learned in Translation: Contextualized Word Vectors"
        },
        {
            "paperId": "ffb949d3493c3b2f3c9acf9c75cb03938933ddf0",
            "title": "Adversarial Examples for Evaluating Reading Comprehension Systems"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
            "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data"
        },
        {
            "paperId": "664ec878de4b7170712baae4a7821fc2602bba25",
            "title": "Learning to Generate Reviews and Discovering Sentiment"
        },
        {
            "paperId": "1ecec941252788e09531a8f2e57a2e7af03108e2",
            "title": "Story Cloze Task: UW NLP System"
        },
        {
            "paperId": "668db48c6a79826456341680ee1175dfc4cced71",
            "title": "Get To The Point: Summarization with Pointer-Generator Networks"
        },
        {
            "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
        },
        {
            "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
            "title": "Overcoming catastrophic forgetting in neural networks"
        },
        {
            "paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc",
            "title": "Unsupervised Pretraining for Sequence to Sequence Learning"
        },
        {
            "paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
            "title": "Improving Neural Language Models with a Continuous Cache"
        },
        {
            "paperId": "6eec608f266de95eb817e9a6086641abc3c91e5f",
            "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a",
            "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"
        },
        {
            "paperId": "3bbf2ee642ed311e500017def1f54df453a935c1",
            "title": "Dialog-based Language Learning"
        },
        {
            "paperId": "5f0625c30014c12f333eb518268647673d18f9f1",
            "title": "What can the brain teach us about building artificial intelligence?"
        },
        {
            "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "title": "Identity Mappings in Deep Residual Networks"
        },
        {
            "paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa",
            "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"
        },
        {
            "paperId": "26e743d5bd465f49b9538deaf116c15e61b7951f",
            "title": "Learning Distributed Representations of Sentences from Unlabelled Data"
        },
        {
            "paperId": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "title": "Exploring the Limits of Language Modeling"
        },
        {
            "paperId": "13497bd108d4412d02050e646235f456568cf822",
            "title": "Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin"
        },
        {
            "paperId": "4dabd6182ce2681c758f654561d351739e8df7bf",
            "title": "Multilingual Language Processing From Bytes"
        },
        {
            "paperId": "08969cbeb4ac3fe0b20754cbcf221f8031fc682f",
            "title": "Towards Principled Unsupervised Learning"
        },
        {
            "paperId": "35b91b365ceb016fb3e022577cec96fb9b445dc5",
            "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations"
        },
        {
            "paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
            "title": "Semi-supervised Sequence Learning"
        },
        {
            "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
            "title": "Neural Machine Translation of Rare Words with Subword Units"
        },
        {
            "paperId": "86311b182786bfde19446f6ded0854de973d4060",
            "title": "A Neural Conversational Model"
        },
        {
            "paperId": "40be3888daa5c2e5af4d36ae22f690bcc8caf600",
            "title": "Visualizing and Understanding Recurrent Networks"
        },
        {
            "paperId": "f4c018bcc8ea707b83247866bdc8ccb87cd9f5da",
            "title": "Neural Word Embedding as Implicit Matrix Factorization"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "5d833331b0e22ff359db05c62a8bca18c4f04b68",
            "title": "One billion word benchmark for measuring progress in statistical language modeling"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "9cd610dd495f2dbbf4a65f79887772435e5da260",
            "title": "Content extraction using diverse feature sets"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f",
            "title": "The Winograd Schema Challenge"
        },
        {
            "paperId": "bc1022b031dc6c7019696492e8116598097a8c12",
            "title": "Natural Language Processing (Almost) from Scratch"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49",
            "title": "Multitask Learning"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "title": "Interpolated estimation of Markov source parameters from sparse data"
        },
        {
            "paperId": null,
            "title": "Preliminary code for downloading and using the small model is"
        },
        {
            "paperId": null,
            "title": "Two years later, he was promoted to second lieutenant"
        }
    ]
}