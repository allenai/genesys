{
    "paperId": "1da8eabfd02436dda5f3131fe404dc840a04e6cc",
    "externalIds": {
        "DBLP": "conf/iclr/Chen17",
        "ArXiv": "1707.02377",
        "MAG": "2950107615",
        "CorpusId": 8328889
    },
    "title": "Efficient Vector Representation for Documents through Corruption",
    "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.",
    "venue": "International Conference on Learning Representations",
    "year": 2017,
    "referenceCount": 38,
    "citationCount": 112,
    "influentialCitationCount": 18,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1743082",
            "name": "Minmin Chen"
        }
    ],
    "references": [
        {
            "paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
            "title": "Semi-supervised Sequence Learning"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "04ebd82c48a580476fc5acad61b8ee036f92f1f5",
            "title": "Document Embedding with Paragraph Vectors"
        },
        {
            "paperId": "66021a920001bc3e6258bffe7076d647614147b7",
            "title": "From Word Embeddings To Document Distances"
        },
        {
            "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "title": "Skip-Thought Vectors"
        },
        {
            "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
            "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "f1b0df6b2977d28e55df82576b108e4f5d87e044",
            "title": "Text Understanding from Scratch"
        },
        {
            "paperId": "54e840c8973db7665a6388b2d992ef08ed7f0260",
            "title": "Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews"
        },
        {
            "paperId": "11ec56898a9e7f401a2affe776b5297bd4e25025",
            "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment"
        },
        {
            "paperId": "dcb75b83cfeaddc6e0a596b9d9761c63935ef013",
            "title": "Marginalized Denoising Auto-encoders for Nonlinear Representations"
        },
        {
            "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
            "title": "Distributed Representations of Sentences and Documents"
        },
        {
            "paperId": "0ca7d208ff8d81377e0eaa9723820aeae7a7322d",
            "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "d124a098cdc6f99b9a152fcf8afa9327dac583be",
            "title": "Dropout Training as Adaptive Regularization"
        },
        {
            "paperId": "3c20df69865df6a627cc45c524869ccc0297048f",
            "title": "Learning with Marginalized Corrupted Features"
        },
        {
            "paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
            "title": "Linguistic Regularities in Continuous Space Word Representations"
        },
        {
            "paperId": "607cca37c1429b7380df35b3f761ae1499aa84ab",
            "title": "Multi-Step Regression Learning for Compositional Distributional Semantics"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa",
            "title": "Improving Word Representations via Global Context and Multiple Word Prototypes"
        },
        {
            "paperId": "5e9fa46f231c59e6573f9a116f77f53703347659",
            "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification"
        },
        {
            "paperId": "8db26a22942404bd435909a16bb3a50cd67b4318",
            "title": "Marginalized Denoising Autoencoders for Domain Adaptation"
        },
        {
            "paperId": "bd5fc28c7356915ec71abafbe86b7596c60720aa",
            "title": "The conference paper"
        },
        {
            "paperId": "2063745d08868c928455f422202b72146a1960fb",
            "title": "Compositional Matrix-Space Models for Sentiment Analysis"
        },
        {
            "paperId": "6f4065f0cc99a0839b0248ffb4457e5f0277b30d",
            "title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach"
        },
        {
            "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
            "title": "Learning Word Vectors for Sentiment Analysis"
        },
        {
            "paperId": "b71ead55fd6520dd4604de66cbb732bc2b7a6070",
            "title": "Language Modeling for Information Retrieval"
        },
        {
            "paperId": "745d86adca56ec50761591733e157f84cfb19671",
            "title": "Composition in Distributional Models of Semantics"
        },
        {
            "paperId": "8d9da542a6aa92fece5dfb7eecfb44ae7de0f664",
            "title": "Estimating Linear Models for Compositional Distributional Semantics"
        },
        {
            "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
            "title": "Extracting and composing robust features with denoising autoencoders"
        },
        {
            "paperId": "268a4f8da15a42f3e0e71691f760ff5edbf9cec8",
            "title": "LIBLINEAR: A Library for Large Linear Classification"
        },
        {
            "paperId": "164125a65d42a791d2c1e108559344caef96d08b",
            "title": "Indexing by Latent Semantic Analysis"
        },
        {
            "paperId": "e50a316f97c9a405aa000d883a633bd5707f1a34",
            "title": "Term-Weighting Approaches in Automatic Text Retrieval"
        },
        {
            "paperId": "0c2563caba6bcdb113817d560ce9492467e45873",
            "title": "Under review as a conference paper at ICLR 2020 many domain adaptation methods"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "4574d77fff19e093782178595a8988a7f3aa1969",
            "title": "Latent Dirichlet Allocation"
        },
        {
            "paperId": "1c46943103bd7b7a2c7be86859995a4144d1938b",
            "title": "Visualizing Data using t-SNE"
        }
    ]
}