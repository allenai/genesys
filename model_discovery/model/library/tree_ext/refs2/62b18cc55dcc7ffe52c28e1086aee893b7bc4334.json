{
    "paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
    "externalIds": {
        "DBLP": "conf/icml/YangWSPK24",
        "ArXiv": "2312.06635",
        "DOI": "10.48550/arXiv.2312.06635",
        "CorpusId": 266162792
    },
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "abstract": "Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 117,
    "citationCount": 50,
    "influentialCitationCount": 11,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.228206157684326,
            2.448209285736084,
            2.5423474311828613,
            2.4612479209899902,
            1.144890546798706,
            1.5775953531265259,
            2.0912322998046875,
            -0.9567265510559082,
            1.7738075256347656,
            -0.2020890712738037,
            0.016296744346618652,
            6.8375043869018555,
            1.40474534034729,
            1.1507914066314697,
            -4.586499214172363,
            -3.480821371078491,
            -2.6390862464904785,
            3.5511422157287598,
            5.570123672485352,
            2.4144840240478516,
            -0.7990830540657043,
            0.9397273063659668,
            -2.9778270721435547,
            2.293226480484009,
            -0.6752892732620239,
            -0.9262641668319702,
            3.9758267402648926,
            1.3760830163955688,
            -5.015871524810791,
            1.8228005170822144,
            -0.8024842739105225,
            -2.026350975036621,
            1.8961999416351318,
            -3.4705965518951416,
            3.599048137664795,
            -2.190880298614502,
            -5.768057823181152,
            6.395651817321777,
            -3.9905991554260254,
            -2.496150016784668,
            -0.237568199634552,
            -0.622846245765686,
            -2.0678281784057617,
            -1.802266001701355,
            -1.4378520250320435,
            0.6866534352302551,
            5.1374382972717285,
            2.9392404556274414,
            -4.02363395690918,
            2.0581612586975098,
            3.9581732749938965,
            -0.4999607801437378,
            -1.713303565979004,
            2.1936116218566895,
            -1.308384895324707,
            -3.338348865509033,
            1.8488543033599854,
            0.22602799534797668,
            1.3828966617584229,
            -0.4257332682609558,
            3.871539354324341,
            2.6166207790374756,
            1.326900601387024,
            0.5709843635559082,
            3.056990623474121,
            -3.5237016677856445,
            1.2361433506011963,
            2.397430181503296,
            2.1723954677581787,
            1.362349271774292,
            -0.11550131440162659,
            -5.143781661987305,
            -0.7874810695648193,
            -0.7774276733398438,
            -0.22137552499771118,
            -0.7168655395507812,
            -1.6111502647399902,
            -3.0695083141326904,
            0.7556023597717285,
            -4.278782844543457,
            -1.9775111675262451,
            2.4981446266174316,
            -0.4337015748023987,
            0.28099536895751953,
            5.516365051269531,
            -4.22481632232666,
            -4.829169273376465,
            2.6492085456848145,
            2.644866943359375,
            -3.6868162155151367,
            1.244154453277588,
            1.138582706451416,
            -1.9520690441131592,
            1.0172386169433594,
            -1.67288339138031,
            0.6198455095291138,
            1.0870366096496582,
            -0.591259777545929,
            -5.003489017486572,
            2.576582908630371,
            -0.0865166187286377,
            -0.4105089604854584,
            -1.2378828525543213,
            0.19332221150398254,
            2.492548942565918,
            -2.7110707759857178,
            -4.435121059417725,
            3.1947622299194336,
            2.002922296524048,
            -0.32147663831710815,
            -3.087742567062378,
            2.1150033473968506,
            -1.5336873531341553,
            0.6432248950004578,
            -0.8624348044395447,
            -7.354734897613525,
            0.6561111211776733,
            2.439232110977173,
            -1.1155645847320557,
            5.232388973236084,
            -1.728318452835083,
            0.006540566682815552,
            -3.6045546531677246,
            -1.5005910396575928,
            -0.7711483240127563,
            2.028007745742798,
            -2.4962007999420166,
            1.6210017204284668,
            1.992291808128357,
            -3.231335163116455,
            -1.1416149139404297,
            -3.4751646518707275,
            1.6936203241348267,
            -3.7047958374023438,
            5.314663887023926,
            -0.9775625467300415,
            -5.319949150085449,
            0.1597558856010437,
            -0.8303061127662659,
            -0.3342858552932739,
            0.7046199440956116,
            1.5183974504470825,
            2.4797635078430176,
            1.7245728969573975,
            4.035308837890625,
            3.7525947093963623,
            -2.4950432777404785,
            1.072052001953125,
            0.6875728368759155,
            6.773688316345215,
            4.1444926261901855,
            -3.859614849090576,
            3.6225147247314453,
            0.8030025959014893,
            -0.2077905535697937,
            3.284470558166504,
            -3.207664728164673,
            3.8169193267822266,
            -4.384156703948975,
            0.31903305649757385,
            1.9542031288146973,
            1.0355262756347656,
            -10.11801815032959,
            -0.6999767422676086,
            4.659567832946777,
            -4.309551239013672,
            -1.6724058389663696,
            0.9677670001983643,
            -1.256096363067627,
            3.2081265449523926,
            0.33915624022483826,
            2.4170751571655273,
            1.3548076152801514,
            2.030060052871704,
            4.893329620361328,
            4.585845947265625,
            2.576599597930908,
            -2.946857213973999,
            -2.855177402496338,
            -1.2127101421356201,
            -1.602868676185608,
            2.704284906387329,
            -4.738534927368164,
            3.3011255264282227,
            -4.078165054321289,
            -5.172833442687988,
            -4.078702449798584,
            -2.4447810649871826,
            -4.94020938873291,
            -1.2071212530136108,
            -0.401444673538208,
            -0.14687207341194153,
            7.370955467224121,
            4.555842399597168,
            3.6412739753723145,
            -1.0258376598358154,
            1.4609417915344238,
            1.6166071891784668,
            -5.5276780128479,
            -2.0330867767333984,
            4.014339447021484,
            -0.3453108072280884,
            -3.049860954284668,
            -0.12224572896957397,
            2.886484384536743,
            0.5968756079673767,
            -3.1345057487487793,
            3.9814836978912354,
            1.6410014629364014,
            1.367478847503662,
            0.5422515869140625,
            -2.6521410942077637,
            -1.4453961849212646,
            -0.3343654274940491,
            -2.9586892127990723,
            -2.734808921813965,
            -6.880012035369873,
            3.915752649307251,
            4.865586280822754,
            -0.31668978929519653,
            3.247581958770752,
            -1.18179190158844,
            0.9709705114364624,
            -6.488886833190918,
            0.3150816857814789,
            -2.8792569637298584,
            1.8375608921051025,
            -2.50576114654541,
            0.8091906309127808,
            2.756230115890503,
            -2.150233268737793,
            -3.223538398742676,
            1.0341854095458984,
            -1.5684106349945068,
            -5.574199199676514,
            -5.935054779052734,
            -6.522757530212402,
            -1.240381121635437,
            0.8937088847160339,
            -4.149057388305664,
            5.021053314208984,
            4.283797264099121,
            2.4608218669891357,
            3.0074453353881836,
            -0.31419217586517334,
            -1.461437702178955,
            -1.2132039070129395,
            1.4878149032592773,
            2.9506280422210693,
            -0.028512388467788696,
            0.15329241752624512,
            -1.7240614891052246,
            4.570333480834961,
            -0.6027355790138245,
            0.7692877054214478,
            0.7679082751274109,
            2.560318946838379,
            -2.602485418319702,
            -0.8084469437599182,
            1.3010172843933105,
            -4.020736217498779,
            4.201318264007568,
            4.631674766540527,
            5.568729400634766,
            -2.1121912002563477,
            -0.8039034605026245,
            0.7683631777763367,
            0.6269165277481079,
            -1.0247914791107178,
            2.2175025939941406,
            1.9572079181671143,
            2.716503143310547,
            -1.4873734712600708,
            -1.5325320959091187,
            2.03110408782959,
            -5.4000396728515625,
            -4.350980758666992,
            0.5839404463768005,
            1.0858161449432373,
            3.9848568439483643,
            1.3922100067138672,
            -1.6077861785888672,
            -3.605931282043457,
            2.716919422149658,
            -0.32894963026046753,
            -0.6901322603225708,
            -1.760915994644165,
            -1.9350241422653198,
            2.3148536682128906,
            0.4833531379699707,
            -5.617222309112549,
            2.220470905303955,
            -4.880331516265869,
            -1.3146600723266602,
            -1.952360987663269,
            3.035121440887451,
            3.7429287433624268,
            -1.1595566272735596,
            0.46435701847076416,
            -1.6307787895202637,
            -3.0650291442871094,
            1.9505469799041748,
            2.596583843231201,
            1.6362833976745605,
            0.5820796489715576,
            4.045452117919922,
            3.419717788696289,
            -4.842084884643555,
            -2.6480445861816406,
            -6.352895736694336,
            -2.257570266723633,
            -1.117323875427246,
            3.721668243408203,
            -1.8233870267868042,
            -1.1252543926239014,
            -1.99442458152771,
            5.4073333740234375,
            1.7650134563446045,
            -1.1313879489898682,
            0.9013766050338745,
            -2.153122663497925,
            0.4979168772697449,
            -6.764201641082764,
            -3.717643976211548,
            0.5313739776611328,
            -4.519586086273193,
            1.3679091930389404,
            1.1516047716140747,
            -3.5440332889556885,
            4.394989490509033,
            1.0419186353683472,
            5.089959144592285,
            4.59257698059082,
            4.113266468048096,
            -0.9888085722923279,
            -2.7697010040283203,
            -2.8561928272247314,
            0.3090035319328308,
            -2.087143898010254,
            1.5864880084991455,
            -3.0500423908233643,
            7.545071601867676,
            2.288267135620117,
            0.2689511179924011,
            1.397731065750122,
            0.022207260131835938,
            -0.16134189069271088,
            -3.1758062839508057,
            -1.47005295753479,
            0.2710038125514984,
            0.705825924873352,
            0.7383766174316406,
            1.235621452331543,
            -5.529874801635742,
            2.2056682109832764,
            4.907754421234131,
            0.2843990921974182,
            1.9268434047698975,
            2.280604839324951,
            -0.975440263748169,
            -1.2927660942077637,
            -3.0477454662323,
            2.4822182655334473,
            -3.351466178894043,
            -0.7972599267959595,
            -1.3485714197158813,
            11.778790473937988,
            -1.3157789707183838,
            -2.217003107070923,
            -4.4668779373168945,
            -3.5913820266723633,
            1.3080737590789795,
            -2.850991725921631,
            3.056349992752075,
            0.39302515983581543,
            -0.18662047386169434,
            1.154397964477539,
            -2.0040361881256104,
            -1.4038546085357666,
            0.6701180934906006,
            -3.3979294300079346,
            3.059234857559204,
            2.153003454208374,
            3.5283255577087402,
            -2.87300705909729,
            -0.4426693916320801,
            -0.5451180934906006,
            1.8415563106536865,
            0.23675081133842468,
            -0.3864060640335083,
            -1.9592572450637817,
            1.210780382156372,
            4.175309181213379,
            2.553689956665039,
            -2.0418083667755127,
            -4.400681495666504,
            -2.841606616973877,
            -6.1714067459106445,
            -0.43690377473831177,
            3.0490574836730957,
            -1.5921344757080078,
            0.3226064145565033,
            7.3259663581848145,
            5.0714945793151855,
            -3.665121555328369,
            0.20132923126220703,
            3.8765454292297363,
            1.8963344097137451,
            2.6682240962982178,
            -0.27873069047927856,
            -4.791215419769287,
            -1.624305009841919,
            -1.7355893850326538,
            -0.05533352494239807,
            -1.0204856395721436,
            -1.6757597923278809,
            0.6594870090484619,
            0.29484885931015015,
            6.643593788146973,
            -0.12039646506309509,
            -1.2742362022399902,
            1.1806988716125488,
            7.298460006713867,
            2.7197651863098145,
            -1.196080207824707,
            1.5738673210144043,
            3.0603485107421875,
            3.3128814697265625,
            -2.375905752182007,
            -1.4759306907653809,
            -1.5570335388183594,
            2.5464658737182617,
            -3.603663206100464,
            1.0817387104034424,
            0.04361164569854736,
            2.2736945152282715,
            0.5669481754302979,
            4.106029510498047,
            1.0667650699615479,
            2.318643808364868,
            2.0633883476257324,
            5.088497161865234,
            -2.174018144607544,
            2.6477935314178467,
            -0.34871795773506165,
            1.1267188787460327,
            5.092339515686035,
            -0.9455090761184692,
            -1.692483901977539,
            -2.9083900451660156,
            2.4835429191589355,
            -4.673809051513672,
            -3.646315813064575,
            0.07609507441520691,
            2.170085906982422,
            -0.9147341847419739,
            -4.210052490234375,
            -2.9878904819488525,
            0.8929350972175598,
            1.436133861541748,
            -5.197052955627441,
            1.432125449180603,
            -0.769675612449646,
            3.0272653102874756,
            0.6466506719589233,
            6.287097930908203,
            -2.4691452980041504,
            -4.840643882751465,
            -2.017296314239502,
            2.3953893184661865,
            3.4483535289764404,
            2.511664867401123,
            -2.0346405506134033,
            -1.042269229888916,
            1.8432064056396484,
            -0.5276488661766052,
            2.303983688354492,
            1.9058868885040283,
            2.4704766273498535,
            -3.724848747253418,
            -2.9957032203674316,
            1.635606288909912,
            5.077101707458496,
            -2.640305995941162,
            -5.306241035461426,
            5.6403069496154785,
            1.9090899229049683,
            3.266383647918701,
            -1.0679457187652588,
            3.315683126449585,
            -1.0957707166671753,
            -3.318056106567383,
            4.5114054679870605,
            0.5439901351928711,
            1.5353511571884155,
            1.8411967754364014,
            -2.3455042839050293,
            -0.8793646097183228,
            2.059359073638916,
            -1.422759771347046,
            -0.750169575214386,
            -3.2271313667297363,
            -1.122191071510315,
            -1.5393688678741455,
            -3.725266933441162,
            6.1569085121154785,
            3.2461342811584473,
            3.821793794631958,
            -0.06578832864761353,
            -0.7574806809425354,
            0.5364482402801514,
            3.0910556316375732,
            -3.1825742721557617,
            4.064260482788086,
            1.0906760692596436,
            2.7474770545959473,
            5.887713432312012,
            1.4731698036193848,
            3.3190298080444336,
            2.3255844116210938,
            -3.223952293395996,
            1.4921199083328247,
            -4.3590288162231445,
            3.4418299198150635,
            3.2420597076416016,
            0.22295689582824707,
            -3.210476875305176,
            3.042271137237549,
            3.567878007888794,
            5.327751159667969,
            6.659093379974365,
            7.560806751251221,
            1.2833620309829712,
            -2.8688578605651855,
            -0.759553849697113,
            -3.874631643295288,
            3.876586437225342,
            6.011279106140137,
            -2.3310000896453857,
            -1.3985199928283691,
            -0.1615837812423706,
            2.9337518215179443,
            -1.137825846672058,
            -0.2509903311729431,
            -0.23690861463546753,
            2.4178237915039062,
            -2.9235222339630127,
            -0.04307691752910614,
            -2.889329433441162,
            0.8674163222312927,
            1.4862935543060303,
            2.9828250408172607,
            0.7936415672302246,
            -5.480592727661133,
            -2.2056074142456055,
            -0.5730524063110352,
            -4.630197525024414,
            -2.3557300567626953,
            -2.1853280067443848,
            4.70050048828125,
            1.4843883514404297,
            -0.5661269426345825,
            -1.5422136783599854,
            4.073858261108398,
            -7.236629486083984,
            -1.9777582883834839,
            0.2992270290851593,
            2.4271280765533447,
            -2.1076321601867676,
            -2.3396456241607666,
            -0.6450333595275879,
            -4.027393817901611,
            2.667287826538086,
            1.001155972480774,
            2.228785753250122,
            5.313461780548096,
            2.0569074153900146,
            5.0185747146606445,
            -3.1316604614257812,
            2.0002427101135254,
            -1.2365069389343262,
            -1.8819262981414795,
            -4.2516937255859375,
            -5.507284164428711,
            2.2277183532714844,
            -6.072881698608398,
            -5.949127197265625,
            2.383849859237671,
            -3.156909942626953,
            -1.621461033821106,
            1.6879345178604126,
            -5.824551105499268,
            -0.17131072282791138,
            -4.4555792808532715,
            -1.31276535987854,
            -1.1746981143951416,
            2.7813048362731934,
            -0.9409471750259399,
            -2.748549461364746,
            2.6814799308776855,
            1.1984820365905762,
            3.2623846530914307,
            2.3296079635620117,
            1.0907047986984253,
            -5.294435024261475,
            -1.994685411453247,
            2.845252513885498,
            1.7689728736877441,
            0.32552021741867065,
            2.2381956577301025,
            0.9835067987442017,
            -0.6681519746780396,
            14.075153350830078,
            1.801405906677246,
            -0.922824501991272,
            -3.759448528289795,
            -3.4401021003723145,
            -5.156139850616455,
            -3.8848419189453125,
            0.4803454279899597,
            0.33653080463409424,
            1.1264967918395996,
            -4.254426956176758,
            -0.026124894618988037,
            0.9844031929969788,
            -0.5754172801971436,
            0.9156325459480286,
            0.7787302136421204,
            -2.2343697547912598,
            2.7165417671203613,
            -1.7600090503692627,
            -2.073827028274536,
            -0.19816040992736816,
            2.787578582763672,
            -1.2419514656066895,
            1.1564900875091553,
            0.5875769853591919,
            4.913054466247559,
            0.514011800289154,
            -0.5878855586051941,
            -4.791015625,
            3.2694852352142334,
            1.0218284130096436,
            2.8179688453674316,
            0.682770311832428,
            0.5134572982788086,
            -3.2712230682373047,
            3.795064926147461,
            2.0427262783050537,
            0.008909672498703003,
            4.411235809326172,
            2.057116985321045,
            -1.6754107475280762,
            0.79436856508255,
            -3.2316606044769287,
            -2.7733311653137207,
            -2.722456216812134,
            1.060325264930725,
            -0.4517296850681305,
            -4.0584611892700195,
            -2.7227842807769775,
            1.3917666673660278,
            0.007514983415603638,
            -1.141054630279541,
            -2.1863949298858643,
            1.627058982849121,
            1.4768164157867432,
            6.8377838134765625,
            2.18135929107666,
            0.8929460048675537,
            2.867464542388916,
            -3.241542339324951,
            0.6007309556007385,
            0.8668215870857239,
            -1.7463874816894531,
            -2.3325753211975098,
            -1.835261583328247,
            0.7513969540596008,
            -3.2372512817382812,
            2.8838424682617188,
            0.24443086981773376,
            1.4032970666885376,
            2.6077797412872314,
            1.3559026718139648,
            1.3584849834442139,
            -2.0295181274414062,
            -1.3158631324768066,
            -2.9271767139434814,
            2.843233585357666,
            -0.6024088859558105,
            -2.1474690437316895,
            7.598849773406982,
            -5.579721450805664,
            1.3742727041244507,
            -2.2477595806121826,
            -1.7295619249343872,
            6.131889343261719,
            -2.58941650390625,
            4.555422782897949,
            1.1299974918365479,
            -5.740535736083984,
            5.451951026916504,
            -0.9325675964355469,
            -0.5766513347625732,
            2.4524428844451904,
            5.104218482971191,
            5.382709503173828,
            -2.9839682579040527,
            -2.2047295570373535,
            -0.37138646841049194,
            -2.9349300861358643,
            -4.357066631317139,
            4.310945510864258,
            4.788565635681152,
            -0.12280283123254776,
            -0.42844027280807495,
            -1.578919768333435,
            -0.6314124464988708,
            -1.3443419933319092,
            -3.7131431102752686,
            -2.629991054534912,
            -2.6105709075927734,
            4.012582778930664,
            -5.86024284362793,
            -0.9957153797149658,
            -1.899922490119934,
            1.5573513507843018,
            -1.6891067028045654,
            -0.28039053082466125,
            7.018924713134766,
            2.365849018096924,
            3.2310733795166016,
            0.8503206372261047,
            0.43395575881004333,
            -1.6368085145950317,
            -2.7299318313598633,
            -2.384190082550049,
            1.511606216430664,
            1.6295669078826904,
            -1.7137351036071777,
            3.90358567237854,
            -0.14262840151786804,
            1.6446493864059448,
            -6.611485481262207,
            -5.022509574890137,
            -1.1319079399108887,
            -1.7824726104736328,
            -2.751213550567627,
            0.5115800499916077,
            1.578297734260559,
            -3.3130481243133545,
            -0.22296957671642303,
            4.457196235656738,
            -3.073894739151001,
            -3.356853485107422,
            9.278043746948242,
            -2.702420234680176,
            0.6716970801353455,
            -0.2201330065727234,
            -1.5221912860870361,
            -0.5739750862121582,
            1.047818660736084,
            1.6967368125915527,
            -1.2016390562057495,
            4.540256500244141,
            0.08080291748046875,
            -0.04662330448627472,
            -2.3397581577301025
        ]
    },
    "authors": [
        {
            "authorId": "50591392",
            "name": "Songlin Yang"
        },
        {
            "authorId": "2257409822",
            "name": "Bailin Wang"
        },
        {
            "authorId": "2273540596",
            "name": "Yikang Shen"
        },
        {
            "authorId": "1819152",
            "name": "Rameswar Panda"
        },
        {
            "authorId": "2261394948",
            "name": "Yoon Kim"
        }
    ],
    "references": [
        {
            "paperId": "1d4c48335d841014d0145256c3c4e7f6c426b8fb",
            "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models"
        },
        {
            "paperId": "98372f2e164a4ae44c390a72a39bd6d7675cae89",
            "title": "xLSTM: Extended Long Short-Term Memory"
        },
        {
            "paperId": "46732358e98ce6be0c564ae11f71d556a64b4c35",
            "title": "HGRN2: Gated Linear RNNs with State Expansion"
        },
        {
            "paperId": "157ed5647da39a7f5d33a84a90414b2a9e97e301",
            "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence"
        },
        {
            "paperId": "cde66097f4123a62bf3e28d48c764648e8c69f72",
            "title": "Simple linear attention language models balance the recall-throughput tradeoff"
        },
        {
            "paperId": "f4a0c4154203808f362e4678f3741b3d317fdc82",
            "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry"
        },
        {
            "paperId": "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
            "title": "Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces"
        },
        {
            "paperId": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
            "title": "MambaByte: Token-free Selective State Space Model"
        },
        {
            "paperId": "5358b0e98934f1bbe8f6123a529bbb91dd36d662",
            "title": "SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation"
        },
        {
            "paperId": "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
            "title": "VMamba: Visual State Space Model"
        },
        {
            "paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444",
            "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"
        },
        {
            "paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c",
            "title": "Transformers are Multi-State RNNs"
        },
        {
            "paperId": "c1a04730c83967d0bb904b02263b17893cb50bad",
            "title": "U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation"
        },
        {
            "paperId": "7294c426b8a95975ca932eaf8f700acdd3d950b2",
            "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models"
        },
        {
            "paperId": "1be73fa3e856c33d0aed1d9e46693523e7fa3c60",
            "title": "Zoology: Measuring and Improving Recall in Efficient Language Models"
        },
        {
            "paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082",
            "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
        },
        {
            "paperId": "31245344a6eb6cd897a71928dc4b174ab75e4070",
            "title": "Diffusion Models Without Attention"
        },
        {
            "paperId": "e10ee483325f590b1e139dfafdb03edeb2e1766a",
            "title": "Linear Log-Normal Attention with Unbiased Concentration"
        },
        {
            "paperId": "ade22704be8a0fc3730d320cc7934b2ccbcd97e4",
            "title": "Striped Attention: Faster Ring Attention for Causal Transformers"
        },
        {
            "paperId": "5c104f905fcacf390270f619f232a2ba4eb873f2",
            "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores"
        },
        {
            "paperId": "434d751d355d7a7c20efa570e785c76286245e77",
            "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"
        },
        {
            "paperId": "813987d484a9eea03e95e677707fd011947a4154",
            "title": "First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models"
        },
        {
            "paperId": "d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797",
            "title": "GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling"
        },
        {
            "paperId": "cb0ac335adda4ceef9987cbcbca9129e71c37f0a",
            "title": "Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions"
        },
        {
            "paperId": "c67052a650aa5c8d9138f90972372cdd858a87eb",
            "title": "Efficient Parallelization of a Ubiquitous Sequential Computation"
        },
        {
            "paperId": "09dbfd54e5c8bdcd99b72ed7f8c45428f1e69541",
            "title": "Recurrent Linear Transformers"
        },
        {
            "paperId": "c85268696fe1435605ae66a18653cfdcf8153753",
            "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"
        },
        {
            "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
            "title": "Mistral 7B"
        },
        {
            "paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"
        },
        {
            "paperId": "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b",
            "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization"
        },
        {
            "paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0",
            "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
        },
        {
            "paperId": "240103933ffe3dac2179cc160a2bd91299357a53",
            "title": "Retentive Network: A Successor to Transformer for Large Language Models"
        },
        {
            "paperId": "d2d0371158803df93a249c9f7237ffd79b875816",
            "title": "Sparse Modular Activation for Efficient Sequence Modeling"
        },
        {
            "paperId": "026b3396a63ed5772329708b7580d633bb86bec9",
            "title": "RWKV: Reinventing RNNs for the Transformer Era"
        },
        {
            "paperId": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
            "title": "Toeplitz Neural Network for Sequence Modeling"
        },
        {
            "paperId": "fc7f626f37f3cf7751523a99bc3f0fac10ec89cf",
            "title": "The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge"
        },
        {
            "paperId": "2ef1c2438c3a4552db9e7080e15d8c51bc071f58",
            "title": "Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
            "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"
        },
        {
            "paperId": "54155c2977a977bf129849455dcae3a2b79b3f41",
            "title": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling"
        },
        {
            "paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421",
            "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
        },
        {
            "paperId": "a128b1c47e6842605fb95bceae930d2135fc38fc",
            "title": "Pretraining Without Attention"
        },
        {
            "paperId": "9575afb5702bc33d7df14c48feeee5901ea00369",
            "title": "A Length-Extrapolatable Transformer"
        },
        {
            "paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
            "title": "The Devil in Linear Transformer"
        },
        {
            "paperId": "240300b1da360f22bf0b82c6817eacebba6deed4",
            "title": "What Makes Convolutional Models Great on Long Sequence Modeling?"
        },
        {
            "paperId": "f6d8beb02771791d628f7e0773d8906261ce707c",
            "title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights"
        },
        {
            "paperId": "b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc",
            "title": "Liquid Structural State-Space Models"
        },
        {
            "paperId": "70e91e16eb321067d9402710e14a40cf28311f73",
            "title": "Mega: Moving Average Equipped Gated Attention"
        },
        {
            "paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
            "title": "Simplified State Space Layers for Sequence Modeling"
        },
        {
            "paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
            "title": "Long Range Language Modeling via Gated State Spaces"
        },
        {
            "paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
        },
        {
            "paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
            "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"
        },
        {
            "paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87",
            "title": "Diagonal State Spaces are as Effective as Structured State Spaces"
        },
        {
            "paperId": "e2ee883fca5f8f32a1dfa2dc06c742d57f2c38b9",
            "title": "Linearizing Transformer with Key-Value Memory"
        },
        {
            "paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c",
            "title": "Transformer Quality in Linear Time"
        },
        {
            "paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
            "title": "Efficiently Modeling Long Sequences with Structured State Spaces"
        },
        {
            "paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
            "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"
        },
        {
            "paperId": "e0cbbca02b332f398c6639b3bea0613f79166220",
            "title": "ABC: Attention with Bounded-memory Control"
        },
        {
            "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
        },
        {
            "paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
            "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"
        },
        {
            "paperId": "16e623059ffccab60f4c35be028a2d4f10933515",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective"
        },
        {
            "paperId": "379c9fe562c47a65a2c9864c48d75fc03149c8d0",
            "title": "Accelerating non-power-of-2 size Fourier transforms with GPU Tensor Cores"
        },
        {
            "paperId": "880a9714271da2af2bff4463d06c37f04f39cae7",
            "title": "tcFFT: Accelerating Half-Precision FFT through Tensor Cores"
        },
        {
            "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
        },
        {
            "paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20",
            "title": "Finetuning Pretrained Transformers into RNNs"
        },
        {
            "paperId": "9ed25f101f19ea735ca300848948ed64064b97ca",
            "title": "Random Feature Attention"
        },
        {
            "paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
            "title": "Linear Transformers Are Secretly Fast Weight Programmers"
        },
        {
            "paperId": "7a485356e538cf5d259912f41a7e54d2370397ca",
            "title": "3.2 The A100 Datacenter GPU and Ampere Architecture"
        },
        {
            "paperId": "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
            "title": "CKConv: Continuous Kernel Convolution For Sequential Data"
        },
        {
            "paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
            "title": "Rethinking Attention with Performers"
        },
        {
            "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
            "title": "Big Bird: Transformers for Longer Sequences"
        },
        {
            "paperId": "6f68e1bb253925d8431588555d3010419f322e04",
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
        },
        {
            "paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5",
            "title": "The hardware lottery"
        },
        {
            "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
            "title": "Longformer: The Long-Document Transformer"
        },
        {
            "paperId": "657329c633709dd1ac34a30d57341b186b1a47c2",
            "title": "Efficient Content-Based Sparse Attention with Routing Transformers"
        },
        {
            "paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
            "title": "GLU Variants Improve Transformer"
        },
        {
            "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "title": "Reformer: The Efficient Transformer"
        },
        {
            "paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
            "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"
        },
        {
            "paperId": "f51497f463566581874c941353dd9d80069c5b77",
            "title": "Compressive Transformers for Long-Range Sequence Modelling"
        },
        {
            "paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd",
            "title": "Root Mean Square Layer Normalization"
        },
        {
            "paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f",
            "title": "Triton: an intermediate language and compiler for tiled neural network computations"
        },
        {
            "paperId": "b31eef8d9263b02f7d0c1ab55b26012550a2e95a",
            "title": "OpenCeres: When Open Information Extraction Meets the Semi-Structured Web"
        },
        {
            "paperId": "9770fff7379a7ab9006b48939462354dda9a2053",
            "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"
        },
        {
            "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
            "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
        },
        {
            "paperId": "21da617a0f79aabf94272107184606cefe90ab75",
            "title": "Generating Long Sequences with Sparse Transformers"
        },
        {
            "paperId": "fea820b7d953d32069e189af2961c28fd213470b",
            "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"
        },
        {
            "paperId": "9fde884f635c2c6a8d8586a027e5e637bfad78ba",
            "title": "Accelerating reduction and scan using tensor core units"
        },
        {
            "paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094",
            "title": "CoQA: A Conversational Question Answering Challenge"
        },
        {
            "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
            "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
        },
        {
            "paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
            "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
        },
        {
            "paperId": "10428cdda9387933c64f518cefe6ca0a685f579e",
            "title": "The unreasonable effectiveness of the forget gate"
        },
        {
            "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
            "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
        },
        {
            "paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a",
            "title": "Fixing Weight Decay Regularization in Adam"
        },
        {
            "paperId": "4f57f486adea0bf95c252620a4e8af39232ef8bc",
            "title": "Swish: a Self-Gated Activation Function"
        },
        {
            "paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498",
            "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55",
            "title": "Using Fast Weights to Attend to the Recent Past"
        },
        {
            "paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a",
            "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"
        },
        {
            "paperId": "82ef2354ca03cb3ad69e75a07d2a5163f82c4dbd",
            "title": "Compiling high performance recursive filters"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447",
            "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"
        },
        {
            "paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "title": "Learning to Forget: Continual Prediction with LSTM"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "bc22e87a26d020215afe91c751e5bdaddd8e4922",
            "title": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"
        },
        {
            "paperId": "dc48bc1a4d81e0f37603013fd2a95644dc233bd0",
            "title": "Functional Interpolation for Relative Positions Improves Long Context Transformers"
        },
        {
            "paperId": "3719ad19da30771aba5d5c48491a21d6c393832d",
            "title": "Vivim: a Video Vision Mamba for Medical Video Object Segmentation"
        },
        {
            "paperId": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9",
            "title": "Scaling TransNormer to 175 Billion Parameters"
        },
        {
            "paperId": "5e424004958853f4e366e7a86a1c3a56a76cb2a4",
            "title": "LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers"
        },
        {
            "paperId": null,
            "title": "A framework for few-shot language model evaluation"
        },
        {
            "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
            "title": "An Adversarial Winograd Schema Challenge at Scale"
        },
        {
            "paperId": "6f8c6d0df740eee17395c29a6a2b7410dee644cd",
            "title": "Partitioning"
        },
        {
            "paperId": "ec0bc920662693375a881d8beb8c97bf08281dba",
            "title": "Prefix sums and their applications"
        },
        {
            "paperId": "7257eacd80458e70c74494eb1b6759b52ff21399",
            "title": "Using fast weights to deblur old memories"
        },
        {
            "paperId": null,
            "title": ": Fast transformers via sketching"
        },
        {
            "paperId": null,
            "title": "with Hardware-Ef\ufb01cient Training"
        },
        {
            "paperId": null,
            "title": "SlimPajama: A 627B token cleaned and deduplicated version of"
        },
        {
            "paperId": null,
            "title": "tokenizer"
        }
    ]
}