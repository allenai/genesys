{
    "paperId": "91166a75f0e32b782a57028f1501aba6335ac550",
    "externalIds": {
        "DBLP": "journals/corr/abs-2302-06015",
        "ArXiv": "2302.06015",
        "DOI": "10.48550/arXiv.2302.06015",
        "CorpusId": 256827101
    },
    "title": "A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity",
    "abstract": "Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 84,
    "citationCount": 41,
    "influentialCitationCount": 4,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2302.06015",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations, and it is proved that a training process using stochastic gradient descent (SGD) leads to a sparse attention map."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.7713305950164795,
            0.7399321794509888,
            2.2170042991638184,
            3.8272480964660645,
            -0.6423687934875488,
            -0.6069623231887817,
            6.7779340744018555,
            -1.1145474910736084,
            2.810304641723633,
            -0.6346039772033691,
            -1.7112805843353271,
            6.233569145202637,
            0.13969770073890686,
            0.90370112657547,
            -3.480208158493042,
            -0.2316436469554901,
            -2.5724599361419678,
            -0.7223801612854004,
            4.242986679077148,
            1.9172425270080566,
            0.07640162855386734,
            2.789428472518921,
            -4.380751609802246,
            -1.2448219060897827,
            -0.8427726030349731,
            -1.8058712482452393,
            2.228410482406616,
            0.7664992213249207,
            -5.548550605773926,
            -1.3087096214294434,
            0.2376895695924759,
            -6.266689300537109,
            5.915175914764404,
            -3.0944600105285645,
            0.5485848188400269,
            -0.536651611328125,
            0.48132753372192383,
            7.336318492889404,
            -1.2407617568969727,
            -0.8953675627708435,
            -0.031399182975292206,
            1.470050573348999,
            1.4538103342056274,
            -1.4160655736923218,
            0.8534661531448364,
            -1.1601470708847046,
            -0.3332974314689636,
            4.08360481262207,
            -0.5390677452087402,
            1.4054979085922241,
            1.782047986984253,
            1.3810920715332031,
            -0.8225462436676025,
            1.391128420829773,
            -1.8180983066558838,
            -3.4558658599853516,
            2.8379034996032715,
            0.6716600060462952,
            1.9593027830123901,
            -1.7079306840896606,
            3.9886631965637207,
            3.3073034286499023,
            0.5927220582962036,
            -2.51562237739563,
            3.315979480743408,
            -2.6839094161987305,
            0.39494788646698,
            5.140010833740234,
            1.1175696849822998,
            -0.14772284030914307,
            -0.49104636907577515,
            -5.172235012054443,
            2.3778390884399414,
            1.6293208599090576,
            -2.3814234733581543,
            0.45633411407470703,
            -0.6911793947219849,
            -4.952931880950928,
            -1.0180927515029907,
            1.1487452983856201,
            -0.21012365818023682,
            2.1544158458709717,
            -1.4645448923110962,
            0.27925604581832886,
            5.691633701324463,
            -1.4188904762268066,
            -5.132447719573975,
            -2.3166651725769043,
            -0.737917423248291,
            -0.5271362066268921,
            -0.7191234230995178,
            4.076539516448975,
            0.3932793140411377,
            0.05430707335472107,
            -5.894288539886475,
            -0.942767858505249,
            2.1194300651550293,
            0.5307276248931885,
            -2.071089744567871,
            1.451458215713501,
            2.651838779449463,
            -1.2058451175689697,
            1.6733318567276,
            -0.29482707381248474,
            3.4496545791625977,
            -1.0302157402038574,
            0.8096516132354736,
            0.6855348944664001,
            2.5573859214782715,
            -4.281778335571289,
            -1.3487093448638916,
            4.278985023498535,
            -2.743478536605835,
            -3.063735008239746,
            -0.5806745886802673,
            -0.2060239613056183,
            1.2341389656066895,
            0.8224519491195679,
            0.08275672793388367,
            3.8040037155151367,
            -2.2782673835754395,
            -1.3709218502044678,
            -3.7398455142974854,
            1.7014580965042114,
            0.8969194889068604,
            2.886949062347412,
            -0.48558497428894043,
            3.2122316360473633,
            -0.6829581260681152,
            -5.397779941558838,
            3.3030948638916016,
            -3.349371910095215,
            3.4967689514160156,
            -1.6439317464828491,
            5.064571857452393,
            4.07206916809082,
            -2.906853675842285,
            -0.5741926431655884,
            -4.316378116607666,
            -0.1428813636302948,
            0.011984523385763168,
            1.3984591960906982,
            0.08422726392745972,
            3.1586432456970215,
            3.045057535171509,
            5.983482360839844,
            -1.6835359334945679,
            2.510319948196411,
            3.3264801502227783,
            5.8777337074279785,
            4.520277976989746,
            -5.468755722045898,
            0.10661374032497406,
            2.4082140922546387,
            0.3639308512210846,
            0.8103860020637512,
            -6.762736797332764,
            3.05843448638916,
            -3.9899234771728516,
            0.3689756393432617,
            -0.7286977171897888,
            0.5873115658760071,
            -10.47009563446045,
            -0.33647704124450684,
            3.527729034423828,
            -5.33430814743042,
            -0.8720424175262451,
            1.373241901397705,
            -3.2969744205474854,
            1.9561617374420166,
            1.1242363452911377,
            0.8889780044555664,
            1.571925163269043,
            4.5436692237854,
            0.9566168785095215,
            3.2528367042541504,
            0.8767012357711792,
            -2.3374245166778564,
            0.144150972366333,
            -0.7098156809806824,
            -1.5813090801239014,
            -1.3763560056686401,
            -5.397150039672852,
            3.083329200744629,
            -4.0357279777526855,
            -1.37177312374115,
            -0.3837575316429138,
            -2.413341522216797,
            -2.6712231636047363,
            -0.4859984517097473,
            0.31045767664909363,
            0.11378121376037598,
            6.4054484367370605,
            4.4605712890625,
            2.74239444732666,
            3.047292709350586,
            1.8632293939590454,
            3.041741371154785,
            -1.7195392847061157,
            0.6993452906608582,
            3.2224178314208984,
            -0.5036208629608154,
            -1.8736317157745361,
            -3.3463022708892822,
            1.7413861751556396,
            0.2862778902053833,
            -3.81320858001709,
            4.19000244140625,
            3.076378345489502,
            -2.009504795074463,
            -2.3982720375061035,
            0.4051904082298279,
            -3.5095860958099365,
            -0.6701152920722961,
            -4.809536933898926,
            -1.073735237121582,
            -7.124608039855957,
            -0.45004040002822876,
            4.49043083190918,
            0.39810192584991455,
            1.8060253858566284,
            1.815814733505249,
            -0.8182662129402161,
            -2.138584613800049,
            -0.5721651315689087,
            -0.5444132685661316,
            3.8976199626922607,
            1.1429717540740967,
            -1.067135214805603,
            4.264095783233643,
            -1.9215824604034424,
            -3.9494056701660156,
            0.9318917393684387,
            -1.6867643594741821,
            -6.021559715270996,
            -0.5563803315162659,
            -3.0293538570404053,
            1.4765470027923584,
            -0.41870594024658203,
            -0.20041874051094055,
            7.048262596130371,
            3.0337071418762207,
            0.31476372480392456,
            2.93121600151062,
            0.10228979587554932,
            -1.7149708271026611,
            -4.799927711486816,
            2.3262388706207275,
            0.4613179564476013,
            -2.22190523147583,
            -0.8841527700424194,
            1.0583921670913696,
            1.7438042163848877,
            -0.9326767921447754,
            1.667827844619751,
            -0.36164170503616333,
            1.0071063041687012,
            -1.313432216644287,
            0.4963463842868805,
            1.1391751766204834,
            -0.45481836795806885,
            2.4400367736816406,
            0.1583452969789505,
            2.1876699924468994,
            0.9496002197265625,
            -0.4261050224304199,
            -6.150311470031738,
            -1.0410916805267334,
            -2.158088207244873,
            5.339461326599121,
            1.530409336090088,
            1.0381042957305908,
            -0.1690632402896881,
            -5.832075119018555,
            -1.656301736831665,
            -3.3286142349243164,
            -3.0869033336639404,
            -1.2315722703933716,
            4.397257328033447,
            3.9109997749328613,
            1.690671443939209,
            -2.0776333808898926,
            -1.0958497524261475,
            -0.8450645208358765,
            -1.6958904266357422,
            -4.833683013916016,
            -0.20584261417388916,
            -1.0138605833053589,
            3.3652148246765137,
            0.7001655101776123,
            -4.018313884735107,
            5.209327220916748,
            -4.613601207733154,
            1.0554147958755493,
            -1.6834571361541748,
            4.148144721984863,
            3.5966482162475586,
            -0.8166441321372986,
            0.7501928210258484,
            -0.4860771894454956,
            0.5094959139823914,
            1.2310326099395752,
            5.276610374450684,
            -2.330660581588745,
            1.5855613946914673,
            5.257946014404297,
            4.328535079956055,
            -1.130856990814209,
            -0.45423826575279236,
            -2.61904239654541,
            -0.8997524976730347,
            -2.9197769165039062,
            2.014136791229248,
            -2.445634365081787,
            -2.812753677368164,
            -1.4407426118850708,
            1.5697541236877441,
            1.1886131763458252,
            -0.07209122180938721,
            2.638749122619629,
            0.7618089318275452,
            2.0954132080078125,
            -5.885136127471924,
            -3.7408230304718018,
            -2.914832830429077,
            0.9978926777839661,
            3.7794535160064697,
            2.8869357109069824,
            -1.654392123222351,
            2.7717490196228027,
            -1.3053125143051147,
            4.153229713439941,
            3.1743905544281006,
            1.273284912109375,
            0.9076610803604126,
            -5.607295989990234,
            -1.6603535413742065,
            0.2746623754501343,
            0.3138238787651062,
            4.433577537536621,
            -1.7210769653320312,
            9.055802345275879,
            -0.4994620680809021,
            3.1946659088134766,
            0.7595692873001099,
            1.9396706819534302,
            0.48259174823760986,
            0.32616370916366577,
            0.18775497376918793,
            -3.1748857498168945,
            -2.2145886421203613,
            -0.040298014879226685,
            3.1217215061187744,
            -3.1547250747680664,
            3.244915723800659,
            3.16502046585083,
            3.513571262359619,
            0.5159177184104919,
            3.9668920040130615,
            1.8297381401062012,
            1.589369773864746,
            0.0017326287925243378,
            3.5963761806488037,
            -0.5915528535842896,
            -3.1264266967773438,
            -4.114351272583008,
            8.216651916503906,
            0.11122281849384308,
            -1.034731149673462,
            -5.4154276847839355,
            -3.9937524795532227,
            0.3130479156970978,
            -1.5770533084869385,
            2.9165756702423096,
            -2.6570844650268555,
            -2.205259084701538,
            0.6801744103431702,
            -4.184566020965576,
            0.8000978827476501,
            1.858310580253601,
            1.0202395915985107,
            5.8055620193481445,
            1.3367760181427002,
            2.2009105682373047,
            1.4028613567352295,
            -0.9416492581367493,
            -6.136925220489502,
            0.2447126805782318,
            1.0759458541870117,
            1.0197981595993042,
            -1.2831629514694214,
            4.7140793800354,
            1.4109556674957275,
            0.19592031836509705,
            -3.748481273651123,
            -3.028412103652954,
            -3.6561126708984375,
            -4.890271186828613,
            -1.6629638671875,
            -0.8387284874916077,
            1.106276273727417,
            5.4639177322387695,
            5.384217262268066,
            1.7886369228363037,
            -1.4380074739456177,
            -1.4716540575027466,
            4.3095622062683105,
            -0.5343493819236755,
            -0.2604624032974243,
            1.4618752002716064,
            -4.3153977394104,
            -1.0564608573913574,
            -3.541213274002075,
            -0.8600051403045654,
            0.8946691155433655,
            -0.7076312899589539,
            0.7145767211914062,
            2.70231294631958,
            4.985657691955566,
            1.0040708780288696,
            -1.8348419666290283,
            4.802858352661133,
            1.9301162958145142,
            2.065692901611328,
            -0.2167602777481079,
            2.896376609802246,
            2.4265389442443848,
            4.25935173034668,
            -2.091715097427368,
            -0.31168776750564575,
            -4.1517181396484375,
            3.5723514556884766,
            -4.1440510749816895,
            0.11441576480865479,
            1.5378570556640625,
            2.58974027633667,
            1.2848769426345825,
            0.7852684259414673,
            1.7896907329559326,
            0.011395573616027832,
            -1.21225905418396,
            2.7030344009399414,
            -3.4457452297210693,
            4.85200834274292,
            1.5627291202545166,
            -1.637734055519104,
            -0.6660463213920593,
            1.281432867050171,
            0.5503495931625366,
            -3.8400912284851074,
            1.561417818069458,
            -6.698554992675781,
            -4.371166229248047,
            0.3817466199398041,
            0.9593027234077454,
            0.056556880474090576,
            -0.997606635093689,
            -1.3204774856567383,
            1.002185344696045,
            -0.773996114730835,
            -6.499783992767334,
            1.3006149530410767,
            -0.41318315267562866,
            1.3298752307891846,
            1.810854434967041,
            5.934666633605957,
            -0.8285414576530457,
            -0.6212189197540283,
            -3.8307125568389893,
            1.2398388385772705,
            4.61143159866333,
            0.001738905906677246,
            -2.324662208557129,
            -2.267331600189209,
            0.5133182406425476,
            -0.7525372505187988,
            1.778287649154663,
            3.7294671535491943,
            0.11656567454338074,
            -2.2804696559906006,
            -4.543229103088379,
            -2.433711290359497,
            3.5656633377075195,
            0.10702735185623169,
            -3.350611925125122,
            5.232272148132324,
            3.186286211013794,
            2.3108789920806885,
            5.4558892250061035,
            4.966185569763184,
            2.9041996002197266,
            1.3556500673294067,
            4.6404218673706055,
            -0.41243961453437805,
            2.808626174926758,
            1.211748480796814,
            -1.7408455610275269,
            0.11896166205406189,
            -1.4299204349517822,
            -0.04448935389518738,
            2.325890302658081,
            -3.18674635887146,
            -1.9945638179779053,
            -0.30128130316734314,
            -1.5049500465393066,
            4.51703405380249,
            1.385459065437317,
            2.110619068145752,
            1.2366268634796143,
            -2.307771921157837,
            -3.083988666534424,
            3.214779853820801,
            -8.789278984069824,
            0.6087531447410583,
            -2.9213368892669678,
            1.7653312683105469,
            3.0656938552856445,
            -3.0049071311950684,
            -0.1254318654537201,
            -0.25819000601768494,
            0.017046809196472168,
            0.7288671731948853,
            -1.7077784538269043,
            0.7267667055130005,
            0.7067058086395264,
            1.3140325546264648,
            -3.9187517166137695,
            2.532000780105591,
            -0.923690676689148,
            6.148531913757324,
            6.833248138427734,
            5.619472980499268,
            0.4685375690460205,
            -3.2720160484313965,
            -3.472191333770752,
            -2.870474338531494,
            3.4272191524505615,
            4.122983455657959,
            -1.2707600593566895,
            -2.19761061668396,
            2.024968147277832,
            1.9307951927185059,
            -1.1135385036468506,
            3.1952106952667236,
            0.6125056743621826,
            2.716566801071167,
            -4.614483833312988,
            -3.5923385620117188,
            -0.8101300001144409,
            -0.682196319103241,
            2.155233144760132,
            -0.21931886672973633,
            2.7727584838867188,
            -2.869030475616455,
            -3.3778903484344482,
            -0.4439302086830139,
            -0.5412473082542419,
            -4.119258880615234,
            -1.2123202085494995,
            5.269839286804199,
            -1.1975175142288208,
            2.0975186824798584,
            -0.6838923096656799,
            3.9072046279907227,
            -2.2205517292022705,
            -1.4034416675567627,
            2.124727725982666,
            1.3713693618774414,
            -3.389650821685791,
            -4.866459846496582,
            -0.41893625259399414,
            -4.1727094650268555,
            -1.4229657649993896,
            1.7867307662963867,
            0.11233764886856079,
            3.2788333892822266,
            6.9956207275390625,
            5.138358116149902,
            -4.818238258361816,
            0.36241427063941956,
            -2.32808256149292,
            -0.9509720802307129,
            -2.76080060005188,
            -4.3597917556762695,
            2.4557628631591797,
            -3.7592742443084717,
            -3.3636789321899414,
            -1.921247959136963,
            -3.2382607460021973,
            0.20111265778541565,
            1.3934898376464844,
            -0.7812111973762512,
            1.3869779109954834,
            -5.749099254608154,
            -1.8434960842132568,
            -2.4341001510620117,
            3.3520689010620117,
            0.26674893498420715,
            -1.4475502967834473,
            3.856494665145874,
            3.1114501953125,
            1.9696729183197021,
            2.8485324382781982,
            1.2707998752593994,
            -2.539724826812744,
            1.9165698289871216,
            2.5840206146240234,
            0.46505188941955566,
            -2.137266159057617,
            1.7113673686981201,
            0.6898395419120789,
            1.4080402851104736,
            14.173739433288574,
            2.0384464263916016,
            -0.690231442451477,
            -2.1645328998565674,
            1.0001357793807983,
            -4.084206581115723,
            0.6624423861503601,
            2.3474788665771484,
            0.7282171249389648,
            3.432811975479126,
            -2.7981886863708496,
            -4.56407356262207,
            0.10606501251459122,
            1.2830430269241333,
            -4.243515968322754,
            -0.355955570936203,
            -2.673086166381836,
            -0.14656710624694824,
            -2.2125306129455566,
            1.3428609371185303,
            -3.5037670135498047,
            1.8306450843811035,
            -2.3239803314208984,
            -0.9358271956443787,
            0.347708523273468,
            4.935938358306885,
            -0.7529265284538269,
            1.1567888259887695,
            -8.693929672241211,
            1.301363229751587,
            0.06490957736968994,
            -0.2153874635696411,
            2.5287394523620605,
            -0.6837738752365112,
            -2.5743775367736816,
            1.5997834205627441,
            4.908836364746094,
            -0.48824411630630493,
            2.4871654510498047,
            0.8351812958717346,
            0.08189194649457932,
            -0.16335085034370422,
            -4.392427444458008,
            -0.5719488263130188,
            -0.2144298553466797,
            0.00843760371208191,
            3.929957389831543,
            -2.3079662322998047,
            -1.6170761585235596,
            1.8522231578826904,
            1.6096352338790894,
            0.561103105545044,
            -0.4039214849472046,
            1.196413278579712,
            4.9478230476379395,
            3.2246665954589844,
            -0.5882667899131775,
            -1.6339056491851807,
            2.636467933654785,
            0.745849609375,
            0.7056922912597656,
            2.4989283084869385,
            -3.5150387287139893,
            -2.5874412059783936,
            -0.5554572343826294,
            -0.4815303683280945,
            -6.135963439941406,
            3.373533248901367,
            3.482369899749756,
            -0.3952799439430237,
            4.441458702087402,
            1.5255941152572632,
            -0.20614761114120483,
            -3.8347787857055664,
            -2.542475700378418,
            -6.5076799392700195,
            -3.2033512592315674,
            0.25364699959754944,
            -0.34456953406333923,
            4.705125331878662,
            -4.3191938400268555,
            4.9634857177734375,
            -5.886623382568359,
            2.338390827178955,
            6.178705215454102,
            -3.1868791580200195,
            3.380938768386841,
            -0.7897875308990479,
            -2.099703788757324,
            3.6673130989074707,
            -0.8092995285987854,
            -1.3248777389526367,
            3.781989097595215,
            1.142691969871521,
            3.527005195617676,
            -6.069967269897461,
            -0.48878014087677,
            -0.644132137298584,
            -5.233384132385254,
            -4.054836273193359,
            6.072864055633545,
            4.20713472366333,
            -0.3692752718925476,
            -3.4693808555603027,
            -2.0660831928253174,
            -1.491825819015503,
            0.6915867924690247,
            -3.1692395210266113,
            -5.393261909484863,
            -4.295464515686035,
            5.9203410148620605,
            -0.6225067973136902,
            -0.10223650932312012,
            -0.9770668148994446,
            -1.351749300956726,
            -1.0801701545715332,
            -1.0846834182739258,
            2.206071615219116,
            1.2496068477630615,
            3.1021454334259033,
            1.7038981914520264,
            -2.7523000240325928,
            -3.374321699142456,
            -3.9674324989318848,
            -4.952048301696777,
            -1.1558372974395752,
            0.841028094291687,
            -2.6812856197357178,
            1.4439334869384766,
            -6.242249965667725,
            2.311211347579956,
            -4.322473049163818,
            -2.3441057205200195,
            -0.23824599385261536,
            -2.3709685802459717,
            -0.45117172598838806,
            1.6220287084579468,
            1.5690608024597168,
            -4.909179210662842,
            3.889712333679199,
            1.1883341073989868,
            -3.1114211082458496,
            -1.41762113571167,
            10.425155639648438,
            -0.5647889971733093,
            0.7310653328895569,
            -2.645195245742798,
            -2.651817798614502,
            -3.1238741874694824,
            1.1131465435028076,
            -0.4803653061389923,
            1.77922785282135,
            4.311603546142578,
            2.353813409805298,
            -2.3262438774108887,
            -2.9403634071350098
        ]
    },
    "authors": [
        {
            "authorId": "2162633775",
            "name": "Hongkang Li"
        },
        {
            "authorId": "39872583",
            "name": "M. Wang"
        },
        {
            "authorId": "143743061",
            "name": "Sijia Liu"
        },
        {
            "authorId": "2158177808",
            "name": "Pin-Yu Chen"
        }
    ],
    "references": [
        {
            "paperId": "377e5e0e5f4253442ad1e476c076a3928b031811",
            "title": "Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks"
        },
        {
            "paperId": "13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0",
            "title": "Vision Transformers provably learn spatial structure"
        },
        {
            "paperId": "ad855e85e324f9d82347537e0ff79d4c0ddfff7f",
            "title": "Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling"
        },
        {
            "paperId": "d2b297c553b5820ec114bfb1d037a537f2f66aad",
            "title": "A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features"
        },
        {
            "paperId": "159be298e25b7210ae577d7962cceb5e73aee687",
            "title": "Automated Progressive Learning for Efficient Training of Vision Transformers"
        },
        {
            "paperId": "732e57698f9d50586a8006ff0e6467d07727ec6f",
            "title": "Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning"
        },
        {
            "paperId": "6df7ca762a5347a125295db395797edcfca43645",
            "title": "Learning and generalization of one-hidden-layer neural networks, going beyond standard Gaussian data"
        },
        {
            "paperId": "625270a54be430ad6262f848babb0d106af5b183",
            "title": "Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations"
        },
        {
            "paperId": "eb92a453cf982126fa2125d4c8915352a52af54d",
            "title": "Online Decision Transformer"
        },
        {
            "paperId": "48e84128b0f288f544176138805a97fbe592a1dd",
            "title": "DynaMixer: A Vision MLP Architecture with Dynamic Mixing"
        },
        {
            "paperId": "13f7a106bb3814ad1fab25fd1356e99e91f402d3",
            "title": "Q-ViT: Fully Differentiable Quantization for Vision Transformer"
        },
        {
            "paperId": "c2a0c18e810535db52e5ebaf180c64ce70356748",
            "title": "A-ViT: Adaptive Tokens for Efficient Vision Transformer"
        },
        {
            "paperId": "1ee05cd919590eaba129caa0fda5e850c87b75a5",
            "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer"
        },
        {
            "paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737",
            "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"
        },
        {
            "paperId": "0e9ac2cfc5a3ecb66eeace720901390f7809ba0a",
            "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"
        },
        {
            "paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006",
            "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"
        },
        {
            "paperId": "c295391129426d89ec58cebb049d1cd2e976deec",
            "title": "Post-Training Quantization for Vision Transformer"
        },
        {
            "paperId": "148011adfae37b821407aae84fcbbf7fb4619eb6",
            "title": "On the Expressive Power of Self-Attention Matrices"
        },
        {
            "paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f",
            "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"
        },
        {
            "paperId": "33fd56e5067a1e8a9713378af3e1c1c08d5ce93b",
            "title": "Patch Slimming for Efficient Vision Transformers"
        },
        {
            "paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2",
            "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"
        },
        {
            "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
            "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
        },
        {
            "paperId": "ac78850445bdeb4d3f8a273297916d6e6a90b2fc",
            "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning"
        },
        {
            "paperId": "6709d5583f658f589ae6a2184805933aceb18849",
            "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"
        },
        {
            "paperId": "cc9f3a61ea4eaabf43cbb30cd1dd718074932679",
            "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers"
        },
        {
            "paperId": "93efaf8c27940aaef145d8bcbca957be634d26e5",
            "title": "Vision Transformer Pruning"
        },
        {
            "paperId": "ac591dbf261777e05d89c27f9a7bcb06f88aab5a",
            "title": "Scalable Vision Transformers with Hierarchical Pooling"
        },
        {
            "paperId": "4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506",
            "title": "Approximating How Single Head Attention Learns"
        },
        {
            "paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e",
            "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"
        },
        {
            "paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881",
            "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "255e6239bcc51047d020d41ce0179c1270f3c22f",
            "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "10c86505de83647c7b4157595ab10f64e97c94ef",
            "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"
        },
        {
            "paperId": "2f1978e44a63fff9cb2787d417916646ffa3ba36",
            "title": "Improved Linear Convergence of Training CNNs With Generalizability Guarantees: A One-Hidden-Layer Case"
        },
        {
            "paperId": "6349901133797ab211cfd8c8dfcdc57828f20dfb",
            "title": "A Mathematical Theory of Attention"
        },
        {
            "paperId": "8095cb807d1c7577376c470bb6d43702634524c2",
            "title": "Tensor Programs II: Neural Tangent Kernel for Any Architecture"
        },
        {
            "paperId": "e3b2f31bbd33e1f59fa2eb83101d288e1159ad2e",
            "title": "Fast Learning of Graph Neural Networks with Guaranteed Generalizability: One-hidden-layer Case"
        },
        {
            "paperId": "c014f8bc3b521453a93a13bb2c90700fcf462738",
            "title": "Limits to Depth Efficiencies of Self-Attention"
        },
        {
            "paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510",
            "title": "Infinite attention: NNGP and NTK for deep attention networks"
        },
        {
            "paperId": "75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1",
            "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling"
        },
        {
            "paperId": "45988d39ab1b0e5199e1f0f31952760bc763e611",
            "title": "The Lipschitz Constant of Self-Attention"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "23c84238f9b15d7fd87a95d3b16882a21b953d8a",
            "title": "Feature Purification: How Adversarial Training Performs Robust Deep Learning"
        },
        {
            "paperId": "26e858cf3c82b66bbd539bb79356b0e885bdc694",
            "title": "An Investigation of Why Overparameterization Exacerbates Spurious Correlations"
        },
        {
            "paperId": "2f33459606121e25e3e6142416b0203bc6b40358",
            "title": "Learning Parities with Neural Networks"
        },
        {
            "paperId": "a6294f64b06a15bc6c853bf6e4e50209f25f1a48",
            "title": "A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks"
        },
        {
            "paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d",
            "title": "Are Transformers universal approximators of sequence-to-sequence functions?"
        },
        {
            "paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614",
            "title": "On the Relationship between Self-Attention and Convolutional Layers"
        },
        {
            "paperId": "0feb7c19f98ac44703126dc46e60d166da1f118c",
            "title": "An Improved Analysis of Training Over-parameterized Deep Neural Networks"
        },
        {
            "paperId": "4b5744dd44a0026c6f386d5cb21b795499d5efb7",
            "title": "Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks"
        },
        {
            "paperId": "d849c09c319f2ce12efc14bddc7581ab9aa688ac",
            "title": "What Can ResNet Learn Efficiently, Going Beyond Kernels?"
        },
        {
            "paperId": "5eda6d680dc1360e6d1b835c236a91f9dd1918fc",
            "title": "Behavior sequence transformer for e-commerce recommendation in Alibaba"
        },
        {
            "paperId": "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
            "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"
        },
        {
            "paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443",
            "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"
        },
        {
            "paperId": "611fe6e34df07ea1b2104899e49642b4531b53e9",
            "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"
        },
        {
            "paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
            "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"
        },
        {
            "paperId": "42ec3db12a2e4628885451b13035c2e975220a25",
            "title": "A Convergence Theory for Deep Learning via Over-Parameterization"
        },
        {
            "paperId": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6",
            "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"
        },
        {
            "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
            "title": "Universal Transformers"
        },
        {
            "paperId": "7a84a692327534fd227fa1e07fcb3816b633c591",
            "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
        },
        {
            "paperId": "f60244af6cd197c5686fafb179e74709d5577266",
            "title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy"
        },
        {
            "paperId": "97326ad593fc141e2cf3f9f6450ea523f443ff20",
            "title": "Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels"
        },
        {
            "paperId": "b8c6ccd5c1eb4f9837fc4877d27e55b7349781be",
            "title": "Deep Interest Network for Click-Through Rate Prediction"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "99ed21b585f4d6cbc4e20002bedca8d6c08169c6",
            "title": "Recovery Guarantees for One-hidden-layer Neural Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "4406c54f40e0f73db2180704d454951649df32f2",
            "title": "Introduction to the non-asymptotic analysis of random matrices"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "32e0057c9a06d23182ff41553c9df1c9a8c4b757",
            "title": "Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators"
        },
        {
            "paperId": null,
            "title": "2022) is a concurrent work which theoretically studies Vision Transformers"
        },
        {
            "paperId": "d85f1aa35b308a37223c88a6a6f6202af8dfc751",
            "title": "Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels"
        },
        {
            "paperId": "2b323b6746f3dc052543ad891a179406acfeb5ee",
            "title": "An optimization and generalization analysis for max-pooling networks"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": "9af62668cb87f11fffb53a194588c8158fde6b00",
            "title": "DynamicViT: Ef\ufb01cient Vision Transformers with Dynamic Token Sparsi\ufb01cation"
        },
        {
            "paperId": "2068b4d5c95ea5c66c8a81e73337fc52466b8b18",
            "title": "Reinforcement Learning as One Big Sequence Modeling Problem"
        },
        {
            "paperId": "3ede1108e6cbad247b875aa46b4541967a83b980",
            "title": "Limits to Depth Ef\ufb01ciencies of Self-Attention Supplementary Material"
        },
        {
            "paperId": "5c43d84705cc320414ebad1525599146bcd68ca3",
            "title": "Infinite attention: NNGP and NTK for deep attention networks"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": "60c3e583b32a51f38b35073b0e1758d8a6ac81e2",
            "title": "Concise Formulas for the Area and Volume of a Hyperspherical Cap"
        },
        {
            "paperId": null,
            "title": "Cifar-10 (canadian institute for advanced research)"
        },
        {
            "paperId": null,
            "title": "x nl"
        }
    ]
}