{
    "paperId": "99c81b2dc2a5def38f62db269bba7765cff129e0",
    "externalIds": {
        "DBLP": "conf/nips/GoyalKGB17",
        "MAG": "2963919683",
        "ArXiv": "1711.02282",
        "CorpusId": 27248616
    },
    "title": "Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net",
    "abstract": "We propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems. The proposed training objective, which we derive via principled variational methods, encourages the transition operator to \"walk back\" in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution. Source Code: this http URL",
    "venue": "Neural Information Processing Systems",
    "year": 2017,
    "referenceCount": 36,
    "citationCount": 55,
    "influentialCitationCount": 1,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1996705",
            "name": "Anirudh Goyal"
        },
        {
            "authorId": "145604319",
            "name": "Nan Rosemary Ke"
        },
        {
            "authorId": "25769960",
            "name": "S. Ganguli"
        },
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        }
    ],
    "references": [
        {
            "paperId": "d2860bb05f747e4628e95e4d84018263831bab0d",
            "title": "Learning to Generate Samples from Noise through Infusion Training"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "6a97d2668187965743d1b825b306defccbabbb4c",
            "title": "Improved Variational Inference with Inverse Autoregressive Flow"
        },
        {
            "paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
            "title": "Improved Techniques for Training GANs"
        },
        {
            "paperId": "6b570069f14c7588e066f7138e1f21af59d62e61",
            "title": "Theano: A Python framework for fast computation of mathematical expressions"
        },
        {
            "paperId": "64d698ecd01eab99e81e586400e86d3d70b9cba7",
            "title": "Ladder Variational Autoencoders"
        },
        {
            "paperId": "6a0fa9e2181027cc306c6df7948dd51894edc692",
            "title": "Near-optimal protocols in complex nonequilibrium transformations"
        },
        {
            "paperId": "d181ee5ea23ce57b0e673dfc5724dcc316013429",
            "title": "Why are deep nets reversible: A simple theory, with implications for training"
        },
        {
            "paperId": "39e0c341351f8f4a39ac890b96217c7f4bde5369",
            "title": "A note on the evaluation of generative models"
        },
        {
            "paperId": "a3ab782bec5b395df9d8f71f5fef7e9c82a8bc79",
            "title": "An objective function for STDP"
        },
        {
            "paperId": "2dcef55a07f8607a819c21fe84131ea269cc2e3c",
            "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
        },
        {
            "paperId": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca",
            "title": "DRAW: A Recurrent Neural Network For Image Generation"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "80a63f7e42166c64e330934339d18a72c330ae35",
            "title": "Accurate and conservative estimates of MRF log-likelihood using reverse annealing"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
            "title": "Deep Learning Face Attributes in the Wild"
        },
        {
            "paperId": "9a99c2453c4239662d093eb0715c846aef4cb84a",
            "title": "Random feedback weights support learning in deep neural networks"
        },
        {
            "paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
            "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
        },
        {
            "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "title": "Auto-Encoding Variational Bayes"
        },
        {
            "paperId": "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
            "title": "Deep Generative Stochastic Networks Trainable by Backprop"
        },
        {
            "paperId": "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
            "title": "Generalized Denoising Auto-Encoders as Generative Models"
        },
        {
            "paperId": "31a2053ebda7f6f77afe8c3fc53269b73567e446",
            "title": "What regularized auto-encoders learn from the data-generating distribution"
        },
        {
            "paperId": "d0965d8f9842f2db960b36b528107ca362c00d1a",
            "title": "Better Mixing via Deep Representations"
        },
        {
            "paperId": "f8a208c3601e913d1b9adb4d79fecff59ba494a5",
            "title": "Thermodynamic metrics and optimal paths."
        },
        {
            "paperId": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
        },
        {
            "paperId": "ddc45fad8d15771d9f1f8579331458785b2cdd93",
            "title": "Deep Boltzmann Machines"
        },
        {
            "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
            "title": "Extracting and composing robust features with denoising autoencoders"
        },
        {
            "paperId": "ef0c9a16d8ce313a1316514044e7c7cef1084777",
            "title": "Optimal finite-time processes in stochastic thermodynamics."
        },
        {
            "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "title": "A Fast Learning Algorithm for Deep Belief Nets"
        },
        {
            "paperId": "e10d9d75dbad52198e78ea61debe767c168d95b3",
            "title": "Path-ensemble averages in systems driven far from equilibrium"
        },
        {
            "paperId": "2f59406cce55c7bb9a78521bd14755a0db0aee7d",
            "title": "Annealed importance sampling"
        },
        {
            "paperId": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "title": "The Helmholtz Machine"
        },
        {
            "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
            "title": "GENERATIVE ADVERSARIAL NETS"
        },
        {
            "paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda",
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": null,
            "title": "Action potentials propagating back into dendrites triggers changes in ef\ufb01cacy"
        }
    ]
}