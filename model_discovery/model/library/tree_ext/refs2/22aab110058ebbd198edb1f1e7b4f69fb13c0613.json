{
    "paperId": "22aab110058ebbd198edb1f1e7b4f69fb13c0613",
    "externalIds": {
        "ArXiv": "1809.11096",
        "DBLP": "journals/corr/abs-1809-11096",
        "MAG": "2893749619",
        "CorpusId": 52889459
    },
    "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 60,
    "citationCount": 4795,
    "influentialCitationCount": 586,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2065040247",
            "name": "Andrew Brock"
        },
        {
            "authorId": "7408951",
            "name": "Jeff Donahue"
        },
        {
            "authorId": "34838386",
            "name": "K. Simonyan"
        }
    ],
    "references": [
        {
            "paperId": "fed5546113d945807e2f24317c0e560d1d21068c",
            "title": "The Unusual Effectiveness of Averaging in GAN Training"
        },
        {
            "paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af",
            "title": "Self-Attention Generative Adversarial Networks"
        },
        {
            "paperId": "577ae124136e0e76caf8fbc52e6b6d2072d70bff",
            "title": "Comparing Generative Adversarial Network Techniques for Image Creation and Modification"
        },
        {
            "paperId": "cf3d878d9c64bdff14dac364df8376a44225eec7",
            "title": "Is Generator Conditioning Causally Related to GAN Performance?"
        },
        {
            "paperId": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
            "title": "Spectral Normalization for Generative Adversarial Networks"
        },
        {
            "paperId": "ab559473a01836e72b9fb9393d6e07c5745528f3",
            "title": "cGANs with Projection Discriminator"
        },
        {
            "paperId": "69902406e7d08f8865f02185699978db499d25e7",
            "title": "Improving GANs Using Optimal Transport"
        },
        {
            "paperId": "3a6d33cb1a76fffc531e35c80be8eae20a30b2ef",
            "title": "On Convergence and Stability of GANs"
        },
        {
            "paperId": "6b4ca249b3b28d3fee65f69714440c08d42cee64",
            "title": "Which Training Methods for GANs do actually Converge?"
        },
        {
            "paperId": "da4e3270085fe5f59d9e89c072345c6600e7eb9a",
            "title": "A Note on the Inception Score"
        },
        {
            "paperId": "9723066a5587e6267d8abfd7feefd0637a5a211c",
            "title": "Demystifying MMD GANs"
        },
        {
            "paperId": "8899094797e82c5c185a0893896320ef77f60e64",
            "title": "Non-local Neural Networks"
        },
        {
            "paperId": "744fe47157477235032f7bb3777800f9f2f45e52",
            "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation"
        },
        {
            "paperId": "471908e99d6965f0f6d249c9cd013485dc2b21df",
            "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step"
        },
        {
            "paperId": "7cfa5c97164129ce3630511f639040d28db1d4b7",
            "title": "FiLM: Visual Reasoning with a General Conditioning Layer"
        },
        {
            "paperId": "acd87843a451d18b4dc6474ddce1ae946429eaf1",
            "title": "Wasserstein Generative Adversarial Networks"
        },
        {
            "paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca",
            "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"
        },
        {
            "paperId": "feeb3a2aa35a02e06546d05d94bac9a2123fc0c8",
            "title": "Modulating early visual processing by language"
        },
        {
            "paperId": "231af7dc01a166cac3b5b01ca05778238f796e41",
            "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
        },
        {
            "paperId": "56f5005c4be6f816f6f43795cc4825d798cd53ef",
            "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium"
        },
        {
            "paperId": "6d94575f2ce46d4dd14161a9f4447f7a361ad6cd",
            "title": "Megapixel Size Image Creation using Generative Adversarial Networks"
        },
        {
            "paperId": "cfcf66e4b22dc7671a5941e94e9d4afae75ba2f8",
            "title": "The Cramer Distance as a Solution to Biased Wasserstein Gradients"
        },
        {
            "paperId": "5e4d2e73e71806e9147afbedcd504424721fa059",
            "title": "Geometric GAN"
        },
        {
            "paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
            "title": "Improved Training of Wasserstein GANs"
        },
        {
            "paperId": "375aa8e5ed444c6f83e2abeff8475f76833a0a2e",
            "title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference"
        },
        {
            "paperId": "74ff6d48f9c62e937023106629d27ef2d2ddf8bc",
            "title": "Least Squares Generative Adversarial Networks"
        },
        {
            "paperId": "858dc7408c27702ec42778599fc8d11f73ef3f76",
            "title": "On the Quantitative Analysis of Decoder-Based Generative Models"
        },
        {
            "paperId": "ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
            "title": "Conditional Image Synthesis with Auxiliary Classifier GANs"
        },
        {
            "paperId": "99542f614d7e4146cad17196e76c997e57a69e4d",
            "title": "A Learned Representation For Artistic Style"
        },
        {
            "paperId": "d9dab7574d56ae81efe6c90c213c6509b36cf950",
            "title": "Deconvolution and Checkerboard Artifacts"
        },
        {
            "paperId": "3c7092347a5b7804d9534b6f3f52032c1502cbf8",
            "title": "Amortised MAP Inference for Image Super-resolution"
        },
        {
            "paperId": "1dcda6ac3fd33bf938574aca9542aec5e5cace26",
            "title": "Neural Photo Editing with Introspective Adversarial Networks"
        },
        {
            "paperId": "eb7ee0bc355652654990bcf9f92f124688fde493",
            "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
        },
        {
            "paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
            "title": "Improved Techniques for Training GANs"
        },
        {
            "paperId": "ffdcad14d2f6a12f607b59f88da4a939f4821691",
            "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization"
        },
        {
            "paperId": "4954fa180728932959997a4768411ff9136aac81",
            "title": "TensorFlow: A system for large-scale machine learning"
        },
        {
            "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "title": "Rethinking the Inception Architecture for Computer Vision"
        },
        {
            "paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
        },
        {
            "paperId": "39e0c341351f8f4a39ac890b96217c7f4bde5369",
            "title": "A note on the evaluation of generative models"
        },
        {
            "paperId": "47900aca2f0b50da3010ad59b394c870f0e6c02e",
            "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c",
            "title": "Conditional Generative Adversarial Nets"
        },
        {
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "99c970348b8f70ce23d6641e201904ea49266b6e",
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
        },
        {
            "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
            "paperId": "14299f5eb9efa328ee6b59b5ee3ff335fc07e175",
            "title": "Eigenvalue computation in the 20th century"
        },
        {
            "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
            "title": "GENERATIVE ADVERSARIAL NETS"
        },
        {
            "paperId": null,
            "title": "2018), which employ a z \u2208 R512 latent space on a highly constrained dataset with 30,000 images) is improper"
        },
        {
            "paperId": null,
            "title": "We use a single shared class embedding in G, and skip connections"
        },
        {
            "paperId": null,
            "title": "2016), and is chosen to allow some factors of variation"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": null,
            "title": "\u2022 We swept the strength of the modified Orthogonal Regularization penalty in G through"
        },
        {
            "paperId": null,
            "title": "We find that the strength of the penalty correlates negatively with performance, but that settings above 0.5 impart training stability"
        },
        {
            "paperId": null,
            "title": "\u2022 We swept the R1 gradient penalty strength through"
        },
        {
            "paperId": null,
            "title": "] and found it to have a light regularization effect similar to DropOut, but not to significantly improve results"
        }
    ]
}