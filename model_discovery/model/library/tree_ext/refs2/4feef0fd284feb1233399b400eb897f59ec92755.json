{
    "paperId": "4feef0fd284feb1233399b400eb897f59ec92755",
    "externalIds": {
        "MAG": "2950225141",
        "DBLP": "journals/corr/abs-1710-09412",
        "ArXiv": "1710.09412",
        "CorpusId": 3162051
    },
    "title": "mixup: Beyond Empirical Risk Minimization",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
    "venue": "International Conference on Learning Representations",
    "year": 2017,
    "referenceCount": 39,
    "citationCount": 8456,
    "influentialCitationCount": 1447,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes mixup, a simple learning principle that trains a neural network on convex combinations of pairs of examples and their labels, which improves the generalization of state-of-the-art neural network architectures."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2108880169",
            "name": "Hongyi Zhang"
        },
        {
            "authorId": "5723508",
            "name": "Moustapha Ciss\u00e9"
        },
        {
            "authorId": "2921469",
            "name": "Yann Dauphin"
        },
        {
            "authorId": "1401804750",
            "name": "David Lopez-Paz"
        }
    ],
    "references": [
        {
            "paperId": "4c75b748911ddcd888c5122f7672f69caa5d661f",
            "title": "Statistical Learning Theory"
        },
        {
            "paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341",
            "title": "Random Erasing Data Augmentation"
        },
        {
            "paperId": "9753967a3af8e1db1e2da52a9bb3255bd1ce5c51",
            "title": "Spectrally-normalized margin bounds for neural networks"
        },
        {
            "paperId": "5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
            "title": "A Closer Look at Memorization in Deep Networks"
        },
        {
            "paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5",
            "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"
        },
        {
            "paperId": "71e92abb1e504d56b3f18b4909d73eee3b6048fb",
            "title": "Sobolev Training for Neural Networks"
        },
        {
            "paperId": "255d2c2af6d7abbbebfc03dab51cd8574ad3558e",
            "title": "Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation"
        },
        {
            "paperId": "013efe3ff541e518c51f08d1b62a62e0c57c0b14",
            "title": "Parseval Networks: Improving Robustness to Adversarial Examples"
        },
        {
            "paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
            "title": "Improved Training of Wasserstein GANs"
        },
        {
            "paperId": "8edbd132765e72f5887b7ef8c38624f31dd53a77",
            "title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks"
        },
        {
            "paperId": "82070bef06578a24e63b2b739ec86d4d31bb576a",
            "title": "Dataset Augmentation in Feature Space"
        },
        {
            "paperId": "6ce1922802169f757bbafc6e087cc274a867c763",
            "title": "Regularizing Neural Networks by Penalizing Confident Output Distributions"
        },
        {
            "paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c",
            "title": "Aggregated Residual Transformations for Deep Neural Networks"
        },
        {
            "paperId": "54ddb00fa691728944fd8becea90a373d21597cf",
            "title": "Understanding deep learning requires rethinking generalization"
        },
        {
            "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "title": "Densely Connected Convolutional Networks"
        },
        {
            "paperId": "1c4e9156ca07705531e45960b7a919dc473abb51",
            "title": "Wide Residual Networks"
        },
        {
            "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "title": "Identity Mappings in Deep Residual Networks"
        },
        {
            "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "title": "Mastering the game of Go with deep neural networks and tree search"
        },
        {
            "paperId": "13497bd108d4412d02050e646235f456568cf822",
            "title": "Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin"
        },
        {
            "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "title": "Rethinking the Inception Architecture for Computer Vision"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "33af9298e5399269a12d4b9901492fe406af62b4",
            "title": "Striving for Simplicity: The All Convolutional Net"
        },
        {
            "paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "title": "Explaining and Harnessing Adversarial Examples"
        },
        {
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
            "title": "Intriguing properties of neural networks"
        },
        {
            "paperId": "5d833331b0e22ff359db05c62a8bca18c4f04b68",
            "title": "One billion word benchmark for measuring progress in statistical language modeling"
        },
        {
            "paperId": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "title": "Speech recognition with deep recurrent neural networks"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "fbd86e19157ea0e4ffb05f14d7b94603a5667e0a",
            "title": "Improving generalization performance using double backpropagation"
        },
        {
            "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
            "title": "GENERATIVE ADVERSARIAL NETS"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": null,
            "title": "UCI machine learning repository"
        },
        {
            "paperId": "31868290adf1c000c611dfc966b514d5a34e8d23",
            "title": "FUNDAMENTAL TECHNOLOGIES IN MODERN SPEECH RECOGNITION Digital Object Identifier 10.1109/MSP.2012.2205597"
        },
        {
            "paperId": "8cb44f06586f609a29d9b496cc752ec01475dffe",
            "title": "SMOTE: Synthetic Minority Over-sampling Technique"
        },
        {
            "paperId": "39664b871e5b90aa0f82d89469a230d9ecd02498",
            "title": "Vicinal Risk Minimization"
        },
        {
            "paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "title": "Gradient-based learning applied to document recognition"
        },
        {
            "paperId": "153f64ab7c1c24b1b136d8da2f36c6333b8dbfdd",
            "title": "Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent Propagation"
        },
        {
            "paperId": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "title": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
        }
    ]
}