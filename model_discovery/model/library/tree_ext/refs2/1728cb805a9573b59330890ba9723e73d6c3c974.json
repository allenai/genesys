{
    "paperId": "1728cb805a9573b59330890ba9723e73d6c3c974",
    "externalIds": {
        "DBLP": "journals/corr/abs-2006-05525",
        "MAG": "3034368386",
        "ArXiv": "2006.05525",
        "DOI": "10.1007/s11263-021-01453-z",
        "CorpusId": 219559263
    },
    "title": "Knowledge Distillation: A Survey",
    "abstract": null,
    "venue": "International Journal of Computer Vision",
    "year": 2020,
    "referenceCount": 374,
    "citationCount": 2067,
    "influentialCitationCount": 82,
    "openAccessPdf": {
        "url": "https://eprints.bbk.ac.uk/id/eprint/44038/1/KD_Survey-arxiv.pdf",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications is provided."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.2872934341430664,
            -1.3746297359466553,
            -0.07633429765701294,
            6.1605377197265625,
            -1.8557732105255127,
            0.47315430641174316,
            5.0279622077941895,
            -1.1076364517211914,
            -0.6013441681861877,
            -1.1448500156402588,
            -0.015609771013259888,
            3.3526251316070557,
            1.3462095260620117,
            0.5818207859992981,
            -2.9877843856811523,
            -1.2222011089324951,
            -0.28747251629829407,
            2.6084036827087402,
            6.578091621398926,
            0.7568615674972534,
            -2.16770601272583,
            1.2454791069030762,
            -1.2276164293289185,
            -0.6009385585784912,
            -1.8698636293411255,
            -1.530189037322998,
            2.9143154621124268,
            0.21508730947971344,
            -1.6164547204971313,
            0.3700183629989624,
            0.28523504734039307,
            -5.180723667144775,
            3.467925548553467,
            -5.153571605682373,
            3.118088960647583,
            -2.491560697555542,
            -3.117971181869507,
            6.759532928466797,
            -1.6806399822235107,
            0.10184890031814575,
            2.553288221359253,
            0.5067492127418518,
            2.5139355659484863,
            -0.39365148544311523,
            0.26645028591156006,
            0.676823616027832,
            -0.16220714151859283,
            -0.22862936556339264,
            1.6741416454315186,
            0.8268136978149414,
            1.2205877304077148,
            1.0084490776062012,
            -1.062089443206787,
            -0.6905556321144104,
            0.4077332615852356,
            0.5711976289749146,
            1.2842566967010498,
            1.8989050388336182,
            0.2868313193321228,
            0.18536926805973053,
            4.0116353034973145,
            5.623907089233398,
            1.3259637355804443,
            -0.23142409324645996,
            2.437169075012207,
            -2.1088647842407227,
            -0.17017203569412231,
            3.9064486026763916,
            0.5976368188858032,
            3.605405569076538,
            -2.021855592727661,
            -3.149691104888916,
            3.309603691101074,
            -0.016754567623138428,
            -3.6826021671295166,
            -0.7196067571640015,
            -0.3322603106498718,
            -3.6189351081848145,
            1.4487823247909546,
            -0.19578903913497925,
            0.1854119598865509,
            4.068120002746582,
            -1.720824956893921,
            2.386975049972534,
            4.166537761688232,
            0.0387251079082489,
            -1.3463237285614014,
            -2.359902858734131,
            2.194126844406128,
            -3.39201021194458,
            2.133086681365967,
            -1.1583435535430908,
            1.559666395187378,
            -0.6203822493553162,
            -3.403958320617676,
            -0.37288743257522583,
            2.0966062545776367,
            0.921292781829834,
            0.924001932144165,
            2.635903835296631,
            0.5227814316749573,
            0.5631113052368164,
            2.4777636528015137,
            0.24233800172805786,
            3.6114635467529297,
            -3.0477681159973145,
            1.498795509338379,
            2.1620430946350098,
            1.8630430698394775,
            -3.5072762966156006,
            -4.770786762237549,
            3.9912524223327637,
            -0.5596279501914978,
            -0.39675962924957275,
            -2.5057015419006348,
            -0.23723098635673523,
            0.23903656005859375,
            -0.31931769847869873,
            0.2430136799812317,
            2.2345786094665527,
            -0.020540639758110046,
            -0.9931396245956421,
            -4.810443878173828,
            0.4942035675048828,
            2.51416277885437,
            1.998258113861084,
            0.051974907517433167,
            3.299736976623535,
            -3.5073909759521484,
            -4.246631622314453,
            3.322301149368286,
            -3.0706405639648438,
            4.296615123748779,
            -0.9600298404693604,
            0.4144812822341919,
            3.0643649101257324,
            -3.273357629776001,
            1.8566182851791382,
            -1.4059650897979736,
            1.1180872917175293,
            0.4896918535232544,
            2.4658961296081543,
            -1.2131094932556152,
            0.5758373737335205,
            2.4295616149902344,
            2.4570858478546143,
            0.5504196286201477,
            -2.2237722873687744,
            -0.9388437271118164,
            3.7203946113586426,
            4.5436296463012695,
            -3.8696093559265137,
            1.1449956893920898,
            3.738072395324707,
            5.500964164733887,
            3.8024983406066895,
            -3.6378631591796875,
            2.2913033962249756,
            -1.892791748046875,
            1.1115046739578247,
            1.8704345226287842,
            -2.9245412349700928,
            -10.626955032348633,
            0.14519506692886353,
            2.3664536476135254,
            -3.9065983295440674,
            -2.7454047203063965,
            1.5747060775756836,
            -0.6155901551246643,
            3.550088405609131,
            2.4035911560058594,
            6.3292131423950195,
            3.0275959968566895,
            4.432742118835449,
            2.487241268157959,
            4.72340726852417,
            1.5719956159591675,
            -1.2936692237854004,
            -0.24269811809062958,
            2.4443535804748535,
            -2.729161500930786,
            -2.644322395324707,
            -4.5302414894104,
            1.9484919309616089,
            -1.1745085716247559,
            -0.22276532649993896,
            -1.6485618352890015,
            0.09138962626457214,
            -0.34598785638809204,
            1.2326140403747559,
            -0.3138540983200073,
            -1.1218739748001099,
            2.465928554534912,
            4.7006072998046875,
            4.200028896331787,
            -0.9001423716545105,
            -0.21877458691596985,
            2.463127374649048,
            -2.7529585361480713,
            2.734217643737793,
            4.002099990844727,
            -0.252361536026001,
            -0.7174766659736633,
            -0.1357330083847046,
            3.208852767944336,
            1.3574814796447754,
            -3.4566140174865723,
            3.432281970977783,
            0.6349930763244629,
            2.79203724861145,
            0.6222525835037231,
            -0.0933167040348053,
            -1.6981792449951172,
            1.41568124294281,
            -3.124988555908203,
            -2.2091808319091797,
            -7.239013671875,
            5.739418029785156,
            6.126269817352295,
            0.9216978549957275,
            -0.08325053006410599,
            0.33586934208869934,
            0.11078208684921265,
            -1.0545940399169922,
            3.5756566524505615,
            -3.7914509773254395,
            3.541471004486084,
            2.6593399047851562,
            0.13557234406471252,
            2.256653308868408,
            -3.27860164642334,
            -5.3284382820129395,
            2.379861831665039,
            -0.36461901664733887,
            -6.725011825561523,
            -0.9941545128822327,
            -2.4545655250549316,
            1.499107837677002,
            -0.1453235149383545,
            -0.4323936700820923,
            5.457456588745117,
            3.265859842300415,
            2.3970134258270264,
            0.9396060705184937,
            0.6794190406799316,
            -5.356900215148926,
            -1.0603829622268677,
            1.027862787246704,
            0.5500821471214294,
            -2.665614604949951,
            -0.6774656772613525,
            -1.3295512199401855,
            1.10514497756958,
            -0.056239813566207886,
            1.382676124572754,
            3.935608386993408,
            1.8775025606155396,
            2.0502891540527344,
            1.4772179126739502,
            2.069443702697754,
            -0.32266974449157715,
            5.257277011871338,
            4.569921970367432,
            4.14502477645874,
            -0.7749272584915161,
            -1.424153208732605,
            -1.3110198974609375,
            1.3245306015014648,
            -0.7219519019126892,
            0.9541116952896118,
            3.021355628967285,
            0.03162059187889099,
            -2.1047496795654297,
            -2.9792871475219727,
            -3.881974220275879,
            -2.3223836421966553,
            -3.0966944694519043,
            -0.32817938923835754,
            1.0526223182678223,
            1.4926972389221191,
            0.5252574682235718,
            -1.8998801708221436,
            -0.14502954483032227,
            -1.3775177001953125,
            0.3738134801387787,
            -3.841174602508545,
            -1.9875420331954956,
            -0.6097429990768433,
            0.40336257219314575,
            -1.4640908241271973,
            -4.1819353103637695,
            4.085821628570557,
            -3.7369844913482666,
            -0.6727017760276794,
            -2.2036261558532715,
            1.072895884513855,
            2.0480003356933594,
            0.19399622082710266,
            0.8383812308311462,
            -0.2515069246292114,
            0.1414433717727661,
            1.9383134841918945,
            1.5565335750579834,
            -2.7507569789886475,
            1.7407171726226807,
            5.893070220947266,
            2.060798406600952,
            -2.4787864685058594,
            2.9057676792144775,
            -0.8684288859367371,
            -0.20354771614074707,
            -0.194888174533844,
            2.956545114517212,
            -5.010875225067139,
            2.532526969909668,
            -1.2129814624786377,
            0.7707873582839966,
            -0.9672722816467285,
            -2.0471010208129883,
            0.67302405834198,
            0.46141988039016724,
            -1.0893571376800537,
            -5.060245037078857,
            -5.449345588684082,
            -3.0582354068756104,
            1.4384723901748657,
            3.124711275100708,
            2.855318069458008,
            -1.9213099479675293,
            2.0473618507385254,
            3.764472484588623,
            2.143683433532715,
            4.0296173095703125,
            1.9902682304382324,
            -0.6437665224075317,
            -4.595569610595703,
            -0.007347263395786285,
            -2.211622714996338,
            0.308193564414978,
            0.8527780771255493,
            0.017348513007164,
            5.4158935546875,
            -0.9744110703468323,
            2.114224433898926,
            -0.3187946081161499,
            0.9716168642044067,
            2.647078037261963,
            -0.8278279900550842,
            -0.47735151648521423,
            -2.1327319145202637,
            -2.2225003242492676,
            0.8062396049499512,
            3.256232261657715,
            -2.8049731254577637,
            3.1088621616363525,
            4.354337692260742,
            2.078312635421753,
            1.2484300136566162,
            -1.9596209526062012,
            1.2676846981048584,
            -2.048016309738159,
            0.5410996675491333,
            3.7503457069396973,
            -1.2889750003814697,
            0.24826328456401825,
            -2.03424072265625,
            8.306291580200195,
            -3.2636497020721436,
            -1.8644676208496094,
            -5.135897636413574,
            -1.0471338033676147,
            -3.9346275329589844,
            -5.367252826690674,
            2.738229274749756,
            -0.8030807375907898,
            -2.5099544525146484,
            0.4888090491294861,
            -6.276672840118408,
            2.6035714149475098,
            -0.37019944190979004,
            2.3506054878234863,
            1.4476823806762695,
            -0.8014394640922546,
            3.46773362159729,
            0.17996683716773987,
            0.0613865852355957,
            -5.073731422424316,
            3.6531972885131836,
            2.5111300945281982,
            3.1566128730773926,
            -2.1375720500946045,
            3.8780369758605957,
            0.03117235004901886,
            -0.8108470439910889,
            -2.886725425720215,
            -4.339115142822266,
            -4.4679036140441895,
            -4.743003845214844,
            0.5147598385810852,
            1.3369024991989136,
            0.24118569493293762,
            1.2595462799072266,
            4.7619476318359375,
            4.513699054718018,
            -4.570813179016113,
            -1.1590394973754883,
            2.3252601623535156,
            0.8619184494018555,
            -1.594614028930664,
            -0.7955017685890198,
            -3.215317964553833,
            -1.4223078489303589,
            -4.400182247161865,
            -3.9157798290252686,
            -1.207897663116455,
            0.20558834075927734,
            2.2052040100097656,
            2.1232612133026123,
            3.6928653717041016,
            -0.8481330871582031,
            -0.7880167961120605,
            2.119488477706909,
            4.963698387145996,
            3.7834103107452393,
            -2.2759182453155518,
            1.3671965599060059,
            0.9098650217056274,
            1.5825684070587158,
            -2.218703269958496,
            2.132211208343506,
            -1.2096755504608154,
            5.20768404006958,
            -1.7204434871673584,
            0.5917916297912598,
            -1.5345416069030762,
            2.2857248783111572,
            1.0453581809997559,
            -0.466560959815979,
            3.8446197509765625,
            -1.484427571296692,
            -0.8758348226547241,
            4.328979969024658,
            -4.120848655700684,
            3.6731510162353516,
            -0.9452741742134094,
            0.19012397527694702,
            0.5286673307418823,
            2.0855460166931152,
            -2.5593347549438477,
            -3.438413381576538,
            0.9140881299972534,
            -6.429923057556152,
            -2.718388080596924,
            -3.686953544616699,
            -0.42987483739852905,
            0.3119369447231293,
            -3.7172298431396484,
            -1.1371108293533325,
            1.1080479621887207,
            -1.3533930778503418,
            -2.8375115394592285,
            3.7451891899108887,
            -1.1401859521865845,
            0.6249645352363586,
            0.2807271480560303,
            4.732515335083008,
            -1.1968404054641724,
            -3.0830068588256836,
            -2.2360639572143555,
            0.8571550250053406,
            3.6080799102783203,
            -0.0021295100450515747,
            -1.9973739385604858,
            -0.43639886379241943,
            -0.053152620792388916,
            1.9240915775299072,
            2.865708112716675,
            4.208549976348877,
            0.5397366285324097,
            -5.484044551849365,
            -3.1728808879852295,
            -2.3915963172912598,
            2.049799919128418,
            -3.933896064758301,
            -1.604593276977539,
            0.4948801100254059,
            4.584705352783203,
            4.3587422370910645,
            1.684684157371521,
            2.2951645851135254,
            0.4738420248031616,
            0.93370521068573,
            2.7613019943237305,
            -0.7665197253227234,
            -0.5022257566452026,
            -2.3270184993743896,
            -2.3106470108032227,
            3.3590261936187744,
            3.500067710876465,
            2.022482395172119,
            -0.9972482919692993,
            -3.765899658203125,
            0.5689911842346191,
            -0.47233277559280396,
            -2.993886709213257,
            4.7011823654174805,
            2.342224359512329,
            -0.4339650273323059,
            -4.5376176834106445,
            -0.26428717374801636,
            -1.2385179996490479,
            1.838281273841858,
            -4.6968092918396,
            2.9175045490264893,
            -2.9268672466278076,
            2.6895689964294434,
            2.1382508277893066,
            -2.3730580806732178,
            -1.5620285272598267,
            -0.11601495742797852,
            -1.8457114696502686,
            -0.7161623239517212,
            0.08598992228507996,
            2.1755683422088623,
            1.9582180976867676,
            1.1752777099609375,
            -4.707213401794434,
            3.8107564449310303,
            -1.1350882053375244,
            2.7338192462921143,
            5.115246295928955,
            3.079052686691284,
            2.711648941040039,
            -2.559572458267212,
            -2.119601249694824,
            -3.828183174133301,
            1.7290093898773193,
            3.519509792327881,
            -2.0550029277801514,
            -0.05445301532745361,
            -0.16205093264579773,
            4.545690536499023,
            0.46407097578048706,
            3.015336513519287,
            -1.6015193462371826,
            1.5751872062683105,
            -4.2055768966674805,
            -0.3110215365886688,
            -3.2711822986602783,
            1.2268297672271729,
            0.5736504793167114,
            -1.9078024625778198,
            0.5636838674545288,
            -2.74696946144104,
            -4.0555644035339355,
            -1.1711668968200684,
            0.19328156113624573,
            0.4285390377044678,
            0.5668070316314697,
            2.9333508014678955,
            1.5485987663269043,
            0.32218340039253235,
            -2.147765874862671,
            1.9539295434951782,
            -2.7953290939331055,
            -1.7670854330062866,
            -2.0570309162139893,
            2.9358041286468506,
            -1.5492677688598633,
            -3.629019260406494,
            0.7165933847427368,
            -3.075802803039551,
            -1.669408917427063,
            1.3650257587432861,
            -1.987372636795044,
            1.212288498878479,
            4.551236152648926,
            5.1416144371032715,
            -4.7565765380859375,
            -0.8276249170303345,
            -3.2524871826171875,
            -4.602962493896484,
            -4.330774784088135,
            -3.3275675773620605,
            0.5873197317123413,
            -0.4082866907119751,
            -2.3872790336608887,
            1.2006536722183228,
            -4.601288318634033,
            0.24518707394599915,
            0.0030506253242492676,
            -2.402179479598999,
            -1.9145108461380005,
            -3.7799057960510254,
            0.24283576011657715,
            -3.7311840057373047,
            4.6828742027282715,
            0.9698492288589478,
            -1.0011262893676758,
            3.281749963760376,
            2.455263137817383,
            4.8326826095581055,
            2.710960626602173,
            -0.22041833400726318,
            -3.8364877700805664,
            -0.12138819694519043,
            2.841784715652466,
            0.5847684144973755,
            -1.9449774026870728,
            -0.9549627304077148,
            -2.2616207599639893,
            2.638451099395752,
            16.8799991607666,
            -0.24378104507923126,
            -0.683667778968811,
            -2.4162375926971436,
            -1.272178292274475,
            -4.081403732299805,
            0.10949738323688507,
            2.204501152038574,
            1.5819621086120605,
            1.9247348308563232,
            -0.49734655022621155,
            -2.1445693969726562,
            0.5829420685768127,
            1.026627779006958,
            0.4388914704322815,
            0.22901922464370728,
            -1.7700937986373901,
            0.19767412543296814,
            -2.296220541000366,
            0.44993942975997925,
            -1.5526821613311768,
            0.9508458971977234,
            -1.687362551689148,
            -2.685387372970581,
            -2.2473411560058594,
            4.080284595489502,
            3.5388872623443604,
            0.6517717838287354,
            -4.738182067871094,
            -0.4061569571495056,
            1.722907304763794,
            0.41866278648376465,
            1.7873598337173462,
            0.47847017645835876,
            -3.985255241394043,
            3.6514596939086914,
            4.3807220458984375,
            0.32261401414871216,
            -0.3460453450679779,
            0.46489417552948,
            -2.889042854309082,
            -0.35957276821136475,
            -2.9469070434570312,
            1.0767159461975098,
            -0.833000659942627,
            0.17854559421539307,
            1.0104988813400269,
            -1.0938299894332886,
            -2.01027250289917,
            4.117411136627197,
            -2.244356632232666,
            -1.41328763961792,
            -1.8789211511611938,
            -1.258273720741272,
            2.7516865730285645,
            4.235082149505615,
            1.566889762878418,
            -3.9973175525665283,
            1.6417444944381714,
            0.1682204008102417,
            0.348448783159256,
            0.7631427645683289,
            -3.243391990661621,
            -2.151111364364624,
            -1.1668732166290283,
            2.3360767364501953,
            -4.820823669433594,
            0.9140903353691101,
            3.4747743606567383,
            3.57053279876709,
            2.192392587661743,
            0.4714861512184143,
            1.1922276020050049,
            -2.55556321144104,
            -1.9496411085128784,
            -5.27307653427124,
            0.9662317037582397,
            -2.5098302364349365,
            1.4348764419555664,
            4.425110340118408,
            -4.264252662658691,
            4.003332614898682,
            -3.239521026611328,
            -2.3638782501220703,
            2.7801907062530518,
            -1.3475415706634521,
            4.5370941162109375,
            2.5818612575531006,
            -1.1211109161376953,
            5.642271518707275,
            4.274187088012695,
            -2.7529525756835938,
            3.8723936080932617,
            2.3109233379364014,
            1.2183711528778076,
            -6.147238731384277,
            -3.23068904876709,
            -1.2728239297866821,
            -5.285367488861084,
            -5.73115873336792,
            4.441588401794434,
            3.087371349334717,
            3.266282320022583,
            -2.6049585342407227,
            0.6305387616157532,
            -1.1132783889770508,
            -1.7453449964523315,
            -5.502129077911377,
            -7.240417957305908,
            -1.654496669769287,
            1.2197697162628174,
            -2.323549747467041,
            -0.06819826364517212,
            0.34322434663772583,
            2.831151247024536,
            -2.0541470050811768,
            -1.8513033390045166,
            3.752655029296875,
            3.8362812995910645,
            2.5569252967834473,
            1.0360569953918457,
            -0.9938298463821411,
            0.18341493606567383,
            -6.685179710388184,
            -0.05151087045669556,
            0.11705414950847626,
            0.5615270137786865,
            -2.091313362121582,
            -1.7549080848693848,
            -2.0715601444244385,
            0.9203639626502991,
            1.7754334211349487,
            -0.9530202746391296,
            0.19557325541973114,
            -1.7998374700546265,
            -1.1820846796035767,
            0.36809873580932617,
            3.7156002521514893,
            -1.0795557498931885,
            3.6476926803588867,
            2.1327245235443115,
            -2.588555097579956,
            0.8215941786766052,
            5.684380054473877,
            -0.07162994146347046,
            -0.7265965938568115,
            -0.3975016474723816,
            0.49473947286605835,
            -1.325060248374939,
            -0.5921606421470642,
            -0.42704319953918457,
            1.0105581283569336,
            0.23206135630607605,
            -2.382699728012085,
            -1.2955029010772705,
            -4.471170425415039
        ]
    },
    "authors": [
        {
            "authorId": "38978232",
            "name": "Jianping Gou"
        },
        {
            "authorId": "2425630",
            "name": "B. Yu"
        },
        {
            "authorId": "144555237",
            "name": "S. Maybank"
        },
        {
            "authorId": "143719920",
            "name": "D. Tao"
        }
    ],
    "references": [
        {
            "paperId": "ebe55cf5c8e47f9068abeefef20c84c7d0b3628c",
            "title": "Adversarial co-distillation learning for image recognition"
        },
        {
            "paperId": "f6b77e674e8abfe56eddc26240b0677dc425e24e",
            "title": "Stochasticity and Skip Connection Improve Knowledge Transfer"
        },
        {
            "paperId": "902f8845574b6e69b399384f0cabbed6f0050d78",
            "title": "Collaborative Teacher-Student Learning via Multiple Knowledge Transfer"
        },
        {
            "paperId": "f38dacf69cbe327e9c4f3622dd12525f4692cf21",
            "title": "Data-free Knowledge Distillation for Object Detection"
        },
        {
            "paperId": "a817d740f64dcc04734ece08e20e136ccff240e7",
            "title": "Learning Light-Weight Translation Models from Deep Transformer"
        },
        {
            "paperId": "e339c5d31ffc7029c1f72d567ac07b4606701c72",
            "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation"
        },
        {
            "paperId": "cc5afa8f1d35ec8fb3179bf760552da496d2838f",
            "title": "Future-Guided Incremental Transformer for Simultaneous Translation"
        },
        {
            "paperId": "f46d48e2afc0ec4c397db66557e80f42e765809a",
            "title": "Diverse Knowledge Distillation for End-to-End Person Search"
        },
        {
            "paperId": "4c3b044cc98def3defc3d562e5c0d811f7b0d200",
            "title": "LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding"
        },
        {
            "paperId": "63c966e28b471551f2d9c7a5b4c639de6c8953b0",
            "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation"
        },
        {
            "paperId": "883ea3b74ec1e26ad20f2c5c94f45e0d6db16364",
            "title": "Progressive Network Grafting for Few-Shot Knowledge Distillation"
        },
        {
            "paperId": "d4f2b9fe16b69c3c705b5426793a11f21c3b7851",
            "title": "Robust Domain Randomised Reinforcement Learning through Peer-to-Peer Distillation"
        },
        {
            "paperId": "e3dbe8d2e8d045ae5a8cfa52a520f3aa31a35a21",
            "title": "Cross-Layer Distillation with Semantic Calibration"
        },
        {
            "paperId": "c54f798cd3db3908f3790ecfdd2adb133cadc572",
            "title": "Residual error based knowledge distillation"
        },
        {
            "paperId": "8d6e707f7e36fbab242e3d88cc7991427879301a",
            "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation"
        },
        {
            "paperId": "ae06bfcb5828683b3892cb2eb6ea8fc244c4c79f",
            "title": "Online Ensemble Model Compression Using Knowledge Distillation"
        },
        {
            "paperId": "a5fa5f5a340966b28132fca9c89ea5adecc7f1da",
            "title": "Federated Knowledge Distillation"
        },
        {
            "paperId": "19477cbadc03fbbca4601a4588f21fab28edaf2e",
            "title": "Adaptive multi-teacher multi-level knowledge distillation"
        },
        {
            "paperId": "39e3c9d9546c055488c6e643ac3e793ceb80590e",
            "title": "Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search"
        },
        {
            "paperId": "e2be9fc4875f24ea13dbd66ba13c24af4556f91e",
            "title": "Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection"
        },
        {
            "paperId": "607e2974515819f856477f58583d607641d14a68",
            "title": "Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher"
        },
        {
            "paperId": "85bc7bac671da416c46b42903ab831b28c561ec9",
            "title": "Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control"
        },
        {
            "paperId": "4f90326d1d032ce3ce1ce54fe762318781c173bb",
            "title": "KT-GAN: Knowledge-Transfer Generative Adversarial Network for Text-to-Image Synthesis"
        },
        {
            "paperId": "261aa442c219e6a388642d51834740bdb863a30a",
            "title": "Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks"
        },
        {
            "paperId": "a1027f7c27273c8c0aac30bbd31da87a0fa342db",
            "title": "Discriminability Distillation in Group Representation Learning"
        },
        {
            "paperId": "0679bd79dfef440f9e2d95450538bd81ee690188",
            "title": "Local Correlation Consistency for Knowledge Distillation"
        },
        {
            "paperId": "b535a9b6cb51a6644fbf35c4fb2561132084168a",
            "title": "Matching Guided Distillation"
        },
        {
            "paperId": "412180449eed7199d9a234f5f018d2b0eaf2fa1d",
            "title": "Knowledge Transfer via Dense Cross-Layer Mutual-Distillation"
        },
        {
            "paperId": "9d14eedd992081c5011d43747b80535629f68372",
            "title": "Prime-Aware Adaptive Distillation"
        },
        {
            "paperId": "c45953cd3aa87760a61f72775501b1fb7d13c87d",
            "title": "Differentiable Feature Aggregation Search for Knowledge Distillation"
        },
        {
            "paperId": "70de83272caac9af3bb1dd6a159ec3c1b314ff8e",
            "title": "CKD: Cross-Task Knowledge Distillation for Text-to-Image Synthesis"
        },
        {
            "paperId": "c6c023c2209ce2b8f593dfea0b5b88493c0c00e3",
            "title": "Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge"
        },
        {
            "paperId": "f6e2ea9137d83620864ce6f814fcf048c8fecfb0",
            "title": "Defocus Blur Detection via Depth Distillation"
        },
        {
            "paperId": "40c0a66a491778ea00c26b4c2709e50983e6ffe6",
            "title": "BERT-INT: A BERT-based Interaction Model For Knowledge Graph Alignment"
        },
        {
            "paperId": "3d856d797356ab32be936e74a7eed39766bfc0d3",
            "title": "Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation"
        },
        {
            "paperId": "6b67b1f55ebad02eaa73328c3989d64c1dc23dff",
            "title": "Flexible Dataset Distillation: Learn Labels Instead of Images"
        },
        {
            "paperId": "e4bf68ddfd2a1ca5cf4326b8795f8b88ef86e148",
            "title": "Knowledge Distillation Meets Self-Supervision"
        },
        {
            "paperId": "053f4d6715a4dba6f8103456fc1bb5fd6a5266c4",
            "title": "Ensemble Distillation for Robust Model Fusion in Federated Learning"
        },
        {
            "paperId": "2bdfc6d8f6d03b38b80b8aa4112088323b6b552f",
            "title": "Self-Distillation as Instance-Specific Label Smoothing"
        },
        {
            "paperId": "a7058717ce5d37094fed32147d3b98d0edc0df16",
            "title": "Peer Collaborative Learning for Online Knowledge Distillation"
        },
        {
            "paperId": "cc384eecd111f08e22a1e8e111129ea42aa11646",
            "title": "Probabilistic Knowledge Transfer for Lightweight Deep Representation Learning"
        },
        {
            "paperId": "00a656134e13d82bf96fb23ac7c71d2a489bb85e",
            "title": "A Multi-Task Mean Teacher for Semi-Supervised Shadow Detection"
        },
        {
            "paperId": "7dfadd8a0c31dc1fd558268deb88de2359e24ef4",
            "title": "Online Knowledge Distillation via Collaborative Learning"
        },
        {
            "paperId": "2d664a33cfdff7de78167710eac995aaf34c9003",
            "title": "Reliable Data Distillation on Graph Convolutional Network"
        },
        {
            "paperId": "3f68de71a79d0f689902f8709a6a6259be30d9b1",
            "title": "Efficient Low-Resolution Face Recognition via Bridge Distillation"
        },
        {
            "paperId": "df9cf466b04975070d071d765b5e6e4b0a07a127",
            "title": "Heterogeneous Knowledge Distillation Using Information Flow Modeling"
        },
        {
            "paperId": "699b29745ec527e4d3b08ea38adf34888a85469f",
            "title": "Improving Non-autoregressive Neural Machine Translation with Monolingual Data"
        },
        {
            "paperId": "a38b47e73cc60c839d275663ecb1de29cc67d04a",
            "title": "Adaptive Knowledge Distillation Based on Entropy"
        },
        {
            "paperId": "7493604c39962bd488480a488e913ac1c49d65f0",
            "title": "Understanding Generalization in Recurrent Neural Networks"
        },
        {
            "paperId": "2528a82dd2266600d4ee2b54165556a984de94d4",
            "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks"
        },
        {
            "paperId": "a837b3144c73797e79cfc5da1b92b09c618263b1",
            "title": "Inter-Region Affinity Distillation for Road Marking Segmentation"
        },
        {
            "paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa",
            "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"
        },
        {
            "paperId": "dd08eb4ab7d1cd9e3fa02c70f24426baf6ab8f2b",
            "title": "Knowledge As Priors: Cross-Modal Knowledge Generalization for Datasets Without Superior Knowledge"
        },
        {
            "paperId": "f669a973d3ac83f9dea52b8a629a2ae48be4532f",
            "title": "Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing"
        },
        {
            "paperId": "59ed3740b1c4b681d629128dc07ad492cb5da86e",
            "title": "Regularizing Class-Wise Predictions via Self-Knowledge Distillation"
        },
        {
            "paperId": "a444d32f30d38fb0cb811fa1a9b601511244fb5b",
            "title": "Spatio-Temporal Graph for Video Captioning With Knowledge Distillation"
        },
        {
            "paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582",
            "title": "Designing Network Design Spaces"
        },
        {
            "paperId": "1c449fe4bb3ad1f66372d15ce433a2e818299366",
            "title": "Distilling Knowledge From Graph Convolutional Networks"
        },
        {
            "paperId": "ae5405fd06e96de5a0a9e8cffde76d9064c249d9",
            "title": "Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN"
        },
        {
            "paperId": "b6207686ecf40f1eb1752425299f265dffbc4abe",
            "title": "GAN Compression: Efficient Architectures for Interactive Conditional GANs"
        },
        {
            "paperId": "3913afb4d9747dcbf27e75e75dcf4c48e6da00b2",
            "title": "Knowledge distillation via adaptive instance normalization"
        },
        {
            "paperId": "8ff5aa002316b7d15838d571cadfaa87cf57610b",
            "title": "Explaining Knowledge Distillation by Quantifying the Knowledge"
        },
        {
            "paperId": "1e889d45eaaa12d44b2a4c30b826934b749f9de8",
            "title": "Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning"
        },
        {
            "paperId": "501a8b86428563539667e8117cd8409674ef97c3",
            "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"
        },
        {
            "paperId": "f1dd557a8839733a5ee06d19989a265e61f603c1",
            "title": "Object Relational Graph With Teacher-Recommended Learning for Video Captioning"
        },
        {
            "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
            "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
        },
        {
            "paperId": "edd800c4912993d472e005e4a436f769823a12cf",
            "title": "Residual Knowledge Distillation"
        },
        {
            "paperId": "65e170ba65e94ea7310367fc5540fbb9629010b5",
            "title": "Knowledge Integration Networks for Action Recognition"
        },
        {
            "paperId": "f74cb4f88f0023bbc350786808ba84c1b7833cec",
            "title": "Self-Distillation Amplifies Regularization in Hilbert Space"
        },
        {
            "paperId": "f2757aa80c3dbb86f7a7034fc193e6bb61a2171e",
            "title": "Understanding and Improving Knowledge Distillation"
        },
        {
            "paperId": "0e488f4f83303868b9fa9f807c4ad5c703fd7d61",
            "title": "Feature-map-level Online Adversarial Knowledge Distillation"
        },
        {
            "paperId": "5d13dd08292c3df308a6dffaa49ef2e574de2c15",
            "title": "Search for Better Students to Learn Distilled Knowledge"
        },
        {
            "paperId": "d7c6b3628725638188dee085b58752f754697d99",
            "title": "Learning an Evolutionary Embedding via Massive Knowledge Distillation"
        },
        {
            "paperId": "f1faaa76fa93138ca2c2a0c95d31b6e84929a5a4",
            "title": "Unpaired Multi-Modal Segmentation via Knowledge Distillation"
        },
        {
            "paperId": "0dd89a3b906e9a90a47834dcc766a0b9967d10d1",
            "title": "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion"
        },
        {
            "paperId": "6b1e454d6debd8be885806164e15ccf1599410ab",
            "title": "Joint architecture and knowledge distillation in CNN for Chinese text recognition"
        },
        {
            "paperId": "d1ba8c532b954ea8b3a66d9c155e769fc2081af6",
            "title": "Explaining Sequence-Level Knowledge Distillation as Data-Augmentation for Neural Machine Translation"
        },
        {
            "paperId": "05906bdfe1bec0a4f5a4f32d765a3422772505b7",
            "title": "The Knowledge Within: Methods for Data-Free Model Compression"
        },
        {
            "paperId": "35d39c2f61277a89d09dc899fa467ade6a3789af",
            "title": "Online Knowledge Distillation with Diverse Peers"
        },
        {
            "paperId": "72eeb6c1ad990880823d10455fa86a734f2b1b96",
            "title": "Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation"
        },
        {
            "paperId": "2fda7941f2cc8e15417b82ec13ffa04b05572057",
            "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search"
        },
        {
            "paperId": "c038b9997979a9a13d24184e5e2bea7217bbdd24",
            "title": "QKD: Quantization-aware Knowledge Distillation"
        },
        {
            "paperId": "bc1c8c9b571039976a9dc6f78dc8d0521f48a1fb",
            "title": "Few Shot Network Compression via Cross Distillation"
        },
        {
            "paperId": "63dda2ff703c22cfdfac6d7cd0fc64b0dd170e2d",
            "title": "Search to Distill: Pearls Are Everywhere but Not the Eyes"
        },
        {
            "paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d",
            "title": "Self-Training With Noisy Student Improves ImageNet Classification"
        },
        {
            "paperId": "206152e7e8f5c17db2ea112d4c2783d1b9e6e342",
            "title": "Privileged Modality Distillation for Vessel Border Detection in Intracoronary Imaging"
        },
        {
            "paperId": "6a3f87ef19c707e347c3fd8010b4e784065ef613",
            "title": "Knowledge Distillation in Document Retrieval"
        },
        {
            "paperId": "d4c773179291a5089730ce4f0940f4bcec62abee",
            "title": "Graph Representation Learning via Multi-task Knowledge Distillation"
        },
        {
            "paperId": "75352cc69a29bd5fc411e0e79737cb96b6309161",
            "title": "Distilling Knowledge Learned in BERT for Text Generation"
        },
        {
            "paperId": "1e5b826ddf0754f6e93234ba1260bd939c255e7f",
            "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation"
        },
        {
            "paperId": "41747cbdbed84762dfbfc305254c97021279dc6e",
            "title": "Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings"
        },
        {
            "paperId": "33bd59074d5e3e46a9417aea31917ccdf43fd535",
            "title": "Complete Random Forest Based Class Noise Filtering Learning for Improving the Generalizability of Classifiers"
        },
        {
            "paperId": "197498cb2ad787e4f71c05098cee6b10d9d067bd",
            "title": "Contrastive Representation Distillation"
        },
        {
            "paperId": "3de9d381813ec99441a55f248c41570410e4062b",
            "title": "Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System"
        },
        {
            "paperId": "6bfa8f8b09ec21fe06d4eee28c3fb6cce879be1d",
            "title": "Self-supervised Label Augmentation via Input Transformations"
        },
        {
            "paperId": "b6b2ebcd2f1b25ca87813832a60ab056c008e31d",
            "title": "Knowledge Distillation from Internal Representations"
        },
        {
            "paperId": "ca1b417cf0f7906b52851e650ee0c7b9f85a5089",
            "title": "Graph Few-shot Learning via Knowledge Transfer"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "f729f2270fc8f1795241e65faa0e5b5507e70be6",
            "title": "VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition"
        },
        {
            "paperId": "9d047a9c96d1e929846b28a44498a230fffee06f",
            "title": "Few-Shot Image Recognition With Knowledge Transfer"
        },
        {
            "paperId": "006e3d431e103c51557dac008a63c3cd3069bce4",
            "title": "Improved Knowledge Distillation for Training Fast Low Resolution Face Recognition Model"
        },
        {
            "paperId": "8de7f044a673d1f5e3b454d0663811f91aa9811a",
            "title": "On the Efficacy of Knowledge Distillation"
        },
        {
            "paperId": "544e39cb689e937d8954f139366346c45ac08dd9",
            "title": "Distillation-Based Training for Multi-Exit Architectures"
        },
        {
            "paperId": "31cd3291e43588a48d4f3c5e1fac1ac38435ca6d",
            "title": "Training convolutional neural networks with cheap convolutions and online distillation"
        },
        {
            "paperId": "27cb0b42e0573c4891ae2ca444776dee57bfe2ac",
            "title": "Compact Trilinear Interaction for Visual Question Answering"
        },
        {
            "paperId": "3ebb1181b49979ac68d02ec4eb5ae77d0bdd1a19",
            "title": "Two-stage Image Classification Supervised by a Single Teacher Single Student Model"
        },
        {
            "paperId": "88d08a5bd00a301455e0f6a454642550775fb427",
            "title": "Rethinking Data Augmentation: Self-Supervision and Self-Distillation"
        },
        {
            "paperId": "ea053ee44eefc52ed79604a00a145dd885657c01",
            "title": "Regularizing Predictions via Class-wise Self-knowledge Distillation"
        },
        {
            "paperId": "bf96f5c2d68f73d3f4d45603a5ccb803cdd92ea8",
            "title": "Revisit Knowledge Distillation: a Teacher-free Framework"
        },
        {
            "paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97",
            "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"
        },
        {
            "paperId": "cfa1a6eff349cec56323a0f39ae028dbcb4841a4",
            "title": "FEED: Feature-level Ensemble for Knowledge Distillation"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "11439c0a2b39f1ea2c228f121c3a205e491c315b",
            "title": "Ensemble Knowledge Distillation for Learning Improved and Efficient Networks"
        },
        {
            "paperId": "3117ac0b3946a06218eeb1cdf2c4005e8ed5b866",
            "title": "Knowledge Transfer Graph for Deep Collaborative Learning"
        },
        {
            "paperId": "838fb86e9a77a1bf7337071798062129337b58a4",
            "title": "Knowledge Extraction with No Observable Data"
        },
        {
            "paperId": "f84af831a8406fb3d66a36632fe14904e9b70386",
            "title": "Knowledge Distillation for Optimization of Quantized Deep Neural Networks"
        },
        {
            "paperId": "69e232a29aaf30bce388fdd30b4dadfcd627a9cd",
            "title": "Empirical Analysis of Knowledge Distillation Technique for Optimization of Quantized Deep Neural Networks"
        },
        {
            "paperId": "0ae4162693c0b1b7cd9eaa048d112d89cc0de75c",
            "title": "Knowledge Distillation for End-to-End Person Search"
        },
        {
            "paperId": "ac32b94a87b145cd82521ddaf5954f261799e28e",
            "title": "Cross-Modal Knowledge Distillation for Action Recognition"
        },
        {
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression"
        },
        {
            "paperId": "fe7c446bd038999ba120bcc27cab958a0f166339",
            "title": "Dynamic Kernel Distillation for Efficient Pose Estimation in Videos"
        },
        {
            "paperId": "93ad19fbc85360043988fa9ea7932b7fdf1fa948",
            "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"
        },
        {
            "paperId": "672983edfa927ecb786811e3aca49e0d14c1ede9",
            "title": "Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation"
        },
        {
            "paperId": "caca60425cea701fe087d5e8d48047b863cfe614",
            "title": "Adaptive Regularization of Labels"
        },
        {
            "paperId": "47e5adcdb42e5be764218e0ba7c50202ce3cec93",
            "title": "UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation"
        },
        {
            "paperId": "6741eb1ce7837eed6da6402b456b55cf9e626d63",
            "title": "Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection"
        },
        {
            "paperId": "70ac49e014e5679b5557c3f6a3e0238122abb623",
            "title": "Learning Lightweight Lane Detection CNNs by Self Attention Distillation"
        },
        {
            "paperId": "8271311ceeabe333d4555deedcd3926b2145314a",
            "title": "Self-Knowledge Distillation in Natural Language Processing"
        },
        {
            "paperId": "f6f49af9244e1081ee862c7839e84eef09ec7b34",
            "title": "Distilling Knowledge From a Deep Pose Regressor Network"
        },
        {
            "paperId": "04d27bbbc875bd8fe52521112841d47b21950e7c",
            "title": "Spatiotemporal distilled dense-connectivity network for video action recognition"
        },
        {
            "paperId": "ea4b3030aa9bf851a28956db3ddd3cf0225eb43b",
            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching"
        },
        {
            "paperId": "b48606f226ab6b9a4c391a0bd0e60987de818de4",
            "title": "Lifelong GAN: Continual Learning for Conditional Image Generation"
        },
        {
            "paperId": "9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08",
            "title": "Similarity-Preserving Knowledge Distillation"
        },
        {
            "paperId": "1669bd15bf1d6d1569bd2d5ee1bd6c500e4486ef",
            "title": "Exploiting the Ground-Truth: An Adversarial Imitation Based Knowledge Distillation Approach for Event Detection"
        },
        {
            "paperId": "d40b927d9db1e0f2dc6a084ac95698863480e71d",
            "title": "Data-Distortion Guided Self-Distillation for Deep Neural Networks"
        },
        {
            "paperId": "aeec7b8ae2b63d0b8ffcce74a81677d48ad2c691",
            "title": "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition"
        },
        {
            "paperId": "ef6948edae12eba6f1d486b8600108b9762f36ab",
            "title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding"
        },
        {
            "paperId": "0120f50562552d8a49bec2261166e1e8ba8e0aaa",
            "title": "A Two-Teacher Framework for Knowledge Distillation"
        },
        {
            "paperId": "b959851238988b4683092f0bcb38d2e9c6957c1f",
            "title": "Graph-based Knowledge Distillation by Multi-head Attention Network"
        },
        {
            "paperId": "d465dc8b416857a90b35f762376a9a636ca7b578",
            "title": "Compression of Acoustic Event Detection Models With Quantized Distillation"
        },
        {
            "paperId": "d37f03d397a102eb8958f5942161b564d95a9338",
            "title": "GAN-Knowledge Distillation for One-Stage Object Detection"
        },
        {
            "paperId": "f8de25118af2abc4c48afb947d6ec298e05ef1e5",
            "title": "When Does Label Smoothing Help?"
        },
        {
            "paperId": "36894fbbc8bf40e5563048e00e0c298fdc727710",
            "title": "Online Distilling from Checkpoints for Neural Machine Translation"
        },
        {
            "paperId": "b82b9427ec8e49f7e9fd6757f7fce6bd4bfb0bf4",
            "title": "On Knowledge distillation from complex networks for response prediction"
        },
        {
            "paperId": "66e901e3cbe20152240e6e477e55dce4e6d3b1e1",
            "title": "Distilled Person Re-Identification: Towards a More Scalable System"
        },
        {
            "paperId": "48c601d0029c25ba02480c473d1bd31960acb2e2",
            "title": "Progressive Teacher-Student Learning for Early Action Prediction"
        },
        {
            "paperId": "ad20c60cd67e94841b63b1746b5dce42646dcb8f",
            "title": "Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation"
        },
        {
            "paperId": "99550cbf48c0eab75cb66594d2feff56b5f5510e",
            "title": "DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation"
        },
        {
            "paperId": "9e145fc992e983a8d7c193a8933d08532194ae67",
            "title": "CrDoCo: Pixel-Level Domain Transfer With Cross-Domain Consistency"
        },
        {
            "paperId": "7e488d0ad51fba6b1cde75c439af3052e9d405f3",
            "title": "Distilling Object Detectors With Fine-Grained Feature Imitation"
        },
        {
            "paperId": "d697f212009159cc97f383c9ee39dc97341abe16",
            "title": "Knowledge Distillation via Instance Relationship Graph"
        },
        {
            "paperId": "4211b0ebaede85cf7e15867dab1631b0a4da8f03",
            "title": "Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning"
        },
        {
            "paperId": "d8b997237a30f7fd87a824c065597b759f5be72f",
            "title": "Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and Residual Knowledge Distillation"
        },
        {
            "paperId": "35ddd4896d9396d5dcff3f32bd79ff7cf3e82777",
            "title": "ShrinkTeaNet: Million-scale Lightweight Face Recognition via Shrinking Teacher-Student Networks"
        },
        {
            "paperId": "247d6c9cb92d4afabee20b0ea29ac3d8ea92f120",
            "title": "Towards Understanding Knowledge Distillation"
        },
        {
            "paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
        },
        {
            "paperId": "38098abafe05fe839af09e41104998109e741e95",
            "title": "Adversarially Robust Distillation"
        },
        {
            "paperId": "c9f3cf0996a67114d30e3bfbf549f5c6721a5388",
            "title": "Zero-shot Knowledge Transfer via Adversarial Belief Matching"
        },
        {
            "paperId": "eced39bd4b6aa1097a7615dd0c502e4a7510796a",
            "title": "Zero-Shot Knowledge Distillation in Deep Networks"
        },
        {
            "paperId": "a8cab29d2230924dffe89d6dda15ba42790c5ebf",
            "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation"
        },
        {
            "paperId": "290357314d0c339bcce31cfbe6b29aa50f89b026",
            "title": "Learning What and Where to Transfer"
        },
        {
            "paperId": "e33ebb37c2692c3839956bda2ad801464c8d2d42",
            "title": "Low-resolution Visual Recognition via Deep Feature Distillation"
        },
        {
            "paperId": "a0ff692a03f8ed4b6831baba6cd51dc245ebc706",
            "title": "Interactive Learning of Teacher-student Model for Short Utterance Spoken Language Identification"
        },
        {
            "paperId": "8cb847b8ff42194165bd3cc55368e8df15c992f7",
            "title": "Multi-teacher Knowledge Distillation for Compressed Video Action Recognition on Deep Neural Networks"
        },
        {
            "paperId": "a7727311ea4d8e546c5489366d4c01ffc93c960e",
            "title": "Semi-supervised Acoustic Event Detection Based on Tri-training"
        },
        {
            "paperId": "c4a2a5e01a9ce9a2dda4f2f2f5d59518e5b9931d",
            "title": "Conditional Teacher-student Learning"
        },
        {
            "paperId": "c4a0167ff3fe9d64e563b881603a3631028692d7",
            "title": "Student Becoming the Master: Knowledge Amalgamation for Joint Scene Parsing, Depth Estimation, and More"
        },
        {
            "paperId": "abcbfa04b14ee6cc76afe8c5e504daf815d14fa2",
            "title": "TextKD-GAN: Text Generation Using Knowledge Distillation and Generative Adversarial Networks"
        },
        {
            "paperId": "7ebed46b7f3ec913e508e6468304fcaea832eda1",
            "title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"
        },
        {
            "paperId": "b50995cb56ffb0f8a72aea996a93e149baa74836",
            "title": "Feature Fusion for Online Mutual Knowledge Distillation"
        },
        {
            "paperId": "7b0ed3d67375a4542133c992f4e55fd4ade0cd90",
            "title": "Knowledge Distillation via Route Constrained Optimization"
        },
        {
            "paperId": "f7c6ab8303a03dc9e8454c070030c3e6b0233d40",
            "title": "Audio-Visual Model Distillation Using Acoustic Images"
        },
        {
            "paperId": "eaff90799e71e3f1a03c4ace4af8864ada9b2693",
            "title": "Unifying Heterogeneous Classifiers With Distillation"
        },
        {
            "paperId": "6dd986b2621d7f420ca80be5b71e12dece41e877",
            "title": "Knowledge Flow: Improve Upon Your Teachers"
        },
        {
            "paperId": "92ebadf9913e6800331e5f9b2699812fe77313ff",
            "title": "Variational Information Distillation for Knowledge Transfer"
        },
        {
            "paperId": "3f76324a4a26ff3d9097642676878aa7b1122581",
            "title": "Spatiotemporal Knowledge Distillation for Efficient Estimation of Aerial Video Saliency"
        },
        {
            "paperId": "0f736d2067ee9c950b876f14521268c6009e67d6",
            "title": "Relational Knowledge Distillation"
        },
        {
            "paperId": "515844aeb6705cbbc0cc899de66a631abf4b8311",
            "title": "Knowledge Squeezed Adversarial Network Compression"
        },
        {
            "paperId": "5a2304ba4e4401db2e0df8188a5f761646b52480",
            "title": "Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization"
        },
        {
            "paperId": "5ad7c05fe1c9837793df0f1db62deffb788fa4a8",
            "title": "Learning Metrics From Teachers: Compact Networks for Image Embedding"
        },
        {
            "paperId": "d974d0bdba8974e5bafb5211efbb8303c7fdc540",
            "title": "Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image Retrieval"
        },
        {
            "paperId": "360d05b60c0c8e86d191796e0774e7a7522294c3",
            "title": "White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks"
        },
        {
            "paperId": "b096edfdecaf44c3783ee71cda9bf829809d0f17",
            "title": "A Comprehensive Overhaul of Feature Distillation"
        },
        {
            "paperId": "1ffb4eb0cc564673cc8908e0887c2fb740dd3efd",
            "title": "M2KD: Multi-model and Multi-level Knowledge Distillation for Incremental Learning"
        },
        {
            "paperId": "c4696d47e0d43a6761742e15c262a224466c4b85",
            "title": "Correlation Congruence for Knowledge Distillation"
        },
        {
            "paperId": "ebf9cb4e9e59ad1b50781dee168a5fc74cb8dde3",
            "title": "Why ResNet Works? Residuals Generalize"
        },
        {
            "paperId": "947928b4e7ff7ebb4a65d88d9c553a1fe5da7070",
            "title": "Data-Free Learning of Student Networks"
        },
        {
            "paperId": "bc789aef715498e79a74f857fa090ece9e383bf1",
            "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"
        },
        {
            "paperId": "8a5602feba578b3fff78d2e19587657bde8143b5",
            "title": "Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild"
        },
        {
            "paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89",
            "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"
        },
        {
            "paperId": "2f276b84e6045c9f91e0fe99014ea502f7e6700f",
            "title": "Improving Neural Architecture Search Image Classifiers via Ensemble Learning"
        },
        {
            "paperId": "a7e2c8c52c5a0202c11d9d6b3fe3427976608c1b",
            "title": "Knowledge Adaptation for Efficient Semantic Segmentation"
        },
        {
            "paperId": "38be3697a9cd4bf50853d0c2e226e8a5bf9aa052",
            "title": "Structured Knowledge Distillation for Semantic Segmentation"
        },
        {
            "paperId": "358a85b85f3dec83ffaa50c2252615e7a1a42483",
            "title": "Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation"
        },
        {
            "paperId": "3203324b938259ad620f3cc39c221e7714eeb354",
            "title": "A novel Enhanced Collaborative Autoencoder with knowledge distillation for top-N recommender systems"
        },
        {
            "paperId": "58f32f1e294569f88d20892c11b389105da9c615",
            "title": "Efficient Video Classification Using Fewer Frames"
        },
        {
            "paperId": "24466bb59d19206ce027729f238b68a848d47f94",
            "title": "DDFlow: Learning Optical Flow with Unlabeled Data Distillation"
        },
        {
            "paperId": "db4cae4bd16c6c6d581bca24d80fa40341d203c5",
            "title": "Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes"
        },
        {
            "paperId": "bc6dfc6bda2d929fec91042dce1831fd07999b39",
            "title": "Improved Knowledge Distillation via Teacher Assistant"
        },
        {
            "paperId": "425482094edea7deef11287bc27d73b84ee7c32e",
            "title": "Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher"
        },
        {
            "paperId": "1b24b7b4ac2427d20ab60c8451563eb8d99caf9c",
            "title": "Multilingual Neural Machine Translation with Knowledge Distillation"
        },
        {
            "paperId": "e7f2dc1c52dfce9c29f879808c7a6a601845d7bc",
            "title": "Compressing GANs using Knowledge Distillation"
        },
        {
            "paperId": "bc38dbf06fe17b3c81001dcb939ae0f0a432f0b6",
            "title": "Learning Student Networks via Feature Embedding"
        },
        {
            "paperId": "381673e9b16a99cd06d1d226d4262223de343f72",
            "title": "Adversarial Distillation for Efficient Recommendation with External Knowledge"
        },
        {
            "paperId": "ac12def5e116402d992b7b13b37fbebd75a6ded4",
            "title": "Spatial Knowledge Distillation to Aid Visual Reasoning"
        },
        {
            "paperId": "45532bffbfbb5553da0b2d0844e95a1b37e59147",
            "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search"
        },
        {
            "paperId": "df32b9ab3ef7484fd5077f807492a70111a92a0d",
            "title": "MEAL: Multi-Model Ensemble via Adversarial Learning"
        },
        {
            "paperId": "b042f9d9a5aa81451abbb252aa45bf74e8fa64cb",
            "title": "Online Model Distillation for Efficient Video Inference"
        },
        {
            "paperId": "60b2e0b0f91432aa7e6500d6d0f92d391c96e717",
            "title": "Teacher-Student Compression with Generative Adversarial Networks"
        },
        {
            "paperId": "0d737e86745903d6a9ab5c6c21d2ce74fe10164f",
            "title": "Knowledge Distillation with Feature Maps for Image Classification"
        },
        {
            "paperId": "a167d8a4ee261540c2b709dde2d94572c6ea3fc8",
            "title": "Snapshot Distillation: Teacher-Student Optimization in One Generation"
        },
        {
            "paperId": "8bc85c70b671cd9c4932290f1bd7cdb3822fe9d3",
            "title": "Teacher-Student Training for Text-Independent Speaker Recognition"
        },
        {
            "paperId": "07b1a0ed6ba8a497355ac105e9110a927e3cf913",
            "title": "Dataset Distillation"
        },
        {
            "paperId": "c50e498ede6f5216cffd0645e747ce67fae2096a",
            "title": "Low-Resolution Face Recognition in the Wild via Selective Knowledge Distillation"
        },
        {
            "paperId": "1dcda812bc1cd62c4f930b2a01293d9a232d2c76",
            "title": "Self-Referenced Deep Learning"
        },
        {
            "paperId": "652f431670405ebc56148d77c290a07c8408136f",
            "title": "Fast Human Pose Estimation"
        },
        {
            "paperId": "183e10cfcdaecbbe51a8b962e42a1259d18a2068",
            "title": "Private Model Compression via Knowledge Distillation"
        },
        {
            "paperId": "611fe6e34df07ea1b2104899e49642b4531b53e9",
            "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"
        },
        {
            "paperId": "8cb3000e8959d1065532d54a07cf8fe97ef6b9c6",
            "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons"
        },
        {
            "paperId": "769c79021d39968102a27981e85fc105fc64bdb1",
            "title": "Amalgamating Knowledge towards Comprehensive Classification"
        },
        {
            "paperId": "b4d6bb9700de0ac61f7b3cb284cf269c9a57f283",
            "title": "Cogni-Net: Cognitive Feature Learning Through Deep Visual Perception"
        },
        {
            "paperId": "ad7fd5298a575b393fc3989a8d358bbf044fbbbd",
            "title": "Improving the Interpretability of Deep Neural Networks with Knowledge Distillation"
        },
        {
            "paperId": "872e53f04ec4d5eeefc477ff92f29b1cb4636ffa",
            "title": "KTAN: Knowledge Transfer Adversarial Network"
        },
        {
            "paperId": "d2ca9c6971652589cf67f3e33d3e3e9a9e0cac62",
            "title": "Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting"
        },
        {
            "paperId": "09e49c88eefc9ecfd9e2e8dff5141ff6bfeb2747",
            "title": "Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem"
        },
        {
            "paperId": "82d6794d7898fbbafc64bc2432ba3a6c951a7872",
            "title": "Discover the Effective Strategy for Face Recognition Model Compression by Improved Knowledge Distillation"
        },
        {
            "paperId": "0c37a1feac9a09a2fa5554cb7ff735d4bcd6ed5b",
            "title": "Transferring Knowledge across Learning Processes"
        },
        {
            "paperId": "70cf4e030af070f1a596b413ebe32ee1e054bc8f",
            "title": "Model Compression with Generative Adversarial Networks"
        },
        {
            "paperId": "c951998e95ba8066569ae36c33c28d65f2ff92c5",
            "title": "Knowledge Distillation from Few Samples"
        },
        {
            "paperId": "42800a905cd5c1b55a9cd6157478d6f9d933c3f1",
            "title": "Advancing Multi-Accented Lstm-CTC Speech Recognition Using a Domain Specific Student-Teacher Learning Paradigm"
        },
        {
            "paperId": "273cfe6b6bc058af386658f58ec412cc005c5f50",
            "title": "Feature Representation of Short Utterances Based on Knowledge Distillation for Spoken Language Identification"
        },
        {
            "paperId": "ec2565196b3d843c39aee5f2341d624058cd231a",
            "title": "Knowledge Distillation for Sequence Model"
        },
        {
            "paperId": "48fdf50da3d2bbd3b85ea9d17bbf3d173f6164ea",
            "title": "Attention-Guided Answer Distillation for Machine Reading Comprehension"
        },
        {
            "paperId": "540831094fd9b80469c8dacb9320b7e342b50e03",
            "title": "Emotion Recognition in Speech using Cross-Modal Transfer in the Wild"
        },
        {
            "paperId": "f5c7d0998b4cf8de9b5b171e3593427724ec4600",
            "title": "Lifelong Machine Learning, Second Edition"
        },
        {
            "paperId": "f1dd97952cfdd5a8672faa4c6d33321314003323",
            "title": "Teacher Guided Architecture Search"
        },
        {
            "paperId": "693c97ecedb0a84539b7162c95e89fa3cd84ca73",
            "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile"
        },
        {
            "paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e",
            "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"
        },
        {
            "paperId": "7f01c6fe27f57ee6191b51efa18b9199baf7b82a",
            "title": "Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System"
        },
        {
            "paperId": "ea0e55bb82feceef4b8b828dc5ccdb50beff761a",
            "title": "Self-supervised Knowledge Distillation Using Singular Value Decomposition"
        },
        {
            "paperId": "a7375343b2c90c230a028de0ad77c08c0118d3ed",
            "title": "Progressive Blockwise Knowledge Distillation for Neural Network Acceleration"
        },
        {
            "paperId": "9882468937742a0a6e27e8b86d0211e6d2a64965",
            "title": "Teaching Semi-Supervised Classifier via Generalized Distillation"
        },
        {
            "paperId": "4f472fb027775554187fa3688a95aff9c3c5d977",
            "title": "Modality Distillation with Multiple Stream Networks for Action Recognition"
        },
        {
            "paperId": "c864e3785a9aecf25296781c272980eaed78e51a",
            "title": "Knowledge Distillation by On-the-Fly Native Ensemble"
        },
        {
            "paperId": "e42b2981f4e8de54213d624d1ef12bad4fe02f0a",
            "title": "Through-Wall Human Pose Estimation Using Radio Signals"
        },
        {
            "paperId": "ffc8e2f565e29b6718d1a823e2824eb62fe2a3ed",
            "title": "Knowledge Distillation with Adversarial Samples Supporting Decision Boundary"
        },
        {
            "paperId": "e2c72b79c2f3ca6b980c540b821323467456ad4a",
            "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students"
        },
        {
            "paperId": "4f9e19e349a0e034d840496caca0c9377d8c5ce4",
            "title": "Knowledge Distillation in Generations: More Tolerant Teachers Educate Better Students"
        },
        {
            "paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1",
            "title": "Born Again Neural Networks"
        },
        {
            "paperId": "ab25d65142b736dbae006bf2f268b39659dd44c7",
            "title": "Label Refinery: Improving ImageNet Classification through Label Progression"
        },
        {
            "paperId": "aab3561acbd19f7397cbae39dd34b3be33220309",
            "title": "Quantization Mimic: Towards Very Tiny CNN for Object Detection"
        },
        {
            "paperId": "2a1f38e4451e826e01c9874954ba7c6f32ff79f4",
            "title": "Boosting Self-Supervised Learning via Knowledge Transfer"
        },
        {
            "paperId": "f37a8472b00f4a00a91abb41e5ab764d5a5076a8",
            "title": "Adversarial Learning of Portable Student Networks"
        },
        {
            "paperId": "d1e20aa33354a07c9eb4e5c5818edd0072070f34",
            "title": "Learning With Single-Teacher Multi-Student"
        },
        {
            "paperId": "fba4722be1472dec76a8cc9c22a5c5d3f7f1ab4d",
            "title": "An Information-Theoretic View for Deep Learning"
        },
        {
            "paperId": "41132ebe016fbe953e607831c4f688ef63f657b4",
            "title": "Neural Compatibility Modeling with Attentive Knowledge Distillation"
        },
        {
            "paperId": "2e286fe30e2dac87fe340853941eb35851f9f326",
            "title": "Cross-Modality Distillation: A Case for Conditional Generative Adversarial Networks"
        },
        {
            "paperId": "ac7e97a486500f1af7453f68c8cea80db095ffa5",
            "title": "An Investigation of a Knowledge Distillation Method for CTC Acoustic Models"
        },
        {
            "paperId": "f00925f8c8acc5450b5482a12753653cfeaeb268",
            "title": "Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification"
        },
        {
            "paperId": "82c8dedbd58a8a8b7031dc49ce57553231fb3ca4",
            "title": "Learning Deep Representations with Probabilistic Knowledge Transfer"
        },
        {
            "paperId": "b82accfcc4572d1cb33c2a41aecfb6e7e606def9",
            "title": "Adversarial Network Compression"
        },
        {
            "paperId": "e54a7bf9b5f610a2c19e058e005296049e180aad",
            "title": "Knowledge Transfer with Jacobian Matching"
        },
        {
            "paperId": "6ea8cbf0cc4cda3d981348a279b464524a8485cc",
            "title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization"
        },
        {
            "paperId": "f6a4bf043af1a9ec7f104a7b7ab56806b241ceda",
            "title": "Model compression via distillation and quantization"
        },
        {
            "paperId": "cc59b4b1eb7d4629f753bc24f029c5cced301381",
            "title": "Large scale distributed neural network training through online distillation"
        },
        {
            "paperId": "d241aa0c8245f8ca2e531f79a4dc2552ff0c0628",
            "title": "Few-shot learning of neural networks from scratch by pseudo example optimization"
        },
        {
            "paperId": "9f58a7e844cd15d8aef8b2de7f246979ababe429",
            "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer"
        },
        {
            "paperId": "5dff6badc55a45ace7ae82bc22cca1af144b12ce",
            "title": "Predicting Human Decision-Making: From Prediction to Action"
        },
        {
            "paperId": "81f2de0fcd60af2f182c49262ee3d80b7ab8877f",
            "title": "Deep Net Triage: Analyzing the Importance of Network Layers via Structural Compression"
        },
        {
            "paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4",
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
        },
        {
            "paperId": "710bcef2c7c2e6a1ba455c136cb0aaa5580fb8e5",
            "title": "Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges"
        },
        {
            "paperId": "e9f5bb1f7ca4bb7d6fee35b1d1acd26a597b9945",
            "title": "Deep Net Triage: Assessing the Criticality of Network Layers by Structural Compression"
        },
        {
            "paperId": "757b27a3ceb2293b8284fc24a7084a0c3fc2ae21",
            "title": "Data Distillation: Towards Omni-Supervised Learning"
        },
        {
            "paperId": "73b5932b72c780408a29b38ce19641a4c411dc53",
            "title": "Learning Efficient Object Detection Models with Knowledge Distillation"
        },
        {
            "paperId": "8a990e81436422ebdf5de97c01c98b511dd4192c",
            "title": "Graph Distillation for Action Detection with Privileged Modalities"
        },
        {
            "paperId": "f6cbf83e1ce3b099d656d2346b261d5ef7f2b62e",
            "title": "Parallel WaveNet: Fast High-Fidelity Speech Synthesis"
        },
        {
            "paperId": "ac8e45a0451ac578f17f631fc2663ee4b98b83a9",
            "title": "Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients"
        },
        {
            "paperId": "7b8d67593c4ab1b1e3eccc158daee76703b328aa",
            "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy"
        },
        {
            "paperId": "194b3300da7592fdae5b6e29cd83220a5caa9d07",
            "title": "Moonshine: Distilling with Cheap Convolutions"
        },
        {
            "paperId": "ffd0ba45cc6b0c8f72a09617144786ffb26be771",
            "title": "Data-Free Knowledge Distillation for Deep Neural Networks"
        },
        {
            "paperId": "92d621a603cda8c32214d70953e180fe5a442f3e",
            "title": "N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning"
        },
        {
            "paperId": "dd8084b2878ca95d8f14bae73e1072922f0cc5da",
            "title": "Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification"
        },
        {
            "paperId": "86448ec1e48740163567e494592bb699d4a1900f",
            "title": "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks"
        },
        {
            "paperId": "e180e4751ced487d5ba763608737fb632b04ad71",
            "title": "Knowledge Distillation for Bilingual Dictionary Induction"
        },
        {
            "paperId": "b8ccc5341a1b0214e9d155b019962023f344c2ee",
            "title": "Incremental Learning of Object Detectors without Catastrophic Forgetting"
        },
        {
            "paperId": "86dc692fc0b6ee97077ae4132517cb8538802bcc",
            "title": "Efficient Knowledge Distillation from an Ensemble of Teachers"
        },
        {
            "paperId": "3c4b6f59a4dd9b6fb589abb826d063f7872a5808",
            "title": "Learning from Multiple Teacher Networks"
        },
        {
            "paperId": "ee52857e5f808b672b0d2c8f16b20c981ac6a5c5",
            "title": "Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net"
        },
        {
            "paperId": "0410659b6a311b281d10e0e44abce9b1c06be462",
            "title": "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning"
        },
        {
            "paperId": "f399b174645144efdbe202a081ad3acb3c2a1411",
            "title": "Mimicking Very Efficient Network for Object Detection"
        },
        {
            "paperId": "9806871bdcf0a9f926f6b4aebd20ee4580d69f00",
            "title": "On Compressing Deep Models by Low Rank and Sparse Decomposition"
        },
        {
            "paperId": "57504ac5c0e34618c2f6a9805c10b8e3a7458831",
            "title": "DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer"
        },
        {
            "paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89",
            "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"
        },
        {
            "paperId": "f06a12928307e17b1aff2b9f4a6c11791f19b6a7",
            "title": "Deep Mutual Learning"
        },
        {
            "paperId": "1ede979a4479abc0f8d80207fa50778273a33527",
            "title": "Label Propagation via Teaching-to-Learn and Learning-to-Teach"
        },
        {
            "paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
        },
        {
            "paperId": "76697e29b72b1980beb54784226ab7235ce298aa",
            "title": "Domain adaptation of DNN acoustic models using knowledge distillation"
        },
        {
            "paperId": "6494cd26511c076186673c9a636d21d1dfed8d5a",
            "title": "Student-teacher network learning with enhanced features"
        },
        {
            "paperId": "58def848174596119326b4ee981111d5ed538e9d",
            "title": "Knowledge distillation across ensembles of multilingual models for low-resource languages"
        },
        {
            "paperId": "7493389667058116dbc7e808987f129325ee60d7",
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results"
        },
        {
            "paperId": "b892687f095b78c21b7204c9823468fc07472511",
            "title": "Transferring Knowledge to Smaller Network with Class-Distance Loss"
        },
        {
            "paperId": "6e99c98a45fb09b5a777cfb7821b0df51acac665",
            "title": "Knowledge Adaptation: Teaching to Adapt"
        },
        {
            "paperId": "5a3a8e0a3ee8711ca9e7df7d1a22e532680c78ed",
            "title": "Ensemble Distillation for Neural Machine Translation"
        },
        {
            "paperId": "1102f8f37b4cb3747cf392db116a00af805339eb",
            "title": "CNNpack: Packing Convolutional Neural Networks in the Frequency Domain"
        },
        {
            "paperId": "e3b14c7cf882dcb9ae22a42851cff5c69d1c0a31",
            "title": "In Teacher We Trust: Learning Compressed Models for Pedestrian Detection"
        },
        {
            "paperId": "414f5e01a4c5901ec2743fa8993c04789ac34ab9",
            "title": "Wise teachers train better DNN acoustic models"
        },
        {
            "paperId": "c3a159d6a85be68af4c985c281718bc5801ada19",
            "title": "Lifelong Machine Learning"
        },
        {
            "paperId": "f7b032a4df721d4ed2bab97f6acd33d62477b7a5",
            "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer"
        },
        {
            "paperId": "d06255dccc262690740397174ea8d1ba5d2b1ce6",
            "title": "Doubly Convolutional Neural Networks"
        },
        {
            "paperId": "2d5e4b61cac1a509ad5f174863130c5bbd6d1a95",
            "title": "Deep Model Compression: Distilling Knowledge from Noisy Teachers"
        },
        {
            "paperId": "e70b9a38fcf8373865dd6e7b45e45cca7ff2eaa9",
            "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data"
        },
        {
            "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions"
        },
        {
            "paperId": "d43b4801ea469a71b346698bf41197ef97e97d53",
            "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser"
        },
        {
            "paperId": "c78c3b2593b808ed7af498bba48768dd607f8787",
            "title": "Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition"
        },
        {
            "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "title": "Densely Connected Convolutional Networks"
        },
        {
            "paperId": "9991d9ef171cda52b1fdc9fcac93cba80703c067",
            "title": "Knowledge distillation for small-footprint highway networks"
        },
        {
            "paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a",
            "title": "Learning without Forgetting"
        },
        {
            "paperId": "e879b12b0acc051e54b9b42b8f857deb3283ed8f",
            "title": "Learning with Side Information through Modality Hallucination"
        },
        {
            "paperId": "57a10537978600fd33dcdd48922c791609a4851a",
            "title": "Sequence-Level Knowledge Distillation"
        },
        {
            "paperId": "4186b9a6e13199150b086c5d07c0576fa75715cf",
            "title": "Wise teachers train better DNN acoustic models"
        },
        {
            "paperId": "477ca04e9c6b9fd8326af7e11c6d60b6ada2f42a",
            "title": "Adapting Models to Signal Degradation using Distillation"
        },
        {
            "paperId": "def52f9e3650ec4e9cc18b3e316f48e9bf046adf",
            "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?"
        },
        {
            "paperId": "435642641312364e45f4989fac0901b205c49d53",
            "title": "Face Model Compression by Distilling Knowledge from Neurons"
        },
        {
            "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "title": "Mastering the game of Go with deep neural networks and tree search"
        },
        {
            "paperId": "d3cb9bad655197b52932978dd8186b36c512bf92",
            "title": "Quantized Convolutional Neural Networks for Mobile Devices"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "16cb6876666f3a7b56a636c1d85ad00bd0d98bf3",
            "title": "Net2Net: Accelerating Learning via Knowledge Transfer"
        },
        {
            "paperId": "6adf016e7531c91100d3cf4a74f5d4c87b26b528",
            "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks"
        },
        {
            "paperId": "f341fa61da527e64a349334836d52626fe9d6c79",
            "title": "Unifying distillation and privileged information"
        },
        {
            "paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec",
            "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"
        },
        {
            "paperId": "bf76be8df2f2bc56edac98a5d0dfc19c85882eaa",
            "title": "Structured Transforms for Small-Footprint Deep Learning"
        },
        {
            "paperId": "53d1e022961e241164ecb6ec58378d7033a280f8",
            "title": "Cross Modal Distillation for Supervision Transfer"
        },
        {
            "paperId": "7613d67c7348f746ecaf71c6fd034fd577154050",
            "title": "Distilling Word Embeddings: An Encoding Approach"
        },
        {
            "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "title": "Learning both Weights and Connections for Efficient Neural Network"
        },
        {
            "paperId": "edb1e4bd20731b292e36df7f80dc5c1ad61febb6",
            "title": "Transferring knowledge from a RNN to a DNN"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "8604f376633af8b347e31d84c6150a93b11e34c2",
            "title": "FitNets: Hints for Thin Deep Nets"
        },
        {
            "paperId": "86ee1835a56722b76564119437070782fc90eb19",
            "title": "Generative Adversarial Nets"
        },
        {
            "paperId": "e5ae8ab688051931b4814f6d32b18391f8d1fa8d",
            "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation"
        },
        {
            "paperId": "d770060812fb646b3846a7d398a3066145b5e3c8",
            "title": "Do Deep Nets Really Need to be Deep?"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "title": "Representation Learning: A Review and New Perspectives"
        },
        {
            "paperId": "202da29a051383df4d8d37976539e0d64f93adc2",
            "title": "Access to Unlabeled Data can Speed up Prediction Time"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9",
            "title": "Model compression"
        },
        {
            "paperId": "df79c4dd73e76a91d2090305fdbc6fc0488302f2",
            "title": "Ensemble"
        },
        {
            "paperId": "fc1b3d97af68be808f81ed3e71bdfe198fcb05f7",
            "title": "Knowledge distillation"
        },
        {
            "paperId": "6a2b55f55da7f94bc00a7b070e77008c4d1f449a",
            "title": "Supplementary Material for Few Sample Knowledge Distillation for Ef\ufb01cient Network Compression"
        },
        {
            "paperId": "e8b78ce0d1749b4549b492f398db0c01b8a57841",
            "title": "Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition"
        },
        {
            "paperId": "684b0a5a92acfd044e44858cfea7270741aebcf0",
            "title": "Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts"
        },
        {
            "paperId": "aae38a2c51b4aa79b43d712e0d8a91ce4ad55bf4",
            "title": "Task-Oriented Feature Distillation"
        },
        {
            "paperId": "b86f7a5a83c76f82c40a72f39303d8b46840799d",
            "title": "Feature Normalized Knowledge Distillation for Image Classification"
        },
        {
            "paperId": "322630e0c91b6050169faf6688514aff902e1c4d",
            "title": "Feature-Level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks"
        },
        {
            "paperId": "798f537bef58a80514c397c17eb37af56765944b",
            "title": "Distributed Distillation for On-Device Learning"
        },
        {
            "paperId": "e37922fa83b85fedee5dcd74c65e1e5f8caee24b",
            "title": "Knowledge Distillation-Based Representation Learning for Short-Utterance Spoken Language Identification"
        },
        {
            "paperId": "c6f93cd84b942323c9fc87a6b26624f8344b3d42",
            "title": "Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space"
        },
        {
            "paperId": "0313dec02e7ea7b98f8353b42c3ca31923d3e15c",
            "title": "Knowledge Augmented Deep Neural Networks for Joint Facial Expression and Action Unit Recognition"
        },
        {
            "paperId": "74e3264172b1f293b37faff51db3795605c6d02d",
            "title": "Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "2ab705fb1af32f55a84b5c755c71b4f9c1ce7c6c",
            "title": "An Adversarial Feature Distillation Method for Audio Classification"
        },
        {
            "paperId": "bd3ec3f8856bcfc2e5cc3ee83f6e14fe66e1375a",
            "title": "A Teacher-Student Framework for Maintainable Dialog Manager"
        },
        {
            "paperId": "bc7a573469407fa597c14d9999bb78c79b7344c6",
            "title": "Training Student Networks for Acceleration with Conditional Adversarial Networks"
        },
        {
            "paperId": "2a4f50cca273813da26ef25e5bda7d01b5e0dde6",
            "title": "KDGAN: Knowledge Distillation with Generative Adversarial Networks"
        },
        {
            "paperId": "7adef3d0200207baec75e39bbb852cacfaf8268b",
            "title": "Learning to Specialize with Knowledge Distillation for Visual Question Answering"
        },
        {
            "paperId": "8b6bd623a19a0f2219fa637cafd7658c8797ffe6",
            "title": "International Journal of Computer Vision"
        },
        {
            "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
            "title": "GENERATIVE ADVERSARIAL NETS"
        },
        {
            "paperId": null,
            "title": "Among these KD-based NLP methods, most of them belong to natural language understanding (NLU), and many of these KD methods for NLU are designed as the taskspecific distillation"
        },
        {
            "paperId": null,
            "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer"
        },
        {
            "paperId": "3d6a796684a8b6cb0d32dff0e1e5d441168f3572",
            "title": "Sequence Student-Teacher Training of Deep Neural Networks"
        },
        {
            "paperId": null,
            "title": "The existing NLP tasks using KD contain neural machine translation (NMT) (Hahn and Choi"
        },
        {
            "paperId": "d237de6e4974e6a34d2b35d7a3a223f6fb611219",
            "title": "Learning using privileged information: similarity control and knowledge transfer"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": null,
            "title": "Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations"
        }
    ]
}