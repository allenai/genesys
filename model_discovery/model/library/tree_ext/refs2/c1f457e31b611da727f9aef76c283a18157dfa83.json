{
    "paperId": "c1f457e31b611da727f9aef76c283a18157dfa83",
    "externalIds": {
        "DBLP": "journals/corr/abs-1806-09055",
        "MAG": "2810075754",
        "ArXiv": "1806.09055",
        "CorpusId": 49411844
    },
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 48,
    "citationCount": 3952,
    "influentialCitationCount": 1229,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The proposed algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2391802",
            "name": "Hanxiao Liu"
        },
        {
            "authorId": "34838386",
            "name": "K. Simonyan"
        },
        {
            "authorId": "35729970",
            "name": "Yiming Yang"
        }
    ],
    "references": [
        {
            "paperId": "45b7b5514a65126d39a51d5a68da53e7aa244c1f",
            "title": "Understanding and Simplifying One-Shot Architecture Search"
        },
        {
            "paperId": "15561ab20c298e113b0008b7a029486a422e7ca3",
            "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning"
        },
        {
            "paperId": "1a83c891d5a04b8ceca839b7d25f52e79d8dba1f",
            "title": "Differentiable Neural Network Architecture Search"
        },
        {
            "paperId": "7864c8cd08ff4da9acc37de2576e9cdbabe03107",
            "title": "Neural Architecture Search with Bayesian Optimisation and Optimal Transport"
        },
        {
            "paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7",
            "title": "Efficient Neural Architecture Search via Parameter Sharing"
        },
        {
            "paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5",
            "title": "Regularized Evolution for Image Classifier Architecture Search"
        },
        {
            "paperId": "5f79398057bf0bbda9ff50067bc1f2950c2a2266",
            "title": "Progressive Neural Architecture Search"
        },
        {
            "paperId": "0e0ee672ebd9ec0019c414d1c0524f3bb888dd6d",
            "title": "Simple And Efficient Architecture Search for Convolutional Neural Networks"
        },
        {
            "paperId": "ef9ddbc35676ce8ffc2a8067044473727839dbac",
            "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"
        },
        {
            "paperId": "856451974cce2d353d5d8a5a72104984a252375c",
            "title": "Hierarchical Representations for Efficient Architecture Search"
        },
        {
            "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
            "title": "Automatic differentiation in PyTorch"
        },
        {
            "paperId": "610eaed09988471e49fc458a9dd828889dff23b0",
            "title": "Connectivity Learning in Multi-Branch Networks"
        },
        {
            "paperId": "fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d",
            "title": "Dynamic Evaluation of Neural Sequence Models"
        },
        {
            "paperId": "8a1ce657dd41a4f49990a4769000dc8049b83404",
            "title": "Practical Block-Wise Neural Network Architecture Generation"
        },
        {
            "paperId": "e56b10f7cd4bf037beac84da5925dc4544fab974",
            "title": "SMASH: One-Shot Model Architecture Search through HyperNetworks"
        },
        {
            "paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f",
            "title": "Improved Regularization of Convolutional Neural Networks with Cutout"
        },
        {
            "paperId": "58c6f890a1ae372958b7decf56132fe258152722",
            "title": "Regularizing and Optimizing LSTM Language Models"
        },
        {
            "paperId": "d0611891b9e8a7c5731146097b6f201578f47b2f",
            "title": "Learning Transferable Architectures for Scalable Image Recognition"
        },
        {
            "paperId": "2397ce306e5d7f3d0492276e357fb1833536b5d8",
            "title": "On the State of the Art of Evaluation in Neural Language Models"
        },
        {
            "paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89",
            "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"
        },
        {
            "paperId": "84e65a5bdb735d62eef4f72c2f01af354b2285ba",
            "title": "Efficient Architecture Search by Network Transformation"
        },
        {
            "paperId": "3b8aa7a38bc60f7b42757e53a0801f6e71dcef5c",
            "title": "Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks"
        },
        {
            "paperId": "2ddc07ca7af4578fde868f102d178dc3ba6a0751",
            "title": "Accelerating Neural Architecture Search using Performance Prediction"
        },
        {
            "paperId": "71a80e7342e56f33fd120246e907151a0cf1b4d0",
            "title": "DeepArchitect: Automatically Designing and Training Deep Architectures"
        },
        {
            "paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
        },
        {
            "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
        },
        {
            "paperId": "67d968c7450878190e45ac7886746de867bf673d",
            "title": "Neural Architecture Search with Reinforcement Learning"
        },
        {
            "paperId": "6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
            "title": "Designing Neural Network Architectures using Reinforcement Learning"
        },
        {
            "paperId": "488bb25e0b1777847f04c943e6dbc4f84415b712",
            "title": "Unrolled Generative Adversarial Networks"
        },
        {
            "paperId": "424aef7340ee618132cc3314669400e23ad910ba",
            "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"
        },
        {
            "paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
            "title": "Improving Neural Language Models with a Continuous Cache"
        },
        {
            "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "title": "Densely Connected Convolutional Networks"
        },
        {
            "paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86",
            "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"
        },
        {
            "paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
            "title": "Recurrent Highway Networks"
        },
        {
            "paperId": "197c8988ef21d0b58d363c21bafe1900c3089e3e",
            "title": "Convolutional Neural Fabrics"
        },
        {
            "paperId": "66edb2a8d33db2d133d3a3c8c032a06a95c6cd3b",
            "title": "Hyperparameter optimization with approximate gradient"
        },
        {
            "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
            "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
        },
        {
            "paperId": "dcad2fd88e432f337018e410c44fd45d52851039",
            "title": "Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters"
        },
        {
            "paperId": "e2820bffe5b42cb7d88b7f65c12171c62ab4aae2",
            "title": "Gradient-based Hyperparameter Optimization through Reversible Learning"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "title": "Going deeper with convolutions"
        },
        {
            "paperId": "6609c558425c9e1848944049c6e302c69eeb2842",
            "title": "An overview of bilevel optimization"
        },
        {
            "paperId": "6dc61f37ecc552413606d8c89ffbc46ec98ed887",
            "title": "Acceleration of stochastic approximation by averaging"
        },
        {
            "paperId": "65d68b7edb405a3681c412168cb28dafe3a3991f",
            "title": "Hierarchical optimization: An introduction"
        },
        {
            "paperId": "efcce85b288ef18b53da7488feaea118307cf6b0",
            "title": "Marktform und Gleichgewicht"
        },
        {
            "paperId": null,
            "title": "BPTT length 35, and weight decay"
        },
        {
            "paperId": null,
            "title": "The training takes 12 days on a single GPU. A.2.4 WIKITEXT-2 We use embedding and hidden sizes 700, weight decay 5\u00d710\u22127, and hidden-node variational dropout"
        }
    ]
}