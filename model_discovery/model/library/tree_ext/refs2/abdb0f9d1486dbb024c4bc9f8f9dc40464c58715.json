{
    "paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715",
    "externalIds": {
        "DBLP": "conf/iclr/XiaGZ024",
        "ArXiv": "2310.06694",
        "DOI": "10.48550/arXiv.2310.06694",
        "CorpusId": 263830786
    },
    "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
    "abstract": "The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building competitive small-scale LLMs",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 85,
    "citationCount": 133,
    "influentialCitationCount": 20,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2310.06694",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.430856227874756,
            0.7581017017364502,
            -1.0146287679672241,
            3.887589454650879,
            0.970076322555542,
            0.15328681468963623,
            2.4622747898101807,
            -0.10221326351165771,
            -1.791368007659912,
            1.027271032333374,
            -0.25771859288215637,
            3.7726147174835205,
            0.9467995166778564,
            -2.415757179260254,
            -4.970721244812012,
            0.19599103927612305,
            0.8177393674850464,
            -1.6089171171188354,
            4.225096702575684,
            2.5423479080200195,
            -2.853010892868042,
            2.5661001205444336,
            -2.2811412811279297,
            -0.12463495135307312,
            -2.540195941925049,
            -0.31830263137817383,
            4.185492515563965,
            -0.5824742317199707,
            -0.5759881734848022,
            1.7638965845108032,
            0.7424745559692383,
            -5.72077751159668,
            4.560966968536377,
            -6.09188175201416,
            2.7549667358398438,
            -4.294345855712891,
            -0.049020916223526,
            8.363274574279785,
            -0.8777227997779846,
            -1.532458782196045,
            -1.3649396896362305,
            -1.6423730850219727,
            -2.7196948528289795,
            3.418428897857666,
            0.14657540619373322,
            3.282461166381836,
            3.7645821571350098,
            -1.0800892114639282,
            1.2848775386810303,
            2.6168689727783203,
            2.4453072547912598,
            0.3686862587928772,
            0.842512309551239,
            -0.5422958135604858,
            -0.2850027084350586,
            -1.7486852407455444,
            -3.046607255935669,
            0.7778090238571167,
            -0.26063066720962524,
            -3.274272918701172,
            3.5600545406341553,
            3.979090690612793,
            -1.8368096351623535,
            0.616521954536438,
            1.186051607131958,
            -1.9788033962249756,
            -1.4122116565704346,
            7.130558967590332,
            0.6043292284011841,
            0.07796734571456909,
            -2.2231032848358154,
            -4.620418071746826,
            1.3528969287872314,
            0.4963557720184326,
            -4.126768589019775,
            1.538313865661621,
            -2.1967933177948,
            -9.163566589355469,
            -2.5258865356445312,
            -2.429049491882324,
            -1.1608152389526367,
            -0.6016938090324402,
            0.5569595098495483,
            2.11268949508667,
            3.75268292427063,
            0.9080815315246582,
            -4.573702812194824,
            0.5951469540596008,
            6.826814651489258,
            -5.298541069030762,
            0.8108168244361877,
            0.41804084181785583,
            -1.4453885555267334,
            1.404900312423706,
            -5.099376678466797,
            0.9830892086029053,
            -1.5061466693878174,
            -2.038146495819092,
            -0.1462298035621643,
            -0.9024858474731445,
            3.001893997192383,
            -0.21255181729793549,
            3.4847166538238525,
            0.2278890162706375,
            4.520368576049805,
            -2.9063844680786133,
            -1.742074728012085,
            -0.5967701077461243,
            -0.523883581161499,
            -1.0072005987167358,
            -3.8270905017852783,
            2.995161533355713,
            -0.7786803841590881,
            -1.5348278284072876,
            -3.2821922302246094,
            -2.6217398643493652,
            -2.934201717376709,
            -1.7624760866165161,
            -1.4946280717849731,
            4.523797035217285,
            -0.8113266825675964,
            0.08111684769392014,
            -1.7807221412658691,
            -1.3634743690490723,
            -0.41869229078292847,
            3.8754630088806152,
            -2.7348878383636475,
            -0.9689431190490723,
            -2.9999654293060303,
            -1.8880411386489868,
            3.8700451850891113,
            -0.44891050457954407,
            2.7113726139068604,
            -0.41636571288108826,
            1.3455449342727661,
            5.209717750549316,
            -3.8035459518432617,
            4.359120845794678,
            -4.062310218811035,
            -2.1824846267700195,
            0.7890785932540894,
            6.1468071937561035,
            -0.07660174369812012,
            -0.29396986961364746,
            -0.6008010506629944,
            3.669741630554199,
            1.4186710119247437,
            0.8079419136047363,
            -0.5003356337547302,
            5.7482075691223145,
            3.3306286334991455,
            -3.3068313598632812,
            0.8845131397247314,
            -0.9983469843864441,
            -0.6342340111732483,
            4.458407878875732,
            -5.970432281494141,
            1.4232975244522095,
            0.4096120595932007,
            -0.4319695830345154,
            0.8669838905334473,
            0.42538899183273315,
            -10.517569541931152,
            -0.10667848587036133,
            4.54525089263916,
            -3.0165276527404785,
            0.30709564685821533,
            2.6803507804870605,
            0.7892436981201172,
            0.5324498414993286,
            -0.8148808479309082,
            3.618436813354492,
            2.6327931880950928,
            2.7126846313476562,
            3.5432658195495605,
            2.79803466796875,
            3.641826629638672,
            -1.2535426616668701,
            -2.1229898929595947,
            0.13148069381713867,
            -0.038824379444122314,
            -0.7552810907363892,
            -1.7656053304672241,
            0.6787028312683105,
            -3.0492606163024902,
            -0.41822507977485657,
            -3.0703392028808594,
            -6.41258430480957,
            -0.22774969041347504,
            1.2006044387817383,
            -0.5441665649414062,
            -0.2644394338130951,
            3.863570213317871,
            4.918692111968994,
            1.9915755987167358,
            0.7413965463638306,
            1.1510510444641113,
            5.165047645568848,
            -3.5979509353637695,
            1.2276588678359985,
            4.6038408279418945,
            0.04922275245189667,
            0.6369881629943848,
            -1.7125998735427856,
            1.3229789733886719,
            1.5057415962219238,
            -2.839000701904297,
            1.8497965335845947,
            2.583594560623169,
            1.6421761512756348,
            2.253178119659424,
            0.08240270614624023,
            -0.5930936336517334,
            1.1606967449188232,
            -3.1099283695220947,
            -2.7896604537963867,
            -5.001664161682129,
            2.28999924659729,
            3.523155927658081,
            1.8130114078521729,
            -2.02209734916687,
            -1.195732593536377,
            2.4053566455841064,
            -2.465630292892456,
            3.6230688095092773,
            -4.204676628112793,
            -0.08319628238677979,
            -1.6119532585144043,
            0.20713293552398682,
            0.3304928243160248,
            -0.1838282346725464,
            -4.345359802246094,
            1.0657978057861328,
            -0.05286794900894165,
            -7.110879898071289,
            -0.690335214138031,
            -6.125849723815918,
            0.01251065731048584,
            -1.428166151046753,
            -2.7948830127716064,
            3.6448419094085693,
            0.9049514532089233,
            -0.6892800331115723,
            5.770159721374512,
            4.176538467407227,
            -1.1275264024734497,
            -2.6257567405700684,
            -0.002716362476348877,
            -2.033484697341919,
            0.694719672203064,
            -1.5489819049835205,
            -1.785217523574829,
            2.8610708713531494,
            -0.8047797083854675,
            1.972579002380371,
            5.255622863769531,
            2.3078207969665527,
            -2.37630033493042,
            0.6653806567192078,
            1.0062179565429688,
            -0.5366522669792175,
            6.279470443725586,
            1.7499122619628906,
            4.403131484985352,
            -2.36244535446167,
            1.4913787841796875,
            -4.004354476928711,
            -3.285433292388916,
            -2.050999879837036,
            3.2778918743133545,
            4.985635757446289,
            1.5721368789672852,
            -0.4910828769207001,
            -4.681997299194336,
            -1.5037696361541748,
            -9.204151153564453,
            -0.7580323219299316,
            1.0602993965148926,
            0.7443256378173828,
            2.6396780014038086,
            0.3643444776535034,
            -3.3110904693603516,
            -0.07326674461364746,
            0.04838758707046509,
            -3.3690109252929688,
            -1.1566662788391113,
            -0.4350305199623108,
            0.08656203746795654,
            -2.362044334411621,
            -3.684523820877075,
            -1.9139440059661865,
            3.4495022296905518,
            -2.369368076324463,
            -4.449178695678711,
            -2.592254877090454,
            2.7985525131225586,
            3.333503246307373,
            -0.44574224948883057,
            -1.4415185451507568,
            -2.404788017272949,
            0.35938018560409546,
            3.4278552532196045,
            4.063004016876221,
            -0.25493621826171875,
            1.1592869758605957,
            3.3137295246124268,
            0.3185536861419678,
            -1.307268500328064,
            1.612239956855774,
            -3.7620177268981934,
            -2.83834171295166,
            -0.07001993060112,
            2.8781356811523438,
            -3.823188543319702,
            2.3426573276519775,
            3.8002078533172607,
            1.8096303939819336,
            0.3810124397277832,
            -1.5650689601898193,
            -0.17267733812332153,
            -0.5314100980758667,
            -1.459287166595459,
            -7.54483699798584,
            -0.9559433460235596,
            -2.548614025115967,
            0.7638602256774902,
            3.890526056289673,
            5.268650054931641,
            -5.107566833496094,
            2.658069133758545,
            -0.6537412405014038,
            2.6426801681518555,
            2.9607315063476562,
            2.579761028289795,
            -5.653798580169678,
            -5.432522773742676,
            -4.0726847648620605,
            -3.868664264678955,
            2.5470030307769775,
            1.150366187095642,
            2.8133487701416016,
            6.752155303955078,
            -1.332948923110962,
            1.9166781902313232,
            -2.0726370811462402,
            0.46052995324134827,
            2.2970852851867676,
            -0.04397141933441162,
            0.6682653427124023,
            -1.2388231754302979,
            0.9570021629333496,
            0.4903947412967682,
            1.1629177331924438,
            -0.48641514778137207,
            -1.0095887184143066,
            2.8644609451293945,
            2.7761504650115967,
            2.6866817474365234,
            2.126924514770508,
            1.877505898475647,
            3.4182586669921875,
            0.16333037614822388,
            3.683797836303711,
            1.084207534790039,
            3.015092611312866,
            -1.1896055936813354,
            11.8326416015625,
            -2.643859624862671,
            5.587663650512695,
            -5.782051086425781,
            0.5721917748451233,
            -1.6453009843826294,
            -2.5397372245788574,
            3.1471848487854004,
            -3.1282505989074707,
            -2.621762275695801,
            1.9790016412734985,
            -6.66718864440918,
            2.006544828414917,
            -0.14115098118782043,
            -0.285847544670105,
            5.6298065185546875,
            -2.011932611465454,
            3.6436166763305664,
            -2.5505566596984863,
            0.8011795878410339,
            -1.1673452854156494,
            2.8648436069488525,
            -0.20251601934432983,
            -1.5768203735351562,
            -0.5794188380241394,
            1.8460949659347534,
            -0.6648815274238586,
            3.944462776184082,
            -5.146589756011963,
            -2.2166552543640137,
            -3.6584224700927734,
            -1.657080054283142,
            0.2874811887741089,
            0.6763114333152771,
            -2.206181049346924,
            -0.3987041711807251,
            6.0037760734558105,
            2.4806995391845703,
            -3.418903350830078,
            -1.0281946659088135,
            4.366944789886475,
            0.02466866374015808,
            -2.1905648708343506,
            -0.07448256015777588,
            -3.399156093597412,
            -1.9164345264434814,
            -1.8781459331512451,
            -3.6551780700683594,
            0.01924651861190796,
            -0.1941644549369812,
            3.3207716941833496,
            3.92642879486084,
            3.7198410034179688,
            3.558384656906128,
            -2.2702796459198,
            0.878920316696167,
            3.733325481414795,
            3.0102410316467285,
            0.6340710520744324,
            -0.06923624873161316,
            1.881880760192871,
            2.0714914798736572,
            -2.054025650024414,
            1.7221137285232544,
            -1.6545865535736084,
            4.380691051483154,
            -2.213268518447876,
            -0.8808784484863281,
            0.5702518820762634,
            2.482758045196533,
            2.557821750640869,
            0.6237806081771851,
            0.6347081065177917,
            -3.1624364852905273,
            0.9848436117172241,
            2.4636807441711426,
            -2.1286215782165527,
            5.91304874420166,
            2.5451998710632324,
            1.8215795755386353,
            1.1957367658615112,
            2.2642486095428467,
            -3.5644211769104004,
            -2.1710174083709717,
            3.2285494804382324,
            -6.1599860191345215,
            -3.0728774070739746,
            -0.07040618360042572,
            1.349149227142334,
            1.0037835836410522,
            -2.6773877143859863,
            -1.1060669422149658,
            -0.5426842570304871,
            0.7629883289337158,
            -3.0155751705169678,
            5.286801338195801,
            1.14036226272583,
            1.8727126121520996,
            1.2729547023773193,
            0.6995445489883423,
            -2.8108344078063965,
            -2.723968982696533,
            -2.4211318492889404,
            2.88523530960083,
            1.105527400970459,
            -2.6380815505981445,
            -0.5997256636619568,
            1.1737275123596191,
            -2.9344229698181152,
            -1.6763602495193481,
            2.133373260498047,
            1.0982822179794312,
            1.3555790185928345,
            -5.480198860168457,
            -2.011012554168701,
            -1.7059389352798462,
            3.4794788360595703,
            -3.0597848892211914,
            -0.5999277830123901,
            3.6438698768615723,
            2.813832998275757,
            0.19962504506111145,
            4.416142463684082,
            -2.2401087284088135,
            1.2255512475967407,
            2.612217664718628,
            2.4960174560546875,
            -3.2509069442749023,
            0.41169604659080505,
            0.03464101254940033,
            -3.0276849269866943,
            -0.6769886016845703,
            2.0413432121276855,
            -0.43381422758102417,
            -0.8990517258644104,
            -6.1213836669921875,
            0.9679233431816101,
            -0.19854825735092163,
            -4.776983737945557,
            3.1110117435455322,
            6.915080547332764,
            0.3371192216873169,
            -1.2401314973831177,
            -0.10231804847717285,
            -1.2331044673919678,
            1.0088456869125366,
            -4.6397857666015625,
            4.55964469909668,
            0.7713390588760376,
            -1.8965063095092773,
            3.045719623565674,
            -1.1569855213165283,
            -0.33890819549560547,
            -1.0959042310714722,
            -0.5531861782073975,
            -1.2226648330688477,
            -1.7835931777954102,
            1.5368294715881348,
            1.452956199645996,
            0.039656102657318115,
            -1.103237509727478,
            0.8120251297950745,
            3.073245048522949,
            5.9740471839904785,
            6.173731803894043,
            1.6191562414169312,
            4.297921180725098,
            -1.7324292659759521,
            -0.7534775733947754,
            -1.8226711750030518,
            4.268622398376465,
            1.4551365375518799,
            -5.631541728973389,
            -1.3006997108459473,
            -1.01613187789917,
            -0.6177164316177368,
            0.35321491956710815,
            2.612114429473877,
            -2.2479333877563477,
            3.9222326278686523,
            -3.028724193572998,
            -2.138859748840332,
            -1.1252756118774414,
            0.7075900435447693,
            2.161346912384033,
            -0.9257402420043945,
            1.1609082221984863,
            -0.9009222984313965,
            -1.7482322454452515,
            -0.7523754239082336,
            -3.229975461959839,
            1.1779494285583496,
            -1.561161994934082,
            4.156815528869629,
            2.932375907897949,
            -0.5434873104095459,
            2.4048938751220703,
            3.514735460281372,
            -1.0537511110305786,
            -1.761242151260376,
            -5.340035438537598,
            4.772176742553711,
            0.9240655899047852,
            -1.0185811519622803,
            0.8624440431594849,
            -2.7809929847717285,
            -1.1633774042129517,
            -0.8277284502983093,
            0.3574085831642151,
            2.601377487182617,
            4.7784833908081055,
            2.632516384124756,
            -1.699674129486084,
            -2.5135717391967773,
            -2.614719867706299,
            -3.6023671627044678,
            -1.8254716396331787,
            -4.936310768127441,
            -0.29893162846565247,
            -1.6194133758544922,
            -4.526460647583008,
            1.1332261562347412,
            -2.6855087280273438,
            1.3323302268981934,
            1.1892638206481934,
            -0.9513106942176819,
            -1.7699378728866577,
            -1.2041404247283936,
            0.28898483514785767,
            -2.8182196617126465,
            3.289686441421509,
            -3.189638614654541,
            2.5457875728607178,
            3.045362710952759,
            -1.5309607982635498,
            6.7143168449401855,
            3.2516462802886963,
            -0.45097726583480835,
            -0.6559877395629883,
            -2.182478189468384,
            3.5821776390075684,
            0.569425106048584,
            -4.37188720703125,
            1.6684447526931763,
            0.2834268808364868,
            4.5572967529296875,
            17.40115737915039,
            -3.2333498001098633,
            -2.649136543273926,
            2.0903639793395996,
            -1.9598031044006348,
            -3.1486921310424805,
            -3.4890544414520264,
            2.787266254425049,
            0.09204493463039398,
            1.4345123767852783,
            -1.9837958812713623,
            -2.259554862976074,
            0.2977273762226105,
            2.2660722732543945,
            -1.394256830215454,
            -2.3718936443328857,
            -1.4640724658966064,
            1.1647123098373413,
            -4.825052738189697,
            0.3505980968475342,
            2.1059212684631348,
            0.49405744671821594,
            -0.19081735610961914,
            -1.9137537479400635,
            0.4511678218841553,
            4.033843994140625,
            2.5339956283569336,
            2.2150495052337646,
            -1.6998927593231201,
            -0.18482966721057892,
            1.9613568782806396,
            3.8860726356506348,
            0.4383317828178406,
            2.5196805000305176,
            -1.265191912651062,
            3.5271201133728027,
            1.7282261848449707,
            -1.5854356288909912,
            1.6886839866638184,
            3.3636298179626465,
            -3.303563117980957,
            0.5276625156402588,
            -2.8635895252227783,
            1.5445743799209595,
            -1.3175004720687866,
            2.3689136505126953,
            1.9996896982192993,
            -1.848201036453247,
            -0.8162345290184021,
            2.484475612640381,
            -0.40916818380355835,
            -1.4102299213409424,
            -0.1793978214263916,
            1.405052661895752,
            2.1031343936920166,
            3.6249518394470215,
            2.514108419418335,
            -4.657035827636719,
            2.296234130859375,
            -1.0553252696990967,
            -0.20996201038360596,
            -2.2988476753234863,
            -1.3253881931304932,
            -2.680323839187622,
            -1.6343185901641846,
            -0.2817760407924652,
            -4.917229652404785,
            2.584954261779785,
            3.467495918273926,
            0.3811187446117401,
            3.6532697677612305,
            0.838009238243103,
            0.7049660682678223,
            -1.135854959487915,
            0.3340144455432892,
            -3.9390761852264404,
            2.196078300476074,
            -1.112622857093811,
            0.3017037510871887,
            10.352790832519531,
            -1.8281872272491455,
            1.7199068069458008,
            -1.565345048904419,
            -0.44446277618408203,
            3.4115676879882812,
            -0.7307330965995789,
            4.097591400146484,
            3.2706055641174316,
            -2.132397174835205,
            2.7899651527404785,
            -2.1266117095947266,
            -0.6696189641952515,
            3.169644355773926,
            3.7431702613830566,
            1.8267723321914673,
            -3.743710517883301,
            -2.675424098968506,
            -3.424410343170166,
            -2.9708995819091797,
            -1.2581748962402344,
            6.566310405731201,
            3.5067529678344727,
            1.0549838542938232,
            -2.605325222015381,
            -0.19591903686523438,
            -0.8237823247909546,
            -5.73679780960083,
            -7.767450332641602,
            -3.4033114910125732,
            -3.4826676845550537,
            1.89225172996521,
            -4.27149772644043,
            -0.35878080129623413,
            0.40421944856643677,
            2.4249868392944336,
            -1.531813383102417,
            0.100814089179039,
            1.5879621505737305,
            -1.2010242938995361,
            5.820537567138672,
            0.14470471441745758,
            -0.7237705588340759,
            -2.3228840827941895,
            -3.4402577877044678,
            -0.6599274277687073,
            0.13251006603240967,
            1.3346818685531616,
            -2.618166446685791,
            -2.445862293243408,
            1.6345996856689453,
            0.16902422904968262,
            -3.848740816116333,
            -0.13182657957077026,
            1.349172830581665,
            -2.7641146183013916,
            -2.451493263244629,
            1.2085893154144287,
            -0.7547162771224976,
            -0.7999221086502075,
            3.288970947265625,
            3.7726731300354004,
            -3.074089527130127,
            -1.2395012378692627,
            8.881538391113281,
            0.2806553244590759,
            -0.03821098804473877,
            -0.711179792881012,
            -1.596806526184082,
            -2.4451396465301514,
            1.8355408906936646,
            0.9936150312423706,
            0.5056759715080261,
            5.464756011962891,
            1.4362596273422241,
            -0.030918419361114502,
            -2.886780261993408
        ]
    },
    "authors": [
        {
            "authorId": "67284811",
            "name": "Mengzhou Xia"
        },
        {
            "authorId": "2256993940",
            "name": "Tianyu Gao"
        },
        {
            "authorId": "2150468823",
            "name": "Zhiyuan Zeng"
        },
        {
            "authorId": "50536468",
            "name": "Danqi Chen"
        }
    ],
    "references": [
        {
            "paperId": "560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd",
            "title": "TinyLlama: An Open-Source Small Language Model"
        },
        {
            "paperId": "7d011d6a9e1704acc29bab88d616089089ea1006",
            "title": "PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation"
        },
        {
            "paperId": "39bb5d44735c07b1e1f4341a2d4bc8d5e783f491",
            "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training"
        },
        {
            "paperId": "193955704f66923ac20a664bd184ed4663b2bdf9",
            "title": "Continual Pre-Training of Large Language Models: How to (re)warm your model?"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "881883842c2661b41bbfc999d56c763b1ceef0bd",
            "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models"
        },
        {
            "paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6",
            "title": "A Simple and Effective Pruning Approach for Large Language Models"
        },
        {
            "paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce",
            "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"
        },
        {
            "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e",
            "title": "Large Language Models are not Fair Evaluators"
        },
        {
            "paperId": "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d",
            "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"
        },
        {
            "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
        },
        {
            "paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
            "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"
        },
        {
            "paperId": "017010b941d902a467f6d329ae5e74fd67e67912",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"
        },
        {
            "paperId": "9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d",
            "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"
        },
        {
            "paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
            "title": "StarCoder: may the source be with you!"
        },
        {
            "paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da",
            "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
        },
        {
            "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
            "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458",
            "title": "Symbolic Discovery of Optimization Algorithms"
        },
        {
            "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"
        },
        {
            "paperId": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e",
            "title": "Training Trajectories of Language Models Across Scales"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "5f6a5f319aca3f3d3f7a3c7c1ef0b8fc97ee1458",
            "title": "Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation"
        },
        {
            "paperId": "59fed7ca092c7e83583906456756abba8ce9295a",
            "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "6a8db14262ca2017cb253e12b8daeb57989a38df",
            "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"
        },
        {
            "paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5",
            "title": "Structured Pruning Learns Compact and Accurate Models"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback"
        },
        {
            "paperId": "68f141724814839d556a989646194be88641b143",
            "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
        },
        {
            "paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561",
            "title": "Block Pruning For Faster Transformers"
        },
        {
            "paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5",
            "title": "Deduplicating Training Data Makes Language Models Better"
        },
        {
            "paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
        },
        {
            "paperId": "2310d893abf4ec900cb9e0c5da58284a37329780",
            "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"
        },
        {
            "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
            "title": "Measuring Massive Multitask Language Understanding"
        },
        {
            "paperId": "389036b1366b64579725457993c1f63a4f3370ba",
            "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"
        },
        {
            "paperId": "f46c562229c5bc419bbbfb63239431590e4b340a",
            "title": "Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"
        },
        {
            "paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc",
            "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"
        },
        {
            "paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"
        },
        {
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
        },
        {
            "paperId": "1c332cfa211400fc6f56983fb01a6692046116dd",
            "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"
        },
        {
            "paperId": "e70d609ce18cd61799b087bf3a5e14c1ce70a41a",
            "title": "Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey"
        },
        {
            "paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
            "title": "GLU Variants Improve Transformer"
        },
        {
            "paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
            "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"
        },
        {
            "paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e",
            "title": "Structured Pruning of Large Language Models"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "9f73c3f86026c21d0e5e55c70462952c6ada1175",
            "title": "Accelerating Deep Learning by Focusing on the Biggest Losers"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "7823292e5c4b05c47af91ab6ddf671a0da709e82",
            "title": "Once for All: Train One Network and Specialize it for Efficient Deployment"
        },
        {
            "paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0",
            "title": "Natural Questions: A Benchmark for Question Answering Research"
        },
        {
            "paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
            "title": "Efficient Training of BERT by Progressively Stacking"
        },
        {
            "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
            "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
        },
        {
            "paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535",
            "title": "A Simple Method for Commonsense Reasoning"
        },
        {
            "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
            "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
        },
        {
            "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
            "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
        },
        {
            "paperId": "2ec7156913117949ab933f27f492d0149bc0031f",
            "title": "Learning Sparse Neural Networks through L0 Regularization"
        },
        {
            "paperId": "90a16f34d109b63d95ab4da2d491cbe3a1c8b656",
            "title": "Learning Efficient Convolutional Networks through Network Slimming"
        },
        {
            "paperId": "049fd80f52c0b1fa4d532945d95a24734b62bdf3",
            "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression"
        },
        {
            "paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738",
            "title": "Crowdsourcing Multiple Choice Science Questions"
        },
        {
            "paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
            "title": "Pruning Filters for Efficient ConvNets"
        },
        {
            "paperId": "7601b995303f953955004db7b9b8b206c0e02ff8",
            "title": "Learning Structured Sparsity in Deep Neural Networks"
        },
        {
            "paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a",
            "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"
        },
        {
            "paperId": "642d0f49b7826adcf986616f4af77e736229990f",
            "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
        },
        {
            "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
            "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": null,
            "title": "Redpajama-incite-base-3b-v1"
        },
        {
            "paperId": null,
            "title": "Openllama: An open reproduction of llama"
        },
        {
            "paperId": null,
            "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms"
        },
        {
            "paperId": null,
            "title": "TUNING During instruction tuning training, the instruction is prepended with \u201cYou are a helpful assistant. Write a response that appropriately completes the request."
        },
        {
            "paperId": null,
            "title": "Stanford alpaca: An instruction-following llama model, 2023"
        },
        {
            "paperId": "89654f548e541a8ef233be6585316ce6cc201535",
            "title": "Structured Pruning Learns Compact and Accurate Models"
        },
        {
            "paperId": null,
            "title": "A framework for few-shot language model evaluation"
        },
        {
            "paperId": null,
            "title": "The pushshift reddit"
        },
        {
            "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
            "title": "An Adversarial Winograd Schema Challenge at Scale"
        },
        {
            "paperId": "86f86f7017ca11d4d849006b2938e6f02bfe16d9",
            "title": "news-please - A Generic News Crawler and Extractor"
        },
        {
            "paperId": "015ca32bca81dbda1e2e432445eef798582236e1",
            "title": "Conference Paper"
        },
        {
            "paperId": "9aba7fc67b2265cbc7fb1e91c7baa6c896ff928c",
            "title": "Constrained Differential Optimization"
        },
        {
            "paperId": null,
            "title": ": An open source recipe to reproduce"
        },
        {
            "paperId": null,
            "title": "TogetherAI"
        },
        {
            "paperId": null,
            "title": "MosaicML"
        },
        {
            "paperId": null,
            "title": "Large language model distillation doesn\u2019t need a teacher"
        },
        {
            "paperId": null,
            "title": "Falcon-40B: an open large language model with state-of-the-art performance"
        }
    ]
}