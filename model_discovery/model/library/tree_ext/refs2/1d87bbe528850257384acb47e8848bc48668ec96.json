{
    "paperId": "1d87bbe528850257384acb47e8848bc48668ec96",
    "externalIds": {
        "MAG": "2891962243",
        "DBLP": "journals/corr/abs-1810-12482",
        "ArXiv": "1810.12482",
        "CorpusId": 53114131
    },
    "title": "Using Large Ensembles of Control Variates for Variational Inference",
    "abstract": "Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient's variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.",
    "venue": "Neural Information Processing Systems",
    "year": 2018,
    "referenceCount": 33,
    "citationCount": 29,
    "influentialCitationCount": 3,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Bayesian risk minimization framework is presented in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "17966996",
            "name": "Tomas Geffner"
        },
        {
            "authorId": "1722101",
            "name": "Justin Domke"
        }
    ],
    "references": [
        {
            "paperId": "9884d18f265f9178ff9862d53cacbbc9957ddc4c",
            "title": "Advances in Variational Inference"
        },
        {
            "paperId": "682d194235ba3b573889836ba118502e8b525728",
            "title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation"
        },
        {
            "paperId": "56b624f65a506d73a70684ab6550f3ff72724444",
            "title": "Reducing Reparameterization Gradient Variance"
        },
        {
            "paperId": "e6a7f9261861a0eba6074079545c2fd51d1265df",
            "title": "Sticking the Landing: An Asymptotically Zero-Variance Gradient Estimator for Variational Inference"
        },
        {
            "paperId": "0c73c7cf2558814a73779379477e1987f917c533",
            "title": "Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference"
        },
        {
            "paperId": "a642bbbaf8822565f9b812ea279c596cc54ce4c3",
            "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"
        },
        {
            "paperId": "29e944711a354c396fad71936f536e83025b6ce0",
            "title": "Categorical Reparameterization with Gumbel-Softmax"
        },
        {
            "paperId": "515a21e90117941150923e559729c59f5fdade1c",
            "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"
        },
        {
            "paperId": "787ffb182e0555691d1c90047e16b8f3ff49bf0b",
            "title": "The Generalized Reparameterization Gradient"
        },
        {
            "paperId": "ae1b1d3d71b4da49abb51d9f6520b0a1a8e1d321",
            "title": "A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification"
        },
        {
            "paperId": "fccf3f8fea450a32a5c1f85e113127d70ed9ad7c",
            "title": "Overdispersed Black-Box Variational Inference"
        },
        {
            "paperId": "6f24d7a6e1c88828e18d16c6db20f5329f6a6827",
            "title": "Variational Inference: A Review for Statisticians"
        },
        {
            "paperId": "d87a6168f15452bc50a4bbd034fa097c223100ea",
            "title": "Local Expectation Gradients for Black Box Variational Inference"
        },
        {
            "paperId": "4443e2e5bfd112562ecef578f772cee3c692f19f",
            "title": "Variational Dropout and the Local Reparameterization Trick"
        },
        {
            "paperId": "d62a9746861aecbfb689745abb860c1c773a12b1",
            "title": "Variational Recurrent Auto-Encoders"
        },
        {
            "paperId": "d6559f35be0679c6b3371a2e44e3be293704b600",
            "title": "Deep Exponential Families"
        },
        {
            "paperId": "2538378f9b2aee6d2cfe106138070274f8edaa1f",
            "title": "Doubly Stochastic Variational Bayes for non-Conjugate Inference"
        },
        {
            "paperId": "331f0fb3b6176c6e463e0401025b04f6ace9ccd3",
            "title": "Neural Variational Inference and Learning in Belief Networks"
        },
        {
            "paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
            "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
        },
        {
            "paperId": "6a667700100e228cb30a5d884258a0db921603fe",
            "title": "Black Box Variational Inference"
        },
        {
            "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "title": "Auto-Encoding Variational Bayes"
        },
        {
            "paperId": "5fb8a9271af105a5065a5a855e71a7d25c7a6f1b",
            "title": "Variance Reduction for Stochastic Gradient Optimization"
        },
        {
            "paperId": "bccb2f99a9d1c105699f5d88c479569085e2c7ba",
            "title": "Stochastic variational inference"
        },
        {
            "paperId": "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3",
            "title": "Variational Bayesian Inference with Stochastic Search"
        },
        {
            "paperId": "63078445fa1aca9887ccb59da20503854e8779ab",
            "title": "Concave Gaussian Variational Approximations for Inference in Large-Scale Bayesian Linear Models"
        },
        {
            "paperId": "ff673be7a9367a7a79844a96f9299a2da64a41c6",
            "title": "Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization"
        },
        {
            "paperId": "766084051fa7168370dc49d64fd491b794e5a929",
            "title": "Variational methods for Reinforcement Learning"
        },
        {
            "paperId": "d98d0d1900b13b87aa4ffd6b69c046beb63f0434",
            "title": "Graphical Models, Exponential Families, and Variational Inference"
        },
        {
            "paperId": "96912cf06fe63e7047cbd6df3063a08f8827d398",
            "title": "The Infinite PCFG Using Hierarchical Dirichlet Processes"
        },
        {
            "paperId": "742432ccfa877d29ec30b19f0b7a3f5d4422681c",
            "title": "Variational Message Passing"
        },
        {
            "paperId": "6120cc252bc74239012f11b8b075cb7cb16bee26",
            "title": "An Introduction to Variational Methods for Graphical Models"
        },
        {
            "paperId": "4574d77fff19e093782178595a8988a7f3aa1969",
            "title": "Latent Dirichlet Allocation"
        },
        {
            "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
        }
    ]
}