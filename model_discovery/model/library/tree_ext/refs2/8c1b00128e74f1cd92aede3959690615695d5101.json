{
    "paperId": "8c1b00128e74f1cd92aede3959690615695d5101",
    "externalIds": {
        "MAG": "2963564796",
        "DBLP": "conf/iclr/YuDLZ00L18",
        "ArXiv": "1804.09541",
        "CorpusId": 4842909
    },
    "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
    "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 49,
    "citationCount": 1053,
    "influentialCitationCount": 175,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new Q\\&A architecture called QANet is proposed, which does not require recurrent networks, and its encoder consists exclusively of convolution and self-attention, where convolution models local interactions andSelf-att attention models global interactions."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "40625240",
            "name": "Adams Wei Yu"
        },
        {
            "authorId": "35363891",
            "name": "David Dohan"
        },
        {
            "authorId": "1707242",
            "name": "Minh-Thang Luong"
        },
        {
            "authorId": "2114012077",
            "name": "Rui Zhao"
        },
        {
            "authorId": "2118440152",
            "name": "Kai Chen"
        },
        {
            "authorId": "144739074",
            "name": "Mohammad Norouzi"
        },
        {
            "authorId": "2827616",
            "name": "Quoc V. Le"
        }
    ],
    "references": [
        {
            "paperId": "8490431f3a76fbd165d108eba938ead212a2a639",
            "title": "Stochastic Answer Networks for Machine Reading Comprehension"
        },
        {
            "paperId": "3c78c6df5eb1695b6a399e346dde880af27d1016",
            "title": "Simple and Effective Multi-Paragraph Reading Comprehension"
        },
        {
            "paperId": "adc276e6eae7051a027a4c269fb21dae43cadfed",
            "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding"
        },
        {
            "paperId": "de0c30321b22c56d637e7c29cb59180f157272a8",
            "title": "Globally Normalized Reader"
        },
        {
            "paperId": "34288ac693ce572bf202d2dfa3edebdd74f25073",
            "title": "Learning to Paraphrase for Question Answering"
        },
        {
            "paperId": "12e20e4ea572dbe476fd894c5c9a9930cf250dd2",
            "title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension"
        },
        {
            "paperId": "ffb949d3493c3b2f3c9acf9c75cb03938933ddf0",
            "title": "Adversarial Examples for Evaluating Reading Comprehension Systems"
        },
        {
            "paperId": "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f",
            "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83",
            "title": "Depthwise Separable Convolutions for Neural Machine Translation"
        },
        {
            "paperId": "d3304b926cfcd91110bd5ba01db21d26ce5fca2d",
            "title": "Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext"
        },
        {
            "paperId": "42d2338a8c2e44154e10ff4d68a3c389aeca3913",
            "title": "Reinforced Mnemonic Reader for Machine Comprehension"
        },
        {
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning"
        },
        {
            "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
            "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
        },
        {
            "paperId": "c50cd7df4271ef94a0a60894f0e2cf4ef89fb912",
            "title": "Ruminating Reader: Reasoning with Gated Multi-hop Attention"
        },
        {
            "paperId": "c25a67ad7e8629a9d12b9e2fc356cd73af99a060",
            "title": "Learning to Skim Text"
        },
        {
            "paperId": "baa1079d1ee40754c44f5ee1498d6ba0c9c71b32",
            "title": "Paraphrasing Revisited with Neural Machine Translation"
        },
        {
            "paperId": "a8b21f72bdc251689f636d3d7ff52a6b85ab7ce9",
            "title": "Neural Question Generation from Text: A Preliminary Study"
        },
        {
            "paperId": "104715e1097b7ebee436058bfd9f45540f269845",
            "title": "Reading Wikipedia to Answer Open-Domain Questions"
        },
        {
            "paperId": "46a7afc2b23bb3406fb64c36b6f2696145b54f24",
            "title": "Making Neural QA as Simple as Possible but not Simpler"
        },
        {
            "paperId": "62f88e8fc3b44c5627f2b4721b08498d78103893",
            "title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering"
        },
        {
            "paperId": "d2ad6ae5f57844c4474cd3f318c3cdc469994fae",
            "title": "Structural Embedding of Syntactic Trees for Machine Comprehension"
        },
        {
            "paperId": "e94697b98b707f557436e025bdc8498fa261d3bc",
            "title": "Multi-Perspective Context Matching for Machine Comprehension"
        },
        {
            "paperId": "e978d832a4d86571e1b52aa1685dc32ccb250f50",
            "title": "Dynamic Coattention Networks For Question Answering"
        },
        {
            "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
            "title": "Bidirectional Attention Flow for Machine Comprehension"
        },
        {
            "paperId": "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72",
            "title": "Learning Recurrent Span Representations for Extractive Question Answering"
        },
        {
            "paperId": "a8c33413a626bafc67d46029ed28c2a28cc08899",
            "title": "End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking"
        },
        {
            "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions"
        },
        {
            "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
            "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
        },
        {
            "paperId": "c636a2dd242908fe2e598a1077c0c57bfdea8633",
            "title": "ReasoNet: Learning to Stop Reading in Machine Comprehension"
        },
        {
            "paperId": "832fc9327695f7425d8759c6aaeec0fa2d7b0a90",
            "title": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "c6e5df6322659276da6133f9b734a389d7a255e8",
            "title": "Attention-over-Attention Neural Networks for Reading Comprehension"
        },
        {
            "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
            "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
            "title": "Deep Networks with Stochastic Depth"
        },
        {
            "paperId": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d",
            "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
        },
        {
            "paperId": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb",
            "title": "Improving Neural Machine Translation Models with Monolingual Data"
        },
        {
            "paperId": "35b91b365ceb016fb3e022577cec96fb9b445dc5",
            "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations"
        },
        {
            "paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e",
            "title": "Character-level Convolutional Networks for Text Classification"
        },
        {
            "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
            "title": "Effective Approaches to Attention-based Neural Machine Translation"
        },
        {
            "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
            "title": "Teaching Machines to Read and Comprehend"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "c022200710e62003ef324ad6771269e5ea4b5247",
            "title": "Machine Comprehension using match-LSTM and Answer-Pointer"
        }
    ]
}