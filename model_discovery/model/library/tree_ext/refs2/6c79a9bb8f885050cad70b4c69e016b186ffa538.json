{
    "paperId": "6c79a9bb8f885050cad70b4c69e016b186ffa538",
    "externalIds": {
        "MAG": "2047706513",
        "DOI": "10.1121/1.2017061",
        "CorpusId": 121084921
    },
    "title": "Trainable grammars for speech recognition",
    "abstract": "Algorithms which are based on modeling speech as a finite\u2010state, hidden Markov process have been very successful in recent years. This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes. This algorithm permits automatic training of the stochastic analog of an arbitrary context free grammar. In particular, in contrast to many grammatical inference methods, the new algorithm allows the grammar to have an arbitrary degree of ambiguity. Since natural language is often syntactically ambiguous, it is necessary for the grammatical inference algorithm to allow for this ambiguity. Furthermore, allowing ambiguity in the grammar allows errors in the recognition process to be explicitly modeled in the grammar rather than added as an extra component.",
    "venue": "",
    "year": 1979,
    "referenceCount": 0,
    "citationCount": 669,
    "influentialCitationCount": 76,
    "openAccessPdf": {
        "url": "https://pubs.aip.org/asa/jasa/article-pdf/65/S1/S132/11520317/s132_1_online.pdf",
        "status": "BRONZE"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes that permits automatic training of the stochastic analog of an arbitrary context free grammar."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "144153201",
            "name": "J. Baker"
        }
    ],
    "references": []
}