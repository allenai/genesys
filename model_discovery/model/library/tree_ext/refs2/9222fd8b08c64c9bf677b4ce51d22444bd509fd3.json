{
    "paperId": "9222fd8b08c64c9bf677b4ce51d22444bd509fd3",
    "externalIds": {
        "MAG": "3048352504",
        "DBLP": "phd/dnb/Irie20",
        "DOI": "10.18154/RWTH-2020-04984",
        "CorpusId": 221980941
    },
    "title": "Advancing neural language modeling in automatic speech recognition",
    "abstract": "Statistical language modeling is one of the fundamental problems in natural language processing. In the recent years, language modeling has seen great advances by active research and engineering efforts in applying artificial neural networks, especially those which are recurrent. The application of neural language models to speech recognition has now become well established and ubiquitous. Despite this impression of some degree of maturity, we claim that the full potential of the neural network based language modeling is yet to be explored. In this thesis, we further advance neural language modeling in automatic speech recognition, by investigating a number of new perspectives. From the architectural view point, we investigate the newly proposed Transformer neural networks for language modeling application. The original model architecture proposed for machine translation is studied and modified to accommodate the specific task of language modeling. Particularly deep models with about one hundred layers are developed. We present an in-depth comparison with the state-of-the-art recurrent neural network language models based on the long short-term memory. While scaling up language modeling to larger scale datasets, the diversity of the data emerges as an opportunity and a challenge. The current state-of-the-art neural language modeling lacks a mechanism of handling diverse data from different domains for a single model to perform well across different domains. In this context, we introduce domain robust language modeling with neural networks, and propose two solutions. As a first solution, we propose a new type of adaptive mixture of experts model which is fully based on neural networks. In the second approach, we investigate knowledge distillation from multiple domain expert models, as a solution to the large model size problem seen in the first approach. Methods for practical applications of knowledge distillation to large vocabulary language modeling are proposed, and studied to a large extent. Finally, we investigate the potential of neural language models to leverage long-span crosssentence contexts for cross-utterance speech recognition. The appropriate training method for such a scenario is under-explored in the existing works. We carry out systematic comparisons of the training methods, allowing us to achieve improvements in cross-utterance speech recognition. In the same context, we study the sequence length robustness for both recurrent neural networks based on the long short-term memory and Transformers, because such a robustness is one of the fundamental properties we wish to have, in neural networks with the ability to handle variable length contexts. Throughout the thesis, we tackle these problems through novel perspectives of neural language modeling, while keeping the traditional spirit of language modeling in speech recognition.",
    "venue": "",
    "year": 2020,
    "referenceCount": 302,
    "citationCount": 9,
    "influentialCitationCount": 1,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This thesis introduces domain robust language modeling with neural networks, and proposes a new type of adaptive mixture of experts model which is fully based on neural networks as a solution to the large model size problem seen in the current state-of-the-art neural language modeling."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -0.2874143123626709,
            0.5485033988952637,
            -0.8044332265853882,
            2.352151870727539,
            3.7085299491882324,
            0.5400300025939941,
            3.1990931034088135,
            -1.747611165046692,
            -1.4237143993377686,
            1.630643606185913,
            0.9477316737174988,
            4.1688127517700195,
            -1.2908927202224731,
            -1.3674215078353882,
            0.6487953662872314,
            2.1367990970611572,
            -0.7054246664047241,
            0.12511855363845825,
            2.7486186027526855,
            0.05306220054626465,
            -3.2624902725219727,
            3.176002025604248,
            -3.292623996734619,
            -3.716118335723877,
            1.0832412242889404,
            -0.9931368231773376,
            2.3089847564697266,
            1.7002902030944824,
            -4.000146389007568,
            2.150508165359497,
            -1.6106531620025635,
            -3.830517292022705,
            1.9093663692474365,
            -4.121044158935547,
            4.5142998695373535,
            1.8254166841506958,
            -6.407318115234375,
            6.738681793212891,
            -3.5615925788879395,
            -3.1588339805603027,
            1.2880325317382812,
            1.9066221714019775,
            -0.025057069957256317,
            4.116920471191406,
            4.1554388999938965,
            -0.020360372960567474,
            -0.0035718679428100586,
            -0.4833909273147583,
            -0.32255467772483826,
            1.1593542098999023,
            3.2985951900482178,
            -1.3850702047348022,
            0.03089892864227295,
            3.94620680809021,
            0.7439929246902466,
            0.767143964767456,
            1.3333107233047485,
            -0.506045401096344,
            2.1374635696411133,
            -4.93577241897583,
            1.0312154293060303,
            4.520514965057373,
            0.4898500442504883,
            1.348510980606079,
            4.349835395812988,
            -6.872976303100586,
            -3.2253897190093994,
            0.8638471961021423,
            4.990631580352783,
            4.59375524520874,
            -1.0185737609863281,
            -7.672943115234375,
            0.1492580771446228,
            -1.1777511835098267,
            -1.5518208742141724,
            3.358181953430176,
            -1.747666597366333,
            -4.576970100402832,
            1.1573735475540161,
            -3.9629697799682617,
            -1.1262602806091309,
            -0.46502813696861267,
            1.1588584184646606,
            0.6037112474441528,
            7.815781593322754,
            0.6517353057861328,
            -7.83963680267334,
            -1.3140864372253418,
            1.7096307277679443,
            -2.138339042663574,
            0.5276615619659424,
            -2.3859007358551025,
            1.38805091381073,
            2.6409175395965576,
            0.15341264009475708,
            -2.3920552730560303,
            2.1138088703155518,
            -2.8153350353240967,
            -1.151956558227539,
            -0.9847238063812256,
            5.213952541351318,
            -3.325883388519287,
            1.7765880823135376,
            0.08392912149429321,
            3.5419397354125977,
            -5.412689685821533,
            4.010512351989746,
            2.3267018795013428,
            -1.7135748863220215,
            -0.7059428691864014,
            -5.829414367675781,
            4.636240482330322,
            -1.7081334590911865,
            -2.2839317321777344,
            1.4760875701904297,
            0.5242693424224854,
            -0.6209490299224854,
            2.4314098358154297,
            -2.5789947509765625,
            3.425692319869995,
            -3.171725034713745,
            -1.7259787321090698,
            -1.2064825296401978,
            -0.42294973134994507,
            -1.7988457679748535,
            3.6188747882843018,
            0.7955358028411865,
            2.1227529048919678,
            2.0433154106140137,
            0.6651644110679626,
            2.0005180835723877,
            -1.3584120273590088,
            -0.3645436763763428,
            -1.4929113388061523,
            2.0337307453155518,
            3.6950855255126953,
            -2.3162038326263428,
            5.333158493041992,
            0.7872543334960938,
            0.2525583803653717,
            0.6129909157752991,
            2.3114981651306152,
            1.6267640590667725,
            1.8082377910614014,
            0.6465264558792114,
            1.2747050523757935,
            2.483773708343506,
            -2.702733278274536,
            -0.9945676922798157,
            4.2440996170043945,
            1.4516555070877075,
            -1.3322057723999023,
            0.44359251856803894,
            -1.6155428886413574,
            -1.8870971202850342,
            5.474832057952881,
            -5.2651472091674805,
            1.0146504640579224,
            -1.036658525466919,
            2.5589780807495117,
            -1.411387324333191,
            -3.0789151191711426,
            -10.073616027832031,
            -3.0896759033203125,
            1.004832148551941,
            -5.75077486038208,
            -0.24187755584716797,
            2.7862119674682617,
            1.0169916152954102,
            5.975653648376465,
            -1.2397528886795044,
            -1.6697475910186768,
            -0.2037951499223709,
            3.4459757804870605,
            3.920177459716797,
            4.308176517486572,
            3.094959020614624,
            -4.522435665130615,
            -1.18367600440979,
            0.2577130198478699,
            -0.4834640622138977,
            -2.2789556980133057,
            -5.725510597229004,
            -2.126845121383667,
            -2.277834415435791,
            -3.439483165740967,
            -0.4783782660961151,
            -1.1665472984313965,
            2.399646282196045,
            -1.142380952835083,
            -0.007234424352645874,
            3.620063066482544,
            1.6041183471679688,
            7.727153778076172,
            1.247696876525879,
            0.9462462067604065,
            2.5140533447265625,
            5.005855560302734,
            -4.206621170043945,
            -3.084946632385254,
            3.6080260276794434,
            -2.6645400524139404,
            2.7189812660217285,
            -3.2349119186401367,
            3.04947566986084,
            3.0706562995910645,
            -5.680622577667236,
            2.2873477935791016,
            0.7888972163200378,
            -2.1162497997283936,
            0.7555442452430725,
            -0.983674168586731,
            -0.7243856191635132,
            3.5630409717559814,
            -2.13655424118042,
            -0.64853835105896,
            -3.0167236328125,
            1.1195955276489258,
            7.056562423706055,
            -0.34443169832229614,
            -4.853869915008545,
            1.9680224657058716,
            -2.6035544872283936,
            -1.681896448135376,
            2.6558375358581543,
            -0.5033556818962097,
            3.753492593765259,
            0.19236773252487183,
            1.3573461771011353,
            -1.7592490911483765,
            -2.110928773880005,
            -0.7470076084136963,
            1.3283573389053345,
            0.9022167325019836,
            -2.541884183883667,
            1.3167235851287842,
            -6.624603271484375,
            0.42771920561790466,
            2.2611746788024902,
            -1.0554898977279663,
            6.007923603057861,
            -1.2445943355560303,
            1.5926854610443115,
            2.250398874282837,
            -1.986226201057434,
            -2.022245407104492,
            -2.1297945976257324,
            -1.2285354137420654,
            -2.351897954940796,
            -4.430941104888916,
            -1.154288649559021,
            -0.29793035984039307,
            1.4760973453521729,
            2.08032488822937,
            2.3301970958709717,
            0.1497911512851715,
            1.1054848432540894,
            -0.42887330055236816,
            0.8062999248504639,
            3.343050479888916,
            -0.4580160081386566,
            6.5344648361206055,
            0.26636067032814026,
            3.0217552185058594,
            -2.1264004707336426,
            -0.3700491189956665,
            -1.6629635095596313,
            1.2373113632202148,
            -1.9827816486358643,
            4.611748218536377,
            1.4881521463394165,
            2.1329636573791504,
            -0.8093112111091614,
            -2.851801872253418,
            0.9332866072654724,
            -1.9135384559631348,
            -5.209359645843506,
            -3.2681446075439453,
            6.239985942840576,
            3.2895803451538086,
            2.0826635360717773,
            -3.6860384941101074,
            0.022862285375595093,
            -7.654595375061035,
            -2.0022482872009277,
            -3.511434555053711,
            -5.361608505249023,
            -3.1408114433288574,
            -2.6894707679748535,
            -2.656536817550659,
            -4.422420024871826,
            1.9295191764831543,
            -3.2078986167907715,
            -0.14084169268608093,
            -1.1792573928833008,
            0.5715577006340027,
            1.7515876293182373,
            1.3467313051223755,
            0.5444132089614868,
            3.032881736755371,
            0.11912635713815689,
            1.437760353088379,
            5.262670516967773,
            0.70879065990448,
            -0.12018361687660217,
            2.6046142578125,
            1.400161862373352,
            -4.362249374389648,
            -0.7036629319190979,
            -2.5709409713745117,
            -1.8950589895248413,
            2.014686107635498,
            5.421750545501709,
            -2.9211878776550293,
            0.8054714798927307,
            0.486422061920166,
            -0.2843160927295685,
            -1.5652413368225098,
            -7.768474578857422,
            3.1015214920043945,
            -3.8161396980285645,
            -1.7803064584732056,
            -7.7913408279418945,
            -1.3967063426971436,
            -1.570347785949707,
            1.5702013969421387,
            4.706730842590332,
            0.11796359717845917,
            -0.5662409663200378,
            6.681424140930176,
            4.096938133239746,
            1.698257327079773,
            1.9072177410125732,
            4.063943386077881,
            -0.18971776962280273,
            -4.939731121063232,
            0.13317346572875977,
            2.319444179534912,
            1.6575944423675537,
            0.021268486976623535,
            0.40833553671836853,
            6.2179388999938965,
            -1.5709617137908936,
            0.8437469005584717,
            2.49746036529541,
            -1.5031614303588867,
            1.4737945795059204,
            0.538331151008606,
            0.9618131518363953,
            1.5010349750518799,
            2.0561747550964355,
            2.489842414855957,
            2.518904209136963,
            -0.8469278216362,
            2.1294445991516113,
            -1.215736746788025,
            -0.5529589056968689,
            0.7924173474311829,
            -1.9538078308105469,
            2.00236439704895,
            0.2969619929790497,
            2.5838112831115723,
            3.9662349224090576,
            -1.5933557748794556,
            2.7452054023742676,
            -3.314173936843872,
            11.297964096069336,
            -4.727177143096924,
            -0.2499885857105255,
            -6.3851318359375,
            1.6835176944732666,
            -2.083164930343628,
            -2.611760139465332,
            4.20028829574585,
            -1.5795938968658447,
            -4.716440200805664,
            0.5695735812187195,
            -3.3844356536865234,
            2.423266649246216,
            2.2020649909973145,
            -2.489386558532715,
            4.011955261230469,
            -1.400631070137024,
            1.2522096633911133,
            0.5983991622924805,
            2.884828567504883,
            -3.2290518283843994,
            2.5279057025909424,
            3.1744778156280518,
            -1.148852825164795,
            -3.417571783065796,
            1.7525887489318848,
            -0.897247850894928,
            -0.36991846561431885,
            -5.698829174041748,
            -6.987228870391846,
            -2.7181448936462402,
            -1.8698288202285767,
            0.9839659929275513,
            5.94962215423584,
            1.5132648944854736,
            2.51104736328125,
            3.928696870803833,
            2.0780930519104004,
            -4.999098777770996,
            -1.8308417797088623,
            4.544094085693359,
            0.5034849643707275,
            -3.3939104080200195,
            1.6200535297393799,
            -2.8219387531280518,
            -2.622330665588379,
            -1.5786619186401367,
            -2.8008041381835938,
            -2.728944778442383,
            -2.4229249954223633,
            4.702113628387451,
            4.505061626434326,
            4.322046756744385,
            0.9053071141242981,
            -3.2220935821533203,
            2.5146677494049072,
            5.332485675811768,
            3.5506155490875244,
            -3.2292561531066895,
            1.1429722309112549,
            2.7733640670776367,
            2.024068832397461,
            -0.02241307497024536,
            3.2134552001953125,
            -1.3569257259368896,
            2.565896987915039,
            -5.213709354400635,
            -1.5809465646743774,
            1.325908899307251,
            1.3496090173721313,
            1.8590354919433594,
            0.35784369707107544,
            1.365049123764038,
            -0.8820092678070068,
            2.8130414485931396,
            1.1814075708389282,
            0.8672462701797485,
            3.300297260284424,
            -2.030604839324951,
            0.5442520380020142,
            0.21087145805358887,
            2.5724003314971924,
            0.11512774229049683,
            -2.731698989868164,
            4.456696510314941,
            -3.484069347381592,
            -3.72868013381958,
            -0.39340096712112427,
            -0.7878189086914062,
            2.4022791385650635,
            -2.146611213684082,
            0.4655909240245819,
            2.2322614192962646,
            1.739977478981018,
            -1.0746207237243652,
            3.293396234512329,
            -6.378567695617676,
            0.2546306848526001,
            -2.8262758255004883,
            -1.566638469696045,
            1.5816587209701538,
            -3.044934034347534,
            1.8582465648651123,
            1.6288045644760132,
            1.9812812805175781,
            -0.2777629494667053,
            -4.660555839538574,
            0.8436941504478455,
            -0.035237908363342285,
            -0.16226013004779816,
            3.1143264770507812,
            -0.5583242177963257,
            -0.5428415536880493,
            -3.2196640968322754,
            -5.397866725921631,
            -2.830397844314575,
            1.9525840282440186,
            -3.155031681060791,
            -2.1360723972320557,
            4.119887351989746,
            0.8829785585403442,
            1.3991694450378418,
            0.5123828649520874,
            -3.639239549636841,
            -0.3657471239566803,
            3.3462657928466797,
            5.677399158477783,
            0.42995452880859375,
            -1.3410918712615967,
            -0.922906756401062,
            -5.2981157302856445,
            2.5000383853912354,
            2.9192867279052734,
            -1.4015231132507324,
            1.846732258796692,
            -4.915299892425537,
            0.21279925107955933,
            1.9240578413009644,
            -1.2149622440338135,
            5.3633904457092285,
            4.807662010192871,
            -2.0591747760772705,
            -3.2135438919067383,
            -1.2944837808609009,
            -2.3834052085876465,
            2.305335760116577,
            -3.483081102371216,
            1.10076904296875,
            1.8293757438659668,
            -0.2836214601993561,
            3.177842617034912,
            -2.232896327972412,
            2.537489891052246,
            -3.0769293308258057,
            -2.4420526027679443,
            -4.191332817077637,
            0.016322612762451172,
            3.6446285247802734,
            -2.4168102741241455,
            1.264115571975708,
            2.645123243331909,
            1.0487924814224243,
            1.4997642040252686,
            3.8577253818511963,
            3.577772378921509,
            2.5349717140197754,
            2.647512435913086,
            -3.6681342124938965,
            0.7337824106216431,
            -0.5392045378684998,
            -0.5105903744697571,
            -0.11869901418685913,
            -5.322534561157227,
            -0.4131423830986023,
            1.8156142234802246,
            3.301511764526367,
            1.8161412477493286,
            2.7789688110351562,
            -0.3682383596897125,
            1.369685173034668,
            -3.7527902126312256,
            -1.5787053108215332,
            -3.2373790740966797,
            0.5122544169425964,
            -0.6139984726905823,
            2.1015281677246094,
            2.192103862762451,
            3.5796608924865723,
            -3.4733364582061768,
            -1.6910123825073242,
            0.9310437440872192,
            -0.6154981851577759,
            -1.72781503200531,
            5.101252555847168,
            4.923837184906006,
            0.9150110483169556,
            -1.9031057357788086,
            5.279958724975586,
            0.9531606435775757,
            0.10653179883956909,
            0.8663618564605713,
            6.467988967895508,
            0.9326846599578857,
            -4.331036567687988,
            1.5839142799377441,
            1.1896388530731201,
            2.9527647495269775,
            2.451603651046753,
            0.7139922380447388,
            2.6944241523742676,
            2.2612569332122803,
            0.09081947803497314,
            -5.073239803314209,
            1.5933971405029297,
            -2.599148988723755,
            -0.8477312922477722,
            -0.6462380290031433,
            -2.963995933532715,
            -0.19703230261802673,
            -4.437774181365967,
            -3.2699594497680664,
            3.7417664527893066,
            -5.80495548248291,
            0.26683440804481506,
            4.066056728363037,
            -1.4176429510116577,
            1.8630057573318481,
            -0.7101058959960938,
            -2.3058223724365234,
            -6.505094051361084,
            1.5891937017440796,
            6.309387683868408,
            -3.2896132469177246,
            2.880526542663574,
            -0.4122586250305176,
            2.2750864028930664,
            0.8819002509117126,
            0.20223669707775116,
            3.7416415214538574,
            1.0020618438720703,
            3.1332626342773438,
            1.409877896308899,
            0.4034458100795746,
            -1.3051791191101074,
            1.7047719955444336,
            3.2185680866241455,
            17.179794311523438,
            2.684145927429199,
            -0.17130114138126373,
            1.415496826171875,
            -3.097818374633789,
            -2.2488932609558105,
            -2.027686595916748,
            0.9827028512954712,
            0.05450662970542908,
            2.9712936878204346,
            -2.812370777130127,
            -0.8944023847579956,
            -1.851578712463379,
            2.9232282638549805,
            -3.040708303451538,
            -0.1370076835155487,
            -1.2835302352905273,
            3.6528079509735107,
            -0.26071834564208984,
            -1.5701345205307007,
            -1.2562698125839233,
            1.094892978668213,
            -0.538719654083252,
            -0.8003212213516235,
            -0.3838953971862793,
            2.7152585983276367,
            2.2128705978393555,
            0.5177313089370728,
            -3.2979564666748047,
            0.7407552003860474,
            -0.8250542283058167,
            0.6544016599655151,
            -0.20150691270828247,
            1.2279367446899414,
            -5.932975769042969,
            3.410216808319092,
            3.109382390975952,
            -2.4994029998779297,
            1.4621121883392334,
            -0.17168845236301422,
            -0.11586427688598633,
            0.8090806603431702,
            -5.058979034423828,
            1.3578753471374512,
            -0.20131805539131165,
            2.382540702819824,
            1.3886821269989014,
            -1.0590274333953857,
            -0.45517683029174805,
            7.800966262817383,
            2.3540778160095215,
            -2.476850748062134,
            -0.5927662253379822,
            -1.8817248344421387,
            0.012801103293895721,
            2.955016613006592,
            -0.2669585943222046,
            -3.7013816833496094,
            4.88962459564209,
            -2.2012038230895996,
            3.5311341285705566,
            -1.9471651315689087,
            -2.1584370136260986,
            -3.683835506439209,
            -3.99807071685791,
            -0.3306674361228943,
            -7.410292148590088,
            2.2989609241485596,
            1.530484914779663,
            2.174882173538208,
            -1.8850386142730713,
            5.624382495880127,
            -1.1009126901626587,
            -0.323936402797699,
            -2.4674580097198486,
            -2.3676891326904297,
            4.400228500366211,
            -2.950216293334961,
            0.07129335403442383,
            4.979789733886719,
            -3.7570314407348633,
            4.000396251678467,
            -4.298105239868164,
            -0.7296560406684875,
            2.4738433361053467,
            -1.8289633989334106,
            3.643908977508545,
            2.2531490325927734,
            -1.4069417715072632,
            1.1672601699829102,
            -1.6971851587295532,
            -1.219519853591919,
            4.282679080963135,
            2.8101675510406494,
            -1.7558963298797607,
            -5.766570091247559,
            -0.9906851649284363,
            -2.4356346130371094,
            -7.030148983001709,
            -2.6167078018188477,
            3.31903076171875,
            1.4344134330749512,
            -0.40548816323280334,
            -2.9183802604675293,
            -0.16294915974140167,
            -1.4072481393814087,
            0.9832096695899963,
            -3.4333443641662598,
            -5.193608283996582,
            -0.47032061219215393,
            2.549823522567749,
            -2.90093731880188,
            -3.2229480743408203,
            1.9091110229492188,
            1.8432180881500244,
            -1.8669686317443848,
            -2.763251781463623,
            3.162810802459717,
            -0.9757692813873291,
            0.816490113735199,
            0.8722527027130127,
            -3.2210726737976074,
            -2.991564989089966,
            -4.9995574951171875,
            -5.59776496887207,
            0.8357105851173401,
            5.006381511688232,
            -1.0825824737548828,
            -0.847137451171875,
            -3.111732244491577,
            3.224335193634033,
            -3.231919288635254,
            2.092806339263916,
            -1.7809721231460571,
            -2.697253465652466,
            -1.424498438835144,
            -0.2885136902332306,
            0.642632246017456,
            -1.7931907176971436,
            2.275510787963867,
            4.282604217529297,
            -0.02775813639163971,
            1.159635305404663,
            9.865877151489258,
            -0.195604145526886,
            -2.1480536460876465,
            0.23321940004825592,
            3.690734624862671,
            -0.3678617477416992,
            0.2680368423461914,
            1.3912627696990967,
            -0.9900526404380798,
            3.4396893978118896,
            -1.3997111320495605,
            -0.8012940883636475,
            -0.5145419836044312
        ]
    },
    "authors": [
        {
            "authorId": "2350348",
            "name": "Kazuki Irie"
        }
    ],
    "references": [
        {
            "paperId": "183ae1cf7242e372050c3bea3b44b7613b3df6aa",
            "title": "Applying GPGPU to recurrent neural network language model based fast network search in the real-time LVCSR"
        },
        {
            "paperId": "24c88df5ffea7d7ff649a9d214e580ae4f1dfdb8",
            "title": "Domain Robust, Fast, and Compact Neural Language Models"
        },
        {
            "paperId": "a88c5474d96a6629bd1f36c3c6c06242034cb0a4",
            "title": "How Much Self-Attention Do We Need\u0192 Trading Attention for Feed-Forward Layers"
        },
        {
            "paperId": "f27b5050cf8fa8258ef3c988c6fd853f3de5c17f",
            "title": "The Rwth Asr System for Ted-Lium Release 2: Improving Hybrid Hmm With Specaugment"
        },
        {
            "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "title": "Reformer: The Efficient Transformer"
        },
        {
            "paperId": "d0e28f5dc1feae19e41087a92a87992977fd85af",
            "title": "Encoding word order in complex embeddings"
        },
        {
            "paperId": "96b7abb1d35ec4bd6822f5a55996b3c4dd5c91ae",
            "title": "Training Language Models for Long-Span Cross-Sentence Evaluation"
        },
        {
            "paperId": "bc1ab519c225b08332f243269ad6d99284bbf1bf",
            "title": "A Comparison of Transformer and LSTM Encoder Decoder Models for ASR"
        },
        {
            "paperId": "24472a31618bbc260e2bf45bd72427097875142b",
            "title": "End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures"
        },
        {
            "paperId": "f51497f463566581874c941353dd9d80069c5b77",
            "title": "Compressive Transformers for Long-Range Sequence Modelling"
        },
        {
            "paperId": "678cba6df672a9160085b75d4e4294165e4bbed8",
            "title": "Recognizing Long-Form Speech Using Streaming End-to-End Models"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "d9c3bc1c2915ff02d33f66c800ad756cb87821d6",
            "title": "Transformer-Based Acoustic Modeling for Hybrid Speech Recognition"
        },
        {
            "paperId": "c611b5256ea200f08adae5a2825da0839a345bfe",
            "title": "State-of-the-Art Speech Recognition Using Multi-Stream Self-Attention with Dilated 1D Convolutions"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "93800f8d7dd7647167d2a41bc479fa8cab7b24f3",
            "title": "Masked Translation Model"
        },
        {
            "paperId": "57341a6a1689f9b30d02ef602dc40c77217ab968",
            "title": "Speech Recognition with Augmented Synthesized Speech"
        },
        {
            "paperId": "0ce184bd55a4736ec64e5d82a85421298e0373ea",
            "title": "A Comparative Study on Transformer vs RNN in Speech Applications"
        },
        {
            "paperId": "95ea73fe5d635c8471c2d4370c2378921d9484f1",
            "title": "Universal Acoustic Modeling Using Neural Mixture Models"
        },
        {
            "paperId": "bf442ab269074665a68e4dbbe19e4efc97862541",
            "title": "Large Memory Layers with Product Keys"
        },
        {
            "paperId": "8dea407aab8dd9bbd664ad1f471f7c9b5fd17980",
            "title": "Scalable Multi Corpora Neural Language Models for ASR"
        },
        {
            "paperId": "33c9daa8a6a6a3c16f9670cc0cf357a2832b948b",
            "title": "LSTM Language Models for LVCSR in First-Pass Decoding and Lattice-Rescoring"
        },
        {
            "paperId": "8a568abeba0c2668cbbd58ccc094b8257c9ad4d1",
            "title": "Cumulative Adaptation for BLSTM Acoustic Models"
        },
        {
            "paperId": "da7bc6335b9a1144fee892e52fecc52f2333e45c",
            "title": "Show Some Love to Your n-grams: A Bit of Progress and Stronger n-gram Language Modeling Baselines"
        },
        {
            "paperId": "3928b2177086532775fbf607ae3e05a0375a5061",
            "title": "Language Modeling with Deep Transformers"
        },
        {
            "paperId": "744196b6cb5091c0760d05ef068a92a6cd531587",
            "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention - w/o Data Augmentation"
        },
        {
            "paperId": "8942d4b7a5d73f811687845383c2945b4f234a3d",
            "title": "Teach an All-rounder with Experts in Different Domains"
        },
        {
            "paperId": "b0fae9fbb4e580d92395eabafe73e317ae6510e3",
            "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition"
        },
        {
            "paperId": "9be8c1b661e4ffa7e3d061c70c7a771313b39a8f",
            "title": "Model Unit Exploration for Sequence-to-Sequence Speech Recognition"
        },
        {
            "paperId": "612a42e2fa4c33b609aade451528d3c11989f88a",
            "title": "Self-attention Networks for Connectionist Temporal Classification in Speech Recognition"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
            "title": "Adaptive Input Representations for Neural Language Modeling"
        },
        {
            "paperId": "d84743b91541907e830f902c59fcf2fd65b94069",
            "title": "Role Play Dialogue Aware Language Models Based on Conditional Hierarchical Recurrent Encoder-Decoder"
        },
        {
            "paperId": "ec3738e45785b225d2e691299556ed7c838db4ba",
            "title": "Investigation on LSTM Recurrent N-gram Language Models for Speech Recognition"
        },
        {
            "paperId": "5ceb3cf83107c22180d862362bde260d032a1cfe",
            "title": "Investigation on Estimation of Sentence Probability by Combining Forward, Backward and Bi-directional LSTM-RNNs"
        },
        {
            "paperId": "ac61568c081c03730c58bd34c023a6952803da13",
            "title": "Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency"
        },
        {
            "paperId": "c3e937bba9cdb4ac6a93f92154843c041c806ba9",
            "title": "Simple Fusion: Return of the Language Model"
        },
        {
            "paperId": "7407b6fd730b498450257b4113ca698b17e5d0b5",
            "title": "Session-level Language Modeling for Conversational Speech"
        },
        {
            "paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d",
            "title": "Character-Level Language Modeling with Deeper Self-Attention"
        },
        {
            "paperId": "da4b7f47d0d9c4d46740cdd6fb712c6a87c10db3",
            "title": "A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition"
        },
        {
            "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
            "title": "Universal Transformers"
        },
        {
            "paperId": "dd14c968a5e5212bc6951a7998a433806bdb2788",
            "title": "Self-Normalization Properties of Language Modeling"
        },
        {
            "paperId": "f0ead45e8c1e4cc390ff6603bc0738b8c57f99ec",
            "title": "Improved training of end-to-end attention models for speech recognition"
        },
        {
            "paperId": "1296631b36a3b390466978f4fd80b683bf13573a",
            "title": "RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition"
        },
        {
            "paperId": "bb669de2fce407df2f5cb2f8c51dedee3f467e04",
            "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"
        },
        {
            "paperId": "37b98efcb9d18c55ec948697cc13569c1e7a2c3c",
            "title": "Prediction of LSTM-RNN Full Context States as a Subtask for N-Gram Feedforward Language Models"
        },
        {
            "paperId": "3504bc0739501220d07e8e3ecb4ad26a06fc50ad",
            "title": "RADMM: Recurrent Adaptive Mixture Model with Applications to Domain Robust Language Modeling"
        },
        {
            "paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
            "title": "Self-Attention with Relative Position Representations"
        },
        {
            "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "title": "Deep Contextualized Word Representations"
        },
        {
            "paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9",
            "title": "Generating Wikipedia by Summarizing Long Sequences"
        },
        {
            "paperId": "c6b61535f1544835cca3851ceb34222ebc5b4377",
            "title": "State-of-the-Art Speech Recognition with Sequence-to-Sequence Models"
        },
        {
            "paperId": "5a7f3f0fdbdc29fddc7a41098ee8bbc3f7cfd1a1",
            "title": "Toward Human Parity in Conversational Speech Recognition"
        },
        {
            "paperId": "eac48f406c46527f5ca821de7fe8d62d6db56a27",
            "title": "Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer"
        },
        {
            "paperId": "9ccab88476394179e9e7dcc5d0abe081b9dfac6d",
            "title": "Lattice rescoring strategies for long short term memory language models in speech recognition"
        },
        {
            "paperId": "ef9ddbc35676ce8ffc2a8067044473727839dbac",
            "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"
        },
        {
            "paperId": "892a2aaca549689ad315c8874f5f0e0e252ab516",
            "title": "The 2016 RWTH Keyword Search System for Low-Resource Languages"
        },
        {
            "paperId": "33125ec92a0b4b1687ccd153762d6275668e3d09",
            "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models"
        },
        {
            "paperId": "3dac8b53c0a43df06129986eed236d179f1d16a8",
            "title": "Fast Neural Network Language Model Lookups at N-Gram Speeds"
        },
        {
            "paperId": "6cc68e8adf34b580f3f37d1bd267ee701974edde",
            "title": "A Comparison of Sequence-to-Sequence Models for Speech Recognition"
        },
        {
            "paperId": "8fb0151c211b6394fe6fda64abff62bbb0061fef",
            "title": "Approaches for Neural-Network Language Model Adaptation"
        },
        {
            "paperId": "7fe37b79f80e8937ecba653b57ebc989a56b29f9",
            "title": "Investigating Bidirectional Recurrent Neural Network Language Models for Speech Recognition"
        },
        {
            "paperId": "44ca9b3d070ea55056093f6fb928927a23405e29",
            "title": "Parallel Neural Network Features for Improved Tandem Acoustic Modeling"
        },
        {
            "paperId": "58c6f890a1ae372958b7decf56132fe258152722",
            "title": "Regularizing and Optimizing LSTM Language Models"
        },
        {
            "paperId": "2397ce306e5d7f3d0492276e357fb1833536b5d8",
            "title": "On the State of the Art of Evaluation in Neural Language Models"
        },
        {
            "paperId": "1ca4670e9967fcb91ef1826f52d4d621fa7a950d",
            "title": "Listening while speaking: Speech chain by deep learning"
        },
        {
            "paperId": "785bd8ff24188829f8522e2be58574a1df1a7841",
            "title": "Domain Attention with an Ensemble of Experts"
        },
        {
            "paperId": "30109a213aa10765486c676ecfa511db227ab543",
            "title": "An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation"
        },
        {
            "paperId": "f359ce24c65b9bfe93a3eb4afab10e4cce018633",
            "title": "Nonrecurrent Neural Structure for Long-Term Dependence"
        },
        {
            "paperId": "dda047fd87610911c82778243f72f60d1c063383",
            "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech"
        },
        {
            "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
            "title": "A Structured Self-attentive Sentence Embedding"
        },
        {
            "paperId": "58def848174596119326b4ee981111d5ed538e9d",
            "title": "Knowledge distillation across ensembles of multilingual models for low-resource languages"
        },
        {
            "paperId": "6494cd26511c076186673c9a636d21d1dfed8d5a",
            "title": "Student-teacher network learning with enhanced features"
        },
        {
            "paperId": "f99f034c909a0db62f8aa3056a4b92c9b7b46f34",
            "title": "Investigations on byte-level convolutional neural networks for language modeling in low resource speech recognition"
        },
        {
            "paperId": "31eb323786d7b7379b53b8d747eece17b12b1aac",
            "title": "A Neural Network approach for mixing language models"
        },
        {
            "paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586",
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "b53ffe63b210a2a94fb870f519e27ff367fa0be8",
            "title": "Fast Gated Neural Domain Adaptation: Language Model as a Case Study"
        },
        {
            "paperId": "b0a2719ffd8c0cf8fe8b4aeca118c7a71cf94fef",
            "title": "Ensemble Learning for Multi-Source Neural Machine Translation"
        },
        {
            "paperId": "b1cb867270f87f96397cb5f0d76cbb58cdf2c2f2",
            "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition"
        },
        {
            "paperId": "d43b4801ea469a71b346698bf41197ef97e97d53",
            "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser"
        },
        {
            "paperId": "f4819c08515a03f086ada34f5babbe3395b3ffe9",
            "title": "Character-level language modeling with hierarchical recurrent neural networks"
        },
        {
            "paperId": "49899bd9e5a7f59aa14e6d21ed501e3c3acd5852",
            "title": "LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition"
        },
        {
            "paperId": "eb62dabac5f62f267a42b9f2615e057dd21eb9d3",
            "title": "Compact Feedforward Sequential Memory Networks for Large Vocabulary Continuous Speech Recognition"
        },
        {
            "paperId": "9ae3f23cbe1f32abc644d0edf0522b18ee6ec39f",
            "title": "Unsupervised Adaptation of Recurrent Neural Network Language Models"
        },
        {
            "paperId": "e05711b6ef102753e6d0010081d9a86be12a4e59",
            "title": "Automatic Speech Recognition Based on Neural Networks"
        },
        {
            "paperId": "9991d9ef171cda52b1fdc9fcac93cba80703c067",
            "title": "Knowledge distillation for small-footprint highway networks"
        },
        {
            "paperId": "d0b9ad3c2b4f46eb0bce9414141351ca39e78043",
            "title": "Two Efficient Lattice Rescoring Methods Using Recurrent Neural Network Language Models"
        },
        {
            "paperId": "055681baf0722ef6ed3e27ade1e5d7c99efefa6c",
            "title": "A comprehensive study of deep bidirectional LSTM RNNS for acoustic modeling in speech recognition"
        },
        {
            "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "title": "A Decomposable Attention Model for Natural Language Inference"
        },
        {
            "paperId": "4954fa180728932959997a4768411ff9136aac81",
            "title": "TensorFlow: A system for large-scale machine learning"
        },
        {
            "paperId": "2159e559172e783f199937f9c10f9a2800aeb82c",
            "title": "Investigation on log-linear interpolation of multi-domain neural network language model"
        },
        {
            "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "title": "Identity Mappings in Deep Residual Networks"
        },
        {
            "paperId": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "title": "Exploring the Limits of Language Modeling"
        },
        {
            "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
            "title": "Long Short-Term Memory-Networks for Machine Reading"
        },
        {
            "paperId": "4a197ce36461849bcaee565b510a8ef71b7dcae3",
            "title": "Recurrent Memory Network for Language Modeling"
        },
        {
            "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
            "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a",
            "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"
        },
        {
            "paperId": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb",
            "title": "Improving Neural Machine Translation Models with Monolingual Data"
        },
        {
            "paperId": "972f861e1063a3286acbd22a1ae40cdb7f537923",
            "title": "Blending LSTMs into CNNs"
        },
        {
            "paperId": "961073143d3cfe662e9e820d24c0a88f0ae94c83",
            "title": "Document Context Language Models"
        },
        {
            "paperId": "722e01d5ba05083f7a091f3188cfdfcf183a325d",
            "title": "Larger-Context Language Modelling with Recurrent Neural Network"
        },
        {
            "paperId": "f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4",
            "title": "Hierarchical Recurrent Neural Network for Document Modeling"
        },
        {
            "paperId": "e237e6124ff6f3827b832133b3a56624d7d74618",
            "title": "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition"
        },
        {
            "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
            "title": "Neural Machine Translation of Rare Words with Subword Units"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "878ba5458e9e51f0b341fd9117fa0b43ef4096d3",
            "title": "End-to-end attention-based large vocabulary speech recognition"
        },
        {
            "paperId": "3056add22b20e3361c38c0472d294a79d4031cb4",
            "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"
        },
        {
            "paperId": "b92aa7024b87f50737b372e5df31ef091ab54e62",
            "title": "Training Very Deep Networks"
        },
        {
            "paperId": "5b8364c21155d3d2cd38ea4c8b8580beba9a3250",
            "title": "An Empirical Exploration of Recurrent Network Architectures"
        },
        {
            "paperId": "758e950fd9d535c56d134b39ad6935e90a676097",
            "title": "The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models"
        },
        {
            "paperId": "b624504240fa52ab76167acfe3156150ca01cf3b",
            "title": "Attention-Based Models for Speech Recognition"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "8ba93e77b83bca5f90b7d697041ded442986890b",
            "title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition"
        },
        {
            "paperId": "34038d9424ce602d7ac917a4e582d977725d4393",
            "title": "Librispeech: An ASR corpus based on public domain audio books"
        },
        {
            "paperId": "edb1e4bd20731b292e36df7f80dc5c1ad61febb6",
            "title": "Transferring knowledge from a RNN to a DNN"
        },
        {
            "paperId": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081",
            "title": "LSTM: A Search Space Odyssey"
        },
        {
            "paperId": "5fcd41ca42659ff792fc8ee7d535156e8e69f987",
            "title": "On Using Monolingual Corpora in Neural Machine Translation"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "8cb72cf5490c2a532d52237f688f915a92afe04c",
            "title": "From Feedforward to Recurrent LSTM Neural Networks for Language Modeling"
        },
        {
            "paperId": "9665247ea3421929f9b6ad721f139f11edb1dbb8",
            "title": "Learning Longer Memory in Recurrent Neural Networks"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "fd76044388b6cb3ae8228075da020a6d60790e8a",
            "title": "Incremental Adaptation Strategies for Neural Network Language Models"
        },
        {
            "paperId": "7a3abe4c140aad447d1b90dfa1130f6fc299185d",
            "title": "Towards Unsupervised Learning for Handwriting Recognition"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "1938624bb9b0f999536dcc8d8f519810bb4e1b3b",
            "title": "On Using Very Large Target Vocabulary for Neural Machine Translation"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "0894b06cff1cd0903574acaa7fcf071b144ae775",
            "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation"
        },
        {
            "paperId": "31d3686b33a89488f8b0235a3fd7c8112a00f990",
            "title": "Real-time one-pass decoding with recurrent neural network language model for speech recognition"
        },
        {
            "paperId": "f89f4a93b8b8e034830e62fbb6eb86da66832576",
            "title": "Efficient lattice rescoring using recurrent neural network language models"
        },
        {
            "paperId": "e11274bb4549fbdb9a7ab64d09076976e6f71e75",
            "title": "Cache based recurrent neural network language model inference for first pass speech recognition"
        },
        {
            "paperId": "2df0053debb85d1e6d5b3737b46e157547e7b3ff",
            "title": "Enhancing the TED-LIUM Corpus with Selected Data for Language Modeling and More TED Talks"
        },
        {
            "paperId": "ffdebc5f8a86d4040db41a94c1df65334b73bace",
            "title": "Language Modeling with Power Low Rank Ensembles"
        },
        {
            "paperId": "d770060812fb646b3846a7d398a3066145b5e3c8",
            "title": "Do Deep Nets Really Need to be Deep?"
        },
        {
            "paperId": "1bc084e123794ca0c89798dfccd3a6b02b0af3e0",
            "title": "Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "167ad306d84cca2455bc50eb833454de9f2dcd02",
            "title": "Joint Language and Translation Modeling with Recurrent Neural Networks"
        },
        {
            "paperId": "5af6a1e1733bea321d574b9fd247b2f4dcdfa9f1",
            "title": "K-Component Adaptive Recurrent Neural Network Language Models"
        },
        {
            "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
            "title": "Generating Sequences With Recurrent Neural Networks"
        },
        {
            "paperId": "5cea23330c76994cb626df20bed31cc2588033df",
            "title": "Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets"
        },
        {
            "paperId": "fcfb39e64678fe9cb681f11b9a3314becec82bb2",
            "title": "Comparison of feedforward and recurrent neural network language models"
        },
        {
            "paperId": "f4a3c46720d54be5fc824db9aaf9d09ddadc23e5",
            "title": "The Deep Tensor Neural Network With Applications to Large Vocabulary Speech Recognition"
        },
        {
            "paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4",
            "title": "Context dependent recurrent neural network language model"
        },
        {
            "paperId": "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f",
            "title": "Sequence Transduction with Recurrent Neural Networks"
        },
        {
            "paperId": "23f0615819cded0067d2271eaee50efcadfe18a7",
            "title": "Long-short term memory neural networks language modeling for handwriting recognition"
        },
        {
            "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
            "title": "Improving neural networks by preventing co-adaptation of feature detectors"
        },
        {
            "paperId": "473f0739666af2791ad6592822118240ed968b70",
            "title": "Conversational Speech Transcription Using Context-Dependent Deep Neural Networks"
        },
        {
            "paperId": "5b0d644f5c4b9880cbaf79932c0a4fa98996f068",
            "title": "A fast and simple algorithm for training neural probabilistic language models"
        },
        {
            "paperId": "522e90b9fccfd3c1c0603359eb04757d770c1ab5",
            "title": "Practical Recommendations for Gradient-Based Training of Deep Architectures"
        },
        {
            "paperId": "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff",
            "title": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation"
        },
        {
            "paperId": "ed6262b569c0a62c51d941228c54f34e563af022",
            "title": "Japanese and Korean voice search"
        },
        {
            "paperId": "6658bbf68995731b2083195054ff45b4eca38b3a",
            "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"
        },
        {
            "paperId": "07ca885cb5cc4328895bfaec9ab752d5801b14cd",
            "title": "Extensions of recurrent neural network language model"
        },
        {
            "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
        },
        {
            "paperId": "ecd4bc32bb2717c96f76dd100fcd1255a07bd656",
            "title": "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
        },
        {
            "paperId": "0fcc184b3b90405ec3ceafd6a4007c749df7c363",
            "title": "Continuous space language models"
        },
        {
            "paperId": "8b395470a57c48d174c4216ea21a7a58bc046917",
            "title": "Training Neural Network Language Models on Very Large Corpora"
        },
        {
            "paperId": "24062f6adfd655e34b6f137b8008e69cee687201",
            "title": "The AMI meeting corpus"
        },
        {
            "paperId": "567dc4e26ece98e96c2e798ae8acafa5883945a9",
            "title": "Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm"
        },
        {
            "paperId": "047655e733a9eed9a500afd916efa566915b9110",
            "title": "Learning Precise Timing with LSTM Recurrent Networks"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "399da68d3b97218b6c80262df7963baa89dcc71b",
            "title": "SRILM - an extensible language modeling toolkit"
        },
        {
            "paperId": "d73d5fdb90fd3abcab3954d2bfd14b8d67473bed",
            "title": "Testing the correlation of word error rate and perplexity"
        },
        {
            "paperId": "0509bf552a0d1fe895c019e4e8f1b1599c7112e4",
            "title": "Minimum Phone Error and I-smoothing for improved discriminative training"
        },
        {
            "paperId": "e41498c05d4c68e4750fb84a380317a112d97b01",
            "title": "Connectionist language modeling for large vocabulary continuous speech recognition"
        },
        {
            "paperId": "4af41f4d838daa7ca6995aeb4918b61989d1ed80",
            "title": "Classes for fast maximum entropy training"
        },
        {
            "paperId": "7afe2bdfbdc816742be2778315828ae63f1923ae",
            "title": "Explicit word error minimization using word hypothesis posterior probabilities"
        },
        {
            "paperId": "28bb28488c30e26b047916cbc3276bee213c7ab9",
            "title": "Spoken Language Processing: A Guide to Theory, Algorithm and System Development"
        },
        {
            "paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "title": "Learning to Forget: Continual Prediction with LSTM"
        },
        {
            "paperId": "5e9082caea65c76bfd23b8763872804473ee7872",
            "title": "Tandem connectionist feature extraction for conventional HMM systems"
        },
        {
            "paperId": "5c89405ad33313765b10396e91b5ce5466c1a24d",
            "title": "Efficient lattice representation and generation"
        },
        {
            "paperId": "05cdd83816bd4789325af3eb92750847b16cedb7",
            "title": "The applicability of adaptive language modelling for the broadcast news task"
        },
        {
            "paperId": "3f32d76f4cac1e39d8313bf11ec31d5d43fd40cd",
            "title": "Learning to perceive the world as articulated: an approach for hierarchical learning in sensory-motor systems"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "e4efb4b4c07d02b4031111b1cb97a7a13b5c928a",
            "title": "Language model adaptation using mixtures and an exponentially decaying cache"
        },
        {
            "paperId": "b158a006bebb619e2ea7bf0a22c27d45c5d19004",
            "title": "LSTM can Solve Hard Long Time Lag Problems"
        },
        {
            "paperId": "404fffebcdb9b597489f62735d8ce59eff41f623",
            "title": "Modeling long distance dependence in language: topic mixtures vs. dynamic cache models"
        },
        {
            "paperId": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb",
            "title": "A maximum entropy approach to adaptive statistical language modelling"
        },
        {
            "paperId": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "title": "An Empirical Study of Smoothing Techniques for Language Modeling"
        },
        {
            "paperId": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "title": "Improved backing-off for M-gram language modeling"
        },
        {
            "paperId": "be1c1f5ee7ad3a49206b174201e495488494a85e",
            "title": "State of the art in continuous speech recognition."
        },
        {
            "paperId": "dd8bdcd6969670e8e43ff74cf458cfd5804c9e2f",
            "title": "A word graph algorithm for large vocabulary continuous speech recognition"
        },
        {
            "paperId": "34f8c5769899dfd9450bb13c3f52c18c88444515",
            "title": "Experimental Comparison of the Effect of Order in Recurrent Neural Networks"
        },
        {
            "paperId": "267fe4b32093a1294d565b31a0c69bfcd02f30f0",
            "title": "Word graphs: an efficient interface between continuous-speech recognition and language understanding"
        },
        {
            "paperId": "606df60d518db088986e74fad1f357ea6e5312f2",
            "title": "On the dynamic adaptation of stochastic language models"
        },
        {
            "paperId": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "title": "Class-Based n-gram Models of Natural Language"
        },
        {
            "paperId": "af06d65387266a3fe35973cbf77593229049bb10",
            "title": "The Meta-Pi Network: Building Distributed Knowledge Representations for Robust Multisource Pattern Recognition"
        },
        {
            "paperId": "3c92ffacba4eaddcb86c8fde50d1b183ac995052",
            "title": "Global optimization of a neural network-hidden Markov model hybrid"
        },
        {
            "paperId": "758eae04fc9f4331b0ceab797387c8fc9f00db58",
            "title": "A recurrent error propagation network speech recognition system"
        },
        {
            "paperId": "fa9b9743a55aa43b59d82f756a2bd370cf59e3d9",
            "title": "On smoothing techniques for bigram-based natural language modelling"
        },
        {
            "paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56",
            "title": "Adaptive Mixtures of Local Experts"
        },
        {
            "paperId": "0687165a9f0360bde0469fd401d966540e0897c3",
            "title": "A Dynamic Language Model for Speech Recognition"
        },
        {
            "paperId": "9d822e05eeb7cb7f023495f3b67247952c6820c5",
            "title": "Neural Network Approach to Word Category Prediction for English Texts"
        },
        {
            "paperId": "be1fed9544830df1137e72b1d2396c40d3e18365",
            "title": "A Cache-Based Natural Language Model for Speech Recognition"
        },
        {
            "paperId": "8cf661487d8708a3e9a74e9cc83ce290aa5355b8",
            "title": "Stochastic modeling for automatic speech understanding"
        },
        {
            "paperId": "f866ac085771f5676800db2d9b102975b2a1b2d7",
            "title": "Connectionist Viterbi training: a new hybrid method for continuous speech recognition"
        },
        {
            "paperId": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "title": "Phoneme recognition using time-delay neural networks"
        },
        {
            "paperId": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "title": "A tutorial on hidden Markov models and selected applications in speech recognition"
        },
        {
            "paperId": "52f164f715febf097b8756a7308b416539b7c7da",
            "title": "A study of English word category prediction based on neural networks"
        },
        {
            "paperId": "0908d52124ec2a31bfe9d966d1374a80919155ad",
            "title": "An introduction to computing with neural nets"
        },
        {
            "paperId": "1e4193d03eb3c5a695a3d8b3506f80704f9dfc19",
            "title": "The use of a one-stage dynamic programming algorithm for connected word recognition"
        },
        {
            "paperId": "8f93ee2e335d83ee32787693861f4d48e78e6786",
            "title": "Recognition of consonant based on the perceptron model"
        },
        {
            "paperId": "8d350f2d767a70d55275a17d0b3dfcc80b2e0fee",
            "title": "Perplexity\u2014a measure of the difficulty of speech recognition tasks"
        },
        {
            "paperId": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9",
            "title": "Continuous speech recognition by statistical methods"
        },
        {
            "paperId": "22b6737a38179c01444d69443e327850c9956c15",
            "title": "Design of a linguistic statistical decoder for the recognition of continuous speech"
        },
        {
            "paperId": null,
            "title": "Lample: Cross-lingual Language Model Pretraining"
        },
        {
            "paperId": null,
            "title": "Residual Learning Without Normalization via Better Initialization"
        },
        {
            "paperId": "11d36cf5fc2fc35c52bd5a735cdd29c685bae95e",
            "title": "MetaInit: Initializing learning by learning to initialize"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "Khudanpur: Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks"
        },
        {
            "paperId": "b12ccd118974839db290f15c989649b2b5188636",
            "title": "Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": null,
            "title": "Polosukhin: Attention is All you Need"
        },
        {
            "paperId": "6a9006331491c76390501b39826f6b82483a76f2",
            "title": "Progress in Decoding for Large Vocabulary Continuous Speech Recognition"
        },
        {
            "paperId": "7c9f80b5d265463bcb17cf19d941b05f4c0f1986",
            "title": "Comparison of Parametric Representation for Monosyllabic Word Recognition in Continuously Spoken Sentences"
        },
        {
            "paperId": "269515a94b1e7d1bce849fa2fd8fd36edf1370e3",
            "title": "Improvements in language and translation modeling"
        },
        {
            "paperId": null,
            "title": "Rush: Sequence-Level Knowledge Distillation"
        },
        {
            "paperId": "8c177a7e3e95d7cbf087d61c2dc51d683ac986c2",
            "title": "The RWTH/UPB/FORTH System Combination for the 4th CHiME Challenge Evaluation"
        },
        {
            "paperId": null,
            "title": "A Practical Guide to Real-Time Machine Translation"
        },
        {
            "paperId": "0e5f32db95f15a2862a3f395c92e0a3fbb347953",
            "title": "Bag-of-words input for long history representation in neural network-based language models for speech recognition"
        },
        {
            "paperId": "382d4ce71d39f68e77be0029cef0026810b2b0f3",
            "title": "On efficient training of word classes and their application to recurrent neural network language models"
        },
        {
            "paperId": "51f7ff895b854e421fbabf76a1039f1abb41d5d4",
            "title": "Lattice decoding and rescoring with long-Span neural network language models"
        },
        {
            "paperId": "0cea6b034ee949d10604ba163270b699e711ded8",
            "title": "Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "067e07b725ab012c80aa2f87857f6791c1407f6d",
            "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling"
        },
        {
            "paperId": "8d25d04051074be7590cbe5e4e34c45bb26674e1",
            "title": "Learning small-size DNN with output-distribution-based criteria"
        },
        {
            "paperId": "439df5a9b2b6e02abab00b445afa69365c5c3227",
            "title": "Multilingual hierarchical MRASTA features for ASR"
        },
        {
            "paperId": "96364af2d208ea75ca3aeb71892d2f7ce7326b55",
            "title": "Statistical Language Models Based on Neural Networks"
        },
        {
            "paperId": "f9a1b3850dfd837793743565a8af95973d395a4e",
            "title": "LSTM Neural Networks for Language Modeling"
        },
        {
            "paperId": "e4a404524e8bcaf2fafb977d52b93eaf4667fddc",
            "title": "Bayesian Language Model Interpolation for Mobile Speech Input"
        },
        {
            "paperId": "df19ec3e6de233053cb14d342debacbf91d919ab",
            "title": "On the Estimation of Discount Parameters for Language Model Smoothing"
        },
        {
            "paperId": "d6bebc84bbfc9c1d0103ce36bdccf92c59022a52",
            "title": "Bayes risk decoding and its application to system combination"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "44de484da59b640aaf3e8b9615fb946812e38b1a",
            "title": "The RWTH 2009 quaero ASR evaluation system for English and German"
        },
        {
            "paperId": "60650f0a6530cebd78d894c15f7c06738dc65418",
            "title": "Study on interaction between entropy pruning and kneser-ney smoothing"
        },
        {
            "paperId": null,
            "title": "Niculescu-Mizil: Model compression"
        },
        {
            "paperId": "6d12a1d23b21a9b170118a56386552bc5d4727de",
            "title": "A Mathematical Theory of Communication"
        },
        {
            "paperId": "ed22171ce376d213bf64d998d594f876f6912cb7",
            "title": "Neural network language models for conversational speech recognition"
        },
        {
            "paperId": "bc9b8023d0a7f881c0dd29f7e206f3fc6c28669e",
            "title": "An essay towards solving a problem in the doctrine of chances"
        },
        {
            "paperId": "469c4f553ddf7b5195530d4cf10adbbcc9b18df8",
            "title": "Evaluation Metrics For Language Models"
        },
        {
            "paperId": "c0dc20ada590a912134ef7147ae12ea599b5e82d",
            "title": "Word Triggers and the EM Algorithm"
        },
        {
            "paperId": null,
            "title": "Connectionist speech recognition: a hybrid"
        },
        {
            "paperId": "30f8620032dfae4fe5fd36f2cbfe31c57efaff53",
            "title": "Connectionist probability estimators in HMM speech recognition"
        },
        {
            "paperId": "b3e53b9c0e3a7a60e7a5295e9b08af74d6fb3dbf",
            "title": "On structuring probabilistic dependences in stochastic language modelling"
        },
        {
            "paperId": "01a9a9686d45a3dc8182f59a1a77f1ac4f233761",
            "title": "Forming Word Classes by Statistical Clustering for Statistical Language Modelling"
        },
        {
            "paperId": "2392e94df520e707e8b1422311bfdc552954dea9",
            "title": "Fundamentals of speech recognition"
        },
        {
            "paperId": null,
            "title": "The General Use of Tying in Phoneme Based HMM Recognizers"
        },
        {
            "paperId": null,
            "title": "Elman: Finding structure in time"
        },
        {
            "paperId": "5001470e8808afe9887afbe48e2eaaf1a0395d10",
            "title": "A Continuous Speech Recognition System Embedding MLP into HMM"
        },
        {
            "paperId": "b2f8876482c97e804bb50a5e2433881ae31d0cdd",
            "title": "Binary codes capable of correcting deletions, insertions, and reversals"
        },
        {
            "paperId": "c1e3f2d537e50e0d5263e4731ab6c7983acd6687",
            "title": "Prediction and Entropy of Printed English"
        },
        {
            "paperId": null,
            "title": "No trigger is obtained, the model chooses the most recent context from the GRU. Quaero development perplexity of 109.1, which is similar to 110.6 of the model without the attention layer"
        },
        {
            "paperId": null,
            "title": "Standalone perplexities of the 48 K vocabulary word-level baseline models on AMI. Perplexities after fine-tuning on the"
        },
        {
            "paperId": null,
            "title": "Interpolation weights (scaled by factor 100) for each domain on the AppTek development text for 4-gram models. We removed values smaller than 10 \u22122 . We show 8 most relevant subsets out of 33"
        },
        {
            "paperId": null,
            "title": "Effect of training consistent with evaluation segmentation (split after punctuation). The development and evaluation sets are not segmented"
        },
        {
            "paperId": null,
            "title": "Effect of the depth. Perplexities on Quaero development set"
        },
        {
            "paperId": null,
            "title": "Perplexity on LibriSpeech after 2.5 epoch for (L, d ff = 8192, d res = 1024, H = 16) models with shared parameters across all layers"
        },
        {
            "paperId": null,
            "title": "Highway connection in Transformer models (L = 24, d ff = 2048, d res = 512, H = 8)"
        },
        {
            "paperId": null,
            "title": "Effect of the teacher weight \u03bb in Eq. (5.4) on the Switchboard cross validation set"
        },
        {
            "paperId": null,
            "title": "The dashed arrows indicate the second pass lattice rescoring with a second language model (1.2.2), as is done with neural language models in this thesis"
        },
        {
            "paperId": null,
            "title": "Listen Attend and Spell. Figure taken from [Irie & Prabhavalkar + 19b"
        },
        {
            "paperId": null,
            "title": "69 5.3 Perplexity results on Switchboard of knowledge distillation based on class based output, using contexts across sentence boundaries (up to 100 words)"
        },
        {
            "paperId": null,
            "title": "Perplexities on the YouTube data of models based on 8192-unit LSTMs"
        },
        {
            "paperId": null,
            "title": "The number of heads H is 8 for all models below"
        },
        {
            "paperId": null,
            "title": "For Switchboard, numbers for Switchboard and CallHome parts of Hub5 00 set are presented as Dev and Eval for this table"
        },
        {
            "paperId": null,
            "title": "Concatenated sentences (d.) Concatenated CCO. One this same figure, we can also visualize the two evaluation modes: (a.) sentence-wise evaluation and (b.) full context evaluation"
        },
        {
            "paperId": null,
            "title": "Final perplexities on LibriSpeech after full convergence. The baseline 4-gram and LSTM numbers are taken from Table 3.3. d res is 512 for all Transformer models"
        },
        {
            "paperId": null,
            "title": "Perplexities computed on the second pass 133 K vocabulary"
        },
        {
            "paperId": null,
            "title": "All models including the 4-gram Kneser-Ney model are trained on 50 M words for comparison. A hidden layer size of 500 is used"
        },
        {
            "paperId": null,
            "title": "Sundermeyer's push-forward lattice rescoring algorithm"
        },
        {
            "paperId": null,
            "title": "CNN with class output based distillation. The best perplexities for the MLP are copied from Table 5.2 for easy comparison. All modes are 5-grams"
        },
        {
            "paperId": null,
            "title": "Comparison of different feed-forward layer types. Perplexities are reported with 2-layer models on Quaero development set"
        },
        {
            "paperId": null,
            "title": "The 4-gram model is used in the first pass to generate lattices for rescoring. The row \"Lattice\" shows oracle WERs of the lattices"
        },
        {
            "paperId": null,
            "title": "Perplexity after 5 epochs (13 M updates; full convergence) for (L, d ff = 2048, d res = 512, H = 8) models"
        },
        {
            "paperId": null,
            "title": "WERs (%) for attention-based models on LibriSpeech"
        },
        {
            "paperId": null,
            "title": "Some triggers can be observed, but the perplexity is bad: 157.6 which is close to the perplexity of 4-gram model"
        },
        {
            "paperId": null,
            "title": "Effect of sharing KV for both standard and small state Transformers. Perplexity on TED-LIUM 2 (152K vocab)"
        },
        {
            "paperId": null,
            "title": "All results are reported after interpolation with the baseline count model"
        },
        {
            "paperId": null,
            "title": "%) results on Quaero for neural language models with an additional bag-of-words input feature. Perplexities are those of models interpolated with the 4-gram Kneser-Ney model trained on 3.1 B"
        },
        {
            "paperId": null,
            "title": "Perplexities (PPL) after interpolation with the 4-gram Kneser-Ney language model. CCO denotes context carry-over"
        },
        {
            "paperId": null,
            "title": "When an additional linear bottleneck layer is inserted before softmax (Bottleneck), its dimension is set to 512"
        },
        {
            "paperId": null,
            "title": "Perplexities of Transformers on Switchboard (no positional encoding is used except for the model in the last row). CCO denotes context carry-over"
        },
        {
            "paperId": null,
            "title": "Background and RADMM are single models while Experts are one model per category"
        },
        {
            "paperId": "261a056f8b21918e8616a429b2df6e1d5d33be41",
            "title": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
        },
        {
            "paperId": null,
            "title": "Example 2: Category News & Politics. The x-axis corresponds to the input words. The y-axis shows the expert domains"
        },
        {
            "paperId": null,
            "title": "57 4.12 Perplexity of the word-level (152K vocab) models on TED-LIUM 2. d kv = 768 and H = 12 for all models. The models with F = 1 are standard Transformers"
        },
        {
            "paperId": null,
            "title": "Perplexity on the LibriSpeech Dev set after 1 subepoch for (L = 24, d ff = 2048, d ff = 512, H = 8) with highway connections"
        },
        {
            "paperId": null,
            "title": "Illustration for the modified Transformer layer"
        },
        {
            "paperId": null,
            "title": "Perplexities on Quaero development set. The number of hidden units are set to 300 in each layer"
        },
        {
            "paperId": null,
            "title": "Both axes are on the natural log scale. The regression has the equation: log(WER) = 0.62 + 0.39 * log(PPL)"
        },
        {
            "paperId": null,
            "title": "116 A.2 Number of running words, OOV rates and average sentence lengths in terms of number of words (Avg. length) of all data sets and subsets used. The vocabulary size is 30 K"
        },
        {
            "paperId": null,
            "title": "How Does BERT Answer Questions?"
        },
        {
            "paperId": null,
            "title": "CCO denotes context carry-over. We report average sequence length information for the development set which is similar to the evaluation data"
        },
        {
            "paperId": null,
            "title": "Perplexity on word level LibriSpeech after 2.5 epoch (25 sub-epochs in our setup"
        },
        {
            "paperId": null,
            "title": "Attention is rather intra-sentence and blur. The x-axis corresponds to the input token. The y-axis shows the target words, where for each target word position, 8 attention heads are shown vertically"
        },
        {
            "paperId": null,
            "title": "98 7.10 BLEU and TER results for WMT 2016 Romanian-English task. The baseline NMT performance was provided by Arne Nix"
        },
        {
            "paperId": null,
            "title": "Expert models are interpolated to form the teacher model. Interpolation weights are either optimized for each domain (Domain optimized) or only optimized once on the whole development set (All)"
        }
    ]
}