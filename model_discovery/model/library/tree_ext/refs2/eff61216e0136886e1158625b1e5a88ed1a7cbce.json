{
    "paperId": "eff61216e0136886e1158625b1e5a88ed1a7cbce",
    "externalIds": {
        "DBLP": "conf/nips/DenilSDRF13",
        "MAG": "2952899695",
        "ArXiv": "1306.0543",
        "DOI": "10.14288/1.0165555",
        "CorpusId": 1639981
    },
    "title": "Predicting Parameters in Deep Learning",
    "abstract": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy.",
    "venue": "Neural Information Processing Systems",
    "year": 2013,
    "referenceCount": 36,
    "citationCount": 1262,
    "influentialCitationCount": 63,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is demonstrated that there is significant redundancy in the parameterization of several deep learning models and not only can the parameter values be predicted, but many of them need not be learned at all."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1715051",
            "name": "Misha Denil"
        },
        {
            "authorId": "3355894",
            "name": "B. Shakibi"
        },
        {
            "authorId": "46573521",
            "name": "Laurent Dinh"
        },
        {
            "authorId": "1706809",
            "name": "Marc'Aurelio Ranzato"
        },
        {
            "authorId": "1737568",
            "name": "Nando de Freitas"
        }
    ],
    "references": [
        {
            "paperId": "5a434953b58c72fe2089531d6c4b4fc1325defcb",
            "title": "Learning Separable Filters"
        },
        {
            "paperId": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95",
            "title": "Deep learning with COTS HPC systems"
        },
        {
            "paperId": "72d32c986b47d6b880dad0c3f155fe23d2939038",
            "title": "Deep Learning of Representations: Looking Forward"
        },
        {
            "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "title": "Maxout Networks"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "80a9b191b559f3316348c39b24fc14ec4c38c107",
            "title": "Emergence of Object-Selective Features in Unsupervised Feature Learning"
        },
        {
            "paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "title": "Large Scale Distributed Deep Networks"
        },
        {
            "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
            "title": "Improving neural networks by preventing co-adaptation of feature detectors"
        },
        {
            "paperId": "5352b7ca90cbe4938f8e71a25d49517e7f94670a",
            "title": "Scalable stacking and learning for building deep architectures"
        },
        {
            "paperId": "398c296d0cc7f9d180f84969f8937e6d3a413796",
            "title": "Multi-column deep neural networks for image classification"
        },
        {
            "paperId": "6658bbf68995731b2083195054ff45b4eca38b3a",
            "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"
        },
        {
            "paperId": "72e93aa6767ee683de7f001fa72f1314e40a8f35",
            "title": "Building high-level features using large scale unsupervised learning"
        },
        {
            "paperId": "51e93552fe55be91a5711ff2aabc04b742503e68",
            "title": "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning"
        },
        {
            "paperId": "182015c5edff1956cbafbcb3e7bbe294aa54f9fc",
            "title": "Selecting Receptive Fields in Deep Networks"
        },
        {
            "paperId": "be9a17321537d9289875fe475b71f4821457b435",
            "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"
        },
        {
            "paperId": "2d851f681f82c71a934aebd16e8112adf1239f85",
            "title": "On Autoencoders and Score Matching for Energy Based Models"
        },
        {
            "paperId": "82b9099ddf092463f497bd48bb112c46ca52c4d1",
            "title": "High-Performance Neural Networks for Visual Object Classification"
        },
        {
            "paperId": "05cc38e249a6f642363b5a5cbd71cda67cea5893",
            "title": "Tiled convolutional neural networks"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "31f04f8f83365fabf7ba9c9be1179c0da6815128",
            "title": "Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields"
        },
        {
            "paperId": "0eb2e4a205a628ab059cab41d3b772f614ad29f2",
            "title": "Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines"
        },
        {
            "paperId": "e7c64258997838087c9ba4e87225627b015122b2",
            "title": "Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images"
        },
        {
            "paperId": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
            "title": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition"
        },
        {
            "paperId": "054bbbab7d8324d381903b2f33d4e8a17b54eff0",
            "title": "Double Sparsity: Learning Sparse Dictionaries for Sparse Signal Approximation"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "title": "A Fast Learning Algorithm for Deep Belief Nets"
        },
        {
            "paperId": "1267fe36b5ece49a9d8f913eb67716a040bbcced",
            "title": "On the limited memory BFGS method for large scale optimization"
        },
        {
            "paperId": "3034afcd45fc190ed71982828b77f6e4154bdc5c",
            "title": "Speaker-independent phone recognition using hidden Markov models"
        },
        {
            "paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "title": "Learning representations by back-propagating errors"
        },
        {
            "paperId": "69e68bfaadf2dccff800158749f5a50fe82d173b",
            "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"
        },
        {
            "paperId": "d2b62f77cb2864e465aa60bca6c26bb1d2f84963",
            "title": "Acoustic Modeling Using Deep Belief Networks"
        },
        {
            "paperId": "baedf3036e56edddf400f04e5a68b167014991ef",
            "title": "Kernel Methods for Pattern Analysis"
        },
        {
            "paperId": "22caccb8f64c258fcd051e9ab68179faa0219bb3",
            "title": "A Neural Support Vector Network architecture with adaptive kernels"
        },
        {
            "paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "title": "Gradient-based learning applied to document recognition"
        },
        {
            "paperId": "b5affc896bcb291bca6e3ba60d34eeac28214e2e",
            "title": "Dimensionality Reduction and Prior Knowledge in E-Set Recognition"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        }
    ]
}