{
    "paperId": "50eb97f832ffcd2114f79957c977215176384e3d",
    "externalIds": {
        "DBLP": "journals/corr/abs-2305-16380",
        "ArXiv": "2305.16380",
        "DOI": "10.48550/arXiv.2305.16380",
        "CorpusId": 258947127
    },
    "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer",
    "abstract": "Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText).",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 88,
    "citationCount": 48,
    "influentialCitationCount": 4,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.16380",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is proved that self-attention acts as adiscriminative scanning algorithm: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -1.2796387672424316,
            0.8105015754699707,
            -1.7678579092025757,
            4.054466247558594,
            0.61323082447052,
            0.5803498029708862,
            4.041045665740967,
            -2.7254512310028076,
            1.1578001976013184,
            -0.17119884490966797,
            -1.571448802947998,
            4.69715690612793,
            0.5657452344894409,
            3.1574034690856934,
            -3.4444546699523926,
            0.28062325716018677,
            -2.1034774780273438,
            1.2247596979141235,
            4.568727016448975,
            2.5265886783599854,
            -0.7225652933120728,
            3.4474949836730957,
            -0.6101524829864502,
            -0.044587425887584686,
            -3.499866008758545,
            0.17409616708755493,
            4.55874490737915,
            3.340698480606079,
            -4.774587154388428,
            2.2559516429901123,
            -1.1369514465332031,
            -5.682741641998291,
            4.385583877563477,
            -3.1243557929992676,
            1.6394004821777344,
            -2.900266408920288,
            -2.578758716583252,
            6.921876430511475,
            -1.979573130607605,
            0.6023251414299011,
            -1.3904086351394653,
            0.7769243717193604,
            1.0690401792526245,
            2.0153326988220215,
            -0.012095168232917786,
            0.6219561100006104,
            0.026087388396263123,
            2.180878162384033,
            -1.1435214281082153,
            2.7260801792144775,
            2.6523704528808594,
            0.4787585735321045,
            -2.560074806213379,
            2.8093297481536865,
            0.4223276376724243,
            -0.08123743534088135,
            2.8192901611328125,
            -1.332162618637085,
            1.9290627241134644,
            -1.9139094352722168,
            6.5448737144470215,
            6.251550674438477,
            0.7580302953720093,
            -1.6561020612716675,
            3.4205267429351807,
            -4.713030815124512,
            -2.2484641075134277,
            4.257717132568359,
            0.1623021364212036,
            3.3654229640960693,
            -0.5996408462524414,
            -5.339776039123535,
            0.4477499723434448,
            1.9067294597625732,
            -2.999601364135742,
            3.244466543197632,
            -3.1474153995513916,
            -7.651387691497803,
            -3.1891093254089355,
            -0.6763643026351929,
            -2.2670960426330566,
            2.7136871814727783,
            0.31253713369369507,
            2.055962562561035,
            0.22736838459968567,
            -0.14348822832107544,
            -6.2689008712768555,
            -0.7912358045578003,
            0.5591485500335693,
            -3.680732488632202,
            1.0914349555969238,
            -0.41893500089645386,
            1.1844674348831177,
            1.3274248838424683,
            -5.204864025115967,
            0.5482901930809021,
            1.1476343870162964,
            0.42995932698249817,
            -3.6661221981048584,
            1.13538658618927,
            3.380204677581787,
            -1.937485933303833,
            1.6678935289382935,
            0.5371257066726685,
            3.919477939605713,
            -3.033403158187866,
            1.9795013666152954,
            -0.3936260938644409,
            1.2714636325836182,
            -2.2534804344177246,
            -4.183908462524414,
            3.78713321685791,
            -1.36318039894104,
            -1.480529546737671,
            -2.788607120513916,
            -0.26166728138923645,
            -1.0965285301208496,
            -0.03589153289794922,
            0.734221339225769,
            5.131737232208252,
            -1.5250647068023682,
            1.4589065313339233,
            -2.727971076965332,
            0.5789353847503662,
            -0.4518653154373169,
            1.430497169494629,
            -2.117819309234619,
            0.6623615026473999,
            1.198030948638916,
            -3.6382951736450195,
            3.5031003952026367,
            -1.11243736743927,
            4.1903181076049805,
            -1.4110671281814575,
            5.264582633972168,
            4.849086761474609,
            -4.269442558288574,
            1.183858871459961,
            -2.222302198410034,
            -0.9673041105270386,
            -1.9754420518875122,
            3.0188324451446533,
            -0.805627167224884,
            -1.5079824924468994,
            2.582498550415039,
            4.1899027824401855,
            0.44408825039863586,
            4.179449558258057,
            2.9786198139190674,
            5.398649215698242,
            2.972073554992676,
            -6.634261131286621,
            -1.8583014011383057,
            1.9038375616073608,
            1.9305871725082397,
            1.3776313066482544,
            -5.785274028778076,
            0.15975984930992126,
            -4.455539703369141,
            0.0738656222820282,
            -0.12771224975585938,
            -1.1596646308898926,
            -10.913415908813477,
            -2.8224096298217773,
            -0.05023002624511719,
            -4.130627632141113,
            0.9010968208312988,
            1.6757733821868896,
            -0.4091399312019348,
            5.237705230712891,
            1.5666263103485107,
            1.6344307661056519,
            0.8711403608322144,
            5.5752763748168945,
            4.932255744934082,
            5.374501705169678,
            1.6693592071533203,
            -1.9022341966629028,
            1.5893774032592773,
            -2.4899165630340576,
            -4.251035213470459,
            -1.6703120470046997,
            -7.366903781890869,
            1.1760810613632202,
            -3.02679705619812,
            -1.7735285758972168,
            -1.6890579462051392,
            -0.8693079948425293,
            -1.520615816116333,
            -2.4399964809417725,
            1.3181345462799072,
            -0.39250341057777405,
            5.2458882331848145,
            8.350502014160156,
            3.950197696685791,
            2.7752647399902344,
            0.28747349977493286,
            4.0742597579956055,
            -2.0068602561950684,
            -1.6245636940002441,
            3.4276397228240967,
            -0.27471649646759033,
            -0.6242203116416931,
            -4.136661529541016,
            0.8597708344459534,
            4.393864631652832,
            -3.328075647354126,
            -0.23396092653274536,
            4.589173793792725,
            -2.7233943939208984,
            0.35144370794296265,
            -1.3392086029052734,
            -0.34172362089157104,
            4.960031032562256,
            -5.041309833526611,
            -2.4090166091918945,
            -7.070059776306152,
            1.9507420063018799,
            5.627912521362305,
            -0.08546298742294312,
            -1.9698925018310547,
            2.603947877883911,
            1.009263038635254,
            -2.6419734954833984,
            -0.1851218342781067,
            -2.6591362953186035,
            3.4300804138183594,
            -1.1727410554885864,
            -1.0136563777923584,
            3.179459571838379,
            -3.300734043121338,
            -3.8750576972961426,
            1.1083694696426392,
            -1.1609904766082764,
            -6.332859039306641,
            -2.1425228118896484,
            -2.9420323371887207,
            0.4153813421726227,
            0.6419298648834229,
            -2.72676157951355,
            7.040116786956787,
            0.07533159852027893,
            1.1294503211975098,
            4.6140947341918945,
            3.41194486618042,
            1.660202980041504,
            -4.955324172973633,
            -0.24687999486923218,
            0.5711832046508789,
            -1.2935808897018433,
            -0.23043373227119446,
            0.8301059007644653,
            2.789769411087036,
            -0.8760528564453125,
            1.5915857553482056,
            -0.4353075623512268,
            1.5532792806625366,
            1.151330590248108,
            -0.04770234227180481,
            0.89908766746521,
            -0.6601530313491821,
            1.7630022764205933,
            3.6641218662261963,
            5.633080959320068,
            -2.089040756225586,
            2.159714698791504,
            -1.772046685218811,
            -1.7297693490982056,
            -1.8076708316802979,
            3.5469605922698975,
            0.2975393533706665,
            1.7136822938919067,
            -1.6565505266189575,
            -5.282598495483398,
            -0.4846454858779907,
            -4.126771926879883,
            -4.209376811981201,
            -2.5862984657287598,
            3.591073513031006,
            2.9326813220977783,
            -0.04249793291091919,
            -4.362342834472656,
            -1.604272484779358,
            -2.4776105880737305,
            -0.8457715511322021,
            -4.748823165893555,
            -3.323011636734009,
            2.3752927780151367,
            -1.0859107971191406,
            -3.407485246658325,
            -4.1737775802612305,
            2.882858991622925,
            -3.9354019165039062,
            -0.8009230494499207,
            -1.9689899682998657,
            2.994365692138672,
            4.266170501708984,
            -2.6724441051483154,
            0.008681803941726685,
            -1.997368574142456,
            -3.163729190826416,
            -1.7001473903656006,
            3.088690757751465,
            -0.5474390387535095,
            2.676370143890381,
            5.922735691070557,
            0.41185855865478516,
            -5.101846218109131,
            -0.6665981411933899,
            -4.3357343673706055,
            -3.2100956439971924,
            -2.4491326808929443,
            6.358403205871582,
            -1.1704702377319336,
            1.8718770742416382,
            1.0993903875350952,
            -2.054375410079956,
            -1.5997493267059326,
            -1.471898078918457,
            2.53729248046875,
            0.17173826694488525,
            -0.1875011920928955,
            -7.009641170501709,
            -4.099592208862305,
            -3.2124760150909424,
            -1.8636938333511353,
            3.2896199226379395,
            3.6596508026123047,
            -3.8409457206726074,
            2.2521727085113525,
            1.4892725944519043,
            3.6863083839416504,
            0.6940563917160034,
            2.4225118160247803,
            -0.15713781118392944,
            -5.124167442321777,
            -0.08152119070291519,
            -0.3551526963710785,
            -2.265223264694214,
            3.912470817565918,
            -0.9110064506530762,
            7.588268280029297,
            0.03190815448760986,
            0.8634023666381836,
            2.7603394985198975,
            0.9434793591499329,
            2.5196568965911865,
            -1.0520281791687012,
            -0.8779842257499695,
            -5.1604814529418945,
            0.07915493845939636,
            1.4688389301300049,
            3.7845826148986816,
            -3.6639206409454346,
            1.3249952793121338,
            2.752375841140747,
            1.4184054136276245,
            2.581178665161133,
            -0.34115439653396606,
            3.1940081119537354,
            -0.864008903503418,
            0.07103359699249268,
            3.0473663806915283,
            -1.0310859680175781,
            -0.3947860598564148,
            -0.6456272602081299,
            9.106780052185059,
            -2.3182668685913086,
            -1.556671142578125,
            -5.87176513671875,
            -1.41880464553833,
            -3.7792673110961914,
            -3.764493942260742,
            5.743793487548828,
            -1.5223667621612549,
            -2.253103256225586,
            0.24599218368530273,
            -2.5260517597198486,
            0.6610168814659119,
            -0.6713566780090332,
            1.0305440425872803,
            6.380462169647217,
            1.3871381282806396,
            2.7313761711120605,
            -0.20604518055915833,
            -2.4299278259277344,
            -4.6934356689453125,
            1.8890166282653809,
            4.102557182312012,
            -1.198040246963501,
            -2.7073588371276855,
            3.034196138381958,
            2.4758808612823486,
            -0.728071928024292,
            -2.763535976409912,
            -2.477203130722046,
            -3.3985960483551025,
            -5.292363166809082,
            0.7754874229431152,
            -1.045853614807129,
            -0.5070785284042358,
            2.0345678329467773,
            2.4083480834960938,
            3.064410924911499,
            -2.116786479949951,
            -3.687987804412842,
            6.26816463470459,
            -2.2104930877685547,
            -1.3160783052444458,
            1.718959093093872,
            -1.324641227722168,
            -1.9661316871643066,
            -0.02458849549293518,
            -2.7321534156799316,
            -0.5897966623306274,
            -2.1599950790405273,
            0.04151660203933716,
            2.872753381729126,
            6.292243003845215,
            -0.14331011474132538,
            -1.9211788177490234,
            3.9800732135772705,
            3.570829153060913,
            1.8892061710357666,
            -0.2631954848766327,
            2.7622392177581787,
            3.6591551303863525,
            2.2370493412017822,
            -0.9410233497619629,
            1.7017877101898193,
            -2.652141809463501,
            5.454124927520752,
            -3.562209367752075,
            0.0759086012840271,
            0.23532569408416748,
            4.2857465744018555,
            2.312662363052368,
            2.317974090576172,
            -0.9998915791511536,
            -3.834219217300415,
            1.668846845626831,
            3.3166770935058594,
            -3.5489158630371094,
            4.633850574493408,
            1.4569231271743774,
            -0.26302897930145264,
            0.09464126825332642,
            -2.0069692134857178,
            -2.070746421813965,
            -6.252616882324219,
            0.10711738467216492,
            -4.609921932220459,
            -5.555881023406982,
            -1.3859593868255615,
            -0.20222890377044678,
            1.559424638748169,
            -1.4630415439605713,
            -0.4376690983772278,
            0.08644473552703857,
            0.6439144611358643,
            -5.5579423904418945,
            0.752600908279419,
            -0.7970373630523682,
            5.911844253540039,
            0.6639162302017212,
            3.0869362354278564,
            -0.28264346718788147,
            -1.44220769405365,
            -1.0235852003097534,
            3.7091310024261475,
            3.879706859588623,
            0.2876129746437073,
            -0.20391470193862915,
            -4.748301029205322,
            2.5640690326690674,
            -0.8822299242019653,
            1.0807924270629883,
            3.890383720397949,
            0.4812083840370178,
            -6.352252960205078,
            -5.075076103210449,
            -1.5222084522247314,
            3.5300493240356445,
            -1.7501848936080933,
            -1.7122395038604736,
            5.776655197143555,
            5.303980827331543,
            2.2383477687835693,
            3.057878017425537,
            1.7019703388214111,
            1.897189974784851,
            1.906922698020935,
            3.256523847579956,
            -0.01979401707649231,
            5.492525100708008,
            -0.7994564175605774,
            -2.88293719291687,
            1.9627432823181152,
            0.5554814338684082,
            -0.7546868324279785,
            1.7184679508209229,
            -4.58948278427124,
            -0.9296890497207642,
            -1.3877917528152466,
            -3.55275821685791,
            2.3486239910125732,
            4.065455436706543,
            1.6524443626403809,
            -1.846626877784729,
            -1.9260227680206299,
            -4.791686058044434,
            1.7102476358413696,
            -7.952063083648682,
            1.0903228521347046,
            -0.27604830265045166,
            0.8440232276916504,
            3.6129226684570312,
            -0.46139732003211975,
            0.41550004482269287,
            3.171865701675415,
            -3.021315813064575,
            0.1419246792793274,
            1.4473626613616943,
            2.4506614208221436,
            0.12461113929748535,
            0.015390165150165558,
            -2.137026309967041,
            -0.6230857372283936,
            1.1165412664413452,
            5.828458786010742,
            8.093388557434082,
            4.7380571365356445,
            4.180325031280518,
            -1.9450137615203857,
            -1.7500287294387817,
            -4.013519287109375,
            2.481752634048462,
            2.6137516498565674,
            -0.37579983472824097,
            -0.6780349612236023,
            3.089864730834961,
            -0.9102650880813599,
            1.091534972190857,
            1.0348892211914062,
            -1.131539225578308,
            4.272637367248535,
            -3.9253132343292236,
            -0.640794038772583,
            0.7513952255249023,
            -2.472377300262451,
            -1.3833441734313965,
            -2.03562068939209,
            2.4994447231292725,
            -0.6221483945846558,
            -2.298725128173828,
            -0.5509899258613586,
            -4.388584613800049,
            -2.3470969200134277,
            -1.7402888536453247,
            2.0627171993255615,
            0.882949709892273,
            0.9163491129875183,
            2.514374256134033,
            4.640083312988281,
            -0.7889548540115356,
            -1.843247890472412,
            -0.47646886110305786,
            4.572265625,
            0.8359897136688232,
            -2.032679557800293,
            2.1512982845306396,
            -4.073263168334961,
            -1.473638653755188,
            1.9670385122299194,
            2.627117872238159,
            4.257944107055664,
            4.103198051452637,
            3.4713454246520996,
            -2.9221603870391846,
            -0.25718337297439575,
            -1.3965749740600586,
            -3.209581136703491,
            -2.823153018951416,
            -5.517398834228516,
            2.6685125827789307,
            -3.0512163639068604,
            -4.577535629272461,
            1.044267177581787,
            -2.795288562774658,
            -1.5475430488586426,
            2.575594186782837,
            0.5453555583953857,
            1.9044270515441895,
            -4.279091835021973,
            -1.1911284923553467,
            -3.96380352973938,
            5.138500213623047,
            -0.4325638711452484,
            -2.018528461456299,
            2.7280421257019043,
            2.155974864959717,
            3.439588785171509,
            5.5639424324035645,
            4.338461875915527,
            -1.6331374645233154,
            0.45731791853904724,
            2.9294960498809814,
            1.0481809377670288,
            -1.1533879041671753,
            1.6983660459518433,
            1.734492301940918,
            0.2093566656112671,
            16.7878360748291,
            -0.8090618848800659,
            0.3574472665786743,
            -0.9971153736114502,
            -2.224018096923828,
            -3.4053735733032227,
            1.9568203687667847,
            1.129562258720398,
            -0.2086244523525238,
            2.76517391204834,
            -1.965552568435669,
            -5.4293060302734375,
            -0.891543984413147,
            -1.464515209197998,
            -2.6828746795654297,
            -1.777697205543518,
            -2.5308334827423096,
            1.2799638509750366,
            -4.978705883026123,
            -1.188061237335205,
            -0.9952041506767273,
            3.335409164428711,
            -2.084007740020752,
            -1.667159914970398,
            0.06118938326835632,
            5.563255310058594,
            2.1368470191955566,
            2.784177780151367,
            -6.131000518798828,
            -1.5491557121276855,
            1.6955899000167847,
            1.6987667083740234,
            0.4401679039001465,
            0.37758660316467285,
            -2.8085875511169434,
            3.7707228660583496,
            4.694049835205078,
            -2.037506103515625,
            3.6951823234558105,
            0.17956525087356567,
            -0.1344560980796814,
            4.059865474700928,
            -5.4683685302734375,
            1.9058544635772705,
            -0.5744150876998901,
            1.776164174079895,
            1.6503870487213135,
            -2.0537521839141846,
            -1.977170705795288,
            2.3227486610412598,
            1.535766839981079,
            0.28232431411743164,
            -1.4530208110809326,
            3.675551176071167,
            2.600444793701172,
            2.0341622829437256,
            1.776435375213623,
            -0.5629421472549438,
            4.693408966064453,
            3.090824604034424,
            2.207223415374756,
            -0.1083364188671112,
            -4.145527362823486,
            -2.7649283409118652,
            -1.0283739566802979,
            -0.7788167595863342,
            -2.748729944229126,
            2.8152804374694824,
            3.317150354385376,
            2.2184431552886963,
            4.714358806610107,
            2.856430768966675,
            -1.711153507232666,
            -4.418074131011963,
            -1.314295768737793,
            -5.949192047119141,
            -1.1592767238616943,
            1.5737155675888062,
            -0.0690721869468689,
            6.159135341644287,
            -5.986605644226074,
            3.064361095428467,
            -3.6556015014648438,
            -1.5782976150512695,
            2.937300682067871,
            -0.24661844968795776,
            5.602372169494629,
            -0.8325244784355164,
            -2.4766650199890137,
            4.265060901641846,
            0.9446225166320801,
            -0.8508414626121521,
            2.355267286300659,
            0.9827207922935486,
            2.509537696838379,
            -6.007716178894043,
            -2.279125213623047,
            -2.501610279083252,
            -4.0652689933776855,
            -3.8326542377471924,
            4.523865699768066,
            4.082486629486084,
            2.60107421875,
            -5.089291572570801,
            -1.4822115898132324,
            -0.16741406917572021,
            -1.3405892848968506,
            -6.548955917358398,
            -6.008024215698242,
            -5.387574672698975,
            6.287428855895996,
            -3.345932960510254,
            -0.994602382183075,
            0.0732850432395935,
            -2.9459009170532227,
            -4.343240261077881,
            -2.233529567718506,
            3.0521299839019775,
            2.904963970184326,
            6.501619338989258,
            -0.971420168876648,
            -3.6570184230804443,
            -1.9897983074188232,
            -2.4498143196105957,
            -3.804825782775879,
            -0.10149919986724854,
            1.7908300161361694,
            -1.0581159591674805,
            -0.8693151473999023,
            -1.4088826179504395,
            4.043418884277344,
            -3.8040194511413574,
            -3.9946811199188232,
            -2.1212315559387207,
            -2.2207319736480713,
            0.10911887884140015,
            -1.1653093099594116,
            3.025036334991455,
            -3.4429080486297607,
            4.216960906982422,
            1.8464264869689941,
            -2.4058332443237305,
            1.5842866897583008,
            9.65689468383789,
            -1.609961986541748,
            0.3812910318374634,
            -3.2304835319519043,
            -0.9410576820373535,
            -2.0545408725738525,
            -1.005441665649414,
            0.4301679730415344,
            3.2825353145599365,
            5.375404357910156,
            1.505468487739563,
            -1.0728542804718018,
            -2.1294758319854736
        ]
    },
    "authors": [
        {
            "authorId": "1932187449",
            "name": "Yuandong Tian"
        },
        {
            "authorId": "2167496459",
            "name": "Yiping Wang"
        },
        {
            "authorId": "4319427",
            "name": "Beidi Chen"
        },
        {
            "authorId": "145697585",
            "name": "S. Du"
        }
    ],
    "references": [
        {
            "paperId": "a87f40a49da377c0d00bebe711e417fc3b1d8969",
            "title": "Max-Margin Token Selection in Attention Mechanism"
        },
        {
            "paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3",
            "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"
        },
        {
            "paperId": "3f16d91bdfca925df761d27fd3b11af7d68c63d8",
            "title": "On the Role of Attention in Prompt-tuning"
        },
        {
            "paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
            "title": "The Impact of Positional Encoding on Length Generalization in Transformers"
        },
        {
            "paperId": "a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3",
            "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression"
        },
        {
            "paperId": "15288293edeae26dad6e37218cc1c0fc96316635",
            "title": "Do Transformers Parse while Predicting the Masked Word?"
        },
        {
            "paperId": "f3fde8a09b757ab356da1314d7a938504edf8314",
            "title": "How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "971091fd7e80ffb9e96c04fa00c4f6d1fb0c8213",
            "title": "Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron"
        },
        {
            "paperId": "91166a75f0e32b782a57028f1501aba6335ac550",
            "title": "A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity"
        },
        {
            "paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
            "title": "Transformers learn in-context by gradient descent"
        },
        {
            "paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
            "title": "What learning algorithm is in-context learning? Investigations with linear models"
        },
        {
            "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
            "title": "Scaling Instruction-Finetuned Language Models"
        },
        {
            "paperId": "13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0",
            "title": "Vision Transformers provably learn spatial structure"
        },
        {
            "paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f",
            "title": "In-context Learning and Induction Heads"
        },
        {
            "paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9",
            "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"
        },
        {
            "paperId": "f5f5616f39493566a9d502f611adcc8f1ceb394e",
            "title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit"
        },
        {
            "paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",
            "title": "Exploring Length Generalization in Large Language Models"
        },
        {
            "paperId": "f5db3b0a99e9ab7777b2fecf8b5d237715a3464d",
            "title": "Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning"
        },
        {
            "paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8",
            "title": "UL2: Unifying Language Learning Paradigms"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "0b0d7d87c58d41b92d907347b778032be5966f60",
            "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"
        },
        {
            "paperId": "8f2bca9d684005675e294b33c26481e36f528cdb",
            "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"
        },
        {
            "paperId": "3d3521b3fe6341befc9dd98ce96e091bc7b855da",
            "title": "Understanding Deep Contrastive Learning via Coordinate-wise Optimization"
        },
        {
            "paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
            "title": "Masked Autoencoders Are Scalable Vision Learners"
        },
        {
            "paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737",
            "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"
        },
        {
            "paperId": "2582a04918f6fe62dc142f2fca9ca0bb0b1d7895",
            "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization"
        },
        {
            "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
        },
        {
            "paperId": "0e9ac2cfc5a3ecb66eeace720901390f7809ba0a",
            "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"
        },
        {
            "paperId": "148011adfae37b821407aae84fcbbf7fb4619eb6",
            "title": "On the Expressive Power of Self-Attention Matrices"
        },
        {
            "paperId": "b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea",
            "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"
        },
        {
            "paperId": "4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506",
            "title": "Approximating How Single Head Attention Learns"
        },
        {
            "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "title": "Learning Transferable Visual Models From Natural Language Supervision"
        },
        {
            "paperId": "901b546ae60d1e3b6cfe80f19f0786321e701bf4",
            "title": "Why are Adaptive Methods Good for Attention Models?"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "10c86505de83647c7b4157595ab10f64e97c94ef",
            "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"
        },
        {
            "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
            "title": "Learning to summarize from human feedback"
        },
        {
            "paperId": "dd0cbd365304eddec5ee961ced19291e1463bad1",
            "title": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks"
        },
        {
            "paperId": "49a049dc85e2380dde80501a984878341dd8efdf",
            "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"
        },
        {
            "paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510",
            "title": "Infinite attention: NNGP and NTK for deep attention networks"
        },
        {
            "paperId": "75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1",
            "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "19e0fa37631c7588651ef8c335928a9a2d4b2e2c",
            "title": "A Mean-field Analysis of Deep ResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth"
        },
        {
            "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
            "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "0f1afdbbc806e191af4d294cd3e84787bd8de376",
            "title": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks"
        },
        {
            "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "title": "Reformer: The Efficient Transformer"
        },
        {
            "paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d",
            "title": "Are Transformers universal approximators of sequence-to-sequence functions?"
        },
        {
            "paperId": "fd9649aa3b3151615fffb6bd1b547ee7d82766ee",
            "title": "Transformer-Transducer: End-to-End Speech Recognition with Self-Attention"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "437fbb0cc305b7d3f0b2462a316cef0327099b26",
            "title": "Gradient descent optimizes over-parameterized deep ReLU networks"
        },
        {
            "paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd",
            "title": "Root Mean Square Layer Normalization"
        },
        {
            "paperId": "8215da38ec3b3ae08deb818ba3fc821377a120df",
            "title": "Towards Understanding the Importance of Shortcut Connections in Residual Networks"
        },
        {
            "paperId": "9b81a4df6fbc2702f335ff984381a1634d1be23d",
            "title": "Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks"
        },
        {
            "paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443",
            "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"
        },
        {
            "paperId": "62a0bafd54099a79d78a013611a0c7d38e237032",
            "title": "On Lazy Training in Differentiable Programming"
        },
        {
            "paperId": "611fe6e34df07ea1b2104899e49642b4531b53e9",
            "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"
        },
        {
            "paperId": "03e7e8663c69e691be6b6403b1eb1bbf593d31f2",
            "title": "Gradient Descent Finds Global Minima of Deep Neural Networks"
        },
        {
            "paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
            "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"
        },
        {
            "paperId": "134c165953e23a6dc7d4f0d86989e92362ca4335",
            "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks"
        },
        {
            "paperId": "42ec3db12a2e4628885451b13035c2e975220a25",
            "title": "A Convergence Theory for Deep Learning via Over-Parameterization"
        },
        {
            "paperId": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6",
            "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"
        },
        {
            "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
            "title": "Universal Transformers"
        },
        {
            "paperId": "7a84a692327534fd227fa1e07fcb3816b633c591",
            "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
        },
        {
            "paperId": "9c7de616d16e5643e9e29dfdf2d7d6001c548132",
            "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"
        },
        {
            "paperId": "f1d48ad5a04360bf65e793b84298d8e0570bf1cc",
            "title": "A mean field view of the landscape of two-layer neural networks"
        },
        {
            "paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
            "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
        },
        {
            "paperId": "63ae7e196430fe250e65b65fdef261d357921d28",
            "title": "Gradient Descent with Identity Initialization Efficiently Learns Positive-Definite Linear Transformations by Deep Residual Networks"
        },
        {
            "paperId": "181350e78a5c9e1f44e9cc2ace2137b7180856ba",
            "title": "Learning One Convolutional Layer with Overlapping Patches"
        },
        {
            "paperId": "f91248a4f587f89f1d1d8e557cee08b8114686d9",
            "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "fbe1d29737b66840d2bb9b74cd093858ef1805dd",
            "title": "When is a Convolutional Filter Easy To Learn?"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "97db7860df3ee3624f8b55d5820b00f893bc4f9a",
            "title": "Learning ReLUs via Gradient Descent"
        },
        {
            "paperId": "c6f2f35169abb6bfeb4dd2deec15d38587910168",
            "title": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis"
        },
        {
            "paperId": "fc756b45678ef7ffc1a796de62365013011b659e",
            "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "c4cb90a67f45e7cbacb5286e934b309e89843922",
            "title": "Attention is Turing-Complete"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": null,
            "title": "A mathematical framework for transformer circuits"
        },
        {
            "paperId": "5c43d84705cc320414ebad1525599146bcd68ca3",
            "title": "Infinite attention: NNGP and NTK for deep attention networks"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "c43b41816a62f8aa260130b6fe8c1d3cb4463628",
            "title": "Toward Understanding the Importance of Noise in Training Neural Networks"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607",
            "title": "A Survey on Context Learning"
        },
        {
            "paperId": "6038d62f22be3162324d3cb5214512966fc6ddb0",
            "title": "Music Transformer \uae30\ubc18 \uc74c\uc545"
        }
    ]
}