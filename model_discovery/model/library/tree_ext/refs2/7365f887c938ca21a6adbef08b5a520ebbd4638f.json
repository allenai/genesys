{
    "paperId": "7365f887c938ca21a6adbef08b5a520ebbd4638f",
    "externalIds": {
        "ArXiv": "1810.03993",
        "DBLP": "journals/corr/abs-1810-03993",
        "MAG": "2897042519",
        "DOI": "10.1145/3287560.3287596",
        "CorpusId": 52946140
    },
    "title": "Model Cards for Model Reporting",
    "abstract": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",
    "venue": "FAT",
    "year": 2018,
    "referenceCount": 48,
    "citationCount": 1545,
    "influentialCitationCount": 90,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1810.03993",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes model cards, a framework that can be used to document any trained machine learning model in the application fields of computer vision and natural language processing, and provides cards for two supervised models: One trained to detect smiling faces in images, and one training to detect toxic comments in text."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "49501003",
            "name": "Margaret Mitchell"
        },
        {
            "authorId": "81120201",
            "name": "Simone Wu"
        },
        {
            "authorId": "2064225225",
            "name": "Andrew Zaldivar"
        },
        {
            "authorId": "80940648",
            "name": "Parker Barnes"
        },
        {
            "authorId": "145177877",
            "name": "Lucy Vasserman"
        },
        {
            "authorId": "2044655623",
            "name": "Ben Hutchinson"
        },
        {
            "authorId": "79542084",
            "name": "Elena Spitzer"
        },
        {
            "authorId": "81316798",
            "name": "Inioluwa Deborah Raji"
        },
        {
            "authorId": "2076288",
            "name": "Timnit Gebru"
        }
    ],
    "references": [
        {
            "paperId": "44fc8d79fb8e0f8c6c6f680179b5803a789c6227",
            "title": "Measuring and Mitigating Unintended Bias in Text Classification"
        },
        {
            "paperId": "7985ae16f8eeff20adc91cc0f40d9b728b5b1efb",
            "title": "Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions"
        },
        {
            "paperId": "3bd24f29705b5f8ca5077e571ed78ea6aa4b11f5",
            "title": "Increasing Trust in AI Services through Supplier's Declarations of Conformity"
        },
        {
            "paperId": "a78f9467070992fc8742641ec97f9972597d869a",
            "title": "Fairness Definitions Explained"
        },
        {
            "paperId": "4a9831e5fec549edee454709048a51997ef60fb7",
            "title": "Did the Model Understand the Question?"
        },
        {
            "paperId": "e2bc01813d6ef50194588086e2adea42bdf2323b",
            "title": "The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards"
        },
        {
            "paperId": "0df347f5e3118fac7c351917e3a497899b071d1e",
            "title": "Datasheets for datasets"
        },
        {
            "paperId": "87eb23f934a0e6293ee8ee9b147fe0d456e65c96",
            "title": "Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science"
        },
        {
            "paperId": "18858cc936947fc96b5c06bbe3c6c2faa5614540",
            "title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"
        },
        {
            "paperId": "682b9d2212258fd5edbfca589c86390c31a956b0",
            "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"
        },
        {
            "paperId": "7d4cd64e135178a0f90108f05959192571f65085",
            "title": "Detecting representative data and generating synthetic samples to improve learning accuracy with imbalanced data sets"
        },
        {
            "paperId": "37f5d47019f467c74acff22a38ffd4b98bdcb5d4",
            "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments"
        },
        {
            "paperId": "d42b11ce90c9c69a20ed015b73dc33e0e4100a7b",
            "title": "Equality of Opportunity in Supervised Learning"
        },
        {
            "paperId": "06dadb8ce6c167c6dbaf27c75635bad792043c72",
            "title": "Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): The TRIPOD statement"
        },
        {
            "paperId": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
            "title": "Deep Learning Face Attributes in the Wild"
        },
        {
            "paperId": "70fda5147aedd42c64143a464117b5ffde18a2e4",
            "title": "Differential Privacy: A Survey of Results"
        },
        {
            "paperId": "e3999a46cb8aaf71131a77670da5c5c113aad01d",
            "title": "A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation"
        },
        {
            "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
        },
        {
            "paperId": "e5739d508e7390b8fb9e7754b09d028362ece0ed",
            "title": "The validity and practicality of sun-reactive skin types I through VI."
        },
        {
            "paperId": "35ea1c3e6c5f483c8f3437d82dbe0475d7314876",
            "title": "Hybrid sensing face detection and registration for low-light and unconstrained conditions."
        },
        {
            "paperId": null,
            "title": "Recruitment Software | Entelo"
        },
        {
            "paperId": null,
            "title": "Revision Assistant"
        },
        {
            "paperId": null,
            "title": "An Ethics Checklist for Data Scientists"
        },
        {
            "paperId": null,
            "title": "Responsible AI Practices"
        },
        {
            "paperId": null,
            "title": "of Enterprise - A Conversation with Naveen Rao"
        },
        {
            "paperId": null,
            "title": "Digital Reasoning Systems"
        },
        {
            "paperId": null,
            "title": "Institute for the Future, Omidyar Network's Tech, and Society Solutions Lab"
        },
        {
            "paperId": null,
            "title": "EthicsinTechnologyPractice: An Overview"
        },
        {
            "paperId": null,
            "title": "Project InnerEye -Medical Imaging AI to Empower"
        },
        {
            "paperId": null,
            "title": "Black Panther Face Scorecard: Wakandans Under the Coded Gaze"
        },
        {
            "paperId": null,
            "title": "AI For Recruiting Software | Talent Intelligence for High-Volume Hiring"
        },
        {
            "paperId": null,
            "title": "Litigating Algorithms: Challenging Government Use Of Algorithmic Decision Systems"
        },
        {
            "paperId": null,
            "title": "Unintended bias and names of frequently targeted groups"
        },
        {
            "paperId": null,
            "title": "Follow the Data: Deep Learning Leads the Transformation of Enterprise -A Conversation with"
        },
        {
            "paperId": null,
            "title": "The medical research gender gap: how excluding women from clinical trials is hurting our health"
        },
        {
            "paperId": "cb6a2c110f9fe675799c6aefe1082bb6390fdf49",
            "title": "COMPAS Risk Scales : Demonstrating Accuracy Equity and Predictive Parity Performance of the COMPAS Risk Scales in Broward County"
        },
        {
            "paperId": null,
            "title": "Machine Bias"
        },
        {
            "paperId": null,
            "title": "Big Data: A Tool for Inclusion or Exclusion? Understanding the Issues"
        },
        {
            "paperId": null,
            "title": "The Perpetual Line-Up"
        },
        {
            "paperId": null,
            "title": "How I\u2019m fighting Bias in Algorithms"
        },
        {
            "paperId": null,
            "title": "FDA Drug Safety Communication: Risk of next-morning impairment after use of insomnia drugs; FDA requires lower recommended doses for certain drugs containing zolpidem"
        },
        {
            "paperId": null,
            "title": "HP computers are racist"
        },
        {
            "paperId": null,
            "title": "IIHS (Insurance Institute for Highway Safety: Highway Loss Data Institute)"
        },
        {
            "paperId": null,
            "title": "Special Issue: Side Impact Crashworthiness"
        },
        {
            "paperId": null,
            "title": "Guidance for the Study of Drugs Likely to Be Used in the Elderly"
        },
        {
            "paperId": null,
            "title": "Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics"
        },
        {
            "paperId": "b19270b120b855f41ca727a74b5e8395096d8b9c",
            "title": "To appear: IEEE Transactions on Information Forensics and Security Face Recognition Performance: Role of Demographic Information"
        },
        {
            "paperId": null,
            "title": "FAT* \u201919, January 29\u201331, 2019"
        }
    ]
}