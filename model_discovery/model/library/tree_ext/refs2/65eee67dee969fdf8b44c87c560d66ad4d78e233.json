{
    "paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
    "externalIds": {
        "MAG": "2951633259",
        "DBLP": "conf/iclr/ChungAB17",
        "ArXiv": "1609.01704",
        "CorpusId": 1463401
    },
    "title": "Hierarchical Multiscale Recurrent Neural Networks",
    "abstract": "Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.",
    "venue": "International Conference on Learning Representations",
    "year": 2016,
    "referenceCount": 72,
    "citationCount": 525,
    "influentialCitationCount": 72,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel multiscale approach, called the hierarchical multiscales recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism is proposed."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "8270717",
            "name": "Junyoung Chung"
        },
        {
            "authorId": "3103594",
            "name": "Sungjin Ahn"
        },
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        }
    ],
    "references": [
        {
            "paperId": "563783de03452683a9206e85fe6d661714436686",
            "title": "HyperNetworks"
        },
        {
            "paperId": "ca9d174c70c9102d88f2707bc395d6a384e1de1d",
            "title": "Surprisal-Driven Feedback in Recurrent Networks"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
            "title": "Recurrent Highway Networks"
        },
        {
            "paperId": "7dded890956b37df5ac4c42b8ffbc142725f2801",
            "title": "Recurrent Memory Array Structures"
        },
        {
            "paperId": "136cf66392f1d6bf42da4cc070888996dc472b91",
            "title": "On Multiplicative Integration with Recurrent Neural Networks"
        },
        {
            "paperId": "4ba25cb493ac7a03fc15d3b936257c9a6c689c1d",
            "title": "Strategic Attentive Writer for Learning Macro-Actions"
        },
        {
            "paperId": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105",
            "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"
        },
        {
            "paperId": "bbd0e204f48a45735e1065c8b90b298077b73192",
            "title": "One-shot Learning with Memory-Augmented Neural Networks"
        },
        {
            "paperId": "6b570069f14c7588e066f7138e1f21af59d62e61",
            "title": "Theano: A Python framework for fast computation of mathematical expressions"
        },
        {
            "paperId": "952454718139dba3aafc6b3b67c4f514ac3964af",
            "title": "Recurrent Batch Normalization"
        },
        {
            "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
            "title": "Deep Networks with Stochastic Depth"
        },
        {
            "paperId": "acec46ffd3f6046af97529127d98f1d623816ea4",
            "title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"
        },
        {
            "paperId": "4d070993cb75407b285e14cb8aac0077624ef4d9",
            "title": "Character-based Neural Machine Translation"
        },
        {
            "paperId": "f6fda11d2b31ad66dd008a65f7e708aa64a27703",
            "title": "Architectural Complexity Measures of Recurrent Neural Networks"
        },
        {
            "paperId": "6eecc808d4c74e7d0d7ef6b8a4112c985ced104d",
            "title": "Binarized Neural Networks"
        },
        {
            "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "title": "Mastering the game of Go with deep neural networks and tree search"
        },
        {
            "paperId": "f84d5add20d4df0a6c89c47a920354c272cbdbd8",
            "title": "Regularizing RNNs by Stabilizing Activations"
        },
        {
            "paperId": "6b904d6e84c98c6ce22ce6923224b205a2a24ee1",
            "title": "Segmental Recurrent Neural Networks"
        },
        {
            "paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec",
            "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "878ba5458e9e51f0b341fd9117fa0b43ef4096d3",
            "title": "End-to-end attention-based large vocabulary speech recognition"
        },
        {
            "paperId": "3056add22b20e3361c38c0472d294a79d4031cb4",
            "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"
        },
        {
            "paperId": "b3e11af71552d39070dd9183acb8b8171bc22b38",
            "title": "A Hierarchical Recurrent Encoder-Decoder for Generative Context-Aware Query Suggestion"
        },
        {
            "paperId": "5b791cd374c7109693aaddee2c12d659ae4e3ec0",
            "title": "Grid Long Short-Term Memory"
        },
        {
            "paperId": "0c3b69b5247ef18fd5bab1109d87a04184ea8f4b",
            "title": "A Recurrent Latent Variable Model for Sequential Data"
        },
        {
            "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
            "title": "Human-level control through deep reinforcement learning"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "d14c7e5f5cace4c925abc74c88baa474e9f31a28",
            "title": "Gated Feedback Recurrent Neural Networks"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "title": "Show and tell: A neural image caption generator"
        },
        {
            "paperId": "081651b38ff7533550a3adfc1c00da333a8fe86c",
            "title": "How transferable are features in deep neural networks?"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "193edd20cae92c6759c18ce93eeea96afd9528eb",
            "title": "Deep learning in neural networks: An overview"
        },
        {
            "paperId": "5522764282c85aea422f1c4dc92ff7e0ca6987bc",
            "title": "A Clockwork RNN"
        },
        {
            "paperId": "331f0fb3b6176c6e463e0401025b04f6ace9ccd3",
            "title": "Neural Variational Inference and Learning in Belief Networks"
        },
        {
            "paperId": "6492351b7e6a33d7a06d6141572467acd8ee3051",
            "title": "One-Shot Adaptation of Supervised Deep Convolutional Models"
        },
        {
            "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "title": "Auto-Encoding Variational Bayes"
        },
        {
            "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
            "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
        },
        {
            "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
            "title": "Generating Sequences With Recurrent Neural Networks"
        },
        {
            "paperId": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "title": "Speech recognition with deep recurrent neural networks"
        },
        {
            "paperId": "4ef03716945bd3907458efbe1bbf8928dafc1efc",
            "title": "Regularization and nonlinearities for neural language models: when are they needed?"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "93c20e38c85b69fc2d2eb314b3c1217913f7db11",
            "title": "Generating Text with Recurrent Neural Networks"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "68db33b01ef82cbafb440e5f4bee30458cbb9871",
            "title": "Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks"
        },
        {
            "paperId": "12496bf48ebdb5ab3c92bc911d6ee42369fa70bc",
            "title": "Sequence Labelling in Structured Domains with Hierarchical Recurrent Neural Networks"
        },
        {
            "paperId": "f758d8e6dee5348c2b5bf56f33994ad96e88a591",
            "title": "Adaptive weighing of context models for lossless data compression"
        },
        {
            "paperId": "9eb7daa88879f283ae05e359d6c502a320b833c9",
            "title": "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard"
        },
        {
            "paperId": "a38468d0c7bc403581740324f229b61f85c2a706",
            "title": "Bursty and Hierarchical Structure in Streams"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "f6e91c9e7e8f8a577a98ecfcfa998212a683195a",
            "title": "Learning long-term dependencies in NARX recurrent neural networks"
        },
        {
            "paperId": "b13813b49f160e1a2010c44bd4fb3d09a28446e3",
            "title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "50c770b425a5bb25c77387f687a9910a9d130722",
            "title": "Learning Complex, Extended Sequences Using the Principle of History Compression"
        },
        {
            "paperId": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "title": "Induction of Multiscale Temporal Structure"
        },
        {
            "paperId": "bcd857d75841aa3e92cd4284a8818aba9f6c0c3f",
            "title": "Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS"
        },
        {
            "paperId": "fd0611608567d9a278e2f030c9b19544f8fae035",
            "title": "Neural Networks for Machine Learning"
        },
        {
            "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "title": "Deep Learning"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": null,
            "title": "The human knowledge compression contest"
        },
        {
            "paperId": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb",
            "title": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": null,
            "title": "Large text compression benchmark"
        },
        {
            "paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
            "title": "Learning Deep Architectures for AI"
        },
        {
            "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
        },
        {
            "paperId": "4cf3569e045993dfe090749f26a55a768684ab86",
            "title": "Mixture density networks"
        },
        {
            "paperId": "c69201d091dd92699fd90a17b9e3407319726791",
            "title": "Neural sequence chunkers"
        },
        {
            "paperId": null,
            "title": "REFERENCES"
        }
    ]
}