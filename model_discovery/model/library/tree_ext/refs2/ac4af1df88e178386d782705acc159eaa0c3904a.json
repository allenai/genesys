{
    "paperId": "ac4af1df88e178386d782705acc159eaa0c3904a",
    "externalIds": {
        "DBLP": "phd/ndltd/Konda02",
        "MAG": "2156737235",
        "CorpusId": 207779694
    },
    "title": "Actor-Critic Algorithms",
    "abstract": "Many complex decision making problems like scheduling in manufacturing systems, portfolio management in finance, admission control in communication networks etc., with clear and precise objectives, can be formulated as stochastic dynamic programming problems in which the objective of decision making is to maximize a single \u201coverall\u201d reward. In these formulations, finding an optimal decision policy involves computing a certain \u201cvalue function\u201d which assigns to each state the optimal reward one would obtain if the system was started from that state. This function then naturally prescribes the optimal policy, which is to take decisions that drive the system to states with maximum value. \nFor many practical problems, the computation of the exact value function is intractable, analytically and numerically, due to the enormous size of the state space. Therefore one has to resort to one of the following approximation methods to find a good sub-optimal policy: (1)\u00a0Approximate the value function. (2)\u00a0Restrict the search for a good policy to a smaller family of policies. \nIn this thesis, we propose and study actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies. Actor-critic algorithms have two learning units: an actor and a critic. An actor is a decision maker with a tunable parameter. A critic is a function approximator. The critic tries to approximate the value function of the policy used by the actor, and the actor in turn tries to improve its policy based on the current approximation provided by the critic. Furthermore, the critic evolves on a faster time-scale than the actor. \nWe propose several variants of actor-critic algorithms. In all the variants, the critic uses Temporal Difference (TD) learning with linear function approximation. Some of the variants are inspired by a new geometric interpretation of the formula for the gradient of the overall reward with respect to the actor parameters. This interpretation suggests a natural set of basis functions for the critic, determined by the family of policies parameterized by the actor's parameters. We concentrate on the average expected reward criterion but we also show how the algorithms can be modified for other objective criteria. We prove convergence of the algorithms for problems with general (finite, countable, or continuous) state and decision spaces. \nTo compute the rate of convergence (ROC) of our algorithms, we develop a general theory of the ROC of two-time-scale algorithms and we apply it to study our algorithms. In the process, we study the ROC of TD learning and compare it with related methods such as Least Squares TD (LSTD). We study the effect of the basis functions used for linear function approximation on the ROC of TD. We also show that the ROC of actor-critic algorithms does not depend on the actual basis functions used in the critic but depends only on the subspace spanned by them and study this dependence. \nFinally, we compare the performance of our algorithms with other algorithms that optimize over a parameterized family of policies. We show that when only the \u201cnatural\u201d basis functions are used for the critic, the rate of convergence of the actor critic algorithms is the same as that of certain stochastic gradient descent algorithms. However, with appropriate additional basis functions for the critic, we show that our algorithms outperform the existing ones in terms of ROC. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)",
    "venue": "Neural Information Processing Systems",
    "year": 1999,
    "referenceCount": 20,
    "citationCount": 3016,
    "influentialCitationCount": 200,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This thesis proposes and studies actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies, and proves convergence of the algorithms for problems with general state and decision spaces."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "50844636",
            "name": "Vijay R. Konda"
        },
        {
            "authorId": "144224173",
            "name": "J. Tsitsiklis"
        }
    ],
    "references": [
        {
            "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
            "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
        },
        {
            "paperId": "abe0f00db308460ec36618d2a59238c58d8affcd",
            "title": "Actor-Critic - Type Learning Algorithms for Markov Decision Processes"
        },
        {
            "paperId": "f608268033a797a38047575e6b4de65899eedd5f",
            "title": "Simulation-based optimization of Markov reward processes"
        },
        {
            "paperId": "f76a76747699db19d469587d6d66ae72e958c43b",
            "title": "Perturbation realization, potentials, and sensitivity analysis of Markov processes"
        },
        {
            "paperId": "5c29049c6cc7e93bd42ccd55d70a5b92120ceec6",
            "title": "Stochastic approximation with two time scales"
        },
        {
            "paperId": "4caabff8b795d5d7334b1ff0d55d4aab366fa6ca",
            "title": "Likelihood ratio gradient estimation for stochastic recursions"
        },
        {
            "paperId": "a82db864e472b5aa6313596ef9919f64e3363b1f",
            "title": "Dynamic Programming and Optimal Control, Two Volume Set"
        },
        {
            "paperId": "990b10ce4ef643e148b6c719e99dbf2430671a74",
            "title": "Adaptive Algorithms and Stochastic Approximations"
        },
        {
            "paperId": "dbf409d9f6e83b5724d116223669a58f655f4794",
            "title": "A New Approach to the Limit Theory of Recurrent Markov Chains"
        },
        {
            "paperId": "9813c4933a6a3f5d0c9a95cdff43258522979fef",
            "title": "The O.D.E. Method for Convergence of Stochastic Approximation and Reinforcement Learning"
        },
        {
            "paperId": "2d6177244636f892c1620e3e5c2870c5e3902b55",
            "title": "Average cost temporal-difference learning"
        },
        {
            "paperId": "24da3011dfdefbdcee20d12ca3c2a348ad6f3aad",
            "title": "An Analysis of Temporal-Difference Learning with Function Approximation"
        },
        {
            "paperId": null,
            "title": "Reinfor ement Learning: An Introdu tion"
        },
        {
            "paperId": "3598ce7f42fcf458f67c6034ed48d342eb3f08f9",
            "title": "Neuro-dynamic programming"
        },
        {
            "paperId": null,
            "title": "Reinfor ement learning algorithms for partially observable Markov de ision problems"
        },
        {
            "paperId": null,
            "title": "Control of multiple servi e, multiple reseour e ommuni ation networks"
        },
        {
            "paperId": "7c268394c54e26f1488a23640291d71267d6a97c",
            "title": "Markov Chains and Stochastic Stability"
        },
        {
            "paperId": null,
            "title": "Neuron-like elements that an solve di(cid:30) ult learning ontrol problems"
        },
        {
            "paperId": "08bcd967e6ca896eb85d6e03561aabf138df65d1",
            "title": "Proceedings of Ihe 1986 Winter Simulation"
        },
        {
            "paperId": null,
            "title": "for some 0 (cid:20) (cid:11) < 1 and non-negative sequen e f\u00c6kg. 1. If sup k \u00c6k < 1 then sup k ak < 1. 2."
        }
    ]
}