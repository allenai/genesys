{
    "paperId": "11e6b5a30a921e6028662105148fac41a76f0500",
    "externalIds": {
        "MAG": "2160840682",
        "DBLP": "journals/jmlr/DrineasM05",
        "DOI": "10.1007/11503415_22",
        "CorpusId": 215012
    },
    "title": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning",
    "abstract": null,
    "venue": "Journal of machine learning research",
    "year": 2005,
    "referenceCount": 42,
    "citationCount": 1053,
    "influentialCitationCount": 84,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An algorithm to compute an easily-interpretable low-rank approximation to an n x n Gram matrix G such that computations of interest may be performed more rapidly."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1738441",
            "name": "P. Drineas"
        },
        {
            "authorId": "143884206",
            "name": "Michael W. Mahoney"
        }
    ],
    "references": [
        {
            "paperId": "ed183bc71216446b69e32f21d5aad1e636b7be7c",
            "title": "Fast Monte Carlo Algorithms for Matrices I: Approximating Matrix Multiplication"
        },
        {
            "paperId": "e6cc62d8a66b49cd060205456c6f6a23c77ccfd7",
            "title": "Matrix Approximation and Projective Clustering via Iterative Sampling"
        },
        {
            "paperId": "2e4e5e1d51e2b3e7c8bedfbc725dfdf1dffd3530",
            "title": "Sampling Sub-problems of Heterogeneous Max-cut Problems and Approximation Algorithms"
        },
        {
            "paperId": "cfa15801bf4e7bf610d38dc86da62b83e2ddedcb",
            "title": "Learning Eigenfunctions Links Spectral Embedding and Kernel PCA"
        },
        {
            "paperId": "2ff87fef3fc79b5bb47f2783e5b28084ef6b83c4",
            "title": "Learning a kernel matrix for nonlinear dimensionality reduction"
        },
        {
            "paperId": "94f7f1ccd7e25c4d5f997d66365b00231478e987",
            "title": "A kernel view of the dimensionality reduction of manifolds"
        },
        {
            "paperId": "5f39fe2659603f4194fd638d1a2e17985415c3bb",
            "title": "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering"
        },
        {
            "paperId": "88816ae492956f3004daa41357166f1181c0c1bf",
            "title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"
        },
        {
            "paperId": "57a66ac4a4e0a00d2cdee8711ce0a18b49e9f7a2",
            "title": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data"
        },
        {
            "paperId": "d3f27fcb3cae41f2d4cc990674908b3201c14d4b",
            "title": "Pass efficient algorithms for approximating large matrices"
        },
        {
            "paperId": "93ee729418053ef7c30ad5b2348eb6e8eb2b815e",
            "title": "Observations on the Nystr\u00f6m Method for Gaussian Process Prediction"
        },
        {
            "paperId": "0db7af02be7cbadc029f9104a8c784d02de42df7",
            "title": "Efficient SVM Training Using Low-Rank Kernel Representations"
        },
        {
            "paperId": "81d4797f753e70d4bbac3002860d4ae846241a78",
            "title": "Fast Monte-Carlo algorithms for approximate matrix multiplication"
        },
        {
            "paperId": "620cf63f101f20c03a2e530c287c2603839de15e",
            "title": "Spectral analysis of data"
        },
        {
            "paperId": "9fe4e58bcf31d528a30f15bd0b180fbe298f91ef",
            "title": "Fast computation of low rank matrix approximations"
        },
        {
            "paperId": "0b41df73cf0aeda76b060a735d9fcc252ee761c9",
            "title": "An Introduction to Support Vector Machines and Other Kernel\u2010based Learning Methods"
        },
        {
            "paperId": "e5e12dd2604cb7cc9cde165509ca8db764784688",
            "title": "Sampling Techniques for Kernel Methods"
        },
        {
            "paperId": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "title": "A global geometric framework for nonlinear dimensionality reduction."
        },
        {
            "paperId": "f0a3e1752e1146da927adc24ae07144ab2e744ec",
            "title": "Nonlinear dimensionality reduction by locally linear embedding."
        },
        {
            "paperId": "9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "title": "Sparse Greedy Matrix Approximation for Machine Learning"
        },
        {
            "paperId": "763f4b3c0e830b92a2d6af3c547fcc4e52b5225e",
            "title": "The Effect of the Input Density Distribution on Kernel-based Classifiers"
        },
        {
            "paperId": "6ec7c724aa1d906e9e9f81c58497adddb22175b8",
            "title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods"
        },
        {
            "paperId": "bbc531f6ca5e83c6fa507bdf6399ecf76ef2e614",
            "title": "Fast Monte-Carlo algorithms for finding low-rank approximations"
        },
        {
            "paperId": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "title": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
        },
        {
            "paperId": "7a61a3bf41fc770186a58fa34466af337e997ef6",
            "title": "An improved training algorithm for support vector machines"
        },
        {
            "paperId": "ec9d9acd9556c7b274ce0752330586661784bb36",
            "title": "A Theory of Pseudoskeleton Approximations"
        },
        {
            "paperId": "fed98ea8ecad5034441fd0ac9f728479183e3b9e",
            "title": "Simplified Support Vector Decision Rules"
        },
        {
            "paperId": null,
            "title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning Approximating a Gram Matrix for Improved Kernel-Based Learning 337"
        },
        {
            "paperId": "f81822838da53f45716620d0e678f41d25f60b4f",
            "title": "Approximating a Gram Matrix for Improved Kernel-Based Learning (Extended Abstract)"
        },
        {
            "paperId": "6f3bb84ee1b5d638e2d605ae0eb1014e2b6e3931",
            "title": "FAST MONTE CARLO ALGORITHMS FOR MATRICES II: COMPUTING A LOW-RANK APPROXIMATION TO A MATRIX\u2217"
        },
        {
            "paperId": "13c82489c1568b67265d17a15720001a5737171e",
            "title": "Spectral grouping using the Nystrom method"
        },
        {
            "paperId": "857e0bdfc1f1115e56a736819ae7bd6a0bb5b1f0",
            "title": "FAST MONTE CARLO ALGORITHMS FOR MATRICES III: COMPUTING A COMPRESSED APPROXIMATE MATRIX DECOMPOSITION\u2217"
        },
        {
            "paperId": null,
            "title": "Diffusion Maps and Geometric Harmonics"
        },
        {
            "paperId": "87845d33186c63ac3fea8051324f800d820e4e04",
            "title": "Observations on the Nystr\u00f6m Method for Gaussian Processes"
        },
        {
            "paperId": "3ca6f4d92d1ea57c5f52170a85d63da1f1355a81",
            "title": "The maximum-volume concept in approximation by low-rank matrices"
        },
        {
            "paperId": "b6fff8b8ea77f157913986e7af53951d9fc1128e",
            "title": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines"
        },
        {
            "paperId": "22ca4af08b52e7051c59dc692d80d8d7a727c3d8",
            "title": "Clustering in large graphs and matrices"
        },
        {
            "paperId": "6c09c25131ac2e7f01fd14ce2a576c209f8ad23e",
            "title": "Matrix Perturbation Theory"
        },
        {
            "paperId": "821728d897c08a0d02d406c17962e21a8c1e19a7",
            "title": "Economic inequality and burden-sharing in the provision of local environmental quality"
        },
        {
            "paperId": null,
            "title": "Computational Methods for Integral Equations"
        },
        {
            "paperId": "3478ff51521bbcc27b2fc04ea75aa27c611f6df4",
            "title": "Generalized inverses: theory and applications"
        },
        {
            "paperId": null,
            "title": "of Our Main Theorem (2 of 4) First, bound the spectral norm: Note: If k >= r = rank(W), then: Proof of Our Main Theorem (3 of 4) Next, bound the Frobenius norm"
        }
    ]
}