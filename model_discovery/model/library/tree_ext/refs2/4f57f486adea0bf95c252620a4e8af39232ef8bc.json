{
    "paperId": "4f57f486adea0bf95c252620a4e8af39232ef8bc",
    "externalIds": {
        "MAG": "2765833400",
        "ArXiv": "1710.05941",
        "CorpusId": 196158220
    },
    "title": "Swish: a Self-Gated Activation Function",
    "abstract": "The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose a new activation function, named Swish, which is simply $f(x) = x \\cdot \\text{sigmoid}(x)$. Our experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.",
    "venue": "",
    "year": 2017,
    "referenceCount": 42,
    "citationCount": 555,
    "influentialCitationCount": 42,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging datasets, and its simplicity and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "3377142",
            "name": "Prajit Ramachandran"
        },
        {
            "authorId": "2368067",
            "name": "Barret Zoph"
        },
        {
            "authorId": "2827616",
            "name": "Quoc V. Le"
        }
    ],
    "references": [
        {
            "paperId": "168b7d0ab57a331a228ce21ffd1becbb93066f79",
            "title": "Neural Optimizer Search with Reinforcement Learning"
        },
        {
            "paperId": "d0611891b9e8a7c5731146097b6f201578f47b2f",
            "title": "Learning Transferable Architectures for Scalable Image Recognition"
        },
        {
            "paperId": "20e70ad5bb2c5b7b996dba8b1548ad23121a55b3",
            "title": "Flexible Rectified Linear Units for Improving Convolutional Neural Networks"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "424a6e62084d919bfc2e39a507c263e5991ebdad",
            "title": "Self-Normalizing Neural Networks"
        },
        {
            "paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
        },
        {
            "paperId": "58123025178256279bb060ca5da971b62bc329ee",
            "title": "Sharp Minima Can Generalize For Deep Nets"
        },
        {
            "paperId": "35ac9f0ab123486f6a3f02f2522475fa2462db6a",
            "title": "Neural Circuit Inference from Function to Structure"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "67d968c7450878190e45ac7886746de867bf673d",
            "title": "Neural Architecture Search with Reinforcement Learning"
        },
        {
            "paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81",
            "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"
        },
        {
            "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "title": "Densely Connected Convolutional Networks"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "03e04983f7ce6a9c2b42948840b3312aea33f9f3",
            "title": "On the Expressive Power of Deep Neural Networks"
        },
        {
            "paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51",
            "title": "Conditional Image Generation with PixelCNN Decoders"
        },
        {
            "paperId": "4954fa180728932959997a4768411ff9136aac81",
            "title": "TensorFlow: A system for large-scale machine learning"
        },
        {
            "paperId": "1c4e9156ca07705531e45960b7a919dc473abb51",
            "title": "Wide Residual Networks"
        },
        {
            "paperId": "0e2b3faed39561f712c3b14a08c7c36272d9857a",
            "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units"
        },
        {
            "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "title": "Identity Mappings in Deep Residual Networks"
        },
        {
            "paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62",
            "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "title": "Rethinking the Inception Architecture for Computer Vision"
        },
        {
            "paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a",
            "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"
        },
        {
            "paperId": "adf3b591281688b7e71b254ab931b2aa39b4b59f",
            "title": "Empirical Evaluation of Rectified Activations in Convolutional Network"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "93100ebe89840bc235c586ddc6daccd262707fec",
            "title": "Learning Activation Functions to Improve Deep Neural Networks"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "title": "Maxout Networks"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "67107f78a84bdb2411053cb54e94fa226eea6d8e",
            "title": "Deep Sparse Rectifier Neural Networks"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
            "paperId": "1f88427d7aa8225e47f946ac41a0667d7b69ac52",
            "title": "What is the best multi-stage architecture for object recognition?"
        },
        {
            "paperId": "9f10ce06b4832721e6007d9399cb650b86f0db8c",
            "title": "Fast and Slow Contrast Adaptation in Retinal Circuitry"
        },
        {
            "paperId": "03de8578480c53677c484e1facfced74f4f5b045",
            "title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "367f2c63a6f6a10b3b64b8729d601e69337ee3cc",
            "title": "Rectifier Nonlinearities Improve Neural Network Acoustic Models"
        },
        {
            "paperId": null,
            "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        }
    ]
}