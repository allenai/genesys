{
    "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
    "externalIds": {
        "DBLP": "journals/corr/abs-2210-02414",
        "ArXiv": "2210.02414",
        "DOI": "10.48550/arXiv.2210.02414",
        "CorpusId": 252715691
    },
    "title": "GLM-130B: An Open Bilingual Pre-trained Model",
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "referenceCount": 154,
    "citationCount": 913,
    "influentialCitationCount": 114,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2210.02414",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained is unveiled and a unique scaling property of GLM-130B is leveraged to reach INT4 quantization without post training, with almost no performance loss."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -4.8591837882995605,
            -1.0163440704345703,
            -1.3057937622070312,
            4.337570667266846,
            -1.346496343612671,
            2.6723508834838867,
            4.5987138748168945,
            -2.6260855197906494,
            -0.025067627429962158,
            0.47950083017349243,
            0.4806477725505829,
            4.097636699676514,
            0.668586015701294,
            2.095245361328125,
            -2.8583548069000244,
            -1.2468931674957275,
            0.4083860218524933,
            -1.3505666255950928,
            2.0558736324310303,
            1.8039275407791138,
            -1.9598829746246338,
            2.0658960342407227,
            -2.3607592582702637,
            -4.828011512756348,
            -1.4288780689239502,
            -0.9506714344024658,
            5.860023021697998,
            0.703876256942749,
            -2.5633699893951416,
            -4.26198673248291,
            0.8344054222106934,
            -6.137989044189453,
            0.6143999099731445,
            -4.165112495422363,
            1.906831979751587,
            -4.1982741355896,
            0.06385037302970886,
            7.271300792694092,
            -4.361853122711182,
            2.722512722015381,
            -2.978471279144287,
            -2.2785089015960693,
            -0.04953788220882416,
            1.6847807168960571,
            0.11464323103427887,
            0.2246330827474594,
            2.1756083965301514,
            -0.078333780169487,
            0.7560566663742065,
            2.2781126499176025,
            2.0919175148010254,
            2.6959729194641113,
            2.49774169921875,
            2.6982765197753906,
            -0.6392301917076111,
            -1.3670107126235962,
            -2.635288953781128,
            0.37118250131607056,
            1.2563014030456543,
            -2.1901278495788574,
            2.603066921234131,
            5.621228218078613,
            0.7279271483421326,
            1.0731616020202637,
            1.1392241716384888,
            -6.237245559692383,
            -1.1462903022766113,
            5.1263532638549805,
            1.6538598537445068,
            -0.4823172688484192,
            1.0403974056243896,
            -5.321340560913086,
            -0.22490344941616058,
            0.20127657055854797,
            -1.4694486856460571,
            -0.25375640392303467,
            0.45538339018821716,
            -3.7542500495910645,
            -1.1520226001739502,
            -1.5275394916534424,
            -0.28866440057754517,
            2.803990602493286,
            1.4078741073608398,
            -0.09286758303642273,
            3.255880832672119,
            0.41471147537231445,
            -3.1583950519561768,
            -1.415550708770752,
            3.130788803100586,
            -4.6610612869262695,
            2.027808904647827,
            -0.3959707021713257,
            1.5641334056854248,
            3.4625473022460938,
            -3.8811419010162354,
            -0.9129513502120972,
            -2.229496717453003,
            -2.734739303588867,
            -0.8261885046958923,
            -0.74488365650177,
            5.4005279541015625,
            0.11626823991537094,
            3.514604330062866,
            -0.5774073600769043,
            1.690131425857544,
            -3.8358006477355957,
            0.760635495185852,
            0.925356388092041,
            2.0141794681549072,
            -3.1808595657348633,
            -6.079370498657227,
            1.8855388164520264,
            -2.7160322666168213,
            -0.09358522295951843,
            -4.036252498626709,
            -3.3694980144500732,
            -2.9163739681243896,
            -0.5906575918197632,
            -0.05320999026298523,
            5.667356491088867,
            -1.06964111328125,
            -3.2346901893615723,
            -2.5800368785858154,
            -1.4984625577926636,
            0.9917946457862854,
            2.1090173721313477,
            -0.48128414154052734,
            1.5110301971435547,
            -1.2160215377807617,
            -2.289968729019165,
            3.7916512489318848,
            -3.338646411895752,
            2.0378828048706055,
            -2.282471179962158,
            3.6521341800689697,
            0.970771074295044,
            -4.214797496795654,
            2.146406888961792,
            -1.233426570892334,
            -0.10483112931251526,
            2.748363494873047,
            0.8429566025733948,
            0.8166399002075195,
            0.7928940057754517,
            -1.114332675933838,
            -0.12622188031673431,
            0.3694390654563904,
            -2.014761447906494,
            -0.5898032188415527,
            4.1666340827941895,
            4.911942481994629,
            -4.120924949645996,
            -2.04826021194458,
            0.7878118753433228,
            -0.7256858944892883,
            3.9420299530029297,
            -4.8170576095581055,
            -0.22014103829860687,
            -2.075871467590332,
            1.5115160942077637,
            -0.7185662388801575,
            0.7778017520904541,
            -9.037890434265137,
            -3.9176700115203857,
            2.550102710723877,
            -3.0171306133270264,
            0.47804486751556396,
            2.2141385078430176,
            -2.0794870853424072,
            1.3509396314620972,
            1.1577835083007812,
            2.717536211013794,
            1.390899896621704,
            4.062068462371826,
            0.023400738835334778,
            3.847811222076416,
            1.2206459045410156,
            -1.2575410604476929,
            0.3021776080131531,
            0.9895431995391846,
            -1.9836283922195435,
            0.5923197269439697,
            -4.640719413757324,
            -0.2526383399963379,
            -0.19161474704742432,
            -3.585989475250244,
            -3.1376707553863525,
            -2.071593761444092,
            0.5884777307510376,
            0.10900940001010895,
            -1.6619584560394287,
            -2.6617050170898438,
            5.99653959274292,
            6.229918479919434,
            2.9682939052581787,
            2.675788164138794,
            2.9527506828308105,
            3.3212122917175293,
            -0.6545949578285217,
            0.9898892641067505,
            3.1335134506225586,
            -1.8459386825561523,
            -3.641422748565674,
            -2.721132278442383,
            2.851266860961914,
            3.8105382919311523,
            -4.268952369689941,
            2.869349718093872,
            3.081411600112915,
            1.6891146898269653,
            2.094858169555664,
            -0.5326090455055237,
            -3.4807567596435547,
            -0.7044824361801147,
            -1.6517114639282227,
            -3.1231632232666016,
            -4.527717113494873,
            2.0574541091918945,
            5.047496795654297,
            1.6615279912948608,
            -1.1694395542144775,
            -0.45701587200164795,
            -1.1157855987548828,
            -3.906426429748535,
            2.3969855308532715,
            -3.8809781074523926,
            1.1443458795547485,
            0.656947672367096,
            -0.22978085279464722,
            0.55545973777771,
            -2.313779354095459,
            -3.457592010498047,
            -0.18822923302650452,
            -1.64432692527771,
            -4.900669097900391,
            -1.7760071754455566,
            -3.910977602005005,
            1.9142413139343262,
            0.8611375689506531,
            -2.052022933959961,
            4.798229694366455,
            1.7231783866882324,
            -1.1287541389465332,
            3.668623447418213,
            2.244263172149658,
            -1.0096533298492432,
            -1.1943366527557373,
            1.5473225116729736,
            -0.5964553356170654,
            -0.31261929869651794,
            -2.829313039779663,
            -3.0903384685516357,
            3.827125072479248,
            -3.931943655014038,
            0.6343753337860107,
            1.3619346618652344,
            0.1747705340385437,
            0.9765748381614685,
            -1.5954819917678833,
            0.7518748641014099,
            2.6684677600860596,
            3.6011624336242676,
            -1.5131748914718628,
            3.059584856033325,
            -1.8654799461364746,
            -2.193449020385742,
            -3.8797833919525146,
            1.2862132787704468,
            -3.517291784286499,
            4.578179836273193,
            3.9345040321350098,
            0.6657285094261169,
            1.2579700946807861,
            -1.6340998411178589,
            -0.45779797434806824,
            -3.4904863834381104,
            -3.021606922149658,
            -1.0127241611480713,
            3.1946909427642822,
            1.523916244506836,
            -0.1904391348361969,
            -3.7206873893737793,
            0.07670912891626358,
            -2.9712977409362793,
            -0.9683401584625244,
            -1.6459596157073975,
            0.5922470688819885,
            0.014522179961204529,
            -0.7327603101730347,
            -2.4757542610168457,
            -0.120747409760952,
            5.040480613708496,
            -2.878277540206909,
            -1.5312861204147339,
            -0.48724043369293213,
            0.44020724296569824,
            3.5395779609680176,
            -0.24670138955116272,
            -0.13693787157535553,
            1.1309776306152344,
            0.9815905094146729,
            2.5235981941223145,
            3.4380087852478027,
            -1.8160444498062134,
            2.553738594055176,
            3.620943069458008,
            -0.10875177383422852,
            -1.4269413948059082,
            4.416477680206299,
            -3.5535166263580322,
            -1.190449833869934,
            3.996260166168213,
            2.3146464824676514,
            -3.612865686416626,
            0.7464256286621094,
            0.16209691762924194,
            -0.5248624682426453,
            -1.4010367393493652,
            -2.7873716354370117,
            3.227105140686035,
            0.6452739834785461,
            1.9104341268539429,
            -6.752110004425049,
            -2.63669753074646,
            -0.2987591028213501,
            1.1150233745574951,
            2.0323033332824707,
            1.5989749431610107,
            -2.729351282119751,
            3.364161491394043,
            0.21291705965995789,
            4.4242844581604,
            1.800584316253662,
            3.997716188430786,
            -0.3417184352874756,
            -3.0838189125061035,
            -1.472527265548706,
            0.05351540446281433,
            1.1435344219207764,
            -0.4086974263191223,
            1.2076220512390137,
            4.902243614196777,
            -2.588702917098999,
            0.3868551254272461,
            3.0322623252868652,
            0.7640421986579895,
            0.37619662284851074,
            -2.007056713104248,
            1.3394931554794312,
            -2.808199644088745,
            2.7412171363830566,
            2.0859339237213135,
            2.53454327583313,
            0.8609495162963867,
            0.8094823956489563,
            2.8461601734161377,
            1.0053480863571167,
            -2.348151206970215,
            1.8968528509140015,
            1.60368812084198,
            0.7160108089447021,
            -3.271815299987793,
            2.5808794498443604,
            0.2179907113313675,
            4.136621475219727,
            -0.37590667605400085,
            11.578038215637207,
            -4.709790229797363,
            3.3346898555755615,
            -4.599721908569336,
            -2.3571224212646484,
            -1.3118832111358643,
            -2.2661731243133545,
            4.573048114776611,
            -3.3855865001678467,
            -2.7932090759277344,
            2.5150647163391113,
            -4.224776268005371,
            0.19166156649589539,
            3.5654075145721436,
            -2.887406826019287,
            3.5886528491973877,
            3.0842442512512207,
            1.7723079919815063,
            0.7985358238220215,
            1.8020477294921875,
            -3.2123823165893555,
            5.1119842529296875,
            0.06660650670528412,
            -2.050334930419922,
            -0.9706594347953796,
            2.7254793643951416,
            -1.0368974208831787,
            -0.032776810228824615,
            -3.975302219390869,
            -3.9636006355285645,
            -2.113097667694092,
            -3.4404687881469727,
            2.1202235221862793,
            1.9892997741699219,
            3.3666181564331055,
            2.8724513053894043,
            2.7241315841674805,
            -0.17608095705509186,
            -2.9155361652374268,
            1.6037753820419312,
            2.8324317932128906,
            -1.6748226881027222,
            -2.868924140930176,
            2.0287139415740967,
            -4.094747543334961,
            -2.1316189765930176,
            -2.4679388999938965,
            -5.154603004455566,
            -1.247580885887146,
            1.2890243530273438,
            1.7444636821746826,
            4.43765926361084,
            4.63144588470459,
            3.6107687950134277,
            -1.9362893104553223,
            3.201969861984253,
            4.300404071807861,
            1.1242876052856445,
            0.2022680938243866,
            0.774315357208252,
            5.984069347381592,
            1.8546192646026611,
            -3.014101266860962,
            4.169348239898682,
            -0.7969763278961182,
            4.7443928718566895,
            -1.7030354738235474,
            -2.781111240386963,
            -0.6640989184379578,
            2.3562331199645996,
            1.467771291732788,
            1.5970783233642578,
            2.778792142868042,
            -2.544482707977295,
            2.028940200805664,
            2.353336811065674,
            -1.5235053300857544,
            7.618814945220947,
            1.7687219381332397,
            2.4313979148864746,
            -0.32348817586898804,
            1.1128852367401123,
            -0.09921985864639282,
            0.6370034217834473,
            7.284086227416992,
            -5.870169639587402,
            -4.178196907043457,
            0.19698366522789001,
            1.134691834449768,
            1.6825909614562988,
            -3.785865306854248,
            -0.5860134363174438,
            0.8067525625228882,
            -0.7718170881271362,
            -3.5130486488342285,
            3.512223720550537,
            1.5620571374893188,
            -0.8974167108535767,
            1.3071969747543335,
            0.3057812750339508,
            -1.2952237129211426,
            -2.19136118888855,
            -0.7103641033172607,
            0.7803993225097656,
            3.0119786262512207,
            -1.057610034942627,
            -1.6029939651489258,
            2.6752827167510986,
            -1.5820268392562866,
            -0.6644673347473145,
            0.8393067121505737,
            0.33757075667381287,
            1.7505733966827393,
            -3.0626251697540283,
            -2.5947563648223877,
            -0.8040125966072083,
            3.703032970428467,
            -3.946802854537964,
            -1.5767396688461304,
            5.286523818969727,
            -0.0009897537529468536,
            2.3341469764709473,
            1.8749349117279053,
            2.323730945587158,
            2.643533706665039,
            3.356682777404785,
            2.718400239944458,
            2.4494166374206543,
            1.8643755912780762,
            -1.9997916221618652,
            -3.893789291381836,
            1.0397220849990845,
            0.3408868908882141,
            -1.7733869552612305,
            1.54629385471344,
            -5.412898540496826,
            -1.2590057849884033,
            0.19160011410713196,
            -2.5505661964416504,
            5.790351867675781,
            5.018286228179932,
            2.0495128631591797,
            -1.3172286748886108,
            -2.480067014694214,
            -0.5518743395805359,
            -0.33808574080467224,
            -4.084324359893799,
            0.4278780221939087,
            -1.6005933284759521,
            0.8703914880752563,
            0.5326390266418457,
            -1.4013102054595947,
            1.168681025505066,
            -1.2438232898712158,
            -3.648444414138794,
            -2.2654552459716797,
            -1.9200167655944824,
            1.6755179166793823,
            1.051656723022461,
            2.2850875854492188,
            -1.9832204580307007,
            -2.1591262817382812,
            2.4400172233581543,
            6.219884395599365,
            3.27608585357666,
            3.7645134925842285,
            5.610998630523682,
            -1.757032036781311,
            -1.2059414386749268,
            -2.179783821105957,
            1.8020110130310059,
            3.2672741413116455,
            -4.339419841766357,
            0.6188191175460815,
            0.3856568932533264,
            -0.9835337400436401,
            1.7171292304992676,
            3.1611640453338623,
            0.6573911309242249,
            1.8413314819335938,
            -3.1535849571228027,
            -4.205783367156982,
            -0.08966493606567383,
            0.4555113613605499,
            -0.367357075214386,
            0.7961350679397583,
            2.302736520767212,
            1.2198677062988281,
            -2.990206003189087,
            -1.3447006940841675,
            -2.0914134979248047,
            -1.5834146738052368,
            -0.7111399173736572,
            4.200801849365234,
            2.9207091331481934,
            -0.6762911677360535,
            2.5906715393066406,
            2.5821080207824707,
            -0.6544699668884277,
            -3.1731581687927246,
            -0.5799050331115723,
            2.315622091293335,
            1.4634802341461182,
            -3.227363109588623,
            -1.6972419023513794,
            -2.605746030807495,
            0.1289866417646408,
            1.6460312604904175,
            1.9133219718933105,
            0.05219626426696777,
            6.348149299621582,
            -0.41755416989326477,
            -0.7120356559753418,
            -1.819063663482666,
            -1.464993953704834,
            -2.733612298965454,
            -1.163520336151123,
            -2.1106643676757812,
            1.761175274848938,
            -2.882425546646118,
            -3.136754274368286,
            2.9692165851593018,
            -2.307983636856079,
            -0.11781758069992065,
            4.776463985443115,
            -2.8540048599243164,
            0.4726617932319641,
            -1.9596726894378662,
            0.3155740797519684,
            -5.530311584472656,
            4.824941635131836,
            2.772787094116211,
            -0.14859038591384888,
            0.2308521270751953,
            1.4598784446716309,
            4.4903998374938965,
            2.9770076274871826,
            1.5679811239242554,
            -0.8970609307289124,
            -1.2938590049743652,
            2.759402275085449,
            -0.13554489612579346,
            -2.9807395935058594,
            0.30504298210144043,
            -2.3857614994049072,
            1.5960932970046997,
            16.477460861206055,
            -0.526548445224762,
            -0.630610466003418,
            -1.9602108001708984,
            -1.8106781244277954,
            -5.371207237243652,
            -2.1741702556610107,
            2.4938318729400635,
            1.4380464553833008,
            0.23413483798503876,
            -1.7925810813903809,
            -2.4173638820648193,
            -2.725681781768799,
            2.0810861587524414,
            -3.0753397941589355,
            0.3530080318450928,
            -1.0707132816314697,
            3.6248626708984375,
            -1.086248517036438,
            -0.48417118191719055,
            -0.24290688335895538,
            3.088390350341797,
            -2.3645856380462646,
            -3.982769727706909,
            -0.9015859365463257,
            3.6848673820495605,
            1.795027256011963,
            2.2178215980529785,
            -3.9604849815368652,
            1.7439260482788086,
            -2.55794358253479,
            2.924384117126465,
            3.507831573486328,
            2.5659894943237305,
            -0.3568350672721863,
            1.5091331005096436,
            2.8342173099517822,
            -4.950418949127197,
            2.1615848541259766,
            1.153080701828003,
            -1.5512516498565674,
            2.490200996398926,
            -1.6126952171325684,
            1.2216578722000122,
            -1.9581315517425537,
            1.3100141286849976,
            1.5095738172531128,
            -4.464354515075684,
            -0.5934807658195496,
            5.8597588539123535,
            3.3636391162872314,
            -1.8195000886917114,
            -0.49134892225265503,
            0.29887789487838745,
            1.1685031652450562,
            -1.969930648803711,
            2.1089015007019043,
            -2.636173725128174,
            0.8444138765335083,
            0.44754675030708313,
            -0.37437254190444946,
            -3.8358359336853027,
            -3.684854745864868,
            -4.336174488067627,
            -1.6112656593322754,
            0.3519030511379242,
            -5.525269031524658,
            3.3738391399383545,
            0.8203481435775757,
            0.8201090693473816,
            3.305762767791748,
            1.904600977897644,
            1.0834237337112427,
            -1.2443565130233765,
            -3.187142848968506,
            -2.8771486282348633,
            4.207411766052246,
            -2.6565182209014893,
            -1.8443742990493774,
            9.249755859375,
            0.4894440770149231,
            1.5888134241104126,
            -3.6005239486694336,
            -4.621070861816406,
            3.183688163757324,
            -3.825969696044922,
            1.4447975158691406,
            0.974282443523407,
            2.361459970474243,
            2.759230852127075,
            -1.7055392265319824,
            -0.18442979454994202,
            2.783025026321411,
            5.449366569519043,
            1.7798081636428833,
            -3.211242914199829,
            -1.802516222000122,
            -4.497703552246094,
            -2.9232254028320312,
            -0.9195497035980225,
            4.601165771484375,
            1.9316012859344482,
            1.2421894073486328,
            -3.053290367126465,
            1.3492367267608643,
            0.45776522159576416,
            -4.705856800079346,
            -6.9417829513549805,
            -3.2300150394439697,
            -1.1511938571929932,
            3.661613941192627,
            -3.2775416374206543,
            -1.2062171697616577,
            0.8132860660552979,
            2.0038907527923584,
            -0.7696230411529541,
            2.0731639862060547,
            1.7728607654571533,
            1.0044811964035034,
            3.357091188430786,
            0.7506803870201111,
            -1.6008074283599854,
            -1.2779942750930786,
            -1.812290906906128,
            -3.3935546875,
            0.18535730242729187,
            0.05661892890930176,
            -3.9032528400421143,
            -0.7310702204704285,
            -0.8267414569854736,
            1.4918309450149536,
            -2.945488452911377,
            -0.979550838470459,
            1.2552146911621094,
            -0.3621636927127838,
            -2.777082920074463,
            1.3933945894241333,
            1.052152156829834,
            -0.8424962162971497,
            0.37609362602233887,
            2.4712748527526855,
            -1.8566160202026367,
            -1.9512207508087158,
            9.882716178894043,
            0.7770816087722778,
            -3.9778666496276855,
            -0.4400508999824524,
            1.0050549507141113,
            1.2877310514450073,
            1.112613320350647,
            -0.45116931200027466,
            1.8817627429962158,
            2.1180367469787598,
            1.6781606674194336,
            -4.108150482177734,
            -2.826237678527832
        ]
    },
    "authors": [
        {
            "authorId": "2051712753",
            "name": "Aohan Zeng"
        },
        {
            "authorId": "2111312892",
            "name": "Xiao Liu"
        },
        {
            "authorId": "66395694",
            "name": "Zhengxiao Du"
        },
        {
            "authorId": null,
            "name": "Zihan Wang"
        },
        {
            "authorId": "2051311700",
            "name": "Hanyu Lai"
        },
        {
            "authorId": "145573466",
            "name": "Ming Ding"
        },
        {
            "authorId": "2109506541",
            "name": "Zhuoyi Yang"
        },
        {
            "authorId": "2125063007",
            "name": "Yifan Xu"
        },
        {
            "authorId": "2163967642",
            "name": "Wendi Zheng"
        },
        {
            "authorId": "2186982651",
            "name": "Xiao Xia"
        },
        {
            "authorId": "1403621152",
            "name": "W. Tam"
        },
        {
            "authorId": "2124489983",
            "name": "Zixuan Ma"
        },
        {
            "authorId": "2114921664",
            "name": "Yufei Xue"
        },
        {
            "authorId": "2467444",
            "name": "Jidong Zhai"
        },
        {
            "authorId": "1712168",
            "name": "Wenguang Chen"
        },
        {
            "authorId": "47243067",
            "name": "P. Zhang"
        },
        {
            "authorId": "2047998",
            "name": "Yuxiao Dong"
        },
        {
            "authorId": "2148911956",
            "name": "Jie Tang"
        }
    ],
    "references": [
        {
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
        },
        {
            "paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f",
            "title": "What Language Model to Train if You Have One Million GPU Hours?"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b",
            "title": "Rationale-Augmented Ensembles in Language Models"
        },
        {
            "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
            "title": "Emergent Abilities of Large Language Models"
        },
        {
            "paperId": "1d650f1afd45c59ff907396fe8b678595dcb85ea",
            "title": "Memory-Based Model Editing at Scale"
        },
        {
            "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
            "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
        },
        {
            "paperId": "707bd332d2c21dc5eb1f02a52d4a0506199aae76",
            "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"
        },
        {
            "paperId": "2f291b0b59483e9c3c4a3391f34e6b29aff848a1",
            "title": "DeepStruct: Pretraining of Language Models for Structure Prediction"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
            "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af",
            "title": "Compression of Generative Pre-trained Language Models via Quantization"
        },
        {
            "paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca",
            "title": "DeepNet: Scaling Transformers to 1,000 Layers"
        },
        {
            "paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca",
            "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"
        },
        {
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
            "paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19",
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
        },
        {
            "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
            "title": "LaMDA: Language Models for Dialog Applications"
        },
        {
            "paperId": "39c77e29a232a9fb62b3a3c89c50f487d73e27ce",
            "title": "Counterfactual Memorization in Neural Language Models"
        },
        {
            "paperId": "a3184d40d390793232c99c89b57b8f65c16320b2",
            "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"
        },
        {
            "paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170",
            "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"
        },
        {
            "paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
            "title": "Ethical and social risks of harm from Language Models"
        },
        {
            "paperId": "68f141724814839d556a989646194be88641b143",
            "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
        },
        {
            "paperId": "4a247cbfca9dcf91e2da24e6d2d84601a9041a8f",
            "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs"
        },
        {
            "paperId": "cbf98ebe967e0f3f3236e7932f37013b98244e94",
            "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"
        },
        {
            "paperId": "2582a04918f6fe62dc142f2fca9ca0bb0b1d7895",
            "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization"
        },
        {
            "paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
            "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"
        },
        {
            "paperId": "dfd104dd0ff28b1bde2fbd4c4d6d3ccb4761f639",
            "title": "Learning Compact Metrics for MT"
        },
        {
            "paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79",
            "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"
        },
        {
            "paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04",
            "title": "8-bit Optimizers via Block-wise Quantization"
        },
        {
            "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
            "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
        },
        {
            "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
            "title": "Finetuned Language Models Are Zero-Shot Learners"
        },
        {
            "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
        },
        {
            "paperId": "e6a237ab883e503b10b73b3a411c0078c47c9830",
            "title": "Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies"
        },
        {
            "paperId": "76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
            "title": "On the Opportunities and Risks of Foundation Models"
        },
        {
            "paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
            "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
        },
        {
            "paperId": "4237cbebe788a97174f48dc398082739bbffe95b",
            "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"
        },
        {
            "paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models"
        },
        {
            "paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
            "title": "CogView: Mastering Text-to-Image Generation via Transformers"
        },
        {
            "paperId": "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
            "title": "Societal Biases in Language Generation: Progress and Challenges"
        },
        {
            "paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f",
            "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"
        },
        {
            "paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c",
            "title": "Carbon Emissions and Large Neural Network Training"
        },
        {
            "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
        },
        {
            "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
        },
        {
            "paperId": "240b0caabb415578bdea4da7d0a32bdff2e8163f",
            "title": "Editing Factual Knowledge in Language Models"
        },
        {
            "paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae",
            "title": "An Empirical Study of Training Self-Supervised Vision Transformers"
        },
        {
            "paperId": "098370508aaf56f718a472511987ac2072d0f917",
            "title": "Detecting Hate Speech with GPT-3"
        },
        {
            "paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17",
            "title": "GPT Understands, Too"
        },
        {
            "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
            "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
        },
        {
            "paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d",
            "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"
        },
        {
            "paperId": "ce9ca56036307217ea565644d3d3bd74b879e045",
            "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP"
        },
        {
            "paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
            "title": "Zero-Shot Text-to-Image Generation"
        },
        {
            "paperId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2",
            "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"
        },
        {
            "paperId": "f96318f202540a554cc28a8db03808e972ddcb38",
            "title": "Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News Detection in English"
        },
        {
            "paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01",
            "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"
        },
        {
            "paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
        },
        {
            "paperId": "6914a7997ff4be207fa7b3472a9c5879abaec646",
            "title": "RealFormer: Transformer Likes Residual Attention"
        },
        {
            "paperId": "5270b626feb66c8c363e93ba6608daae93c5003b",
            "title": "Modifying Memories in Transformer Models"
        },
        {
            "paperId": "b360427d0991143013da6a208ccf28bcc8028fab",
            "title": "Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training"
        },
        {
            "paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6",
            "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"
        },
        {
            "paperId": "399e7d8129c60818ee208f236c8dda17e876d21f",
            "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
        },
        {
            "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
            "title": "Measuring Massive Multitask Language Understanding"
        },
        {
            "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
            "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
        },
        {
            "paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e",
            "title": "Memory-Efficient Pipeline-Parallel DNN Training"
        },
        {
            "paperId": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e",
            "title": "Self-Supervised Learning: Generative or Contrastive"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "24e4d3370dc366d6b353d1d6818a0df266bb31b9",
            "title": "MLSUM: The Multilingual Summarization Corpus"
        },
        {
            "paperId": "babeda48b10a4d638252118f2238d05a06f4ec55",
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models"
        },
        {
            "paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768",
            "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"
        },
        {
            "paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a",
            "title": "Pre-trained models for natural language processing: A survey"
        },
        {
            "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
            "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
        },
        {
            "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
            "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a",
            "title": "On Layer Normalization in the Transformer Architecture"
        },
        {
            "paperId": "80376bdec5f534be78ba82821f540590ebce5559",
            "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"
        },
        {
            "paperId": "681f4fbce872f138cbac9cdd92e8f6ed89ba6f8d",
            "title": "Measurement and Fairness"
        },
        {
            "paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
            "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"
        },
        {
            "paperId": "9e1241f017a627beca2542e378a88c642c32098b",
            "title": "Semantic Noise Matters for Neural Natural Language Generation"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187",
            "title": "Quantifying the Carbon Emissions of Machine Learning"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8",
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations"
        },
        {
            "paperId": "81b4920ad488affaee27389ff9540b7fea90a4ce",
            "title": "\u201cGoing on a vacation\u201d takes longer than \u201cGoing for a walk\u201d: A Study of Temporal Commonsense Understanding"
        },
        {
            "paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0",
            "title": "Natural Questions: A Benchmark for Question Answering Research"
        },
        {
            "paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b",
            "title": "WinoGrande"
        },
        {
            "paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
            "title": "Energy and Policy Considerations for Deep Learning in NLP"
        },
        {
            "paperId": "867db5097ad6aaef098c60b0845785b440eca49a",
            "title": "GLTR: Statistical Detection and Visualization of Generated Text"
        },
        {
            "paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88",
            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"
        },
        {
            "paperId": "d9f6ada77448664b71128bb19df15765336974a6",
            "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
        },
        {
            "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
            "title": "Are Sixteen Heads Really Better than One?"
        },
        {
            "paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c",
            "title": "Parameter-Efficient Transfer Learning for NLP"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"
        },
        {
            "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
            "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
        },
        {
            "paperId": "11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25",
            "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"
        },
        {
            "paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7",
            "title": "Gender Bias in Coreference Resolution"
        },
        {
            "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
            "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
        },
        {
            "paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9",
            "title": "Generating Wikipedia by Summarizing Long Sequences"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
            "title": "Mixed Precision Training"
        },
        {
            "paperId": "400e746bc8027c4b5f915cae6123cd1775484b4d",
            "title": "Position-aware Attention and Supervised Data Improve Slot Filling"
        },
        {
            "paperId": "ffe28f6bf0e9e6bbca6313319aa1a5409d283d9b",
            "title": "Zero-Shot Learning\u2014A Comprehensive Evaluation of the Good, the Bad and the Ugly"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
            "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "title": "Gaussian Error Linear Units (GELUs)"
        },
        {
            "paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a",
            "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"
        },
        {
            "paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3",
            "title": "Semantic Parsing on Freebase from Question-Answer Pairs"
        },
        {
            "paperId": "c92970286c535992a86539b761357761e97a37ee",
            "title": "Towards Robust Linguistic Analysis using OntoNotes"
        },
        {
            "paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f",
            "title": "The Winograd Schema Challenge"
        },
        {
            "paperId": "e7e7b9a731678bf0494fe29cbebb42a822224cc6",
            "title": "Modeling Relations and Their Mentions without Labeled Text"
        },
        {
            "paperId": "25e7efa59a5cf68e0fc9401e4c6fa7b2bfe3f1ae",
            "title": "Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling"
        },
        {
            "paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008",
            "title": "ROUGE: A Package for Automatic Evaluation of Summaries"
        },
        {
            "paperId": "5aa70188f70d349580aed96c10a68f57dace2d33",
            "title": "A Linear Programming Formulation for Global Inference in Natural Language Tasks"
        },
        {
            "paperId": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb",
            "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"
        },
        {
            "paperId": "fd268d35d3e3e4d368e94050163fba9ede42ffe2",
            "title": "The GENIA corpus: an annotated research abstract corpus in molecular biology domain"
        },
        {
            "paperId": "8665c9b459e4161825baf1f25b5141f41a5085ff",
            "title": "A bridging model for parallel computation"
        },
        {
            "paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe",
            "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"
        },
        {
            "paperId": null,
            "title": "Accelerated inference for large transformer models using nvidia triton inference server"
        },
        {
            "paperId": null,
            "title": "leading to in-depth studies of LLMs\u2019 theory, capacity, and flaws. Researchers can also modify the model architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al"
        },
        {
            "paperId": null,
            "title": "Jurassic-1: Technical details and evaluation"
        },
        {
            "paperId": "53d8b356551a2361020a948f64454a6d599af69f",
            "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
        },
        {
            "paperId": "b9478e237b58160c65acd2c41894493d27e2c277",
            "title": "WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"
        },
        {
            "paperId": "3e65f572322e192fe36ae52a8a7f025b0685dfc6",
            "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets"
        },
        {
            "paperId": null,
            "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"
        },
        {
            "paperId": null,
            "title": "Also, it is known that LLMs can suffer from problems in fairness, bias, privacy, and truthful"
        },
        {
            "paperId": null,
            "title": "2021) later observe that Pre-LN is still unable to handle the vulnerable training"
        },
        {
            "paperId": null,
            "title": "Ethos: an online hate speech detection dataset"
        },
        {
            "paperId": "310b8117ae5ce3df8aa6304ad382525b9b46937e",
            "title": "The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+ 2020)"
        },
        {
            "paperId": null,
            "title": "2020) is a comprehensive language modeling benchmark that initially includes 22 different text datasets from diverse domains. We report our results over a part of 18 datasets"
        },
        {
            "paperId": null,
            "title": "2020), or namely Crowdsourced Stereotype Pairs benchmark, is widely used for measuring biases for masked language models. It collects 1508 examples with nine different conventional biases and adopts"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "22d64e6d55c34f1a1a5884305c940526d826deab",
            "title": "MultiWOZ 2."
        },
        {
            "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
            "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
        },
        {
            "paperId": null,
            "title": "transparency to all the researchers and promote the research to reduce the potential harm of LLMs, like algorithms to identify the synthetic text"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": null,
            "title": "Improving language understanding with unsupervised learning"
        },
        {
            "paperId": "fc090a68e45e0e6337136777d21c87b76a90ae72",
            "title": "From TreeBank to PropBank"
        },
        {
            "paperId": null,
            "title": "\u2022 Predicate Recognition: given a segment of a sentence and its corresponding semantic role, identify which verb it is related to"
        },
        {
            "paperId": null,
            "title": "It would not be possible to reach its current status if without the collaboration of multiple teams-the Knowledge Engineering Group (KEG)"
        },
        {
            "paperId": null,
            "title": "According to d'Holbach, people always act according to"
        },
        {
            "paperId": null,
            "title": "Semantic Role Labeling: the traditional task form, where a verb (i.e., predicate) is annotated in text, and the model is asked to generate related semantic roles"
        },
        {
            "paperId": null,
            "title": "Semantic Role Filling: given a verb and a potential semantic role, the model is asked to judge whether the role exists in the sentence and generate it"
        },
        {
            "paperId": null,
            "title": "A) not suitable for the young. (B) not suitable for the old. (C) important, but unpleasant. (D) none of the above"
        },
        {
            "paperId": null,
            "title": "\u2022 Optimize A100 kernel's computing efficiency => A100 kernels prefer square-shaped inputs, and seq_len=2,048 is optimal for our hidden-state dimension"
        },
        {
            "paperId": null,
            "title": "The timeline of major issues that training GLM-130B encountered and addressed, as of"
        },
        {
            "paperId": null,
            "title": "Below is a prompted example with 1-shot priming. We predict the probability on"
        },
        {
            "paperId": null,
            "title": "Technical Report 2022-10-06"
        },
        {
            "paperId": null,
            "title": "GPT-3 was estimated to use 500 tons of carbon emissions footprint (CO2eq)"
        }
    ]
}