{
    "paperId": "e0222a1ae6874f7fff128c3da8769ab95963da04",
    "externalIds": {
        "MAG": "2962808855",
        "DBLP": "conf/ijcai/HuPHQW018",
        "DOI": "10.24963/ijcai.2018/570",
        "CorpusId": 13559921
    },
    "title": "Reinforced Mnemonic Reader for Machine Reading Comprehension",
    "abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2017,
    "referenceCount": 47,
    "citationCount": 212,
    "influentialCitationCount": 27,
    "openAccessPdf": {
        "url": "https://www.ijcai.org/proceedings/2018/0570.pdf",
        "status": "BRONZE"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects: a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "8367832",
            "name": "Minghao Hu"
        },
        {
            "authorId": "49236691",
            "name": "Yuxing Peng"
        },
        {
            "authorId": "1557265719",
            "name": "Zhen Huang"
        },
        {
            "authorId": "1767521",
            "name": "Xipeng Qiu"
        },
        {
            "authorId": "49807919",
            "name": "Furu Wei"
        },
        {
            "authorId": "143849609",
            "name": "M. Zhou"
        }
    ],
    "references": [
        {
            "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "title": "Deep Contextualized Word Representations"
        },
        {
            "paperId": "8490431f3a76fbd165d108eba938ead212a2a639",
            "title": "Stochastic Answer Networks for Machine Reading Comprehension"
        },
        {
            "paperId": "1fe6bee85774244d8674cbb20a25e8d153cecb17",
            "title": "FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension"
        },
        {
            "paperId": "0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a",
            "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering"
        },
        {
            "paperId": "12e20e4ea572dbe476fd894c5c9a9930cf250dd2",
            "title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension"
        },
        {
            "paperId": "ffb949d3493c3b2f3c9acf9c75cb03938933ddf0",
            "title": "Adversarial Examples for Evaluating Reading Comprehension Systems"
        },
        {
            "paperId": "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f",
            "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering"
        },
        {
            "paperId": "f98788f32b0d33d200c9bc7d900d0ef39519c927",
            "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics"
        },
        {
            "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "title": "A Deep Reinforced Model for Abstractive Summarization"
        },
        {
            "paperId": "bc6ad001c395e92920839e45dfd7e05ce69405d2",
            "title": "Machine Comprehension by Text-to-Text Neural Question Generation"
        },
        {
            "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
            "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
        },
        {
            "paperId": "c50cd7df4271ef94a0a60894f0e2cf4ef89fb912",
            "title": "Ruminating Reader: Reasoning with Gated Multi-hop Attention"
        },
        {
            "paperId": "104715e1097b7ebee436058bfd9f45540f269845",
            "title": "Reading Wikipedia to Answer Open-Domain Questions"
        },
        {
            "paperId": "46a7afc2b23bb3406fb64c36b6f2696145b54f24",
            "title": "Making Neural QA as Simple as Possible but not Simpler"
        },
        {
            "paperId": "62f88e8fc3b44c5627f2b4721b08498d78103893",
            "title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering"
        },
        {
            "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
            "title": "A Structured Self-attentive Sentence Embedding"
        },
        {
            "paperId": "d2ad6ae5f57844c4474cd3f318c3cdc469994fae",
            "title": "Structural Embedding of Syntactic Trees for Machine Comprehension"
        },
        {
            "paperId": "e94697b98b707f557436e025bdc8498fa261d3bc",
            "title": "Multi-Perspective Context Matching for Machine Comprehension"
        },
        {
            "paperId": "6c8353697cdbb98dfba4f493875778c4286d3e3a",
            "title": "Self-Critical Sequence Training for Image Captioning"
        },
        {
            "paperId": "e978d832a4d86571e1b52aa1685dc32ccb250f50",
            "title": "Dynamic Coattention Networks For Question Answering"
        },
        {
            "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
            "title": "Bidirectional Attention Flow for Machine Comprehension"
        },
        {
            "paperId": "c636a2dd242908fe2e598a1077c0c57bfdea8633",
            "title": "ReasoNet: Learning to Stop Reading in Machine Comprehension"
        },
        {
            "paperId": "1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6",
            "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction"
        },
        {
            "paperId": "ff1861b71eaedba46cb679bbe2c585dbe18f9b19",
            "title": "Machine Comprehension Using Match-LSTM and Answer Pointer"
        },
        {
            "paperId": "c6e5df6322659276da6133f9b734a389d7a255e8",
            "title": "Attention-over-Attention Neural Networks for Reading Comprehension"
        },
        {
            "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
            "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
            "title": "Deep Reinforcement Learning for Dialogue Generation"
        },
        {
            "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
            "title": "Long Short-Term Memory-Networks for Machine Reading"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "35b91b365ceb016fb3e022577cec96fb9b445dc5",
            "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations"
        },
        {
            "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
            "title": "Teaching Machines to Read and Comprehend"
        },
        {
            "paperId": "9653d5c2c7844347343d073bbedd96e05d52f69b",
            "title": "Pointer Networks"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "title": "End-To-End Memory Networks"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": null,
            "title": "and Percy Liang"
        },
        {
            "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "title": "Deep Learning"
        },
        {
            "paperId": null,
            "title": "Mustafa Suleyman"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "f35855f5dcb76039409f7ec5b788b476d01c7f5f",
            "title": "Proceedings of CoNLL"
        },
        {
            "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
        },
        {
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction"
        },
        {
            "paperId": "e1082fee59f00b1096b2206f3dc434e68f7ffac6",
            "title": "Neural Computation"
        }
    ]
}