{
    "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
    "externalIds": {
        "DBLP": "journals/jmlr/SrivastavaHKSS14",
        "MAG": "2095705004",
        "DOI": "10.5555/2627435.2670313",
        "CorpusId": 6844431
    },
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
    "venue": "Journal of machine learning research",
    "year": 2014,
    "referenceCount": 38,
    "citationCount": 37382,
    "influentialCitationCount": 3282,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2897313",
            "name": "Nitish Srivastava"
        },
        {
            "authorId": "1695689",
            "name": "Geoffrey E. Hinton"
        },
        {
            "authorId": "2064160",
            "name": "A. Krizhevsky"
        },
        {
            "authorId": "1701686",
            "name": "I. Sutskever"
        },
        {
            "authorId": "145124475",
            "name": "R. Salakhutdinov"
        }
    ],
    "references": [
        {
            "paperId": "d124a098cdc6f99b9a152fcf8afa9327dac583be",
            "title": "Dropout Training as Adaptive Regularization"
        },
        {
            "paperId": "ec92efde21707ddf4b81f301cd58e2051c1a2443",
            "title": "Fast dropout training"
        },
        {
            "paperId": "3c20df69865df6a627cc45c524869ccc0297048f",
            "title": "Learning with Marginalized Corrupted Features"
        },
        {
            "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "title": "Maxout Networks"
        },
        {
            "paperId": "0abb49fe138e8fb7332c26b148a48d0db39724fc",
            "title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "8db26a22942404bd435909a16bb3a50cd67b4318",
            "title": "Marginalized Denoising Autoencoders for Domain Adaptation"
        },
        {
            "paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e",
            "title": "Practical Bayesian Optimization of Machine Learning Algorithms"
        },
        {
            "paperId": "9f7f9aba0a6a966ce04e29e401ea28f9eae82f02",
            "title": "Convolutional neural networks applied to house numbers digit classification"
        },
        {
            "paperId": "fc66954e4b7e996b372de650241eba113b2e72f7",
            "title": "Bayesian prediction of tissue-regulated splicing using RNA sequence and cellular context"
        },
        {
            "paperId": "eefcc7bcc05436dac9881acb4ff4e4a0b730e175",
            "title": "High-dimensional signature compression for large-scale image classification"
        },
        {
            "paperId": "90b63e917d5737b06357d50aa729619e933d9614",
            "title": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine"
        },
        {
            "paperId": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
        },
        {
            "paperId": "116b678d7a155845956de2204f6ee5151f8dd98b",
            "title": "Sex, mixability, and modularity"
        },
        {
            "paperId": "1f88427d7aa8225e47f946ac41a0667d7b69ac52",
            "title": "What is the best multi-stage architecture for object recognition?"
        },
        {
            "paperId": "ddc45fad8d15771d9f1f8579331458785b2cdd93",
            "title": "Deep Boltzmann Machines"
        },
        {
            "paperId": "5262fe8369992259be27165ccd09d1d31c7a4def",
            "title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo"
        },
        {
            "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
            "title": "Extracting and composing robust features with denoising autoencoders"
        },
        {
            "paperId": "c0d05cac959f07f924786d68f32f7a3c0d19f22b",
            "title": "Learning to classify with missing and corrupted features"
        },
        {
            "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "title": "A Fast Learning Algorithm for Deep Belief Nets"
        },
        {
            "paperId": "6dd9bb6b38e5b84616e207f00a181dbadce06937",
            "title": "Nightmare at test time: robust learning by feature deletion"
        },
        {
            "paperId": "8423a5782a1acda21a6f68c307ce5376ebef13c7",
            "title": "Rank, Trace-Norm and Max-Norm"
        },
        {
            "paperId": "5562a56da3a96dae82add7de705e2bd841eb00fc",
            "title": "Best practices for convolutional neural networks applied to visual document analysis"
        },
        {
            "paperId": "de75e4e15e22d4376300e5c968e2db44be29ac9e",
            "title": "Simplifying Neural Networks by Soft Weight-Sharing"
        },
        {
            "paperId": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "title": "Backpropagation Applied to Handwritten Zip Code Recognition"
        },
        {
            "paperId": "5d5d4f49d6443c8529a6f5ebef5c499d47a869da",
            "title": "Improving Neural Networks with Dropout"
        },
        {
            "paperId": "d2b62f77cb2864e465aa60bca6c26bb1d2f84963",
            "title": "Acoustic Modeling Using Deep Belief Networks"
        },
        {
            "paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda",
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"
        },
        {
            "paperId": "3a1a2cff2b70fb84a7ca7d97f8adcc5855851795",
            "title": "The Kaldi Speech Recognition Toolkit"
        },
        {
            "paperId": null,
            "title": "Imagenet classification: fast descriptor coding and large-scale svm training. Large scale visual recognition challenge"
        },
        {
            "paperId": "0ea90fac0958d84bcf4a2875c2b169478358b480",
            "title": "CUDAMat: a CUDA-based matrix class for Python"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": "85c1c2d3a7bddfc3f276db479c60b3ec246bdf24",
            "title": "IEEE Workshop on automatic speech recognition and understanding"
        },
        {
            "paperId": "7c59908c946a4157abc030cdbe2b63d08ba97db3",
            "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
        },
        {
            "paperId": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "title": "Regression Shrinkage and Selection via the Lasso"
        },
        {
            "paperId": "a22b36cf5dba3e85eb064220be7ef03be4efba48",
            "title": "Bayesian learning for neural networks"
        },
        {
            "paperId": "1578e606cad01df459162404ed9edfcb17d787aa",
            "title": "The Stability of Inverse Problems"
        },
        {
            "paperId": null,
            "title": "Sutskever and Salakhutdinov References"
        }
    ]
}