{
    "paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0",
    "externalIds": {
        "DBLP": "journals/corr/abs-2204-09656",
        "ArXiv": "2204.09656",
        "DOI": "10.48550/arXiv.2204.09656",
        "CorpusId": 248266822
    },
    "title": "A Fast Post-Training Pruning Framework for Transformers",
    "abstract": "Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining<1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "referenceCount": 106,
    "citationCount": 100,
    "influentialCitationCount": 16,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2204.09656",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a fast post-training pruning framework for Transformers that prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -1.5341415405273438,
            -0.3817022442817688,
            0.4174315333366394,
            4.926752090454102,
            -0.9215023517608643,
            1.3014661073684692,
            2.5809943675994873,
            -3.4793028831481934,
            -0.25154656171798706,
            -1.1370759010314941,
            -3.837979793548584,
            4.537299633026123,
            1.2389293909072876,
            -0.31419891119003296,
            -3.9178600311279297,
            0.1798495501279831,
            0.21398045122623444,
            -0.15767768025398254,
            4.1920294761657715,
            3.23758864402771,
            -1.2779860496520996,
            1.8429840803146362,
            -5.020857810974121,
            3.373213768005371,
            -0.9481903314590454,
            1.1905925273895264,
            1.3345532417297363,
            -2.1942670345306396,
            -3.9621469974517822,
            -0.4069674015045166,
            0.04816544055938721,
            -5.9148335456848145,
            5.606348991394043,
            -1.7038240432739258,
            1.779646635055542,
            -1.6352801322937012,
            -0.8647813200950623,
            7.794706344604492,
            -6.892045497894287,
            -1.4641345739364624,
            -0.9062924981117249,
            0.11971163749694824,
            1.7484323978424072,
            1.0735437870025635,
            1.3988239765167236,
            0.0935317873954773,
            2.784196376800537,
            0.909805953502655,
            -1.4226610660552979,
            2.4516124725341797,
            1.5450137853622437,
            3.5400655269622803,
            0.2993065416812897,
            -1.181980013847351,
            -0.4039207994937897,
            -0.584672212600708,
            2.2386012077331543,
            1.071113109588623,
            3.3313183784484863,
            -1.4024486541748047,
            4.522441864013672,
            2.3960185050964355,
            1.617776870727539,
            1.3679955005645752,
            2.8688621520996094,
            -0.6547577381134033,
            1.0137431621551514,
            4.676249980926514,
            -1.809957504272461,
            -1.8117141723632812,
            1.3489844799041748,
            -4.321297645568848,
            0.969197154045105,
            -1.5173698663711548,
            -3.5543112754821777,
            -1.1284291744232178,
            0.9465855360031128,
            -7.519268035888672,
            1.6105434894561768,
            -3.5283021926879883,
            -0.7769805192947388,
            5.746201038360596,
            -1.8944907188415527,
            2.8578896522521973,
            5.973515510559082,
            -2.3773531913757324,
            -0.6246459484100342,
            -1.0626633167266846,
            1.1773840188980103,
            -2.735724687576294,
            0.6530985832214355,
            2.0867321491241455,
            -1.2532882690429688,
            1.3568028211593628,
            -5.134665012359619,
            -0.03676643967628479,
            -0.053313300013542175,
            0.6838427782058716,
            -0.6428738832473755,
            2.2908241748809814,
            2.148200035095215,
            1.1721524000167847,
            1.049879550933838,
            1.6700352430343628,
            1.986154556274414,
            -1.2194573879241943,
            -2.786710262298584,
            0.9789969325065613,
            -2.014354705810547,
            -4.614898204803467,
            -1.889972448348999,
            2.3137130737304688,
            -2.1886749267578125,
            -2.0311005115509033,
            -3.216425657272339,
            -2.0250377655029297,
            -0.8551998138427734,
            -2.319397211074829,
            -1.516545057296753,
            3.035099506378174,
            1.850750207901001,
            -1.498569130897522,
            -3.8703432083129883,
            -0.9287412166595459,
            -1.0049407482147217,
            4.5072503089904785,
            0.49438872933387756,
            1.8547674417495728,
            -1.121021032333374,
            -5.242044448852539,
            0.7601854801177979,
            -3.0060532093048096,
            1.599257469177246,
            -1.2103842496871948,
            0.7804281711578369,
            2.527038335800171,
            -2.50460147857666,
            3.5095486640930176,
            -4.582035064697266,
            -0.9620537757873535,
            -0.8690619468688965,
            4.849932670593262,
            2.0733723640441895,
            0.09303107857704163,
            2.020923137664795,
            5.740540981292725,
            1.5385000705718994,
            0.9606058597564697,
            -0.5062392950057983,
            4.43063497543335,
            3.3327317237854004,
            -5.888293266296387,
            1.6930954456329346,
            4.9118123054504395,
            3.46034836769104,
            5.858131408691406,
            -5.196658134460449,
            2.419497489929199,
            -0.46719834208488464,
            -0.511826753616333,
            0.4488402307033539,
            1.6275575160980225,
            -7.587462425231934,
            3.1298909187316895,
            7.721440315246582,
            -5.077029228210449,
            -1.567939281463623,
            1.2148115634918213,
            0.43006378412246704,
            -0.18218696117401123,
            2.048839569091797,
            3.5644633769989014,
            3.9037368297576904,
            1.2877764701843262,
            6.608072280883789,
            4.708406925201416,
            2.103869915008545,
            -1.6605944633483887,
            1.9562501907348633,
            -1.2169547080993652,
            -0.8733350038528442,
            0.9033250212669373,
            -4.733830451965332,
            4.150341987609863,
            -3.0034966468811035,
            -2.613748788833618,
            -5.049675941467285,
            -5.0303850173950195,
            -4.146223068237305,
            2.931705951690674,
            0.563174843788147,
            1.691344976425171,
            1.631205677986145,
            2.2320351600646973,
            3.5906381607055664,
            1.246232509613037,
            0.9472774267196655,
            4.011636734008789,
            -3.59419846534729,
            -0.36094141006469727,
            3.229870319366455,
            0.8782472610473633,
            -1.0645110607147217,
            -1.315461277961731,
            4.446211814880371,
            3.436847686767578,
            -2.7569570541381836,
            3.5877685546875,
            -2.158496379852295,
            0.7067652940750122,
            -2.520538806915283,
            -1.117842674255371,
            -1.885082721710205,
            -0.10920816659927368,
            -3.2281441688537598,
            -6.434090614318848,
            -5.274046897888184,
            2.1060564517974854,
            4.252752780914307,
            4.077293872833252,
            0.4866032600402832,
            2.1766676902770996,
            -0.8574756979942322,
            -3.9718217849731445,
            -1.4709665775299072,
            -3.423511028289795,
            1.8503338098526,
            -0.5530091524124146,
            1.050302267074585,
            1.0706779956817627,
            0.8913078308105469,
            -7.5277862548828125,
            -0.8659828305244446,
            -2.112975597381592,
            -4.71547269821167,
            -4.268535137176514,
            -2.125905990600586,
            0.8654333353042603,
            -2.80497407913208,
            1.0988516807556152,
            4.341448783874512,
            0.8274403214454651,
            1.6755001544952393,
            4.253073692321777,
            3.6620819568634033,
            -2.2485270500183105,
            -3.417174816131592,
            2.013636827468872,
            -3.117100954055786,
            -2.5022993087768555,
            -0.850100040435791,
            -2.1236138343811035,
            4.467719554901123,
            -2.798551321029663,
            3.450380802154541,
            3.370490789413452,
            0.36038893461227417,
            -1.2860373258590698,
            3.219475746154785,
            -0.0963371992111206,
            -2.8536102771759033,
            4.651371955871582,
            0.02699778974056244,
            3.3671913146972656,
            -0.22639569640159607,
            -2.3174569606781006,
            -3.9877803325653076,
            -3.3163444995880127,
            -0.7830599546432495,
            1.2519065141677856,
            0.4291107654571533,
            -0.7607635259628296,
            -1.1709283590316772,
            -2.495337724685669,
            -2.5756731033325195,
            -7.863495349884033,
            -2.0650806427001953,
            -2.1063919067382812,
            1.2294102907180786,
            5.047019958496094,
            -0.8204615116119385,
            -4.244133949279785,
            -1.4883707761764526,
            -0.3519672155380249,
            -3.9672114849090576,
            -4.167366981506348,
            -1.646040678024292,
            -2.371873140335083,
            0.1682552546262741,
            -1.0522754192352295,
            -6.739901542663574,
            6.236284255981445,
            -2.167222023010254,
            -0.9010923504829407,
            -1.105844259262085,
            3.6288416385650635,
            3.7278618812561035,
            -0.5644793510437012,
            -0.2447807937860489,
            0.7891311645507812,
            -2.566103458404541,
            0.3332647979259491,
            4.673150062561035,
            0.032341182231903076,
            4.039679050445557,
            3.5003323554992676,
            2.4167990684509277,
            -0.8079735040664673,
            2.5676913261413574,
            -0.17139071226119995,
            -3.6338376998901367,
            1.3668938875198364,
            2.697474479675293,
            -3.4620556831359863,
            1.6926454305648804,
            1.8688490390777588,
            0.8385498523712158,
            -0.48609644174575806,
            -3.459542989730835,
            3.8630025386810303,
            0.02746903896331787,
            1.4511734247207642,
            -1.4151848554611206,
            -5.7429986000061035,
            -2.4413466453552246,
            -1.6866685152053833,
            1.7650374174118042,
            3.7374508380889893,
            -3.7668895721435547,
            2.792294502258301,
            0.9319205284118652,
            0.5615407228469849,
            1.6051547527313232,
            1.031754493713379,
            1.0047016143798828,
            -4.698696136474609,
            1.1872010231018066,
            -1.1643223762512207,
            0.8517234325408936,
            2.720771312713623,
            -1.0958797931671143,
            3.124732494354248,
            -0.18264487385749817,
            3.8846449851989746,
            -2.1074588298797607,
            -0.46254855394363403,
            3.0185189247131348,
            -4.925012111663818,
            1.256062388420105,
            0.4293409585952759,
            -1.1890501976013184,
            -0.23157131671905518,
            1.1625020503997803,
            -1.8382667303085327,
            1.7967243194580078,
            4.406044006347656,
            2.6413111686706543,
            0.30805501341819763,
            0.12375247478485107,
            -0.39249661564826965,
            2.6039586067199707,
            0.8446764349937439,
            -0.45834851264953613,
            -1.3381898403167725,
            0.6806941032409668,
            -4.222285747528076,
            9.482379913330078,
            -0.39571690559387207,
            -0.3334837555885315,
            -3.2131218910217285,
            -2.595667839050293,
            -1.8425344228744507,
            -1.7043365240097046,
            3.9203715324401855,
            -0.09086564183235168,
            -1.226231575012207,
            5.257135391235352,
            -1.0656477212905884,
            0.1071537435054779,
            -0.9928939342498779,
            1.6316871643066406,
            4.393144607543945,
            1.0483274459838867,
            4.965033531188965,
            0.4566316604614258,
            -1.8735618591308594,
            1.4318639039993286,
            -0.7986249327659607,
            2.241157293319702,
            -0.05014351010322571,
            -1.6029826402664185,
            0.7246712446212769,
            -0.9434879422187805,
            4.255157470703125,
            -4.7137532234191895,
            -2.3472280502319336,
            -4.56334114074707,
            -6.87368106842041,
            -0.4487154483795166,
            1.1281375885009766,
            -1.6972404718399048,
            1.7149560451507568,
            4.6040802001953125,
            7.043631076812744,
            -3.2212538719177246,
            -0.7999387979507446,
            5.074161052703857,
            -0.2951299250125885,
            0.0017910599708557129,
            -0.759513795375824,
            -4.213914394378662,
            2.606381416320801,
            -0.8103731870651245,
            -2.723158597946167,
            0.39475584030151367,
            -1.0758090019226074,
            0.8050013780593872,
            3.715754508972168,
            3.143383502960205,
            2.485910415649414,
            -0.019689291715621948,
            3.0996391773223877,
            7.380244255065918,
            1.2876240015029907,
            -1.7269095182418823,
            -3.0466015338897705,
            1.39699387550354,
            0.14277513325214386,
            -1.0078468322753906,
            3.0849223136901855,
            -2.4610471725463867,
            5.246092796325684,
            -2.7034530639648438,
            -0.05670046806335449,
            -0.149147629737854,
            2.3444178104400635,
            0.2990490794181824,
            1.9729137420654297,
            2.5650739669799805,
            1.564123511314392,
            -1.8888264894485474,
            1.112889289855957,
            -0.9788206219673157,
            4.096722602844238,
            0.27805715799331665,
            -0.28537464141845703,
            -0.657499372959137,
            2.481151580810547,
            -1.586599349975586,
            -2.511082172393799,
            3.7336554527282715,
            -5.336082458496094,
            0.19078759849071503,
            0.33044302463531494,
            2.2257063388824463,
            -1.598233699798584,
            -2.7445695400238037,
            -2.347072124481201,
            0.45306864380836487,
            1.6784677505493164,
            -3.544879198074341,
            6.946690559387207,
            0.4530479311943054,
            1.950589895248413,
            2.975527048110962,
            5.026991367340088,
            -0.3508801758289337,
            -0.7485211491584778,
            -2.7894539833068848,
            -1.5674251317977905,
            1.6607637405395508,
            -2.333331823348999,
            -1.1644517183303833,
            -2.2133355140686035,
            -1.0491690635681152,
            -1.2866432666778564,
            5.351147174835205,
            2.1116251945495605,
            3.127776622772217,
            -3.9280202388763428,
            -3.0379815101623535,
            -5.0670576095581055,
            1.6686235666275024,
            1.3174960613250732,
            -2.0513014793395996,
            3.341526985168457,
            5.138139247894287,
            2.6996233463287354,
            4.640819549560547,
            -0.3260517716407776,
            -3.977287530899048,
            2.0378739833831787,
            4.177926063537598,
            -3.2538537979125977,
            2.690476417541504,
            2.2170629501342773,
            -3.017676830291748,
            2.0530076026916504,
            1.2136242389678955,
            0.2448640763759613,
            -0.6602864861488342,
            -6.840928077697754,
            2.160731554031372,
            -2.863337755203247,
            -4.172217845916748,
            2.5954456329345703,
            3.063575267791748,
            2.095205783843994,
            -1.763298511505127,
            -2.290983200073242,
            0.8846668004989624,
            -0.7795284986495972,
            -6.216374397277832,
            1.6814337968826294,
            0.2869623899459839,
            2.3137638568878174,
            1.5317277908325195,
            -1.1560537815093994,
            -0.03486710786819458,
            2.721705198287964,
            -0.7052536606788635,
            -0.04312819242477417,
            -1.5903277397155762,
            -0.5750188231468201,
            0.4996193051338196,
            4.101805210113525,
            -3.7245306968688965,
            5.6682844161987305,
            1.0216524600982666,
            5.206403732299805,
            5.349956512451172,
            3.757880926132202,
            2.7466347217559814,
            -0.16073568165302277,
            -3.32674503326416,
            -0.6648967862129211,
            3.7062532901763916,
            2.6296474933624268,
            -5.400486946105957,
            -0.14984703063964844,
            0.0024718642234802246,
            -0.056708455085754395,
            -1.2479627132415771,
            1.824519395828247,
            -4.550544261932373,
            3.01639461517334,
            0.795375645160675,
            -0.7411372661590576,
            -1.5479501485824585,
            2.870907783508301,
            1.2832270860671997,
            -2.1237387657165527,
            3.3068668842315674,
            -2.6285529136657715,
            -2.6449995040893555,
            -1.8564201593399048,
            -3.317047119140625,
            -1.716974139213562,
            -1.2023210525512695,
            5.267584800720215,
            0.8652653694152832,
            -1.0323936939239502,
            -0.9071487784385681,
            5.1605939865112305,
            -4.586276531219482,
            -1.7110155820846558,
            -4.691103935241699,
            3.2127041816711426,
            1.6053348779678345,
            -2.4067318439483643,
            -0.7270458936691284,
            -0.7341879606246948,
            0.6676235795021057,
            -1.7665324211120605,
            1.0051299333572388,
            1.5662643909454346,
            2.8837225437164307,
            -0.7089105248451233,
            -3.822321891784668,
            -1.7619938850402832,
            -5.062460899353027,
            -1.9067466259002686,
            -4.881679058074951,
            -5.212722301483154,
            0.33484455943107605,
            -6.543911933898926,
            -5.83262825012207,
            2.720165491104126,
            -3.5910258293151855,
            1.4472641944885254,
            -1.849367380142212,
            0.013559222221374512,
            -0.2544872760772705,
            -4.0280914306640625,
            -2.0833849906921387,
            -1.2714076042175293,
            5.352285861968994,
            -1.6814861297607422,
            0.06940028071403503,
            3.114959478378296,
            3.655174732208252,
            4.1329145431518555,
            3.64487624168396,
            -0.12908434867858887,
            -1.4068349599838257,
            -0.5118367671966553,
            0.14807356894016266,
            1.2409334182739258,
            -1.3083428144454956,
            2.171344757080078,
            -0.8577512502670288,
            3.1660144329071045,
            14.502286911010742,
            -0.5400865077972412,
            -2.118143320083618,
            -3.340102434158325,
            0.05170813202857971,
            -5.3737263679504395,
            -0.3554285764694214,
            2.7729504108428955,
            1.017198920249939,
            0.9965770244598389,
            0.6160317063331604,
            -4.810758590698242,
            0.6200699806213379,
            -0.1372992992401123,
            -0.6075074076652527,
            -0.8560516834259033,
            -1.7728261947631836,
            0.10347676277160645,
            0.47769200801849365,
            -1.4937255382537842,
            -1.5573283433914185,
            1.8223521709442139,
            -0.2819478511810303,
            -0.3459285497665405,
            0.16489924490451813,
            3.664280414581299,
            3.3347041606903076,
            6.213587760925293,
            -2.4754905700683594,
            1.0493924617767334,
            2.6584317684173584,
            2.7678115367889404,
            -1.255728840827942,
            3.5632591247558594,
            -3.521824359893799,
            4.324497222900391,
            2.6243982315063477,
            -0.7355326414108276,
            3.329502582550049,
            2.739574909210205,
            -1.1600594520568848,
            0.25288063287734985,
            -2.2229530811309814,
            -0.26380959153175354,
            -0.5621368885040283,
            0.09000999480485916,
            3.5277724266052246,
            -2.8053576946258545,
            -4.52665901184082,
            2.7722396850585938,
            0.5503373742103577,
            -1.788370132446289,
            -2.7275819778442383,
            -0.8456405997276306,
            0.15025095641613007,
            1.7976930141448975,
            0.7174381613731384,
            -3.9506359100341797,
            2.745612859725952,
            2.659010887145996,
            0.12928450107574463,
            0.11034500598907471,
            -1.2260743379592896,
            -2.721303701400757,
            -1.615492820739746,
            0.4728966951370239,
            -2.9752817153930664,
            2.505178689956665,
            2.196484327316284,
            0.8868294954299927,
            2.897853374481201,
            -3.555541753768921,
            -1.0893961191177368,
            -1.0732098817825317,
            0.7294092178344727,
            -2.1341724395751953,
            0.6725854873657227,
            -2.249560594558716,
            -0.4798813462257385,
            5.37992000579834,
            -3.3025317192077637,
            2.4040348529815674,
            -1.159346580505371,
            -3.7511677742004395,
            5.993712902069092,
            -4.96248722076416,
            4.258242607116699,
            2.0600998401641846,
            -1.889289379119873,
            4.446408271789551,
            -1.9567962884902954,
            -1.0469590425491333,
            1.9162869453430176,
            5.361196994781494,
            4.030381679534912,
            -2.900022506713867,
            -0.2755976915359497,
            -3.664696216583252,
            -1.8304800987243652,
            -4.770665645599365,
            6.7077107429504395,
            5.125723361968994,
            0.4058983325958252,
            -2.3721444606781006,
            -3.0406672954559326,
            -0.4252951145172119,
            -1.257026195526123,
            -8.144283294677734,
            -2.8736557960510254,
            -2.763033151626587,
            0.6220303177833557,
            -4.5961737632751465,
            0.3629486560821533,
            -0.20880284905433655,
            1.1801459789276123,
            0.0019866973161697388,
            -0.9625367522239685,
            1.6695963144302368,
            4.242128372192383,
            5.828930854797363,
            0.2124934196472168,
            -0.20763136446475983,
            -1.113668441772461,
            -2.0835487842559814,
            -0.7279921770095825,
            2.3701014518737793,
            -0.8035264015197754,
            -3.801239013671875,
            3.522998809814453,
            -0.11084003746509552,
            1.1679494380950928,
            -4.224601745605469,
            -3.7779035568237305,
            0.2942442297935486,
            -2.71547269821167,
            -5.012665271759033,
            0.7611259818077087,
            3.629532814025879,
            -4.717313766479492,
            5.448563575744629,
            4.320934295654297,
            -2.159125328063965,
            -3.803544521331787,
            6.496832370758057,
            2.2278244495391846,
            0.8624622821807861,
            0.21277078986167908,
            -1.4518468379974365,
            -0.7595970630645752,
            0.3802040219306946,
            0.6564214825630188,
            0.30179670453071594,
            5.533785820007324,
            -4.675349712371826,
            0.5538288950920105,
            -2.5355453491210938
        ]
    },
    "authors": [
        {
            "authorId": "2021195836",
            "name": "Woosuk Kwon"
        },
        {
            "authorId": "2109586102",
            "name": "Sehoon Kim"
        },
        {
            "authorId": "1717098",
            "name": "Michael W. Mahoney"
        },
        {
            "authorId": "1491321888",
            "name": "Joseph Hassoun"
        },
        {
            "authorId": "1732330",
            "name": "K. Keutzer"
        },
        {
            "authorId": "10419477",
            "name": "A. Gholami"
        }
    ],
    "references": [
        {
            "paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5",
            "title": "Structured Pruning Learns Compact and Accurate Models"
        },
        {
            "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77",
            "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"
        },
        {
            "paperId": "cb4f61bf7eb820c9a4bb87547180a4d6fd76e71d",
            "title": "SPDY: Accurate Pruning with Speedup Guarantees"
        },
        {
            "paperId": "416dab850fda842b13a4f28164514d98f836fff7",
            "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing"
        },
        {
            "paperId": "97856fcae0d78f0b614410851c5f13b8a10dc35d",
            "title": "HALP: Hardware-Aware Latency Pruning"
        },
        {
            "paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
            "title": "Primer: Searching for Efficient Transformers for Language Modeling"
        },
        {
            "paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561",
            "title": "Block Pruning For Faster Transformers"
        },
        {
            "paperId": "2dcc6d63e840217306ecdef527ca66706eddc2cb",
            "title": "Group Fisher Pruning for Practical Network Compression"
        },
        {
            "paperId": "ef18db2a18ac61e72783a613328842ce86ef00bf",
            "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"
        },
        {
            "paperId": "4fffa5245d3972077c83614c2a08a47cb578631e",
            "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
        },
        {
            "paperId": "efbe9f591090018f78b42c84613c8afda9292fdb",
            "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"
        },
        {
            "paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9",
            "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"
        },
        {
            "paperId": "75437ae4cf8c8c04d68a9063440f802d211197d9",
            "title": "RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks"
        },
        {
            "paperId": "dd0a27aa2285bc64798fa76944400ab6d9ce3025",
            "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"
        },
        {
            "paperId": "07432dd1f73938a2439dfe74c55bcaabf775aa77",
            "title": "Post-training deep neural network pruning via layer-wise calibration"
        },
        {
            "paperId": "76ef68c7c2410b503e5f1d43ca0c3d6764f72de1",
            "title": "ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques"
        },
        {
            "paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8",
            "title": "I-BERT: Integer-only BERT Quantization"
        },
        {
            "paperId": "0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
            "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72",
            "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"
        },
        {
            "paperId": "96a6ccdb0941691b8a011679f6dac53c5afd47d7",
            "title": "Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning"
        },
        {
            "paperId": "3af8a493cf756f9fe72623204a11e378a9cd71a5",
            "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"
        },
        {
            "paperId": "cae937c13d14a740c6f59c8dc8bd123fef2d4c54",
            "title": "Neuron Merging: Compensating for Pruned Neurons"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "33d735c2f889b5b6a5851f01e741eae535a91127",
            "title": "Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks"
        },
        {
            "paperId": "8fa19377b9cd6d2e9292522774c3a13108cd2ff5",
            "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior"
        },
        {
            "paperId": "48745e3485f84cc5a2dab8e1ce41de0a38afb490",
            "title": "Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning"
        },
        {
            "paperId": "53439309acd147a51555dfcbe797beab652b25c5",
            "title": "Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity"
        },
        {
            "paperId": "389036b1366b64579725457993c1f63a4f3370ba",
            "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"
        },
        {
            "paperId": "49a049dc85e2380dde80501a984878341dd8efdf",
            "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"
        },
        {
            "paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69",
            "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"
        },
        {
            "paperId": "31a68965ea4af87295cb83d8fe72e31f651a4ee1",
            "title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming"
        },
        {
            "paperId": "3b0fb765716ef6861a84abffcbe40643857c613b",
            "title": "Pruning neural networks without any data by iteratively conserving synaptic flow"
        },
        {
            "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
            "title": "Linformer: Self-Attention with Linear Complexity"
        },
        {
            "paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8",
            "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"
        },
        {
            "paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
            "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"
        },
        {
            "paperId": "a81674f480dba239e12c80910528cae5d3a28e97",
            "title": "schuBERT: Optimizing Elements of BERT"
        },
        {
            "paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e",
            "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"
        },
        {
            "paperId": "91ac65431b2dc46919e1673fde67671c29446812",
            "title": "When BERT Plays the Lottery, All Tickets Are Winning"
        },
        {
            "paperId": "90a1491ac32e732c93773354e4e665794ed4d490",
            "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"
        },
        {
            "paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa",
            "title": "Lite Transformer with Long-Short Range Attention"
        },
        {
            "paperId": "0c0dfe47afcec2e229015f3c8f213d4c88e86b28",
            "title": "Up or Down? Adaptive Rounding for Post-Training Quantization"
        },
        {
            "paperId": "39f8cc684f09ea2b43767f5b9590896774802759",
            "title": "On the effect of dropping layers of pre-trained transformer models"
        },
        {
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
        },
        {
            "paperId": "1c332cfa211400fc6f56983fb01a6692046116dd",
            "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"
        },
        {
            "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
            "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
        },
        {
            "paperId": "364a8c7900f17bc97cca72a3172c88a25cd7c6f2",
            "title": "Knapsack Pruning with Inner Distillation"
        },
        {
            "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
            "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963",
            "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"
        },
        {
            "paperId": "54d4ff8d536b292149a4fa017c22349cf4e54ce4",
            "title": "AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search"
        },
        {
            "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "title": "Reformer: The Efficient Transformer"
        },
        {
            "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e",
            "title": "Structured Pruning of Large Language Models"
        },
        {
            "paperId": "c95383f251a62c63217586059c67f63507c3e839",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression"
        },
        {
            "paperId": "3e22c62e752b84ed826992d6b464d3a689f81af1",
            "title": "Performance Aware Convolutional Neural Network Channel Pruning for Embedded GPUs"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "56f66951145444036cb6ec748d90a04ffc487cc1",
            "title": "Data-Independent Neural Pruning via Coresets"
        },
        {
            "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
            "paperId": "a6f4917d043494d2ebaebe6b65cb35e6a07fda41",
            "title": "Importance Estimation for Neural Network Pruning"
        },
        {
            "paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583",
            "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
        },
        {
            "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
            "title": "Are Sixteen Heads Really Better than One?"
        },
        {
            "paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217",
            "title": "The State of Sparsity in Deep Neural Networks"
        },
        {
            "paperId": "bc6dfc6bda2d929fec91042dce1831fd07999b39",
            "title": "Improved Knowledge Distillation via Teacher Assistant"
        },
        {
            "paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
            "title": "The Evolved Transformer"
        },
        {
            "paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235",
            "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"
        },
        {
            "paperId": "f789425a7af1d012675118d7d10cd50afad09074",
            "title": "Post training 4-bit quantization of convolutional networks for rapid-deployment"
        },
        {
            "paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
            "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
        },
        {
            "paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb",
            "title": "Neural Network Acceptability Judgments"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
            "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
        },
        {
            "paperId": "d4d3008262697d379d0cb1642e39a8e0c756ab2c",
            "title": "Faster gaze prediction with dense networks and Fisher pruning"
        },
        {
            "paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096",
            "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"
        },
        {
            "paperId": "ee53c9480132fc0d09b1192226cb2c460462fd6d",
            "title": "Channel Pruning for Accelerating Very Deep Neural Networks"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
        },
        {
            "paperId": "60ae4f18cb53efff0174e3fea7064049737e1e67",
            "title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures"
        },
        {
            "paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "title": "Gaussian Error Linear Units (GELUs)"
        },
        {
            "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
            "paperId": "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d",
            "title": "Data-free Parameter Pruning for Deep Neural Networks"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f",
            "title": "The Winograd Schema Challenge"
        },
        {
            "paperId": "cbde5598c1a78285adfcfd77fb3636f5498987a0",
            "title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning"
        },
        {
            "paperId": "a74b9cc3b08d2a088d6f0b0c037188ccfd1ceaf4",
            "title": "MLPruning: A Multilevel Structured Pruning Framework for Transformer-based Models"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": null,
            "title": "NVIDIA"
        },
        {
            "paperId": "a59da4639436f582e483347a4833e7659fd3e598",
            "title": "CuPy : A NumPy-Compatible Library for NVIDIA GPU Calculations"
        },
        {
            "paperId": null,
            "title": "URL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs"
        },
        {
            "paperId": "6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce",
            "title": "google,\u6211,\u8428\u5a1c"
        },
        {
            "paperId": "475354f10798f110d34792b6d88f31d6d5cb099e",
            "title": "Automatically Constructing a Corpus of Sentential Paraphrases"
        },
        {
            "paperId": "e808f28d411a958c5db81ceb111beb2638698f47",
            "title": "The PASCAL Recognising Textual Entailment Challenge"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        },
        {
            "paperId": null,
            "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"
        },
        {
            "paperId": null,
            "title": "If you used crowdsourcing or conducted research with human subjects."
        },
        {
            "paperId": null,
            "title": "Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating?"
        },
        {
            "paperId": null,
            "title": "c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?"
        }
    ]
}