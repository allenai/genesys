{
    "paperId": "952af139e6a49c5b6490663be967d312c438334d",
    "externalIds": {
        "MAG": "2889260178",
        "DBLP": "journals/corr/abs-1808-10000",
        "ACL": "D18-1544",
        "DOI": "10.18653/v1/D18-1544",
        "CorpusId": 52132013
    },
    "title": "Grammar Induction with Neural Language Models: An Unusual Replication",
    "abstract": "A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "referenceCount": 20,
    "citationCount": 38,
    "influentialCitationCount": 6,
    "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1544.pdf",
        "status": "HYBRID"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "41022736",
            "name": "Phu Mon Htut"
        },
        {
            "authorId": "1979489",
            "name": "Kyunghyun Cho"
        },
        {
            "authorId": "3644767",
            "name": "Samuel R. Bowman"
        }
    ],
    "references": [
        {
            "paperId": "f1cbf097ce436f7304a1984f4a29ab41f75ebfe3",
            "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon"
        },
        {
            "paperId": "8f46c21fef31a4cdf7b1808e67171466a9317882",
            "title": "Do latent tree learning models identify meaningful structure in sentences?"
        },
        {
            "paperId": "027f9695189355d18ec6be8e48f3d23ea25db35d",
            "title": "Learning to Compose Task-Specific Tree Structures"
        },
        {
            "paperId": "3096b9e5b17dedbee9554fbd1d6e20f7a095e48a",
            "title": "Jointly learning sentence embeddings and syntax with unsupervised Tree-LSTMs"
        },
        {
            "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
        },
        {
            "paperId": "13d9323a8716131911bfda048a40e2cde1a76a46",
            "title": "Structured Attention Networks"
        },
        {
            "paperId": "599f7863721d542dcef2da49b41d82b21e4f80b3",
            "title": "Learning to Compose Words into Sentences with Reinforcement Learning"
        },
        {
            "paperId": "3aa52436575cf6768a0a1a476601825f6a62e58f",
            "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"
        },
        {
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference"
        },
        {
            "paperId": "9c0ddf74f87d154db88d79c640578c1610451eec",
            "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"
        },
        {
            "paperId": "42a9c575acb53fac332993087c1e1dbcc8161ccd",
            "title": "An All-Subtrees Approach to Unsupervised Parsing"
        },
        {
            "paperId": "3b98dc29eb5f95470d12d19ae528674129ca0411",
            "title": "Natural language grammar induction with a generative constituent-context model"
        },
        {
            "paperId": "41828fc3dab24784f95e6976e8aaa73f68e1840e",
            "title": "Incremental Parsing with the Perceptron Algorithm"
        },
        {
            "paperId": "77021fb48704b860fa850dd103b79db4dcf920ee",
            "title": "A Generative Constituent-Context Model for Improved Grammar Induction"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "eb34c9981c50bde33d165a7f5faeb72018aa4d09",
            "title": "Two Experiments on Learning Probabilistic Dependency Grammars from Corpora"
        },
        {
            "paperId": "8f4dafaa68ea4bc702e1f13cdb932e566236b0c0",
            "title": "Guiding Unsupervised Grammar Induction Using Contrastive Estimation"
        },
        {
            "paperId": null,
            "title": "Learning task-dependent distributed representations by backpropagation through structure"
        },
        {
            "paperId": null,
            "title": "Bowman . 2018 a . Do latent tree learning models identify meaningful structure in sentences ? Transactions of the Association for Computational Linguistics ( TACL )"
        }
    ]
}