{
    "paperId": "4b308ba40e67b0b4b25c6fde17195d5a456a2f41",
    "externalIds": {
        "DBLP": "journals/corr/abs-2212-14034",
        "ArXiv": "2212.14034",
        "DOI": "10.48550/arXiv.2212.14034",
        "CorpusId": 255185900
    },
    "title": "Cramming: Training a Language Model on a Single GPU in One Day",
    "abstract": "Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "referenceCount": 145,
    "citationCount": 66,
    "influentialCitationCount": 10,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2212.14034",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work investigates the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU, and investigates why scaling down is hard, and which modifications actually improve performance in this scenario."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -4.355143070220947,
            -1.9571781158447266,
            -1.413809061050415,
            6.090556621551514,
            -0.49938029050827026,
            1.3437011241912842,
            3.113576650619507,
            -0.03763002157211304,
            -1.7046656608581543,
            -1.34060800075531,
            -1.8245528936386108,
            2.561563730239868,
            1.2733039855957031,
            0.14203983545303345,
            -3.7470574378967285,
            0.1844281256198883,
            1.9847092628479004,
            -1.369333267211914,
            8.157483100891113,
            4.649463176727295,
            -2.607393741607666,
            1.0524778366088867,
            -1.1020419597625732,
            5.235586166381836,
            -1.6981043815612793,
            0.027495358139276505,
            0.29233551025390625,
            1.513641119003296,
            -1.4069416522979736,
            -0.24673518538475037,
            0.6797875761985779,
            -5.469675540924072,
            6.028170585632324,
            -6.462730884552002,
            3.600311756134033,
            -1.9474155902862549,
            -3.798675298690796,
            5.59868860244751,
            -5.553077697753906,
            -2.5499167442321777,
            -2.246447801589966,
            -0.1450737714767456,
            -1.1681634187698364,
            0.8300465941429138,
            1.1489496231079102,
            0.291073739528656,
            1.8905425071716309,
            -0.048521459102630615,
            2.6831014156341553,
            4.579006195068359,
            1.9753482341766357,
            1.0305269956588745,
            0.6701600551605225,
            -0.2521887719631195,
            -2.7532944679260254,
            0.35939618945121765,
            1.4565317630767822,
            -1.1323521137237549,
            2.7932796478271484,
            -2.8137476444244385,
            2.3150124549865723,
            5.289698123931885,
            0.2939780652523041,
            2.7257559299468994,
            5.591836929321289,
            -2.4381706714630127,
            -1.927408218383789,
            2.3425285816192627,
            0.8607494831085205,
            2.7204363346099854,
            -2.0502877235412598,
            -4.601750373840332,
            1.5870872735977173,
            -0.22177600860595703,
            -3.507016658782959,
            1.1162645816802979,
            -1.0001085996627808,
            -7.3195977210998535,
            -1.0186432600021362,
            -3.154855966567993,
            -1.3109936714172363,
            4.007624626159668,
            0.5216015577316284,
            2.813908576965332,
            2.041048288345337,
            -1.9606451988220215,
            -3.9763121604919434,
            -1.1686732769012451,
            1.859748363494873,
            -2.1352241039276123,
            1.0964953899383545,
            -0.922317385673523,
            -0.5732569694519043,
            -0.20067930221557617,
            -4.574194431304932,
            -0.15713909268379211,
            -1.0912986993789673,
            -0.5442517399787903,
            0.050766319036483765,
            -0.19684594869613647,
            3.292447566986084,
            1.4935187101364136,
            2.7398808002471924,
            -2.560084581375122,
            3.407515287399292,
            -4.203963756561279,
            0.1954769492149353,
            4.130218505859375,
            -3.166403293609619,
            -0.8662329912185669,
            -2.2366912364959717,
            4.223782539367676,
            -1.7160139083862305,
            -0.44515737891197205,
            -1.5385979413986206,
            -2.394972324371338,
            -0.13370424509048462,
            -1.1482839584350586,
            -3.6916418075561523,
            1.9906208515167236,
            -0.23680266737937927,
            -1.0473803281784058,
            -2.3467540740966797,
            1.5325188636779785,
            2.9015793800354004,
            4.229686737060547,
            -0.9295467138290405,
            -0.1709912121295929,
            -1.9861805438995361,
            -4.4121413230896,
            1.9668500423431396,
            -2.7929391860961914,
            3.0679354667663574,
            -2.741527795791626,
            0.6491204500198364,
            1.0240963697433472,
            -1.430281400680542,
            -0.829306960105896,
            -1.20228910446167,
            -1.331811547279358,
            3.8137409687042236,
            5.5486955642700195,
            -0.6520833969116211,
            0.9547805786132812,
            0.9058756828308105,
            2.5063395500183105,
            2.245208501815796,
            0.5177874565124512,
            -1.2898967266082764,
            2.5391087532043457,
            3.591610908508301,
            -3.968900680541992,
            -0.2592224180698395,
            2.5264599323272705,
            0.7577083110809326,
            2.9056966304779053,
            -2.2387964725494385,
            0.30482155084609985,
            -1.789046287536621,
            0.03245347738265991,
            2.420100450515747,
            -1.5942199230194092,
            -9.77295970916748,
            -0.0002471804618835449,
            3.972433567047119,
            -4.963711738586426,
            -1.2877343893051147,
            2.0743215084075928,
            1.3228859901428223,
            1.777788519859314,
            -4.142974853515625,
            4.0135178565979,
            2.28385591506958,
            1.5614876747131348,
            1.9070533514022827,
            1.742323398590088,
            4.132607936859131,
            -0.3360922634601593,
            -0.1334000825881958,
            -0.7655181884765625,
            -0.03977757692337036,
            1.3992702960968018,
            -4.5599164962768555,
            1.9029357433319092,
            -4.846621513366699,
            -2.5329391956329346,
            -3.7345142364501953,
            -0.5618600845336914,
            1.5275609493255615,
            1.075544834136963,
            -0.636833906173706,
            -2.4446072578430176,
            5.83345890045166,
            5.909761905670166,
            6.142199516296387,
            -0.4551132321357727,
            2.2065210342407227,
            2.503194570541382,
            -0.11757439374923706,
            0.4043203294277191,
            1.7123727798461914,
            -1.0786852836608887,
            -0.6922515034675598,
            1.7018543481826782,
            3.268117904663086,
            1.7897799015045166,
            -3.6946420669555664,
            2.174696445465088,
            3.879265785217285,
            -0.051212966442108154,
            0.1285565197467804,
            -1.5554924011230469,
            -3.2204878330230713,
            2.5791192054748535,
            -2.830444812774658,
            -3.588610887527466,
            -5.229814052581787,
            1.4448107481002808,
            6.392869472503662,
            1.7172231674194336,
            -3.1904025077819824,
            -0.730196475982666,
            2.2806475162506104,
            -2.111058235168457,
            1.8874738216400146,
            0.4912363290786743,
            3.1233909130096436,
            3.9427781105041504,
            2.0104215145111084,
            -1.445528507232666,
            -2.347472667694092,
            -3.7013063430786133,
            -0.9468991160392761,
            -1.4195839166641235,
            -4.783781051635742,
            -0.2094166874885559,
            -3.6357812881469727,
            -2.5925240516662598,
            1.3626124858856201,
            0.9336743354797363,
            4.797457695007324,
            -0.8563482165336609,
            -1.1435731649398804,
            7.176039218902588,
            2.9830515384674072,
            -1.98240065574646,
            1.1489206552505493,
            -0.138380765914917,
            -1.0146427154541016,
            -0.34990888833999634,
            0.4428975582122803,
            -4.486000061035156,
            1.8048319816589355,
            -0.21461647748947144,
            2.5093777179718018,
            3.477295160293579,
            1.7501988410949707,
            -1.4072864055633545,
            1.5105904340744019,
            -0.5669541358947754,
            -4.374687194824219,
            9.016338348388672,
            1.619666337966919,
            7.140064239501953,
            -0.0009823739528656006,
            -1.594203233718872,
            -3.1894612312316895,
            1.5967097282409668,
            1.254164457321167,
            3.450451374053955,
            5.314615726470947,
            2.408412456512451,
            -1.3408342599868774,
            -5.093937397003174,
            -0.11781251430511475,
            -3.9250285625457764,
            -1.901613473892212,
            -1.5522549152374268,
            1.466302752494812,
            4.100801944732666,
            2.85304856300354,
            -5.838385581970215,
            -1.2912226915359497,
            -2.580501079559326,
            -1.8366224765777588,
            -0.10934585332870483,
            -0.8491750955581665,
            -0.46635183691978455,
            -1.841947317123413,
            -2.7958874702453613,
            -1.0150245428085327,
            2.735692024230957,
            -0.5670871138572693,
            -0.4343116879463196,
            -1.4566969871520996,
            1.7872474193572998,
            3.5680110454559326,
            -0.603572428226471,
            -2.2178380489349365,
            1.9483935832977295,
            0.6869103312492371,
            2.774587631225586,
            4.191182613372803,
            0.2703554332256317,
            0.07514292001724243,
            2.7789931297302246,
            -2.4693002700805664,
            -4.035775184631348,
            -0.038279175758361816,
            -2.7669124603271484,
            -3.415766477584839,
            1.9638898372650146,
            1.659227967262268,
            -4.601672172546387,
            -0.6949278712272644,
            0.9510347247123718,
            2.3187875747680664,
            -2.597076654434204,
            -1.0268791913986206,
            1.9028263092041016,
            -0.5959391593933105,
            -1.997044563293457,
            -5.32452392578125,
            -3.2747864723205566,
            -2.8024234771728516,
            -0.5922266840934753,
            4.863674640655518,
            1.5247316360473633,
            -1.7709604501724243,
            3.6030192375183105,
            3.082125663757324,
            3.6840600967407227,
            2.514193534851074,
            4.588772773742676,
            -0.029631823301315308,
            -3.0943057537078857,
            -0.23427164554595947,
            -1.0435659885406494,
            -0.9386219382286072,
            -0.43122634291648865,
            -0.34048160910606384,
            5.070363998413086,
            0.7411572933197021,
            1.0558035373687744,
            -0.6049315929412842,
            -0.009560786187648773,
            1.4533131122589111,
            0.3388519883155823,
            0.627307653427124,
            0.6907474994659424,
            0.7256879806518555,
            -0.02676624059677124,
            4.355493068695068,
            0.11397502571344376,
            2.8390297889709473,
            2.050969362258911,
            3.719606399536133,
            0.3075275421142578,
            -0.19968309998512268,
            0.503959596157074,
            -1.1211285591125488,
            1.636099100112915,
            4.131944179534912,
            0.27729105949401855,
            0.2685662508010864,
            -1.9266411066055298,
            11.348395347595215,
            -2.000913143157959,
            1.2132858037948608,
            -2.813436985015869,
            -1.0423893928527832,
            -6.30497932434082,
            -0.0036441683769226074,
            2.734793186187744,
            -2.1682944297790527,
            -1.5411434173583984,
            -0.3558884263038635,
            -5.605927467346191,
            -1.2401963472366333,
            -1.5403488874435425,
            -0.7012788653373718,
            8.18171501159668,
            -1.5066437721252441,
            3.9547119140625,
            0.30268216133117676,
            1.3094829320907593,
            -0.5481932759284973,
            1.953001618385315,
            3.516226291656494,
            0.694818377494812,
            -5.473540306091309,
            1.9796642065048218,
            -2.5549135208129883,
            2.8475754261016846,
            -4.915822982788086,
            -2.893050193786621,
            -3.9331953525543213,
            -2.4585471153259277,
            0.7533719539642334,
            3.3097193241119385,
            3.309668779373169,
            -0.5316492915153503,
            2.1839191913604736,
            3.4186806678771973,
            -6.267860412597656,
            1.5984869003295898,
            1.6249926090240479,
            1.7698705196380615,
            -1.9461252689361572,
            0.5867478251457214,
            -2.6545867919921875,
            -2.660857915878296,
            -0.6144561171531677,
            -4.278142929077148,
            -1.570846438407898,
            -2.469210147857666,
            0.7432726621627808,
            1.0675954818725586,
            1.835935354232788,
            2.053687810897827,
            -1.3072502613067627,
            3.913750410079956,
            4.733478546142578,
            5.629777908325195,
            -2.297945022583008,
            2.686452627182007,
            4.174460411071777,
            1.399641752243042,
            -0.26335814595222473,
            5.337698936462402,
            0.4413851201534271,
            3.9504518508911133,
            -2.9068820476531982,
            -0.0540635883808136,
            -3.417569637298584,
            0.5712311863899231,
            -0.45538920164108276,
            -0.57032710313797,
            0.694040060043335,
            -0.977855920791626,
            -1.0114610195159912,
            5.475494384765625,
            -2.7322540283203125,
            5.513309955596924,
            0.3508569896221161,
            4.091453552246094,
            -0.4943820536136627,
            -0.4070049822330475,
            -4.130649566650391,
            -0.5744692087173462,
            2.609264373779297,
            -7.638954162597656,
            -0.4975632429122925,
            -0.689547598361969,
            3.5556740760803223,
            1.6290878057479858,
            -1.5162383317947388,
            -1.311022400856018,
            -0.38356882333755493,
            -1.0517593622207642,
            -1.3322443962097168,
            4.1378068923950195,
            -1.534417986869812,
            0.06618782877922058,
            1.0258007049560547,
            -0.3550766706466675,
            0.037362486124038696,
            -4.156573295593262,
            1.650252103805542,
            0.5642178654670715,
            1.0973199605941772,
            -2.7796473503112793,
            -1.623122215270996,
            -1.6260426044464111,
            -2.60880184173584,
            -1.5901098251342773,
            3.1435599327087402,
            -1.1573611497879028,
            -0.7284927368164062,
            -7.141217231750488,
            -1.6729981899261475,
            2.59022855758667,
            0.5429370999336243,
            -0.35284823179244995,
            -1.5036181211471558,
            6.101137161254883,
            3.2412424087524414,
            0.2161865085363388,
            1.4319676160812378,
            1.532538890838623,
            1.783901333808899,
            -1.7853713035583496,
            3.445924758911133,
            -2.866112470626831,
            1.878342866897583,
            2.363072633743286,
            -3.9939815998077393,
            2.2758047580718994,
            -0.7980159521102905,
            0.5898762345314026,
            -5.693194389343262,
            -3.2833595275878906,
            -3.691220283508301,
            -4.825055122375488,
            -5.0132036209106445,
            5.04580020904541,
            4.810616493225098,
            -1.6003704071044922,
            0.3675990402698517,
            -1.3495543003082275,
            0.21763189136981964,
            2.0133273601531982,
            -5.292548179626465,
            2.4357659816741943,
            0.05150938034057617,
            3.994983673095703,
            2.4546990394592285,
            -0.6209255456924438,
            -0.022217780351638794,
            1.3227087259292603,
            -3.9065630435943604,
            -0.14286161959171295,
            -0.5885425806045532,
            -1.9395978450775146,
            0.8180471658706665,
            2.6871120929718018,
            -1.04168701171875,
            1.9263429641723633,
            1.7509238719940186,
            4.770421028137207,
            5.222756862640381,
            5.718515396118164,
            0.9792747497558594,
            -3.0217764377593994,
            0.11965502798557281,
            -1.186211109161377,
            2.338749647140503,
            1.9142286777496338,
            -4.23411226272583,
            0.48357316851615906,
            -1.290021300315857,
            3.634507179260254,
            1.7039813995361328,
            2.696763515472412,
            -2.467566967010498,
            4.40512228012085,
            -2.448073148727417,
            1.580647587776184,
            -1.5967868566513062,
            2.982093572616577,
            0.47991928458213806,
            -0.6576071977615356,
            0.5826566219329834,
            -2.4969348907470703,
            -3.123465061187744,
            -4.16550350189209,
            -2.9131691455841064,
            1.4803403615951538,
            -2.3991525173187256,
            1.7937595844268799,
            2.8329238891601562,
            -0.0677245706319809,
            0.39671751856803894,
            2.80448055267334,
            -1.37633216381073,
            -2.306504964828491,
            -1.190914511680603,
            3.6713366508483887,
            1.9223456382751465,
            -3.4645400047302246,
            0.20707876980304718,
            -0.5864309072494507,
            0.5029740929603577,
            0.369576096534729,
            1.6713271141052246,
            0.5535752177238464,
            5.313630104064941,
            0.5925154685974121,
            -0.22695261240005493,
            -2.2119555473327637,
            -2.064059019088745,
            -0.25734466314315796,
            1.098585605621338,
            -3.116323471069336,
            0.20685561001300812,
            -0.4029184579849243,
            -2.4574692249298096,
            4.0603532791137695,
            -3.69743275642395,
            -0.9057700634002686,
            -0.02974611520767212,
            -2.4202022552490234,
            -0.3901950716972351,
            -3.34700345993042,
            0.056066155433654785,
            -3.546813488006592,
            3.796598196029663,
            1.691498041152954,
            1.7799943685531616,
            1.2164629697799683,
            0.16529062390327454,
            5.636568069458008,
            4.268887042999268,
            -0.3809886574745178,
            -0.025916531682014465,
            -3.093626022338867,
            2.242213487625122,
            -0.15090163052082062,
            -1.509187936782837,
            -1.6649560928344727,
            -2.041529655456543,
            -0.8779434561729431,
            15.09231185913086,
            -0.24701857566833496,
            -2.1479530334472656,
            -4.318667411804199,
            -1.207404375076294,
            -3.456200361251831,
            -1.5013935565948486,
            3.449963331222534,
            2.910642385482788,
            1.243337631225586,
            0.07428997755050659,
            -2.9716033935546875,
            2.537710428237915,
            1.5340638160705566,
            -4.606686592102051,
            0.20626860857009888,
            -1.9677414894104004,
            2.443711757659912,
            -1.2999169826507568,
            -0.3071633279323578,
            -2.1847095489501953,
            2.2563679218292236,
            -2.331089973449707,
            -1.3745145797729492,
            -2.5566623210906982,
            2.966562032699585,
            5.941420555114746,
            3.645799160003662,
            -1.900971531867981,
            -1.13461434841156,
            2.136404514312744,
            1.943983554840088,
            0.23832353949546814,
            2.664674758911133,
            -1.902443528175354,
            4.672225475311279,
            2.7764501571655273,
            0.6528510451316833,
            0.7512432932853699,
            4.328725814819336,
            -2.0365242958068848,
            1.2857155799865723,
            -2.2949609756469727,
            -1.925756812095642,
            -3.4179904460906982,
            1.27604341506958,
            0.9000704884529114,
            -2.628047227859497,
            -1.4176865816116333,
            2.026414394378662,
            -0.6490181088447571,
            -3.196200370788574,
            -1.6493510007858276,
            -1.3110113143920898,
            1.8441884517669678,
            1.2462834119796753,
            0.4861709177494049,
            -4.505124568939209,
            1.651463508605957,
            -2.9787707328796387,
            5.030454158782959,
            -1.3931820392608643,
            -3.704540729522705,
            -6.445952892303467,
            -1.7363446950912476,
            2.44679594039917,
            -2.3224010467529297,
            2.1091246604919434,
            2.1655187606811523,
            3.9987223148345947,
            2.402036428451538,
            -0.0014620423316955566,
            0.10981301963329315,
            -2.367938995361328,
            0.7595092058181763,
            -0.9509248733520508,
            1.3590812683105469,
            -1.3364615440368652,
            -0.8235408067703247,
            7.452322959899902,
            -1.5302867889404297,
            3.238959550857544,
            -1.5569833517074585,
            -3.2796523571014404,
            3.096177101135254,
            -3.4329211711883545,
            4.145570755004883,
            0.18737471103668213,
            -1.6124823093414307,
            0.9644178152084351,
            -0.21121001243591309,
            -0.487992525100708,
            2.488210678100586,
            -0.23791220784187317,
            2.8394646644592285,
            -5.082178115844727,
            -3.7868504524230957,
            -4.927920341491699,
            -3.309034585952759,
            -4.567075729370117,
            5.5664143562316895,
            3.8354711532592773,
            2.575226306915283,
            0.20941197872161865,
            -0.7834190130233765,
            -1.187911033630371,
            -2.0811798572540283,
            -6.381701469421387,
            -1.7901908159255981,
            -0.9215646386146545,
            2.5221080780029297,
            -5.2840986251831055,
            -1.928246259689331,
            3.3588523864746094,
            0.607087254524231,
            -2.5163121223449707,
            1.0200504064559937,
            -1.6145261526107788,
            0.4152127504348755,
            3.807180404663086,
            2.4749388694763184,
            0.045166015625,
            -3.4608240127563477,
            -3.562753200531006,
            -2.067857265472412,
            3.8742659091949463,
            -1.3705943822860718,
            0.4937381148338318,
            -2.6383659839630127,
            0.4079628586769104,
            -1.055536150932312,
            -3.820621967315674,
            -4.568921089172363,
            0.9102657437324524,
            -2.2137017250061035,
            -4.445255279541016,
            2.0296943187713623,
            -1.1781022548675537,
            -1.4014955759048462,
            -0.48276442289352417,
            4.571608543395996,
            -5.429107666015625,
            0.12047713994979858,
            9.306655883789062,
            -0.963777482509613,
            -0.9194689393043518,
            -2.8364369869232178,
            -0.2557942569255829,
            1.6806302070617676,
            0.7441949248313904,
            -1.4077057838439941,
            -2.57246994972229,
            2.829679012298584,
            3.196272850036621,
            -1.5681957006454468,
            -1.3073056936264038
        ]
    },
    "authors": [
        {
            "authorId": "8284185",
            "name": "Jonas Geiping"
        },
        {
            "authorId": "1962083",
            "name": "T. Goldstein"
        }
    ],
    "references": [
        {
            "paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6",
            "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"
        },
        {
            "paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f",
            "title": "What Language Model to Train if You Have One Million GPU Hours?"
        },
        {
            "paperId": "fd7e88a2313e176315d99fc299277e752d7703b7",
            "title": "Efficient Methods for Natural Language Processing: A Survey"
        },
        {
            "paperId": "ba9a592438447c2a35afc203be40f7f0a2d3fb5a",
            "title": "A Compact Pretraining Approach for Neural Language Models"
        },
        {
            "paperId": "6edccbd83a9aae204785d4821f97855677c33866",
            "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"
        },
        {
            "paperId": "6ac1fccf1e04487d439ee598f51c03ddac5144ca",
            "title": "N-Grammer: Augmenting Transformers with latent n-grams"
        },
        {
            "paperId": "6a8db14262ca2017cb253e12b8daeb57989a38df",
            "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"
        },
        {
            "paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
        },
        {
            "paperId": "8ce9b1e527c4d9d15239621ec4e3ef3fbbe32202",
            "title": "On the Role of Bidirectionality in Language Model Pre-Training"
        },
        {
            "paperId": "31a28bbf4ffcf5dbb94be4759801428310695cfa",
            "title": "Simple Recurrence Improves Masked Language Models"
        },
        {
            "paperId": "aa4d9972af3264d032dbee58501ed4ac49477103",
            "title": "Scaling Laws and Interpretability of Learning from Repeated Data"
        },
        {
            "paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
            "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"
        },
        {
            "paperId": "b6ec1e8f18185b4b3d46201359a440404575460c",
            "title": "METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals"
        },
        {
            "paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "434f4ecbfdea4496bbcd763427fc605bf11abddc",
            "title": "Token Dropping for Efficient BERT Pretraining"
        },
        {
            "paperId": "1098ca3dbda5778c2bf6c9e8cbb9bc7a02249e10",
            "title": "Staged Training for Transformer Language Models"
        },
        {
            "paperId": "2a359ba342ec0126d64acec8beb2ce1c5546e0db",
            "title": "TrimBERT: Tailoring BERT for Trade-offs"
        },
        {
            "paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c",
            "title": "Transformer Quality in Linear Time"
        },
        {
            "paperId": "9b3fa3a80afdb6d34984307e457656420e60e7e7",
            "title": "Should You Mask 15% in Masked Language Modeling?"
        },
        {
            "paperId": "b77cb53e9a0d9fbe77b4b6d0982ed6fc9ad6f1a8",
            "title": "pNLP-Mixer: an Efficient all-MLP Architecture for Language"
        },
        {
            "paperId": "37ddb9305c8c9120c21a2fae5a851ce8e4384a9c",
            "title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture"
        },
        {
            "paperId": "c2536182c010c41941e8a031071a1880c34cec60",
            "title": "Unified Scaling Laws for Routed Language Models"
        },
        {
            "paperId": "2a7a6648563e6a09e6fea6dd96e68e0563216dcb",
            "title": "Datamodels: Predicting Predictions from Training Data"
        },
        {
            "paperId": "68f141724814839d556a989646194be88641b143",
            "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
        },
        {
            "paperId": "54d7ae7cfee56b0e19fd42c45d365f760a41794d",
            "title": "Membership Inference Attacks From First Principles"
        },
        {
            "paperId": "972706306f85b1bfb40c7d35c796ad5174eb0c9c",
            "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"
        },
        {
            "paperId": "231e768f0cd280faa0f725bb353262cb4fed08d1",
            "title": "Hierarchical Transformers Are More Efficient Language Models"
        },
        {
            "paperId": "66d735987a31d666a6459566ae026c40ab9a1c3a",
            "title": "The Efficiency Misnomer"
        },
        {
            "paperId": "2582a04918f6fe62dc142f2fca9ca0bb0b1d7895",
            "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization"
        },
        {
            "paperId": "7f2dd0a66a9e6570fc6123f0aab193084c1268fc",
            "title": "Sharpness-Aware Minimization Improves Language Model Generalization"
        },
        {
            "paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
            "title": "Primer: Searching for Efficient Transformers for Language Modeling"
        },
        {
            "paperId": "87314e4d9a853ae030cafea20050dbcaa6aa2ade",
            "title": "Metro"
        },
        {
            "paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5",
            "title": "Deduplicating Training Data Makes Language Models Better"
        },
        {
            "paperId": "5b540745f4b51f95bf90fb3420e51edb037fc51a",
            "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis"
        },
        {
            "paperId": "f0993e68c76d940763ef7f8106db86cdc97b3a49",
            "title": "Multi-head or Single-head? An Empirical Comparison for Transformer Training"
        },
        {
            "paperId": "9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73",
            "title": "GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures"
        },
        {
            "paperId": "9a0ce4d9b12337c1c4b65e56cab5b87dcc5f5aa2",
            "title": "XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation"
        },
        {
            "paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f",
            "title": "FNet: Mixing Tokens with Fourier Transforms"
        },
        {
            "paperId": "d13a0c8d49cb268d8d245925baee0316c1fe1875",
            "title": "Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"
        },
        {
            "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
        },
        {
            "paperId": "0672f88d5dc762002b515ca4a0a9f101017fea35",
            "title": "Probing Across Time: What Does RoBERTa Know and When?"
        },
        {
            "paperId": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9",
            "title": "How to Train BERT with an Academic Budget"
        },
        {
            "paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
            "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"
        },
        {
            "paperId": "51f46cb42668cfe3745ecf029d032bf30253574f",
            "title": "GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training"
        },
        {
            "paperId": "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b",
            "title": "Explaining neural scaling laws"
        },
        {
            "paperId": "54b75720b82f2af779e11910985faa9eb343a345",
            "title": "Distilling Large Language Models into Tiny and Effective Students using pQRNN"
        },
        {
            "paperId": "12b71736392209b4292471b7da0aed71ba2aa545",
            "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"
        },
        {
            "paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
        },
        {
            "paperId": "7e9ff94476f41041c75e253e84f487db00e9c861",
            "title": "Long Range Arena: A Benchmark for Efficient Transformers"
        },
        {
            "paperId": "9436806f98d0c71245135d5d45025427bb36cd33",
            "title": "Optimizing Transformer for Low-Resource Neural Machine Translation"
        },
        {
            "paperId": "00b677e971ded11ac4a7da1b80ffda95b4f1ed78",
            "title": "Pre-Training Transformers as Energy-Based Cloze Models"
        },
        {
            "paperId": "2310d893abf4ec900cb9e0c5da58284a37329780",
            "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"
        },
        {
            "paperId": "bc87279d4b32a425377ff18ab63f7ecf95ff228c",
            "title": "Rethinking embedding coupling in pre-trained language models"
        },
        {
            "paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39",
            "title": "On the Transformer Growth for Progressive BERT Training"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
            "title": "Efficient Transformers: A Survey"
        },
        {
            "paperId": "8dc712493df0a46fef830a6c3be64899880100c4",
            "title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching"
        },
        {
            "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
            "title": "Big Bird: Transformers for Longer Sequences"
        },
        {
            "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
            "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
        },
        {
            "paperId": "8256f48f759cf85044db251cc512f965834945b3",
            "title": "Rethinking Positional Encoding in Language Pre-training"
        },
        {
            "paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69",
            "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"
        },
        {
            "paperId": "bcc48c5f68a387c89c75cd80d52ef52284db3c3a",
            "title": "Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks"
        },
        {
            "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
            "title": "Linformer: Self-Attention with Linear Complexity"
        },
        {
            "paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09",
            "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"
        },
        {
            "paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5",
            "title": "The hardware lottery"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "71a72da632d55b4a9e12ca0b9e35bafe0466e318",
            "title": "Normalized Attention Without Probability Cage"
        },
        {
            "paperId": "8d908042f139575d6688c745e94156c9df6eae07",
            "title": "Understanding the Difficulty of Training Transformers"
        },
        {
            "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
            "title": "Longformer: The Long-Document Transformer"
        },
        {
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
        },
        {
            "paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e",
            "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"
        },
        {
            "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
            "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
        },
        {
            "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
            "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a",
            "title": "On Layer Normalization in the Transformer Architecture"
        },
        {
            "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
            "title": "Scaling Laws for Neural Language Models"
        },
        {
            "paperId": "8f18c9da3d1763723c6ef8c3734d74db005d0cff",
            "title": "Fantastic Generalization Measures and Where to Find Them"
        },
        {
            "paperId": "b9ed2fd3237539b0ad539dad8bdee15efbe0a26e",
            "title": "Single Headed Attention RNN: Stop Thinking With Your Head"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd",
            "title": "Root Mean Square Layer Normalization"
        },
        {
            "paperId": "c95383f251a62c63217586059c67f63507c3e839",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97",
            "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression"
        },
        {
            "paperId": "2bf7c350a8280e7c593d46a60127f99b21517121",
            "title": "On the Variance of the Adaptive Learning Rate and Beyond"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc",
            "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"
        },
        {
            "paperId": "92343cecdc990380de362b969eec60081959f507",
            "title": "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"
        },
        {
            "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
            "paperId": "572b6fe8d75032c159896c644319dacdf594e9c6",
            "title": "One Epoch Is All You Need"
        },
        {
            "paperId": "f8de25118af2abc4c48afb947d6ec298e05ef1e5",
            "title": "When Does Label Smoothing Help?"
        },
        {
            "paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13",
            "title": "Adaptive Attention Span in Transformers"
        },
        {
            "paperId": "d9f6ada77448664b71128bb19df15765336974a6",
            "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
        },
        {
            "paperId": "bc789aef715498e79a74f857fa090ece9e383bf1",
            "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"
        },
        {
            "paperId": "d205595e3d1fa4342f29c9517f3b56fffe785d06",
            "title": "Generalization in Deep Networks: The Role of Distance from Initialization"
        },
        {
            "paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
            "title": "Adaptive Input Representations for Neural Language Modeling"
        },
        {
            "paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd",
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"
        },
        {
            "paperId": "b7658f3a180b067e5dd6b218063355c064bd5873",
            "title": "How Machines Learn: Where do Companies get Data for Machine Learning and What Licenses do They Need?"
        },
        {
            "paperId": "966dd274c35c84b1f83e758eb051363e01686562",
            "title": "Robots Welcome? Ethical and Legal Considerations for Web Crawling and Scraping"
        },
        {
            "paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb",
            "title": "Neural Network Acceptability Judgments"
        },
        {
            "paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e",
            "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
            "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
        },
        {
            "paperId": "1269e191091eadeed6f246cf5a6692b178bb4d94",
            "title": "Super-convergence: very fast training of neural networks using large learning rates"
        },
        {
            "paperId": "4e8d226bf69d78ee11cafc216486428102fa90cf",
            "title": "A Progressive Batching L-BFGS Method for Machine Learning"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "4edde76bc68ed0e6aa814d3b4d29eb168274a52d",
            "title": "Adaptive Sampling Strategies for Stochastic Optimization"
        },
        {
            "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
            "title": "Automatic differentiation in PyTorch"
        },
        {
            "paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
            "title": "Mixed Precision Training"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "efcdc46fb412775e0f80d9291f177439f664ebea",
            "title": "Big Batch SGD: Automated Inference using Adaptive Batch Sizes"
        },
        {
            "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
            "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
        },
        {
            "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
            "title": "Neural Machine Translation of Rare Words with Subword Units"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": null,
            "title": "Other Modifications A few recent developments not included in this study are Roy et al"
        },
        {
            "paperId": "070ff40f38675cfa42a104a545a47584ad823e70",
            "title": "Scale Efficiently: Insights from Pretraining and Finetuning Transformers"
        },
        {
            "paperId": "290ef971d936b9b727dc36dbf4f38be6c8d915c5",
            "title": "LiteTransformerSearch: Training-free On-device Search for Efficient Autoregressive Language Models"
        },
        {
            "paperId": "338d0501947b5fc7d92d09eed9a3e299f7b48ec1",
            "title": "Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency"
        },
        {
            "paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6",
            "title": "Unifying Language Learning Paradigms"
        },
        {
            "paperId": "921c1216edbf6b2931b15874f24847ff1007ad8c",
            "title": "EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks"
        },
        {
            "paperId": "177d986fb4944d4b681a988c290f25785cf7a86d",
            "title": "Addressing \"Documentation Debt\" in Machine Learning: A Retrospective Datasheet for BookCorpus"
        },
        {
            "paperId": "d931f84abfc4550c10ceb113b142c8eb3e07571e",
            "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training"
        },
        {
            "paperId": null,
            "title": "Scalable Second Order Optimization for Deep Learning"
        },
        {
            "paperId": null,
            "title": "RWKV-LM"
        },
        {
            "paperId": "f4da4103344ab0f52165f0ec65e4b92395d89381",
            "title": "Making L-BFGS Work with Industrial-Strength Nets"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "hyperparameter-tuning and feature engineering"
        },
        {
            "paperId": null,
            "title": "Modifications further not included in this study are more involved initialization"
        },
        {
            "paperId": null,
            "title": "What the bookcorpus?"
        },
        {
            "paperId": null,
            "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
        },
        {
            "paperId": null,
            "title": "The #BenderRule: On Naming the Languages We Study and Why It Matters, September 2019. URL https://thegradient.pub/the-benderrul e-on-naming-the-languages-we-study-a nd-why-it-matters"
        },
        {
            "paperId": null,
            "title": "TPUs vs GPUs for Transformers (BERT), October 2018. URL https://timdettmers.com/ 2018/10/17/tpus-vs-gpus-for-transform ers-bert"
        },
        {
            "paperId": "a9075f6332542e12b2bf3cdbdb3a6ed44733fb41",
            "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"
        },
        {
            "paperId": "7bea855e19fd13461590e4f2d44bbf7b807ce3e3",
            "title": "A bitter lesson."
        },
        {
            "paperId": "3afee8eaf219c21479a1c4b7c460cf96743eb1ef",
            "title": "Easy to be hard."
        },
        {
            "paperId": null,
            "title": "RWKV: Reinventing RNNs"
        },
        {
            "paperId": null,
            "title": "find 275 TFLOP/s in bfloat16 precision for the TPUv4 and 123 TFLOP/s for the TPUv3, each per chip. The V100 peak performance is given as 125"
        },
        {
            "paperId": null,
            "title": "All total exaFLOP numbers are then computed based on these TFLOP/s numbers over the training time period described in each work"
        },
        {
            "paperId": null,
            "title": "Introducing nvFuser, a deep learning compiler for PyTorch,"
        },
        {
            "paperId": null,
            "title": "BFloat16: The secret to high performance on Cloud TPUs"
        },
        {
            "paperId": null,
            "title": "NVIDIA Clocks World\u2019s Fastest BERT Training Time and Largest Transformer Based Model, Paving Path For Advanced Conversational AI"
        }
    ]
}