{
    "paperId": "7a12bf1df8ffaaf6d44e01d2d62124389f15f59c",
    "externalIds": {
        "MAG": "2134342468",
        "DOI": "10.1109/IJCNN.2004.1380039",
        "CorpusId": 9448547
    },
    "title": "Backpropagation-decorrelation: online recurrent learning with O(N) complexity",
    "abstract": "We introduce a new learning rule for fully recurrent neural networks which we call backpropagation-decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.",
    "venue": "2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)",
    "year": 2004,
    "referenceCount": 25,
    "citationCount": 224,
    "influentialCitationCount": 13,
    "openAccessPdf": {
        "url": "http://ni.www.techfak.uni-bielefeld.de/files/Steil2004-BDO.pdf",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new learning rule for fully recurrent neural networks is introduced which combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "27551792",
            "name": "Jochen J. Steil"
        }
    ],
    "references": [
        {
            "paperId": "063a2333de21e82518a1788d2ce71184f1a8c625",
            "title": "Local stability of recurrent networks with time-varying weights and inputs"
        },
        {
            "paperId": "64ece6c406dfca58b5eacb772ecb71eead741a20",
            "title": "Attractive Periodic Sets in Discrete-Time Recurrent Networks (with Emphasis on Fixed-Point Stability and Bifurcations in Two-Neuron Networks)"
        },
        {
            "paperId": "573dfd36d62d4619196888e27beb946b3747716b",
            "title": "Independent Component Analysis"
        },
        {
            "paperId": "f1bbf82caf10cd6b44415850a87c413eab060b64",
            "title": "Natural Gradient Learning for Spatio-Temporal Decorrelation: Recurrent Network"
        },
        {
            "paperId": "6bdabcdcde21d4d71321935e2e0332e32eda5366",
            "title": "New results on recurrent network training: unifying the algorithms and accelerating convergence"
        },
        {
            "paperId": "47f584c32a24d39cddd9c8a2b8c025a7be899459",
            "title": "A conjugate gradient learning algorithm for recurrent neural networks"
        },
        {
            "paperId": "e9fac1091d9a1646314b1b91e58f40dae3a750cd",
            "title": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "32e97eef94beacace020e79322cef0e1e5a76ee0",
            "title": "Gradient calculations for dynamic recurrent neural networks: a survey"
        },
        {
            "paperId": "89b9a181801f32bf62c4237c4265ba036a79f9dc",
            "title": "A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks"
        },
        {
            "paperId": "2ae5a5507253aa3cada113d41d35fada1e84555f",
            "title": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
        },
        {
            "paperId": "4d605e91c1fa0f3649acd5cab7fdcdbfb6b195cf",
            "title": "Analyzing the weight dynamics of recurrent learning algorithms"
        },
        {
            "paperId": "dd7a25a065236155254ae1765993619f731d4110",
            "title": "On the weight dynamics of recurrent learning"
        },
        {
            "paperId": "fd211b2db0b074bf5b7d969f65afd34eafb0a44a",
            "title": "Analysis and Comparison of Algorithms for Training Recurrent Neural Networks"
        },
        {
            "paperId": "24547f720f472dd92870c1a7c4cb8bb450307f27",
            "title": "Adaptive Nonlinear System Identification with Echo State Networks"
        },
        {
            "paperId": "b7f5aae17195e869187843c74c904f38f7aa4c79",
            "title": "The \"Liquid Computer\": A Novel Strategy for Real-Time Computing on Time Series"
        },
        {
            "paperId": "c79a9db085e67b1c3673e2e6c4d4446cea206753",
            "title": "Local structural stability of recurrent networks with time-varying weights"
        },
        {
            "paperId": "196bbc123ab4716a983e4e89ab2abeb76216175a",
            "title": "Perspectives on learning with recurrent neural networks"
        },
        {
            "paperId": "bcc9ccf6b7dfa515ae7b25568cde54ba2ffe65ab",
            "title": "Natural Gradient Learning for Spatio-temporal Decorrelation : Recurrent Network"
        },
        {
            "paperId": "19252db2811d47a4a0d88223cb49965253d7a5dc",
            "title": "A learning rule for dynamic recruitment and decorrelation"
        },
        {
            "paperId": "ebc058234e0dcb3f69ec39a36c93176443779c2c",
            "title": "Recurrent learning of input-output stable behaviour in function space: A case study with the Roessler attractor"
        },
        {
            "paperId": "af15535b865217f9112f83831042fe54e331c4f1",
            "title": "Recurrent Neural Networks: Design and Applications"
        },
        {
            "paperId": "0a3f94df30fbc48c98097312e5e270b5a76ca1dd",
            "title": "Recurrent neural networks: design and applications"
        },
        {
            "paperId": "10dae7fca6b65b61d155a622f0c6ca2bc3922251",
            "title": "Gradient-based learning algorithms for recurrent networks and their computational complexity"
        },
        {
            "paperId": null,
            "title": "I61 ACKNOWLEDGMENT Cl71 Special thanks to H. Ritter and U. Schiller for very valuable discussions and to U. Schiller for the APRL online-learning implementation according to"
        }
    ]
}