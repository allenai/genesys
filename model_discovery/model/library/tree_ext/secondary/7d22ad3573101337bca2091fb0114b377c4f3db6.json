{
    "paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6",
    "externalIds": {
        "DBLP": "conf/iclr/Sun0BK24",
        "ArXiv": "2306.11695",
        "DOI": "10.48550/arXiv.2306.11695",
        "CorpusId": 259203115
    },
    "title": "A Simple and Effective Pruning Approach for Large Language Models",
    "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 107,
    "citationCount": 160,
    "influentialCitationCount": 42,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2306.11695",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs, which significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.9906744956970215,
            0.69575035572052,
            -1.8104928731918335,
            1.283279538154602,
            0.14893454313278198,
            0.7213845252990723,
            2.4441874027252197,
            -0.5125935673713684,
            -3.402014970779419,
            2.9416663646698,
            -4.000823497772217,
            5.167971611022949,
            -1.8138844966888428,
            -2.8621232509613037,
            -4.650193691253662,
            1.0997779369354248,
            0.8758387565612793,
            -3.612231492996216,
            5.841108322143555,
            4.577768325805664,
            -0.5781126022338867,
            2.7212443351745605,
            -3.125898838043213,
            -0.6568686366081238,
            -2.595586061477661,
            0.49919819831848145,
            1.9226880073547363,
            -1.2607622146606445,
            -2.612978935241699,
            0.6892531514167786,
            -2.9969892501831055,
            -4.8238043785095215,
            5.8524980545043945,
            -3.2923648357391357,
            0.9830557107925415,
            -3.113727331161499,
            -0.9044620990753174,
            7.064481735229492,
            -3.4183902740478516,
            -1.60078763961792,
            -0.4980441927909851,
            -1.7571492195129395,
            0.9751744270324707,
            2.501707077026367,
            2.6736278533935547,
            2.593980550765991,
            0.6587235331535339,
            2.0499026775360107,
            -0.6941186189651489,
            1.0724552869796753,
            2.1041464805603027,
            1.5813696384429932,
            2.1325812339782715,
            2.6065683364868164,
            0.769088625907898,
            0.5741465091705322,
            0.5968576669692993,
            1.6034573316574097,
            2.3899431228637695,
            -4.6832756996154785,
            4.8426103591918945,
            4.022762298583984,
            -1.655899167060852,
            0.44675973057746887,
            2.25827956199646,
            -1.7679308652877808,
            -3.246793746948242,
            4.792098522186279,
            -0.6117080450057983,
            -2.6710619926452637,
            -1.7909629344940186,
            -6.163609981536865,
            1.3001132011413574,
            0.7028520107269287,
            -2.949810028076172,
            -0.14890927076339722,
            0.8118484616279602,
            -7.948258399963379,
            -0.46564722061157227,
            -0.6875391006469727,
            -0.7035788893699646,
            0.7723663449287415,
            0.33506837487220764,
            2.3670835494995117,
            2.290835380554199,
            -2.3170855045318604,
            -3.102088689804077,
            1.701737880706787,
            3.563793420791626,
            -2.1382057666778564,
            -1.0479036569595337,
            1.0063616037368774,
            0.14376798272132874,
            4.220678329467773,
            -3.1221296787261963,
            -0.276553750038147,
            0.970491886138916,
            0.8009853959083557,
            -3.5180821418762207,
            -1.5711098909378052,
            4.758487701416016,
            0.7093302011489868,
            0.5381650924682617,
            1.977365493774414,
            4.996364116668701,
            -6.518087387084961,
            0.11730979382991791,
            1.5281336307525635,
            -1.5779824256896973,
            -3.573482036590576,
            -0.9300186634063721,
            1.8508594036102295,
            -0.5626248121261597,
            -0.35197141766548157,
            -1.0444391965866089,
            -4.389244556427002,
            -1.2653695344924927,
            -0.04846835136413574,
            -2.6161415576934814,
            5.117552757263184,
            -2.6425797939300537,
            -2.1820790767669678,
            -2.622504234313965,
            0.38291433453559875,
            3.1947362422943115,
            1.1319313049316406,
            -0.4060891568660736,
            -2.5108118057250977,
            -0.8015148639678955,
            -3.1913609504699707,
            1.858899474143982,
            -1.245487928390503,
            2.304583787918091,
            -1.9052600860595703,
            2.8753645420074463,
            2.405054807662964,
            -6.791302680969238,
            1.6383682489395142,
            -2.255496025085449,
            -2.5431928634643555,
            2.2654261589050293,
            6.024738788604736,
            1.730525255203247,
            -0.5864638090133667,
            -1.0932306051254272,
            6.877125263214111,
            0.8509910106658936,
            0.8281375169754028,
            -0.5973016023635864,
            6.3621110916137695,
            5.269617080688477,
            -4.527352333068848,
            0.5790786743164062,
            2.5008180141448975,
            -0.4330761432647705,
            5.351891994476318,
            -4.441283226013184,
            3.2711617946624756,
            -0.37041348218917847,
            1.5186145305633545,
            -1.1575673818588257,
            0.7885317206382751,
            -11.218412399291992,
            -0.26956847310066223,
            5.337832450866699,
            -4.520179748535156,
            -1.5282636880874634,
            1.589863896369934,
            0.7770286202430725,
            2.5009493827819824,
            -1.4290378093719482,
            2.6090054512023926,
            0.7405927777290344,
            3.507019281387329,
            -0.006688356399536133,
            4.743988990783691,
            3.11623215675354,
            -1.0680090188980103,
            -1.526632308959961,
            0.48092201352119446,
            -0.9395887851715088,
            0.3065761625766754,
            -5.875339984893799,
            -2.0219032764434814,
            -4.7054853439331055,
            -3.4057211875915527,
            -4.063291549682617,
            -3.2093424797058105,
            -0.3392333388328552,
            0.051723748445510864,
            -2.0527517795562744,
            0.26854372024536133,
            5.676688194274902,
            6.599711894989014,
            3.0848474502563477,
            1.2428836822509766,
            4.337287902832031,
            2.904365062713623,
            -4.112963676452637,
            0.4593433141708374,
            2.289997100830078,
            2.3200910091400146,
            -1.5255768299102783,
            -1.015932559967041,
            3.224127769470215,
            1.0637805461883545,
            -2.658247470855713,
            3.819788932800293,
            2.1909403800964355,
            -0.6994134187698364,
            0.0984254777431488,
            -2.222752094268799,
            -3.9748196601867676,
            0.7107303142547607,
            -2.9854342937469482,
            -2.9569993019104004,
            -4.052530288696289,
            1.219311237335205,
            6.038525581359863,
            1.3754141330718994,
            -0.8981512188911438,
            -3.985781669616699,
            0.48990002274513245,
            -3.1334614753723145,
            2.0438151359558105,
            -2.0508675575256348,
            -0.42956241965293884,
            -1.1107337474822998,
            -0.009479433298110962,
            -0.4786040782928467,
            -1.4199836254119873,
            -8.312891006469727,
            -1.7637975215911865,
            -0.6190727353096008,
            -4.583459377288818,
            0.04176601767539978,
            -4.262370586395264,
            3.1549384593963623,
            -1.1973799467086792,
            0.4330459535121918,
            1.964547872543335,
            1.2229410409927368,
            1.8014211654663086,
            6.111113548278809,
            2.346576690673828,
            -1.577235221862793,
            -3.9547152519226074,
            2.361374616622925,
            0.6151368021965027,
            -0.4088302552700043,
            -0.6514725685119629,
            -0.9599546194076538,
            2.848484992980957,
            0.4149361550807953,
            3.737056016921997,
            1.95992910861969,
            0.38408052921295166,
            -0.26788899302482605,
            -0.5303208827972412,
            -0.4895751178264618,
            0.06898024678230286,
            6.26829195022583,
            -0.4489215314388275,
            3.773696184158325,
            -1.3873540163040161,
            -2.103259563446045,
            -7.099307060241699,
            -2.0709643363952637,
            -0.35761910676956177,
            2.0697953701019287,
            4.321234226226807,
            -0.14943155646324158,
            -0.13202549517154694,
            -3.3931572437286377,
            -1.0182310342788696,
            -7.019834041595459,
            -1.412488579750061,
            -0.7312674522399902,
            2.369286060333252,
            2.7583138942718506,
            1.0502310991287231,
            -2.61917781829834,
            -1.5638576745986938,
            -0.9724566340446472,
            -4.21491003036499,
            -4.756309986114502,
            -0.6891277432441711,
            -2.031519889831543,
            -0.020424097776412964,
            -3.4806439876556396,
            -3.3337042331695557,
            5.8198370933532715,
            -2.112487316131592,
            -3.415135622024536,
            -2.7177929878234863,
            0.8301892280578613,
            4.82427453994751,
            1.2649568319320679,
            -0.2164021134376526,
            1.020041823387146,
            -0.005764514207839966,
            5.006479263305664,
            3.0702810287475586,
            -0.3055346608161926,
            3.318849802017212,
            2.3385517597198486,
            1.8911538124084473,
            -0.7471959590911865,
            2.7488813400268555,
            -4.764824867248535,
            -0.7965433597564697,
            0.7789005041122437,
            2.586697578430176,
            -1.827833652496338,
            1.3794646263122559,
            1.5163929462432861,
            3.6266255378723145,
            0.20265844464302063,
            -3.277406930923462,
            2.3736350536346436,
            0.7387731075286865,
            2.778822898864746,
            -4.34691047668457,
            -2.077986240386963,
            -4.460934162139893,
            0.17876553535461426,
            5.260868549346924,
            3.2899060249328613,
            -3.4138052463531494,
            2.8240668773651123,
            -1.5247902870178223,
            5.32895040512085,
            2.8810503482818604,
            1.4076950550079346,
            -3.0132906436920166,
            -5.201545715332031,
            -1.4671016931533813,
            -3.815321922302246,
            3.3962242603302,
            2.1946558952331543,
            0.9141631126403809,
            6.4148383140563965,
            0.7536584138870239,
            0.4990624189376831,
            -2.9902191162109375,
            0.09996217489242554,
            1.2221572399139404,
            -3.2841248512268066,
            0.13052384555339813,
            1.5665793418884277,
            1.6169497966766357,
            2.727048873901367,
            3.0540387630462646,
            -2.277200937271118,
            1.997799277305603,
            4.468592166900635,
            0.6862837672233582,
            -0.3353573679924011,
            0.9144104719161987,
            1.5158522129058838,
            0.7685823440551758,
            0.23899275064468384,
            3.7656121253967285,
            0.7543237209320068,
            2.3356945514678955,
            -2.3909778594970703,
            11.340335845947266,
            -1.964141607284546,
            1.7172462940216064,
            -5.545350074768066,
            -3.272589683532715,
            -0.6992034912109375,
            -2.3698174953460693,
            3.8501689434051514,
            -2.6560158729553223,
            -2.8593530654907227,
            1.5396008491516113,
            -6.265812397003174,
            1.6041135787963867,
            1.3573886156082153,
            -0.32231420278549194,
            1.4018042087554932,
            -3.9541430473327637,
            2.1690239906311035,
            -0.48199743032455444,
            1.3312715291976929,
            1.0258326530456543,
            0.4399751126766205,
            -0.2326960563659668,
            0.056315064430236816,
            -0.6007257699966431,
            1.6427797079086304,
            -0.3792690634727478,
            5.0711164474487305,
            -4.994171619415283,
            -1.643687129020691,
            -4.555879592895508,
            -3.0427181720733643,
            -2.155963897705078,
            2.922497510910034,
            -1.9844948053359985,
            -1.507361888885498,
            6.298145771026611,
            2.4929397106170654,
            -2.991757392883301,
            -0.3770598769187927,
            4.071648597717285,
            0.16427084803581238,
            -3.047926902770996,
            0.5496049523353577,
            -3.6697487831115723,
            -1.419229507446289,
            -3.3298816680908203,
            -5.514795780181885,
            -0.13083064556121826,
            0.2015274167060852,
            2.885761022567749,
            5.0037994384765625,
            2.7607297897338867,
            2.9378786087036133,
            -0.3391385078430176,
            2.6064724922180176,
            6.323920249938965,
            1.329501986503601,
            1.4968180656433105,
            0.2693188786506653,
            1.2896909713745117,
            2.3520309925079346,
            -2.539536714553833,
            1.210015058517456,
            -0.759214460849762,
            4.220468521118164,
            -3.367943525314331,
            -0.6916488409042358,
            0.2503625452518463,
            3.6989760398864746,
            1.2061024904251099,
            1.17825186252594,
            2.7208075523376465,
            -0.32021069526672363,
            -0.6629413366317749,
            1.907407283782959,
            -1.435739278793335,
            3.261227607727051,
            3.63128924369812,
            2.7686452865600586,
            3.0269014835357666,
            1.122065782546997,
            -2.5069727897644043,
            -3.721994400024414,
            4.8173017501831055,
            -2.406949996948242,
            -1.471560001373291,
            -1.5959912538528442,
            -0.09602159261703491,
            0.14617294073104858,
            -0.5829863548278809,
            0.46487855911254883,
            0.8087152242660522,
            -0.3617847263813019,
            -4.810925483703613,
            4.982251167297363,
            -0.6503074169158936,
            -0.040160536766052246,
            0.24458788335323334,
            -1.1000494956970215,
            -3.2998414039611816,
            -1.1230146884918213,
            -1.942865252494812,
            0.353610098361969,
            -0.26117536425590515,
            -0.501469075679779,
            -1.9994441270828247,
            1.826096534729004,
            0.5501713752746582,
            1.0132970809936523,
            4.2451629638671875,
            2.8501572608947754,
            4.050404071807861,
            -3.0214571952819824,
            -2.3658242225646973,
            -2.398829698562622,
            3.8573296070098877,
            -0.8531337976455688,
            -2.5057942867279053,
            5.416521072387695,
            2.423677921295166,
            0.9349499344825745,
            5.06523323059082,
            -0.26568853855133057,
            0.8113372325897217,
            2.5846245288848877,
            1.6909644603729248,
            0.05292731523513794,
            1.729002594947815,
            -0.7220543622970581,
            -5.9308247566223145,
            1.5979282855987549,
            3.7820191383361816,
            -0.7811152935028076,
            2.3220598697662354,
            -6.48306941986084,
            -0.3247409760951996,
            -0.38919633626937866,
            -2.763535499572754,
            6.175912857055664,
            3.2598090171813965,
            -1.6779226064682007,
            -1.7677198648452759,
            0.28800398111343384,
            -0.5393675565719604,
            0.06992554664611816,
            -5.475902557373047,
            1.5644309520721436,
            0.29492461681365967,
            0.002700112760066986,
            4.198549747467041,
            -1.0747493505477905,
            -1.9479868412017822,
            1.797271490097046,
            -2.8533871173858643,
            -0.12916241586208344,
            -1.6360881328582764,
            1.0242938995361328,
            1.330087661743164,
            3.14414119720459,
            -2.2196850776672363,
            0.5369870662689209,
            0.6256827712059021,
            5.871124267578125,
            4.407485485076904,
            4.016486167907715,
            2.472980499267578,
            -1.9740030765533447,
            -1.6140297651290894,
            -2.8637914657592773,
            1.5445173978805542,
            2.278465747833252,
            -5.756124496459961,
            -2.0102529525756836,
            -1.5150891542434692,
            -0.3376237154006958,
            -1.8497830629348755,
            3.7772767543792725,
            -1.7615816593170166,
            4.2836995124816895,
            -4.737490177154541,
            0.060564249753952026,
            -0.7527192831039429,
            3.7224044799804688,
            2.030066728591919,
            -0.0885830968618393,
            3.3816189765930176,
            0.9976959228515625,
            -2.0583763122558594,
            -1.5080972909927368,
            -1.3963613510131836,
            0.4167748689651489,
            -1.3358709812164307,
            1.758698582649231,
            2.7885141372680664,
            0.29512643814086914,
            3.0199942588806152,
            3.4594812393188477,
            -0.7097539901733398,
            -0.10884667187929153,
            -6.65407133102417,
            3.8986940383911133,
            0.9623770117759705,
            -1.857650876045227,
            -0.5341672897338867,
            -2.2056617736816406,
            3.766559362411499,
            0.5583078861236572,
            3.351231813430786,
            -0.14530545473098755,
            1.9818999767303467,
            1.8385119438171387,
            -0.35390186309814453,
            -2.940075397491455,
            -4.570253372192383,
            -1.0242931842803955,
            -0.411698579788208,
            -5.51161527633667,
            -0.4638942778110504,
            -2.527573585510254,
            -1.1835119724273682,
            0.788358747959137,
            -3.604409694671631,
            0.4858309030532837,
            2.516355037689209,
            -2.4673473834991455,
            -0.5854786038398743,
            -2.857786178588867,
            1.4407453536987305,
            -2.447704315185547,
            4.390631675720215,
            -0.12516498565673828,
            -0.1747075915336609,
            3.598095417022705,
            1.1254608631134033,
            5.135643482208252,
            1.3062756061553955,
            2.7545547485351562,
            -2.939605236053467,
            -0.8142488598823547,
            2.594708204269409,
            -1.0806065797805786,
            -1.564263105392456,
            -0.652743399143219,
            0.44668033719062805,
            4.937928199768066,
            16.03817367553711,
            -0.3988648056983948,
            -0.6505072116851807,
            -0.05604051053524017,
            -1.9628335237503052,
            -3.5131759643554688,
            -2.5098979473114014,
            3.018477439880371,
            0.042213499546051025,
            0.08337220549583435,
            -0.1963483691215515,
            -3.010254144668579,
            -0.48613470792770386,
            2.0057339668273926,
            -3.5031185150146484,
            -2.370652675628662,
            -3.5193159580230713,
            0.9600035548210144,
            -1.0945204496383667,
            -0.1942073404788971,
            -0.8494481444358826,
            1.2530698776245117,
            -0.2452980875968933,
            -3.129598379135132,
            1.8759344816207886,
            3.496610403060913,
            3.0566601753234863,
            3.130605936050415,
            -1.9259233474731445,
            -0.9948699474334717,
            -0.43995100259780884,
            2.4927046298980713,
            -1.7673859596252441,
            1.5479212999343872,
            -1.8548290729522705,
            3.3287136554718018,
            2.7298026084899902,
            -2.5063834190368652,
            1.2902226448059082,
            -0.8656706809997559,
            -2.7358150482177734,
            0.8749999403953552,
            -3.480135440826416,
            1.641012191772461,
            -4.383027076721191,
            -0.9542465806007385,
            0.4881829619407654,
            -2.112236976623535,
            -3.2907001972198486,
            3.3842525482177734,
            -1.224224328994751,
            -0.3929294943809509,
            -0.755239725112915,
            1.2408967018127441,
            -0.09456861019134521,
            -0.7761211395263672,
            -1.2267704010009766,
            -1.6207411289215088,
            1.7967488765716553,
            -3.7485604286193848,
            1.3936219215393066,
            0.6597861647605896,
            -2.382826566696167,
            -2.989933967590332,
            -0.8777908682823181,
            -0.9857955574989319,
            -3.407142162322998,
            3.1984174251556396,
            4.491030693054199,
            0.9447935819625854,
            4.793837547302246,
            2.722839832305908,
            -0.0981246829032898,
            -3.5835466384887695,
            1.0180773735046387,
            -4.120720386505127,
            1.3134465217590332,
            -2.4757113456726074,
            -1.731794834136963,
            7.011015892028809,
            -2.3669331073760986,
            0.8347465991973877,
            -2.5454280376434326,
            -3.9759984016418457,
            3.6840009689331055,
            -2.9523801803588867,
            4.558666229248047,
            1.5469924211502075,
            -0.23329567909240723,
            2.7061707973480225,
            -3.1466000080108643,
            -1.8541226387023926,
            5.413815498352051,
            3.9881772994995117,
            3.1018786430358887,
            -3.156749725341797,
            -0.638382077217102,
            -1.691597819328308,
            -3.2135887145996094,
            -3.3351194858551025,
            6.658180236816406,
            4.933958053588867,
            0.633100152015686,
            -2.5305163860321045,
            0.04881942272186279,
            -0.5335777401924133,
            -0.029791995882987976,
            -8.914190292358398,
            -3.373769760131836,
            -2.0529181957244873,
            3.8747732639312744,
            -5.076853275299072,
            0.33677297830581665,
            -1.0538877248764038,
            2.197578191757202,
            -0.7179528474807739,
            1.946506381034851,
            0.1386493444442749,
            2.5909476280212402,
            2.7640085220336914,
            0.7960228323936462,
            -1.5311424732208252,
            -1.1923232078552246,
            -2.689814805984497,
            -2.8140101432800293,
            -0.331972599029541,
            -0.7476616501808167,
            -5.22317361831665,
            0.48141753673553467,
            -1.1445558071136475,
            1.4293208122253418,
            -0.6309959292411804,
            -0.4112071692943573,
            0.809049665927887,
            0.980345606803894,
            -2.5259430408477783,
            0.6638263463973999,
            0.7462974786758423,
            -1.4930331707000732,
            4.3518290519714355,
            2.163361072540283,
            -3.3903613090515137,
            -4.616519927978516,
            8.511842727661133,
            -2.006180763244629,
            -2.242668867111206,
            -1.5943714380264282,
            0.8416105508804321,
            -1.8430430889129639,
            0.7225093841552734,
            1.973465919494629,
            1.721727967262268,
            2.664940357208252,
            1.4619286060333252,
            -0.47778815031051636,
            -1.5198932886123657
        ]
    },
    "authors": [
        {
            "authorId": "2984183",
            "name": "Mingjie Sun"
        },
        {
            "authorId": "2109168016",
            "name": "Zhuang Liu"
        },
        {
            "authorId": "25901845",
            "name": "Anna Bair"
        },
        {
            "authorId": "145116464",
            "name": "J. Z. Kolter"
        }
    ],
    "references": [
        {
            "paperId": "95240dda409e28acccdc5cf619ad0c036cf4292d",
            "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
        },
        {
            "paperId": "9b98ca94b7733feee1cff5c57596611ad35fa7aa",
            "title": "Scaling Laws for Sparsely-Connected Foundation Models"
        },
        {
            "paperId": "110804428354df709b3693f9efc81946a9036ebf",
            "title": "Neurons in Large Language Models: Dead, N-gram, Positional"
        },
        {
            "paperId": "d315ca681e95b73f2a6a6115d1e218dec9720d6f",
            "title": "QuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive Algorithm"
        },
        {
            "paperId": "d2a42864605a502325a874bc470481ca1904ea0a",
            "title": "Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df",
            "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"
        },
        {
            "paperId": "17dfa45f14fcc1b861bc06b7f0b4678d870628d9",
            "title": "Intriguing Properties of Quantization at Scale"
        },
        {
            "paperId": "ae3fe0f334a1dd969047be1a725cd85bfe20d3d0",
            "title": "A Three-regime Model of Network Pruning"
        },
        {
            "paperId": "734d6a6dc2382a673b29e93ce68837f1eb95bd47",
            "title": "Pruning Pre-trained Language Models with Principled Importance and Self-regularization"
        },
        {
            "paperId": "017010b941d902a467f6d329ae5e74fd67e67912",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"
        },
        {
            "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
            "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
        },
        {
            "paperId": "574beee702be3856d60aa482ec725168fe64fc99",
            "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3",
            "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"
        },
        {
            "paperId": "3d60a54a47b346608430344ff37935d897a14c09",
            "title": "Gradient-Free Structured Pruning with Unlabeled Data"
        },
        {
            "paperId": "fdacdbc6a00eeb42efe7f81848b0bc09be5ca997",
            "title": "Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!"
        },
        {
            "paperId": "0c40850c24bf543b14ceb44124db1f4cf88211f3",
            "title": "Fast as CHITA: Neural Network Pruning with Combinatorial Optimization"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "da075ad0ec2c88335af85602a76a33e034536896",
            "title": "DepGraph: Towards Any Structural Pruning"
        },
        {
            "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"
        },
        {
            "paperId": "458147b5f7242c998ec4f33798a59b7c48867329",
            "title": "GPT Takes the Bar Exam"
        },
        {
            "paperId": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e",
            "title": "Training Trajectories of Language Models Across Scales"
        },
        {
            "paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1",
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"
        },
        {
            "paperId": "34bc28087e1d6f047e2736791f79d769293f447c",
            "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
        },
        {
            "paperId": "0696c125606bcbeefde756f7a8a66055e298e9a2",
            "title": "Pruning's Effect on Generalization Through the Lens of Training and Regularization"
        },
        {
            "paperId": "0bcedac1643e22b39df3c820c4f9b767a910877c",
            "title": "Structural Pruning via Latency-Saliency Knapsack"
        },
        {
            "paperId": "3de645f0c1993cd3f3374ad747640a1aa6658a82",
            "title": "Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?"
        },
        {
            "paperId": "11422fff4d42b70c31af69381ff32d35031c939d",
            "title": "Why Random Pruning Is All We Need to Start Sparse"
        },
        {
            "paperId": "3f6243097a58e386aea1215fed4f372dee07a100",
            "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
        },
        {
            "paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3",
            "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "76d40153acfbb35a7eb8272a4215854cafa10e78",
            "title": "PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance"
        },
        {
            "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
            "title": "Emergent Abilities of Large Language Models"
        },
        {
            "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
            "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
        },
        {
            "paperId": "09171a3f87fcb94456eaabefc65731683374f983",
            "title": "Outliers Dimensions that Disrupt Transformers Are Driven by Frequency"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5",
            "title": "Structured Pruning Learns Compact and Accurate Models"
        },
        {
            "paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0",
            "title": "A Fast Post-Training Pruning Framework for Transformers"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "cb4f61bf7eb820c9a4bb87547180a4d6fd76e71d",
            "title": "SPDY: Accurate Pruning with Speedup Guarantees"
        },
        {
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
            "paperId": "177e957f5cd93229c9794ea652c646d2557b4a69",
            "title": "A ConvNet for the 2020s"
        },
        {
            "paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
            "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
        },
        {
            "paperId": "51b0c571d89bd2d39a194f60f91f0a03d74574b5",
            "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality"
        },
        {
            "paperId": "3451010e8fa6a3032c8dd3be1daadb4a08375c64",
            "title": "AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks"
        },
        {
            "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "title": "LoRA: Low-Rank Adaptation of Large Language Models"
        },
        {
            "paperId": "5a09edeb26f9f116f2c0503cd020f38fb943f79b",
            "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers"
        },
        {
            "paperId": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb",
            "title": "Accelerating Sparse Deep Neural Networks"
        },
        {
            "paperId": "b4d207a2096aee4a3764933373eef6edb574c952",
            "title": "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks"
        },
        {
            "paperId": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f",
            "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch"
        },
        {
            "paperId": "9d6acac70b2d1fdb861a08b00766ef263109cd7f",
            "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "829580d6fc73fa601c4982e2b1b6832f2796270b",
            "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
            "title": "Measuring Massive Multitask Language Understanding"
        },
        {
            "paperId": "389036b1366b64579725457993c1f63a4f3370ba",
            "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"
        },
        {
            "paperId": "fbd2be5f16c6cd91e2127eb430b4699d7195e588",
            "title": "On the training dynamics of deep networks with L2 regularization"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"
        },
        {
            "paperId": "6701ae0675f344b14705c7b9dec14273a87e310e",
            "title": "WoodFisher: Efficient Second-Order Approximation for Neural Network Compression"
        },
        {
            "paperId": "de66ada65cd9d36e46f1f8dd2c8be480180038ec",
            "title": "What is the State of Neural Network Pruning?"
        },
        {
            "paperId": "850464c9006261bd632c4203f3e630db09a32faf",
            "title": "Comparing Rewinding and Fine-tuning in Neural Network Pruning"
        },
        {
            "paperId": "8771679aac0e90371340bd8c657317f5be113e81",
            "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"
        },
        {
            "paperId": "8eb599d5d7f1821b205e3b56fef5340b1622ba52",
            "title": "Soft Threshold Weight Reparameterization for Learnable Sparsity"
        },
        {
            "paperId": "2e3002f131e1815bda7a10303eff97f79dea01ec",
            "title": "Rigging the Lottery: Making All Tickets Winners"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout"
        },
        {
            "paperId": "a6f4917d043494d2ebaebe6b65cb35e6a07fda41",
            "title": "Importance Estimation for Neural Network Pruning"
        },
        {
            "paperId": "9770fff7379a7ab9006b48939462354dda9a2053",
            "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"
        },
        {
            "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
            "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"
        },
        {
            "paperId": "075da5ebbb890924267b4b163292ad21d0b100a0",
            "title": "Stabilizing the Lottery Ticket Hypothesis"
        },
        {
            "paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217",
            "title": "The State of Sparsity in Deep Neural Networks"
        },
        {
            "paperId": "4a1004ecd34118116344633c7cdcc34493c423ee",
            "title": "Rethinking the Value of Network Pruning"
        },
        {
            "paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d",
            "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"
        },
        {
            "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
            "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
        },
        {
            "paperId": "1432654a204391b6e2ec197138be0f7c8cb83ae5",
            "title": "Coreset-Based Neural Network Compression"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
            "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
        },
        {
            "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
            "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
        },
        {
            "paperId": "eedaa9eb9c6a8228c79caf560614a431736b62d5",
            "title": "Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip"
        },
        {
            "paperId": "2f201c77e7ccdf1f37115e16accac3486a65c03d",
            "title": "Stochastic Activation Pruning for Robust Adversarial Defense"
        },
        {
            "paperId": "2ec7156913117949ab933f27f492d0149bc0031f",
            "title": "Learning Sparse Neural Networks through L0 Regularization"
        },
        {
            "paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c",
            "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"
        },
        {
            "paperId": "90a16f34d109b63d95ab4da2d491cbe3a1c8b656",
            "title": "Learning Efficient Convolutional Networks through Network Slimming"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "d43c746056601d514e49c9fd0ebed2ed55724eeb",
            "title": "NoiseOut: A Simple Way to Prune Neural Networks"
        },
        {
            "paperId": "3db8730c203f88d7f08a6a99e8c02a077dc9b011",
            "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "60ae4f18cb53efff0174e3fea7064049737e1e67",
            "title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures"
        },
        {
            "paperId": "642d0f49b7826adcf986616f4af77e736229990f",
            "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
        },
        {
            "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "title": "Learning both Weights and Connections for Efficient Neural Network"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "61b0f5cd87a88ac98ab91239147157b92ecaade9",
            "title": "L2 Regularization for Learning Kernels"
        },
        {
            "paperId": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724",
            "title": "Optimal Brain Surgeon and general network pruning"
        },
        {
            "paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af",
            "title": "Are Emergent Abilities of Large Language Models a Mirage?"
        },
        {
            "paperId": "fda9a8f0664456dc4accb4018cfad2e6fde2d460",
            "title": "Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph"
        },
        {
            "paperId": "2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c",
            "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
        },
        {
            "paperId": "121c3860d5e3917c658dfac7e1f56ba297e87a6c",
            "title": "Channel Permutations for N: M Sparsity"
        },
        {
            "paperId": null,
            "title": "A framework for few-shot language model evaluation"
        },
        {
            "paperId": null,
            "title": "In CVPR, 2022"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28",
            "title": "An Adversarial Winograd Schema Challenge at Scale"
        },
        {
            "paperId": "015ca32bca81dbda1e2e432445eef798582236e1",
            "title": "Conference Paper"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        }
    ]
}