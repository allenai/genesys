{
    "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
    "externalIds": {
        "DBLP": "conf/iclr/LinFSYXZB17",
        "ArXiv": "1703.03130",
        "MAG": "2963386218",
        "CorpusId": 15280949
    },
    "title": "A Structured Self-attentive Sentence Embedding",
    "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",
    "venue": "International Conference on Learning Representations",
    "year": 2017,
    "referenceCount": 39,
    "citationCount": 2028,
    "influentialCitationCount": 159,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new model for extracting an interpretable sentence embedding by introducing self-attention is proposed, which uses a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "3146592",
            "name": "Zhouhan Lin"
        },
        {
            "authorId": "2521552",
            "name": "Minwei Feng"
        },
        {
            "authorId": "1790831",
            "name": "C. D. Santos"
        },
        {
            "authorId": "2482533",
            "name": "Mo Yu"
        },
        {
            "authorId": "144028698",
            "name": "Bing Xiang"
        },
        {
            "authorId": "145218984",
            "name": "Bowen Zhou"
        },
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        }
    ],
    "references": [
        {
            "paperId": "1261fe9bfde319abcc5d011bc70f7e7547b5258f",
            "title": "Improved Representation Learning for Question Answer Matching"
        },
        {
            "paperId": "bdf28e3cadbabda3261bd904c37edea66ab84766",
            "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering"
        },
        {
            "paperId": "705dcc8eadba137834e4b0359e2d696d4b209f5b",
            "title": "Neural Tree Indexers for Text Understanding"
        },
        {
            "paperId": "cff79255a94b9b05a4ce893eb403a522e0923f04",
            "title": "Neural Semantic Encoders"
        },
        {
            "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "title": "A Decomposable Attention Model for Natural Language Inference"
        },
        {
            "paperId": "f93a0a3e8a3e6001b4482430254595cf737697fa",
            "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention"
        },
        {
            "paperId": "6b570069f14c7588e066f7138e1f21af59d62e61",
            "title": "Theano: A Python framework for fast computation of mathematical expressions"
        },
        {
            "paperId": "f292139b371f5b34e835f4dbff102dc28f972876",
            "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks"
        },
        {
            "paperId": "36c097a225a95735271960e2b63a2cb9e98bff83",
            "title": "A Fast Unified Model for Parsing and Sentence Understanding"
        },
        {
            "paperId": "a2dc06c8da0ff9344dc558d6df571fc704b81ae7",
            "title": "Attentive Pooling Networks"
        },
        {
            "paperId": "26e743d5bd465f49b9538deaf116c15e61b7951f",
            "title": "Learning Distributed Representations of Sentences from Unlabelled Data"
        },
        {
            "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
            "title": "Long Short-Term Memory-Networks for Machine Reading"
        },
        {
            "paperId": "ea407573bfcd39f9a478fe33cf6ce0ee1780a5f0",
            "title": "Natural Language Inference by Tree-Based Convolution and Heuristic Matching"
        },
        {
            "paperId": "46b8cbcdff87b842c2c1d4a003c831f845096ba7",
            "title": "Order-Embeddings of Images and Language"
        },
        {
            "paperId": "1347bd4f826f72ff561b70e665477edadb2a72be",
            "title": "Not All Contexts Are Created Equal: Better Word Representations with Variable Attention"
        },
        {
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference"
        },
        {
            "paperId": "f33e86970c11f9bb6d0abb60acdc9274d5c3f342",
            "title": "Applying deep learning to answer selection: A study and an open task"
        },
        {
            "paperId": "57e562b46338f176e3b20c2dd0b66f17dfbef9e8",
            "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding"
        },
        {
            "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "title": "Skip-Thought Vectors"
        },
        {
            "paperId": "bd32ebb9fac53a14202fb1a4f76ef96d1ff68c6c",
            "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "22ae02d81c21cb90b0de071550cfb99e6a623e62",
            "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
            "paperId": "b0aca3e7877c3c20958b0fae5cbf2dd602104859",
            "title": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"
        },
        {
            "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
            "title": "Distributed Representations of Sentences and Documents"
        },
        {
            "paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
            "title": "A Convolutional Neural Network for Modelling Sentences"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "f282338fa4cd5516cdcd33cd4b6034f9739c45f4",
            "title": "Learning to Relate Images"
        },
        {
            "paperId": "cfa2646776405d50533055ceb1b7f050e9014dcb",
            "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "bcd857d75841aa3e92cd4284a8818aba9f6c0c3f",
            "title": "Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS"
        },
        {
            "paperId": "ce8ec6352d9ed8212de5e2441880b37452030383",
            "title": "A Batch-Normalized Recurrent Network for Sentiment Classification"
        },
        {
            "paperId": "1682b8b395c7d7fa30b3cec961ac81fdda53e72d",
            "title": "Convolutional Neural Network for Paraphrase Identification"
        },
        {
            "paperId": "015ca32bca81dbda1e2e432445eef798582236e1",
            "title": "Conference Paper"
        },
        {
            "paperId": null,
            "title": "F r = F h (cid:12) F p (12) Here (cid:12) stands for element-wise product. After the F r layer, we then use an MLP with softmax output to classify the relation into different categlories"
        }
    ]
}