{
    "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
    "externalIds": {
        "ArXiv": "2204.14198",
        "DBLP": "journals/corr/abs-2204-14198",
        "CorpusId": 248476411
    },
    "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
    "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "referenceCount": 182,
    "citationCount": 2249,
    "influentialCitationCount": 251,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces Flamingo, a family of Visual Language Models (VLM) with this ability to bridge powerful pretrained vision-only and language-only models, handle sequences of arbitrarily interleaved visual and textual data, and seamlessly ingest images or videos as inputs."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.9909820556640625,
            -2.8729517459869385,
            -1.3945153951644897,
            5.435105323791504,
            -0.5027523636817932,
            0.8939738869667053,
            5.692511081695557,
            -0.6144828200340271,
            -3.1536717414855957,
            0.5569362640380859,
            -1.655745506286621,
            4.880865573883057,
            -1.0110459327697754,
            2.900070905685425,
            -2.746687650680542,
            -3.532461643218994,
            0.25581634044647217,
            1.1346051692962646,
            4.795563697814941,
            1.684792399406433,
            1.4608463048934937,
            0.30881762504577637,
            -3.214869737625122,
            -0.9494435787200928,
            -4.2858171463012695,
            -2.5630412101745605,
            4.913156032562256,
            4.2986273765563965,
            -3.1310977935791016,
            1.1331385374069214,
            2.0966835021972656,
            -3.1477882862091064,
            4.361099720001221,
            -3.421862840652466,
            1.0005195140838623,
            -3.127283811569214,
            -3.8305344581604004,
            7.311635971069336,
            -4.136155605316162,
            -1.570529818534851,
            -1.4161968231201172,
            1.9227914810180664,
            0.20303581655025482,
            0.7724941968917847,
            1.1405092477798462,
            0.581622302532196,
            0.3996722400188446,
            0.32659101486206055,
            -0.39534103870391846,
            0.16122639179229736,
            0.29844996333122253,
            -0.3792691230773926,
            -0.6475573778152466,
            0.5313231348991394,
            -0.6063511967658997,
            -2.173980712890625,
            0.6661256551742554,
            3.1484060287475586,
            1.107018232345581,
            0.18190914392471313,
            4.093031883239746,
            6.663613319396973,
            -1.583195686340332,
            1.010488748550415,
            3.26981782913208,
            -4.306316375732422,
            -2.9487361907958984,
            1.395830512046814,
            -1.1509017944335938,
            1.288590431213379,
            -1.9972022771835327,
            -6.233201503753662,
            -0.24663427472114563,
            1.1100084781646729,
            -1.8611621856689453,
            0.5699309706687927,
            -0.9662371873855591,
            -6.71921968460083,
            -3.6979970932006836,
            -2.0256435871124268,
            -0.9955185651779175,
            -1.1531257629394531,
            -0.29288220405578613,
            2.4655818939208984,
            5.339770793914795,
            -0.5198937654495239,
            -3.751077890396118,
            -1.2885456085205078,
            0.8447514772415161,
            -5.380503177642822,
            -0.9146929979324341,
            -1.4452062845230103,
            1.160380482673645,
            3.4442355632781982,
            -1.5919904708862305,
            -1.2031733989715576,
            0.8130031824111938,
            -0.6736874580383301,
            -4.1100993156433105,
            -3.198911190032959,
            5.557126998901367,
            -0.2981683313846588,
            1.6271309852600098,
            4.961725234985352,
            5.66256046295166,
            -5.355106353759766,
            -0.08374307304620743,
            0.3841586709022522,
            3.8463330268859863,
            -2.773883819580078,
            -1.3079898357391357,
            4.353818893432617,
            0.6647377610206604,
            1.1945672035217285,
            -0.8819267153739929,
            -2.9294791221618652,
            1.4430491924285889,
            1.1380343437194824,
            -3.477003812789917,
            3.302795648574829,
            0.568732500076294,
            -0.671808660030365,
            -4.006704330444336,
            -0.25228649377822876,
            2.6735424995422363,
            -2.232457160949707,
            -2.307098388671875,
            -0.21085597574710846,
            -0.7732390761375427,
            -4.350838661193848,
            4.805513381958008,
            -0.9302579164505005,
            4.7736992835998535,
            -3.2205958366394043,
            3.7884891033172607,
            3.796778678894043,
            -3.4116246700286865,
            1.460906744003296,
            1.3801941871643066,
            0.7204566597938538,
            0.9637061953544617,
            1.266740322113037,
            -1.643432378768921,
            0.3667045533657074,
            -1.8192538022994995,
            1.8858928680419922,
            -0.16330714523792267,
            1.176937460899353,
            1.789087176322937,
            6.8250298500061035,
            3.241797924041748,
            -3.9619288444519043,
            1.9536969661712646,
            0.7521252632141113,
            2.646982431411743,
            4.155997276306152,
            -8.019316673278809,
            4.397855758666992,
            -3.577331066131592,
            -0.9555513858795166,
            1.2839428186416626,
            1.378851056098938,
            -11.53422737121582,
            -0.29565203189849854,
            2.9677271842956543,
            -5.296991348266602,
            -0.46009859442710876,
            3.1456754207611084,
            -0.06507298350334167,
            3.4082937240600586,
            0.014265403151512146,
            2.3596181869506836,
            0.10162337124347687,
            3.262279987335205,
            4.23370885848999,
            2.475253105163574,
            0.6839815378189087,
            0.47370362281799316,
            -3.5626206398010254,
            -0.6649624109268188,
            -0.7878972291946411,
            -0.6233048439025879,
            -6.479974746704102,
            0.9769116044044495,
            -5.521491050720215,
            0.410308837890625,
            -1.4255660772323608,
            1.877585530281067,
            1.410027027130127,
            -1.6693801879882812,
            0.03121301531791687,
            -0.7622098922729492,
            7.108224868774414,
            6.626948833465576,
            3.2192318439483643,
            1.501873254776001,
            2.7744979858398438,
            5.101267337799072,
            -0.2710474133491516,
            2.64194917678833,
            3.6726572513580322,
            -0.5521062612533569,
            -2.2945358753204346,
            -2.984391212463379,
            4.894675254821777,
            3.173271656036377,
            -3.046180248260498,
            3.1057729721069336,
            2.1233084201812744,
            0.4994933009147644,
            0.13334113359451294,
            -2.6341159343719482,
            -2.7845585346221924,
            5.072236061096191,
            -0.060079336166381836,
            -0.9560383558273315,
            -6.3738813400268555,
            1.308690071105957,
            7.30870246887207,
            0.3307093381881714,
            -1.1141546964645386,
            1.4347858428955078,
            1.2669856548309326,
            -1.601974368095398,
            1.0988013744354248,
            -4.866420745849609,
            0.3418828845024109,
            -0.8283575773239136,
            0.2484930157661438,
            0.5906985998153687,
            -1.5405189990997314,
            -3.0622951984405518,
            -1.3534388542175293,
            -0.5594074726104736,
            -9.926972389221191,
            -2.6330389976501465,
            -7.422541618347168,
            0.7322261333465576,
            -1.3518685102462769,
            -4.023261070251465,
            3.7951455116271973,
            0.6066684126853943,
            0.23946945369243622,
            4.3251237869262695,
            1.9519665241241455,
            -0.2806503474712372,
            -4.197866916656494,
            1.3107129335403442,
            -0.7572363615036011,
            -0.2975517213344574,
            1.0889285802841187,
            -1.3745301961898804,
            1.8320565223693848,
            -1.782942295074463,
            1.2602676153182983,
            0.5705288648605347,
            0.8551007509231567,
            1.0711473226547241,
            -0.45309263467788696,
            0.9835364818572998,
            4.149881362915039,
            4.383017539978027,
            1.4849541187286377,
            5.265076160430908,
            -3.863125801086426,
            -0.005819052457809448,
            -2.5143203735351562,
            -2.3542733192443848,
            -4.7959394454956055,
            4.855907440185547,
            1.9052926301956177,
            2.73529052734375,
            2.203390121459961,
            -3.563706398010254,
            -4.1106791496276855,
            -5.7432780265808105,
            -3.9271435737609863,
            -0.4696730375289917,
            3.590178966522217,
            2.2062954902648926,
            0.28241539001464844,
            -1.915984869003296,
            -1.133251667022705,
            -3.907409429550171,
            -0.3087426424026489,
            -4.656558990478516,
            -2.556124448776245,
            -0.5183215737342834,
            -2.478823661804199,
            -3.25125789642334,
            -1.0170941352844238,
            4.537965297698975,
            -3.834242105484009,
            -1.5744352340698242,
            -1.943183422088623,
            3.216444492340088,
            5.454141139984131,
            1.760324239730835,
            -1.3452643156051636,
            0.4254383444786072,
            -0.43837305903434753,
            2.954775333404541,
            4.163837432861328,
            -0.6934210062026978,
            -0.4882493019104004,
            5.232973098754883,
            -2.1493589878082275,
            0.14577895402908325,
            2.7830114364624023,
            -5.2134928703308105,
            -1.390438199043274,
            2.412719249725342,
            2.1151137351989746,
            -9.357077598571777,
            -1.5031116008758545,
            1.8511663675308228,
            4.4325385093688965,
            -1.1771291494369507,
            -0.028360068798065186,
            -0.5865428447723389,
            1.4608426094055176,
            1.1287827491760254,
            -4.429762840270996,
            -6.180418014526367,
            -4.800994873046875,
            0.7918583154678345,
            2.4276764392852783,
            -0.01670757681131363,
            -4.04746150970459,
            2.8846476078033447,
            1.0851333141326904,
            3.913900375366211,
            3.4292960166931152,
            1.7248480319976807,
            -3.5110630989074707,
            -4.971449851989746,
            1.124610424041748,
            -2.8480870723724365,
            0.9665105938911438,
            -1.4835309982299805,
            0.5284724235534668,
            7.103229522705078,
            -2.081103801727295,
            -2.130499839782715,
            4.994818210601807,
            3.445887327194214,
            2.332137107849121,
            0.3455854654312134,
            0.4406644403934479,
            -4.3521037101745605,
            3.3391289710998535,
            3.2869162559509277,
            4.2065277099609375,
            -2.6381359100341797,
            1.566860556602478,
            6.034365653991699,
            4.224318504333496,
            -0.5794018507003784,
            -1.202883243560791,
            2.5123980045318604,
            2.652172327041626,
            -1.2812576293945312,
            1.600476861000061,
            4.4937920570373535,
            -0.4921814203262329,
            0.7907639741897583,
            12.75271224975586,
            -0.96335369348526,
            -0.05085131525993347,
            -4.649848937988281,
            -3.9945240020751953,
            -2.744745969772339,
            -4.741621971130371,
            4.301859378814697,
            -2.1053404808044434,
            -3.843273162841797,
            -0.13952937722206116,
            -6.312502861022949,
            1.9199566841125488,
            3.348891496658325,
            -2.4729256629943848,
            4.272777557373047,
            -1.6404831409454346,
            3.5871355533599854,
            2.3359804153442383,
            1.2623622417449951,
            -4.309683322906494,
            2.8684659004211426,
            2.9454712867736816,
            0.6974993348121643,
            -1.3697257041931152,
            1.3941986560821533,
            1.462270975112915,
            2.0646164417266846,
            -4.429173946380615,
            -4.829837322235107,
            -1.9006199836730957,
            -2.256511926651001,
            -0.6264064311981201,
            -1.0649337768554688,
            1.7144147157669067,
            1.0335863828659058,
            1.0263099670410156,
            3.5080928802490234,
            -3.9273808002471924,
            0.44477081298828125,
            4.293097019195557,
            -2.34474515914917,
            -3.043715000152588,
            1.3160014152526855,
            -2.3436503410339355,
            -4.389647960662842,
            -2.2438793182373047,
            -2.778829574584961,
            -1.5564982891082764,
            3.2241525650024414,
            2.9402995109558105,
            1.9049060344696045,
            1.4730470180511475,
            1.8738598823547363,
            -3.6498260498046875,
            3.8036434650421143,
            4.588735580444336,
            0.45385196805000305,
            1.9877805709838867,
            2.793431282043457,
            -0.2294575572013855,
            1.6955277919769287,
            -4.411081314086914,
            2.3512444496154785,
            0.5052913427352905,
            4.000641345977783,
            -1.3476711511611938,
            -0.9515411853790283,
            -0.4248505234718323,
            -0.3609711527824402,
            1.9729418754577637,
            3.7117133140563965,
            2.5604195594787598,
            -7.373466968536377,
            3.309347152709961,
            4.990791320800781,
            -1.151590347290039,
            1.2112723588943481,
            1.8421404361724854,
            2.3650336265563965,
            1.5094654560089111,
            1.555253267288208,
            -1.4776992797851562,
            -4.595174789428711,
            4.561478137969971,
            -3.5398032665252686,
            -5.040896892547607,
            0.6097202301025391,
            2.30944561958313,
            2.3062610626220703,
            -4.489296913146973,
            0.22402840852737427,
            2.577643871307373,
            -0.5306119918823242,
            -5.786230087280273,
            0.9214287400245667,
            1.5129772424697876,
            1.0987260341644287,
            1.7193925380706787,
            0.18764200806617737,
            -3.1795029640197754,
            1.4132758378982544,
            -2.9865853786468506,
            2.466629981994629,
            5.475404262542725,
            1.1192353963851929,
            -2.662296772003174,
            2.733168125152588,
            0.32546859979629517,
            1.662895679473877,
            1.282182216644287,
            1.303647756576538,
            2.589886426925659,
            -5.075870513916016,
            -3.6495227813720703,
            -1.9396215677261353,
            4.305698394775391,
            -1.7443790435791016,
            -0.28360509872436523,
            2.5810699462890625,
            2.6774816513061523,
            1.548651933670044,
            2.997499942779541,
            2.1378774642944336,
            -0.8425548672676086,
            1.4533320665359497,
            4.8879923820495605,
            -0.2370617538690567,
            0.04857361316680908,
            -1.440951943397522,
            -4.50990104675293,
            2.27514910697937,
            0.7587429881095886,
            -0.5753834247589111,
            2.082139015197754,
            -4.216887474060059,
            -1.241462230682373,
            -1.9992973804473877,
            -1.141408920288086,
            5.5909624099731445,
            3.1860361099243164,
            0.5822315216064453,
            -2.0747365951538086,
            -2.4581496715545654,
            -1.3184349536895752,
            1.246772289276123,
            -2.08080792427063,
            -0.6911008358001709,
            -2.1891977787017822,
            0.6148717999458313,
            5.3792619705200195,
            -3.2741689682006836,
            0.17230179905891418,
            0.6643064022064209,
            -2.0095629692077637,
            1.415022611618042,
            -1.5699000358581543,
            2.241819381713867,
            4.8707051277160645,
            1.1132783889770508,
            -4.367753982543945,
            -0.35297641158103943,
            2.2172904014587402,
            3.7886886596679688,
            4.800184726715088,
            5.916323661804199,
            2.268115997314453,
            -2.450126886367798,
            -2.7981960773468018,
            -4.792758464813232,
            -0.5504107475280762,
            2.6990785598754883,
            -2.8288724422454834,
            -0.20873820781707764,
            -0.5520809292793274,
            1.9910215139389038,
            3.7736141681671143,
            5.227306842803955,
            1.2761011123657227,
            2.832369804382324,
            -4.832284927368164,
            -1.8177499771118164,
            -1.3943672180175781,
            0.3380153775215149,
            -0.8991532921791077,
            -1.73601233959198,
            -0.43673083186149597,
            0.2790263593196869,
            -0.9527425765991211,
            -0.061203256249427795,
            -3.864600896835327,
            -2.3198063373565674,
            -1.2597922086715698,
            1.0959864854812622,
            1.4529354572296143,
            -5.344780921936035,
            0.7783357501029968,
            2.0875136852264404,
            -0.5446699261665344,
            -2.0261363983154297,
            -0.5011992454528809,
            4.677979946136475,
            2.072840690612793,
            -1.2622737884521484,
            -2.632291316986084,
            -6.361340522766113,
            -0.36418670415878296,
            2.1259965896606445,
            2.029038906097412,
            0.21548712253570557,
            4.180382251739502,
            2.416553020477295,
            -1.6885608434677124,
            -0.4321047067642212,
            -5.915209770202637,
            -0.42987173795700073,
            -2.178250789642334,
            -3.979067802429199,
            1.2816135883331299,
            -2.8461670875549316,
            -4.5351738929748535,
            0.5179882049560547,
            -2.879883050918579,
            0.9681307077407837,
            0.7394442558288574,
            -0.6082806587219238,
            1.694858193397522,
            -3.8893535137176514,
            0.6939258575439453,
            -4.399918079376221,
            6.011445999145508,
            3.732421875,
            1.3204734325408936,
            3.4418163299560547,
            0.8724029064178467,
            5.691863059997559,
            1.7451589107513428,
            1.793899416923523,
            -4.662664413452148,
            0.7430185079574585,
            1.1310726404190063,
            -0.9474348425865173,
            -1.7759754657745361,
            -0.6201987266540527,
            -0.12124728411436081,
            1.1269116401672363,
            18.48046112060547,
            0.44972872734069824,
            -0.3945956528186798,
            -1.2367967367172241,
            -4.046899318695068,
            -1.8485386371612549,
            -0.4613434672355652,
            4.718731880187988,
            0.9458288550376892,
            0.1594730019569397,
            -0.6790705323219299,
            -3.220496654510498,
            -2.732135057449341,
            2.4018020629882812,
            -3.142061233520508,
            -0.614776611328125,
            -2.8148369789123535,
            3.3618147373199463,
            -1.101316213607788,
            1.046951174736023,
            -1.380696415901184,
            -1.1142737865447998,
            -1.9362753629684448,
            -2.653236150741577,
            0.8161799907684326,
            4.912683010101318,
            -0.38144737482070923,
            5.789493083953857,
            -3.6158337593078613,
            1.979061484336853,
            -0.15980589389801025,
            4.00040864944458,
            0.14799261093139648,
            -0.6217173933982849,
            -1.8937280178070068,
            2.8526525497436523,
            5.790948867797852,
            -1.2390373945236206,
            -0.21328693628311157,
            0.4075815677642822,
            -1.2286025285720825,
            -0.25061464309692383,
            -1.7694982290267944,
            -0.7006705403327942,
            -3.880756378173828,
            1.9904018640518188,
            3.778111457824707,
            -3.5545878410339355,
            -1.1030011177062988,
            7.195404529571533,
            -1.0810455083847046,
            -1.3160983324050903,
            1.060777187347412,
            0.5443124175071716,
            2.1264288425445557,
            1.9024856090545654,
            1.6582751274108887,
            -4.5864481925964355,
            2.2277612686157227,
            -1.4462096691131592,
            0.8153401017189026,
            -0.29195380210876465,
            -2.986640691757202,
            -5.267596244812012,
            -1.284775733947754,
            0.610046923160553,
            -5.831909656524658,
            4.313882350921631,
            4.04157018661499,
            -0.8007001876831055,
            1.0669361352920532,
            6.262124061584473,
            1.449512004852295,
            -2.729123592376709,
            -1.521530032157898,
            -5.9905924797058105,
            1.072996973991394,
            -1.0427639484405518,
            1.6427572965621948,
            7.697240829467773,
            -3.0325446128845215,
            2.891986131668091,
            -1.9487026929855347,
            -2.632978916168213,
            3.8420939445495605,
            -1.4434597492218018,
            -0.2161480188369751,
            1.3000943660736084,
            -0.8582357168197632,
            1.6969847679138184,
            2.0916638374328613,
            0.8484573364257812,
            3.094609498977661,
            2.7924535274505615,
            0.34084179997444153,
            -5.2632317543029785,
            -2.6736087799072266,
            0.02750188112258911,
            -3.964163303375244,
            -2.6560423374176025,
            6.8723344802856445,
            5.695952892303467,
            0.5957661867141724,
            -4.750624656677246,
            -0.8038387894630432,
            -3.6472725868225098,
            -1.4181208610534668,
            -6.264761447906494,
            -2.897128105163574,
            -1.9618775844573975,
            2.708433151245117,
            -4.609899520874023,
            1.6250964403152466,
            -0.2844245433807373,
            2.15848970413208,
            -2.5709738731384277,
            1.7296632528305054,
            1.706531286239624,
            1.7441054582595825,
            4.412221431732178,
            1.395655632019043,
            -0.17186890542507172,
            -3.831075668334961,
            -3.1585428714752197,
            0.0865713357925415,
            -0.7007454633712769,
            1.07306706905365,
            -2.9726600646972656,
            -1.5256394147872925,
            -1.4675557613372803,
            1.6313823461532593,
            -1.8967459201812744,
            -2.375854015350342,
            -0.23064842820167542,
            -0.8031583428382874,
            -1.4694100618362427,
            2.0926177501678467,
            2.616112470626831,
            -1.9241697788238525,
            5.283675193786621,
            2.550098180770874,
            -2.466007709503174,
            -2.743274688720703,
            9.217694282531738,
            -1.0915191173553467,
            -0.38304319977760315,
            -3.6660618782043457,
            -3.0816831588745117,
            -1.2989850044250488,
            -0.4566263258457184,
            0.7917479276657104,
            0.5934446454048157,
            2.0434913635253906,
            1.3076581954956055,
            -2.857529640197754,
            -1.8612315654754639
        ]
    },
    "authors": [
        {
            "authorId": "2285263",
            "name": "Jean-Baptiste Alayrac"
        },
        {
            "authorId": "7408951",
            "name": "Jeff Donahue"
        },
        {
            "authorId": "152831141",
            "name": "Pauline Luc"
        },
        {
            "authorId": "19200186",
            "name": "Antoine Miech"
        },
        {
            "authorId": "2159207795",
            "name": "Iain Barr"
        },
        {
            "authorId": "66535271",
            "name": "Yana Hasson"
        },
        {
            "authorId": "3257286",
            "name": "Karel Lenc"
        },
        {
            "authorId": "1697879",
            "name": "A. Mensch"
        },
        {
            "authorId": "2143434227",
            "name": "Katie Millican"
        },
        {
            "authorId": "47447264",
            "name": "Malcolm Reynolds"
        },
        {
            "authorId": "81387328",
            "name": "Roman Ring"
        },
        {
            "authorId": "2143538252",
            "name": "Eliza Rutherford"
        },
        {
            "authorId": "12159303",
            "name": "Serkan Cabi"
        },
        {
            "authorId": "22237490",
            "name": "Tengda Han"
        },
        {
            "authorId": "48398849",
            "name": "Zhitao Gong"
        },
        {
            "authorId": "2412073",
            "name": "Sina Samangooei"
        },
        {
            "authorId": "49601928",
            "name": "Marianne Monteiro"
        },
        {
            "authorId": "10698483",
            "name": "Jacob Menick"
        },
        {
            "authorId": "148016269",
            "name": "Sebastian Borgeaud"
        },
        {
            "authorId": "2065040422",
            "name": "Andy Brock"
        },
        {
            "authorId": "3208081",
            "name": "Aida Nematzadeh"
        },
        {
            "authorId": "7782886",
            "name": "Sahand Sharifzadeh"
        },
        {
            "authorId": "9961753",
            "name": "Mikolaj Binkowski"
        },
        {
            "authorId": "2026369796",
            "name": "Ricardo Barreira"
        },
        {
            "authorId": "1689108",
            "name": "O. Vinyals"
        },
        {
            "authorId": "1688869",
            "name": "Andrew Zisserman"
        },
        {
            "authorId": "34838386",
            "name": "K. Simonyan"
        }
    ],
    "references": [
        {
            "paperId": "039c0f0b4142823fe214ddf2470230e211c7b8ee",
            "title": "STAR: A Benchmark for Situated Reasoning in Real-World Videos"
        },
        {
            "paperId": "c57293882b2561e1ba03017902df9fc2f289dea2",
            "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"
        },
        {
            "paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"
        },
        {
            "paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b",
            "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41",
            "title": "Teaching language models to support answers with verified quotes"
        },
        {
            "paperId": "095ccdb08837a4b44a62638fb8dc391818707e5a",
            "title": "All in One: Exploring Unified Video-Language Pre-Training"
        },
        {
            "paperId": "7f71875f8214dffa4f3276da123c4990a6d437cc",
            "title": "Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation"
        },
        {
            "paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84",
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
        },
        {
            "paperId": "f4df78183261538e718066331898ee5cad7cad05",
            "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
        },
        {
            "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
            "title": "Red Teaming Language Models with Language Models"
        },
        {
            "paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99",
            "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
        },
        {
            "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
            "title": "LaMDA: Language Models for Dialog Applications"
        },
        {
            "paperId": "c783e1fb3ce8514f981925ee590c00884660ee4e",
            "title": "CM3: A Causal Masked Multimodal Model of the Internet"
        },
        {
            "paperId": "842104ef0575823498f26cdd57b4b4dba655df9e",
            "title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization"
        },
        {
            "paperId": "217aa2a24e5916835d04e384f729c21ad65deadc",
            "title": "Multiview Transformers for Video Recognition"
        },
        {
            "paperId": "400d619cbabeb669115bb7281a889ab869829ef5",
            "title": "MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound"
        },
        {
            "paperId": "ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45",
            "title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language"
        },
        {
            "paperId": "55a19318cc93714802c7ac59e07651789749b20c",
            "title": "VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"
        },
        {
            "paperId": "bdea16e93fc70f316002e5f6aac8ce17388c6ee9",
            "title": "MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning"
        },
        {
            "paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02",
            "title": "FLAVA: A Foundational Language And Vision Alignment Model"
        },
        {
            "paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
            "title": "Ethical and social risks of harm from Language Models"
        },
        {
            "paperId": "68f141724814839d556a989646194be88641b143",
            "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
        },
        {
            "paperId": "91dc75f94da13452a54ad5c03fab2c5fda87e9ba",
            "title": "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks"
        },
        {
            "paperId": "3ea60cbce6c9065661d207fccf021c5d58a83f01",
            "title": "Scaling Up Vision-Language Pretraining for Image Captioning"
        },
        {
            "paperId": "ba9d736006b897d06f75586ad46e28e00a5e566e",
            "title": "VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling"
        },
        {
            "paperId": "21ec90872abd986c12afe39bebe807732ffa70c9",
            "title": "Florence: A New Foundation Model for Computer Vision"
        },
        {
            "paperId": "d257547d681f3a153f4bbf85f5955b5a0189e500",
            "title": "Combined Scaling for Zero-shot Transfer Learning"
        },
        {
            "paperId": "c05cd00ae61f3c1c39be2603a2f96fdfe0c59dd8",
            "title": "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning"
        },
        {
            "paperId": "a7aa150b55d64d339b1c154d6d88455fc3cbc44f",
            "title": "ClipCap: CLIP Prefix for Image Captioning"
        },
        {
            "paperId": "ecb9ed7b42c9c146a57e28c1ce81eddf95434727",
            "title": "Achieving Human Parity on Visual Question Answering"
        },
        {
            "paperId": "197d5867a45a2988f4dd159063cdfbfe90164962",
            "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning"
        },
        {
            "paperId": "f675c62abfa788ea0be85d3124eba15a14d5e9d6",
            "title": "FILIP: Fine-grained Interactive Language-Image Pre-Training"
        },
        {
            "paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0",
            "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"
        },
        {
            "paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df",
            "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"
        },
        {
            "paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
            "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"
        },
        {
            "paperId": "19b3b074d38b250d024920732ae51a8ffa0996dd",
            "title": "Pix2seq: A Language Modeling Framework for Object Detection"
        },
        {
            "paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
            "title": "Primer: Searching for Efficient Transformers for Language Modeling"
        },
        {
            "paperId": "2672777d25562c9df6fc13b653181db62d39bece",
            "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"
        },
        {
            "paperId": "fe0969ea1875c4c76bcdb205d818daaee7a84bfc",
            "title": "MURAL: Multimodal, Multitask Retrieval Across Languages"
        },
        {
            "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
            "title": "Finetuned Language Models Are Zero-Shot Learners"
        },
        {
            "paperId": "96ea07447d2f9adefe03852a878517a2a6d45b96",
            "title": "Learning to Prompt for Vision-Language Models"
        },
        {
            "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
        },
        {
            "paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13",
            "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"
        },
        {
            "paperId": "e800c0ff410cf664f5082ceef8f4feb50ccf4a5d",
            "title": "Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs"
        },
        {
            "paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1",
            "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"
        },
        {
            "paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d",
            "title": "Multimodal Few-Shot Learning with Frozen Language Models"
        },
        {
            "paperId": "9ee87103ccf8ce60c2e049c6411840593cd0af57",
            "title": "Winner Team Mia at TextVQA Challenge 2021: Vision-and-Language Representation Learning with Pre-trained Sequence-to-Sequence Model"
        },
        {
            "paperId": "339b2b711fb5b228d097b03ebc3e62a521779235",
            "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"
        },
        {
            "paperId": "558f811c61c35f8fbe0d6ab3f4195c6725784573",
            "title": "Understanding and Evaluating Racial Biases in Image Captioning"
        },
        {
            "paperId": "2a805d0e1b067444a554c5169d189fa1f649f411",
            "title": "Scaling Vision Transformers"
        },
        {
            "paperId": "90357a6dc817e2f7cec477a51156675fbf545cf1",
            "title": "MERLOT: Multimodal Neural Script Knowledge Models"
        },
        {
            "paperId": "63c74d15940af1af9b386b5762e4445e54c73719",
            "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"
        },
        {
            "paperId": "b58d8579ece27a60432e667bfbdb750590fa65d9",
            "title": "True Few-Shot Learning with Language Models"
        },
        {
            "paperId": "18f37f62d2bf3c2e34e2bde78545b47e92d7b72d",
            "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding"
        },
        {
            "paperId": "1b937ff4b05e2b56c2c2fcdfa5baa3085cd5a08c",
            "title": "NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions"
        },
        {
            "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
        },
        {
            "paperId": "26e479f571d6634ae351dd941a6bd483646637e9",
            "title": "ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition"
        },
        {
            "paperId": "bac87bdb1cabc35fafb8176a234d332ebcc02864",
            "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"
        },
        {
            "paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
            "title": "Perceiver: General Perception with Iterative Attention"
        },
        {
            "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "title": "Learning Transferable Visual Models From Natural Language Supervision"
        },
        {
            "paperId": "616e0ed02ca024a8c1d4b86167f7486ea92a13d9",
            "title": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"
        },
        {
            "paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69",
            "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"
        },
        {
            "paperId": "394be105b87e9bfe72c20efe6338de10604e1a11",
            "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"
        },
        {
            "paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0",
            "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"
        },
        {
            "paperId": "141a5033d9994242b18bb3b217e79582f1ee9306",
            "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"
        },
        {
            "paperId": "c16835c8e535ebd9c10a550ca9455fe384a14449",
            "title": "High-Performance Large-Scale Image Recognition Without Normalization"
        },
        {
            "paperId": "cb596bffc5c5042c254058b62317a57fa156fea4",
            "title": "Unifying Vision-and-Language Tasks via Text Generation"
        },
        {
            "paperId": "81002fbb777f860f9aac2bbc24467a62345af279",
            "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers"
        },
        {
            "paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74",
            "title": "What Makes Good In-Context Examples for GPT-3?"
        },
        {
            "paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
        },
        {
            "paperId": "76e266d5220f963886cf30ae747d72fdf8c84378",
            "title": "A Multimodal Framework for the Detection of Hateful Memes"
        },
        {
            "paperId": "6cd2c56f54dede618a38287de9157019fb5631e5",
            "title": "Enhance Multimodal Transformer With External Label And In-Domain Pretrain: Hateful Meme Challenge Winning Solution"
        },
        {
            "paperId": "8deceb13cb3afcfbaab06a2c655f1935445635fe",
            "title": "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption"
        },
        {
            "paperId": "b3383f4ec36e024f7dda3f1c708f076b8465e307",
            "title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "fb034caef3720744402363162e8f9b8758f0b82c",
            "title": "A Short Note on the Kinetics-700-2020 Human Action Dataset"
        },
        {
            "paperId": "e508e1b4404f0c2c8fe55877422b69c1004a1e56",
            "title": "RareAct: A video dataset of unusual interactions"
        },
        {
            "paperId": "0fee1138854bd786697dcdb1f052b079d077b9e9",
            "title": "CrossTransformers: spatially-aware few-shot transfer"
        },
        {
            "paperId": "10d11f0045dc7f217c7f01bc6cbb47929e9b8808",
            "title": "Self-Supervised MultiModal Versatile Networks"
        },
        {
            "paperId": "3e86f5a0e2a97894de1cf1f1587799ac79bad0f2",
            "title": "VirTex: Learning Visual Representations from Textual Annotations"
        },
        {
            "paperId": "2f5f81bc516a6d085d39479378af1fc27104f91e",
            "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning"
        },
        {
            "paperId": "8cda672bd5487ec2c67d5c217dc84ed8fb786640",
            "title": "ActBERT: Learning Global-Local Video-Text Representations"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "title": "End-to-End Object Detection with Transformers"
        },
        {
            "paperId": "3f6570fd55dc5855f93a56150e6d99c7944a1c1e",
            "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes"
        },
        {
            "paperId": "5546e6073f3b82967b12c87d6b90ba722c4b85c6",
            "title": "Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training"
        },
        {
            "paperId": "06c7269c10125589d2599f684b751b1640f7a0cc",
            "title": "VD-BERT: A Unified Vision and Dialog Transformer with BERT"
        },
        {
            "paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0",
            "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"
        },
        {
            "paperId": "80455126562cfe6a483e02b3446a3f30b8e9f229",
            "title": "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?"
        },
        {
            "paperId": "e52051204cb1179584f3b008c9d38848b52c1f28",
            "title": "ReZero is All You Need: Fast Convergence at Large Depth"
        },
        {
            "paperId": "4243555758433880a67b15b50f752b1e2a8c4609",
            "title": "UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation"
        },
        {
            "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
            "title": "Scaling Laws for Neural Language Models"
        },
        {
            "paperId": "c82cad6b6ca4cb97bc2424ca207e2fb6200159eb",
            "title": "Diagnosing Gender Bias in Image Recognition Systems"
        },
        {
            "paperId": "9de403a58395a1b56bfceee6e009788c43db6d08",
            "title": "End-to-End Learning of Visual Representations From Uncurated Instructional Videos"
        },
        {
            "paperId": "21d4d97fed755a9ecd5b67a42e1c69008989738c",
            "title": "Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3",
            "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"
        },
        {
            "paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290",
            "title": "UNITER: UNiversal Image-TExt Representation Learning"
        },
        {
            "paperId": "6648b4db5f12c30941ea78c695e77aded19672bb",
            "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"
        },
        {
            "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
        },
        {
            "paperId": "4aa6298b606941a282d735fa3143da293199d2ca",
            "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"
        },
        {
            "paperId": "79c93274429d6355959f1e4374c2147bb81ea649",
            "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"
        },
        {
            "paperId": "4c163d4942117179d3e97182e1b280027d7d60a9",
            "title": "Attention on Attention for Image Captioning"
        },
        {
            "paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
            "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"
        },
        {
            "paperId": "d8bafd3a23c5ce9a7ebef036d5f2c67e1386ff11",
            "title": "Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes"
        },
        {
            "paperId": "c0aaee2337e5af680e5dca1bfc349a737dfec573",
            "title": "Fixing the train-test resolution discrepancy"
        },
        {
            "paperId": "9311779489e597315488749ee6c386bfa3f3512e",
            "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"
        },
        {
            "paperId": "7a86bdaf401943e36fe34f4461c7761a3e9b99e9",
            "title": "Does Object Recognition Work for Everyone?"
        },
        {
            "paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
            "title": "Energy and Policy Considerations for Deep Learning in NLP"
        },
        {
            "paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70",
            "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"
        },
        {
            "paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907",
            "title": "Towards VQA Models That Can Read"
        },
        {
            "paperId": "28b74bb7c8b08cceb2430ec2d54dfa0f3225d796",
            "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"
        },
        {
            "paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a",
            "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"
        },
        {
            "paperId": "eefede7052454b77b80d7695f29fa962c604986b",
            "title": "Doing more with less: meta-reasoning and meta-learning in humans and machines"
        },
        {
            "paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c",
            "title": "Parameter-Efficient Transfer Learning for NLP"
        },
        {
            "paperId": "2f240424ca0761d0549252dacfbbeece14bb3cb6",
            "title": "Fast Context Adaptation via Meta-Learning"
        },
        {
            "paperId": "7365f887c938ca21a6adbef08b5a520ebbd4638f",
            "title": "Model Cards for Model Reporting"
        },
        {
            "paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0",
            "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"
        },
        {
            "paperId": "7b0aad12a6917b7d444ba2f87c7f8ccc5357797a",
            "title": "Meta-Learning Probabilistic Inference for Prediction"
        },
        {
            "paperId": "208cd4b25768f0096fb2e80e7690473da0e2a563",
            "title": "Meta-learning with differentiable closed-form solvers"
        },
        {
            "paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7",
            "title": "Gender Bias in Coreference Resolution"
        },
        {
            "paperId": "b0c5dc3fa19a2bc97606ccb6f55226b913984395",
            "title": "Women also Snowboard: Overcoming Bias in Captioning Models"
        },
        {
            "paperId": "0df347f5e3118fac7c351917e3a497899b071d1e",
            "title": "Datasheets for datasets"
        },
        {
            "paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c",
            "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"
        },
        {
            "paperId": "18858cc936947fc96b5c06bbe3c6c2faa5614540",
            "title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"
        },
        {
            "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "title": "Universal Language Model Fine-tuning for Text Classification"
        },
        {
            "paperId": "057b80e235b10799d03876ad25465208a4c64caf",
            "title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "c269858a7bb34e8350f2442ccf37797856ae9bca",
            "title": "Prototypical Networks for Few-shot Learning"
        },
        {
            "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
        },
        {
            "paperId": "e10a5e0baf2aa87d804795af071808a9377cc80a",
            "title": "Towards Automatic Learning of Procedures From Web Instructional Videos"
        },
        {
            "paperId": "6c8353697cdbb98dfba4f493875778c4286d3e3a",
            "title": "Self-Critical Sequence Training for Image Captioning"
        },
        {
            "paperId": "1b80416cc2b05954941ac7e7dcbcc358c10e5ace",
            "title": "Visual Dialog"
        },
        {
            "paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "title": "Gaussian Error Linear Units (GELUs)"
        },
        {
            "paperId": "4423357dd21cc59662c6fabaf9839b15ef0fb8a8",
            "title": "Learning feed-forward one-shot learners"
        },
        {
            "paperId": "be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6",
            "title": "Matching Networks for One Shot Learning"
        },
        {
            "paperId": "7c7ab73469c25437332f5c1c1c5cb67c7b2f0855",
            "title": "Low-Shot Visual Recognition by Shrinking and Hallucinating Features"
        },
        {
            "paperId": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "title": "Exploring the Limits of Language Modeling"
        },
        {
            "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
            "title": "VQA: Visual Question Answering"
        },
        {
            "paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628",
            "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"
        },
        {
            "paperId": "f01fc808592ea7c473a69a6e7484040a435f36d9",
            "title": "Long-term recurrent convolutional networks for visual recognition and description"
        },
        {
            "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "title": "Show and tell: A neural image caption generator"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "44040913380206991b1991daf1192942e038fe31",
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"
        },
        {
            "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
            "title": "Generating Sequences With Recurrent Neural Networks"
        },
        {
            "paperId": "93c20e38c85b69fc2d2eb314b3c1217913f7db11",
            "title": "Generating Text with Recurrent Neural Networks"
        },
        {
            "paperId": "812355cec91fa30bb50e9e992a3549af39e4f6eb",
            "title": "One-shot learning of object categories"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "e6a6b66eeb506dc326e3c3f7f49a1f260469c281",
            "title": "VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training"
        },
        {
            "paperId": "ecce44df1956db4ec486539c6543345344809958",
            "title": "Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"
        },
        {
            "paperId": "53d8b356551a2361020a948f64454a6d599af69f",
            "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": null,
            "title": "Few-shot classification by recycling deep learning. Invited Talk at the S2D-OLAD Workshop"
        },
        {
            "paperId": null,
            "title": "Enhancing textual cues in multi-modal transformers for VQA. VizWiz Challenge"
        },
        {
            "paperId": null,
            "title": "Haiku: Sonnet for JAX"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "Row 3: First two images are provided under license by Unsplash"
        },
        {
            "paperId": null,
            "title": "JAX: composable transformations of Python+NumPy programs"
        },
        {
            "paperId": null,
            "title": "Optimization of image description metrics using policy gradient methods"
        },
        {
            "paperId": null,
            "title": "YFCC100M: The new data inmultimedia research"
        },
        {
            "paperId": "100a038fdf29b4b20801887f0ec40e3f10d9a4f9",
            "title": "One shot learning of simple visual concepts"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "a7d95a72a0c4f99a5d1a3eb99731a26e42c9e842",
            "title": "Categorization and Naming in Children: Problems of Induction"
        },
        {
            "paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1",
            "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"
        },
        {
            "paperId": "1f462943c8d0af69c12a09058251848324135e5a",
            "title": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
        },
        {
            "paperId": null,
            "title": "Vatex video captioning challenge 2020: Multi-view features and hybrid reward strategies for video captioning"
        },
        {
            "paperId": null,
            "title": "Metadataset: A dataset of datasets for learning"
        },
        {
            "paperId": null,
            "title": "Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes] Our data was automatically scraped from million of webpages"
        },
        {
            "paperId": null,
            "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"
        },
        {
            "paperId": null,
            "title": "e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]"
        },
        {
            "paperId": null,
            "title": "c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?"
        },
        {
            "paperId": null,
            "title": "(a) If your work uses existing assets, did you cite the creators? [Yes] We properly cited the prior methods on which our work is based, as well as prior datasets when appropriate"
        },
        {
            "paperId": null,
            "title": "If you used crowdsourcing or conducted research with human subjects."
        },
        {
            "paperId": null,
            "title": "Did you mention the license of the assets?"
        },
        {
            "paperId": null,
            "title": "The first icon is provided under license by Flaticon, the second image is provided under license by Unsplash, the third one is provided under license by Sketchfab"
        },
        {
            "paperId": null,
            "title": "All visuals are sourced from various sources including the COCO dataset"
        },
        {
            "paperId": null,
            "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"
        },
        {
            "paperId": null,
            "title": "a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"
        }
    ]
}