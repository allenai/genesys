{
    "paperId": "f1cbf097ce436f7304a1984f4a29ab41f75ebfe3",
    "externalIds": {
        "MAG": "2951126022",
        "DBLP": "journals/corr/abs-1711-02013",
        "ArXiv": "1711.02013",
        "CorpusId": 3347806
    },
    "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
    "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.",
    "venue": "International Conference on Learning Representations",
    "year": 2017,
    "referenceCount": 66,
    "citationCount": 168,
    "influentialCitationCount": 27,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model is proposed."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2305979",
            "name": "Yikang Shen"
        },
        {
            "authorId": "3146592",
            "name": "Zhouhan Lin"
        },
        {
            "authorId": "2144639252",
            "name": "Chin-Wei Huang"
        },
        {
            "authorId": "1760871",
            "name": "Aaron C. Courville"
        }
    ],
    "references": [
        {
            "paperId": "8f46c21fef31a4cdf7b1808e67171466a9317882",
            "title": "Do latent tree learning models identify meaningful structure in sentences?"
        },
        {
            "paperId": "cffe8c3f181b423999005b3d8120b17269db89ca",
            "title": "Learning to parse from a semantic objective: It works. Is it syntax?"
        },
        {
            "paperId": "2397ce306e5d7f3d0492276e357fb1833536b5d8",
            "title": "On the State of the Art of Evaluation in Neural Language Models"
        },
        {
            "paperId": "c79d8c3768b8dbde4e9fbbd8924805d4a02a1158",
            "title": "Sequence-to-Dependency Neural Machine Translation"
        },
        {
            "paperId": "29bda122ac5454879cb4248133cd6d5fbd5e59fd",
            "title": "Generative Neural Machine for Tree Structures"
        },
        {
            "paperId": "fce10a1a9727cbda33d44b62409e303f1009417a",
            "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?"
        },
        {
            "paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
            "title": "Improving Neural Language Models with a Continuous Cache"
        },
        {
            "paperId": "67d968c7450878190e45ac7886746de867bf673d",
            "title": "Neural Architecture Search with Reinforcement Learning"
        },
        {
            "paperId": "424aef7340ee618132cc3314669400e23ad910ba",
            "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"
        },
        {
            "paperId": "9c69926bdb72912725d20a55af7147f86bed01ae",
            "title": "Tree-structured decoding with doubly-recurrent neural networks"
        },
        {
            "paperId": "f735122727511fe7593401099012e34bf51df4e5",
            "title": "An Evaluation of Parser Robustness for Ungrammatical Sentences"
        },
        {
            "paperId": "563783de03452683a9206e85fe6d661714436686",
            "title": "HyperNetworks"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
            "title": "Hierarchical Multiscale Recurrent Neural Networks"
        },
        {
            "paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559",
            "title": "Using the Output Embedding to Improve Language Models"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
            "title": "Recurrent Highway Networks"
        },
        {
            "paperId": "136cf66392f1d6bf42da4cc070888996dc472b91",
            "title": "On Multiplicative Integration with Recurrent Neural Networks"
        },
        {
            "paperId": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105",
            "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"
        },
        {
            "paperId": "952454718139dba3aafc6b3b67c4f514ac3964af",
            "title": "Recurrent Batch Normalization"
        },
        {
            "paperId": "36c097a225a95735271960e2b63a2cb9e98bff83",
            "title": "A Fast Unified Model for Parsing and Sentence Understanding"
        },
        {
            "paperId": "7345843e87c81e24e42264859b214d26042f8d51",
            "title": "Recurrent Neural Network Grammars"
        },
        {
            "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
            "title": "Long Short-Term Memory-Networks for Machine Reading"
        },
        {
            "paperId": "f84d5add20d4df0a6c89c47a920354c272cbdbd8",
            "title": "Regularizing RNNs by Stabilizing Activations"
        },
        {
            "paperId": "e5f0762b9cfd07f88608f5502ed4467a8b5546cb",
            "title": "Top-down Tree Long Short-Term Memory Networks"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "d8ac015407cf68c695043b23d905cddd880e5844",
            "title": "Generative Incremental Dependency Parsing with Neural Networks"
        },
        {
            "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "title": "End-To-End Memory Networks"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "9665247ea3421929f9b6ad721f139f11edb1dbb8",
            "title": "Learning Longer Memory in Recurrent Neural Networks"
        },
        {
            "paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97",
            "title": "Recurrent Neural Network Regularization"
        },
        {
            "paperId": "193edd20cae92c6759c18ce93eeea96afd9528eb",
            "title": "Deep learning in neural networks: An overview"
        },
        {
            "paperId": "5522764282c85aea422f1c4dc92ff7e0ca6987bc",
            "title": "A Clockwork RNN"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "a5991db236c230b6e1dca0ddb8944cd478c20fa5",
            "title": "The Expressive Power of Word Embeddings"
        },
        {
            "paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4",
            "title": "Context dependent recurrent neural network language model"
        },
        {
            "paperId": "0c98d39127f334abe0e5915dbe9fdd00a9390917",
            "title": "Classifying Chart Cells for Quadratic Complexity Context-Free Inference"
        },
        {
            "paperId": "9762f5d2ca8c2132942d10ef6b064bf370262585",
            "title": "A Latent Variable Model for Generative Dependency Parsing"
        },
        {
            "paperId": "42a9c575acb53fac332993087c1e1dbcc8161ccd",
            "title": "An All-Subtrees Approach to Unsupervised Parsing"
        },
        {
            "paperId": "3b98dc29eb5f95470d12d19ae528674129ca0411",
            "title": "Natural language grammar induction with a generative constituent-context model"
        },
        {
            "paperId": "ffea7f0fd89dc940107cdf94f7decfcc42315c67",
            "title": "A Neural Syntactic Language Model"
        },
        {
            "paperId": "ca2858b2040724ae9f29ba601df12aae2e539596",
            "title": "Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency"
        },
        {
            "paperId": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "title": "Accurate Unlexicalized Parsing"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "77021fb48704b860fa850dd103b79db4dcf920ee",
            "title": "A Generative Constituent-Context Model for Improved Grammar Induction"
        },
        {
            "paperId": "771f5c0bf2540c71ad0ca05c2b9840ca7ef3850a",
            "title": "Unsupervised induction of stochastic context-free grammars using distributional clustering"
        },
        {
            "paperId": "436772d9a916f0382800cf18581cfdfd4f83c457",
            "title": "Immediate-Head Parsing for Language Models"
        },
        {
            "paperId": "e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3",
            "title": "Probabilistic Top-Down Parsing and Language Modeling"
        },
        {
            "paperId": "11e8957985944c72ff9c8e49c81c5831207a0689",
            "title": "Morphological structure, lexical representation and lexical access"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "1e3cd66275ba8b8a0cb6e3531a6bbf54ed93adb7",
            "title": "A Structured Language Model"
        },
        {
            "paperId": "b13813b49f160e1a2010c44bd4fb3d09a28446e3",
            "title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"
        },
        {
            "paperId": "063fe6ed19c0204d55bde174483c5a93eb4819c0",
            "title": "Learning long-term dependencies is not as difficult with NARX recurrent neural networks"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "eb34c9981c50bde33d165a7f5faeb72018aa4d09",
            "title": "Two Experiments on Learning Probabilistic Dependency Grammars from Corpora"
        },
        {
            "paperId": "50843155d395d3f93151652220ac618c9efa90e6",
            "title": "Twelve Years of Unsupervised Dependency Parsing"
        },
        {
            "paperId": "2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
            "title": "Deep Learning"
        },
        {
            "paperId": null,
            "title": "Aspects of the Theory of Syntax , volume 11"
        },
        {
            "paperId": null,
            "title": "Large text compression benchmark"
        },
        {
            "paperId": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb",
            "title": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"
        },
        {
            "paperId": "81b3b3fe994a9eda6d3f9d2149aa4492d1933975",
            "title": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
            "title": "Learning Deep Architectures for AI"
        },
        {
            "paperId": "0f8b27636c1cd458451400cf7a819f1f556ed84c",
            "title": "Automatic Acquisition and Efficient Representation of Syntactic Structures"
        },
        {
            "paperId": "c69201d091dd92699fd90a17b9e3407319726791",
            "title": "Neural sequence chunkers"
        }
    ]
}