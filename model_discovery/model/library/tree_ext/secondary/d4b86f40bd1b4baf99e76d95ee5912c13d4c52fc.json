{
    "paperId": "d4b86f40bd1b4baf99e76d95ee5912c13d4c52fc",
    "externalIds": {
        "MAG": "2890401883",
        "ArXiv": "1810.02309",
        "DBLP": "journals/corr/abs-1810-02309",
        "CorpusId": 52832817,
        "PubMed": "31130799"
    },
    "title": "Learning Compressed Transforms with Low Displacement Rank",
    "abstract": "The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a rich class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters.",
    "venue": "Neural Information Processing Systems",
    "year": 2018,
    "referenceCount": 53,
    "citationCount": 37,
    "influentialCitationCount": 5,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A rich class of LDR matrices with more general displacement operators is introduced, and explicitly learn over both the operators and the low-rank component, which exceeds the accuracy of existing compression approaches and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "49219898",
            "name": "Anna T. Thomas"
        },
        {
            "authorId": "39499001",
            "name": "Albert Gu"
        },
        {
            "authorId": "24593911",
            "name": "Tri Dao"
        },
        {
            "authorId": "1755572",
            "name": "A. Rudra"
        },
        {
            "authorId": "2114485554",
            "name": "C. R\u00e9"
        }
    ],
    "references": [
        {
            "paperId": "4c75b748911ddcd888c5122f7672f69caa5d661f",
            "title": "Statistical Learning Theory"
        },
        {
            "paperId": "84032c19bad3493957d1319babd19bde2821fee3",
            "title": "On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups"
        },
        {
            "paperId": "52ce57ccd25f30b9ff3b63eba8a78a430dff7a27",
            "title": "Learning Compact Neural Networks with Regularization"
        },
        {
            "paperId": "ce2845cadc5233ff0a647aa22ae3bbe646258890",
            "title": "Spherical CNNs"
        },
        {
            "paperId": "a4a09fadf38dd65daf98cf5d06d5b69f24546b0b",
            "title": "Non-Parametric Transformation Networks"
        },
        {
            "paperId": "bce22675d77e1ef28e92f3793c02f8f5ccdb0ddd",
            "title": "A Two-pronged Progress in Structured Dense Matrix Vector Multiplication"
        },
        {
            "paperId": "2c031aea0d2eec0ff0e1947bb3c94dc81b46e3a5",
            "title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-Circulant Weight Matrices"
        },
        {
            "paperId": "9b464642fabd44ca9891d9ef9cdbd324eb5878f4",
            "title": "Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks"
        },
        {
            "paperId": "8edbd132765e72f5887b7ef8c38624f31dd53a77",
            "title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks"
        },
        {
            "paperId": "983db08bf7f6c837616780098cb363fd435c4d5c",
            "title": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank"
        },
        {
            "paperId": "e1f2a547ca6aeac20765eabd11042dd2ae25d66d",
            "title": "Rotation Equivariant Vector Field Networks"
        },
        {
            "paperId": "0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "title": "Geometric Deep Learning: Going beyond Euclidean data"
        },
        {
            "paperId": "b1e2841edb7a854404835a38f5bd606434be74f0",
            "title": "Generalization Error of Invariant Classifiers"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "bf31e4aaa0f2f0cc1318d1742c60a409d68f12ce",
            "title": "Computational Methods for Linear Matrix Equations"
        },
        {
            "paperId": "d66298840e9c4268a4c1487c758dda8e54c61c85",
            "title": "Unsupervised learning of invariant representations"
        },
        {
            "paperId": "559faf4b49ef3f317e894f7ba3947d1769bdeb8c",
            "title": "Recycling Randomness with Structure for Sublinear time Kernel Expansions"
        },
        {
            "paperId": "369427f52ede6326b228b0e27f3e62292e0b0480",
            "title": "Learning compact recurrent neural networks"
        },
        {
            "paperId": "fafcaf5ca3fab8dc4fad15c2391c0fdb4a7dc005",
            "title": "Group Equivariant Convolutional Networks"
        },
        {
            "paperId": "b2e8f988e4c4ef0b38f6c8766aca7f2e5a2575db",
            "title": "Exploiting Cyclic Symmetry in Convolutional Neural Networks"
        },
        {
            "paperId": "54c3e878bf0ff2fdde16e439b5579ee99ee0d0d8",
            "title": "ACDC: A Structured Efficient Linear Layer"
        },
        {
            "paperId": "bf76be8df2f2bc56edac98a5d0dfc19c85882eaa",
            "title": "Structured Transforms for Small-Footprint Deep Learning"
        },
        {
            "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "title": "Learning both Weights and Connections for Efficient Neural Network"
        },
        {
            "paperId": "dbb6ded623159c867fbeca0772db7b2eb9489523",
            "title": "Spatial Transformer Networks"
        },
        {
            "paperId": "dfbfaaec46d38392f61d683c340ee92a0a66e5d9",
            "title": "Learning to See by Moving"
        },
        {
            "paperId": "efb5032e6199c80f83309fd866b25be9545831fd",
            "title": "Compressing Neural Networks with the Hashing Trick"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "5934400081d9541339da0f16d2613263f1a4c2a2",
            "title": "An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections"
        },
        {
            "paperId": "27a99c21a1324f087b2f144adc119f04137dfd87",
            "title": "Deep Fried Convnets"
        },
        {
            "paperId": "4f939bb8dd6a660a8f6851bf8ad2ce8a55c974ae",
            "title": "Deep Symmetry Networks"
        },
        {
            "paperId": "371400e61632592146f40b621fb3dbb6971721be",
            "title": "Understanding Image Representations by Measuring Their Equivariance and Equivalence"
        },
        {
            "paperId": "eff61216e0136886e1158625b1e5a88ed1a7cbce",
            "title": "Predicting Parameters in Deep Learning"
        },
        {
            "paperId": "8ce9de8009d7186a16804e55ad7d2d8ce595d350",
            "title": "Fastfood - Computing Hilbert Space Expansions in loglinear time"
        },
        {
            "paperId": "5cea23330c76994cb626df20bed31cc2588033df",
            "title": "Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets"
        },
        {
            "paperId": "23593d08d7fe3b87e59343f194cf72a092d36304",
            "title": "Learning rotation-aware features: From invariant priors to equivariant descriptors"
        },
        {
            "paperId": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "title": "An empirical evaluation of deep architectures on problems with many factors of variation"
        },
        {
            "paperId": "f354310098e09c1e1dc88758fca36767fd9d084d",
            "title": "Learning methods for generic object recognition with invariance to pose and lighting"
        },
        {
            "paperId": "4a6b67b40b84c647916faa0a5e6ee3023f65b085",
            "title": "Symmetry-based matrix factorization"
        },
        {
            "paperId": "fbeb8dd4d5e9a8187e5f32e0027df3728236302b",
            "title": "Automatic generation of fast discrete signal transforms"
        },
        {
            "paperId": "da07bb3688f5ec5b42ee19fb06270a68be6ac46a",
            "title": "Neural Network Learning: Theoretical Foundations"
        },
        {
            "paperId": "fe5e6b810957da9bc10b099b00fd64c221d97961",
            "title": "Almost Linear VC-Dimension Bounds for Piecewise Polynomial Networks"
        },
        {
            "paperId": "54946e3ef95b6eb93d89346a768a9c73053870d0",
            "title": "Algebraic complexity theory"
        },
        {
            "paperId": "26108e32b0891ebfff74276e8b7ea1301b1de1a3",
            "title": "Algebraic Complexity Theory"
        },
        {
            "paperId": "54fae67c6386e4a351c0d38421a5136de2bb8556",
            "title": "Learning, invariance, and generalization in high-order neural networks."
        },
        {
            "paperId": "57923211710a9684f7156b8dc6ebd8dc0dba0e55",
            "title": "An Introduction to Orthogonal Polynomials"
        },
        {
            "paperId": "7306af13eb2052fe9b9652c7d8b669655d307635",
            "title": "Displacement ranks of matrices and linear equations"
        },
        {
            "paperId": "ee34b5cc38241ed5eb39d08f2eea322469103471",
            "title": "\"PROOF\""
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": "7c9411bc50b6a33947575f37ae16931578beae23",
            "title": "Structured Matrices and Polynomials: Unified Superfast Algorithms"
        },
        {
            "paperId": "cb11771cbfd45aa09e00f1f3b0fc156a087fa544",
            "title": "Evaluating derivatives - principles and techniques of algorithmic differentiation, Second Edition"
        },
        {
            "paperId": "06c126b1238dc11cc1e36a3b876dab4044637fa1",
            "title": "Lower bounds for approximation by nonlinear manifolds"
        },
        {
            "paperId": "5a47ba057a858f8c024d2518cc3731fc7eb40de1",
            "title": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence Flexible, High Performance Convolutional Neural Networks for Image Classification"
        },
        {
            "paperId": "dbb42e49583829a013d72a3f918a07b2c16239ae",
            "title": "c \u25cb 2003 Society for Industrial and Applied Mathematics INVERSION OF DISPLACEMENT OPERATORS \u2217"
        }
    ]
}