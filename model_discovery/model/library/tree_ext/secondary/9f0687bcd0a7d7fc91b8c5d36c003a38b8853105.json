{
    "paperId": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105",
    "externalIds": {
        "ArXiv": "1606.01305",
        "MAG": "2409027918",
        "DBLP": "conf/iclr/KruegerMKPBKGBC17",
        "CorpusId": 12200521
    },
    "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
    "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.",
    "venue": "International Conference on Learning Representations",
    "year": 2016,
    "referenceCount": 42,
    "citationCount": 312,
    "influentialCitationCount": 41,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes zoneout, a novel method for regularizing RNNs that uses random noise to train a pseudo-ensemble, improving generalization and performs an empirical investigation of various RNN regularizers, and finds that zoneout gives significant performance improvements across tasks."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "145055042",
            "name": "David Krueger"
        },
        {
            "authorId": "3422058",
            "name": "Tegan Maharaj"
        },
        {
            "authorId": "2064949280",
            "name": "J'anos Kram'ar"
        },
        {
            "authorId": "145507036",
            "name": "M. Pezeshki"
        },
        {
            "authorId": "2482072",
            "name": "Nicolas Ballas"
        },
        {
            "authorId": "145604319",
            "name": "Nan Rosemary Ke"
        },
        {
            "authorId": "1996705",
            "name": "Anirudh Goyal"
        },
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        },
        {
            "authorId": "1777528",
            "name": "H. Larochelle"
        },
        {
            "authorId": "1760871",
            "name": "Aaron C. Courville"
        },
        {
            "authorId": "1972076",
            "name": "C. Pal"
        }
    ],
    "references": [
        {
            "paperId": "0a46e856ded34fbf6a5a8667bf8f9f376118d382",
            "title": "Test Data"
        },
        {
            "paperId": "3b3584f38107f2490baaa4a232f4d675c4ce993e",
            "title": "Surprisal-Driven Zoneout"
        },
        {
            "paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
            "title": "Hierarchical Multiscale Recurrent Neural Networks"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "ff6548fcecb00b2db0bc20567339a0be9fe70b39",
            "title": "Swapout: Learning an ensemble of deep architectures"
        },
        {
            "paperId": "6b570069f14c7588e066f7138e1f21af59d62e61",
            "title": "Theano: A Python framework for fast computation of mathematical expressions"
        },
        {
            "paperId": "952454718139dba3aafc6b3b67c4f514ac3964af",
            "title": "Recurrent Batch Normalization"
        },
        {
            "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
            "title": "Deep Networks with Stochastic Depth"
        },
        {
            "paperId": "cf76789618f5db929393c1187514ce6c3502c3cd",
            "title": "Recurrent Dropout without Memory Loss"
        },
        {
            "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
            "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "4a5e68033940785468cbe34bd6249106c311acfb",
            "title": "RNNDROP: A novel dropout for RNNS in ASR"
        },
        {
            "paperId": "f84d5add20d4df0a6c89c47a920354c272cbdbd8",
            "title": "Regularizing RNNs by Stabilizing Activations"
        },
        {
            "paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec",
            "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "97acdfb3d247f8250d865ef8a9169f06e40f138b",
            "title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding"
        },
        {
            "paperId": "59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e",
            "title": "Blocks and Fuel: Frameworks for deep learning"
        },
        {
            "paperId": "d46b81707786d18499f911b4ab72bb10c65406ba",
            "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units"
        },
        {
            "paperId": "5f425b7abf2ed3172ed060df85bb1885860a297e",
            "title": "Describing Videos by Exploiting Temporal Structure"
        },
        {
            "paperId": "9665247ea3421929f9b6ad721f139f11edb1dbb8",
            "title": "Learning Longer Memory in Recurrent Neural Networks"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "03cd6f2297637a322bdd4519b8cee331ef42984b",
            "title": "Learning with Pseudo-Ensembles"
        },
        {
            "paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97",
            "title": "Recurrent Neural Network Regularization"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "5522764282c85aea422f1c4dc92ff7e0ca6987bc",
            "title": "A Clockwork RNN"
        },
        {
            "paperId": "533ee188324b833e059cb59b654e6160776d5812",
            "title": "How to Construct Deep Recurrent Neural Networks"
        },
        {
            "paperId": "c0b624c46b51920dfec5aa02cc86323c0beb0df5",
            "title": "Dropout Improves Recurrent Neural Networks for Handwriting Recognition"
        },
        {
            "paperId": "ba0c34d72dd42ef78c1f1514d92c2c22b9c0d454",
            "title": "On Fast Dropout and its Applicability to Recurrent Networks"
        },
        {
            "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
            "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
        },
        {
            "paperId": "ec92efde21707ddf4b81f301cd58e2051c1a2443",
            "title": "Fast dropout training"
        },
        {
            "paperId": "c5145b1d15fea9340840cc8bb6f0e46e8934827f",
            "title": "Understanding the exploding gradient problem"
        },
        {
            "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
            "title": "Improving neural networks by preventing co-adaptation of feature detectors"
        },
        {
            "paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "title": "Learning to Forget: Continual Prediction with LSTM"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "b13813b49f160e1a2010c44bd4fb3d09a28446e3",
            "title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"
        },
        {
            "paperId": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "title": "Learning long-term dependencies with gradient descent is difficult"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "e4351041d25c272a008bcd5765868dc3a28fe470",
            "title": "Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": null,
            "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude"
        },
        {
            "paperId": null,
            "title": "Hierarchical recurrent neural networks for long-term depen- dencies URL http://papers.nips.cc/paper/ 1102-hierarchical-recurrent-neural-networks-for-long-term-dependencies.pdf"
        },
        {
            "paperId": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "title": "Untersuchungen zu dynamischen neuronalen Netzen"
        },
        {
            "paperId": null,
            "title": "Untersuchungen zu dynamischen neuronalen netzen. Master's thesis"
        }
    ]
}