{
    "paperId": "65c0712514efa9139fce00bb17362c6cc0779950",
    "externalIds": {
        "CorpusId": 1308459
    },
    "title": "Learning Long-Term Dependencies with",
    "abstract": "Recurrent neural networks can be used to map input sequences to output sequences , such as for recognition, production or prediction problems. However, practical diiculties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly diicult problem as the duration of the dependencies to be captured increases. These results expose a trade-oo between eecient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.",
    "venue": "",
    "year": 2007,
    "referenceCount": 24,
    "citationCount": 66,
    "influentialCitationCount": 3,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work shows why gradient based learning algorithms face an increasingly diicult problem as the duration of the dependencies to be captured increases, and exposes a trade-oo between eecient learning by gradient descent and latching on information for long periods."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2087902035",
            "name": "Patrice Simardy"
        },
        {
            "authorId": "2087902026",
            "name": "Paolo FrasconizyAT"
        }
    ],
    "references": [
        {
            "paperId": "3f1d04f57e420f0f1b2cd059deab309bc7073ca1",
            "title": "The problem of learning long-term dependencies in recurrent networks"
        },
        {
            "paperId": "b107cfe948f5d3c6e00b7cf1ebeacadd40224e0d",
            "title": "Inserting rules into recurrent neural networks"
        },
        {
            "paperId": "fee5cc60c185e5d2942fd925bbde612f368ea7ef",
            "title": "Using random weights to train multilayer networks of hard-limiting units"
        },
        {
            "paperId": "9a607334c1d963b4af29676578e1ef6aa11ba6e7",
            "title": "Local Feedback Multilayered Networks"
        },
        {
            "paperId": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "title": "Induction of Multiscale Temporal Structure"
        },
        {
            "paperId": "e9fd22ab8679f9d83fd9780570d7c95c3a3d996d",
            "title": "Nonlinear dynamics and stability of analog neural networks"
        },
        {
            "paperId": "3c92ffacba4eaddcb86c8fde50d1b183ac995052",
            "title": "Global optimization of a neural network-hidden Markov model hybrid"
        },
        {
            "paperId": "d6deb1ddc764259fbdc7733ef80473081bff31d5",
            "title": "Learning by Choice of Internal Representations"
        },
        {
            "paperId": "8f0d57ada00d761b380cd60b9db3860ea6059866",
            "title": "Minimizing multimodal functions of continuous variables with the \u201csimulated annealing\u201d algorithmCorrigenda for this article is available here"
        },
        {
            "paperId": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "title": "Optimization by Simulated Annealing"
        },
        {
            "paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7",
            "title": "In Advances in Neural Information Processing Systems"
        },
        {
            "paperId": "8b35b1ac7a52100a23aa095502162f2eb89b4323",
            "title": "A method of training multi-layer networks with heaviside characteristics using internal representations"
        },
        {
            "paperId": "cde2937cb41cf461f624ded2012743ac4624eca8",
            "title": "Artificial neural networks and their application to sequence recognition"
        },
        {
            "paperId": "589d377b23e2bdae7ad161b36a5d6613bcfccdde",
            "title": "Improving the convergence of back-propagation learning with second-order methods"
        },
        {
            "paperId": "7fb4d10f6d2ee3133135958aefd50bf22dcced9d",
            "title": "A Focused Backpropagation Algorithm for Temporal Pattern Recognition"
        },
        {
            "paperId": "7ab73467f48f9c453be07c389c5dd1d1bfa70f25",
            "title": "BPS: a learning algorithm for capturing the dynamic nature of speech"
        },
        {
            "paperId": null,
            "title": "\\A learning algorithm for continuously running fully recurrent neural networks"
        },
        {
            "paperId": null,
            "title": "\\The development of the Time-Delay Neural Network architecture for speech recognition"
        },
        {
            "paperId": null,
            "title": "\\A rst look at phonetic discrimination using connectionist models with recurrent links"
        },
        {
            "paperId": "56b771c4c3a54910dc3e7ff838940de89ed282db",
            "title": "Learning processes in an asymmetric threshold network"
        },
        {
            "paperId": null,
            "title": "Learning internal representation  by error propagation,\" Parallel Distributed Processing volume 1"
        },
        {
            "paperId": null,
            "title": "Iterative Solution of Non-linear Equations in Several Variables and Systems of Equations"
        },
        {
            "paperId": "12b03af504d0960334c77567dab38791bf0f739a",
            "title": "AND T"
        },
        {
            "paperId": null,
            "title": "\\Uniied Integration of Explicit Rules and Learning by Example in Recurrent Networks"
        }
    ]
}