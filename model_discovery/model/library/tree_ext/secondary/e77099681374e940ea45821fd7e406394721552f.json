{
    "paperId": "e77099681374e940ea45821fd7e406394721552f",
    "externalIds": {
        "ArXiv": "1805.10369",
        "MAG": "2963575866",
        "DBLP": "conf/iclr/MillerH19",
        "CorpusId": 58981389
    },
    "title": "Stable Recurrent Models",
    "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 34,
    "citationCount": 109,
    "influentialCitationCount": 13,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Theoretically, stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent and it is demonstrated stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1489216270",
            "name": "John Miller"
        },
        {
            "authorId": "1775622",
            "name": "Moritz Hardt"
        }
    ],
    "references": [
        {
            "paperId": "aa6c2814ff94ca098d90f188f95126b5b06ebb69",
            "title": "Nonlinear Programming"
        },
        {
            "paperId": "f7c3076a4d064de2ee55cd8a4bb28e3f8dfeee9f",
            "title": "FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network"
        },
        {
            "paperId": "0a065f42ccbb3bf54ddcb5e1963c6694f0de8d2c",
            "title": "Stochastic Gradient Descent Learns State Equations with Nonlinear Activations"
        },
        {
            "paperId": "921196c32213a229245a9705ee4768bc941e7a26",
            "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"
        },
        {
            "paperId": "956d8106a84e6e1d0d7c58df722c68e30f4e8e43",
            "title": "Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization"
        },
        {
            "paperId": "58c6f890a1ae372958b7decf56132fe258152722",
            "title": "Regularizing and Optimizing LSTM Language Models"
        },
        {
            "paperId": "851d89b4ecd55237ea6456b9606c0cc883b43b10",
            "title": "Non-Asymptotic Analysis of Robust Control from Coarse-Grained Identification"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "ba3971f737618edc9e697357ce99d9a07023267d",
            "title": "Kronecker Recurrent Units"
        },
        {
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning"
        },
        {
            "paperId": "9d1fb89b99aafc01ee56fc92df1ad150fba67c22",
            "title": "On orthogonality and learning recurrent networks with long term dependencies"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "1d782819afafe0d391e5b67151cb510e621f243d",
            "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs"
        },
        {
            "paperId": "6e99f4859eb420ace7f03f098940135c1c355075",
            "title": "Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections"
        },
        {
            "paperId": "f451d0212e65ab9970a58b584b3363a853c9811c",
            "title": "A recurrent neural network without chaos"
        },
        {
            "paperId": "79c78c98ea317ba8cdf25a9783ef4b8a7552db75",
            "title": "Full-Capacity Unitary Recurrent Neural Networks"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "8a765725a44b91b60d414551dc555175cfff3cd9",
            "title": "Gradient Descent Learns Linear Dynamical Systems"
        },
        {
            "paperId": "df0402517a7338ae28bc54acaac400de6b456a46",
            "title": "WaveNet: A Generative Model for Raw Audio"
        },
        {
            "paperId": "aa88701fa7ed699a04a428e9371b2462354ecc68",
            "title": "Training Input-Output Recurrent Neural Networks through Spectral Methods"
        },
        {
            "paperId": "e9c771197a6564762754e48c1daafb066f449f2e",
            "title": "Unitary Evolution Recurrent Neural Networks"
        },
        {
            "paperId": "0f7c85357c366b314b5b55c400869a62fd23372c",
            "title": "Train faster, generalize better: Stability of stochastic gradient descent"
        },
        {
            "paperId": "b2b6b4d1d201dbddbde5c2a213404a2309bc42aa",
            "title": "Using recurrent neural networks for slot filling in spoken language understanding"
        },
        {
            "paperId": "3b9484449d77317ca1cb6a6c44c50c99879a8f0e",
            "title": "Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "0f3bd4e6b16817e332b9be8357e3eddb8a461990",
            "title": "Harmonising Chorales by Probabilistic Inference"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "42fb61d4b818848f552dd90ac7aa78324187e7d7",
            "title": "Absolute stability conditions for discrete-time recurrent neural networks"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "8df509919b31397e225280962c59384fbe83144e",
            "title": "Evaluation of Spoken Language Systems: the ATIS Domain"
        },
        {
            "paperId": null,
            "title": "wtrunc \u2212 wrecurr\u2016 \u2264 2\u03bb. Stable vs. unstable models. The word and character level language modeling experiments are based on publically available code from Merity et al"
        },
        {
            "paperId": null,
            "title": "The input-hidden forget gate matrix U f should satisfy (cid:107) U f (cid:107) \u221e \u2264 0 . 25. This is enforced by normalizing the (cid:96) 1 - norm of each row to have value at most 0 . 25"
        },
        {
            "paperId": null,
            "title": "is stable if for all x, h, h (cid:48) , we"
        },
        {
            "paperId": null,
            "title": "The input vectors x t must satisfy (cid:107) x t (cid:107) \u221e \u2264 B x = 0 . 75, which is achieved by thresholding all values to lie in [ 0 . 75 , 0 . 75]"
        }
    ]
}