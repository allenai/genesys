{
    "paperId": "188e247506ad992b8bc62d6c74789e89891a984f",
    "externalIds": {
        "MAG": "2097998348",
        "DBLP": "journals/jmlr/BergstraB12",
        "DOI": "10.5555/2503308.2188395",
        "CorpusId": 15700257
    },
    "title": "Random Search for Hyper-Parameter Optimization",
    "abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.",
    "venue": "Journal of machine learning research",
    "year": 2012,
    "referenceCount": 36,
    "citationCount": 8341,
    "influentialCitationCount": 424,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid, and shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper- parameter optimization algorithms."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "32837403",
            "name": "J. Bergstra"
        },
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        }
    ],
    "references": [
        {
            "paperId": "273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "title": "LIBSVM: A library for support vector machines"
        },
        {
            "paperId": "73bbaf200e38a3a684ad6329ef11221b93bb7280",
            "title": "Parameter Screening and Optimisation for ILP using Designed Experiments"
        },
        {
            "paperId": "728744423ff0fb7e327664ed4e6352a95bb6c893",
            "title": "Sequential Model-Based Optimization for General Algorithm Configuration"
        },
        {
            "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
            "paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
            "title": "Why Does Unsupervised Pre-training Help Deep Learning?"
        },
        {
            "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
            "title": "Extracting and composing robust features with denoising autoencoders"
        },
        {
            "paperId": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "title": "An empirical evaluation of deep architectures on problems with many factors of variation"
        },
        {
            "paperId": "1f4b6cb09c1ec7833cd84e7360e0160524dfd6dd",
            "title": "Quasi-Monte Carlo strategies for stochastic optimization"
        },
        {
            "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "title": "A Fast Learning Algorithm for Deep Belief Nets"
        },
        {
            "paperId": "6f08a8d26f8567e8c6f04a7cf628e43f04756292",
            "title": "Response Surface Methodology for Optimizing Hyper Parameters"
        },
        {
            "paperId": "554894f70b28dba58b396c2d84080ac01051261b",
            "title": "Gaussian Processes For Machine Learning"
        },
        {
            "paperId": "26afab5607f4bfaf2fb9f786e4ed4f2d93c88e84",
            "title": "Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES)"
        },
        {
            "paperId": "530b5a1998ad7e291cb7f47a3a67b30d1e11892e",
            "title": "GNU Scientific Library Reference Manual - Third Edition"
        },
        {
            "paperId": "7e0be2938e25d52eeb47c43584e1fecfcc0efe51",
            "title": "A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output From a Computer Code"
        },
        {
            "paperId": "b82001c7398f8178ce063cd91c1c98d5258d1927",
            "title": "Simulation-Based Optimization with Stochastic Approximation Using Common Random Numbers"
        },
        {
            "paperId": "688df9bbf469ae877e4cd995fb5b6bbd4106ea76",
            "title": "Implementation and tests of low-discrepancy sequences"
        },
        {
            "paperId": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "title": "Optimization by Simulated Annealing"
        },
        {
            "paperId": "1729f731482a628177a0fb81050966514c385e5e",
            "title": "Adaptive Control Processes: A Guided Tour"
        },
        {
            "paperId": "67f20264565f5f46dcb5d11eae4c26a0b28f8d1d",
            "title": "On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals"
        },
        {
            "paperId": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
            "title": "Efficient BackProp"
        },
        {
            "paperId": "e95d3934e51107da7610acd0b1bcb6551671f9f1",
            "title": "A Practical Guide to Training Restricted Boltzmann Machines"
        },
        {
            "paperId": "3762dad3280acf61da0508df96103967bdb6cd76",
            "title": "Global Optimization Algorithms -- Theory and Application"
        },
        {
            "paperId": "08c4fdd974d874c87ea87faa6b404a7b8eb72c73",
            "title": "Automated configuration of algorithms for solving hard computational problems"
        },
        {
            "paperId": null,
            "title": "RANDOM SEARCH FOR HYPER-PARAMETER OPTIMIZATION"
        },
        {
            "paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
            "title": "Learning Deep Architectures for AI"
        },
        {
            "paperId": "f484a03e1bab2bf059ae0b85d3d20fc9b3f59c4a",
            "title": "Choosing search heuristics by non-stationary reinforcement learning"
        },
        {
            "paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "title": "Gradient-based learning applied to document recognition"
        },
        {
            "paperId": "86153ffe37063d967a1128674db44a166e6a11b6",
            "title": "Assessing Relevance determination methods using DELVE"
        },
        {
            "paperId": "6ac7497e941375197034be07842f1dd38c9f95e3",
            "title": "Valuation of mortgage-backed securities using Brownian bridges to reduce effective dimension"
        },
        {
            "paperId": "38bcb711e38e1c702d4c1851461708bd32970394",
            "title": "A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation"
        },
        {
            "paperId": "83b6755242f1cff6bacf270f65b4626d4d118f32",
            "title": "Neural Networks for Pattern Recognition"
        },
        {
            "paperId": "74f2f96339950aa6936af83dcd398ff06e0a2f61",
            "title": "An economic method of computing LP\u03c4-sequences"
        },
        {
            "paperId": "0acf50ce5c4e1268742f31e98ed294b8c967b829",
            "title": "Rechenberg, Ingo, Evolutionsstrategie \u2014 Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. 170 S. mit 36 Abb. Frommann\u2010Holzboog\u2010Verlag. Stuttgart 1973. Broschiert"
        },
        {
            "paperId": "d04942a086f9cafbb1c6453b64ba188beeb03823",
            "title": "Evolutionsstrategie : Optimierung technischer Systeme nach Prinzipien der biologischen Evolution"
        },
        {
            "paperId": "017ddb7e815236defd0566bc46f6ed8401cc6ba6",
            "title": "A Simplex Method for Function Minimization"
        },
        {
            "paperId": null,
            "title": "Theano: a CPU and GPU math expression compiler"
        }
    ]
}