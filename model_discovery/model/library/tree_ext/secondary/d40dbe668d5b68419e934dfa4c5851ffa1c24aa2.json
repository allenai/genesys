{
    "paperId": "d40dbe668d5b68419e934dfa4c5851ffa1c24aa2",
    "externalIds": {
        "DBLP": "journals/corr/abs-2306-00946",
        "ArXiv": "2306.00946",
        "DOI": "10.48550/arXiv.2306.00946",
        "CorpusId": 258999835
    },
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "abstract": "Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 108,
    "citationCount": 31,
    "influentialCitationCount": 2,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2306.00946",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning, and hypothesizes that attention glitches account for some of the closed-domain hallucinations in natural LLMs."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.249566078186035,
            -0.7636404037475586,
            1.820900559425354,
            3.0275778770446777,
            4.055878162384033,
            0.8440786600112915,
            0.7818271517753601,
            -0.6178518533706665,
            -0.23579764366149902,
            2.887382984161377,
            -0.8740534782409668,
            3.180267572402954,
            2.819995403289795,
            1.7815039157867432,
            -4.009286403656006,
            0.14114049077033997,
            -1.2117805480957031,
            0.625723659992218,
            3.3822853565216064,
            5.828580856323242,
            0.4336484968662262,
            -0.6953927278518677,
            -1.9025611877441406,
            -0.3591427206993103,
            -1.1788427829742432,
            -1.3192973136901855,
            3.890378475189209,
            -0.3473191559314728,
            -5.286280632019043,
            7.112420082092285,
            -0.1699383705854416,
            -4.453776836395264,
            4.015839576721191,
            -5.481706142425537,
            1.210016131401062,
            -1.924226999282837,
            -0.6532840728759766,
            8.31590747833252,
            -2.751627206802368,
            -0.3390635848045349,
            -3.3257029056549072,
            0.6982364058494568,
            -0.3889835774898529,
            -1.764773964881897,
            0.6539584994316101,
            2.0728743076324463,
            3.501041889190674,
            2.1821136474609375,
            -1.5180695056915283,
            4.518274784088135,
            6.163352966308594,
            1.0453455448150635,
            -1.3145431280136108,
            1.6924855709075928,
            0.7168875932693481,
            -1.5782804489135742,
            1.5742436647415161,
            -2.238856077194214,
            -0.22320818901062012,
            -2.6543326377868652,
            3.922882080078125,
            6.00665283203125,
            1.034420132637024,
            -1.5992611646652222,
            2.730900526046753,
            -5.371588230133057,
            -2.772474527359009,
            1.4844882488250732,
            4.142519474029541,
            0.6546691656112671,
            3.2593252658843994,
            -4.468106269836426,
            1.2078007459640503,
            0.09012210369110107,
            0.7477223873138428,
            1.7761836051940918,
            0.03011249005794525,
            -6.4150896072387695,
            -1.0490798950195312,
            -2.2302608489990234,
            -0.39681482315063477,
            1.7528584003448486,
            -1.001761555671692,
            5.687635898590088,
            0.5480155944824219,
            -1.4680336713790894,
            -3.733246326446533,
            -2.393244743347168,
            3.5878710746765137,
            0.15206055343151093,
            -0.8555004596710205,
            2.4932310581207275,
            -1.2086467742919922,
            3.6063685417175293,
            -5.68233585357666,
            -0.04527004063129425,
            0.9584670066833496,
            -1.7481849193572998,
            -1.4315656423568726,
            -1.4115054607391357,
            3.76603102684021,
            -1.3102093935012817,
            5.944418907165527,
            -1.0980722904205322,
            5.486589431762695,
            -3.3592045307159424,
            -0.010351061820983887,
            2.8807783126831055,
            2.554628372192383,
            -2.7554330825805664,
            -3.99662446975708,
            1.08230459690094,
            -1.4774495363235474,
            -0.7919172048568726,
            2.0525901317596436,
            -0.7958632707595825,
            -1.984755516052246,
            -1.1709012985229492,
            -0.8784531950950623,
            4.265535354614258,
            -5.122432708740234,
            -3.167210578918457,
            0.4847506284713745,
            -0.9834705591201782,
            -1.4925620555877686,
            -0.7207823991775513,
            -0.7435238361358643,
            -0.030195236206054688,
            1.6541520357131958,
            -1.8744916915893555,
            3.5856497287750244,
            -1.9390329122543335,
            3.824836254119873,
            -3.8242101669311523,
            -0.25091251730918884,
            4.259008407592773,
            -3.7790908813476562,
            3.264702558517456,
            -1.0053502321243286,
            -1.2314330339431763,
            0.742924690246582,
            4.864189624786377,
            -0.9451031684875488,
            -0.2564399540424347,
            0.6556896567344666,
            4.515647888183594,
            2.258124351501465,
            5.147233963012695,
            0.9963640570640564,
            7.300335884094238,
            4.113394260406494,
            -4.824987411499023,
            2.5529251098632812,
            -0.2261480689048767,
            -3.137667655944824,
            1.4889774322509766,
            -2.00205397605896,
            3.366806983947754,
            -2.064547538757324,
            0.45072242617607117,
            -0.2257128655910492,
            -4.415925979614258,
            -8.950200080871582,
            -4.199638843536377,
            2.286362409591675,
            -6.08317232131958,
            -2.6978871822357178,
            0.6106577515602112,
            -1.2269201278686523,
            6.067032814025879,
            -1.9337973594665527,
            3.360874652862549,
            0.9897850751876831,
            3.8721275329589844,
            5.053696632385254,
            3.336970567703247,
            0.9222553372383118,
            -3.572150707244873,
            -2.0810060501098633,
            -0.9930967092514038,
            0.12410920858383179,
            -4.21038293838501,
            -6.171573162078857,
            2.096890926361084,
            -2.9714505672454834,
            -5.0865020751953125,
            -2.8298146724700928,
            -1.6347440481185913,
            -3.6373863220214844,
            0.6124024987220764,
            -4.619655609130859,
            1.2613930702209473,
            5.804216384887695,
            6.460877895355225,
            1.2768433094024658,
            0.054354846477508545,
            6.660699844360352,
            1.5582470893859863,
            -3.235611915588379,
            -2.0662918090820312,
            0.17349782586097717,
            0.7566914558410645,
            -1.9417871236801147,
            0.9327350854873657,
            5.523496627807617,
            0.335104376077652,
            -3.9979848861694336,
            3.3234663009643555,
            2.3554868698120117,
            1.8159650564193726,
            2.6030571460723877,
            2.4062023162841797,
            -4.814667701721191,
            0.09810465574264526,
            -1.8790769577026367,
            -4.367299556732178,
            -9.493803024291992,
            1.1571375131607056,
            1.6954432725906372,
            2.601750373840332,
            -0.015489578247070312,
            0.02260459214448929,
            1.9735972881317139,
            -1.9806203842163086,
            -0.06872829794883728,
            0.23575742542743683,
            2.6871414184570312,
            -2.856262683868408,
            3.1145129203796387,
            -1.6037241220474243,
            -3.0497984886169434,
            -1.3202714920043945,
            -0.40233445167541504,
            0.05453021824359894,
            -5.208545684814453,
            2.1587774753570557,
            -3.337731122970581,
            1.5021144151687622,
            -1.47910737991333,
            -4.380168437957764,
            2.4315478801727295,
            -0.2191162109375,
            -3.3319804668426514,
            4.597085952758789,
            4.20466423034668,
            -1.203464388847351,
            -3.5334830284118652,
            2.7508082389831543,
            -3.0279603004455566,
            0.5708737373352051,
            -1.6747775077819824,
            -1.5394376516342163,
            4.867025375366211,
            -0.6131025552749634,
            3.3123250007629395,
            3.509340286254883,
            -1.639268398284912,
            -1.1313247680664062,
            0.01421181857585907,
            -2.2666068077087402,
            0.04247039556503296,
            0.5169621109962463,
            1.3963408470153809,
            -0.9969118237495422,
            -0.49929875135421753,
            -1.595353126525879,
            -5.907652854919434,
            -2.323937177658081,
            -1.4261140823364258,
            2.8107786178588867,
            4.303832530975342,
            -0.27620118856430054,
            -1.5690726041793823,
            -5.228756427764893,
            0.5240857005119324,
            -4.485657215118408,
            -0.1487656682729721,
            -2.571481466293335,
            1.1485369205474854,
            6.885734558105469,
            1.596538782119751,
            -3.4859752655029297,
            -1.084010362625122,
            0.7078296542167664,
            -1.5413522720336914,
            -3.0975780487060547,
            1.4678817987442017,
            1.3860899209976196,
            0.7836052179336548,
            0.020429976284503937,
            -0.8423831462860107,
            4.456111907958984,
            -6.336668014526367,
            -1.74971604347229,
            -1.4827247858047485,
            2.336374044418335,
            6.179039001464844,
            -4.615353584289551,
            -1.4065712690353394,
            -3.2986960411071777,
            -0.5827213525772095,
            0.19541087746620178,
            2.729548692703247,
            -0.7682689428329468,
            -1.7747435569763184,
            2.8022985458374023,
            -3.4659934043884277,
            -4.893283843994141,
            -1.5155681371688843,
            -3.2789974212646484,
            -0.9678736329078674,
            0.11045407503843307,
            2.6971888542175293,
            -3.0677459239959717,
            -0.6325691342353821,
            0.9650694727897644,
            2.902275800704956,
            0.04749257117509842,
            -2.777578353881836,
            4.2641496658325195,
            -2.3931612968444824,
            -0.19766667485237122,
            -3.3182783126831055,
            -6.224605560302734,
            -2.706444501876831,
            1.8209128379821777,
            1.6567206382751465,
            1.4275636672973633,
            -0.6074886322021484,
            3.897099494934082,
            0.40640294551849365,
            3.546602725982666,
            2.584423780441284,
            3.758990526199341,
            -2.4432945251464844,
            -5.376156806945801,
            1.1489636898040771,
            0.14176523685455322,
            0.801654577255249,
            1.103827953338623,
            -1.8290200233459473,
            9.910212516784668,
            1.0498861074447632,
            0.12304074317216873,
            -2.2175116539001465,
            -0.37348684668540955,
            -1.7482527494430542,
            1.9390324354171753,
            -1.558724284172058,
            -0.8603916168212891,
            -1.2344012260437012,
            0.06206458806991577,
            1.943421721458435,
            -1.3167327642440796,
            4.194028854370117,
            3.4060893058776855,
            2.8206605911254883,
            -0.24112993478775024,
            1.7451797723770142,
            2.594205379486084,
            -2.8364946842193604,
            2.2389116287231445,
            -0.5590084791183472,
            1.5732334852218628,
            -0.28895333409309387,
            -1.998835563659668,
            14.129678726196289,
            1.1220861673355103,
            -0.07266175746917725,
            -4.177456855773926,
            -1.910570502281189,
            -2.205373525619507,
            -5.252384662628174,
            1.2536894083023071,
            -2.4066057205200195,
            0.7949540615081787,
            -1.1358184814453125,
            -6.552828788757324,
            3.2424674034118652,
            0.4790648818016052,
            -1.7298979759216309,
            5.453830718994141,
            -0.38900992274284363,
            5.063300132751465,
            -0.9708449840545654,
            3.9606778621673584,
            -1.8467930555343628,
            1.3138184547424316,
            1.5404484272003174,
            -1.3454668521881104,
            -3.8009347915649414,
            2.24955677986145,
            3.394948720932007,
            1.963127613067627,
            -4.541393280029297,
            -2.5622615814208984,
            -2.95975399017334,
            -4.394306182861328,
            1.3215856552124023,
            1.470201849937439,
            -0.37756311893463135,
            -2.1170339584350586,
            4.903431415557861,
            5.353829383850098,
            -3.0949041843414307,
            -0.2777133882045746,
            6.729364395141602,
            -0.46611297130584717,
            -1.4626529216766357,
            1.8701554536819458,
            -4.938450813293457,
            -0.32912904024124146,
            -0.7396948933601379,
            -3.059131622314453,
            1.596178650856018,
            -4.210295677185059,
            4.43497371673584,
            -0.22782915830612183,
            0.3626726567745209,
            3.2942116260528564,
            -0.23686383664608002,
            5.965099334716797,
            6.265206813812256,
            1.5745240449905396,
            -0.6074670553207397,
            3.6288280487060547,
            3.549687385559082,
            2.5775439739227295,
            -2.6774446964263916,
            2.0888590812683105,
            -0.5407192707061768,
            5.198329925537109,
            -5.373362064361572,
            1.9623961448669434,
            1.0630853176116943,
            0.9577564001083374,
            1.6653199195861816,
            -1.8248746395111084,
            0.8463730216026306,
            -2.854188919067383,
            0.4393978416919708,
            1.484261155128479,
            -1.5239239931106567,
            -2.898707389831543,
            -1.755544662475586,
            2.770477771759033,
            2.2723329067230225,
            1.4469006061553955,
            -1.0818371772766113,
            -2.4951837062835693,
            3.419764995574951,
            -3.6300413608551025,
            -1.9951896667480469,
            -1.0684770345687866,
            1.1178938150405884,
            -0.2567266523838043,
            -1.1322076320648193,
            -1.260401725769043,
            -0.5473667979240417,
            -0.34427401423454285,
            -2.7199835777282715,
            5.314418792724609,
            -0.8844835758209229,
            -2.100362539291382,
            1.3506441116333008,
            0.22622662782669067,
            -2.22129225730896,
            -6.631515979766846,
            -2.182258367538452,
            0.5643620491027832,
            3.012938976287842,
            1.4690014123916626,
            -2.5025746822357178,
            2.008687973022461,
            1.5486257076263428,
            0.9306437373161316,
            2.9113399982452393,
            -1.042149305343628,
            -2.778383493423462,
            -2.2194364070892334,
            -4.946050643920898,
            -2.4064338207244873,
            3.9240217208862305,
            0.19426380097866058,
            -1.2479075193405151,
            3.8563709259033203,
            2.49637770652771,
            0.21064002811908722,
            0.4244442582130432,
            1.626613736152649,
            0.866788387298584,
            2.2558534145355225,
            5.163398742675781,
            -3.481444835662842,
            -0.3233516812324524,
            1.7424890995025635,
            -3.09865403175354,
            0.8916038274765015,
            -2.9210081100463867,
            -1.0274219512939453,
            -2.662468910217285,
            -0.45284074544906616,
            -0.5897173285484314,
            -5.519689559936523,
            -4.247601509094238,
            4.276803970336914,
            0.7216395139694214,
            5.654354095458984,
            1.2931839227676392,
            0.5580712556838989,
            -2.4928927421569824,
            2.0841054916381836,
            -7.045416831970215,
            4.643697261810303,
            1.130685567855835,
            -0.7032400965690613,
            6.165142059326172,
            2.428889751434326,
            1.087449073791504,
            0.8750805258750916,
            -0.5097046494483948,
            -2.5486388206481934,
            -1.4832706451416016,
            1.3417704105377197,
            1.3543609380722046,
            0.8844560384750366,
            -4.008625507354736,
            1.0789353847503662,
            1.4622886180877686,
            5.6200456619262695,
            6.561251640319824,
            6.131511211395264,
            3.9389500617980957,
            -6.046458721160889,
            1.209234595298767,
            -0.12421160936355591,
            0.09664648771286011,
            4.5440545082092285,
            -6.929171562194824,
            2.0681793689727783,
            0.9363396167755127,
            2.8820719718933105,
            -3.5641603469848633,
            3.372445821762085,
            0.07292990386486053,
            7.041889190673828,
            -5.912035942077637,
            -2.429543972015381,
            -0.1408580243587494,
            3.0544495582580566,
            2.2165656089782715,
            -2.2573559284210205,
            1.9208064079284668,
            -3.5366573333740234,
            -0.9075941443443298,
            -2.468940019607544,
            -0.8747358322143555,
            0.4808342456817627,
            -3.0968010425567627,
            3.990112543106079,
            3.753526449203491,
            0.24214404821395874,
            -1.1556367874145508,
            5.056767463684082,
            -0.21636372804641724,
            -0.5780342221260071,
            0.14302970468997955,
            5.221663475036621,
            1.4287903308868408,
            -1.6868094205856323,
            -1.8295645713806152,
            -2.897489070892334,
            0.14677780866622925,
            -0.7057309150695801,
            2.40269136428833,
            2.28688645362854,
            2.1996853351593018,
            0.7749394774436951,
            0.09184032678604126,
            1.2045015096664429,
            -0.44947290420532227,
            -2.3920817375183105,
            0.4649449586868286,
            -4.503057956695557,
            0.19979551434516907,
            -1.7922101020812988,
            -5.723991870880127,
            1.8814446926116943,
            -4.680386543273926,
            0.99787437915802,
            0.7575650215148926,
            0.2571413516998291,
            -1.1604053974151611,
            -0.5849077105522156,
            -1.6076263189315796,
            0.2812802493572235,
            1.0632741451263428,
            -0.7388455271720886,
            0.14043599367141724,
            2.1479341983795166,
            -0.15838050842285156,
            3.5307483673095703,
            4.707644462585449,
            2.690047264099121,
            -2.241377830505371,
            -0.6679099798202515,
            3.837446928024292,
            2.8657870292663574,
            1.561776876449585,
            -2.99314022064209,
            -1.544035792350769,
            -0.13810135424137115,
            19.871631622314453,
            -1.7544066905975342,
            0.2467593252658844,
            -0.6311108469963074,
            1.8792608976364136,
            -2.875979423522949,
            -2.1779379844665527,
            2.763720989227295,
            2.523353099822998,
            -1.4370458126068115,
            -6.234001159667969,
            -1.7755913734436035,
            0.009357646107673645,
            2.606651782989502,
            1.8669896125793457,
            -2.876002550125122,
            -2.7710185050964355,
            3.016836404800415,
            -3.1972460746765137,
            -1.7121994495391846,
            -1.6335529088974,
            3.8471786975860596,
            -2.744417667388916,
            0.09190350770950317,
            0.17740416526794434,
            0.11602313816547394,
            -0.7667166590690613,
            3.6862316131591797,
            -1.5782432556152344,
            -0.5030333995819092,
            2.920545816421509,
            3.1193063259124756,
            -0.4296342730522156,
            1.9352810382843018,
            -1.8848798274993896,
            2.475201368331909,
            2.5883562564849854,
            1.567509651184082,
            3.8937673568725586,
            2.4140326976776123,
            -0.5860229730606079,
            -3.0178539752960205,
            -2.4175925254821777,
            2.779686450958252,
            -3.1561994552612305,
            1.6983211040496826,
            0.6556783318519592,
            0.36781054735183716,
            -3.9093434810638428,
            3.234492063522339,
            -0.44212931394577026,
            -1.5418970584869385,
            -2.6319711208343506,
            0.8844828605651855,
            -2.5787503719329834,
            4.965857982635498,
            -1.8988347053527832,
            -2.6377456188201904,
            1.929909110069275,
            -3.795328140258789,
            1.6026068925857544,
            -1.8239291906356812,
            -1.258094310760498,
            -2.7210946083068848,
            -7.2091264724731445,
            -2.2196598052978516,
            -3.662785053253174,
            2.5853991508483887,
            2.7742133140563965,
            -0.6636297106742859,
            3.765965461730957,
            3.7123799324035645,
            -1.62863028049469,
            -1.6742327213287354,
            -0.7618508338928223,
            -5.921474933624268,
            3.5717153549194336,
            -1.2960819005966187,
            -2.5191688537597656,
            5.517498970031738,
            -3.396138906478882,
            2.0229649543762207,
            -1.1843342781066895,
            1.4130048751831055,
            4.696585655212402,
            -3.6780989170074463,
            5.813074111938477,
            -3.6013145446777344,
            -1.165331482887268,
            3.1638717651367188,
            -3.0677919387817383,
            -1.101991891860962,
            4.204724311828613,
            3.225729465484619,
            2.185448169708252,
            -4.712409973144531,
            -1.4006996154785156,
            -1.9764927625656128,
            -3.9223968982696533,
            -1.8107894659042358,
            2.604830503463745,
            5.441909313201904,
            0.27043217420578003,
            -5.531302452087402,
            1.7069748640060425,
            -1.540984869003296,
            -3.1663663387298584,
            -4.646232604980469,
            -4.175012111663818,
            -1.8101060390472412,
            5.181290626525879,
            -5.945965766906738,
            -5.017419815063477,
            0.9733920097351074,
            1.2281949520111084,
            1.184459924697876,
            -0.3115009069442749,
            2.0941128730773926,
            -3.0065150260925293,
            2.442960500717163,
            1.2655162811279297,
            -0.3493501543998718,
            -3.0713064670562744,
            -3.8185765743255615,
            -4.760927200317383,
            0.011429950594902039,
            -0.3843535780906677,
            -0.6168285608291626,
            0.7490835785865784,
            0.7232669591903687,
            1.8825173377990723,
            -2.166738510131836,
            -0.22860628366470337,
            -0.8118319511413574,
            -1.8293401002883911,
            -1.5173134803771973,
            0.5665814280509949,
            -0.17013181746006012,
            -3.8362631797790527,
            5.664470195770264,
            5.2172369956970215,
            -2.640636920928955,
            -6.270340442657471,
            11.176846504211426,
            0.0029512643814086914,
            1.1619142293930054,
            -4.338708400726318,
            -0.4476904273033142,
            -3.214918613433838,
            0.8024731874465942,
            -0.6619870662689209,
            -1.7994195222854614,
            3.8624660968780518,
            2.211595296859741,
            0.9926421642303467,
            -2.28310489654541
        ]
    },
    "authors": [
        {
            "authorId": "51033208",
            "name": "Bingbin Liu"
        },
        {
            "authorId": "40401847",
            "name": "J. Ash"
        },
        {
            "authorId": "9935792",
            "name": "Surbhi Goel"
        },
        {
            "authorId": "37019006",
            "name": "A. Krishnamurthy"
        },
        {
            "authorId": "15943185",
            "name": "Cyril Zhang"
        }
    ],
    "references": [
        {
            "paperId": "e956e7598a8349683da7b16fc6d9ce64b5ae72e8",
            "title": "Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars"
        },
        {
            "paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082",
            "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
        },
        {
            "paperId": "c35c41dc9c93eff38ec8913cb4d639b06a915d33",
            "title": "Linear attention is (maybe) all you need (to understand transformer optimization)"
        },
        {
            "paperId": "0db0af0cd3ceb0531a050a03e6ceb849580ff53b",
            "title": "Teaching Arithmetic to Small Transformers"
        },
        {
            "paperId": "4631398b0d61061b9ca9489d76ded4dd05bcf1ec",
            "title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python"
        },
        {
            "paperId": "6825ba09383bc758f9a2feaebabe35a6cd4adc4c",
            "title": "How Language Model Hallucinations Can Snowball"
        },
        {
            "paperId": "026b3396a63ed5772329708b7580d633bb86bec9",
            "title": "RWKV: Reinventing RNNs for the Transformer Era"
        },
        {
            "paperId": "28085f480ce456a376ebace9b899e3bc93dbc048",
            "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"
        },
        {
            "paperId": "dbc368bc8b49347dd27679894524fa62f88492c9",
            "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"
        },
        {
            "paperId": "1db6836f61695e558608bb57166feca6876edabf",
            "title": "Learning to Reason and Memorize with Self-Notes"
        },
        {
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models"
        },
        {
            "paperId": "be48aae5d57efeff477199e55a536466280c853a",
            "title": "MLRegTest: A Benchmark for the Machine Learning of Regular Languages"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "15288293edeae26dad6e37218cc1c0fc96316635",
            "title": "Do Transformers Parse while Predicting the Masked Word?"
        },
        {
            "paperId": "f393aff1593c2d370ec0ae004910d18e40524967",
            "title": "Resurrecting Recurrent Neural Networks for Long Sequences"
        },
        {
            "paperId": "f3fde8a09b757ab356da1314d7a938504edf8314",
            "title": "How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding"
        },
        {
            "paperId": "69619a2a47faee7a29ec596db13172e2a42ff921",
            "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"
        },
        {
            "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context"
        },
        {
            "paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421",
            "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
        },
        {
            "paperId": "4d17732d90440682b0500f4e209c6cc4fac20e0e",
            "title": "Teaching Algorithmic Reasoning via In-context Learning"
        },
        {
            "paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6",
            "title": "Transformers Learn Shortcuts to Automata"
        },
        {
            "paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
            "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"
        },
        {
            "paperId": "13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0",
            "title": "Vision Transformers provably learn spatial structure"
        },
        {
            "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
            "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought"
        },
        {
            "paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9",
            "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"
        },
        {
            "paperId": "dec263e87f0c049d584b4948cfbcd0c47f345a52",
            "title": "Formal Algorithms for Transformers"
        },
        {
            "paperId": "f5f5616f39493566a9d502f611adcc8f1ceb394e",
            "title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit"
        },
        {
            "paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",
            "title": "Exploring Length Generalization in Large Language Models"
        },
        {
            "paperId": "d253beffd28d88cc3150c9e80511a6187ea6613b",
            "title": "Unveiling Transformers with LEGO: a synthetic reasoning task"
        },
        {
            "paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881",
            "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"
        },
        {
            "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd",
            "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"
        },
        {
            "paperId": "e9f28c98a00766e484810598886cf48b0de66cfa",
            "title": "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?"
        },
        {
            "paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b",
            "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"
        },
        {
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
        },
        {
            "paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
            "title": "Memorizing Transformers"
        },
        {
            "paperId": "d3dd80269f2542cc173afb3a1df24b582a1e4af2",
            "title": "Overcoming a Theoretical Limitation of Self-Attention"
        },
        {
            "paperId": "3def68bd0f856886d34272840a7f81588f2bc082",
            "title": "Survey of Hallucination in Natural Language Generation"
        },
        {
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
            "paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
            "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"
        },
        {
            "paperId": "92173d081b15824d22a9ef070e118744ceee8052",
            "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"
        },
        {
            "paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737",
            "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"
        },
        {
            "paperId": "a421ba0a9150cd35e231dddc323bdd9a59b3af93",
            "title": "Coherence boosting: When your pretrained language model is not paying enough attention"
        },
        {
            "paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1",
            "title": "Do Long-Range Language Models Actually Use Long-Range Context?"
        },
        {
            "paperId": "238deab37e201c57505a4a47bb854e462af79bd7",
            "title": "Entity-Based Knowledge Conflicts in Question Answering"
        },
        {
            "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
            "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
        },
        {
            "paperId": "0e9ac2cfc5a3ecb66eeace720901390f7809ba0a",
            "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"
        },
        {
            "paperId": "b61de520bc1ae57abde895601b62b4f92d82c0b4",
            "title": "Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization"
        },
        {
            "paperId": "bc73d53ba859c56ab08c41c475d45a9ae6d021cb",
            "title": "Is Sparse Attention more Interpretable?"
        },
        {
            "paperId": "b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea",
            "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"
        },
        {
            "paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145",
            "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"
        },
        {
            "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
        },
        {
            "paperId": "6399471944fdf64ad2d43de5a83e02035c138788",
            "title": "Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection"
        },
        {
            "paperId": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding"
        },
        {
            "paperId": "9b9dc2b3d95d2f4e4269a9818c14c70c1f801384",
            "title": "An Interpretability Illusion for BERT"
        },
        {
            "paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
            "title": "Investigating the Limitations of Transformers with Simple Arithmetic Tasks"
        },
        {
            "paperId": "58c74cec28f3416b9a1d308bb2d6519d21d53ab0",
            "title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"
        },
        {
            "paperId": "87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1",
            "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"
        },
        {
            "paperId": "7e9ff94476f41041c75e253e84f487db00e9c861",
            "title": "Long Range Arena: A Benchmark for Efficient Transformers"
        },
        {
            "paperId": "9001eb3c3d5a96ad3d804410c2437e6f60feade9",
            "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps"
        },
        {
            "paperId": "10c86505de83647c7b4157595ab10f64e97c94ef",
            "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"
        },
        {
            "paperId": "6f68e1bb253925d8431588555d3010419f322e04",
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
        },
        {
            "paperId": "5335fe1bf347f7ad1dce1611ea4b60bd8391a090",
            "title": "Transferring Inductive Biases through Knowledge Distillation"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "47fd2e73a9be04ed2186c03d8f88d5c87f64e4e4",
            "title": "ToTTo: A Controlled Table-To-Text Generation Dataset"
        },
        {
            "paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
            "title": "GLU Variants Improve Transformer"
        },
        {
            "paperId": "7be8c119dbe065c52125ee7716601751f3116844",
            "title": "Generalization through Memorization: Nearest Neighbor Language Models"
        },
        {
            "paperId": "c95383f251a62c63217586059c67f63507c3e839",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
            "paperId": "80c65cac35a43f7e88917cb6d517554c7b175259",
            "title": "Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation"
        },
        {
            "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
            "title": "Language Models as Knowledge Bases?"
        },
        {
            "paperId": "b3564be8b79f25585acb035f3deaf4ae93c26d8f",
            "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"
        },
        {
            "paperId": "02cbb0db288af2c83b48a023f245812bd22a2408",
            "title": "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation"
        },
        {
            "paperId": "cb51b53d5d26537ed6d458f83b81afe0f2efa289",
            "title": "Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?"
        },
        {
            "paperId": "1cbf4e4804ea636d78293964bffe7b2493444f9a",
            "title": "Attention With Sparsity Regularization for Neural Machine Translation and Summarization"
        },
        {
            "paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f",
            "title": "Attention is not Explanation"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
            "title": "Universal Transformers"
        },
        {
            "paperId": "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299",
            "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
            "title": "Automatic differentiation in PyTorch"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "3aa52436575cf6768a0a1a476601825f6a62e58f",
            "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"
        },
        {
            "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
            "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
        },
        {
            "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
            "title": "Effective Approaches to Attention-based Neural Machine Translation"
        },
        {
            "paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de",
            "title": "Neural Turing Machines"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning"
        },
        {
            "paperId": "69c5e99f390592976409ef0660392cf538c4c779",
            "title": "Applications of Automata Theory and Algebra: Via the Mathematical Theory of Complexity to Biology, Physics, Psychology, Philosophy, and Games"
        },
        {
            "paperId": "c517040d6b1d3f01d3a759e2dd1b7ab85811d0fd",
            "title": "Long Range Dependence"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "title": "Finding Structure in Time"
        },
        {
            "paperId": "6610644b16f789042223c66328d3d11f3372a847",
            "title": "Finite monoids and the fine structure of NC1"
        },
        {
            "paperId": "fed7aca2268f3c7ebfc0c52740144d14197db589",
            "title": "Automata, languages, and machines. A"
        },
        {
            "paperId": "0e83b9ad091596cf5c8edce23a669f1dbc34bac2",
            "title": "Cascade synthesis of finite-state machines"
        },
        {
            "paperId": "3b0cc63c910d36b8e29cedeb97f29e4b0dcfc105",
            "title": "Algebraic theory of machines. I. Prime decomposition theorem for finite semigroups and machines"
        },
        {
            "paperId": null,
            "title": "2023). Such failures can result in \u201challucinations\u201d: incorrect outputs which either directly contradict factual input in the context, or contain information absent in the context"
        },
        {
            "paperId": null,
            "title": "A mechanistic interpretability analysis of grokking"
        },
        {
            "paperId": "ed1fbc8829224a446bcd79f324cc71579e3794fe",
            "title": "The Algebraic Theory of Context-Free Languages*"
        },
        {
            "paperId": null,
            "title": "A trigger relay utilizing three-electrode thermionic vacuum tubes"
        },
        {
            "paperId": null,
            "title": "Improvements in ionic relays"
        },
        {
            "paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5",
            "title": "of the Association for Computational Linguistics"
        },
        {
            "paperId": null,
            "title": "t \u2208 [ T ] \\ { 1 , T \u2212 1 }"
        },
        {
            "paperId": null,
            "title": "R5) Resource scaling (in-distribution data, training steps, network size) helps"
        },
        {
            "paperId": null,
            "title": "we need s T \u2192 T \u2212 1 > s T \u2192 1 , it"
        },
        {
            "paperId": null,
            "title": "Despite many partial mitigations"
        },
        {
            "paperId": null,
            "title": "R6) Many algorithmic choices influence extrapolative behavior"
        },
        {
            "paperId": null,
            "title": "(R2) 1-layer LSTM extrapolates perfectly. We train a 1-layer LSTM (Hochreiter and Schmidhuber, 1997) network, with hidden state dimension 128 (for a total of 133 K parameters), for 500 21"
        },
        {
            "paperId": null,
            "title": "R7) Despite many partial mitigations, nothing eliminates attention glitches entirely"
        },
        {
            "paperId": null,
            "title": "(R3) 10B-scale natural LMs can correctly process flip-flop languages, but not robustly"
        }
    ]
}