{
    "paperId": "520bd2331cca8d5a9c032c186a2a0f7704ead6ff",
    "externalIds": {
        "ArXiv": "2106.14448",
        "DBLP": "journals/corr/abs-2106-14448",
        "CorpusId": 235658346
    },
    "title": "R-Drop: Regularized Dropout for Neural Networks",
    "abstract": "Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on $\\bf{5}$ widely used deep learning tasks ($\\bf{18}$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English$\\to$German translation ($\\bf{30.91}$ BLEU) and WMT14 English$\\to$French translation ($\\bf{43.95}$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub{\\url{https://github.com/dropreg/R-Drop}}.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "referenceCount": 101,
    "citationCount": 360,
    "influentialCitationCount": 55,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "R-Drop is introduced, which forces the output distributions of different sub models generated by dropout to be consistent with each other in model training, and yields substantial improvements when applied to fine-tune large-scale pre-trained models."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.884556293487549,
            -0.31808850169181824,
            -4.06381893157959,
            1.5924986600875854,
            -0.4345232844352722,
            -1.4559131860733032,
            1.494512677192688,
            -4.5218892097473145,
            -0.7333410382270813,
            0.4592154920101166,
            -0.885466456413269,
            4.253914833068848,
            1.335608959197998,
            -1.8488422632217407,
            -2.382431745529175,
            3.2489280700683594,
            -0.3885129690170288,
            -0.4265206456184387,
            2.4140610694885254,
            1.5615863800048828,
            -1.639209508895874,
            2.1543285846710205,
            -2.7165236473083496,
            -2.6157588958740234,
            -3.8740200996398926,
            -2.3708183765411377,
            1.926274299621582,
            -0.1669740080833435,
            -2.390857696533203,
            -1.5803520679473877,
            -0.17917406558990479,
            -4.406792163848877,
            3.5427141189575195,
            -3.695711374282837,
            2.5947420597076416,
            -0.24387308955192566,
            -1.8487517833709717,
            5.507951736450195,
            -5.311580181121826,
            1.2069346904754639,
            -0.13474519550800323,
            1.7009029388427734,
            0.7855110168457031,
            0.33187562227249146,
            1.0429069995880127,
            -1.637384295463562,
            -2.046858072280884,
            1.43758225440979,
            0.10129398107528687,
            1.8065366744995117,
            2.0929741859436035,
            -0.05003419518470764,
            0.7401106357574463,
            2.275893211364746,
            -0.021658137440681458,
            -0.680334210395813,
            1.3467236757278442,
            -0.5546764135360718,
            1.6927214860916138,
            -5.050449371337891,
            6.455869674682617,
            4.851536273956299,
            0.4001411199569702,
            -0.4347599744796753,
            2.2085163593292236,
            -5.844578742980957,
            -2.3637287616729736,
            6.956087112426758,
            2.0012149810791016,
            1.4406834840774536,
            -1.1756916046142578,
            -3.813385009765625,
            1.478834867477417,
            0.5286995768547058,
            -1.6499375104904175,
            0.6309093236923218,
            1.9506170749664307,
            -7.517830848693848,
            -3.4123783111572266,
            -1.465294361114502,
            0.6721501350402832,
            2.692068099975586,
            1.0213935375213623,
            1.7652254104614258,
            5.1061553955078125,
            1.402190089225769,
            -2.3756394386291504,
            -1.2690883874893188,
            -0.3579539656639099,
            -4.0400848388671875,
            1.8292042016983032,
            0.05956879258155823,
            2.1085686683654785,
            1.4924890995025635,
            -5.404162406921387,
            0.38361436128616333,
            -1.4772460460662842,
            -1.0491032600402832,
            -3.4025466442108154,
            1.4165037870407104,
            3.123134136199951,
            1.3510141372680664,
            2.8928871154785156,
            2.030806303024292,
            2.3324639797210693,
            -3.367246150970459,
            0.37076497077941895,
            0.2546699047088623,
            -1.301748514175415,
            -1.9192357063293457,
            -1.0061613321304321,
            1.8613247871398926,
            -0.23714032769203186,
            2.14857816696167,
            -3.370098352432251,
            0.6572043299674988,
            -1.3468143939971924,
            0.09797996282577515,
            -1.5521775484085083,
            5.354384899139404,
            -0.9422053098678589,
            -1.0141373872756958,
            -3.9882731437683105,
            -2.0866453647613525,
            0.8742988705635071,
            3.20351243019104,
            -1.6088685989379883,
            1.5590757131576538,
            -1.6867477893829346,
            -3.7070775032043457,
            1.509400725364685,
            -0.37472978234291077,
            1.0477063655853271,
            -1.4705500602722168,
            6.9711103439331055,
            4.987383842468262,
            -3.5896966457366943,
            2.4807302951812744,
            -1.3000975847244263,
            -1.8322889804840088,
            0.4980163872241974,
            0.8435013890266418,
            -0.132456973195076,
            -0.23285503685474396,
            3.344311475753784,
            3.9319870471954346,
            -1.3953883647918701,
            0.25272583961486816,
            0.7814018130302429,
            4.911033630371094,
            3.201000690460205,
            -4.823256969451904,
            1.3671249151229858,
            1.3836987018585205,
            0.7270976901054382,
            2.0881481170654297,
            -5.2079973220825195,
            -0.03836888074874878,
            -0.4148592948913574,
            0.2424788773059845,
            1.2326096296310425,
            -1.7998772859573364,
            -9.905375480651855,
            -2.3140058517456055,
            0.6022599339485168,
            -1.4481275081634521,
            1.5727981328964233,
            3.5680043697357178,
            -0.688554048538208,
            0.18162868916988373,
            0.9421210289001465,
            0.6324489116668701,
            -0.03277307003736496,
            0.3199923038482666,
            2.385321617126465,
            5.588010787963867,
            3.4064831733703613,
            -3.2233545780181885,
            0.5663808584213257,
            2.2379469871520996,
            -0.5317997336387634,
            -2.4643359184265137,
            -5.756496429443359,
            -1.873154878616333,
            -1.7765146493911743,
            -2.577707529067993,
            -2.4256932735443115,
            -3.8056893348693848,
            0.8324277997016907,
            -2.084829568862915,
            0.3967527151107788,
            0.3043678402900696,
            5.384223461151123,
            6.205575942993164,
            4.202347755432129,
            -0.5069623589515686,
            -0.0970458984375,
            2.387535810470581,
            -1.9650938510894775,
            -1.496840476989746,
            3.247554063796997,
            1.747238278388977,
            -0.6542890071868896,
            -2.6165122985839844,
            4.350222110748291,
            2.7324023246765137,
            -3.3029165267944336,
            2.040581464767456,
            3.5238335132598877,
            -1.7449204921722412,
            0.23840747773647308,
            0.7835997939109802,
            -1.8954530954360962,
            2.1880929470062256,
            -2.580409526824951,
            -0.7062685489654541,
            -2.8013978004455566,
            2.871753692626953,
            2.8204312324523926,
            0.03942453861236572,
            -1.0495470762252808,
            1.4706549644470215,
            0.5189832448959351,
            -1.9820523262023926,
            0.4979618787765503,
            -5.997809410095215,
            1.5618451833724976,
            -1.972022294998169,
            -0.3212205171585083,
            1.6130180358886719,
            0.03377413749694824,
            -2.183382034301758,
            2.553895950317383,
            -0.877193033695221,
            -6.65974760055542,
            -0.5710664987564087,
            -2.304314374923706,
            3.113133430480957,
            -1.5605403184890747,
            1.4230321645736694,
            2.49930739402771,
            -0.1710897982120514,
            -0.7867728471755981,
            3.574909210205078,
            3.5747134685516357,
            0.47410327196121216,
            -3.051499843597412,
            -1.1598386764526367,
            -3.124523639678955,
            2.145876884460449,
            -4.1044135093688965,
            1.6809872388839722,
            0.9391764998435974,
            -1.7586238384246826,
            2.990468740463257,
            2.265594244003296,
            1.2420761585235596,
            2.9564270973205566,
            2.543342113494873,
            3.523838520050049,
            1.2738991975784302,
            6.870151042938232,
            0.3321212828159332,
            2.997199773788452,
            -1.063574194908142,
            -0.28371143341064453,
            -2.153316020965576,
            -3.255100727081299,
            -0.13929876685142517,
            2.76369571685791,
            2.4313838481903076,
            -0.16995131969451904,
            0.5622943043708801,
            -3.175787925720215,
            -2.270251750946045,
            -5.025066375732422,
            -2.1361608505249023,
            -1.0548627376556396,
            4.449158668518066,
            3.0801632404327393,
            -1.3003250360488892,
            -1.7495068311691284,
            1.0043139457702637,
            -1.0808253288269043,
            -5.51319694519043,
            -3.268049716949463,
            0.30206167697906494,
            -1.6705024242401123,
            -1.7100560665130615,
            -2.934608221054077,
            -3.4146251678466797,
            3.3226490020751953,
            -0.649638831615448,
            -1.2337180376052856,
            -4.265387058258057,
            2.9382541179656982,
            3.1325812339782715,
            1.4250544309616089,
            -1.794112205505371,
            0.8836883306503296,
            1.0906901359558105,
            1.9840059280395508,
            3.0682597160339355,
            -2.1915647983551025,
            1.9268841743469238,
            3.8000810146331787,
            1.3551902770996094,
            -4.930047035217285,
            -1.6432557106018066,
            -5.191325664520264,
            -2.382995128631592,
            -1.6861557960510254,
            3.3110084533691406,
            -3.3918259143829346,
            0.7879601716995239,
            0.1659158319234848,
            1.1497552394866943,
            1.9508875608444214,
            -3.1819674968719482,
            0.9380901455879211,
            1.4309101104736328,
            0.4446870684623718,
            -6.457273960113525,
            -1.5139116048812866,
            -3.1723990440368652,
            1.9991532564163208,
            4.006451606750488,
            3.52958345413208,
            -1.9527661800384521,
            0.8126105070114136,
            1.3319649696350098,
            4.0595622062683105,
            2.9840707778930664,
            5.9410319328308105,
            -1.3707475662231445,
            -4.020095348358154,
            -2.3766214847564697,
            -1.2861175537109375,
            0.7067082524299622,
            2.938419818878174,
            -0.8664443492889404,
            5.170849800109863,
            0.26958879828453064,
            0.2411799132823944,
            -1.9423972368240356,
            -2.0649662017822266,
            2.8892247676849365,
            -2.3608546257019043,
            3.1653196811676025,
            -2.8745524883270264,
            0.530699610710144,
            3.315302848815918,
            1.1245183944702148,
            -3.8318521976470947,
            0.9493064880371094,
            2.9251837730407715,
            4.594038009643555,
            1.662121057510376,
            1.8023064136505127,
            2.913886070251465,
            -0.3644857704639435,
            -2.5775339603424072,
            -0.7877394556999207,
            -0.3292785882949829,
            0.018654286861419678,
            -2.7690675258636475,
            9.658151626586914,
            -4.279199123382568,
            -0.18407082557678223,
            -4.270701885223389,
            -0.20793971419334412,
            -0.8332294225692749,
            -1.2161641120910645,
            3.2736282348632812,
            -1.9354472160339355,
            -4.58278751373291,
            2.875424861907959,
            -7.278262138366699,
            1.347172498703003,
            2.053013563156128,
            -1.8482327461242676,
            3.1671416759490967,
            0.24398678541183472,
            1.6017687320709229,
            -1.5221272706985474,
            0.1145453155040741,
            -2.262845277786255,
            0.7601051330566406,
            0.598059892654419,
            0.08176575601100922,
            -1.8499361276626587,
            2.081392288208008,
            0.7680684328079224,
            -0.30667275190353394,
            -1.7010389566421509,
            -5.912579536437988,
            -2.931532859802246,
            -2.458254814147949,
            0.4740091562271118,
            2.189211845397949,
            -2.9745423793792725,
            2.6719846725463867,
            5.633628845214844,
            0.7546344995498657,
            -5.650620460510254,
            -2.9448556900024414,
            5.718896865844727,
            -1.8679962158203125,
            -1.0626765489578247,
            1.086586594581604,
            0.34311443567276,
            -3.293543815612793,
            -1.1108951568603516,
            -3.282733917236328,
            -0.3645018935203552,
            -3.003718137741089,
            -2.040855884552002,
            8.913959503173828,
            2.269890785217285,
            1.6265791654586792,
            -1.2332247495651245,
            2.5681042671203613,
            4.019131183624268,
            2.645142078399658,
            -2.3084847927093506,
            2.4235477447509766,
            3.3494272232055664,
            -0.35096460580825806,
            -4.496436595916748,
            2.0789523124694824,
            -3.228424072265625,
            5.165343284606934,
            -1.6911849975585938,
            -0.17176619172096252,
            3.3145132064819336,
            2.1982102394104004,
            4.474102020263672,
            1.9763896465301514,
            1.1791739463806152,
            -1.9492254257202148,
            0.9621375799179077,
            2.1034066677093506,
            -2.252875328063965,
            4.562486171722412,
            1.7849371433258057,
            -0.2763744294643402,
            1.7137784957885742,
            4.5163068771362305,
            -1.721397876739502,
            -3.7197279930114746,
            3.874786376953125,
            -6.572355270385742,
            -1.286698579788208,
            -1.1315124034881592,
            1.0574066638946533,
            0.8600640296936035,
            -3.3898680210113525,
            -1.2245737314224243,
            2.8607370853424072,
            0.6755066514015198,
            -5.441318988800049,
            4.540327072143555,
            -0.6166635751724243,
            2.7262096405029297,
            -0.5048640966415405,
            2.419013738632202,
            -1.1104344129562378,
            -3.0766797065734863,
            -1.8372013568878174,
            1.4000208377838135,
            2.1249306201934814,
            -1.5442628860473633,
            -3.46281099319458,
            0.255632609128952,
            -0.6142135262489319,
            0.2925037145614624,
            0.1497894525527954,
            1.1754335165023804,
            -0.6540766358375549,
            -2.5738413333892822,
            -2.2458019256591797,
            -2.54194974899292,
            4.129977226257324,
            0.687020480632782,
            -4.349161148071289,
            8.393210411071777,
            1.857049822807312,
            2.5263445377349854,
            2.1634132862091064,
            4.084898948669434,
            1.709442377090454,
            2.73895001411438,
            2.2577900886535645,
            -0.11256346106529236,
            3.5829262733459473,
            -0.5303895473480225,
            -4.5547194480896,
            3.481635093688965,
            1.8552207946777344,
            -1.3227065801620483,
            1.5187461376190186,
            -3.407404899597168,
            -2.267780303955078,
            -0.46876102685928345,
            -2.274134874343872,
            3.887510061264038,
            3.999232769012451,
            1.986339807510376,
            -2.7074265480041504,
            -1.7722091674804688,
            -2.018867254257202,
            0.8981760144233704,
            -4.910229682922363,
            1.6062736511230469,
            -2.5857558250427246,
            2.7510557174682617,
            1.6874611377716064,
            -1.9079248905181885,
            -1.4329599142074585,
            0.4040529131889343,
            -2.758300304412842,
            0.19123759865760803,
            -1.0380218029022217,
            3.515542984008789,
            -1.2851568460464478,
            0.752145528793335,
            -2.1847357749938965,
            -2.2519845962524414,
            2.0919432640075684,
            8.211944580078125,
            5.4428300857543945,
            3.859786033630371,
            3.7416176795959473,
            -2.1869442462921143,
            -1.6829036474227905,
            -2.8708858489990234,
            0.8085558414459229,
            4.475014686584473,
            -3.830986499786377,
            -1.4981815814971924,
            0.25125208497047424,
            -0.773305356502533,
            -1.4865394830703735,
            0.5860951542854309,
            -2.075098991394043,
            2.388737440109253,
            -4.488049507141113,
            -2.007398843765259,
            -1.854645013809204,
            -0.3726593255996704,
            -2.387667655944824,
            1.9526180028915405,
            2.435476064682007,
            -0.9306690096855164,
            -2.467038631439209,
            -4.112766742706299,
            -3.1553735733032227,
            -0.929408073425293,
            0.5636645555496216,
            1.0355112552642822,
            1.6767970323562622,
            0.026101820170879364,
            0.8852811455726624,
            4.844046115875244,
            0.5431851148605347,
            -4.380583763122559,
            -1.2318220138549805,
            4.62369441986084,
            -1.6308765411376953,
            -2.481991767883301,
            0.8434855937957764,
            -1.1790939569473267,
            -0.12476298213005066,
            0.5728814601898193,
            0.33895257115364075,
            2.0395941734313965,
            6.724242210388184,
            3.09452486038208,
            -4.441009044647217,
            -1.5501635074615479,
            -1.4448633193969727,
            -0.9563058018684387,
            -2.971280574798584,
            -4.5628557205200195,
            -0.5928326845169067,
            -3.929072856903076,
            -0.7791650295257568,
            3.3234851360321045,
            -3.8469855785369873,
            0.8528516888618469,
            4.391843795776367,
            -0.3786085247993469,
            2.2847776412963867,
            -2.828890323638916,
            -0.6749914288520813,
            -3.809272527694702,
            3.761932849884033,
            2.9714808464050293,
            -0.33631300926208496,
            -0.04106760025024414,
            -0.17344145476818085,
            4.325832843780518,
            2.608426094055176,
            0.4502246379852295,
            -0.7526346445083618,
            0.529481828212738,
            2.305593967437744,
            -0.6020451784133911,
            -3.984935998916626,
            0.9487694501876831,
            -0.2673155665397644,
            4.847702980041504,
            18.733722686767578,
            2.1117870807647705,
            -0.2555029094219208,
            2.296476364135742,
            -1.5773959159851074,
            -2.544725179672241,
            -3.6794800758361816,
            3.443542003631592,
            -2.50881028175354,
            2.0811073780059814,
            -1.3291658163070679,
            -1.7200216054916382,
            -1.3821296691894531,
            2.7734715938568115,
            -3.4024441242218018,
            -1.6294149160385132,
            -2.7617580890655518,
            1.7610785961151123,
            -1.7447504997253418,
            -0.7887983322143555,
            -0.020232021808624268,
            1.3110570907592773,
            1.7835637331008911,
            -2.6107535362243652,
            4.664508819580078,
            3.8490943908691406,
            1.0408570766448975,
            2.6272082328796387,
            -5.919553756713867,
            0.061208561062812805,
            2.4003283977508545,
            2.4261362552642822,
            0.37355703115463257,
            0.4040554165840149,
            -2.3427963256835938,
            1.4307050704956055,
            5.771961212158203,
            -2.6739208698272705,
            2.391065835952759,
            0.7922914624214172,
            -1.2284566164016724,
            0.3958735764026642,
            -2.9872794151306152,
            0.32229509949684143,
            -0.2726627588272095,
            1.5116089582443237,
            1.2089054584503174,
            0.16526095569133759,
            -2.5914015769958496,
            2.5196619033813477,
            2.0350799560546875,
            0.9361432790756226,
            -2.5070905685424805,
            0.48408493399620056,
            0.6867498159408569,
            1.107485055923462,
            -0.1069977879524231,
            -0.4800875186920166,
            4.000444412231445,
            2.2162795066833496,
            0.5675091743469238,
            -0.2705420255661011,
            -1.2899470329284668,
            -2.9416990280151367,
            -1.7306632995605469,
            0.33775514364242554,
            -6.769965648651123,
            3.196789026260376,
            3.932920217514038,
            0.9458451867103577,
            6.473311424255371,
            1.3155462741851807,
            1.300389289855957,
            -1.4139190912246704,
            -1.8981616497039795,
            -3.6952805519104004,
            -0.06688094139099121,
            1.0351473093032837,
            0.1381695568561554,
            4.902998447418213,
            -3.812485694885254,
            1.3955307006835938,
            -2.819110631942749,
            -0.7676499485969543,
            4.3612518310546875,
            -0.8779580593109131,
            3.3471925258636475,
            -1.940215826034546,
            2.020545482635498,
            4.592743873596191,
            0.34196779131889343,
            -0.45549476146698,
            0.14768727123737335,
            4.737926006317139,
            2.664334774017334,
            -4.642095565795898,
            -2.0667037963867188,
            -3.3824074268341064,
            -4.028961181640625,
            -5.045854568481445,
            4.938928604125977,
            1.5209178924560547,
            1.3716731071472168,
            -0.5547103881835938,
            2.1328039169311523,
            1.17232346534729,
            -0.7116233110427856,
            -5.286738395690918,
            -2.4126861095428467,
            -1.2230191230773926,
            2.5060131549835205,
            -3.6396284103393555,
            -0.1660015732049942,
            3.34071946144104,
            -0.10450762510299683,
            0.15399914979934692,
            0.5103968977928162,
            2.020597457885742,
            -0.3410990834236145,
            2.1173367500305176,
            0.29491955041885376,
            -2.5938053131103516,
            -2.920431137084961,
            -4.679310321807861,
            -1.5464649200439453,
            1.7612985372543335,
            1.7830830812454224,
            -2.5309243202209473,
            0.6022730469703674,
            -4.045731067657471,
            1.9436595439910889,
            -2.0324301719665527,
            -3.0040762424468994,
            -0.5838755965232849,
            -1.323480486869812,
            -1.2014673948287964,
            1.118247628211975,
            2.008842706680298,
            -1.201404094696045,
            1.5629887580871582,
            3.000919818878174,
            -2.1276206970214844,
            -1.9443696737289429,
            8.875154495239258,
            1.1664934158325195,
            0.8941866159439087,
            -1.4444730281829834,
            1.3968123197555542,
            -1.2046077251434326,
            -1.1121925115585327,
            0.9615973830223083,
            2.396496057510376,
            5.394688129425049,
            -2.2236156463623047,
            -4.418717384338379,
            -4.400134563446045
        ]
    },
    "authors": [
        {
            "authorId": "48083523",
            "name": "Xiaobo Liang"
        },
        {
            "authorId": "47767791",
            "name": "Lijun Wu"
        },
        {
            "authorId": "2109013629",
            "name": "Juntao Li"
        },
        {
            "authorId": "2118462606",
            "name": "Yue Wang"
        },
        {
            "authorId": "47580728",
            "name": "Qi Meng"
        },
        {
            "authorId": "143826491",
            "name": "Tao Qin"
        },
        {
            "authorId": "2154939268",
            "name": "Wei Chen"
        },
        {
            "authorId": "39767557",
            "name": "M. Zhang"
        },
        {
            "authorId": "2110264337",
            "name": "Tie-Yan Liu"
        }
    ],
    "references": [
        {
            "paperId": "c26759e6c701201af2f62f7ee4eb68742b5bf085",
            "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
        },
        {
            "paperId": "dbcff24e72e8360f2026018a5cde646f369767cb",
            "title": "Not All Attention Is All You Need"
        },
        {
            "paperId": "be51e9141ae2af4daf3a1ba745ad3ff66a5990f3",
            "title": "Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective"
        },
        {
            "paperId": "2ff7d8d79c1ab50c1826d965475a1eb32db0c133",
            "title": "SEED: Self-supervised Distillation For Visual Representation"
        },
        {
            "paperId": "fdacf2a732f55befdc410ea927091cad3b791f13",
            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
        },
        {
            "paperId": "ef8854a62e05c8e741894166689a9cd8352a1df0",
            "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks"
        },
        {
            "paperId": "255e6239bcc51047d020d41ce0179c1270f3c22f",
            "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"
        },
        {
            "paperId": "1013750582c20bbdf1164127b5f26b1e06e817e3",
            "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "5a11bd4e678fcb05cb8f5d30c45877fb58bdd3b3",
            "title": "A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation"
        },
        {
            "paperId": "b5271f4522fd72e335535c5f65d3afc01d1cb2bd",
            "title": "Very Deep Transformers for Neural Machine Translation"
        },
        {
            "paperId": "b88c11922cac84e5ea902f82d27ae21c3dda2e04",
            "title": "Better Fine-Tuning by Reducing Representational Collapse"
        },
        {
            "paperId": "c0044588da54ec398deb21f93d92481c35b6757f",
            "title": "The scale-invariant space for attention layer in neural network"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "5c27f7106d2f0c95fc51fb98ece672e00185ffa1",
            "title": "Scheduled DropHead: A Regularization Method for Transformer Models"
        },
        {
            "paperId": "0d37c762336ce69801c7fda5eb140d716ece0859",
            "title": "The Implicit and Explicit Regularization Effects of Dropout"
        },
        {
            "paperId": "dc373d5e108a90a70f55285a852a32706adbeb45",
            "title": "Incorporating BERT into Neural Machine Translation"
        },
        {
            "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
            "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "f74cb4f88f0023bbc350786808ba84c1b7833cec",
            "title": "Self-Distillation Amplifies Regularization in Hilbert Space"
        },
        {
            "paperId": "25db56fc85fe15625c3375064a35e908ba6dfd2a",
            "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training"
        },
        {
            "paperId": "bc51622358d8eea83248ef29402fe10640d07ba6",
            "title": "Big Transfer (BiT): General Visual Representation Learning"
        },
        {
            "paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"
        },
        {
            "paperId": "0c8b40ba2e5202872475120b22da1148c427fb70",
            "title": "A survey of regularization strategies for deep models"
        },
        {
            "paperId": "3bc53c49ae68adacf2d5be2fa795bcb879e2717a",
            "title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning"
        },
        {
            "paperId": "2c7d213b2230dea4ff88f0e50631089d513e9528",
            "title": "Data Diversification: A Simple Strategy For Neural Machine Translation"
        },
        {
            "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
            "paperId": "8271311ceeabe333d4555deedcd3926b2145314a",
            "title": "Self-Knowledge Distillation in Natural Language Processing"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "910aea4a020c329afb8b8a948abaafdf9ebcab3f",
            "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks"
        },
        {
            "paperId": "e146050bfe5e00063e55d467125a86daa45d7e1e",
            "title": "Depth Growing for Neural Machine Translation"
        },
        {
            "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
            "paperId": "a8cab29d2230924dffe89d6dda15ba42790c5ebf",
            "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation"
        },
        {
            "paperId": "941745ff77fa2cce8c1ed9692b4559492cd5d3ce",
            "title": "Survey of Dropout Methods for Deep Neural Networks"
        },
        {
            "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
            "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"
        },
        {
            "paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
            "title": "Adaptive Input Representations for Neural Language Modeling"
        },
        {
            "paperId": "c4ab32dc966bff2de35723374f7410eeab85053f",
            "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation"
        },
        {
            "paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f",
            "title": "Scaling Neural Machine Translation"
        },
        {
            "paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1",
            "title": "Born Again Neural Networks"
        },
        {
            "paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb",
            "title": "A Call for Clarity in Reporting BLEU Scores"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "d08b35243edc5be07387a9ed218070b31e502901",
            "title": "Group Normalization"
        },
        {
            "paperId": "415f18130edbe06e3e4806dfb0a1edcab6c241eb",
            "title": "Fraternal Dropout"
        },
        {
            "paperId": "6cc8fe49b353c77846af7437d8f98d3d7e3b56c3",
            "title": "Orthogonal Weight Normalization: Solution to Optimization over Multiple Dependent Stiefel Manifolds in Deep Neural Networks"
        },
        {
            "paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f",
            "title": "Improved Regularization of Convolutional Neural Networks with Cutout"
        },
        {
            "paperId": "58c6f890a1ae372958b7decf56132fe258152722",
            "title": "Regularizing and Optimizing LSTM Language Models"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "f06a12928307e17b1aff2b9f4a6c11791f19b6a7",
            "title": "Deep Mutual Learning"
        },
        {
            "paperId": "d773718f36ee1cc5bb9bc5b01afa8f76d09f452f",
            "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise"
        },
        {
            "paperId": "34cc3ceae5c3f7c8acbb89f2bff63f9d452b00d5",
            "title": "Variational Dropout Sparsifies Deep Neural Networks"
        },
        {
            "paperId": "437da3d1024f7312fde8a5287c99c36b559ed8f3",
            "title": "Dropout with Expectation-linear Regularization"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "7601b995303f953955004db7b9b8b206c0e02ff8",
            "title": "Learning Structured Sparsity in Deep Neural Networks"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "cf76789618f5db929393c1187514ce6c3502c3cd",
            "title": "Recurrent Dropout without Memory Loss"
        },
        {
            "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"
        },
        {
            "paperId": "310ec7796eeca484d734399d9979e8f74d7d8ed2",
            "title": "Shakeout: A New Regularized Deep Neural Network Training Scheme"
        },
        {
            "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
            "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
        },
        {
            "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "title": "Rethinking the Inception Architecture for Computer Vision"
        },
        {
            "paperId": "80d2e35888a5f072aae0c6f367c52f33dc874f8d",
            "title": "Towards dropout training for convolutional neural networks"
        },
        {
            "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
            "title": "Neural Machine Translation of Rare Words with Subword Units"
        },
        {
            "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
            "title": "Teaching Machines to Read and Comprehend"
        },
        {
            "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
            "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "dc8a5ff6551c6b3419a7843b01910221c964105f",
            "title": "Analyzing noise in autoencoders and deep networks"
        },
        {
            "paperId": "f9f19bee621faf46f90b023f8de8248b57becbc4",
            "title": "Adaptive dropout for training deep neural networks"
        },
        {
            "paperId": "38f35dd624cd1cf827416e31ac5e0e0454028eca",
            "title": "Regularization of Neural Networks using DropConnect"
        },
        {
            "paperId": "ec92efde21707ddf4b81f301cd58e2051c1a2443",
            "title": "Fast dropout training"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
            "title": "Improving neural networks by preventing co-adaptation of feature detectors"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "ccf415df5a83b343dae261286d29a40e8b80e6c6",
            "title": "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training"
        },
        {
            "paperId": "6ca755a0e87da3c25ac27e92b73701fe435a12d9",
            "title": "Manual and automatic evaluation of summaries"
        },
        {
            "paperId": "48e1de7d085808004d5f0493d486669a3d2930b5",
            "title": "A Simple Weight Decay Can Improve Generalization"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": null,
            "title": "Electra: Pre-training text encoders as discriminators rather than generators"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": null,
            "title": "Checklist 1. For all authors"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": "5c64fbdc93e21ebebccd1d6d3418f1749491cdea",
            "title": "Simplifying Neural Nets by Discovering Flat Minima"
        },
        {
            "paperId": null,
            "title": "c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?"
        },
        {
            "paperId": null,
            "title": "Did you discuss any potential negative societal impacts of your work? [No] We think our general training method will not lead to any negative societal impact"
        },
        {
            "paperId": null,
            "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]"
        },
        {
            "paperId": null,
            "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating?"
        },
        {
            "paperId": null,
            "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We provide strong results in the experiments"
        },
        {
            "paperId": null,
            "title": "Did you include complete proofs of all theoretical results? [Yes] We provide discussion in Section 2.3 and Appendix B"
        },
        {
            "paperId": null,
            "title": "Did you state the full set of assumptions of all theoretical results?"
        },
        {
            "paperId": null,
            "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them"
        },
        {
            "paperId": null,
            "title": "code, data, models) or curating/releasing new assets... (a) If your work uses existing assets"
        },
        {
            "paperId": null,
            "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"
        },
        {
            "paperId": null,
            "title": "with respect to the random seed after running experiments multiple times)? [Yes] We report the average results (number) for multiple runs of most experiments instead of the error bars"
        },
        {
            "paperId": null,
            "title": "Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"
        },
        {
            "paperId": null,
            "title": "a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"
        },
        {
            "paperId": null,
            "title": "Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?"
        },
        {
            "paperId": null,
            "title": "If you are including theoretical results"
        },
        {
            "paperId": null,
            "title": "If you used crowdsourcing or conducted research with human subjects"
        }
    ]
}