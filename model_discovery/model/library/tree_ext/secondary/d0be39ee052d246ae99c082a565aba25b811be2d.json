{
    "paperId": "d0be39ee052d246ae99c082a565aba25b811be2d",
    "externalIds": {
        "MAG": "2107878631",
        "DBLP": "journals/tnn/BengioSF94",
        "DOI": "10.1109/72.279181",
        "CorpusId": 206457500,
        "PubMed": "18267787"
    },
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.",
    "venue": "IEEE Trans. Neural Networks",
    "year": 1994,
    "referenceCount": 27,
    "citationCount": 7821,
    "influentialCitationCount": 257,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        },
        {
            "authorId": "2812486",
            "name": "Patrice Y. Simard"
        },
        {
            "authorId": "1688235",
            "name": "P. Frasconi"
        }
    ],
    "references": [
        {
            "paperId": "bc948468c35d7201260c73cad05d14a3fb17e4da",
            "title": "Unified Integration of Explicit Knowledge and Learning by Example in Recurrent Networks"
        },
        {
            "paperId": "3f1d04f57e420f0f1b2cd059deab309bc7073ca1",
            "title": "The problem of learning long-term dependencies in recurrent networks"
        },
        {
            "paperId": "b107cfe948f5d3c6e00b7cf1ebeacadd40224e0d",
            "title": "Inserting rules into recurrent neural networks"
        },
        {
            "paperId": "fee5cc60c185e5d2942fd925bbde612f368ea7ef",
            "title": "Using random weights to train multilayer networks of hard-limiting units"
        },
        {
            "paperId": "9a607334c1d963b4af29676578e1ef6aa11ba6e7",
            "title": "Local Feedback Multilayered Networks"
        },
        {
            "paperId": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "title": "Induction of Multiscale Temporal Structure"
        },
        {
            "paperId": "e9fd22ab8679f9d83fd9780570d7c95c3a3d996d",
            "title": "Nonlinear dynamics and stability of analog neural networks"
        },
        {
            "paperId": "3c92ffacba4eaddcb86c8fde50d1b183ac995052",
            "title": "Global optimization of a neural network-hidden Markov model hybrid"
        },
        {
            "paperId": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
        },
        {
            "paperId": "d6deb1ddc764259fbdc7733ef80473081bff31d5",
            "title": "Learning by Choice of Internal Representations"
        },
        {
            "paperId": "8f0d57ada00d761b380cd60b9db3860ea6059866",
            "title": "Minimizing multimodal functions of continuous variables with the \u201csimulated annealing\u201d algorithmCorrigenda for this article is available here"
        },
        {
            "paperId": "319f22bd5abfd67ac15988aa5c7f705f018c3ccd",
            "title": "Learning internal representations by error propagation"
        },
        {
            "paperId": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "title": "Optimization by Simulated Annealing"
        },
        {
            "paperId": "57b7165b856865eba0a9224e0ada75eb47dca230",
            "title": "Iterative solution of non\u2010linear equations using a computer algebra package"
        },
        {
            "paperId": "d92bf274d0a28378c3638d18415be4756ffa1c52",
            "title": "A solvable connectionist model of immediate recall of ordered lists"
        },
        {
            "paperId": "8b35b1ac7a52100a23aa095502162f2eb89b4323",
            "title": "A method of training multi-layer networks with heaviside characteristics using internal representations"
        },
        {
            "paperId": "cde2937cb41cf461f624ded2012743ac4624eca8",
            "title": "Artificial neural networks and their application to sequence recognition"
        },
        {
            "paperId": "af7802a50a8c294ebfd539ad72158475e5ecd9f2",
            "title": "The \"Moving Targets\" Training Algorithm"
        },
        {
            "paperId": "589d377b23e2bdae7ad161b36a5d6613bcfccdde",
            "title": "Improving the convergence of back-propagation learning with second-order methods"
        },
        {
            "paperId": "7ab73467f48f9c453be07c389c5dd1d1bfa70f25",
            "title": "BPS: a learning algorithm for capturing the dynamic nature of speech"
        },
        {
            "paperId": "7fb4d10f6d2ee3133135958aefd50bf22dcced9d",
            "title": "A Focused Backpropagation Algorithm for Temporal Pattern Recognition"
        },
        {
            "paperId": null,
            "title": "The development of the Time-Delay Neural Network  architecture for speech recognition"
        },
        {
            "paperId": null,
            "title": "\\A rst look at phonetic discrimination using connectionist models with recurrent links"
        },
        {
            "paperId": "56b771c4c3a54910dc3e7ff838940de89ed282db",
            "title": "Learning processes in an asymmetric threshold network"
        },
        {
            "paperId": "44f2679f8169e7f6449c52e058ebe6a45838b3c0",
            "title": "Learning Process in an Asymmetric Threshold Network"
        },
        {
            "paperId": "12b03af504d0960334c77567dab38791bf0f739a",
            "title": "AND T"
        },
        {
            "paperId": null,
            "title": "Arti(cid:12)cial"
        }
    ]
}