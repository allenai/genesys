{
    "paperId": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
    "externalIds": {
        "ArXiv": "1801.07736",
        "DBLP": "journals/corr/abs-1801-07736",
        "MAG": "2951433039",
        "CorpusId": 3655946
    },
    "title": "MaskGAN: Better Text Generation via Filling in the ______",
    "abstract": "Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "referenceCount": 38,
    "citationCount": 454,
    "influentialCitationCount": 35,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context and shows qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "26958176",
            "name": "W. Fedus"
        },
        {
            "authorId": "153440022",
            "name": "I. Goodfellow"
        },
        {
            "authorId": "2555924",
            "name": "Andrew M. Dai"
        }
    ],
    "references": [
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "0d16298285eb347bf951b302e6f2c8e4dc472253",
            "title": "Adversarial Feature Matching for Text Generation"
        },
        {
            "paperId": "a8176a160777bfe82b1c67506835c60073e6fbe8",
            "title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training"
        },
        {
            "paperId": "bad429f1fff54bff3d20cde79651fec2eb805a7c",
            "title": "Adversarial Generation of Natural Language"
        },
        {
            "paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
            "title": "Improved Training of Wasserstein GANs"
        },
        {
            "paperId": "042116e805aa3b5171efaf0c822dc142310ceefe",
            "title": "Boundary-Seeking Generative Adversarial Networks"
        },
        {
            "paperId": "4fc0ea6db600850908264652e1a5d7904f66ca58",
            "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks"
        },
        {
            "paperId": "a642bbbaf8822565f9b812ea279c596cc54ce4c3",
            "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"
        },
        {
            "paperId": "176f1d608b918eec8dc4b75e7b6e0acaba84a447",
            "title": "Adversarial Learning for Neural Dialogue Generation"
        },
        {
            "paperId": "67d968c7450878190e45ac7886746de867bf673d",
            "title": "Neural Architecture Search with Reinforcement Learning"
        },
        {
            "paperId": "424aef7340ee618132cc3314669400e23ad910ba",
            "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"
        },
        {
            "paperId": "29e944711a354c396fad71936f536e83025b6ce0",
            "title": "Categorical Reparameterization with Gumbel-Softmax"
        },
        {
            "paperId": "2fe874a1c85ecc9e848bf9defd76535e19d51f39",
            "title": "Professor Forcing: A New Algorithm for Training Recurrent Networks"
        },
        {
            "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
            "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
        },
        {
            "paperId": "32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
            "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"
        },
        {
            "paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559",
            "title": "Using the Output Embedding to Improve Language Models"
        },
        {
            "paperId": "0d24a0695c9fc669e643bad51d4e14f056329dec",
            "title": "An Actor-Critic Algorithm for Sequence Prediction"
        },
        {
            "paperId": "93d8d45fe8101545ae6d9fab3dbb38f904ff7b4e",
            "title": "Virtual Adversarial Training for Semi-Supervised Text Classification"
        },
        {
            "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
            "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
        },
        {
            "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
            "title": "Sequence Level Training with Recurrent Neural Networks"
        },
        {
            "paperId": "d82b55c35c8673774a708353838918346f6c006f",
            "title": "Generating Sentences from a Continuous Space"
        },
        {
            "paperId": "39e0c341351f8f4a39ac890b96217c7f4bde5369",
            "title": "A note on the evaluation of generative models"
        },
        {
            "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
            "title": "Effective Approaches to Attention-based Neural Machine Translation"
        },
        {
            "paperId": "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99",
            "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "a75fe5ebd7bc2f0271b8a0742b70f29bbc4be3fa",
            "title": "Model-Free reinforcement learning with continuous action in practice"
        },
        {
            "paperId": "a97b5db17acc731ef67321832dbbaf5766153135",
            "title": "Supervised Sequence Labelling with Recurrent Neural Networks"
        },
        {
            "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
            "title": "Learning Word Vectors for Sentiment Analysis"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"
        },
        {
            "paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b",
            "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6",
            "title": "GENERATIVE ADVERSARIAL NETS"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction"
        },
        {
            "paperId": null,
            "title": "some of these issues"
        },
        {
            "paperId": null,
            "title": "Capturing the complexities of natural language with these metrics alone is clearly insuf\ufb01cient"
        }
    ]
}