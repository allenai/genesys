{
    "paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9",
    "externalIds": {
        "ArXiv": "2306.00978",
        "DBLP": "conf/mlsys/0002TTYCWXDG024",
        "CorpusId": 258999941
    },
    "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration",
    "abstract": "Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.",
    "venue": "Conference on Machine Learning and Systems",
    "year": 2023,
    "referenceCount": 67,
    "citationCount": 245,
    "influentialCitationCount": 51,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Activation-aware Weight Quantization (AWQ) is proposed, a hardware-friendly approach for LLM low-bit weight-only quantization that outperforms existing work on various language modeling and domain-specific benchmarks (coding and math) and achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.7123782634735107,
            2.006114959716797,
            -4.696694374084473,
            5.545053005218506,
            -0.7848238348960876,
            0.7126704454421997,
            1.77347731590271,
            2.422592878341675,
            -2.5711684226989746,
            -2.7672362327575684,
            -3.481133460998535,
            2.4619290828704834,
            2.907808780670166,
            -2.9757797718048096,
            -3.580249786376953,
            -3.428603172302246,
            -2.8617541790008545,
            0.876420259475708,
            4.393817901611328,
            5.050917625427246,
            -0.6629789471626282,
            1.1114866733551025,
            -2.9865474700927734,
            4.095303058624268,
            -1.807572603225708,
            1.405882716178894,
            -0.7918992638587952,
            1.2949978113174438,
            -1.0734102725982666,
            -2.2845027446746826,
            -0.7240597605705261,
            -2.787644386291504,
            6.080732345581055,
            -2.1737639904022217,
            5.441061973571777,
            -2.0270297527313232,
            0.368003249168396,
            5.48335075378418,
            -1.0658587217330933,
            -2.8325743675231934,
            2.471957206726074,
            -1.580254077911377,
            -3.015702724456787,
            0.4006548821926117,
            -1.303540825843811,
            -0.44151729345321655,
            3.1341569423675537,
            3.4826736450195312,
            -2.271292209625244,
            2.421966552734375,
            1.4919815063476562,
            2.6100637912750244,
            2.5117945671081543,
            -2.612061023712158,
            -0.3624616861343384,
            -1.306958556175232,
            0.639997124671936,
            2.5492706298828125,
            3.2598605155944824,
            1.5890424251556396,
            7.003261089324951,
            1.6690192222595215,
            -1.0171171426773071,
            2.744476079940796,
            1.1736879348754883,
            -1.4476979970932007,
            1.7135679721832275,
            4.2887067794799805,
            2.0919981002807617,
            1.189496636390686,
            -0.6677269339561462,
            -4.3624982833862305,
            3.0467352867126465,
            0.3996501564979553,
            -7.581639766693115,
            -3.3324365615844727,
            0.3362842798233032,
            -3.1570658683776855,
            1.290693759918213,
            -5.69905948638916,
            -0.7900812029838562,
            4.878640651702881,
            0.34939807653427124,
            0.9324011206626892,
            2.302639961242676,
            -1.5619248151779175,
            -3.559328079223633,
            -0.06925684213638306,
            1.4125823974609375,
            -0.22793084383010864,
            1.5713160037994385,
            3.9908387660980225,
            -3.320960283279419,
            1.538356065750122,
            -1.0198750495910645,
            0.2740611135959625,
            1.1857268810272217,
            -0.9710234999656677,
            -1.633782148361206,
            1.542701244354248,
            -0.3732289671897888,
            0.41093865036964417,
            -1.8157176971435547,
            -0.8399330377578735,
            0.564781665802002,
            -2.8773622512817383,
            -3.6808173656463623,
            2.5361249446868896,
            -3.324795961380005,
            -1.8445549011230469,
            -0.19305402040481567,
            4.081722259521484,
            -1.611898422241211,
            -2.7217698097229004,
            -2.206723213195801,
            -3.6351237297058105,
            -0.7646337151527405,
            -2.8377227783203125,
            -1.717942237854004,
            4.667062282562256,
            -1.8816133737564087,
            0.9159175753593445,
            -5.230746269226074,
            0.936760663986206,
            1.3689920902252197,
            1.0963712930679321,
            -0.6911056637763977,
            1.614108681678772,
            -0.8010194301605225,
            -3.8666746616363525,
            1.1883940696716309,
            -3.4347853660583496,
            0.5394218564033508,
            -1.1416683197021484,
            4.964578628540039,
            1.005979061126709,
            -5.549574851989746,
            -1.5597140789031982,
            -6.8389763832092285,
            -1.7177993059158325,
            1.839387059211731,
            2.017911434173584,
            2.8760738372802734,
            6.183915138244629,
            2.0397660732269287,
            2.411543846130371,
            0.40302008390426636,
            2.774911403656006,
            -1.5897393226623535,
            6.607913970947266,
            6.349510669708252,
            0.23854666948318481,
            -0.09939080476760864,
            2.1869988441467285,
            -0.41936177015304565,
            2.1551337242126465,
            -1.3719067573547363,
            3.043302059173584,
            1.3696973323822021,
            1.9668923616409302,
            2.1621170043945312,
            0.04451477527618408,
            -8.081127166748047,
            -2.9921183586120605,
            5.137522220611572,
            -1.2772746086120605,
            -1.8366405963897705,
            1.903778076171875,
            -2.356440544128418,
            1.745069146156311,
            -0.9998250007629395,
            0.8974890112876892,
            1.7602412700653076,
            2.6511635780334473,
            3.5671844482421875,
            2.027188301086426,
            1.6888397932052612,
            -1.1827133893966675,
            -6.2540130615234375,
            1.1351122856140137,
            -2.8153162002563477,
            3.08823823928833,
            -3.7104403972625732,
            4.096634387969971,
            -2.905972957611084,
            -2.3523173332214355,
            -1.6972957849502563,
            -0.09374344348907471,
            -3.4665884971618652,
            4.87806510925293,
            1.4092159271240234,
            -2.431650161743164,
            5.092033386230469,
            3.096895694732666,
            1.573970079421997,
            -2.41554594039917,
            0.259496808052063,
            2.865898609161377,
            -6.099139213562012,
            -1.6196876764297485,
            4.332488059997559,
            2.669027805328369,
            -1.4887514114379883,
            -0.2938896119594574,
            2.351651668548584,
            1.5368309020996094,
            -7.581474781036377,
            1.1382575035095215,
            0.7024047374725342,
            -0.1927015781402588,
            1.3172380924224854,
            0.7413482666015625,
            -0.2601622939109802,
            -0.9919014573097229,
            -2.4035422801971436,
            -0.8842223882675171,
            -2.431859254837036,
            2.3436944484710693,
            4.261767387390137,
            1.1198549270629883,
            -0.8716481924057007,
            -1.0213977098464966,
            -0.10413134098052979,
            -4.173368453979492,
            5.429996490478516,
            -0.42043060064315796,
            1.6791584491729736,
            0.5976294279098511,
            3.9744443893432617,
            1.0912437438964844,
            0.26197123527526855,
            -4.651421070098877,
            3.2280993461608887,
            -1.3222646713256836,
            -3.364480495452881,
            2.17050838470459,
            -0.2087087631225586,
            1.291875958442688,
            1.6737182140350342,
            -2.0156004428863525,
            5.605942726135254,
            5.4775919914245605,
            0.2987901568412781,
            4.633661270141602,
            2.004032611846924,
            -1.5350353717803955,
            -2.0255308151245117,
            -0.2585793733596802,
            0.5993091464042664,
            -4.953894138336182,
            -1.3405520915985107,
            -3.8553390502929688,
            2.5197865962982178,
            -7.420642852783203,
            3.5020532608032227,
            1.7339245080947876,
            -1.392255425453186,
            -0.7762239575386047,
            1.039988398551941,
            -0.4707414507865906,
            -3.7417314052581787,
            4.420422554016113,
            3.9180097579956055,
            5.461361885070801,
            2.368687391281128,
            -5.156826972961426,
            -4.446164131164551,
            -0.903662383556366,
            0.9354169368743896,
            3.025033950805664,
            -0.46849584579467773,
            0.03189420700073242,
            -1.7507189512252808,
            -2.05973482131958,
            -3.8356809616088867,
            -8.280420303344727,
            0.8130646347999573,
            -1.5262280702590942,
            0.4904325604438782,
            4.719501495361328,
            0.5251227021217346,
            -4.5325775146484375,
            -2.819671154022217,
            -0.09680293500423431,
            -0.8973230123519897,
            0.4406687617301941,
            0.7013270854949951,
            -1.7126286029815674,
            -1.1770035028457642,
            -0.19717282056808472,
            -5.360917091369629,
            -1.1269021034240723,
            -2.08328914642334,
            2.748526096343994,
            -3.414693832397461,
            -3.181912899017334,
            4.4784393310546875,
            -1.200812578201294,
            -0.9682466387748718,
            1.0373034477233887,
            1.280382752418518,
            1.383638620376587,
            3.631916046142578,
            0.9101787805557251,
            1.430887222290039,
            5.450361728668213,
            -0.8111922740936279,
            -4.752367973327637,
            1.3442648649215698,
            -4.7443342208862305,
            -1.2984716892242432,
            -2.1582043170928955,
            3.8773138523101807,
            -0.3525354862213135,
            -0.44711706042289734,
            -1.1990647315979004,
            -0.6783885955810547,
            -0.3074214458465576,
            -0.15874886512756348,
            3.328348398208618,
            1.0852371454238892,
            -0.4990707337856293,
            -3.114002227783203,
            -2.759373188018799,
            -2.375537157058716,
            2.021228790283203,
            2.2418854236602783,
            3.5277833938598633,
            1.7827688455581665,
            7.070772647857666,
            -0.47763076424598694,
            2.6210947036743164,
            5.923313140869141,
            2.819455146789551,
            -3.139059543609619,
            -5.553384304046631,
            -0.9612836241722107,
            -3.080587387084961,
            -0.6669198870658875,
            -2.057734727859497,
            -2.797760248184204,
            5.1862287521362305,
            0.6277163028717041,
            4.8612165451049805,
            -1.7649734020233154,
            -2.0233213901519775,
            3.2349114418029785,
            -4.921237945556641,
            -1.459390640258789,
            2.238842248916626,
            3.159742832183838,
            0.1667855978012085,
            2.3554673194885254,
            -2.418081760406494,
            3.5932321548461914,
            1.7276310920715332,
            -1.0660576820373535,
            -1.3326177597045898,
            3.2569665908813477,
            4.981888771057129,
            -2.1720571517944336,
            0.909456193447113,
            0.9076261520385742,
            -4.141773223876953,
            2.834517478942871,
            0.09422937035560608,
            10.647598266601562,
            -1.1180315017700195,
            2.252995252609253,
            -0.9598420858383179,
            -2.9795351028442383,
            -3.4516239166259766,
            -1.449005126953125,
            -1.0791258811950684,
            -0.45341914892196655,
            1.4547916650772095,
            1.09050714969635,
            -0.45434460043907166,
            -2.0314974784851074,
            -0.32088911533355713,
            4.654754161834717,
            1.3091297149658203,
            2.7696032524108887,
            1.4551331996917725,
            -0.3785594701766968,
            -1.9548752307891846,
            1.6308889389038086,
            1.8148881196975708,
            -0.031119585037231445,
            0.04413330554962158,
            -7.140394687652588,
            0.5216043591499329,
            3.490355968475342,
            4.219094276428223,
            -1.9068057537078857,
            -6.063345909118652,
            -4.93796443939209,
            -5.793428897857666,
            -1.5513191223144531,
            3.7289042472839355,
            -2.923952579498291,
            -2.973832130432129,
            4.354146957397461,
            2.5957045555114746,
            -2.5117077827453613,
            2.4818317890167236,
            0.2157210111618042,
            2.4746172428131104,
            0.7720742225646973,
            0.17824822664260864,
            -3.104034900665283,
            -1.0289456844329834,
            -1.5073728561401367,
            -1.7323195934295654,
            1.5757278203964233,
            0.1265023946762085,
            2.220515727996826,
            5.140544891357422,
            0.3839503526687622,
            -1.3525278568267822,
            -0.6081117391586304,
            2.705272674560547,
            3.2459986209869385,
            2.35758638381958,
            -5.377368927001953,
            1.8051217794418335,
            5.293420791625977,
            -0.8799567222595215,
            1.4661774635314941,
            1.1262381076812744,
            -1.5287261009216309,
            1.2253533601760864,
            -3.120755672454834,
            -0.5478885769844055,
            -2.105076789855957,
            5.950356960296631,
            1.6230617761611938,
            0.021178454160690308,
            0.7230992317199707,
            3.3090662956237793,
            0.013329684734344482,
            3.987484931945801,
            -2.7093145847320557,
            5.455787658691406,
            0.8400332927703857,
            2.250458240509033,
            1.4730594158172607,
            -0.6958763003349304,
            -3.9224729537963867,
            -2.039278507232666,
            -0.12551259994506836,
            -2.6079506874084473,
            2.300415277481079,
            -0.9448091983795166,
            1.5581814050674438,
            -0.4137890934944153,
            -2.169250965118408,
            1.5576329231262207,
            -0.9560167789459229,
            -3.118812084197998,
            -2.4537715911865234,
            2.323045253753662,
            0.8495321869850159,
            -0.4339349865913391,
            1.8037631511688232,
            0.1352677345275879,
            0.9485418200492859,
            -7.65705680847168,
            -0.20145711302757263,
            -2.808697462081909,
            1.50088369846344,
            0.09122252464294434,
            -1.1152520179748535,
            1.7849785089492798,
            -3.1864094734191895,
            -2.3074216842651367,
            5.054505348205566,
            4.361888885498047,
            -0.9659390449523926,
            -6.1078362464904785,
            -1.4137794971466064,
            2.3820345401763916,
            3.3071482181549072,
            -2.9996845722198486,
            -3.013705253601074,
            4.667428016662598,
            2.370059013366699,
            1.9057323932647705,
            -0.5404999256134033,
            2.3044307231903076,
            -3.1142780780792236,
            -5.093266010284424,
            5.682435035705566,
            -1.7633397579193115,
            4.176653861999512,
            1.554901361465454,
            -3.3635761737823486,
            0.6526148319244385,
            1.9688737392425537,
            0.5449058413505554,
            -5.541353702545166,
            -3.0161209106445312,
            -3.8937478065490723,
            -1.2157971858978271,
            0.3301358222961426,
            0.2636701166629791,
            2.8149542808532715,
            1.5466476678848267,
            -1.166602611541748,
            1.3182297945022583,
            1.3335931301116943,
            1.4213013648986816,
            -3.5134975910186768,
            1.7259749174118042,
            1.0644949674606323,
            0.2765474319458008,
            0.65659499168396,
            -1.6471192836761475,
            0.47053200006484985,
            2.4429078102111816,
            -0.7182185649871826,
            0.47560635209083557,
            -1.6348438262939453,
            3.114914894104004,
            -0.10080274939537048,
            2.760127067565918,
            -1.5417439937591553,
            1.8742682933807373,
            1.4588418006896973,
            5.39316463470459,
            7.337038993835449,
            5.207956790924072,
            -2.2761940956115723,
            -3.718935966491699,
            0.5896928310394287,
            -2.059823513031006,
            4.059874057769775,
            0.8920530080795288,
            -2.899991273880005,
            1.0061101913452148,
            -1.3351106643676758,
            2.119636058807373,
            -0.11142221093177795,
            4.946602821350098,
            -3.1908674240112305,
            1.0581048727035522,
            -4.138335227966309,
            -0.42144089937210083,
            -3.3549702167510986,
            2.9263386726379395,
            -0.8608074188232422,
            1.8842239379882812,
            2.252228260040283,
            -0.7739589810371399,
            -4.115506172180176,
            -4.3034515380859375,
            -1.221282958984375,
            -0.24212145805358887,
            1.5002741813659668,
            3.9213151931762695,
            0.4058758318424225,
            -1.4393298625946045,
            -5.639766216278076,
            1.8763772249221802,
            -5.8866448402404785,
            0.20921900868415833,
            -5.764666557312012,
            0.047740429639816284,
            0.09055334329605103,
            -2.5434646606445312,
            -1.1478006839752197,
            -2.088564872741699,
            4.5034685134887695,
            3.403268814086914,
            -0.3518669009208679,
            0.3966102600097656,
            1.6686303615570068,
            -0.06012467294931412,
            -0.7665573954582214,
            -2.3673763275146484,
            -0.09032678604125977,
            -4.212015151977539,
            -4.126784801483154,
            -3.3969645500183105,
            3.1235365867614746,
            -4.359222412109375,
            -2.6813461780548096,
            4.108343124389648,
            -3.8286538124084473,
            4.208003997802734,
            0.19118016958236694,
            -2.4176788330078125,
            -4.733850955963135,
            -2.8654277324676514,
            -1.358811616897583,
            -3.1409010887145996,
            -0.23363226652145386,
            0.5976852178573608,
            -3.4460153579711914,
            -0.1503322422504425,
            0.6213871836662292,
            6.092701435089111,
            4.817128658294678,
            0.38346603512763977,
            -4.761276721954346,
            1.3521339893341064,
            4.4721174240112305,
            2.068467617034912,
            -0.28942790627479553,
            -1.8855880498886108,
            -1.8536704778671265,
            3.289944648742676,
            15.318538665771484,
            -2.5829763412475586,
            -4.20481538772583,
            -3.7159957885742188,
            3.6404147148132324,
            -2.4841742515563965,
            -4.985347747802734,
            0.8644292950630188,
            -1.3720440864562988,
            2.162656784057617,
            0.25383007526397705,
            2.1325275897979736,
            3.2638025283813477,
            -2.0877859592437744,
            -0.37288421392440796,
            4.547753810882568,
            -4.496650695800781,
            1.864834189414978,
            -1.7673543691635132,
            0.960542619228363,
            1.688686490058899,
            1.562720537185669,
            0.908489465713501,
            -0.04648900032043457,
            -2.419456720352173,
            2.8579609394073486,
            2.189873695373535,
            -0.5413221120834351,
            -2.9392306804656982,
            0.16902470588684082,
            3.0749001502990723,
            -1.0524883270263672,
            1.7100205421447754,
            -1.6580085754394531,
            -3.0545575618743896,
            0.8237249255180359,
            3.8732686042785645,
            1.409228801727295,
            1.990049123764038,
            1.1419248580932617,
            -3.5397772789001465,
            0.11300235986709595,
            -5.6289567947387695,
            -0.4879807233810425,
            -0.4702818989753723,
            -0.6807633638381958,
            3.7423934936523438,
            -4.655683517456055,
            -5.01938533782959,
            2.0846753120422363,
            -3.4076523780822754,
            1.5387853384017944,
            -0.1618616133928299,
            -0.45310282707214355,
            -0.8315733671188354,
            5.976194381713867,
            -0.9988607168197632,
            -0.3492298424243927,
            3.692697763442993,
            -1.628231167793274,
            2.8069558143615723,
            0.03219986706972122,
            -2.127458095550537,
            0.22742502391338348,
            3.066776752471924,
            5.713593482971191,
            0.13722959160804749,
            -0.7088025808334351,
            2.758841037750244,
            3.217223644256592,
            4.380037307739258,
            2.2368247509002686,
            1.4190952777862549,
            -0.7605382204055786,
            -1.9970026016235352,
            -1.9827628135681152,
            1.0019702911376953,
            -0.6193255186080933,
            -1.9451770782470703,
            10.730103492736816,
            -3.9423813819885254,
            1.3683404922485352,
            3.93912410736084,
            0.3341600298881531,
            6.990513801574707,
            -5.9485273361206055,
            3.853522300720215,
            0.09902513027191162,
            -2.4977810382843018,
            1.8302085399627686,
            0.8070058226585388,
            -3.867441177368164,
            2.503197193145752,
            2.8857195377349854,
            5.724486827850342,
            -0.8560739755630493,
            -4.9760050773620605,
            -3.254106044769287,
            -1.4645054340362549,
            -4.466486930847168,
            5.693670272827148,
            3.0454654693603516,
            2.029973030090332,
            -1.4959239959716797,
            -0.38636478781700134,
            -3.268768310546875,
            0.8553277850151062,
            -0.18809863924980164,
            -3.6019701957702637,
            1.8162202835083008,
            3.1136043071746826,
            -5.209556579589844,
            -0.1609574258327484,
            1.0514779090881348,
            -2.450040817260742,
            -2.3929924964904785,
            -1.3285927772521973,
            0.7439748048782349,
            1.9569976329803467,
            1.1864310503005981,
            -1.826498031616211,
            -2.9690051078796387,
            -2.5140724182128906,
            -1.8470144271850586,
            -2.5680155754089355,
            2.7655062675476074,
            -5.3849639892578125,
            -3.7042770385742188,
            -0.22121021151542664,
            1.7467981576919556,
            0.9492549896240234,
            0.9315885305404663,
            -0.7131222486495972,
            1.630582332611084,
            -2.504403591156006,
            -3.120612144470215,
            1.070780873298645,
            3.3269691467285156,
            -1.656012773513794,
            -1.8079732656478882,
            4.547637462615967,
            -4.546389102935791,
            2.3649001121520996,
            5.608503341674805,
            0.13446366786956787,
            -1.8011987209320068,
            -0.1367175281047821,
            -0.7791447639465332,
            -0.33815285563468933,
            -0.23589220643043518,
            -2.8091955184936523,
            -2.2382590770721436,
            4.218472480773926,
            -0.6668373346328735,
            2.750838279724121,
            -3.1483781337738037
        ]
    },
    "authors": [
        {
            "authorId": "46698300",
            "name": "Ji Lin"
        },
        {
            "authorId": "2214687479",
            "name": "Jiaming Tang"
        },
        {
            "authorId": "150127950",
            "name": "Haotian Tang"
        },
        {
            "authorId": "2202210853",
            "name": "Shang Yang"
        },
        {
            "authorId": "2219266839",
            "name": "Xingyu Dang"
        },
        {
            "authorId": "2115659426",
            "name": "Song Han"
        }
    ],
    "references": [
        {
            "paperId": "2141ed804636a1cf339d606cd03fd3b3e9582133",
            "title": "VILA: On Pre-training for Visual Language Models"
        },
        {
            "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
            "title": "Mistral 7B"
        },
        {
            "paperId": "94972e30504017156ef5b5debc419bf6edc67384",
            "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"
        },
        {
            "paperId": "4309d572a37d655779f9dce6a2c98c66334132de",
            "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "b37b1dc72b1882858f5120f2cd6883134089a6ed",
            "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
        },
        {
            "paperId": "697e0add95e880bd42e00bef838181e105f91981",
            "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"
        },
        {
            "paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce",
            "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"
        },
        {
            "paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
            "title": "Evaluating Object Hallucination in Large Vision-Language Models"
        },
        {
            "paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
            "title": "StarCoder: may the source be with you!"
        },
        {
            "paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679",
            "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"
        },
        {
            "paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3",
            "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"
        },
        {
            "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
            "title": "PaLM-E: An Embodied Multimodal Language Model"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1",
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "8e2930ac8ae9a758b367513cccb7d562f354afea",
            "title": "Who Says Elephants Can\u2019t Run: Bringing Large Scale MoE Models into Cloud Scale Production"
        },
        {
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
        },
        {
            "paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6",
            "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
        },
        {
            "paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
            "title": "Scaling Instruction-Finetuned Language Models"
        },
        {
            "paperId": "3f6243097a58e386aea1215fed4f372dee07a100",
            "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
        },
        {
            "paperId": "d3135733aa39dec20ce72aa138589dda27c8406d",
            "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "6d5c5e0df5dc4bd0bcd524d2a22d65d672bd0b74",
            "title": "TensorIR: An Abstraction for Automatic Tensorized Program Optimization"
        },
        {
            "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
            "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
        },
        {
            "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
            "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
        },
        {
            "paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
            "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"
        },
        {
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback"
        },
        {
            "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
            "title": "Training Verifiers to Solve Math Word Problems"
        },
        {
            "paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
            "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"
        },
        {
            "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
            "title": "Finetuned Language Models Are Zero-Shot Learners"
        },
        {
            "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
            "title": "Program Synthesis with Large Language Models"
        },
        {
            "paperId": "8a0a7170977cf5c94d9079b351562077b78df87a",
            "title": "A White Paper on Neural Network Quantization"
        },
        {
            "paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e",
            "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
        },
        {
            "paperId": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3",
            "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"
        },
        {
            "paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"
        },
        {
            "paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72",
            "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"
        },
        {
            "paperId": "8acc99c96a9cce2a14e049f756f608dab3491f24",
            "title": "MCUNet: Tiny Deep Learning on IoT Devices"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "0c0dfe47afcec2e229015f3c8f213d4c88e86b28",
            "title": "Up or Down? Adaptive Rounding for Post-Training Quantization"
        },
        {
            "paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f",
            "title": "Triton: an intermediate language and compiler for tiled neural network computations"
        },
        {
            "paperId": "d77123b54dcc8014949584ab624e97298617bcad",
            "title": "Data-Free Quantization Through Weight Equalization and Bias Correction"
        },
        {
            "paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907",
            "title": "Towards VQA Models That Can Read"
        },
        {
            "paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c",
            "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"
        },
        {
            "paperId": "dc160709bbe528b506a37ead334f60d258413357",
            "title": "Learned Step Size Quantization"
        },
        {
            "paperId": "54c4642d017830e1faddbb49f0377228d2b01493",
            "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision"
        },
        {
            "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
            "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
        },
        {
            "paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c",
            "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"
        },
        {
            "paperId": "49e60f82d6ae835c56473464f67ca5c11d3e95ec",
            "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks"
        },
        {
            "paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294",
            "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
            "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "642d0f49b7826adcf986616f4af77e736229990f",
            "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
        },
        {
            "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "title": "Learning both Weights and Connections for Efficient Neural Network"
        },
        {
            "paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628",
            "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"
        },
        {
            "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
            "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
        },
        {
            "paperId": "98d7207066b61c15e007dac9758530b99ac06cb9",
            "title": "The Enron Corpus: A New Dataset for Email Classification Research"
        },
        {
            "paperId": "2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75",
            "title": "Grounding Language Models to Images for Multimodal Generation"
        },
        {
            "paperId": "81051b830a4f5606106765902a51ba281c9230f9",
            "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"
        },
        {
            "paperId": "104f7a96eba307056e1038e183ee8c24d009ba13",
            "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"
        },
        {
            "paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603",
            "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"
        },
        {
            "paperId": null,
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
        },
        {
            "paperId": null,
            "title": "Blip-2: Boot-strapping language-image pre-training with frozen image encoders and large language models"
        },
        {
            "paperId": null,
            "title": "Stanford alpaca: An instruction-following llama model"
        },
        {
            "paperId": null,
            "title": "Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
        },
        {
            "paperId": null,
            "title": "MLC-Team"
        }
    ]
}