{
    "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
    "externalIds": {
        "DBLP": "conf/icml/DauphinFAG17",
        "ArXiv": "1612.08083",
        "MAG": "2567070169",
        "CorpusId": 16119010
    },
    "title": "Language Modeling with Gated Convolutional Networks",
    "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",
    "venue": "International Conference on Machine Learning",
    "year": 2016,
    "referenceCount": 36,
    "citationCount": 2108,
    "influentialCitationCount": 195,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens, is developed and is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2921469",
            "name": "Yann Dauphin"
        },
        {
            "authorId": "144270981",
            "name": "Angela Fan"
        },
        {
            "authorId": "2325985",
            "name": "Michael Auli"
        },
        {
            "authorId": "2529182",
            "name": "David Grangier"
        }
    ],
    "references": [
        {
            "paperId": "79baf48bd560060549998d7b61751286de062e2a",
            "title": "Factorization tricks for LSTM networks"
        },
        {
            "paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586",
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
        },
        {
            "paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
            "title": "Improving Neural Language Models with a Continuous Cache"
        },
        {
            "paperId": "98445f4172659ec5e891e031d8202c102135c644",
            "title": "Neural Machine Translation in Linear Time"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a",
            "title": "Efficient softmax approximation for GPUs"
        },
        {
            "paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51",
            "title": "Conditional Image Generation with PixelCNN Decoders"
        },
        {
            "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"
        },
        {
            "paperId": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "title": "Exploring the Limits of Language Modeling"
        },
        {
            "paperId": "41f1d50c85d3180476c4c7b3eea121278b0d8474",
            "title": "Pixel Recurrent Neural Networks"
        },
        {
            "paperId": "759956bb98689dbcc891528636d8994e54318f85",
            "title": "Strategies for Training Large Vocabulary Neural Language Models"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "12a5b7190b981bf478b4c9c04d3c0d41f13b9023",
            "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies"
        },
        {
            "paperId": "c58a0c5fccea5781a3e4d3282e024b9d20a24623",
            "title": "Predicting distributions with Linearizing Belief Networks"
        },
        {
            "paperId": "8645643ad5dfe662fa38f61615432d5c9bdf2ffb",
            "title": "genCNN: A Convolutional Architecture for Word Sequence Prediction"
        },
        {
            "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
        },
        {
            "paperId": "0dc9eb7d17f2def56ad930945f2521653f04c3fa",
            "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation"
        },
        {
            "paperId": "08c13be14da51f2ed531ffe980bb993e45042e41",
            "title": "Automatic Speech Recognition: A Deep Learning Approach"
        },
        {
            "paperId": "5d833331b0e22ff359db05c62a8bca18c4f04b68",
            "title": "One billion word benchmark for measuring progress in statistical language modeling"
        },
        {
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
            "paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
        },
        {
            "paperId": "0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c",
            "title": "Statistical machine translation"
        },
        {
            "paperId": "bd7d93193aad6c4b71cc8942e808753019e87706",
            "title": "Three new graphical models for statistical language modelling"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "f7566f1797eb36429acbb09e581d6b2918a50760",
            "title": "Foundations of Statistical Natural Language Processing"
        },
        {
            "paperId": "563e821bb5ea825efb56b77484f5287f08cf3753",
            "title": "Convolutional networks for images, speech, and time series"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "title": "An Empirical Study of Smoothing Techniques for Language Modeling"
        },
        {
            "paperId": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "title": "Improved backing-off for M-gram language modeling"
        },
        {
            "paperId": "ce8cca19455e8d3055c57a9bafe882984c95a201",
            "title": "Syntactic Process"
        },
        {
            "paperId": "3449b65008b27f6e60a73d80c1fd990f0481126b",
            "title": "Torch7: A Matlab-like Environment for Machine Learning"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "c19fbefdeead6a4154a22a9c8551a18b1530033a",
            "title": "Hierarchical Probabilistic Neural Network Language Model"
        },
        {
            "paperId": null,
            "title": "with Gated Convolutional Networks"
        }
    ]
}