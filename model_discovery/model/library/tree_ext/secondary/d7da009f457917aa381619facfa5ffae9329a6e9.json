{
    "paperId": "d7da009f457917aa381619facfa5ffae9329a6e9",
    "externalIds": {
        "DBLP": "conf/acl/PapineniRWZ02",
        "MAG": "2101105183",
        "ACL": "P02-1040",
        "DOI": "10.3115/1073083.1073135",
        "CorpusId": 11080756
    },
    "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
    "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2002,
    "referenceCount": 5,
    "citationCount": 24740,
    "influentialCitationCount": 5704,
    "openAccessPdf": {
        "url": "https://dl.acm.org/doi/pdf/10.3115/1073083.1073135",
        "status": "BRONZE"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "3323275",
            "name": "K. Papineni"
        },
        {
            "authorId": "46924970",
            "name": "Salim Roukos"
        },
        {
            "authorId": "144582029",
            "name": "T. Ward"
        },
        {
            "authorId": "2587983",
            "name": "Wei-Jing Zhu"
        }
    ],
    "references": [
        {
            "paperId": "3d4e0cdf981747af1d5c687e3c8238f791f95733",
            "title": "Corpus-based comprehensive and diagnostic MT evaluation: initial Arabic, Chinese, French, and Spanish results"
        },
        {
            "paperId": "bf45f9e578cb4b43a2604d6149553ae8cfee3016",
            "title": "The ARPA MT Evaluation Methodologies: Evolution, Lessons, and Future Approaches"
        },
        {
            "paperId": null,
            "title": "Additional mt-eval references"
        },
        {
            "paperId": null,
            "title": "Toward \ufb01nely differentiated evaluation metrics for machine translation"
        },
        {
            "paperId": "682cdcae291aee14bfc49a038b65c7f7701212ae",
            "title": "Proficiency and Performance in Language Testing."
        }
    ]
}