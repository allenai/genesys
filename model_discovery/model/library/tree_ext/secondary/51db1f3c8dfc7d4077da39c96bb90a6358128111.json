{
    "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
    "externalIds": {
        "ArXiv": "1603.09382",
        "DBLP": "conf/eccv/HuangSLSW16",
        "MAG": "2949892913",
        "DOI": "10.1007/978-3-319-46493-0_39",
        "CorpusId": 6773885
    },
    "title": "Deep Networks with Stochastic Depth",
    "abstract": null,
    "venue": "European Conference on Computer Vision",
    "year": 2016,
    "referenceCount": 35,
    "citationCount": 2204,
    "influentialCitationCount": 320,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Stochastic depth is proposed, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time and reduces training time substantially and improves the test error significantly on almost all data sets that were used for evaluation."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "143983679",
            "name": "Gao Huang"
        },
        {
            "authorId": "2117103358",
            "name": "Yu Sun"
        },
        {
            "authorId": "2109168016",
            "name": "Zhuang Liu"
        },
        {
            "authorId": "3371029",
            "name": "Daniel Sedra"
        },
        {
            "authorId": "7446832",
            "name": "Kilian Q. Weinberger"
        }
    ],
    "references": [
        {
            "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "title": "Identity Mappings in Deep Residual Networks"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "23d1f6005128756b5d40f1384980da8351d70d50",
            "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks"
        },
        {
            "paperId": "4b88e948121a87f00fb5aa0081d2044dde51ee36",
            "title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree"
        },
        {
            "paperId": "b92aa7024b87f50737b372e5df31ef091ab54e62",
            "title": "Training Very Deep Networks"
        },
        {
            "paperId": "4b39a79a1dc8e578e38ee4b7f00b4ec9ded50dd0",
            "title": "Recurrent convolutional neural network for object recognition"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "93bc65d2842b8cc5f3cf72ebc5b8f75daeacea35",
            "title": "Scalable Bayesian Optimization Using Deep Neural Networks"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "93100ebe89840bc235c586ddc6daccd262707fec",
            "title": "Learning Activation Functions to Improve Deep Neural Networks"
        },
        {
            "paperId": "33af9298e5399269a12d4b9901492fe406af62b4",
            "title": "Striving for Simplicity: The All Convolutional Net"
        },
        {
            "paperId": "55dda8f230566867acbfaa7bdd08fd8c7b8721ed",
            "title": "Fractional Max-Pooling"
        },
        {
            "paperId": "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd",
            "title": "Deeply-Supervised Nets"
        },
        {
            "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "title": "Going deeper with convolutions"
        },
        {
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
        },
        {
            "paperId": "1109b663453e78a59e4f66446d71720ac58cec25",
            "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
        },
        {
            "paperId": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "title": "Network In Network"
        },
        {
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning"
        },
        {
            "paperId": "38f35dd624cd1cf827416e31ac5e0e0454028eca",
            "title": "Regularization of Neural Networks using DropConnect"
        },
        {
            "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "title": "Maxout Networks"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
            "paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
            "title": "Why Does Unsupervised Pre-training Help Deep Learning?"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "title": "Learning long-term dependencies with gradient descent is difficult"
        },
        {
            "paperId": "6843f4f392280fb1a9dc6c16e072dffc97f4b2e4",
            "title": "On the power of small-depth threshold circuits"
        },
        {
            "paperId": null,
            "title": "Training and investigating residual nets (2016"
        },
        {
            "paperId": null,
            "title": "92.45% on cifar-10"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda",
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"
        },
        {
            "paperId": "3449b65008b27f6e60a73d80c1fd990f0481126b",
            "title": "Torch7: A Matlab-like Environment for Machine Learning"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        },
        {
            "paperId": "995a3b11cc8a4751d8e167abc4aa937abc934df0",
            "title": "The Cascade-Correlation Learning Architecture"
        },
        {
            "paperId": "3067cab09b04637260b85716c605ba578dafec54",
            "title": "Computational limitations of small-depth circuits"
        }
    ]
}