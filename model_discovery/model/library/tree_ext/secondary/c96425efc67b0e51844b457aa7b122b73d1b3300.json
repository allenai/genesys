{
    "paperId": "c96425efc67b0e51844b457aa7b122b73d1b3300",
    "externalIds": {
        "ArXiv": "2405.20935",
        "DBLP": "journals/corr/abs-2405-20935",
        "DOI": "10.48550/arXiv.2405.20935",
        "CorpusId": 270199692
    },
    "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice",
    "abstract": "The increasing size of deep neural networks necessitates effective model compression to improve computational efficiency and reduce their memory footprint. Sparsity and quantization are two prominent compression methods that have individually demonstrated significant reduction in computational and memory footprints while preserving model accuracy. While effective, the interplay between these two methods remains an open question. In this paper, we investigate the interaction between these two methods and assess whether their combination impacts final model accuracy. We mathematically prove that applying sparsity before quantization is the optimal sequence for these operations, minimizing error in computation. Our empirical studies across a wide range of models, including OPT and Llama model families (125M-8B) and ViT corroborate these theoretical findings. In addition, through rigorous analysis, we demonstrate that sparsity and quantization are not orthogonal; their interaction can significantly harm model accuracy, with quantization error playing a dominant role in this degradation. Our findings extend to the efficient deployment of large models in resource-limited compute platforms and reduce serving cost, offering insights into best practices for applying these compression methods to maximize efficacy without compromising accuracy.",
    "venue": "arXiv.org",
    "year": 2024,
    "referenceCount": 67,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is mathematically prove that applying sparsity before quantization is the optimal sequence for these operations, minimizing error in computation, and demonstrates that sparsity and quantization are not orthogonal; their interaction can significantly harm model accuracy."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.2472622394561768,
            0.5614201426506042,
            -2.2742412090301514,
            4.51645565032959,
            -3.4509191513061523,
            0.6246248483657837,
            4.8092732429504395,
            -0.15711882710456848,
            -0.4998227059841156,
            -2.9514830112457275,
            -2.404689311981201,
            1.59134840965271,
            0.37003499269485474,
            -2.209355354309082,
            -6.87434196472168,
            0.4838508069515228,
            -3.824856758117676,
            2.7493228912353516,
            3.6166458129882812,
            3.7953474521636963,
            -4.345822334289551,
            0.2896750569343567,
            -3.931819438934326,
            0.9627580642700195,
            -0.7055241465568542,
            0.6500487923622131,
            -3.943922996520996,
            -2.3928346633911133,
            2.252377510070801,
            -0.3446422815322876,
            1.6919294595718384,
            -4.126804351806641,
            7.700937271118164,
            -2.819667339324951,
            6.438986778259277,
            -1.0821614265441895,
            0.42525601387023926,
            5.880367279052734,
            -2.7149574756622314,
            -0.32064223289489746,
            0.6718030571937561,
            -0.3242633640766144,
            2.2830209732055664,
            -2.5733718872070312,
            -1.946700930595398,
            0.959999680519104,
            -3.077914237976074,
            1.0253937244415283,
            1.6695994138717651,
            3.2774057388305664,
            3.5433273315429688,
            1.076124906539917,
            1.8221406936645508,
            -0.0499764084815979,
            -0.30205675959587097,
            -2.0355818271636963,
            2.1625282764434814,
            -0.836577296257019,
            0.6798637509346008,
            -1.632399082183838,
            2.8839914798736572,
            -1.4833122491836548,
            -1.5660247802734375,
            1.705803632736206,
            1.8683972358703613,
            -3.1991260051727295,
            2.9469871520996094,
            6.487693786621094,
            1.5044811964035034,
            -2.565378427505493,
            -0.00618290901184082,
            -4.165110111236572,
            2.738365888595581,
            0.18875466287136078,
            -6.630557537078857,
            -1.491806983947754,
            -0.8289384841918945,
            -5.208507061004639,
            -2.719964027404785,
            -2.098269462585449,
            0.91498863697052,
            3.4636752605438232,
            0.5166731476783752,
            3.2951111793518066,
            4.86116886138916,
            -0.7866777777671814,
            -4.2176923751831055,
            -2.741072177886963,
            0.49080005288124084,
            -1.379392147064209,
            1.5909961462020874,
            1.232495903968811,
            -0.7197818160057068,
            0.3805572986602783,
            -3.181328773498535,
            -0.6688141822814941,
            0.2686702609062195,
            1.8789037466049194,
            -0.6979392766952515,
            1.0554792881011963,
            1.1045417785644531,
            0.40895703434944153,
            2.610466480255127,
            -0.7572922110557556,
            2.5512609481811523,
            -2.5239663124084473,
            0.011665701866149902,
            2.008939027786255,
            -0.19608724117279053,
            -3.728644847869873,
            -0.28921589255332947,
            2.408722400665283,
            -1.4810607433319092,
            -0.3071904182434082,
            -3.7452428340911865,
            -0.5254390835762024,
            -0.14108040928840637,
            0.3814923167228699,
            1.8651565313339233,
            7.730396270751953,
            -1.1557799577713013,
            -2.6792590618133545,
            -1.3647661209106445,
            1.1611334085464478,
            2.2772536277770996,
            3.3066725730895996,
            2.471683979034424,
            -0.770561695098877,
            -5.185152053833008,
            -4.876510143280029,
            -1.428959846496582,
            -2.5510199069976807,
            4.0295305252075195,
            0.7660920023918152,
            0.6853104829788208,
            -0.8373071551322937,
            -2.685802459716797,
            1.8447067737579346,
            -5.270339488983154,
            -1.5969061851501465,
            3.843285322189331,
            1.495256781578064,
            0.7772673964500427,
            0.36900949478149414,
            3.969451904296875,
            5.009291648864746,
            -0.02612847089767456,
            2.108520984649658,
            -0.13324332237243652,
            4.162010669708252,
            2.6767585277557373,
            -3.812059164047241,
            -0.800710916519165,
            4.123287200927734,
            2.49141788482666,
            0.6716858148574829,
            -0.2623744606971741,
            1.03473961353302,
            -0.7037896513938904,
            1.373835563659668,
            -0.3170452415943146,
            -1.0650842189788818,
            -9.113188743591309,
            -2.9614319801330566,
            5.362717628479004,
            -2.904289484024048,
            -2.532792568206787,
            -2.5906126499176025,
            -3.3455827236175537,
            3.324692726135254,
            0.6838491559028625,
            4.553389549255371,
            4.446771621704102,
            2.540635108947754,
            -0.9670711755752563,
            3.5874176025390625,
            4.814913749694824,
            -2.7166051864624023,
            0.19771820306777954,
            2.3599071502685547,
            -1.5148431062698364,
            -1.2151498794555664,
            -5.3191022872924805,
            -1.2891721725463867,
            -4.296262741088867,
            0.37624314427375793,
            -1.9106361865997314,
            3.1998510360717773,
            0.582682728767395,
            2.3693625926971436,
            1.6334638595581055,
            -2.072845935821533,
            2.852440118789673,
            6.051870346069336,
            1.9282199144363403,
            0.8035667538642883,
            0.8345564007759094,
            -0.7214653491973877,
            -0.3014172911643982,
            0.6749471426010132,
            4.172337532043457,
            1.3646390438079834,
            -1.4957382678985596,
            -1.5917712450027466,
            3.8445186614990234,
            0.24582576751708984,
            -3.7874276638031006,
            4.4918975830078125,
            1.0538147687911987,
            0.947844386100769,
            0.7595325112342834,
            2.6481924057006836,
            0.6440674066543579,
            0.8721654415130615,
            -4.300130367279053,
            -1.488832950592041,
            -3.198854923248291,
            2.4946329593658447,
            1.6127727031707764,
            2.205235004425049,
            -0.6842715740203857,
            -4.080887794494629,
            -3.1780571937561035,
            -3.7880747318267822,
            2.7044296264648438,
            -1.5939481258392334,
            1.8525731563568115,
            2.4610438346862793,
            1.735236406326294,
            2.1886346340179443,
            1.2600339651107788,
            -4.780557632446289,
            -1.027315378189087,
            -1.0873883962631226,
            -7.363560676574707,
            2.256971836090088,
            -1.9578981399536133,
            1.7283631563186646,
            0.6532139778137207,
            2.1688647270202637,
            3.826317310333252,
            -1.2100754976272583,
            2.509878158569336,
            3.358883857727051,
            1.9761338233947754,
            0.8697358965873718,
            -2.6718969345092773,
            0.335523396730423,
            0.5446058511734009,
            -4.429890155792236,
            0.590732991695404,
            0.4049187898635864,
            0.4238854944705963,
            -4.176928520202637,
            1.3346343040466309,
            3.032687187194824,
            3.865344762802124,
            -0.10768024623394012,
            0.2307894229888916,
            -1.179700493812561,
            -3.549050807952881,
            5.355027198791504,
            1.933127522468567,
            3.995313882827759,
            0.7160488367080688,
            -3.489126682281494,
            -5.797284126281738,
            -0.9789063334465027,
            -0.7727720141410828,
            0.9903087615966797,
            -0.38840967416763306,
            0.011868119239807129,
            0.28408652544021606,
            -4.073307991027832,
            -2.743039608001709,
            -4.841164588928223,
            -0.14627337455749512,
            -0.3538511395454407,
            1.6637940406799316,
            3.6540088653564453,
            -0.1735551953315735,
            -3.84444522857666,
            -0.7129677534103394,
            -0.8614757061004639,
            -1.471829891204834,
            -4.515540599822998,
            2.798457145690918,
            -1.2036131620407104,
            -2.889695167541504,
            -0.3520938456058502,
            -3.5815436840057373,
            1.0414226055145264,
            -2.502042055130005,
            0.6690951585769653,
            -4.45590353012085,
            1.098090648651123,
            2.57659912109375,
            -2.1320765018463135,
            -2.458005905151367,
            0.1414424180984497,
            -2.1619443893432617,
            1.7421358823776245,
            3.052459478378296,
            0.44513022899627686,
            3.1536831855773926,
            6.024293422698975,
            -0.7389612197875977,
            -2.112269878387451,
            1.041060209274292,
            0.19695132970809937,
            -1.3000521659851074,
            -2.090639352798462,
            1.7619675397872925,
            -2.228896141052246,
            -1.5570287704467773,
            -1.8334810733795166,
            2.1425890922546387,
            0.11558187007904053,
            -0.33971038460731506,
            4.886688709259033,
            -1.1571753025054932,
            -0.5393669605255127,
            -3.212404251098633,
            -5.444769859313965,
            -3.4804129600524902,
            -1.3230185508728027,
            4.400426864624023,
            3.6195993423461914,
            -2.564664840698242,
            3.349083423614502,
            0.9320789575576782,
            4.630192756652832,
            5.2910566329956055,
            -0.17889243364334106,
            -1.0557293891906738,
            -3.049628257751465,
            -0.6858850717544556,
            0.456607460975647,
            0.5820494890213013,
            -0.725953221321106,
            -1.560279369354248,
            3.7974460124969482,
            -1.3728656768798828,
            4.2784881591796875,
            -6.694328784942627,
            -1.3670021295547485,
            2.8055613040924072,
            -4.6862711906433105,
            0.783995509147644,
            -0.7495125532150269,
            -0.3887707591056824,
            -0.1534208208322525,
            4.187619686126709,
            -2.3676509857177734,
            1.04207444190979,
            0.4610314667224884,
            1.0131975412368774,
            -1.0570796728134155,
            4.469447135925293,
            1.681908130645752,
            1.1832027435302734,
            -0.737421989440918,
            -0.4676121771335602,
            -0.9817306995391846,
            -1.053483247756958,
            -3.1136534214019775,
            8.752119064331055,
            -1.3382447957992554,
            0.20379650592803955,
            -1.750462293624878,
            0.6272590160369873,
            -5.6491498947143555,
            -1.9682329893112183,
            -0.5197656154632568,
            -3.1139729022979736,
            -2.201366424560547,
            1.675286054611206,
            -2.0910744667053223,
            2.16292667388916,
            0.40435412526130676,
            3.4396376609802246,
            3.267054319381714,
            1.342713475227356,
            2.706338882446289,
            0.35794198513031006,
            -0.02425098419189453,
            0.926591157913208,
            1.3496553897857666,
            1.0232638120651245,
            0.15506523847579956,
            -1.6741547584533691,
            2.1636292934417725,
            -2.4002537727355957,
            3.4130895137786865,
            -1.483008861541748,
            -4.248142242431641,
            -5.519430160522461,
            -4.431705951690674,
            -1.1897192001342773,
            0.22548097372055054,
            0.18342524766921997,
            -1.0743811130523682,
            5.086160659790039,
            2.2280619144439697,
            -5.562727451324463,
            -0.9021738767623901,
            2.901397228240967,
            0.8564435839653015,
            -0.8978279829025269,
            1.216383457183838,
            -2.568399429321289,
            -1.0235062837600708,
            -1.4503697156906128,
            -3.8085432052612305,
            1.5032057762145996,
            -0.006691962480545044,
            -3.0225026607513428,
            2.4774482250213623,
            3.2390236854553223,
            3.2116403579711914,
            -0.0033394694328308105,
            4.538765907287598,
            3.244415044784546,
            5.488903999328613,
            -1.482560157775879,
            7.687051773071289,
            2.044712543487549,
            0.852465033531189,
            0.848351240158081,
            3.2029733657836914,
            -3.1336331367492676,
            3.99127197265625,
            -2.686122179031372,
            3.3159477710723877,
            -0.20984375476837158,
            5.336230278015137,
            -0.38368427753448486,
            -0.5477800965309143,
            0.6852380037307739,
            -0.2048404961824417,
            2.2841663360595703,
            2.1910600662231445,
            -2.285836696624756,
            4.8390607833862305,
            0.0447995662689209,
            1.2076494693756104,
            -1.3193879127502441,
            0.21849443018436432,
            -1.3953907489776611,
            -5.305278301239014,
            2.656398296356201,
            -5.030745029449463,
            -0.33739757537841797,
            -0.6732696890830994,
            -0.21705138683319092,
            1.2203476428985596,
            -2.3743929862976074,
            -3.7746894359588623,
            1.715815544128418,
            -2.0184929370880127,
            -3.568777322769165,
            0.14917001128196716,
            -0.5225591063499451,
            -0.6482717394828796,
            -1.8269195556640625,
            1.9409769773483276,
            -0.3910372257232666,
            -3.4491782188415527,
            1.1252609491348267,
            1.7383333444595337,
            1.5307581424713135,
            0.1209036111831665,
            0.00010515749454498291,
            -4.3869781494140625,
            1.2224152088165283,
            -2.1022109985351562,
            5.553290843963623,
            0.9604693651199341,
            -0.8491250276565552,
            -3.14320707321167,
            -2.6199917793273926,
            -3.4976272583007812,
            3.3495044708251953,
            -3.06095290184021,
            0.0849267840385437,
            5.186917781829834,
            1.2263380289077759,
            4.59619140625,
            3.49027156829834,
            -0.7188910245895386,
            -1.1881704330444336,
            0.5871666669845581,
            3.595755100250244,
            0.704309344291687,
            5.142843723297119,
            2.3184595108032227,
            -4.937378406524658,
            4.98193359375,
            1.6587491035461426,
            0.7008423805236816,
            -2.6784510612487793,
            -4.903346538543701,
            -2.410275936126709,
            -1.9041221141815186,
            -2.617218255996704,
            3.4521801471710205,
            0.9357487559318542,
            -0.5258345603942871,
            0.404112845659256,
            1.6952073574066162,
            3.2488625049591064,
            1.032033920288086,
            -4.2217254638671875,
            0.6461840271949768,
            1.8728702068328857,
            0.5860401391983032,
            0.38299548625946045,
            0.10350781679153442,
            -2.692746162414551,
            3.5070128440856934,
            -2.4272782802581787,
            2.024311065673828,
            0.8390722274780273,
            1.6877268552780151,
            0.12966153025627136,
            2.935040235519409,
            -4.9828386306762695,
            1.9307137727737427,
            0.9958481788635254,
            4.1059956550598145,
            5.689692497253418,
            4.487287521362305,
            -0.3359954059123993,
            -2.8378076553344727,
            -2.934868812561035,
            -1.22077214717865,
            5.085874557495117,
            3.4780447483062744,
            -4.41876745223999,
            -5.0252604484558105,
            -0.6967220306396484,
            1.4993116855621338,
            0.4392395615577698,
            -0.3815497159957886,
            -2.6904706954956055,
            3.437106132507324,
            -3.6327896118164062,
            -2.365626573562622,
            -2.577296733856201,
            4.351509094238281,
            -3.1956276893615723,
            -2.230905532836914,
            4.196130275726318,
            -0.7619525194168091,
            -4.443675994873047,
            -2.13431715965271,
            0.9308422803878784,
            0.07820302248001099,
            1.5410730838775635,
            3.3416731357574463,
            0.5640366077423096,
            1.7620923519134521,
            -1.992133617401123,
            2.8451547622680664,
            -1.8514750003814697,
            -0.9314003586769104,
            -2.4943761825561523,
            3.2340927124023438,
            -0.2669494152069092,
            -2.3738300800323486,
            -0.5254731774330139,
            -2.8246700763702393,
            3.659374713897705,
            -1.1733061075210571,
            3.4368996620178223,
            1.9727413654327393,
            4.595119476318359,
            -0.5980403423309326,
            -3.755239248275757,
            0.6691372394561768,
            -0.14079928398132324,
            -5.1876220703125,
            -4.001851558685303,
            -4.7498779296875,
            -0.4712443947792053,
            -5.509687900543213,
            -2.782106399536133,
            2.9231886863708496,
            -3.439364433288574,
            -0.285096675157547,
            2.3321006298065186,
            -5.368768692016602,
            -0.716742753982544,
            -1.6868419647216797,
            1.4680135250091553,
            -0.4570542275905609,
            4.743734359741211,
            1.1932655572891235,
            -0.5421311855316162,
            -1.2905704975128174,
            1.5488977432250977,
            5.4173736572265625,
            6.297386646270752,
            0.4325413107872009,
            -2.6233322620391846,
            0.881214439868927,
            0.2438363879919052,
            0.498760461807251,
            -2.120340585708618,
            -3.5119991302490234,
            2.794063091278076,
            2.4251902103424072,
            14.14962100982666,
            -1.1358660459518433,
            -1.428553581237793,
            -3.85365629196167,
            1.9545985460281372,
            -2.6855242252349854,
            0.4691925346851349,
            4.396949768066406,
            0.2955610156059265,
            0.6918090581893921,
            -0.4474222660064697,
            -2.820518970489502,
            3.1877963542938232,
            1.2834981679916382,
            -1.7609049081802368,
            0.6968677043914795,
            -4.706304550170898,
            -0.8634353280067444,
            -0.2792476415634155,
            0.20622067153453827,
            0.9090964198112488,
            0.894166111946106,
            0.3630185127258301,
            -2.0235397815704346,
            -0.3514665365219116,
            0.6112492084503174,
            3.294509172439575,
            1.1007823944091797,
            -5.562131404876709,
            1.5163013935089111,
            4.496347904205322,
            0.2840692400932312,
            1.5444084405899048,
            2.8284130096435547,
            -1.8755509853363037,
            0.28951263427734375,
            5.925314426422119,
            1.8804185390472412,
            1.9551045894622803,
            -0.12288007140159607,
            -2.0616376399993896,
            1.4236619472503662,
            -1.5507426261901855,
            3.0631608963012695,
            -1.8014793395996094,
            -2.056126117706299,
            1.206559181213379,
            -3.8118395805358887,
            -4.579033851623535,
            2.1434006690979004,
            -2.10101056098938,
            2.4646992683410645,
            -2.634028434753418,
            -0.5544913411140442,
            1.2728971242904663,
            3.5190207958221436,
            -1.5703810453414917,
            3.5101051330566406,
            0.07127737998962402,
            0.7520273923873901,
            3.6995110511779785,
            1.208627462387085,
            -4.344938278198242,
            -3.415523052215576,
            1.5567903518676758,
            -0.5918593406677246,
            -2.9501700401306152,
            0.044539570808410645,
            1.8677349090576172,
            0.7186146378517151,
            4.1914286613464355,
            -1.0561368465423584,
            2.869457721710205,
            -2.870546817779541,
            2.0276763439178467,
            -1.9622783660888672,
            0.4216688275337219,
            0.8597652912139893,
            -0.9861763715744019,
            9.846475601196289,
            -2.258327007293701,
            2.4703731536865234,
            -2.7454817295074463,
            -0.4970703125,
            8.48176383972168,
            -3.145754814147949,
            6.513947486877441,
            -4.5506439208984375,
            -0.578289270401001,
            4.4015703201293945,
            1.131007194519043,
            -1.0661118030548096,
            1.1405956745147705,
            -0.1128384917974472,
            6.766881942749023,
            -2.607534408569336,
            0.7493588328361511,
            -6.010754585266113,
            -5.427579879760742,
            -5.870246887207031,
            5.705569267272949,
            2.5540261268615723,
            1.4868006706237793,
            0.8498015403747559,
            1.9408812522888184,
            -0.2915424704551697,
            1.499545931816101,
            -5.606533050537109,
            -5.625881195068359,
            -0.6581060886383057,
            1.6269735097885132,
            -2.551398992538452,
            -0.8157424926757812,
            0.620481550693512,
            -2.012361764907837,
            -2.0649189949035645,
            0.7218179702758789,
            1.2760233879089355,
            0.9997342824935913,
            2.997342586517334,
            1.5925178527832031,
            -1.1053245067596436,
            -2.956160545349121,
            -2.863177537918091,
            1.1020259857177734,
            5.77847957611084,
            -0.2909529209136963,
            0.06643122434616089,
            -0.9538246393203735,
            -1.9315011501312256,
            -1.0837043523788452,
            -1.8236567974090576,
            -1.4647703170776367,
            2.0654520988464355,
            -1.6854764223098755,
            -4.1425323486328125,
            -0.44392669200897217,
            0.8742879629135132,
            -2.544184684753418,
            2.229668617248535,
            3.971264362335205,
            -2.3568263053894043,
            -4.691744804382324,
            9.47133731842041,
            -1.3248275518417358,
            -0.7120458483695984,
            -1.409271478652954,
            0.11335833370685577,
            -3.1628170013427734,
            0.07540106773376465,
            -0.1367524415254593,
            -2.84403395652771,
            5.752932548522949,
            -2.5014777183532715,
            -2.4040889739990234,
            -0.2436313033103943
        ]
    },
    "authors": [
        {
            "authorId": "2099930229",
            "name": "Simla Burcu Harma"
        },
        {
            "authorId": "2304327514",
            "name": "Ayan Chakraborty"
        },
        {
            "authorId": "2232955980",
            "name": "Elizaveta Kostenok"
        },
        {
            "authorId": "2304324484",
            "name": "Danila Mishin"
        },
        {
            "authorId": "2304324632",
            "name": "Dongho Ha"
        },
        {
            "authorId": "2304325109",
            "name": "Babak Falsafi"
        },
        {
            "authorId": "2256984280",
            "name": "Martin Jaggi"
        },
        {
            "authorId": "2304518596",
            "name": "Ming Liu"
        },
        {
            "authorId": "2304384181",
            "name": "Yunho Oh"
        },
        {
            "authorId": "1929462",
            "name": "Suvinay Subramanian"
        },
        {
            "authorId": "2296991252",
            "name": "Amir Yazdanbakhsh"
        }
    ],
    "references": [
        {
            "paperId": "63167c30b06aa6c3d76e09065ced0412090d6c3b",
            "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
        },
        {
            "paperId": "6bef2713cb85fda11d1c2ce25fca209e09148330",
            "title": "Progressive Gradient Flow for Robust N: M Sparsity Training in Transformers"
        },
        {
            "paperId": "d8799a2d06e4ecd3df91ff1d5c997a64a748d0a1",
            "title": "USM-Lite: Quantization and Sparsity Aware Fine-Tuning for Speech Recognition with Universal Speech Models"
        },
        {
            "paperId": "2c0312c604f9f7638bb4533b39e0ae81e7f6ab12",
            "title": "The Falcon Series of Open Language Models"
        },
        {
            "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
            "title": "Mistral 7B"
        },
        {
            "paperId": "9b98ca94b7733feee1cff5c57596611ad35fa7aa",
            "title": "Scaling Laws for Sparsely-Connected Foundation Models"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6",
            "title": "A Simple and Effective Pruning Approach for Large Language Models"
        },
        {
            "paperId": "07be590365e7fb76680be4ed67a5505763ec2d96",
            "title": "Boost Vision Transformer with GPU-Friendly Sparsity and Quantization"
        },
        {
            "paperId": "0f322ac4f9f12b11fe5d042508497148a84fe8a5",
            "title": "Dynamic Sparse Training with Structured Sparsity"
        },
        {
            "paperId": "3f2a55671ea1993893143811c3aa0b23323c0a52",
            "title": "FP8 versus INT8 for efficient deep learning inference"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "eebe95c31aba4b862e1bc3d312ca60094bd01f61",
            "title": "With Shared Microexponents, A Little Shifting Goes a Long Way"
        },
        {
            "paperId": "05deb6c1862b2f129d6652a09eaedbc1f655cc8f",
            "title": "STEP: Learning N: M Structured Sparsity Masks from Scratch with Precondition"
        },
        {
            "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
        },
        {
            "paperId": "a10dd5e0f74a14ca0e7c37a121bf77970f932e32",
            "title": "Training Recipe for N: M Structured Sparsity with Decaying Pruning Mask"
        },
        {
            "paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3",
            "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77",
            "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"
        },
        {
            "paperId": "c7fe31c57af039474350b16965e66c7c86794d28",
            "title": "FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding"
        },
        {
            "paperId": "7b16367b575d951a98f1762d8f45d7c0eb840581",
            "title": "OPQ: Compressing Deep Neural Networks with One-shot Pruning-Quantization"
        },
        {
            "paperId": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb",
            "title": "Accelerating Sparse Deep Neural Networks"
        },
        {
            "paperId": "f6a7982725f93b1af9519911f94ce6cc25774b2b",
            "title": "Ps and Qs: Quantization-Aware Pruning for Efficient Low Latency Neural Network Inference"
        },
        {
            "paperId": "b4d207a2096aee4a3764933373eef6edb574c952",
            "title": "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks"
        },
        {
            "paperId": "0c201c52260963665e0d30b3fe0fa31291af210b",
            "title": "VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference"
        },
        {
            "paperId": "1aa219b2069de522081980d2d9d764958197e545",
            "title": "Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "f46c562229c5bc419bbbfb63239431590e4b340a",
            "title": "Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"
        },
        {
            "paperId": "97bac618fc866ae7656660f3965e9aae37993232",
            "title": "Efficient Processing of Deep Neural Networks"
        },
        {
            "paperId": "9bb5665fe48e7122beda73a53316de9f7f243b19",
            "title": "APQ: Joint Search for Network Architecture, Pruning and Quantization Policy"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"
        },
        {
            "paperId": "2e3002f131e1815bda7a10303eff97f79dea01ec",
            "title": "Rigging the Lottery: Making All Tickets Winners"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "9dad5f3b491fba9496c90bb7cfe10d1c0f00fadc",
            "title": "Balanced Sparsity for Efficient DNN Inference on GPU"
        },
        {
            "paperId": "d5a1286e9b19fd250b7fa9b5574d4f71945ab3b3",
            "title": "Accelerator-Aware Pruning for Convolutional Neural Networks"
        },
        {
            "paperId": "339f5523a0cdeea315dc5affc27379ca57398264",
            "title": "Training DNNs with Hybrid Block Floating Point"
        },
        {
            "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
            "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
        },
        {
            "paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
            "title": "Mixed Precision Training"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9",
            "title": "Dynamic Network Surgery for Efficient DNNs"
        },
        {
            "paperId": "7601b995303f953955004db7b9b8b206c0e02ff8",
            "title": "Learning Structured Sparsity in Deep Neural Networks"
        },
        {
            "paperId": "642d0f49b7826adcf986616f4af77e736229990f",
            "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
        },
        {
            "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "title": "Learning both Weights and Connections for Efficient Neural Network"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724",
            "title": "Optimal Brain Surgeon and general network pruning"
        },
        {
            "paperId": "72c03b873e8c5cd86b15bf73186df341da4731c9",
            "title": "Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models"
        },
        {
            "paperId": "8264257f573696fc0a1ef7531c825041832197f8",
            "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases"
        },
        {
            "paperId": "b5c01bfc2ce8cf2ffa5dd15d7b95583f2e4f1cf1",
            "title": "Be Like Water: Adaptive Floating Point for Machine Learning"
        },
        {
            "paperId": "121c3860d5e3917c658dfac7e1f56ba297e87a6c",
            "title": "Channel Permutations for N: M Sparsity"
        },
        {
            "paperId": "516903ffc5afc446bf498322d8589096d98f3767",
            "title": "DominoSearch: Find layer-wise fine-grained N: M sparse schemes from dense neural networks"
        },
        {
            "paperId": null,
            "title": "BFloat16: The Secret to High Performance on Cloud TPUs"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        },
        {
            "paperId": null,
            "title": "Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training"
        },
        {
            "paperId": null,
            "title": "Open Models Based on Gemini Research and Technology"
        },
        {
            "paperId": null,
            "title": "vs Quantization: Which is Better"
        },
        {
            "paperId": null,
            "title": "Quantized Sparse Training: A Unified Trainable Framework for Joint Pruning and Quantization in DNNs"
        },
        {
            "paperId": null,
            "title": "Gemini: A Family of Highly Capable Multimodal Models"
        },
        {
            "paperId": null,
            "title": "Enabling next-generation AI workloads: Announcing TPU v5p and AI Hy-percomputer"
        },
        {
            "paperId": null,
            "title": "FP8 Formats"
        },
        {
            "paperId": null,
            "title": "We can also see that the values of \u03b5 i are much lower that the values of \u03b5 t in most of the cases in both orders. 162 This proves our second hypothesis"
        },
        {
            "paperId": null,
            "title": "MicroXcaling: A Library for Microservices Autoscaling"
        },
        {
            "paperId": null,
            "title": "NVIDIA H100 Tensor Core GPU Architecture. Technical report"
        },
        {
            "paperId": null,
            "title": "OCP Microscaling (MX) Specification"
        }
    ]
}