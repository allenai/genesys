{
    "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
    "externalIds": {
        "ArXiv": "2208.07339",
        "DBLP": "journals/corr/abs-2208-07339",
        "DOI": "10.48550/arXiv.2208.07339",
        "CorpusId": 251564521
    },
    "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
    "abstract": "Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.",
    "venue": "arXiv.org",
    "year": 2022,
    "referenceCount": 82,
    "citationCount": 470,
    "influentialCitationCount": 62,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2208.07339",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance, and makes such models much more accessible."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -4.366799354553223,
            -0.47440817952156067,
            -1.9497627019882202,
            5.334810733795166,
            0.8561508655548096,
            1.8016705513000488,
            2.7437822818756104,
            1.0999600887298584,
            -1.537866234779358,
            0.960643470287323,
            1.8505520820617676,
            1.8489283323287964,
            2.5133838653564453,
            -0.9906046986579895,
            -2.867567539215088,
            -3.541923999786377,
            -0.806519091129303,
            -0.9954078197479248,
            5.759733200073242,
            1.4572882652282715,
            -0.8068879842758179,
            1.1749701499938965,
            -3.283998727798462,
            5.2838616371154785,
            -1.6788554191589355,
            1.2961146831512451,
            -3.2743499279022217,
            -2.048964500427246,
            -2.8121793270111084,
            -0.38351577520370483,
            0.5153777599334717,
            -5.199913024902344,
            3.0656542778015137,
            -4.147723197937012,
            3.864942789077759,
            -0.0972018837928772,
            -2.5874812602996826,
            4.966315269470215,
            -5.40010404586792,
            -0.43723541498184204,
            0.657206654548645,
            -1.771745204925537,
            1.6379733085632324,
            -3.067826986312866,
            -0.8984881043434143,
            1.9216619729995728,
            -1.250025987625122,
            -0.809933602809906,
            -2.2270443439483643,
            2.7683472633361816,
            2.167966842651367,
            -0.056358277797698975,
            2.1949901580810547,
            0.07950838655233383,
            -0.954246461391449,
            0.32698723673820496,
            1.7802280187606812,
            0.2529746890068054,
            0.22141382098197937,
            -1.0774705410003662,
            4.912439823150635,
            3.6000847816467285,
            -1.9502785205841064,
            1.5150532722473145,
            2.820215940475464,
            0.20383986830711365,
            0.18232601881027222,
            1.5432090759277344,
            2.4373555183410645,
            0.772972047328949,
            -0.8086068034172058,
            -2.602654457092285,
            1.7197105884552002,
            -0.5874189138412476,
            -4.805928707122803,
            -2.1724019050598145,
            1.508337140083313,
            -4.674563407897949,
            -0.4108898639678955,
            -3.3686578273773193,
            -1.3182603120803833,
            4.216272830963135,
            -3.00110125541687,
            0.9685775637626648,
            3.2211828231811523,
            -1.1868479251861572,
            -1.2160472869873047,
            -1.352690577507019,
            1.1650524139404297,
            -1.0789364576339722,
            0.4129279851913452,
            1.138113021850586,
            2.652540445327759,
            0.8472225069999695,
            -2.267895221710205,
            -2.333371639251709,
            1.3004846572875977,
            -2.4404044151306152,
            -0.8533502817153931,
            1.6507855653762817,
            2.2509005069732666,
            -3.203939437866211,
            0.1190955638885498,
            -2.3951265811920166,
            0.9074536561965942,
            -4.0199103355407715,
            -1.7475470304489136,
            1.5558695793151855,
            -1.2397994995117188,
            -2.9131789207458496,
            -3.2637996673583984,
            1.7371788024902344,
            -2.17065167427063,
            1.0141026973724365,
            -4.052713394165039,
            -1.9674229621887207,
            1.703519582748413,
            -0.9888664484024048,
            -0.9446115493774414,
            3.764003038406372,
            -1.478277564048767,
            -2.3850793838500977,
            -2.8017754554748535,
            -1.3800044059753418,
            0.1723223626613617,
            2.7439756393432617,
            -0.5591386556625366,
            0.9503000378608704,
            -0.8462278842926025,
            -2.7439465522766113,
            0.28577953577041626,
            -0.9287810325622559,
            2.6292104721069336,
            -2.4404726028442383,
            1.2732129096984863,
            -0.3327881395816803,
            -2.4044559001922607,
            1.173803687095642,
            -1.8402786254882812,
            -0.48417240381240845,
            4.476694107055664,
            3.243260145187378,
            2.649557113647461,
            5.125936985015869,
            0.12378871440887451,
            2.8442189693450928,
            1.192105770111084,
            4.251505374908447,
            -2.0007200241088867,
            3.876215934753418,
            3.6068689823150635,
            -0.562476396560669,
            0.8993498682975769,
            1.059381127357483,
            -0.6139723062515259,
            3.1862778663635254,
            -3.4216556549072266,
            -0.6579110622406006,
            -0.19490918517112732,
            -0.6490858197212219,
            -3.044419765472412,
            1.3788714408874512,
            -8.56750774383545,
            0.8064927458763123,
            4.6001362800598145,
            -2.5529754161834717,
            -0.1486692726612091,
            -1.5807850360870361,
            1.4974631071090698,
            1.4427456855773926,
            -3.5633015632629395,
            0.5503365993499756,
            2.630727767944336,
            2.334315776824951,
            5.109227657318115,
            3.4253461360931396,
            2.227796792984009,
            -1.8204740285873413,
            -1.7361631393432617,
            -0.9898687601089478,
            -0.5680792927742004,
            2.9717421531677246,
            -5.750088214874268,
            2.138986587524414,
            -5.506466388702393,
            -0.6252515912055969,
            -2.9418692588806152,
            -1.5467009544372559,
            -0.8432704210281372,
            2.003222703933716,
            2.585489511489868,
            -2.3982017040252686,
            3.811159610748291,
            5.652042865753174,
            -0.038232386112213135,
            0.9831851720809937,
            2.6856536865234375,
            0.4916449189186096,
            -2.0346689224243164,
            0.07787734270095825,
            1.2255052328109741,
            -1.5437005758285522,
            -1.0274899005889893,
            -1.6247358322143555,
            2.9511685371398926,
            -1.154576301574707,
            -2.4208567142486572,
            2.3043508529663086,
            4.850368022918701,
            -1.9983866214752197,
            0.27980881929397583,
            -1.7509002685546875,
            -2.7655677795410156,
            1.9755549430847168,
            -1.7954883575439453,
            -4.999759197235107,
            -4.352999210357666,
            3.1115074157714844,
            3.9280567169189453,
            0.42872074246406555,
            1.6156259775161743,
            1.2742819786071777,
            -2.0748109817504883,
            -6.657268524169922,
            1.4139316082000732,
            -0.6657023429870605,
            0.9435299038887024,
            -0.6534507870674133,
            -0.7989057898521423,
            -0.08821851015090942,
            0.13718651235103607,
            -7.269842624664307,
            0.5926482081413269,
            0.06451629102230072,
            -3.17124080657959,
            0.06265613436698914,
            -1.7342562675476074,
            -0.9821770787239075,
            0.825351893901825,
            -2.323509693145752,
            5.943037033081055,
            3.0235064029693604,
            1.2938859462738037,
            2.67120361328125,
            2.338777542114258,
            -3.602134943008423,
            -1.1365044116973877,
            -0.4708300232887268,
            0.41520819067955017,
            -3.4554293155670166,
            0.5190051198005676,
            -0.7248275876045227,
            4.017689228057861,
            -2.846858263015747,
            1.4900749921798706,
            -1.2411890029907227,
            2.06046462059021,
            -3.3204498291015625,
            2.748157501220703,
            1.3348393440246582,
            -1.8511450290679932,
            5.707488059997559,
            0.08548058569431305,
            2.973362445831299,
            0.18998748064041138,
            1.1718881130218506,
            -2.4602911472320557,
            1.57485032081604,
            0.8090846538543701,
            1.930624008178711,
            2.629697561264038,
            1.7160801887512207,
            -1.2813783884048462,
            -1.1405160427093506,
            0.5197348594665527,
            -4.623486518859863,
            0.011740893125534058,
            1.2837930917739868,
            -0.6175351142883301,
            5.006725311279297,
            3.504685878753662,
            -2.0593600273132324,
            -3.7346034049987793,
            0.9424641132354736,
            -0.4892808496952057,
            -0.03717014193534851,
            0.28535380959510803,
            -0.13885164260864258,
            -1.2010369300842285,
            0.1739943027496338,
            -3.2641074657440186,
            0.4712807536125183,
            -5.267664909362793,
            -1.6267937421798706,
            -3.892908811569214,
            3.824199676513672,
            4.9065446853637695,
            0.47525888681411743,
            -1.6495556831359863,
            -0.22010549902915955,
            -0.4190026521682739,
            3.781524896621704,
            4.996901988983154,
            -0.5987119674682617,
            2.0173535346984863,
            3.2544703483581543,
            0.6908338665962219,
            -3.874755382537842,
            1.8059430122375488,
            -2.7372076511383057,
            -1.0991594791412354,
            0.12715601921081543,
            2.3588032722473145,
            -2.9857234954833984,
            0.49072831869125366,
            -2.1363842487335205,
            3.6811676025390625,
            -0.17436468601226807,
            -4.628199577331543,
            4.158510684967041,
            -0.4108249545097351,
            1.2546519041061401,
            -3.1860179901123047,
            -2.212656259536743,
            -0.5832853317260742,
            -0.027823686599731445,
            3.5402297973632812,
            4.963898181915283,
            -0.05331793054938316,
            3.9666190147399902,
            3.943159818649292,
            4.270240783691406,
            0.8731987476348877,
            3.766824722290039,
            1.9018969535827637,
            -3.960665702819824,
            -0.7780438661575317,
            0.3056067228317261,
            -0.1929144263267517,
            1.3188927173614502,
            0.5752391815185547,
            5.993953227996826,
            -1.4246989488601685,
            2.7769875526428223,
            -0.8005518913269043,
            -3.568225383758545,
            3.0962753295898438,
            2.4158949851989746,
            1.3667973279953003,
            0.4011361002922058,
            1.6433305740356445,
            -1.7262403964996338,
            4.981370449066162,
            -0.7018122673034668,
            -0.6416252851486206,
            3.1886587142944336,
            1.7463366985321045,
            -0.0796460509300232,
            1.7860984802246094,
            -0.06508985906839371,
            -0.4080055356025696,
            0.42782002687454224,
            3.491719961166382,
            -1.637073278427124,
            1.1548004150390625,
            -4.391895294189453,
            12.990153312683105,
            0.9912959337234497,
            1.590328335762024,
            -1.1894359588623047,
            -1.915001392364502,
            -1.350935697555542,
            -0.25738608837127686,
            1.394938349723816,
            -0.4701087474822998,
            0.9051542282104492,
            0.6277872323989868,
            -5.472752571105957,
            0.7134149074554443,
            -3.5192806720733643,
            -0.20386049151420593,
            4.036861419677734,
            1.501491904258728,
            3.465367317199707,
            -1.7016730308532715,
            1.7051899433135986,
            -0.34875503182411194,
            2.792728900909424,
            2.4822723865509033,
            -0.05125302076339722,
            -5.378823757171631,
            0.775847315788269,
            0.8794599175453186,
            2.4385111331939697,
            -5.007574081420898,
            -3.331498861312866,
            -3.2693874835968018,
            -2.040370225906372,
            1.025679588317871,
            3.0482213497161865,
            -0.1753869503736496,
            -0.7291795015335083,
            6.324925899505615,
            2.189979076385498,
            -4.740028381347656,
            0.32563334703445435,
            2.0111002922058105,
            1.6064424514770508,
            0.3502221703529358,
            0.7577624320983887,
            -4.598260879516602,
            -1.397864580154419,
            -2.3300817012786865,
            -3.0701141357421875,
            1.0573681592941284,
            -1.323805570602417,
            2.066340208053589,
            3.0533576011657715,
            0.6132745742797852,
            -1.0387775897979736,
            -3.8746464252471924,
            1.3571743965148926,
            4.307194232940674,
            5.829601287841797,
            -2.351491928100586,
            0.21388250589370728,
            4.705018043518066,
            1.4782960414886475,
            1.6417667865753174,
            0.5740229487419128,
            -1.789682388305664,
            2.254735231399536,
            -2.9356584548950195,
            0.2134745717048645,
            0.002812132239341736,
            0.20291352272033691,
            -3.988677740097046,
            -1.8615272045135498,
            1.4549100399017334,
            3.282482385635376,
            -0.27789247035980225,
            4.292797565460205,
            -0.1413951814174652,
            3.301438808441162,
            0.727558970451355,
            2.7580647468566895,
            -0.19032537937164307,
            -0.1357954442501068,
            -1.8245415687561035,
            -0.6340736746788025,
            1.209605097770691,
            -6.80315637588501,
            0.6967819929122925,
            -1.2434440851211548,
            0.8943889737129211,
            -2.04909086227417,
            -2.180647850036621,
            -0.5860346555709839,
            -0.19465871155261993,
            0.2557963728904724,
            -2.2860288619995117,
            3.8620710372924805,
            -1.3564763069152832,
            -0.8522568941116333,
            4.0634355545043945,
            1.3136471509933472,
            -0.1538974940776825,
            -3.841136932373047,
            0.4598712921142578,
            -2.3680872917175293,
            5.553395748138428,
            -1.2232613563537598,
            0.3458305299282074,
            -0.15839561820030212,
            -3.630689859390259,
            0.020564012229442596,
            2.311826229095459,
            -2.1482532024383545,
            -1.6903107166290283,
            -6.341456890106201,
            -1.5032373666763306,
            0.22051256895065308,
            3.6860857009887695,
            -2.050356388092041,
            -2.3214476108551025,
            4.439032554626465,
            1.8098841905593872,
            0.16425155103206635,
            0.678554117679596,
            2.0985312461853027,
            -0.8905758857727051,
            -3.4107370376586914,
            4.308591842651367,
            -4.05488395690918,
            1.935925006866455,
            0.9997372031211853,
            -4.107543468475342,
            -0.4864048957824707,
            2.4651870727539062,
            -1.4950666427612305,
            -4.927733898162842,
            -2.27712082862854,
            -1.9043489694595337,
            -4.429177284240723,
            -0.16322386264801025,
            2.6073272228240967,
            1.9324350357055664,
            2.589785099029541,
            -2.6110427379608154,
            -0.9336210489273071,
            2.970200777053833,
            1.0664417743682861,
            -4.992780685424805,
            0.5017609596252441,
            4.165284633636475,
            0.44118422269821167,
            1.6940560340881348,
            -1.8175896406173706,
            3.8523335456848145,
            2.733790397644043,
            -4.138296604156494,
            -0.8584458231925964,
            0.565940797328949,
            3.2399024963378906,
            -0.7273650765419006,
            -0.4420761168003082,
            -2.6166298389434814,
            2.128133535385132,
            0.23126769065856934,
            4.380100727081299,
            5.79464054107666,
            3.2995998859405518,
            -2.1751885414123535,
            -2.584639549255371,
            1.818562626838684,
            -0.42456790804862976,
            2.603006362915039,
            1.3224321603775024,
            -4.244104862213135,
            0.14009711146354675,
            -0.32335323095321655,
            1.4150373935699463,
            -1.889249563217163,
            1.6037635803222656,
            -1.957773208618164,
            0.47650694847106934,
            -0.5857278108596802,
            -1.2383513450622559,
            -3.9007482528686523,
            0.660794734954834,
            0.5727788805961609,
            2.1572117805480957,
            0.9905561208724976,
            -2.743131637573242,
            -3.9649713039398193,
            0.2577660083770752,
            -1.0618559122085571,
            -1.2465659379959106,
            -0.35606586933135986,
            6.9798197746276855,
            0.3785220682621002,
            -0.8329919576644897,
            0.8245108127593994,
            4.767321586608887,
            -4.956139087677002,
            -1.4249014854431152,
            -3.5921716690063477,
            3.0610780715942383,
            -0.6293509006500244,
            -3.79754638671875,
            -1.4283490180969238,
            0.7758975028991699,
            2.3731789588928223,
            1.7508388757705688,
            1.5845367908477783,
            2.6251182556152344,
            4.4136762619018555,
            0.041582852602005005,
            -2.1504201889038086,
            -1.836749792098999,
            -2.136415481567383,
            -1.747443437576294,
            -1.654857873916626,
            -3.662750244140625,
            1.7146583795547485,
            -3.8183112144470215,
            -3.9199724197387695,
            2.744438648223877,
            -4.796270847320557,
            1.252640724182129,
            -0.8411700129508972,
            -3.6532883644104004,
            -1.268306016921997,
            -3.8640952110290527,
            -2.32680082321167,
            -1.9731236696243286,
            2.3193957805633545,
            -2.640221118927002,
            -1.8008270263671875,
            1.7418417930603027,
            1.935163974761963,
            6.653339862823486,
            1.6072614192962646,
            1.276118516921997,
            -3.768627166748047,
            -1.264571189880371,
            1.575098991394043,
            2.5798702239990234,
            0.9866833090782166,
            -0.6727015972137451,
            -1.6084575653076172,
            1.1700091361999512,
            15.973705291748047,
            -0.16645868122577667,
            -2.8385443687438965,
            -2.8251781463623047,
            1.426347017288208,
            -4.446714401245117,
            0.4836846888065338,
            -0.7138761281967163,
            2.826343536376953,
            1.1639180183410645,
            -0.5430606007575989,
            -3.2033820152282715,
            1.3424513339996338,
            0.22931939363479614,
            -1.9671509265899658,
            -0.4500572383403778,
            -0.7002326250076294,
            3.1488900184631348,
            -4.290766716003418,
            -2.5109221935272217,
            -0.22968357801437378,
            1.8351988792419434,
            0.10320830345153809,
            -0.21238556504249573,
            -2.358873128890991,
            3.2223291397094727,
            3.3335981369018555,
            2.7920608520507812,
            -2.4034852981567383,
            0.8688316345214844,
            0.9143831133842468,
            4.087738037109375,
            2.4294867515563965,
            0.16278019547462463,
            -3.126147747039795,
            4.678421974182129,
            1.3583881855010986,
            -0.8413348197937012,
            1.1218317747116089,
            2.6052796840667725,
            -4.215268135070801,
            -1.3735096454620361,
            -3.2672793865203857,
            0.54677814245224,
            -0.2915987968444824,
            0.865759015083313,
            -0.3951139748096466,
            -5.011642932891846,
            -1.9049339294433594,
            1.4820001125335693,
            0.13922524452209473,
            -1.216225504875183,
            -2.406585693359375,
            -0.5687673091888428,
            1.2965996265411377,
            2.602581739425659,
            0.8937458992004395,
            -1.3495030403137207,
            0.2691880762577057,
            1.8144712448120117,
            2.6192896366119385,
            -0.664024829864502,
            -0.23459675908088684,
            -0.6861259937286377,
            -2.156233310699463,
            2.369321346282959,
            -2.5113985538482666,
            1.7820979356765747,
            2.8422977924346924,
            -0.20401279628276825,
            3.1945269107818604,
            1.0857282876968384,
            0.10504677146673203,
            -1.595421314239502,
            -1.6155993938446045,
            -1.9511477947235107,
            3.5636496543884277,
            -0.4878569543361664,
            -1.8847423791885376,
            6.727379322052002,
            -4.395689487457275,
            1.9477458000183105,
            -1.656951904296875,
            -2.9407293796539307,
            4.793162822723389,
            -4.928706169128418,
            5.276919364929199,
            2.1553046703338623,
            -2.9301676750183105,
            4.235871315002441,
            -1.62278413772583,
            -1.7140756845474243,
            4.038604259490967,
            5.402836799621582,
            5.456969261169434,
            -1.6725794076919556,
            -2.0127201080322266,
            -3.6333179473876953,
            -2.6400527954101562,
            -2.584880828857422,
            4.910335540771484,
            4.807684898376465,
            1.6964566707611084,
            -2.2282793521881104,
            -0.65770423412323,
            1.1713783740997314,
            -1.8653533458709717,
            -3.3981680870056152,
            -1.8213244676589966,
            1.9355145692825317,
            5.2801361083984375,
            -4.73504638671875,
            -1.287761926651001,
            1.1456547975540161,
            1.5274724960327148,
            -1.153820514678955,
            -0.396247923374176,
            2.297741413116455,
            -0.23065322637557983,
            3.931222438812256,
            -0.5741692781448364,
            0.24120554327964783,
            -3.69899845123291,
            -4.572364807128906,
            0.0024737119674682617,
            3.786337375640869,
            -0.876171886920929,
            2.001854658126831,
            0.9072524309158325,
            -1.3805826902389526,
            1.3003162145614624,
            -0.0017479360103607178,
            -3.117265224456787,
            0.6552514433860779,
            -1.3008683919906616,
            -3.6547365188598633,
            2.2008490562438965,
            1.3289258480072021,
            -1.7067580223083496,
            1.7465825080871582,
            4.888265132904053,
            -3.0168118476867676,
            -2.238677740097046,
            6.1542134284973145,
            -2.355116844177246,
            -1.3110970258712769,
            -0.6196249127388,
            -0.9742267727851868,
            -2.823749542236328,
            0.6594393253326416,
            -0.7937790155410767,
            -1.903343915939331,
            3.237602472305298,
            0.35419803857803345,
            -0.6015459895133972,
            -1.9914354085922241
        ]
    },
    "authors": [
        {
            "authorId": "3239480",
            "name": "Tim Dettmers"
        },
        {
            "authorId": "35084211",
            "name": "M. Lewis"
        },
        {
            "authorId": "2037496520",
            "name": "Younes Belkada"
        },
        {
            "authorId": "1982950",
            "name": "Luke Zettlemoyer"
        }
    ],
    "references": [
        {
            "paperId": "4247f45a5730e3bda5836e2bc7941e30f5b91cb7",
            "title": "Board"
        },
        {
            "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
            "title": "GLM-130B: An Open Bilingual Pre-trained Model"
        },
        {
            "paperId": "3f6243097a58e386aea1215fed4f372dee07a100",
            "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
        },
        {
            "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
            "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
        },
        {
            "paperId": "09171a3f87fcb94456eaabefc65731683374f983",
            "title": "Outliers Dimensions that Disrupt Transformers Are Driven by Frequency"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "d31a2a1b1d2378030aed23f6888bce02897e20e7",
            "title": "F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization"
        },
        {
            "paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170",
            "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"
        },
        {
            "paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04",
            "title": "8-bit Optimizers via Block-wise Quantization"
        },
        {
            "paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
            "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
        },
        {
            "paperId": "51b0c571d89bd2d39a194f60f91f0a03d74574b5",
            "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality"
        },
        {
            "paperId": "b4ac117b20e06d3e98d9eb3b28ca47e1a1e5dd5d",
            "title": "Automatic Mixed-Precision Quantization Search of BERT"
        },
        {
            "paperId": "5a09edeb26f9f116f2c0503cd020f38fb943f79b",
            "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers"
        },
        {
            "paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e",
            "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
        },
        {
            "paperId": "91cc5192a2c389a6b71b5adb6643c4eb14302f1f",
            "title": "FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference"
        },
        {
            "paperId": "c375e121926db9551f224ff235018ea38bb159b7",
            "title": "BinaryBERT: Pushing the Limit of BERT Quantization"
        },
        {
            "paperId": "829580d6fc73fa601c4982e2b1b6832f2796270b",
            "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings"
        },
        {
            "paperId": "b5613da1f643159c97cbf8555d6f5c4f05b36a9a",
            "title": "High Performance Natural Language Processing"
        },
        {
            "paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8",
            "title": "Scaling Laws for Autoregressive Generative Modeling"
        },
        {
            "paperId": "c6b5ecf84e304cda4dbd1687cf1902790d65cc6e",
            "title": "A Statistical Framework for Low-bitwidth Training of Deep Neural Networks"
        },
        {
            "paperId": "097210dc65924f8ce59523faf444e635523dc714",
            "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"
        },
        {
            "paperId": "503503ec4395ab0e36d4f0a190772f7785649319",
            "title": "Towards Fully 8-bit Integer Inference for the Transformer Model"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "c9d3c181d999b0e11c6e4c51b3f9aefd01489e0f",
            "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation"
        },
        {
            "paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39",
            "title": "Training with Quantization Noise for Extreme Model Compression"
        },
        {
            "paperId": "bc3706d600729f1b9007c91052258c7c22864f69",
            "title": "Binary Neural Networks: A Survey"
        },
        {
            "paperId": "6b648f92178c3091c66143cc577ff3b5a82aa430",
            "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks"
        },
        {
            "paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c",
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "c95383f251a62c63217586059c67f63507c3e839",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "d5096770ab37926e3921ef08ec2795fd895d2e06",
            "title": "Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks"
        },
        {
            "paperId": "55e960535f643637161b2e99a8c21a92c0d13757",
            "title": "Representation Degeneration Problem in Training Natural Language Generation Models"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "d796b68c1cce424d65581de35e93f7f969b71e89",
            "title": "Fully Quantized Network for Object Detection"
        },
        {
            "paperId": "7da843389b95ae2782ac8acd0eb7267b9734dcd6",
            "title": "Mixed Precision Training With 8-bit Floating Point"
        },
        {
            "paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d",
            "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"
        },
        {
            "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
            "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"
        },
        {
            "paperId": "dc160709bbe528b506a37ead334f60d258413357",
            "title": "Learned Step Size Quantization"
        },
        {
            "paperId": "9a1093af92d315def21b90918faf08665157051a",
            "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers"
        },
        {
            "paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f",
            "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"
        },
        {
            "paperId": "a8e1b91b0940a539aca302fb4e5c1f098e4e3860",
            "title": "LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks"
        },
        {
            "paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535",
            "title": "A Simple Method for Commonsense Reasoning"
        },
        {
            "paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f",
            "title": "Scaling Neural Machine Translation"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "d418295cd3027c43eccc5592ae5b8303ba8192be",
            "title": "Trained Ternary Quantization"
        },
        {
            "paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7",
            "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"
        },
        {
            "paperId": "123ae35aa7d6838c817072032ce5615bb891652d",
            "title": "BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1"
        },
        {
            "paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec",
            "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"
        },
        {
            "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
            "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
        },
        {
            "paperId": "1b82d54e9a3b06c603d7987ba3ecf437425f6330",
            "title": "Training deep neural networks with low precision multiplications"
        },
        {
            "paperId": "2144f5c7e3a9ed9f20138be5fb02503390c19fc5",
            "title": "Results of the WMT14 Metrics Shared Task"
        },
        {
            "paperId": "104f7a96eba307056e1038e183ee8c24d009ba13",
            "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"
        },
        {
            "paperId": "d2a2677f6594a316101176c607ad6eae094bba44",
            "title": "HAWQ-V3: Dyadic Neural Network Quantization"
        },
        {
            "paperId": null,
            "title": "A framework for few-shot language model evaluation"
        },
        {
            "paperId": null,
            "title": "If one views matrix multiplication as 1x1 convolution, vector-wise quantization is equivalent to channel-wise quantization for convolution combined with row quantization (Khudia et al., 2021)"
        },
        {
            "paperId": "e6cc6a7bd4db3e7604bae6a654ec29aa8542dafc",
            "title": "Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "c3cb27f9ef7176658f37b607e75cc2c37f5e0ea8",
            "title": "Accurate and Efficient 2-bit Quantized Neural Networks"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "QBERT (Shen et al., 2020), product quantization with quantization noise (Fan et al., 2020), TernaryBERT (Zhang et al., 2020), and BinaryBERT (Bai et al., 2021)"
        },
        {
            "paperId": null,
            "title": "Openwebtext corpus"
        },
        {
            "paperId": "e3ce71a26872c7755e6d8b8fc45bf00c8be64193",
            "title": "Neural Machine Translation Systems for WMT 16"
        },
        {
            "paperId": null,
            "title": "Courbariaux et al., 2015), 2 to 3-bit (Zhu et al., 2017"
        },
        {
            "paperId": null,
            "title": "a) If your work uses existing assets, did you cite the creators?"
        },
        {
            "paperId": null,
            "title": "Did you discuss any potential negative societal impacts of your work?"
        },
        {
            "paperId": null,
            "title": "b) Did you describe any potential participant risks, with links to Institutional Review"
        },
        {
            "paperId": null,
            "title": "(a) Did you state the full set of assumptions of all theoretical results"
        },
        {
            "paperId": null,
            "title": "(b) Did you mention the license of the assets? [No] The license is permissible for all the assets that we use"
        },
        {
            "paperId": null,
            "title": "Did you describe the limitations of your work? [Yes] See the limitation section (c) Did you discuss any potential negative societal impacts of your work?[Yes] See the Broader Impacts section"
        },
        {
            "paperId": null,
            "title": "Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating?"
        },
        {
            "paperId": null,
            "title": "c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?"
        },
        {
            "paperId": null,
            "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes"
        },
        {
            "paperId": null,
            "title": "You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description"
        },
        {
            "paperId": null,
            "title": "b) Did you describe the limitations of your work? [Yes] See the limitation section"
        },
        {
            "paperId": null,
            "title": "b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"
        },
        {
            "paperId": null,
            "title": "(a) Did you include the code, data, and instructions needed to reproduce the main experimental results"
        },
        {
            "paperId": null,
            "title": "c) Did you include any new assets either in the supplemental material or as a URL?"
        },
        {
            "paperId": null,
            "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them?"
        },
        {
            "paperId": null,
            "title": "a) Do the main claims made in the abstract and introduction accurately re\ufb02ect the paper\u2019s contributions and scope? [Yes]"
        }
    ]
}