{
    "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
    "externalIds": {
        "MAG": "2173051530",
        "DBLP": "journals/corr/KaiserS15",
        "ArXiv": "1511.08228",
        "CorpusId": 2009318
    },
    "title": "Neural GPUs Learn Algorithms",
    "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. \nWe present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. \nAn essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. \nTo achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.",
    "venue": "International Conference on Learning Representations",
    "year": 2015,
    "referenceCount": 36,
    "citationCount": 360,
    "influentialCitationCount": 29,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances, and a technique for training deep recurrent networks: parameter sharing relaxation is introduced."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "40527594",
            "name": "Lukasz Kaiser"
        },
        {
            "authorId": "1701686",
            "name": "I. Sutskever"
        }
    ],
    "references": [
        {
            "paperId": "4e8930ae948262a89acf2e43c8e8b6e902c312c4",
            "title": "Variable Rate Image Compression with Recurrent Neural Networks"
        },
        {
            "paperId": "1121ff5cdeaa470521b8dff084ba1424dd613cc1",
            "title": "Fast Algorithms for Convolutional Neural Networks"
        },
        {
            "paperId": "5b791cd374c7109693aaddee2c12d659ae4e3ec0",
            "title": "Grid Long Short-Term Memory"
        },
        {
            "paperId": "f9c990b1b5724e50e5632b94fdb7484ece8a6ce7",
            "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting"
        },
        {
            "paperId": "e837b79de602c69395498c1fbbe39bbb4e6f75ad",
            "title": "Learning to Transduce with Unbounded Memory"
        },
        {
            "paperId": "c9d7afd65b2127e6f0651bc7e13eeec1897ac8dd",
            "title": "Reinforcement Learning Neural Turing Machines"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081",
            "title": "LSTM: A Search Space Odyssey"
        },
        {
            "paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82",
            "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"
        },
        {
            "paperId": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40",
            "title": "Grammar as a Foreign Language"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "title": "Show and tell: A neural image caption generator"
        },
        {
            "paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de",
            "title": "Neural Turing Machines"
        },
        {
            "paperId": "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a",
            "title": "Learning to Execute"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f",
            "title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "c0b624c46b51920dfec5aa02cc86323c0beb0df5",
            "title": "Dropout Improves Recurrent Neural Networks for Handwriting Recognition"
        },
        {
            "paperId": "944a1cfd79dbfb6fef460360a0765ba790f4027a",
            "title": "Recurrent Continuous Translation Models"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "2a84e524a859450d0816e22ea6b981d448a7b5cc",
            "title": "Learning Games from Videos Guided by Descriptive Complexity"
        },
        {
            "paperId": "6658bbf68995731b2083195054ff45b4eca38b3a",
            "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"
        },
        {
            "paperId": "bd5fc28c7356915ec71abafbe86b7596c60720aa",
            "title": "The conference paper"
        },
        {
            "paperId": "aeed631d6a84100b5e9a021ec1914095c66de415",
            "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics"
        },
        {
            "paperId": "18b8ef71bc01b8658b4ef2c8b9a9e4e6e5c2a07b",
            "title": "Dimensions in program synthesis"
        },
        {
            "paperId": "72d531724d9305263f2c844daa80e5581fb2d36b",
            "title": "Inductive Programming: A Survey of Program Synthesis Techniques"
        },
        {
            "paperId": "ed9215b146bfd3a1db72a23fe5de8e2a3ae4c6c8",
            "title": "Automatic structures"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "225331d1700a9544545cc7c54a63c1b485269ce7",
            "title": "Learning Regular Sets from Queries and Counterexamples"
        },
        {
            "paperId": "bcd857d75841aa3e92cd4284a8818aba9f6c0c3f",
            "title": "Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS"
        },
        {
            "paperId": null,
            "title": "Listen, attend and spell"
        },
        {
            "paperId": "e4351041d25c272a008bcd5765868dc3a28fe470",
            "title": "Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks"
        },
        {
            "paperId": "a19a5e29b3eaf0223d0611451ce17b1a07c89aae",
            "title": "Introduction to Cellular automata"
        },
        {
            "paperId": "4b030084ce9df0666fe7c63222039a0dc7aa4be5",
            "title": "An Introduction to Cellular Automata"
        }
    ]
}