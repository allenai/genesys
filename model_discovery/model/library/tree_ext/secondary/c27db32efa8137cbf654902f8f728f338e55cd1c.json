{
    "paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c",
    "externalIds": {
        "MAG": "2766447205",
        "DBLP": "journals/nature/SilverSSAHGHBLB17",
        "DOI": "10.1038/nature24270",
        "CorpusId": 205261034,
        "PubMed": "29052630"
    },
    "title": "Mastering the game of Go without human knowledge",
    "abstract": null,
    "venue": "Nature",
    "year": 2017,
    "referenceCount": 68,
    "citationCount": 8639,
    "influentialCitationCount": 371,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An algorithm based solely on reinforcement learning is introduced, without human data, guidance or domain knowledge beyond game rules, that achieves superhuman performance, winning 100\u20130 against the previously published, champion-defeating AlphaGo."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "145824029",
            "name": "David Silver"
        },
        {
            "authorId": "4337102",
            "name": "Julian Schrittwieser"
        },
        {
            "authorId": "34838386",
            "name": "K. Simonyan"
        },
        {
            "authorId": "2460849",
            "name": "Ioannis Antonoglou"
        },
        {
            "authorId": "1885349",
            "name": "Aja Huang"
        },
        {
            "authorId": "35099444",
            "name": "A. Guez"
        },
        {
            "authorId": "2067208983",
            "name": "T. Hubert"
        },
        {
            "authorId": "2067596385",
            "name": "Lucas baker"
        },
        {
            "authorId": "40227832",
            "name": "Matthew Lai"
        },
        {
            "authorId": "34848283",
            "name": "Adrian Bolton"
        },
        {
            "authorId": "2275897",
            "name": "Yutian Chen"
        },
        {
            "authorId": "2542999",
            "name": "T. Lillicrap"
        },
        {
            "authorId": "2059412590",
            "name": "Fan Hui"
        },
        {
            "authorId": "2175946",
            "name": "L. Sifre"
        },
        {
            "authorId": "47568983",
            "name": "George van den Driessche"
        },
        {
            "authorId": "1686971",
            "name": "T. Graepel"
        },
        {
            "authorId": "48987704",
            "name": "D. Hassabis"
        }
    ],
    "references": [
        {
            "paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22",
            "title": "In-datacenter performance analysis of a tensor processing unit"
        },
        {
            "paperId": "160b49af60c5dabd1f7ce74afeedb5230bb417c8",
            "title": "Residual Networks for Computer Go"
        },
        {
            "paperId": "3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0",
            "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning"
        },
        {
            "paperId": "a2155552ca5afb784a3c1d67a5bcbd4e688b6e05",
            "title": "DeepStack: Expert-level artificial intelligence in heads-up no-limit poker"
        },
        {
            "paperId": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
            "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks"
        },
        {
            "paperId": "4c25f50c7451fa72c562e21e3b11e416b11f74c8",
            "title": "Learning to Act by Predicting the Future"
        },
        {
            "paperId": "a1d2a7ef81960846b9cec00bce8eefa06ccc8796",
            "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games"
        },
        {
            "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "title": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "title": "Mastering the game of Go with deep neural networks and tree search"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "04e3c20a738e2f922574b0a66483a37100187563",
            "title": "Better Computer Go Player with Neural Network and Long-term Prediction"
        },
        {
            "paperId": "4c7028640e3470a73af84d22eafa78855931c70f",
            "title": "Giraffe: Using Deep Reinforcement Learning to Play Chess"
        },
        {
            "paperId": "bb184a6de06a888d136089bc8d76cc70c7401a6e",
            "title": "Training Deep Convolutional Neural Networks to Play Go"
        },
        {
            "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
            "title": "Human-level control through deep reinforcement learning"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "127f464c2dc8d85b7612a6924495f79e5458710f",
            "title": "Move Evaluation in Go Using Deep Convolutional Neural Networks"
        },
        {
            "paperId": "b6cc21b30912bdaecd9f178d700a4c545b1d0838",
            "title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning"
        },
        {
            "paperId": "d746a1f64daae2d3fb91de8ffe08e9e5668cdc38",
            "title": "Approximate Policy Iteration Schemes: A Comparison"
        },
        {
            "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
            "title": "Reinforcement learning in robotics: A survey"
        },
        {
            "paperId": "69235e974adc94428021af15ae9cfb6b5c90fe55",
            "title": "Concurrent Reinforcement Learning from Customer Interactions"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "188cb1e85a59d286240b896cfea4d17bf35e2f3c",
            "title": "Temporal-difference search in computer Go"
        },
        {
            "paperId": "c37f1baac3c8ba30250084f067167ac3837cf6fd",
            "title": "A Survey of Monte Carlo Tree Search Methods"
        },
        {
            "paperId": "b72c321e465b16277236fb36b3f910635d561534",
            "title": "Approximate policy iteration: a survey and some new methods"
        },
        {
            "paperId": "c542aaafcf80a87b37ffa350344e65fe19b9c0ce",
            "title": "Monte-Carlo tree search and rapid action value estimation in computer Go"
        },
        {
            "paperId": "7c0c0445e89347798800aad3497fcf2f2d27d4e6",
            "title": "Multi-armed bandits with episode context"
        },
        {
            "paperId": "d57310247a9822fec7c484edb8ec131547341561",
            "title": "On the Scalability of Parallel UCT"
        },
        {
            "paperId": "1c099cf2b1080699434f9b22b9c5a02ebb4e7509",
            "title": "Bootstrapping from Game Tree Search"
        },
        {
            "paperId": "c79eee317f266d9966dc46c6157631314c940879",
            "title": "Using a monte-carlo approach for bus regulation"
        },
        {
            "paperId": "4ee7fa5ba18c71b5eb18a702391afea15edad0ba",
            "title": "Whole-History Rating: A Bayesian Rating System for Players of Time-Varying Strength"
        },
        {
            "paperId": "e635d81a617d1239232a9c9a11a196c53dab8240",
            "title": "Bandit Based Monte-Carlo Planning"
        },
        {
            "paperId": "02cc6a5944d57d2353a55639c7b77336b94f29b6",
            "title": "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search"
        },
        {
            "paperId": "fa25610fb8586c2b50a3654edc5bb42fa7fc4729",
            "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"
        },
        {
            "paperId": "e1f9cb45bd8f0300a013a3da3e15546bfeeb60d0",
            "title": "Reinforcement Learning as Classification: Leveraging Modern Classifiers"
        },
        {
            "paperId": "59b6c4b77c8d4bdb7bc7240d5a99f674c55d7a64",
            "title": "Empirical comparison of various reinforcement learning strategies for sequential targeted marketing"
        },
        {
            "paperId": "5acbb3f169bc13a0e6b3848adabf856c20edf9c2",
            "title": "World-championship-caliber Scrabble"
        },
        {
            "paperId": "49ca0eda8e224507d341d16a8c3fdb4d566cefe3",
            "title": "Computer Go"
        },
        {
            "paperId": "85941af287e2158bd201a633cbcc763693652c7f",
            "title": "Temporal Difference Learning Applied to a High-Performance Game-Playing Program"
        },
        {
            "paperId": "fb45465f0924795d4eb98d1bf1524d244a05ed3e",
            "title": "Learning to Play Chess Using Temporal Differences"
        },
        {
            "paperId": "03de8578480c53677c484e1facfced74f4f5b045",
            "title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit"
        },
        {
            "paperId": "9d57519d44de48454ab248802b3e1b96547f1aab",
            "title": "From Simple Features to Sophisticated Evaluation Functions"
        },
        {
            "paperId": "563e821bb5ea825efb56b77484f5287f08cf3753",
            "title": "Convolutional networks for images, speech, and time series"
        },
        {
            "paperId": "3552fba431aa866bf9de293bebf7eff168e9e19c",
            "title": "On-line Policy Improvement using Monte-Carlo Search"
        },
        {
            "paperId": "b550e3e05701cbf6c76a8c71e91beb95f950b080",
            "title": "A Reinforcement Learning Approach to job-shop Scheduling"
        },
        {
            "paperId": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
            "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning"
        },
        {
            "paperId": "2b11305f69641ecb8bd4a5e59cfebe41ad9ed989",
            "title": "TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play"
        },
        {
            "paperId": "5e5b25e046f120b296b7c0bad24692ceea492427",
            "title": "Monte Carlo Matrix Inversion and Reinforcement Learning"
        },
        {
            "paperId": "73382b3efd243d330a902e6a1eb0f6b32dbc5f29",
            "title": "Temporal Difference Learning of Position Evaluation in the Game of Go"
        },
        {
            "paperId": "939bb5eadf5d6d1731924a6a6750ee86d6a62f7c",
            "title": "Neurogammon: a neural-network backgammon program"
        },
        {
            "paperId": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
            "title": "Learning to predict by the methods of temporal differences"
        },
        {
            "paperId": "06da7dcbf451d6dbb4358f38ca05fb0c9f58670e",
            "title": "Building expert systems"
        },
        {
            "paperId": "69e68bfaadf2dccff800158749f5a50fe82d173b",
            "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"
        },
        {
            "paperId": "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
            "title": "Some Studies in Machine Learning Using the Game of Checkers"
        },
        {
            "paperId": "c7d3e9a1dd86f9c96f709d0ddb76972862784231",
            "title": "Dynamic Programming and Markov Processes"
        },
        {
            "paperId": "0a2586e0a5f8bb4e35aa0763a6b8bca428af6bd2",
            "title": "Taking the Human Out of the Loop: A Review of Bayesian Optimization"
        },
        {
            "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "title": "Deep Learning"
        },
        {
            "paperId": null,
            "title": "Deepmind AI reduces Google data centre cooling bill"
        },
        {
            "paperId": "a6ee4ae5344033fee613898841e2b9894bbfe4b7",
            "title": "Approximate modified policy iteration and its application to the game of Tetris"
        },
        {
            "paperId": "3be171b274728549e6a348dc40597e17284e7e36",
            "title": "The world of independent learners is not markovian"
        },
        {
            "paperId": "f444bfb9f78ab7037e3126be25139fc3988f933c",
            "title": "Reinforcement learning and simulation-based search in computer go"
        },
        {
            "paperId": "93bc8b66bc8a3386c4711b68e52be7acbcb5080f",
            "title": "Computing \"Elo Ratings\" of Move Patterns in the Game of Go"
        },
        {
            "paperId": "078c68b6f9653b0b64dc07e12e3d3a6208f615c8",
            "title": "Computational Intelligence in Mind Games"
        },
        {
            "paperId": "83ae6a9d4d886d9746a21860dc04a7cdfec39f52",
            "title": "Modification of UCT with Patterns in Monte-Carlo Go"
        },
        {
            "paperId": "5d0a7ebd3bc2d25deee869e8ef3dd80f9278607d",
            "title": "Reinforcement learning with replacing eligibility traces"
        },
        {
            "paperId": "25b139983c1aa777f24aaa375b5e13f4964803d6",
            "title": "Evaluation in Go by a Neural Network using Soft Segmentation"
        },
        {
            "paperId": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "title": "Reinforcement Learning: An Introduction"
        },
        {
            "paperId": "9bcdacaa81b4345db1368863bb0815418b207ec7",
            "title": "The Integration of A Priori Knowledge into a Go Playing Neural Network"
        },
        {
            "paperId": null,
            "title": "Tromp\u2013Taylor rules"
        }
    ]
}