{
    "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
    "externalIds": {
        "MAG": "2284050935",
        "DBLP": "conf/nips/SalimansK16",
        "ArXiv": "1602.07868",
        "CorpusId": 151231
    },
    "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
    "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.",
    "venue": "Neural Information Processing Systems",
    "year": 2016,
    "referenceCount": 34,
    "citationCount": 1794,
    "influentialCitationCount": 124,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction is presented, improving the conditioning of the optimization problem and speeding up convergence of stochastic gradient descent."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2887364",
            "name": "Tim Salimans"
        },
        {
            "authorId": "1726807",
            "name": "Diederik P. Kingma"
        }
    ],
    "references": [
        {
            "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "title": "Densely Connected Convolutional Networks"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "6a97d2668187965743d1b825b306defccbabbb4c",
            "title": "Improved Variational Inference with Inverse Autoregressive Flow"
        },
        {
            "paperId": "952454718139dba3aafc6b3b67c4f514ac3964af",
            "title": "Recurrent Batch Normalization"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "c3af47db3186691270192d5399bb5259e05c87a7",
            "title": "Data-dependent Initializations of Convolutional Neural Networks"
        },
        {
            "paperId": "97dc8df45972e4ed7423fc992a5092ba25b33411",
            "title": "All you need is a good init"
        },
        {
            "paperId": "5b93662a56a11d22efd49afa5aa79e64539260b8",
            "title": "Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix"
        },
        {
            "paperId": "941e30afcae061a115301c65a1afe49d8856f14e",
            "title": "Natural Neural Networks"
        },
        {
            "paperId": "cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487",
            "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"
        },
        {
            "paperId": "340f48901f72278f6bf78a04ee5b01df208cc508",
            "title": "Human-level control through deep reinforcement learning"
        },
        {
            "paperId": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca",
            "title": "DRAW: A Recurrent Neural Network For Image Generation"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "33af9298e5399269a12d4b9901492fe406af62b4",
            "title": "Striving for Simplicity: The All Convolutional Net"
        },
        {
            "paperId": "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd",
            "title": "Deeply-Supervised Nets"
        },
        {
            "paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
            "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
        },
        {
            "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "title": "Auto-Encoding Variational Bayes"
        },
        {
            "paperId": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "title": "Network In Network"
        },
        {
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning"
        },
        {
            "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "title": "Maxout Networks"
        },
        {
            "paperId": "855d0f722d75cc56a66a00ede18ace96bafee6bd",
            "title": "Theano: new features and speed improvements"
        },
        {
            "paperId": "f82e4ff4f003581330338aaae71f60316e58dd26",
            "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"
        },
        {
            "paperId": "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14",
            "title": "Deep Learning Made Easier by Linear Transformations in Perceptrons"
        },
        {
            "paperId": "4c46347fbc272b21468efe3d9af34b4b2bad6684",
            "title": "Deep learning via Hessian-free optimization"
        },
        {
            "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
            "paperId": "8423a5782a1acda21a6f68c307ce5376ebef13c7",
            "title": "Rank, Trace-Norm and Max-Norm"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "fa0c75a9b5f39d166dd875005580687716a236bb",
            "title": "Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient"
        },
        {
            "paperId": "6dc61f37ecc552413606d8c89ffbc46ec98ed887",
            "title": "Acceleration of stochastic approximation by averaging"
        },
        {
            "paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "title": "Deep Learning"
        },
        {
            "paperId": "e70431cba35472f0bd66300ee76cf9649c27b1dc",
            "title": "Rectified linear neural networks with tied-scalar regularization for LVCSR"
        },
        {
            "paperId": "c7e07cf8ba4ad956483f9dd37a168355ef16a041",
            "title": "Markov Chain Monte Carlo and Variational Inference: Bridging the Gap"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        }
    ]
}