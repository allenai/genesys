{
    "paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7",
    "externalIds": {
        "ArXiv": "1708.00107",
        "DBLP": "conf/nips/McCannBXS17",
        "MAG": "2949553999",
        "CorpusId": 9447219
    },
    "title": "Learned in Translation: Contextualized Word Vectors",
    "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
    "venue": "Neural Information Processing Systems",
    "year": 2017,
    "referenceCount": 72,
    "citationCount": 880,
    "influentialCitationCount": 82,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Adding context vectors to a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation to contextualize word vectors improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "143775536",
            "name": "Bryan McCann"
        },
        {
            "authorId": "40518045",
            "name": "James Bradbury"
        },
        {
            "authorId": "2228109",
            "name": "Caiming Xiong"
        },
        {
            "authorId": "2166511",
            "name": "R. Socher"
        }
    ],
    "references": [
        {
            "paperId": "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f",
            "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering"
        },
        {
            "paperId": "9f46d7793995677d15a3fa10c3cb2605a44610e8",
            "title": "Encoding Syntactic Knowledge in Neural Networks for Sentiment Classification"
        },
        {
            "paperId": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
            "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data"
        },
        {
            "paperId": "5729a01765fe23715d02b81a6f1e565be4755eda",
            "title": "The representational geometry of word meanings acquired by neural machine translation models"
        },
        {
            "paperId": "032e2f60781257dcb14035fce73b9b7b708303eb",
            "title": "End-to-End Multi-View Networks for Text Classification"
        },
        {
            "paperId": "664ec878de4b7170712baae4a7821fc2602bba25",
            "title": "Learning to Generate Reviews and Discovering Sentiment"
        },
        {
            "paperId": "7176fdb1c3618c26d331bc49e0d8d0874d2555ed",
            "title": "Deep Learning with Dynamic Computation Graphs"
        },
        {
            "paperId": "bb1087e8dee2039f773c381a3449a1c382482da6",
            "title": "Question Answering through Transfer Learning from Large Fine-grained Supervision Data"
        },
        {
            "paperId": "aab5002a22b9b4244a8329b140bd0a86021aa2d1",
            "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation"
        },
        {
            "paperId": "38596280ca0b3c3f988ef72a8f5a4459350a5361",
            "title": "High Accuracy Rule-based Question Classification using Question Syntax and Semantics"
        },
        {
            "paperId": "9f4d7d622d1f7319cc511bfef661cd973e881a4c",
            "title": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning"
        },
        {
            "paperId": "4c7e85ff37dd8b99d8f443eabd3b163ff8b71538",
            "title": "Reading and Thinking: Re-read LSTM Unit for Textual Entailment Recognition"
        },
        {
            "paperId": "786f95cada23d4639aa1a8b922cdb9fb9a9c03fa",
            "title": "Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling"
        },
        {
            "paperId": "8dbb0b9ca61e2753c6759446c6909acda616095a",
            "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference"
        },
        {
            "paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc",
            "title": "Unsupervised Pretraining for Sequence to Sequence Learning"
        },
        {
            "paperId": "e978d832a4d86571e1b52aa1685dc32ccb250f50",
            "title": "Dynamic Coattention Networks For Question Answering"
        },
        {
            "paperId": "ade0c116120b54b57a91da51235108b75c28375a",
            "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks"
        },
        {
            "paperId": "7ab2166f6cdb1737e000df66d29c6538afc6811d",
            "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency"
        },
        {
            "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
            "title": "Bidirectional Attention Flow for Machine Comprehension"
        },
        {
            "paperId": "a8c33413a626bafc67d46029ed28c2a28cc08899",
            "title": "End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking"
        },
        {
            "paperId": "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2",
            "title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference"
        },
        {
            "paperId": "ff1861b71eaedba46cb679bbe2c585dbe18f9b19",
            "title": "Machine Comprehension Using Match-LSTM and Answer Pointer"
        },
        {
            "paperId": "95d3001ed7782fecea29bdb41e598aa5b41a615b",
            "title": "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description"
        },
        {
            "paperId": "705dcc8eadba137834e4b0359e2d696d4b209f5b",
            "title": "Neural Tree Indexers for Text Understanding"
        },
        {
            "paperId": "cff79255a94b9b05a4ce893eb403a522e0923f04",
            "title": "Neural Semantic Encoders"
        },
        {
            "paperId": "35097ca8d9c8380d8d012a67b7b616f9d662f2fc",
            "title": "Hedged Deep Tracking"
        },
        {
            "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
            "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "title": "A Decomposable Attention Model for Natural Language Inference"
        },
        {
            "paperId": "12f7de07f9b00315418e381b2bd797d21f12b419",
            "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"
        },
        {
            "paperId": "ec64f650fea9b49cbf7d8bddf458388c4b362475",
            "title": "Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents"
        },
        {
            "paperId": "2cd55ded95d5d13430edfa223ba591b514ebe8a5",
            "title": "Adversarial Training Methods for Semi-Supervised Text Classification"
        },
        {
            "paperId": "dd164677214f6ed8d47a3c3c0663e50f3c41cb00",
            "title": "Improving Question Classification by Feature Extraction and Selection"
        },
        {
            "paperId": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194",
            "title": "Dynamic Memory Networks for Visual and Textual Question Answering"
        },
        {
            "paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa",
            "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"
        },
        {
            "paperId": "26e743d5bd465f49b9538deaf116c15e61b7951f",
            "title": "Learning Distributed Representations of Sentences from Unlabelled Data"
        },
        {
            "paperId": "d7db74be6cda0ec2bd28ec187563def85ccef78f",
            "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings"
        },
        {
            "paperId": "558ac446dc26bee9789d660a251b75728cb6eeb2",
            "title": "Language to Logical Form with Neural Attention"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "395044a2e3f5624b2471fb28826e7dbb1009356e",
            "title": "Towards Universal Paraphrastic Sentence Embeddings"
        },
        {
            "paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
            "title": "Semi-supervised Sequence Learning"
        },
        {
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference"
        },
        {
            "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
            "title": "Effective Approaches to Attention-based Neural Machine Translation"
        },
        {
            "paperId": "452059171226626718eb677358836328f884298e",
            "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"
        },
        {
            "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "title": "Skip-Thought Vectors"
        },
        {
            "paperId": "bd32ebb9fac53a14202fb1a4f76ef96d1ff68c6c",
            "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "16084914bc3729f86f46ac6267ea7a42e7951d41",
            "title": "SemEval-2014 Task 10: Multilingual Semantic Textual Similarity"
        },
        {
            "paperId": "4ea80c206b8ad73a6d320c9d8ed0321d84fe6d85",
            "title": "Recursive Neural Networks for Learning Logical Semantics"
        },
        {
            "paperId": "0ca7d208ff8d81377e0eaa9723820aeae7a7322d",
            "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences"
        },
        {
            "paperId": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "title": "Maxout Networks"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "39206a33b1a5223eb4d793ee49d508527c8e3ee3",
            "title": "Question Classification by Weighted Combination of Lexical, Syntactic and Semantic Features"
        },
        {
            "paperId": "9c34c43afd96a6845080842748d0f2f2fcd09d50",
            "title": "Heterogeneous Transfer Learning for Image Classification"
        },
        {
            "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
            "title": "Learning Word Vectors for Sentiment Analysis"
        },
        {
            "paperId": "bc1022b031dc6c7019696492e8116598097a8c12",
            "title": "Natural Language Processing (Almost) from Scratch"
        },
        {
            "paperId": "044b239c207a9decc77a7c2eb6de1f95b92c9fc3",
            "title": "From symbolic to sub-symbolic information in question classification"
        },
        {
            "paperId": "5d9a3036181676e187c9c0ff995d8bed1db3557d",
            "title": "Adapting Visual Category Models to New Domains"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21",
            "title": "Moses: Open Source Toolkit for Statistical Machine Translation"
        },
        {
            "paperId": "f2b18264de28827a061fe9e22c437d1f616fdb4a",
            "title": "Learning question classifiers: the role of semantic information"
        },
        {
            "paperId": "5536d42ce80e129be8cae172ed1b7659c769d31d",
            "title": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
        },
        {
            "paperId": "46be284f1e1ece64465af6fe3a69ce544e0c7e33",
            "title": "The TREC-8 Question Answering Track Evaluation"
        },
        {
            "paperId": "b54268e3b8d148c0695ca52bebb0f80e26a4b987",
            "title": "The IWSLT 2015 Evaluation Campaign"
        },
        {
            "paperId": "b0b36cd24cbb45bc11140def9245af79c313e609",
            "title": "Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding"
        }
    ]
}