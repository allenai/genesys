{
    "paperId": "452059171226626718eb677358836328f884298e",
    "externalIds": {
        "DBLP": "conf/icml/KumarIOIBGZPS16",
        "MAG": "2949215902",
        "ArXiv": "1506.07285",
        "CorpusId": 2319779
    },
    "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
    "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",
    "venue": "International Conference on Machine Learning",
    "year": 2015,
    "referenceCount": 52,
    "citationCount": 1148,
    "influentialCitationCount": 104,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers, is introduced."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2119320633",
            "name": "A. Kumar"
        },
        {
            "authorId": "2329943",
            "name": "Ozan Irsoy"
        },
        {
            "authorId": "3214791",
            "name": "Peter Ondruska"
        },
        {
            "authorId": "2136562",
            "name": "Mohit Iyyer"
        },
        {
            "authorId": "2065251344",
            "name": "James Bradbury"
        },
        {
            "authorId": "2708454",
            "name": "Ishaan Gulrajani"
        },
        {
            "authorId": "3428769",
            "name": "Victor Zhong"
        },
        {
            "authorId": "2896063",
            "name": "Romain Paulus"
        },
        {
            "authorId": "2166511",
            "name": "R. Socher"
        }
    ],
    "references": [
        {
            "paperId": "98ea4abc9bf0e30eb020db2075c9c8a039a848a3",
            "title": "Learning to Compose Neural Networks for Question Answering"
        },
        {
            "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
            "title": "Neural GPUs Learn Algorithms"
        },
        {
            "paperId": "2c1890864c1c2b750f48316dc8b650ba4772adc5",
            "title": "Stacked Attention Networks for Image Question Answering"
        },
        {
            "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
            "title": "Teaching Machines to Read and Comprehend"
        },
        {
            "paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82",
            "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
            "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"
        },
        {
            "paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
            "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "67f9f2cf408b2cc593a9ddb17d74d2b3f72ee225",
            "title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks"
        },
        {
            "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
        },
        {
            "paperId": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88",
            "title": "Deep visual-semantic alignments for generating image descriptions"
        },
        {
            "paperId": "f4af49a1ead3c81cc5d023878cb67c5646dd8a04",
            "title": "Learning a Recurrent Visual Representation for Image Caption Generation"
        },
        {
            "paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de",
            "title": "Neural Turing Machines"
        },
        {
            "paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39",
            "title": "Memory Networks"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "921da70f380e9f9082e5594b128369cfd0fdf120",
            "title": "A Neural Network for Factoid Question Answering over Paragraphs"
        },
        {
            "paperId": "ac64fb7e6d2ddf236332ec9f371fe85d308c114d",
            "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7",
            "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
            "paperId": "70155488a49d51755c1dfea728e03a6dd72703a1",
            "title": "Deep Networks with Internal Selective Attention through Feedback Connections"
        },
        {
            "paperId": "4ea80c206b8ad73a6d320c9d8ed0321d84fe6d85",
            "title": "Recursive Neural Networks for Learning Logical Semantics"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
            "title": "Distributed Representations of Sentences and Documents"
        },
        {
            "paperId": "d53d878cf1a3f0bed5d9c68c925994cb72f47304",
            "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution"
        },
        {
            "paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
            "title": "A Convolutional Neural Network for Modelling Sentences"
        },
        {
            "paperId": "50d53cc562225549457cbc782546bfbe1ac6f0cf",
            "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "944a1cfd79dbfb6fef460360a0765ba790f4027a",
            "title": "Recurrent Continuous Translation Models"
        },
        {
            "paperId": "3054726d298d3c5f7b95b224e4e5b17d88068c3f",
            "title": "Easy Victories and Uphill Battles in Coreference Resolution"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4",
            "title": "Context dependent recurrent neural network language model"
        },
        {
            "paperId": "f2f72cfb48d15d4d2bd1e91a92e7f3ac8635d433",
            "title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing"
        },
        {
            "paperId": "ae5e6c6f5513613a161b2c85563f9708bf2e9178",
            "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection"
        },
        {
            "paperId": "15de5528b04bf3d9cf741122677588140c25ebff",
            "title": "The neurobiology of semantic memory"
        },
        {
            "paperId": "2a6626dadb6b624011c39270cc7678ac98e162a3",
            "title": "Semi-supervised condensed nearest neighbor for part-of-speech tagging"
        },
        {
            "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
        },
        {
            "paperId": "9dfcaad0c019a67c46e0156bc8da52e32628d446",
            "title": "A Bayesian Analysis of Dynamics in Free Recall"
        },
        {
            "paperId": "ad10607412e196279bf056d13c8b6fa27fd61f26",
            "title": "TextRunner: Open Information Extraction on the Web"
        },
        {
            "paperId": "9b1df5b85c7e13edc350b68173eeb2b7840dffb1",
            "title": "A distributed representation of temporal context"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "af3b369c47300a1eb3369573b169310d735a4e69",
            "title": "The hippocampus and memory for orderly stimulus relations."
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "605d738a39df3c5e596613ab0ca6925f0eecdf35",
            "title": "Distributed representations, simple recurrent networks, and grammatical structure"
        },
        {
            "paperId": "37f4dd9e51ba327bd0514017d53f338ccdb38923",
            "title": "Removing the Training Wheels: A Coreference Dataset that Entertains Humans and Challenges Computers"
        },
        {
            "paperId": "e6115db37cd8531f211bf9d4408e6baf26d632e4",
            "title": "Hippocampal activation during transitive inference in humans"
        },
        {
            "paperId": "318982230424906948205941ac3539813867a53f",
            "title": "From Conditioning to Conscious Recollection Memory Systems of the Brain. Oxford Psychology Series, Volume 35."
        },
        {
            "paperId": null,
            "title": "Episodic Memory Module"
        },
        {
            "paperId": null,
            "title": "Question Module"
        },
        {
            "paperId": null,
            "title": "Semantic Memory Module: Semantic memory stores general knowledge about concepts and facts"
        }
    ]
}