{
    "paperId": "8de174ab5419b9d3127695405efd079808e956e8",
    "externalIds": {
        "MAG": "2296073425",
        "DBLP": "conf/icml/BengioLCW09",
        "DOI": "10.1145/1553374.1553380",
        "CorpusId": 873046
    },
    "title": "Curriculum learning",
    "abstract": "Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).",
    "venue": "International Conference on Machine Learning",
    "year": 2009,
    "referenceCount": 33,
    "citationCount": 4983,
    "influentialCitationCount": 369,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is hypothesized that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions)."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        },
        {
            "authorId": "2373952",
            "name": "J. Louradour"
        },
        {
            "authorId": "2939803",
            "name": "R. Collobert"
        },
        {
            "authorId": "145183709",
            "name": "J. Weston"
        }
    ],
    "references": [
        {
            "paperId": "ccf415df5a83b343dae261286d29a40e8b80e6c6",
            "title": "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training"
        },
        {
            "paperId": "d73b02fb80bec1b65bd6187148fd6371e9c669df",
            "title": "Flexible shaping: How learning in small steps helps"
        },
        {
            "paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"
        },
        {
            "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
            "title": "Extracting and composing robust features with denoising autoencoders"
        },
        {
            "paperId": "7ee368e60d0b826e78f965aad8d6c7d406127104",
            "title": "Deep learning via semi-supervised embedding"
        },
        {
            "paperId": "f2e95236f0fccc0b70e757ac2ebbc79b7f51de0a",
            "title": "Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes"
        },
        {
            "paperId": "41fef1a197fab9684a4608b725d3ae72e1ab4b39",
            "title": "Sparse Feature Learning for Deep Belief Networks"
        },
        {
            "paperId": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "title": "An empirical evaluation of deep architectures on problems with many factors of variation"
        },
        {
            "paperId": "1626c940a64ad96a7ed53d7d6c0df63c6696956b",
            "title": "Restricted Boltzmann machines for collaborative filtering"
        },
        {
            "paperId": "ad33d1fa8628cb55c32fb52feb537f65184c3b29",
            "title": "Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure"
        },
        {
            "paperId": "932c2a02d462abd75af018125413b1ceaa1ee3f4",
            "title": "Efficient Learning of Sparse Representations with an Energy-Based Model"
        },
        {
            "paperId": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "title": "Greedy Layer-Wise Training of Deep Networks"
        },
        {
            "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "title": "A Fast Learning Algorithm for Deep Belief Nets"
        },
        {
            "paperId": "273bf4ff0b43e376a94073603af9e000ffb30111",
            "title": "A day of great illumination: B. F. Skinner's discovery of shaping."
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "e41498c05d4c68e4750fb84a380317a112d97b01",
            "title": "Connectionist language modeling for large vocabulary continuous speech recognition"
        },
        {
            "paperId": "47133d54d4a5f1fb3c46bdbf3a7a5e270d930e2f",
            "title": "Language acquisition in the absence of explicit negative evidence: how important is starting small?"
        },
        {
            "paperId": "1150f9289c6151506e3f7cf0e6ebbcfd49f1dace",
            "title": "Active Learning with Statistical Models"
        },
        {
            "paperId": "c3954ae0bfc7e40c013a05e9d332f94be63d6ec3",
            "title": "Smoothing techniques for macromolecular global optimization"
        },
        {
            "paperId": "c6e8c9ababbcd0ab8085312995c3416c9437610a",
            "title": "Global Continuation for Distance Geometry Problems"
        },
        {
            "paperId": "c6b91b64c80d6fcb89ca4b5b6df24abf55bbfb75",
            "title": "Generalization in the programed teaching of a perceptron."
        },
        {
            "paperId": "1af453015162bc2ce1c5b58afbcc153b158192cd",
            "title": "Neural network learning control of robot manipulators using gradually increasing task difficulty"
        },
        {
            "paperId": "ec7072d9af48069009339c1eb3acea5085303c0f",
            "title": "Parallel continuation-based global optimization for molecular conformation and protein folding"
        },
        {
            "paperId": "d5ddb30bf421bdfdf728b636993dc48b1e879176",
            "title": "Learning and development in neural networks: the importance of starting small"
        },
        {
            "paperId": "939d584316be99e2db3fec3fbf7d71f22a477f67",
            "title": "Unsupervised learning of distributions on binary vectors using two layer networks"
        },
        {
            "paperId": "6843f4f392280fb1a9dc6c16e072dffc97f4b2e4",
            "title": "On the power of small-depth threshold circuits"
        },
        {
            "paperId": "dd9dddf15874a30d6b95c0db9401be7ca136306e",
            "title": "Numerical continuation methods - an introduction"
        },
        {
            "paperId": "d04d6db5f0df11d0cff57ec7e15134990ac07a4f",
            "title": "Learning Deep Architectures for AI"
        },
        {
            "paperId": "7c59908c946a4157abc030cdbe2b63d08ba97db3",
            "title": "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks"
        },
        {
            "paperId": "75e50717070e82cdf3945265a75def6960b55a9d",
            "title": "Explanation-based neural network learning a lifelong learning approach"
        },
        {
            "paperId": "24a6bfcaf8857da93f8bdaa50c03d7bc0ca298ee",
            "title": "Numerical Continuation Methods"
        },
        {
            "paperId": null,
            "title": "Reinforcement today"
        },
        {
            "paperId": null,
            "title": "Adv. Neural Inf. Proc. Sys"
        }
    ]
}