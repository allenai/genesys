{
    "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
    "externalIds": {
        "MAG": "2949117887",
        "ArXiv": "1502.03167",
        "DBLP": "conf/icml/IoffeS15",
        "CorpusId": 5808102
    },
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
    "venue": "International Conference on Machine Learning",
    "year": 2015,
    "referenceCount": 54,
    "citationCount": 40777,
    "influentialCitationCount": 2034,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2054165706",
            "name": "Sergey Ioffe"
        },
        {
            "authorId": "2574060",
            "name": "Christian Szegedy"
        }
    ],
    "references": [
        {
            "paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
        },
        {
            "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions"
        },
        {
            "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "title": "Densely Connected Convolutional Networks"
        },
        {
            "paperId": "d0156126edbfc524c8d108bdc0cf811cfe3129aa",
            "title": "FractalNet: Ultra-Deep Neural Networks without Residuals"
        },
        {
            "paperId": "0e2b3faed39561f712c3b14a08c7c36272d9857a",
            "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
            "title": "Multi-Scale Context Aggregation by Dilated Convolutions"
        },
        {
            "paperId": "941e30afcae061a115301c65a1afe49d8856f14e",
            "title": "Natural Neural Networks"
        },
        {
            "paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "title": "You Only Look Once: Unified, Real-Time Object Detection"
        },
        {
            "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
        },
        {
            "paperId": "f5ce3abf942cdd685fb0f290f3e741f7b4749f0a",
            "title": "Deep Image: Scaling up Image Recognition"
        },
        {
            "paperId": "33af9298e5399269a12d4b9901492fe406af62b4",
            "title": "Striving for Simplicity: The All Convolutional Net"
        },
        {
            "paperId": "4030a62e75313110dc4a4c78483f4459dc4526bc",
            "title": "Parallel training of Deep Neural Networks with Natural Gradient and Parameter Averaging"
        },
        {
            "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "title": "Going deeper with convolutions"
        },
        {
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "04f16203f1e66e8d2151dc359fd0405a0f482da7",
            "title": "Mean-normalized stochastic gradient for large-scale deep learning"
        },
        {
            "paperId": "99c970348b8f70ce23d6641e201904ea49266b6e",
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
        },
        {
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning"
        },
        {
            "paperId": "523b12db4004b89284387f978c2af8ae0e79d54b",
            "title": "Knowledge Matters: Importance of Prior Information for Optimization"
        },
        {
            "paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "title": "Large Scale Distributed Deep Networks"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "e952c51379567889753b2df005107520207ab337",
            "title": "Large Scale Visual Recognition"
        },
        {
            "paperId": "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14",
            "title": "Deep Learning Made Easier by Linear Transformations in Perceptrons"
        },
        {
            "paperId": "d530d20a4d5c09ff2e51e31e4980d1675a9be2ba",
            "title": "A Convergence Analysis of Log-Linear Training"
        },
        {
            "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
            "paperId": "25d92fe0ab92ae57bd8fed87c95fa9a207ec61aa",
            "title": "Nonlinear image representation using divisive normalization"
        },
        {
            "paperId": "458aa276338c7805d55cb4bcfefb4f34869f9733",
            "title": "Optimization"
        },
        {
            "paperId": "a29556b09f14253b4b52cd8375a3e08202b40da3",
            "title": "Critical Points of the Singular Value Decomposition"
        },
        {
            "paperId": "235723a15c86c369c99a42e7b666dfe156ad2cba",
            "title": "Improving predictive inference under covariate shift by weighting the log-likelihood function"
        },
        {
            "paperId": "577d19a115f9ef6f002483fcf88adbb3b5479556",
            "title": "Independent component analysis: algorithms and applications"
        },
        {
            "paperId": null,
            "title": "Batch Normalization Presentation"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": null,
            "title": "Figure 5 documents the changes that were performed compared to the architecture with respect to the GoogleNet archictecture. For the interpretation of this table, please consult ("
        },
        {
            "paperId": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
            "title": "Efficient BackProp"
        },
        {
            "paperId": null,
            "title": "J. Mach. Learn. Res"
        },
        {
            "paperId": "73e1644ad4aab9e11c1e46eea513141c4930602d",
            "title": "A Literature Survey on Domain Adaptation of Statistical Classifiers"
        },
        {
            "paperId": "f42b865e20e61a954239f421b42007236e671f19",
            "title": "GradientBased Learning Applied to Document Recognition"
        },
        {
            "paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "title": "Gradient-based learning applied to document recognition"
        },
        {
            "paperId": null,
            "title": "Our model employed separable convolution with depth multiplier 8 on the \ufb01rst convolutional layer. This reduces the computational cost while increasing the memory consumption at training time"
        },
        {
            "paperId": null,
            "title": "Appendix Variant of the Inception Model Used"
        },
        {
            "paperId": null,
            "title": "\u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f \u0441\u0432\u0451\u0440\u0442\u043e\u043a 5 \u00d7 1 \u0438 1 \u00d7 5 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442\u0441\u044f \u043f\u043e\u0447\u0442\u0438 \u0442\u0430\u043a \u0436\u0435 \u043e\u0431\u044b\u0447\u043d\u0430\u044f \u0441\u0432\u0451\u0440\u0442\u043a\u0430 3 \u00d7 3"
        },
        {
            "paperId": null,
            "title": "\u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u043c\u0435\u044e\u0442 \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435. \u0414\u043b\u044f \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u0436\u0435 \u043e\u0447\u0435\u043d\u044c \u043f\u0440\u043e\u0441\u0442\u0430\u044f \u0441\u0435\u0442\u044c \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0445\u043e\u0440\u043e\u0448\u043e (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, 6-10 \u0441\u043b\u043e\u0451\u0432 \u043f\u043e 32 \u0438\u043b\u0438 \u0434\u0430\u0436\u0435 16 \u043a\u0430\u043d\u0430\u043b\u043e\u0432)"
        },
        {
            "paperId": null,
            "title": "Instance segmentation: \u0421\u0430\u043c\u0430\u044f \u0441\u043b\u043e\u0436\u043d\u0430\u044f \u0437\u0430\u0434\u0430\u0447\u0430 - \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c-\u043a\u043e, \u043f\u0440\u0438\u0447\u0451\u043c \u043d\u0430\u0434\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430 \u0440\u0430\u0437\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0441 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0449\u0438\u043c \u043a\u043b\u0430\u0441\u0441\u043e\u043c"
        },
        {
            "paperId": null,
            "title": "Object recognition: \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u0441\u0435\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0441 \u0440\u043e\u0432\u043d\u043e \u043e\u0434\u043d\u0438\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u043c \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0435\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442 \u043f\u0440\u044f\u043c\u043e-\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u0430 \u0438 \u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0441"
        },
        {
            "paperId": null,
            "title": "\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0431\u043e\u043b\u0435\u0435 \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0445 \u043f\u0443\u0442\u0435\u0439 \u0432\u043d\u0443\u0442\u0440\u0438 \u0441\u0435\u0442\u0438 (Fractal net [11], Dense net [12])"
        },
        {
            "paperId": null,
            "title": "Inception-v4, Inception-ResNet"
        },
        {
            "paperId": null,
            "title": "\u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u0430\u044f \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0441\u0435\u0442\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u0442\u044c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u0438\u0437\u043e\u0431-\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043d\u0430 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0435 c \u041e\u0421 Android \u0437\u0430 \u0432\u0440\u0435\u043c\u044f 200-500 \u043c\u0441 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430"
        },
        {
            "paperId": null,
            "title": "\u0415\u0441\u043b\u0438 \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c \u043a \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0443, \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u0432\u0451\u0440\u0442-\u043a\u0438 1 \u00d7 7 \u0438 7 \u00d7 1 - \u043e\u043d\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442 \u0437\u0430 \u043e\u0434\u0438\u043d \u0441\u043b\u043e\u0439 \u0437\u0430\u0445\u0432\u0430\u0442\u0438\u0442\u044c \u0441\u0440\u0430\u0437\u0443 \u0431\u043e\u043b\u044c\u0448\u0443\u044e \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f (\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0432 \u0441\u043e\u0447\u0435\u0442\u0430\u043d\u0438\u0438 \u0441 dilations)"
        },
        {
            "paperId": null,
            "title": "\u041f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 float \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0432 \u0447\u0435\u0442\u044b\u0440\u0435 \u0440\u0430\u0437\u0430 \u0441\u043d\u0438\u0436\u0430\u0435\u0442\u0441\u044f \u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u0435 \u043f\u0430\u043c\u044f\u0442\u0438 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0447\u0438\u0442\u0430\u0435\u043c\u043e\u0439-\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438"
        },
        {
            "paperId": null,
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        }
    ]
}