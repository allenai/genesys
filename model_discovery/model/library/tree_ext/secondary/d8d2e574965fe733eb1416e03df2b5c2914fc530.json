{
    "paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530",
    "externalIds": {
        "DBLP": "journals/aiopen/LinWLQ22",
        "ArXiv": "2106.04554",
        "DOI": "10.1016/j.aiopen.2022.10.001",
        "CorpusId": 235368340
    },
    "title": "A Survey of Transformers",
    "abstract": null,
    "venue": "AI Open",
    "year": 2021,
    "referenceCount": 180,
    "citationCount": 758,
    "influentialCitationCount": 21,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This survey provides a comprehensive review of various Transformer variants and proposes a new taxonomy of X-formers from three perspectives: architectural modification, pre-training, and applications."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -0.2655479609966278,
            -1.259253740310669,
            3.8971195220947266,
            5.439201354980469,
            0.6089279651641846,
            -0.2696371376514435,
            6.3566460609436035,
            -4.292652606964111,
            0.48410001397132874,
            -0.2245407998561859,
            0.9532328844070435,
            4.105387210845947,
            0.6421011686325073,
            -2.1030631065368652,
            -2.464569568634033,
            -4.584197044372559,
            0.7799975275993347,
            -0.19831246137619019,
            5.845466613769531,
            1.3187178373336792,
            -0.8992786407470703,
            0.2856127619743347,
            -6.572284698486328,
            0.050661876797676086,
            1.481131672859192,
            0.8866187930107117,
            0.4658051133155823,
            -1.5929723978042603,
            -2.404676914215088,
            -3.7289352416992188,
            -0.7584900856018066,
            -1.6201045513153076,
            3.6001946926116943,
            -4.59995698928833,
            -2.7967514991760254,
            0.6743624210357666,
            -2.2273659706115723,
            3.664074659347534,
            0.26124054193496704,
            -2.515883207321167,
            0.9296109080314636,
            1.6410497426986694,
            2.1758930683135986,
            0.7075099945068359,
            1.3393874168395996,
            -2.8315322399139404,
            -1.1066832542419434,
            -0.3095090985298157,
            1.813716173171997,
            2.2626445293426514,
            5.058304309844971,
            1.7244958877563477,
            -1.519720196723938,
            2.6277809143066406,
            -0.7125348448753357,
            -1.6976261138916016,
            0.6201947331428528,
            1.634462594985962,
            0.9670770764350891,
            -1.435699224472046,
            3.8235442638397217,
            4.06100606918335,
            3.8534631729125977,
            0.8092267513275146,
            4.433055877685547,
            0.3563687801361084,
            -2.1616556644439697,
            -1.862626552581787,
            2.1687397956848145,
            0.2463327795267105,
            -0.8395289778709412,
            -6.614912033081055,
            4.673937797546387,
            -2.551201820373535,
            -4.140600681304932,
            1.7843737602233887,
            -2.9747471809387207,
            -4.286262035369873,
            4.656157493591309,
            0.12935996055603027,
            0.704845666885376,
            0.971708357334137,
            -0.9773428440093994,
            0.5074292421340942,
            2.43483829498291,
            -1.6319444179534912,
            -3.21158504486084,
            -0.44519925117492676,
            -1.5285072326660156,
            -0.4334206283092499,
            0.024917423725128174,
            -1.1484777927398682,
            1.5321307182312012,
            1.0488312244415283,
            -2.3370094299316406,
            -3.3768978118896484,
            0.8057568073272705,
            -2.043837070465088,
            -0.29036885499954224,
            2.447305679321289,
            0.22937369346618652,
            0.20009154081344604,
            3.1019864082336426,
            0.3434811234474182,
            3.6834723949432373,
            -3.193770408630371,
            -1.4580930471420288,
            1.6340560913085938,
            1.416020154953003,
            -1.1580567359924316,
            -1.4911768436431885,
            1.569413661956787,
            -0.9123432636260986,
            -1.333178997039795,
            -1.5680930614471436,
            -1.3176532983779907,
            3.0572760105133057,
            1.4948275089263916,
            0.09437006711959839,
            0.6436080932617188,
            -0.045968443155288696,
            -0.8172485828399658,
            -1.7097365856170654,
            2.777348279953003,
            -0.7016677260398865,
            5.947162628173828,
            0.0363057404756546,
            3.0433127880096436,
            0.6568523645401001,
            -2.097646713256836,
            3.04616379737854,
            -0.9098342657089233,
            -0.9353892207145691,
            -2.3618228435516357,
            2.2377536296844482,
            2.7651021480560303,
            -3.5602071285247803,
            -0.5351500511169434,
            -0.944900393486023,
            0.4893166422843933,
            0.7586256861686707,
            3.3526434898376465,
            1.5301427841186523,
            1.5086655616760254,
            -0.6485225558280945,
            2.0510191917419434,
            1.6840709447860718,
            -0.6869360208511353,
            -2.572988748550415,
            2.47580623626709,
            2.814087152481079,
            -1.365189552307129,
            0.3143359422683716,
            0.4742724597454071,
            -1.4357061386108398,
            2.1813178062438965,
            -3.713254451751709,
            1.037248134613037,
            0.3722287118434906,
            -0.5797170400619507,
            0.22382710874080658,
            4.702674865722656,
            -8.368539810180664,
            3.071633815765381,
            2.546435832977295,
            -5.311877250671387,
            -0.09201663732528687,
            2.3535642623901367,
            0.5495508909225464,
            2.51446795463562,
            -1.5124047994613647,
            1.377859115600586,
            1.9284480810165405,
            1.7048593759536743,
            1.2529425621032715,
            3.265547037124634,
            1.4941296577453613,
            -4.708300590515137,
            -2.8330235481262207,
            2.3756730556488037,
            -2.0513105392456055,
            -3.141566753387451,
            -2.9447760581970215,
            0.4135530889034271,
            -3.1698858737945557,
            -1.8699541091918945,
            -3.1768031120300293,
            -3.7206084728240967,
            -4.622420310974121,
            0.24515673518180847,
            0.029084980487823486,
            3.5656356811523438,
            4.798795700073242,
            3.4565887451171875,
            1.5799671411514282,
            0.7833004593849182,
            2.519960641860962,
            4.924311637878418,
            -5.227004051208496,
            -1.3402265310287476,
            1.8661373853683472,
            -0.9971962571144104,
            -1.2601704597473145,
            -1.8759284019470215,
            3.474513053894043,
            -0.6257925033569336,
            -3.5471553802490234,
            2.7550108432769775,
            3.310318946838379,
            1.092427134513855,
            0.9370277523994446,
            -1.5348296165466309,
            -0.6780315637588501,
            4.8746747970581055,
            -1.660388708114624,
            -3.2898049354553223,
            -6.883002281188965,
            2.7305338382720947,
            3.099735736846924,
            1.1794564723968506,
            2.378420829772949,
            2.8118696212768555,
            -5.957188606262207,
            -4.128234386444092,
            0.5009968876838684,
            -1.2972792387008667,
            1.033547043800354,
            2.513561725616455,
            -1.4432058334350586,
            0.12913990020751953,
            -0.2096116989850998,
            -4.170566082000732,
            0.03615927696228027,
            -2.6366260051727295,
            -1.9187049865722656,
            -1.9253623485565186,
            -4.094420909881592,
            0.01932486891746521,
            0.31820857524871826,
            -0.6049547791481018,
            7.029689788818359,
            2.856999397277832,
            2.203583002090454,
            1.4106459617614746,
            -3.482302188873291,
            -4.074936389923096,
            -0.09993890672922134,
            0.8654816746711731,
            1.052659511566162,
            -4.73789119720459,
            -0.5160021781921387,
            2.847292184829712,
            3.5179219245910645,
            -0.8820744156837463,
            -0.9844428300857544,
            -0.719419002532959,
            1.692800760269165,
            -1.2545078992843628,
            0.4748416543006897,
            0.36812084913253784,
            -3.5351927280426025,
            4.320590972900391,
            0.5924096703529358,
            0.7828244566917419,
            1.9320182800292969,
            -0.09249281883239746,
            -2.5658998489379883,
            2.5508081912994385,
            -0.3164825439453125,
            2.5125651359558105,
            1.9561538696289062,
            3.2171878814697266,
            -0.6926845908164978,
            -2.6049447059631348,
            -2.642294406890869,
            -2.786069869995117,
            -1.909191608428955,
            -3.328505516052246,
            3.0939950942993164,
            2.252378463745117,
            1.3342130184173584,
            -2.1947720050811768,
            -4.107304573059082,
            -1.4267749786376953,
            2.5079822540283203,
            -0.015776872634887695,
            0.6742258071899414,
            2.0046565532684326,
            -3.3746418952941895,
            -1.060874342918396,
            -4.179376602172852,
            4.8564133644104,
            -3.215388774871826,
            -2.551879644393921,
            0.25880852341651917,
            3.3120369911193848,
            2.883693218231201,
            1.22208833694458,
            0.5833792090415955,
            -2.889099359512329,
            -2.2905097007751465,
            0.8624659180641174,
            5.020354270935059,
            -1.755426049232483,
            2.3859171867370605,
            1.15050208568573,
            3.2812352180480957,
            -1.8389067649841309,
            -0.35641807317733765,
            1.8995847702026367,
            0.4998505115509033,
            1.1193522214889526,
            3.846076726913452,
            -4.268321514129639,
            1.2978293895721436,
            -0.9309935569763184,
            -0.5224609971046448,
            2.22499418258667,
            2.021941661834717,
            2.2780463695526123,
            -2.725071668624878,
            1.2059133052825928,
            -2.78220796585083,
            -6.208709239959717,
            -1.643521785736084,
            -1.843461275100708,
            0.8233689069747925,
            0.45564085245132446,
            -1.6355761289596558,
            1.9105465412139893,
            3.880927085876465,
            1.1206837892532349,
            1.6772081851959229,
            -0.22376558184623718,
            -0.33665069937705994,
            -3.760845422744751,
            -0.13212314248085022,
            0.6384298801422119,
            -0.09955388307571411,
            1.660222053527832,
            3.2117152214050293,
            6.6611223220825195,
            0.9066186547279358,
            2.3035128116607666,
            -0.3882398009300232,
            2.829810619354248,
            4.052260398864746,
            0.11725889146327972,
            0.6494361162185669,
            1.353136658668518,
            -2.105527400970459,
            -3.4188766479492188,
            1.748136043548584,
            -2.5843276977539062,
            2.116105079650879,
            3.123493194580078,
            2.374485731124878,
            -1.7475812435150146,
            -0.23153811693191528,
            2.10603928565979,
            0.05242636799812317,
            1.4913264513015747,
            0.5109284520149231,
            -0.2443009316921234,
            -1.9424560070037842,
            -4.558053016662598,
            8.637796401977539,
            -0.20162159204483032,
            -1.910220980644226,
            -3.4349496364593506,
            -4.03538179397583,
            -1.0348870754241943,
            -1.9826300144195557,
            4.322982311248779,
            0.4234335124492645,
            -3.323349714279175,
            3.3787760734558105,
            -3.6228256225585938,
            2.515174627304077,
            -0.07793751358985901,
            1.4902955293655396,
            4.393980979919434,
            -0.44216784834861755,
            2.5050976276397705,
            1.5115586519241333,
            -1.2223740816116333,
            -3.828252077102661,
            1.398643970489502,
            -0.4660605192184448,
            -1.169744849205017,
            -0.6376205086708069,
            2.6927502155303955,
            0.07599768042564392,
            0.6313294768333435,
            -4.641269683837891,
            -2.654909133911133,
            -2.268190860748291,
            -3.337704658508301,
            1.1999627351760864,
            0.87444669008255,
            -0.27157115936279297,
            0.7643553018569946,
            2.880673885345459,
            4.3136210441589355,
            -0.8566155433654785,
            1.0730371475219727,
            3.086719512939453,
            1.5130152702331543,
            0.6808403730392456,
            -0.20746809244155884,
            -8.155680656433105,
            -0.5418510437011719,
            -3.266165256500244,
            -2.627647876739502,
            3.448499917984009,
            1.8799256086349487,
            2.5090243816375732,
            3.0909039974212646,
            1.1967623233795166,
            -2.4041457176208496,
            -4.908385276794434,
            0.5885087251663208,
            6.119410037994385,
            1.7789655923843384,
            -1.9596076011657715,
            -1.0947003364562988,
            0.632609486579895,
            4.8348469734191895,
            -1.2435739040374756,
            1.1434862613677979,
            -3.0082321166992188,
            1.3014309406280518,
            -3.4850993156433105,
            -2.8013510704040527,
            2.354254722595215,
            0.06949859857559204,
            -3.4260900020599365,
            6.475419998168945,
            4.081751346588135,
            -1.8080909252166748,
            -1.1901041269302368,
            1.2809858322143555,
            -0.5378819704055786,
            0.5275921821594238,
            -0.49534720182418823,
            -0.4676567614078522,
            2.193021297454834,
            -1.8054354190826416,
            1.3984460830688477,
            -3.218780517578125,
            0.8550246357917786,
            -3.258711576461792,
            -4.991364479064941,
            -1.1602015495300293,
            1.7295442819595337,
            -0.683291494846344,
            -0.24655669927597046,
            -0.44871243834495544,
            1.707704782485962,
            0.31039586663246155,
            -3.559817314147949,
            5.281781196594238,
            -1.7351621389389038,
            -0.02336743474006653,
            1.2612742185592651,
            2.7536895275115967,
            1.009021520614624,
            -3.9505414962768555,
            -1.824650526046753,
            -1.3065950870513916,
            4.68629264831543,
            0.8117695450782776,
            -4.2558064460754395,
            -2.2333569526672363,
            0.4006120562553406,
            3.083207845687866,
            0.7337190508842468,
            3.1123383045196533,
            -0.16668879985809326,
            -3.3922505378723145,
            -4.043789863586426,
            -1.2450501918792725,
            0.6165952086448669,
            -1.271088719367981,
            -0.9454916715621948,
            7.134583950042725,
            2.680751085281372,
            1.9466958045959473,
            3.1704914569854736,
            0.39433616399765015,
            -1.3532618284225464,
            1.8354597091674805,
            2.078522205352783,
            -1.6855690479278564,
            -3.076493501663208,
            0.3910134434700012,
            -2.5606584548950195,
            -0.4420400857925415,
            -1.0220941305160522,
            0.38287121057510376,
            0.2335084080696106,
            -7.57721471786499,
            1.9038681983947754,
            0.04306162893772125,
            -2.6873114109039307,
            6.475236892700195,
            2.8113274574279785,
            1.1469130516052246,
            -0.8903795480728149,
            -1.1831762790679932,
            2.3901517391204834,
            0.09562963247299194,
            -6.090680122375488,
            0.4899928569793701,
            -1.6909153461456299,
            3.5525243282318115,
            2.7823853492736816,
            -2.218324899673462,
            -0.6912242770195007,
            -1.77765953540802,
            -1.2647466659545898,
            -1.4510538578033447,
            -0.7563862800598145,
            0.44729989767074585,
            -0.9725785255432129,
            2.9096267223358154,
            -4.649899959564209,
            3.636741876602173,
            0.4274272322654724,
            0.7993824481964111,
            2.8124327659606934,
            0.29229095578193665,
            2.8179821968078613,
            -2.3964645862579346,
            0.7796407341957092,
            -2.4610185623168945,
            -1.3642244338989258,
            4.551652908325195,
            -1.087519645690918,
            1.7247743606567383,
            0.6531023383140564,
            3.458214521408081,
            -2.9808802604675293,
            2.7469911575317383,
            -2.522951126098633,
            -1.3487329483032227,
            -3.354832172393799,
            1.3670793771743774,
            -0.41837629675865173,
            2.646503448486328,
            2.951817035675049,
            -2.208524227142334,
            2.613374710083008,
            0.40188324451446533,
            -2.091120719909668,
            0.5793275833129883,
            -0.612679660320282,
            -2.861138343811035,
            2.012136936187744,
            5.0827741622924805,
            0.6039643883705139,
            1.5320484638214111,
            0.8216453790664673,
            2.937807321548462,
            -3.1175103187561035,
            -3.0182876586914062,
            5.735759735107422,
            2.726945638656616,
            -0.7747517824172974,
            -2.5384693145751953,
            1.325823426246643,
            1.6822714805603027,
            0.9421437382698059,
            2.4858508110046387,
            3.1180381774902344,
            3.088270664215088,
            1.7456023693084717,
            2.003908634185791,
            -2.6737611293792725,
            2.025482416152954,
            -0.3872188329696655,
            1.63187837600708,
            -1.4054288864135742,
            -1.5415873527526855,
            2.3255505561828613,
            -3.9325947761535645,
            -3.473510503768921,
            -0.6102591753005981,
            -1.6212239265441895,
            -0.12962836027145386,
            -1.0866566896438599,
            0.39042383432388306,
            0.433340847492218,
            -3.0016608238220215,
            -3.796780586242676,
            -3.235048770904541,
            0.08170539140701294,
            -3.557663679122925,
            -3.451962471008301,
            4.491273880004883,
            4.766530513763428,
            3.81201434135437,
            1.034677505493164,
            0.5589970350265503,
            -1.5928573608398438,
            1.6395654678344727,
            1.2743737697601318,
            2.576329469680786,
            -0.01660597324371338,
            3.2573890686035156,
            -0.7058265209197998,
            2.966766357421875,
            12.18014907836914,
            6.943925857543945,
            0.34308987855911255,
            -2.496762752532959,
            -2.7705373764038086,
            -4.908940315246582,
            -1.4262789487838745,
            0.7804021835327148,
            2.211400270462036,
            0.8985570669174194,
            -2.5113184452056885,
            -0.09993639588356018,
            -0.5865788459777832,
            -1.1631641387939453,
            -3.5170938968658447,
            -1.4366744756698608,
            1.3102201223373413,
            2.0836269855499268,
            0.005867496132850647,
            0.42396479845046997,
            -0.9494621157646179,
            1.6607216596603394,
            -1.2283583879470825,
            -0.030200153589248657,
            -3.477450370788574,
            3.199533462524414,
            4.453124523162842,
            2.33589243888855,
            -0.5670993328094482,
            1.868033528327942,
            1.474469542503357,
            0.6398299932479858,
            4.128623962402344,
            -1.0514066219329834,
            -5.380043029785156,
            3.3572864532470703,
            0.7386729121208191,
            -0.6801025867462158,
            0.8979249000549316,
            3.0804381370544434,
            0.8892378807067871,
            -1.151990294456482,
            -4.2351484298706055,
            -0.1777210384607315,
            -0.22806531190872192,
            0.8173700571060181,
            0.24885666370391846,
            -3.1103153228759766,
            -1.2140129804611206,
            6.144321918487549,
            2.509049415588379,
            -2.414687395095825,
            0.3424626588821411,
            -0.852967381477356,
            1.8513410091400146,
            1.2996892929077148,
            1.3408764600753784,
            -4.074821949005127,
            -1.545705795288086,
            -3.59914231300354,
            0.15253113210201263,
            -0.3545812964439392,
            1.6314104795455933,
            -2.3991780281066895,
            -2.295461654663086,
            -1.448055624961853,
            -6.558465003967285,
            1.2871376276016235,
            2.1229422092437744,
            -0.9253247380256653,
            -0.3936961889266968,
            -0.9887270927429199,
            -1.3781534433364868,
            -3.5039591789245605,
            -3.9573049545288086,
            -4.54018497467041,
            -1.5535540580749512,
            -2.876768112182617,
            -1.6704285144805908,
            2.875203847885132,
            -4.108254909515381,
            3.956835985183716,
            -4.246822834014893,
            0.9915144443511963,
            6.379744529724121,
            -2.207561492919922,
            1.2007107734680176,
            2.014949083328247,
            0.6632977724075317,
            7.694941520690918,
            -1.523714303970337,
            -4.571523666381836,
            5.694335460662842,
            0.7331593632698059,
            -0.8479023575782776,
            -4.243475914001465,
            0.27240896224975586,
            0.2686283588409424,
            -4.0375752449035645,
            -2.7221908569335938,
            3.8394551277160645,
            1.8346214294433594,
            1.9774281978607178,
            -2.8397512435913086,
            -3.5924830436706543,
            -1.2790719270706177,
            0.47118228673934937,
            -4.059115409851074,
            -5.741060256958008,
            -1.1671411991119385,
            2.849681854248047,
            -0.20027481019496918,
            -1.8269015550613403,
            1.2380869388580322,
            2.461946964263916,
            -1.0039291381835938,
            -0.6945475339889526,
            5.468612194061279,
            3.208765983581543,
            1.2949495315551758,
            0.8755081295967102,
            -0.8486604690551758,
            -1.4003576040267944,
            -8.186524391174316,
            -3.9496521949768066,
            -2.1096901893615723,
            3.3866090774536133,
            -1.479328989982605,
            4.719545841217041,
            -1.4059884548187256,
            2.466731548309326,
            -0.2815213203430176,
            -0.15726524591445923,
            -0.5178513526916504,
            -1.7898962497711182,
            -2.4326210021972656,
            -0.17685893177986145,
            4.256088733673096,
            -0.3346756398677826,
            4.181981086730957,
            3.303126811981201,
            -0.18805253505706787,
            -0.5768404006958008,
            6.380660533905029,
            -1.8739852905273438,
            -1.7224174737930298,
            1.0792832374572754,
            -0.30999755859375,
            -2.145787239074707,
            1.4590661525726318,
            -2.3556299209594727,
            -0.6397055387496948,
            1.8865206241607666,
            -3.2275843620300293,
            -1.4171242713928223,
            -2.806936740875244
        ]
    },
    "authors": [
        {
            "authorId": "2115348804",
            "name": "Tianyang Lin"
        },
        {
            "authorId": "2115828967",
            "name": "Yuxin Wang"
        },
        {
            "authorId": "2144226697",
            "name": "Xiangyang Liu"
        },
        {
            "authorId": "1767521",
            "name": "Xipeng Qiu"
        }
    ],
    "references": [
        {
            "paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe",
            "title": "Hash Layers For Large Sparse Models"
        },
        {
            "paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
            "title": "Luna: Linear Unified Nested Attention"
        },
        {
            "paperId": "84daddd294fa3cc12596b5785f81c2a153d2fb1d",
            "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling"
        },
        {
            "paperId": "77366bef01df1ab277149b330336a0ef9c5041c4",
            "title": "Transformer"
        },
        {
            "paperId": "1a57318be32b740aef1d9b2070db6c0cc565ab0a",
            "title": "Memory-Efficient Differentiable Transformer Architecture Search"
        },
        {
            "paperId": "7777a31341fa4bfbd25b96a5320681af8dccf3af",
            "title": "Exploring Sparse Expert Models and Beyond"
        },
        {
            "paperId": "9631f5bc3e6d345db42425824f1e7d21d35efa0c",
            "title": "Early Exiting with Ensemble Internal Classifiers"
        },
        {
            "paperId": "9c053552dfa6184f7dc56d620bcb1e8f22c729a3",
            "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit"
        },
        {
            "paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
            "title": "CogView: Mastering Text-to-Image Generation via Transformers"
        },
        {
            "paperId": "e32a12b14e212506115cc6804667b3d8297917e1",
            "title": "Poolingformer: Long Document Modeling with Pooling Attention"
        },
        {
            "paperId": "8b8f7c580bb94ace0676be7a5c424b27b1194913",
            "title": "Learning Shared Semantic Space for Speech-to-Text Translation"
        },
        {
            "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
        },
        {
            "paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844",
            "title": "ViViT: A Video Vision Transformer"
        },
        {
            "paperId": "712bf9c7202b8dec3d06491a380bbac9c9600fbc",
            "title": "Mask Attention Networks: Rethinking and Strengthen Transformer"
        },
        {
            "paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e",
            "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"
        },
        {
            "paperId": "9ed25f101f19ea735ca300848948ed64064b97ca",
            "title": "Random Feature Attention"
        },
        {
            "paperId": "a11676f2864b2d923bb9facc9f6548c812f9e005",
            "title": "M6: A Chinese Multimodal Pretrainer"
        },
        {
            "paperId": "0ae67202f0584afccefa770865d14a46655d2975",
            "title": "Transformer in Transformer"
        },
        {
            "paperId": "df6e4e70af7b3f0a7eb630ed8f36538e6258bc4b",
            "title": "LazyFormer: Self Attention with Lazy Update"
        },
        {
            "paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
            "title": "Zero-Shot Text-to-Image Generation"
        },
        {
            "paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a",
            "title": "Conditional Positional Encodings for Vision Transformers"
        },
        {
            "paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
            "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"
        },
        {
            "paperId": "d46ccff4a72e48c6865599221a71c4dfd3c954ac",
            "title": "SETransformer: Speech Enhancement Transformer"
        },
        {
            "paperId": "fdacf2a732f55befdc410ea927091cad3b791f13",
            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
        },
        {
            "paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
            "title": "Transformers in Vision: A Survey"
        },
        {
            "paperId": "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe",
            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer"
        },
        {
            "paperId": "5e5fbc41106db9acaaf3a365801051e477f0e984",
            "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"
        },
        {
            "paperId": "6914a7997ff4be207fa7b3472a9c5879abaec646",
            "title": "RealFormer: Transformer Likes Residual Attention"
        },
        {
            "paperId": "5b9d8bcc46b766b47389c912a8e026f81b91b0d8",
            "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"
        },
        {
            "paperId": "2e1db8cb373f4d4a51d44308b7a457886d855fbb",
            "title": "End-to-End Object Detection with Adaptive Clustering Transformer"
        },
        {
            "paperId": "fef57f4fba2e4f6b71de44c11995252d01b5406b",
            "title": "Reformer-TTS: Neural Speech Synthesis with Reformer Network"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "131ee42e4839d153333e17f46facdb6806e98c73",
            "title": "Developing Real-Time Streaming Transformer Transducer for Speech Recognition on Large-Scale Dataset"
        },
        {
            "paperId": "d387600e5150b381a306221a5bc9bd92aa99157b",
            "title": "Memformer: The Memory-Augmented Transformer"
        },
        {
            "paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504",
            "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"
        },
        {
            "paperId": "3d2660faebfcc1b7c7777f48b32a2eebec346bab",
            "title": "Guiding Attention for Self-Supervised Learning with Transformers"
        },
        {
            "paperId": "f257e1c1d10a4d8388cc132a51351e1b5e594576",
            "title": "On the Sub-Layer Functionalities of Transformer Decoder"
        },
        {
            "paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
            "title": "Rethinking Attention with Performers"
        },
        {
            "paperId": "55c4a747855c74210919c45f7899e1f79e4c97f5",
            "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data"
        },
        {
            "paperId": "54bc3e055d05e44c010febc669e8dea394643efc",
            "title": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size"
        },
        {
            "paperId": "8d39bd11f54f7889eca269899e52323f2702c8fa",
            "title": "Temporal Context Aggregation for Video Retrieval with Contrastive Learning"
        },
        {
            "paperId": "b0cd93e95fb6885db47d755a4c631158b0198047",
            "title": "DeLighT: Very Deep and Light-weight Transformer"
        },
        {
            "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
            "title": "Big Bird: Transformers for Longer Sequences"
        },
        {
            "paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
            "title": "Generative Pretraining From Pixels"
        },
        {
            "paperId": "cd4ffe5e014601a3d6b64121355d29a730591490",
            "title": "Fast Transformers with Clustered Attention"
        },
        {
            "paperId": "a9d0fb74e6f23b7e02b0c0a0f5271a7db0a6eb13",
            "title": "Switching Poisson Gamma Dynamical Systems"
        },
        {
            "paperId": "1882f194cb43828852cc052887671e55a80f945a",
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
        },
        {
            "paperId": "6f68e1bb253925d8431588555d3010419f322e04",
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
        },
        {
            "paperId": "14216c91c7d02e58717204f04131107778a84e7b",
            "title": "Multi-Head Attention: Collaborate Instead of Concatenate"
        },
        {
            "paperId": "8256f48f759cf85044db251cc512f965834945b3",
            "title": "Rethinking Positional Encoding in Language Pre-training"
        },
        {
            "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
            "title": "Linformer: Self-Attention with Linear Complexity"
        },
        {
            "paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8",
            "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"
        },
        {
            "paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583",
            "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
        },
        {
            "paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
            "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"
        },
        {
            "paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09",
            "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "title": "End-to-End Object Detection with Transformers"
        },
        {
            "paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e",
            "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"
        },
        {
            "paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93",
            "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"
        },
        {
            "paperId": "07a9f47885cae97efb7b4aa109392128532433da",
            "title": "Hard-Coded Gaussian Attention for Neural Machine Translation"
        },
        {
            "paperId": "a238109c3969ae681eee0d4f1bf2012f28850593",
            "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"
        },
        {
            "paperId": "39206ad9fe0859a0043bd8caea2e3f8202b67533",
            "title": "Improving End-to-End Speech Synthesis with Local Recurrent Neural Network Enhanced Transformer"
        },
        {
            "paperId": "90a1491ac32e732c93773354e4e665794ed4d490",
            "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"
        },
        {
            "paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa",
            "title": "Lite Transformer with Long-Short Range Attention"
        },
        {
            "paperId": "8d908042f139575d6688c745e94156c9df6eae07",
            "title": "Understanding the Difficulty of Training Transformers"
        },
        {
            "paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d",
            "title": "ETC: Encoding Long and Structured Inputs in Transformers"
        },
        {
            "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
            "title": "Longformer: The Long-Document Transformer"
        },
        {
            "paperId": "7066df8fd89cca546d1ef3d66679cb15eba48d50",
            "title": "FLAT: Chinese NER Using Flat-Lattice Transformer"
        },
        {
            "paperId": "016a3ba7adcae71f5a23ed2663d8062ae1da63e6",
            "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"
        },
        {
            "paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a",
            "title": "Pre-trained models for natural language processing: A survey"
        },
        {
            "paperId": "4076e421d1758fdb68411242044cd45747b7e35b",
            "title": "PowerNorm: Rethinking Batch Normalization in Transformers"
        },
        {
            "paperId": "e8984c6e6c24aab26c332728a5fff616dfb3adbb",
            "title": "Learning to Encode Position for Transformer with Continuous Dynamical Model"
        },
        {
            "paperId": "657329c633709dd1ac34a30d57341b186b1a47c2",
            "title": "Efficient Content-Based Sparse Attention with Routing Transformers"
        },
        {
            "paperId": "e52051204cb1179584f3b008c9d38848b52c1f28",
            "title": "ReZero is All You Need: Fast Convergence at Large Depth"
        },
        {
            "paperId": "26080498fb851b6239114f0871a4957bea3d3684",
            "title": "Talking-Heads Attention"
        },
        {
            "paperId": "34a4e6818d680875ff0bef9a76de0376118446d1",
            "title": "Sparse Sinkhorn Attention"
        },
        {
            "paperId": "b1c39d042fdf8f00a407b0df734764beb6c3b062",
            "title": "Low-Rank Bottleneck in Multi-head Attention Models"
        },
        {
            "paperId": "2118905985d445af8f9686a1b66b703df7842b97",
            "title": "Controlling Computation versus Quality for Neural Sequence Models"
        },
        {
            "paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
            "title": "GLU Variants Improve Transformer"
        },
        {
            "paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a",
            "title": "On Layer Normalization in the Transformer Architecture"
        },
        {
            "paperId": "d14e56568dc5f57ccdae899d84f91e34ad847670",
            "title": "How Much Position Information Do Convolutional Neural Networks Encode?"
        },
        {
            "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "title": "Reformer: The Efficient Transformer"
        },
        {
            "paperId": "d0e28f5dc1feae19e41087a92a87992977fd85af",
            "title": "Encoding word order in complex embeddings"
        },
        {
            "paperId": "fc9c52f55ffe0e860b1bb4222fe86cce60c05551",
            "title": "Meshed-Memory Transformer for Image Captioning"
        },
        {
            "paperId": "988236ed9defc9d040a5cc3844849d846c9dbd85",
            "title": "Multi-Scale Self-Attention for Text Classification"
        },
        {
            "paperId": "2a02c967dd9848064bca0aa69ea6c75b3765d0ee",
            "title": "Low-Rank and Locality Constrained Self-Attention for Sequence Modeling"
        },
        {
            "paperId": "40922d386116975853a743b1d810c1e0f03e886a",
            "title": "Understanding and Improving Layer Normalization"
        },
        {
            "paperId": "7340a9fbe36587bebb3df1bbbfbfce2bcb576d7f",
            "title": "Iterative Answer Prediction With Pointer-Augmented Multimodal Transformers for TextVQA"
        },
        {
            "paperId": "f51497f463566581874c941353dd9d80069c5b77",
            "title": "Compressive Transformers for Long-Range Sequence Modelling"
        },
        {
            "paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
            "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"
        },
        {
            "paperId": "3ff8d265f4351e4b1fdac5b586466bee0b5d6fff",
            "title": "Improving Transformer Models by Reordering their Sublayers"
        },
        {
            "paperId": "d6b414487787d0b6efd735a3236a690ad13aae70",
            "title": "TENER: Adapting Transformer Encoder for Named Entity Recognition"
        },
        {
            "paperId": "dc52b09089704ebd6f471177474bc29741c50023",
            "title": "Fast Transformer Decoding: One Write-Head is All You Need"
        },
        {
            "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "703685e969fed715e13937c11d7ecc5cc7c4dfd0",
            "title": "Transformers without Tears: Improving the Normalization of Self-Attention"
        },
        {
            "paperId": "29962ae812e7142f56f5f67c2db9d00ab3dfa4c4",
            "title": "T-GSA: Transformer with Gaussian-Weighted Self-Attention for Speech Enhancement"
        },
        {
            "paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06",
            "title": "Axial Attention in Multidimensional Transformers"
        },
        {
            "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
        },
        {
            "paperId": "a99855603e7689a4529a1573f0bc4716adb246c1",
            "title": "Improving Multi-Head Attention with Capsule Networks"
        },
        {
            "paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
            "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"
        },
        {
            "paperId": "4aa6298b606941a282d735fa3143da293199d2ca",
            "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"
        },
        {
            "paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103",
            "title": "Revealing the Dark Secrets of BERT"
        },
        {
            "paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
            "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "84898960f68fa78296a102edc8ac81739f9a9408",
            "title": "Gaussian Transformer: A Lightweight Approach for Natural Language Inference"
        },
        {
            "paperId": "449892c8e095a97b4c9e058ae5be1e9177d805b7",
            "title": "R-Transformer: Recurrent Neural Network Enhanced Transformer"
        },
        {
            "paperId": "bf442ab269074665a68e4dbbe19e4efc97862541",
            "title": "Large Memory Layers with Product Keys"
        },
        {
            "paperId": "830995ef17cc291c13f42dfd9f462137de1d2179",
            "title": "Augmenting Self-attention with Persistent Memory"
        },
        {
            "paperId": "81e1d123a85562555befb0243256b1a0d9fca014",
            "title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View"
        },
        {
            "paperId": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736",
            "title": "Learning Deep Transformer Models for Machine Translation"
        },
        {
            "paperId": "7cc730da554003dda77796d2cb4f06da5dfd5592",
            "title": "Hierarchical Transformers for Multi-Document Summarization"
        },
        {
            "paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13",
            "title": "Adaptive Attention Span in Transformers"
        },
        {
            "paperId": "3928b2177086532775fbf607ae3e05a0375a5061",
            "title": "Language Modeling with Deep Transformers"
        },
        {
            "paperId": "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
            "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"
        },
        {
            "paperId": "f2bb7e2f5a1afad5370159c15760c44df93c0438",
            "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition"
        },
        {
            "paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145",
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"
        },
        {
            "paperId": "21da617a0f79aabf94272107184606cefe90ab75",
            "title": "Generating Long Sequences with Sparse Transformers"
        },
        {
            "paperId": "6c9bfe765f076422256fef909bdca186b5880c52",
            "title": "Information Aggregation for Multi-Head Attention with Routing-by-Agreement"
        },
        {
            "paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a",
            "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"
        },
        {
            "paperId": "2a31319e73d4486716168b65cdf7559baeda18ce",
            "title": "Star-Transformer"
        },
        {
            "paperId": "132b07740db20df2c36d6f939d296a7e941feac7",
            "title": "Insertion-based Decoding with Automatically Inferred Generation Order"
        },
        {
            "paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
            "title": "The Evolved Transformer"
        },
        {
            "paperId": "fea820b7d953d32069e189af2961c28fd213470b",
            "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "de20b6488e148a19ae6c63defbfca8a6373e4110",
            "title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction"
        },
        {
            "paperId": "fdbdd4e0461d23905104460a02a176907d945f44",
            "title": "Multi-Head Attention with Disagreement Regularization"
        },
        {
            "paperId": "1af138dc72fa855cc3bc9c0b83750b461c26e29d",
            "title": "Modeling Localness for Self-Attention Networks"
        },
        {
            "paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
            "title": "Adaptive Input Representations for Neural Language Modeling"
        },
        {
            "paperId": "9f2dd5cc190fc713f1339fca838a5537931744f8",
            "title": "Neural Speech Synthesis with Transformer Network"
        },
        {
            "paperId": "e20ff55e87e2b3ef02ae0529880bb705f5efbcae",
            "title": "Document-Level Neural Machine Translation with Hierarchical Attention Networks"
        },
        {
            "paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d",
            "title": "Character-Level Language Modeling with Deeper Self-Attention"
        },
        {
            "paperId": "8fec5d6ac57e90f459e7330775165f2671abc445",
            "title": "Training Deeper Neural Machine Translation Models with Transparent Attention"
        },
        {
            "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
            "title": "Universal Transformers"
        },
        {
            "paperId": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
            "title": "Representation Learning with Contrastive Predictive Coding"
        },
        {
            "paperId": "c1f457e31b611da727f9aef76c283a18157dfa83",
            "title": "DARTS: Differentiable Architecture Search"
        },
        {
            "paperId": "3a58efcc4558727cc5c131c44923635da4524f33",
            "title": "Relational inductive biases, deep learning, and graph networks"
        },
        {
            "paperId": "6e45251b16cd423f3c025f004959c6d2b26efab0",
            "title": "Accelerating Neural Transformer via an Average Attention Network"
        },
        {
            "paperId": "41a78e2885b5dc8c719495a33985b5f4880f5b48",
            "title": "Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition"
        },
        {
            "paperId": "642c1b4a9da95ea4239708afc5929a5007a1870d",
            "title": "Tensor2Tensor for Neural Machine Translation"
        },
        {
            "paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
            "title": "Self-Attention with Relative Position Representations"
        },
        {
            "paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329",
            "title": "Image Transformer"
        },
        {
            "paperId": "603caed9430283db6c7f43169555c8d18e97a281",
            "title": "Matrix capsules with EM routing"
        },
        {
            "paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9",
            "title": "Generating Wikipedia by Summarizing Long Sequences"
        },
        {
            "paperId": "c4c06578f4870e4b126e6837907929f3c900b99f",
            "title": "Dynamic Routing Between Capsules"
        },
        {
            "paperId": "7cfa5c97164129ce3630511f639040d28db1d4b7",
            "title": "FiLM: Visual Reasoning with a General Conditioning Layer"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "d89ee98810039d2061ed42ee8026da49c503d16b",
            "title": "Learning multiple visual domains with residual adapters"
        },
        {
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning"
        },
        {
            "paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586",
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
        },
        {
            "paperId": "aab5002a22b9b4244a8329b140bd0a86021aa2d1",
            "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "df0402517a7338ae28bc54acaac400de6b456a46",
            "title": "WaveNet: A Generative Model for Raw Audio"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "title": "Gaussian Error Linear Units (GELUs)"
        },
        {
            "paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01",
            "title": "Adaptive Computation Time for Recurrent Neural Networks"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7",
            "title": "Random Features for Large-Scale Kernel Machines"
        },
        {
            "paperId": "ac1feccadc1ab3fb3bd90e1c869304ff3ea9ba1a",
            "title": "Complexity"
        },
        {
            "paperId": "2ff74d426e712522030057624510c03713fa77ba",
            "title": "Linear Transformers Are Secretly Fast Weight Memory Systems"
        },
        {
            "paperId": null,
            "title": "Predictive Attention Transformer: Improving Transformer with Attention Map Prediction"
        },
        {
            "paperId": "dc35daba3fb34b2e6a5b12530badb7b799262bbf",
            "title": "On Position Embeddings in BERT"
        },
        {
            "paperId": "5366919840236059252c7f8f510dfb36df9e3206",
            "title": "TransGAN: Two Transformers Can Make One Strong GAN"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": null,
            "title": "n.d.]. On Position Embeddings in BERT, url = https://openreview.net/forum?id=onxoVA9FxMw, year = 2021"
        },
        {
            "paperId": null,
            "title": "Addressing Some Limitations of Transformers with Feedback Memory"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "title": "A Comprehensive Survey on Graph Neural Networks"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "9dca2dcad6798862d2756b1727cea332902f8fc8",
            "title": "TRANSFORMER WITH GAUSSIAN WEIGHTED SELF-ATTENTION FOR SPEECH ENHANCEMENT"
        },
        {
            "paperId": null,
            "title": "2019. Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a",
            "title": "Searching for Activation Functions"
        },
        {
            "paperId": null,
            "title": "Prototype and Memory Compression . This class of methods reduces the number of queries or key-value memory pairs to reduce the size of the attention matrix"
        },
        {
            "paperId": null,
            "title": "Structural prior . Self-attention does no assume any structural bias over inputs"
        },
        {
            "paperId": null,
            "title": "Due to the limited receptive field of convolutional layers"
        },
        {
            "paperId": null,
            "title": "The improvements on attention mechanism can be divided into several directions"
        },
        {
            "paperId": "6038d62f22be3162324d3cb5214512966fc6ddb0",
            "title": "Music Transformer \uae30\ubc18 \uc74c\uc545"
        }
    ]
}