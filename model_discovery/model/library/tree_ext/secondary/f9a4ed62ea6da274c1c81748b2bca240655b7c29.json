{
    "paperId": "f9a4ed62ea6da274c1c81748b2bca240655b7c29",
    "externalIds": {
        "ArXiv": "2308.16898",
        "DBLP": "journals/corr/abs-2308-16898",
        "DOI": "10.48550/arXiv.2308.16898",
        "CorpusId": 261395206
    },
    "title": "Transformers as Support Vector Machines",
    "abstract": "Since its inception in\"Attention Is All You Need\", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\\top X^\\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. We characterize this convergence, highlighting that it can occur toward locally-optimal directions rather than global ones. (2) Complementing this, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias with nonlinear heads. Our findings are applicable to arbitrary datasets and their validity is verified via experiments. We also introduce several open problems and research directions. We believe these findings inspire the interpretation of transformers as a hierarchy of SVMs that separates and selects optimal tokens.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 97,
    "citationCount": 28,
    "influentialCitationCount": 1,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2308.16898",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work establishes a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs and proposes a more general SVM equivalence that predicts the implicit bias with nonlinear heads."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.957188129425049,
            1.6875665187835693,
            0.8296360969543457,
            4.034860134124756,
            1.937995433807373,
            1.8344064950942993,
            0.85182124376297,
            -1.225876808166504,
            1.4381422996520996,
            -0.07025590538978577,
            0.014312177896499634,
            3.0285966396331787,
            -0.4976322054862976,
            -2.0061089992523193,
            -2.8056929111480713,
            0.2939947247505188,
            -0.9764496684074402,
            0.7860581278800964,
            2.3169357776641846,
            0.9024693965911865,
            -0.7640335559844971,
            1.8176865577697754,
            -4.8083720207214355,
            -4.740692138671875,
            -0.7878013849258423,
            -0.9473198652267456,
            -0.5274050235748291,
            -2.1590144634246826,
            -3.9946658611297607,
            -0.6027258634567261,
            -0.8041438460350037,
            -5.706904411315918,
            5.475786209106445,
            -2.6897261142730713,
            0.49439752101898193,
            -2.8653180599212646,
            -0.6553004384040833,
            8.192052841186523,
            -3.416391372680664,
            -2.5372276306152344,
            -2.32025146484375,
            1.0968142747879028,
            -0.4675557315349579,
            0.37117522954940796,
            -0.9157325625419617,
            0.1250268667936325,
            6.068307399749756,
            0.989696204662323,
            -0.3713083565235138,
            1.735851764678955,
            4.344423294067383,
            -0.7785612344741821,
            -1.0335369110107422,
            2.304922342300415,
            -1.1217302083969116,
            -3.472982883453369,
            -1.578963279724121,
            1.2357797622680664,
            3.6655421257019043,
            -1.086348533630371,
            2.060319662094116,
            6.725042343139648,
            1.4246007204055786,
            -0.13949663937091827,
            3.9105498790740967,
            -3.1765551567077637,
            -3.0363118648529053,
            2.032421588897705,
            -0.48142579197883606,
            -0.001530081033706665,
            -4.91236686706543,
            -6.669924259185791,
            -0.5697734951972961,
            1.437972068786621,
            -0.254754900932312,
            1.2157557010650635,
            -0.9954209923744202,
            -6.771700859069824,
            -1.8589280843734741,
            -1.2816636562347412,
            -2.766238212585449,
            1.7978990077972412,
            -1.2894855737686157,
            -1.0409319400787354,
            4.768643856048584,
            -2.206606864929199,
            -0.6985622048377991,
            1.2666054964065552,
            1.975272297859192,
            -2.5988106727600098,
            -0.8897913694381714,
            1.3892221450805664,
            2.473843812942505,
            1.1466712951660156,
            -3.5293354988098145,
            2.028367519378662,
            2.667309284210205,
            0.9988529086112976,
            -3.27069091796875,
            0.980250358581543,
            1.6888757944107056,
            0.6773634552955627,
            2.9760470390319824,
            2.5632705688476562,
            0.12380433082580566,
            -0.3410431444644928,
            -3.1687729358673096,
            1.2794703245162964,
            2.590590715408325,
            0.5369926691055298,
            1.963789463043213,
            1.7742671966552734,
            -0.09862732887268066,
            1.6147632598876953,
            -1.4379836320877075,
            -2.2221243381500244,
            -0.9828663468360901,
            1.2102223634719849,
            -1.497054100036621,
            4.353146076202393,
            -2.0608038902282715,
            -0.7026941776275635,
            -4.0263261795043945,
            1.7917944192886353,
            0.49534711241722107,
            0.8753048777580261,
            -1.9298665523529053,
            2.8368711471557617,
            -0.5536191463470459,
            -3.2968599796295166,
            2.3066904544830322,
            0.2851980924606323,
            4.024323463439941,
            -0.2049405574798584,
            4.200336456298828,
            3.5256690979003906,
            -2.2819457054138184,
            0.806952953338623,
            -3.890657901763916,
            0.4380345046520233,
            1.3370461463928223,
            3.880563735961914,
            2.2826595306396484,
            0.3954516649246216,
            1.4345595836639404,
            4.334364891052246,
            1.0489637851715088,
            3.6074581146240234,
            -1.266150951385498,
            5.742751598358154,
            4.604314804077148,
            -4.205608367919922,
            -0.40476110577583313,
            2.7674126625061035,
            1.3986940383911133,
            3.7726855278015137,
            -5.079987525939941,
            3.305490016937256,
            -1.6656842231750488,
            -1.079483985900879,
            0.14760532975196838,
            2.089432954788208,
            -11.552891731262207,
            -1.2690010070800781,
            3.0137321949005127,
            -4.486961364746094,
            -2.515504837036133,
            3.7269368171691895,
            -1.2047486305236816,
            2.367739200592041,
            1.0244299173355103,
            2.008694648742676,
            2.1247825622558594,
            1.2081398963928223,
            3.6322503089904785,
            3.7997336387634277,
            2.6348423957824707,
            -5.734836101531982,
            -1.9659016132354736,
            2.482640266418457,
            -1.929537296295166,
            -0.8120187520980835,
            -5.103317737579346,
            2.394425868988037,
            -2.88319993019104,
            -3.3240580558776855,
            -2.6012277603149414,
            -3.3559069633483887,
            -6.296142578125,
            -2.2925267219543457,
            2.680039167404175,
            -1.1873477697372437,
            3.205091953277588,
            4.668453216552734,
            -0.6835135221481323,
            -0.20680972933769226,
            2.3055813312530518,
            2.950408697128296,
            -2.2371785640716553,
            -2.4024009704589844,
            3.2555456161499023,
            1.4484336376190186,
            -2.7566657066345215,
            -3.1170461177825928,
            3.2274630069732666,
            1.7904354333877563,
            -0.5396984815597534,
            5.684326171875,
            3.6415669918060303,
            0.0199529230594635,
            2.1944048404693604,
            -3.586214542388916,
            -0.027074098587036133,
            3.137220859527588,
            -2.5668022632598877,
            -2.645859718322754,
            -4.249337196350098,
            -3.1174702644348145,
            1.0445241928100586,
            1.9370061159133911,
            0.21412473917007446,
            0.06720732897520065,
            -1.5198051929473877,
            -2.9782352447509766,
            2.7599353790283203,
            -3.649972915649414,
            3.3870627880096436,
            -0.06159718334674835,
            0.437092125415802,
            0.7341323494911194,
            0.24702855944633484,
            -5.8577189445495605,
            -0.29499489068984985,
            -0.5793104767799377,
            -4.600600242614746,
            -3.3766820430755615,
            -3.9455204010009766,
            1.4120994806289673,
            -2.2754921913146973,
            -3.5684690475463867,
            8.893966674804688,
            -1.6733884811401367,
            1.196475625038147,
            1.3616437911987305,
            1.5010709762573242,
            -1.8381013870239258,
            -3.412891387939453,
            1.3902417421340942,
            -2.6782944202423096,
            -1.9288322925567627,
            -2.956423759460449,
            -0.02910780906677246,
            2.3307130336761475,
            -0.24896618723869324,
            1.2951545715332031,
            1.2817304134368896,
            3.0686869621276855,
            -2.9794821739196777,
            1.0147532224655151,
            0.9719889163970947,
            0.885768711566925,
            2.4623196125030518,
            -0.8281021118164062,
            3.253239154815674,
            -1.6258420944213867,
            0.7344653606414795,
            -2.0740721225738525,
            -3.7047064304351807,
            -2.402592897415161,
            1.4248639345169067,
            0.68587327003479,
            1.6112943887710571,
            -1.2527039051055908,
            -3.3941402435302734,
            -3.2326266765594482,
            -3.38258695602417,
            -2.2188210487365723,
            0.25938308238983154,
            0.9358733892440796,
            5.998847961425781,
            1.1018500328063965,
            -2.38971209526062,
            -3.6246681213378906,
            -1.368919849395752,
            -1.2978932857513428,
            -0.9788641929626465,
            -2.662050724029541,
            1.1408100128173828,
            2.489419460296631,
            0.5993599891662598,
            -4.866911888122559,
            3.5847830772399902,
            -5.498047828674316,
            -1.8614369630813599,
            -2.895963430404663,
            2.6772232055664062,
            2.813957691192627,
            0.24584966897964478,
            0.24110516905784607,
            1.1338777542114258,
            0.013757318258285522,
            0.7534409761428833,
            4.5510711669921875,
            0.9713979959487915,
            1.8525229692459106,
            3.786954879760742,
            2.558241367340088,
            -2.017399787902832,
            -1.310434103012085,
            -2.7546775341033936,
            1.8188509941101074,
            -2.837416648864746,
            5.140752792358398,
            -2.3195903301239014,
            1.8728241920471191,
            -0.8895465135574341,
            -0.7727109789848328,
            0.35161536931991577,
            -1.9415634870529175,
            3.8036117553710938,
            2.1431374549865723,
            0.9636436700820923,
            -4.150350093841553,
            -0.7882696390151978,
            -1.0205886363983154,
            2.539760112762451,
            -0.6673375964164734,
            2.726095676422119,
            -3.7226696014404297,
            4.733367443084717,
            -1.8965740203857422,
            4.620944976806641,
            3.265315055847168,
            0.4504181742668152,
            -0.5639634132385254,
            -4.532365798950195,
            -2.618046760559082,
            0.7608852982521057,
            -1.5186066627502441,
            1.729996919631958,
            0.8303793668746948,
            6.153134822845459,
            -0.5792132019996643,
            -0.6217736005783081,
            1.6939926147460938,
            -1.1474952697753906,
            2.141528844833374,
            -2.245450496673584,
            0.12460580468177795,
            0.538697361946106,
            -0.4757964015007019,
            0.17465046048164368,
            2.910085678100586,
            -1.5231260061264038,
            -0.9714415073394775,
            3.128753185272217,
            4.472916126251221,
            1.7498812675476074,
            0.8108835816383362,
            1.4501898288726807,
            -1.3924590349197388,
            -2.6961846351623535,
            1.1073710918426514,
            0.6204729080200195,
            -0.3868650197982788,
            -4.765133857727051,
            8.860289573669434,
            -5.012611389160156,
            -0.6577919125556946,
            -4.552397727966309,
            -4.7466325759887695,
            0.20146088302135468,
            -2.3429291248321533,
            4.883251190185547,
            -0.3713836073875427,
            -0.7747470140457153,
            -0.6334613561630249,
            -3.4081482887268066,
            1.9754395484924316,
            -0.33643287420272827,
            -0.4580468535423279,
            3.8676884174346924,
            -0.5838099122047424,
            2.5233325958251953,
            -1.981987714767456,
            -0.1492237150669098,
            -2.2023534774780273,
            3.324984312057495,
            0.42604273557662964,
            0.31245607137680054,
            0.05882212519645691,
            2.1484904289245605,
            0.9378610849380493,
            -0.4554760456085205,
            -2.861678123474121,
            -4.6822614669799805,
            -1.9835063219070435,
            -4.795936584472656,
            1.7904936075210571,
            -1.992793083190918,
            -1.5425405502319336,
            -0.08630430698394775,
            5.839752197265625,
            3.4979727268218994,
            -3.3322792053222656,
            0.5905548334121704,
            7.695627212524414,
            -0.19397598505020142,
            -1.4797430038452148,
            -1.293858289718628,
            -5.130788803100586,
            -1.8271468877792358,
            -0.29247182607650757,
            -6.28149938583374,
            1.197213888168335,
            -0.09687809646129608,
            1.3012421131134033,
            3.0002927780151367,
            1.6820638179779053,
            1.0172843933105469,
            -1.2611500024795532,
            3.215132713317871,
            5.414965629577637,
            1.1529124975204468,
            -1.6241494417190552,
            0.1664429008960724,
            3.9436676502227783,
            3.096989631652832,
            0.6097542643547058,
            1.472505807876587,
            -0.5063292384147644,
            4.457045078277588,
            -2.6431808471679688,
            -1.1705663204193115,
            2.8848531246185303,
            5.674410343170166,
            0.8122024536132812,
            1.7391310930252075,
            0.7980884909629822,
            -2.4081807136535645,
            2.093177556991577,
            1.6322834491729736,
            -3.83632493019104,
            3.0856642723083496,
            0.2957403063774109,
            -0.5425629615783691,
            2.054558753967285,
            0.8364688754081726,
            0.7906489372253418,
            -4.52838134765625,
            2.9507298469543457,
            -3.429304838180542,
            -2.7313742637634277,
            -4.791323661804199,
            -0.6891628503799438,
            1.4241575002670288,
            -2.391845703125,
            0.9884164333343506,
            -1.2404038906097412,
            0.9508625268936157,
            -3.973367691040039,
            4.339831829071045,
            -4.346007347106934,
            2.969972610473633,
            1.6636192798614502,
            1.573024034500122,
            0.9003385305404663,
            -4.234231948852539,
            -0.9229743480682373,
            0.9284517765045166,
            -0.1522722989320755,
            0.16330908238887787,
            -1.4393285512924194,
            -0.8789131045341492,
            -0.331701397895813,
            2.436222553253174,
            2.6601202487945557,
            4.785826683044434,
            3.49289870262146,
            -2.7525322437286377,
            -3.6287100315093994,
            -0.572545051574707,
            0.3930451273918152,
            -1.9284790754318237,
            -1.4813562631607056,
            3.8152785301208496,
            2.7057862281799316,
            0.10822635143995285,
            3.973059892654419,
            1.6215894222259521,
            0.39604294300079346,
            0.9675779342651367,
            5.154392719268799,
            -1.9893364906311035,
            2.859266757965088,
            -1.6956332921981812,
            -3.2721118927001953,
            -0.30345702171325684,
            3.0383782386779785,
            -3.903418779373169,
            -0.8246209621429443,
            -3.066816568374634,
            -0.7532671093940735,
            -1.9164516925811768,
            -1.7903083562850952,
            3.7639787197113037,
            4.921847343444824,
            1.235285758972168,
            -2.3021976947784424,
            -2.9715614318847656,
            -1.0886476039886475,
            4.388136863708496,
            -8.805730819702148,
            0.7377559542655945,
            -4.022608280181885,
            0.5845766663551331,
            3.953014373779297,
            -1.7764453887939453,
            1.4650623798370361,
            0.12206131219863892,
            -0.6261773705482483,
            1.1566686630249023,
            -1.0665283203125,
            2.83451771736145,
            0.27296143770217896,
            -3.058285713195801,
            0.11535206437110901,
            2.022505760192871,
            2.8834502696990967,
            3.959740161895752,
            5.015954494476318,
            4.358522415161133,
            1.361397624015808,
            -1.182472586631775,
            1.6441535949707031,
            -2.068751335144043,
            0.645038366317749,
            7.594267845153809,
            -2.3265810012817383,
            1.2111201286315918,
            0.12197214365005493,
            -0.08093522489070892,
            1.182088851928711,
            0.054222047328948975,
            -1.494703769683838,
            1.5302339792251587,
            -0.006868094205856323,
            -0.6284120678901672,
            -3.5106797218322754,
            -0.205591082572937,
            1.2088547945022583,
            -0.8506878614425659,
            3.8011887073516846,
            -2.40714168548584,
            -3.1578428745269775,
            -1.7702746391296387,
            -1.7026257514953613,
            -2.246657371520996,
            2.4005796909332275,
            0.6202163696289062,
            2.082237482070923,
            -2.1470582485198975,
            0.6744071245193481,
            -0.9911478161811829,
            -1.4267487525939941,
            0.3316035270690918,
            0.208705335855484,
            3.7523083686828613,
            0.8614012002944946,
            -1.3319191932678223,
            1.5201566219329834,
            -1.4248687028884888,
            -0.5733213424682617,
            1.116620659828186,
            2.4221320152282715,
            3.3456335067749023,
            1.5003302097320557,
            3.794822931289673,
            -3.052093505859375,
            1.3335628509521484,
            -0.41753435134887695,
            -1.8046658039093018,
            -4.296101093292236,
            -4.108921527862549,
            1.790083646774292,
            -2.738616943359375,
            -3.6440327167510986,
            1.3379275798797607,
            -2.37814998626709,
            -2.276789665222168,
            0.04522152245044708,
            -1.3075493574142456,
            -0.5643007755279541,
            -4.185288906097412,
            2.33054518699646,
            -4.5271220207214355,
            1.3480229377746582,
            -4.156993389129639,
            -0.3772105574607849,
            3.3428802490234375,
            3.2219743728637695,
            4.589902877807617,
            2.2626633644104004,
            1.6640324592590332,
            -0.39928144216537476,
            0.18780206143856049,
            2.575817584991455,
            0.08346864581108093,
            0.9594206809997559,
            2.2312066555023193,
            -0.33372628688812256,
            1.7625758647918701,
            16.002399444580078,
            -1.1134384870529175,
            2.3576245307922363,
            -2.8141074180603027,
            1.2736326456069946,
            -5.812503337860107,
            -3.634030342102051,
            3.6247916221618652,
            1.6127887964248657,
            3.7130017280578613,
            -1.1099408864974976,
            -1.8422181606292725,
            1.1078466176986694,
            -2.5245392322540283,
            -4.505928993225098,
            -0.0827782154083252,
            -2.0965969562530518,
            2.9511313438415527,
            -1.3100844621658325,
            -1.0169625282287598,
            2.8741846084594727,
            2.8226680755615234,
            4.203009605407715,
            0.7596943974494934,
            -0.9622991681098938,
            7.559153079986572,
            1.459169626235962,
            2.576087474822998,
            -3.349321126937866,
            0.7544220685958862,
            -1.017311930656433,
            4.837887287139893,
            0.9713239669799805,
            -0.8142993450164795,
            -1.8980928659439087,
            2.579237937927246,
            4.312352180480957,
            -2.425300121307373,
            1.2766238451004028,
            -0.8129833936691284,
            -0.18058355152606964,
            1.0102088451385498,
            -3.687889575958252,
            -1.3370623588562012,
            0.5196928977966309,
            2.080906867980957,
            3.6194019317626953,
            -2.8789138793945312,
            -0.3913743197917938,
            2.361405611038208,
            2.7975409030914307,
            -0.3619404435157776,
            0.0766935795545578,
            4.133019924163818,
            1.8754332065582275,
            2.8104262351989746,
            -1.4240401983261108,
            1.1369847059249878,
            3.6673102378845215,
            3.7249298095703125,
            -1.119942545890808,
            -1.2120752334594727,
            -0.3391870856285095,
            -4.529487133026123,
            -2.32641863822937,
            1.6274631023406982,
            -6.265314102172852,
            2.0868115425109863,
            0.9345687627792358,
            3.7859396934509277,
            3.6689577102661133,
            -0.3169506788253784,
            -2.2336559295654297,
            -1.8501019477844238,
            -2.630248546600342,
            -7.712355613708496,
            0.03809046745300293,
            0.3940534293651581,
            0.8873959183692932,
            7.032470703125,
            -3.486100673675537,
            0.5534334182739258,
            -2.67462158203125,
            1.049998164176941,
            3.8025975227355957,
            -4.689278602600098,
            3.460017681121826,
            1.9879735708236694,
            -0.4953499436378479,
            3.1505093574523926,
            -1.46094810962677,
            -2.8697540760040283,
            0.11767344176769257,
            0.2916179299354553,
            2.4929869174957275,
            -1.6589226722717285,
            -0.9898864030838013,
            -3.1199560165405273,
            -4.407481670379639,
            -3.5609288215637207,
            4.389781951904297,
            2.1783244609832764,
            1.5387543439865112,
            -0.5080258250236511,
            1.1560454368591309,
            0.5158898830413818,
            -0.6288739442825317,
            -5.25290060043335,
            -4.003665924072266,
            -3.0136168003082275,
            1.804341435432434,
            -4.06374454498291,
            0.770092248916626,
            2.777143955230713,
            -1.6166179180145264,
            -1.6297205686569214,
            -3.0187597274780273,
            2.314777374267578,
            1.605776309967041,
            2.4033875465393066,
            1.0870881080627441,
            -2.5230371952056885,
            -0.0386006236076355,
            -2.6668262481689453,
            -2.475515842437744,
            1.4109441041946411,
            0.6495761871337891,
            -5.746616363525391,
            1.4107664823532104,
            -0.9927222728729248,
            1.2486613988876343,
            -1.5119857788085938,
            -3.9082107543945312,
            -2.721479892730713,
            -0.991855800151825,
            -1.1529871225357056,
            2.294762134552002,
            2.5853567123413086,
            -2.2471659183502197,
            4.95602560043335,
            -0.9231675267219543,
            -1.9553155899047852,
            0.3725197911262512,
            8.058438301086426,
            -1.2952131032943726,
            -1.9519751071929932,
            -0.583220899105072,
            -0.26944318413734436,
            -1.547829270362854,
            -0.9289999008178711,
            0.9081330895423889,
            -0.544567883014679,
            3.2262461185455322,
            -0.6165792942047119,
            -3.680755853652954,
            1.1916838884353638
        ]
    },
    "authors": [
        {
            "authorId": "3281605",
            "name": "Davoud Ataee Tarzanagh"
        },
        {
            "authorId": "1527089987",
            "name": "Yingcong Li"
        },
        {
            "authorId": "2751682",
            "name": "Christos Thrampoulidis"
        },
        {
            "authorId": "3103394",
            "name": "Samet Oymak"
        }
    ],
    "references": [
        {
            "paperId": "c747ba541fa06715543bb6b9b335ce22e5aa1b86",
            "title": "A Primal-Dual Framework for Transformers and Neural Networks"
        },
        {
            "paperId": "c1738f21ea2460e1015d590906a4f43e155f60c8",
            "title": "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit"
        },
        {
            "paperId": "4a7530bbaee7563ee244f3ffed6b706bd96f08a8",
            "title": "Trained Transformers Learn Linear Models In-Context"
        },
        {
            "paperId": "194a048814acc5cd5a9ee08102df3dcb61b2dfc9",
            "title": "Transformers learn through gradual rank increase"
        },
        {
            "paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3",
            "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"
        },
        {
            "paperId": "3f16d91bdfca925df761d27fd3b11af7d68c63d8",
            "title": "On the Role of Attention in Prompt-tuning"
        },
        {
            "paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2",
            "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
        },
        {
            "paperId": "50eb97f832ffcd2114f79957c977215176384e3d",
            "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "91166a75f0e32b782a57028f1501aba6335ac550",
            "title": "A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity"
        },
        {
            "paperId": "d4c60620570801a231a7756f931dda1740288fb9",
            "title": "Looped Transformers as Programmable Computers"
        },
        {
            "paperId": "a7fa71dc6856ebef79f354597128d1c68b19b6e4",
            "title": "Transformers as Algorithms: Generalization and Stability in In-context Learning"
        },
        {
            "paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
            "title": "What learning algorithm is in-context learning? Investigations with linear models"
        },
        {
            "paperId": "14f33b6d2db524cc65774dec0c5292ffe0bb0e80",
            "title": "Convexifying Transformers: Improving optimization and understanding of transformer networks"
        },
        {
            "paperId": "13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0",
            "title": "Vision Transformers provably learn spatial structure"
        },
        {
            "paperId": "095a24fd666bd6b1b36566b33e30219132c6f2b6",
            "title": "Imbalance Trouble: Revisiting Neural-Collapse Geometry"
        },
        {
            "paperId": "fd2e5e75220da8d14f448343cfb5e7c31e76786b",
            "title": "Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer"
        },
        {
            "paperId": "daa531f54e2a57a18def4aebe869d1e368aab4a6",
            "title": "Mirror Descent Maximizes Generalized Margin and Can Be Implemented Efficiently"
        },
        {
            "paperId": "d163cca5cfea5d967873d34023554e3d1771716b",
            "title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers"
        },
        {
            "paperId": "429e5875e59fa30fbb835ef3a1f85036a73db0ba",
            "title": "The Quarks of Attention"
        },
        {
            "paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
            "title": "Flowformer: Linearizing Transformers with Conservation Flows"
        },
        {
            "paperId": "f7deb6a4db69499038b92118c1102e5fbe41449d",
            "title": "Implicit Regularization Towards Rank Minimization in ReLU Networks"
        },
        {
            "paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737",
            "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"
        },
        {
            "paperId": "40c115c43adee6bc7a00ffd444ee9c045360d97d",
            "title": "What Happens after SGD Reaches Zero Loss? -A Mathematical Framework"
        },
        {
            "paperId": "46ebd8e54cd72251642899da51deb8679cd76193",
            "title": "The Benefits of Implicit Regularization from SGD in Least Squares Problems"
        },
        {
            "paperId": "6be4131e17c6d63188348f7781263e8d6c0232c0",
            "title": "Fast Margin Maximization via Dual Acceleration"
        },
        {
            "paperId": "2842ca22551477a0208502ec8441c7adc4ebb10e",
            "title": "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction"
        },
        {
            "paperId": "0f900b9876d03cc2b09b8c113121e749ccddd698",
            "title": "Label Noise SGD Provably Prefers Flat Global Minimizers"
        },
        {
            "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
            "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
        },
        {
            "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
            "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
        },
        {
            "paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6",
            "title": "Multiscale Vision Transformers"
        },
        {
            "paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e",
            "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"
        },
        {
            "paperId": "4a69ad4abbf38420a6df4db541a40d7e5b46ca14",
            "title": "Label-Imbalanced and Group-Sensitive Classification under Overparameterization"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "48d3e2815410ec850f2bae5f48eeccabd13bc3ee",
            "title": "The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks"
        },
        {
            "paperId": "710d06df0842aebe09bd4bf75b1a0a4abce02e0b",
            "title": "Binary Classification of Gaussian Mixtures: Abundance of Support Vectors, Benign Overfitting, and Regularization"
        },
        {
            "paperId": "f10a04a77fd1cd719792de374a60f3fd03f6b944",
            "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent"
        },
        {
            "paperId": "6010bf788eef9825682ea09068ead4617bf46c26",
            "title": "A Unifying View on Implicit Bias in Training Linear Neural Networks"
        },
        {
            "paperId": "e8013384bdcb3d08ed7dad0a43eb970b22e8b38e",
            "title": "On the proliferation of support vectors in high dimensions"
        },
        {
            "paperId": "806e27fb9d8781c3c6e918734453418ebbeae96c",
            "title": "Prevalence of neural collapse during the terminal phase of deep learning training"
        },
        {
            "paperId": "a75494dfe490b82f27ef99c077caeafc6d312f42",
            "title": "Winnowing with Gradient Descent"
        },
        {
            "paperId": "b54b2f314d73d92da6c1bc137e4bbc3536da5240",
            "title": "Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy"
        },
        {
            "paperId": "8424480a0383f20d71f801efa190f31ab716ae68",
            "title": "von Neumann\u2019s trace inequality for Hilbert\u2013Schmidt operators"
        },
        {
            "paperId": "799a6f18c79b56c04c42bd96e4e39f8119b57843",
            "title": "Gradient descent follows the regularization path for general losses"
        },
        {
            "paperId": "82b20ed50126e106091dd16aaeb538cbb3bfddb9",
            "title": "Shape Matters: Understanding the Implicit Bias of the Noise Covariance"
        },
        {
            "paperId": "221a1f9dce1e0ccab0debae7f7f3d4c4d91c876c",
            "title": "Directional convergence and alignment in deep learning"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "cd95817a31d7d991d5b861de81843e7d35f5584b",
            "title": "Classification vs regression in overparameterized regimes: Does the loss function matter?"
        },
        {
            "paperId": "8001cc34050d5f67458ed9410445d4c155c65a6f",
            "title": "Reparameterizing Mirror Descent as Gradient Descent"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "ee8a9a6246ef6fa3ecdd2716560b4cff932e298e",
            "title": "Implicit Regularization for Optimal Sparse Recovery"
        },
        {
            "paperId": "36451fe94b7b1f978b8301cf3f7305566bd3b454",
            "title": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"
        },
        {
            "paperId": "c563c0e06684f42bc5d76dfc7304581c11312393",
            "title": "Benign overfitting in linear regression"
        },
        {
            "paperId": "76c2679deb0b7689c658c199254963889d4d2b69",
            "title": "The implicit bias of gradient descent on nonseparable data"
        },
        {
            "paperId": "b3564be8b79f25585acb035f3deaf4ae93c26d8f",
            "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"
        },
        {
            "paperId": "3a982f7e82bedb82bd2b53cf2da1d08012ed5179",
            "title": "Kernel and Rich Regimes in Overparametrized Models"
        },
        {
            "paperId": "3f46ac38812f9f0f99ff1edddd85d71a84da4497",
            "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks"
        },
        {
            "paperId": "89b78ccb0ab37c5f6a610e61384363798f127b71",
            "title": "Characterizing the implicit bias via a primal-dual analysis"
        },
        {
            "paperId": "247d724b7fd3269d8bde7c5d2163803a40c6a0c5",
            "title": "Stochastic Mirror Descent on Overparameterized Nonlinear Models"
        },
        {
            "paperId": "725c691c1b53d0c132c4b2e8de10f8cc63730813",
            "title": "The Implicit Bias of AdaGrad on Separable Data"
        },
        {
            "paperId": "217a85f667778567d7ffc8b56060783caf5803b0",
            "title": "Implicit Regularization in Deep Matrix Factorization"
        },
        {
            "paperId": "bcf397d57dbcb5d423a341bfadef8dfc09d0cfb8",
            "title": "Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process"
        },
        {
            "paperId": "c0e50efe350e663994c7b1fac490f8423bf68302",
            "title": "Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?"
        },
        {
            "paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
            "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"
        },
        {
            "paperId": "42ec3db12a2e4628885451b13035c2e975220a25",
            "title": "A Convergence Theory for Deep Learning via Over-Parameterization"
        },
        {
            "paperId": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6",
            "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"
        },
        {
            "paperId": "7a84a692327534fd227fa1e07fcb3816b633c591",
            "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
        },
        {
            "paperId": "4a5a17d7849b91a3af583c7b99403844e1a5cdb1",
            "title": "Risk and parameter convergence of logistic regression"
        },
        {
            "paperId": "59ef5569832b2fbb865466b3951ad0957537cd54",
            "title": "Convergence of Gradient Descent on Separable Data"
        },
        {
            "paperId": "33416f2dc49db24cca520a3b234f02463a4e833e",
            "title": "Characterizing Implicit Bias in Terms of Optimization Geometry"
        },
        {
            "paperId": "8b4b861583f698e89c8cd9e198aad86809a71de7",
            "title": "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations"
        },
        {
            "paperId": "11adc8bd35bd897502f9b5452ab7ac668ec9b0fb",
            "title": "The Implicit Bias of Gradient Descent on Separable Data"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "4e8917e73e02c76d55ded62e43541d44684a4c8a",
            "title": "Implicit Regularization in Matrix Factorization"
        },
        {
            "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
            "title": "A Deep Reinforced Model for Abstractive Summarization"
        },
        {
            "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
            "title": "A Structured Self-attentive Sentence Embedding"
        },
        {
            "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "title": "A Decomposable Attention Model for Natural Language Inference"
        },
        {
            "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
            "title": "Long Short-Term Memory-Networks for Machine Reading"
        },
        {
            "paperId": "1eb94de6649346765f920d88b7de0ff05d16718d",
            "title": "Low-rank Solutions of Linear Matrix Equations via Procrustes Flow"
        },
        {
            "paperId": "abbad4e539afe812f02cd42cb052a4f2d21dc651",
            "title": "A simplified approach to recovery conditions for low rank matrices"
        },
        {
            "paperId": "b8686e4c19f491a4a62443ca299ef17686f2b77b",
            "title": "Null space conditions and thresholds for rank minimization"
        },
        {
            "paperId": "0e021141b793a0b880039ee24a5ca06e82b14cec",
            "title": "New Null Space Results and Recovery Thresholds for Matrix Rank Minimization"
        },
        {
            "paperId": "d8c0f0a243070ba05883befce0095a633186dd53",
            "title": "Signal Recovery From Random Measurements Via Orthogonal Matching Pursuit"
        },
        {
            "paperId": "0c9bb579d8ad6ac987f7a16b66ddace671fc57c5",
            "title": "Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization"
        },
        {
            "paperId": "b0816b4f1fdf1ebc324663f66485e05d58cbd1a7",
            "title": "Boosting with early stopping: Convergence and consistency"
        },
        {
            "paperId": "cedf154c28178370d95510112413dc8cb48120a8",
            "title": "Maximum-Margin Matrix Factorization"
        },
        {
            "paperId": "c1180048929ed490ab25e0e612f8f7c3d7196450",
            "title": "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information"
        },
        {
            "paperId": "f6d16b4db8779721715a00008ba03b5c67e4669c",
            "title": "Margin Maximizing Loss Functions"
        },
        {
            "paperId": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "title": "Atomic Decomposition by Basis Pursuit"
        },
        {
            "paperId": "7efdee766894ebc7f3cc8780138875e9b334b436",
            "title": "Margin Maximization in Attention Mechanism"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": "23ca161eb2cf951ddc9aab680fc118fa40473ac2",
            "title": "Momentum Doesn't Change the Implicit Bias"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "7a4122ee1a93986aa60553389957d554561c4180",
            "title": "Connecting Optimization and Regularization Paths"
        },
        {
            "paperId": "e7321ab0f3be0b29aaf5f073fd7de7da5fed2f92",
            "title": "Compressed Sensing"
        },
        {
            "paperId": null,
            "title": "Matrix rank minimization with applications"
        },
        {
            "paperId": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "title": "Regression Shrinkage and Selection via the Lasso"
        }
    ]
}