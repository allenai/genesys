{
    "paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8",
    "externalIds": {
        "ArXiv": "2106.07139",
        "DBLP": "journals/aiopen/HanZDGLHQYZZHHJ21",
        "DOI": "10.1016/j.aiopen.2021.08.002",
        "CorpusId": 235421816
    },
    "title": "Pre-Trained Models: Past, Present and Future",
    "abstract": null,
    "venue": "AI Open",
    "year": 2021,
    "referenceCount": 323,
    "citationCount": 601,
    "influentialCitationCount": 20,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, is taken to reveal the crucial position of PTMs in the AI development spectrum."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -2.288743495941162,
            -1.3246116638183594,
            0.44816145300865173,
            6.412956237792969,
            -3.6042892932891846,
            1.6844935417175293,
            3.897683620452881,
            -2.1751463413238525,
            0.13075613975524902,
            -0.09991466999053955,
            1.1198081970214844,
            5.304198265075684,
            -0.9998779296875,
            0.736952006816864,
            -4.770788192749023,
            1.7226831912994385,
            0.6631691455841064,
            2.216712474822998,
            5.62343692779541,
            -0.30330055952072144,
            -4.923868179321289,
            2.924832820892334,
            0.28549161553382874,
            -1.4115668535232544,
            -4.434640884399414,
            -0.4213681221008301,
            3.156491279602051,
            0.27442580461502075,
            -1.550636649131775,
            -2.0538129806518555,
            1.6488949060440063,
            -6.194716930389404,
            4.167666435241699,
            -4.486536502838135,
            3.257331609725952,
            -4.249642848968506,
            -2.7410972118377686,
            8.57873821258545,
            -3.0471818447113037,
            3.00529146194458,
            0.4716247320175171,
            -2.174619197845459,
            1.2072575092315674,
            0.5179206728935242,
            -0.01211249828338623,
            2.014716386795044,
            1.6165878772735596,
            0.08661852777004242,
            1.6217342615127563,
            1.3946332931518555,
            2.690931558609009,
            1.5914301872253418,
            0.89836585521698,
            1.8215155601501465,
            1.771674394607544,
            -2.6979384422302246,
            -0.1289292275905609,
            0.9842164516448975,
            -1.1806674003601074,
            1.009566068649292,
            4.655808448791504,
            6.809956073760986,
            0.553991436958313,
            0.13078200817108154,
            0.33892762660980225,
            -4.91947603225708,
            -1.7222250699996948,
            3.801809549331665,
            1.2598265409469604,
            0.611962080001831,
            -1.0371286869049072,
            -6.452890396118164,
            -0.5116783380508423,
            0.24399453401565552,
            -2.713698148727417,
            1.2151631116867065,
            -0.2932649850845337,
            -4.661268711090088,
            -1.7918987274169922,
            1.0513951778411865,
            2.4558463096618652,
            2.1285600662231445,
            1.361479640007019,
            0.1523439884185791,
            2.248279571533203,
            0.296323299407959,
            -3.3828721046447754,
            1.23690664768219,
            0.8739398717880249,
            -6.26303768157959,
            1.685497760772705,
            0.15923450887203217,
            1.8738505840301514,
            1.2414367198944092,
            -6.539187431335449,
            0.2220398187637329,
            -0.14877751469612122,
            -3.3222084045410156,
            -1.6482433080673218,
            3.885192394256592,
            4.29604959487915,
            0.9434044361114502,
            4.45369815826416,
            -0.6502882838249207,
            4.457833290100098,
            -2.425018310546875,
            -0.1363891065120697,
            0.3641257882118225,
            4.317068099975586,
            -0.39298680424690247,
            -5.075884819030762,
            2.638671875,
            -1.088118553161621,
            -0.8287138938903809,
            -3.069328546524048,
            -1.5990031957626343,
            -0.24344509840011597,
            -1.8158397674560547,
            -1.9958053827285767,
            3.4215805530548096,
            -0.5240924954414368,
            -0.02037280797958374,
            -1.4291269779205322,
            0.36335277557373047,
            -1.1800928115844727,
            3.513765573501587,
            -0.8710930347442627,
            2.455273151397705,
            -1.30583918094635,
            -3.908421039581299,
            5.076691627502441,
            -2.95603609085083,
            3.622429132461548,
            -1.8411568403244019,
            2.672743082046509,
            3.098466396331787,
            -2.4911670684814453,
            0.44194653630256653,
            -3.643826723098755,
            -1.0370817184448242,
            -0.31137004494667053,
            5.328287124633789,
            0.8275284767150879,
            -1.036296010017395,
            1.0947976112365723,
            3.9917380809783936,
            0.5719961524009705,
            2.426753282546997,
            1.480353593826294,
            4.078495979309082,
            3.762155055999756,
            -3.6418776512145996,
            -1.2367126941680908,
            -1.4697175025939941,
            4.284553050994873,
            3.448840618133545,
            -7.874294281005859,
            1.546694278717041,
            -1.2388577461242676,
            0.27418386936187744,
            1.53606116771698,
            -0.9343649744987488,
            -9.462926864624023,
            1.3262821435928345,
            3.3765039443969727,
            -4.073153972625732,
            -2.748959541320801,
            1.2701088190078735,
            -2.911304235458374,
            -0.09603112936019897,
            2.603365182876587,
            4.0436625480651855,
            1.877582311630249,
            5.184844017028809,
            2.3667185306549072,
            5.028027534484863,
            3.2271647453308105,
            -2.074867010116577,
            -0.29065966606140137,
            0.9443368315696716,
            -1.340608835220337,
            -3.2706637382507324,
            -3.8428587913513184,
            3.9936506748199463,
            -5.699060440063477,
            -1.516291856765747,
            -3.7146494388580322,
            -5.42990255355835,
            2.4273641109466553,
            1.7607728242874146,
            -3.40633487701416,
            1.4678065776824951,
            4.324220657348633,
            6.876199722290039,
            2.8291919231414795,
            1.236006498336792,
            0.6589325070381165,
            5.203704833984375,
            -2.4913811683654785,
            3.3577799797058105,
            3.14656138420105,
            -0.7922998666763306,
            -0.29455551505088806,
            -2.504492998123169,
            1.7892239093780518,
            4.920227527618408,
            -0.8840851783752441,
            2.094696521759033,
            0.5533138513565063,
            3.5204505920410156,
            1.665317177772522,
            -0.6587275862693787,
            -1.171130657196045,
            0.07314532995223999,
            -1.3052432537078857,
            -1.1805808544158936,
            -5.806662082672119,
            3.631333351135254,
            3.828359842300415,
            1.1680352687835693,
            -0.7608394622802734,
            -2.7786407470703125,
            -0.33627068996429443,
            -0.42888784408569336,
            4.1091413497924805,
            -7.203622817993164,
            3.780897617340088,
            1.0058811902999878,
            -0.343389630317688,
            2.1288344860076904,
            -1.5088908672332764,
            -5.626384258270264,
            3.16975736618042,
            0.19076213240623474,
            -7.6200175285339355,
            -2.215498685836792,
            -3.9522297382354736,
            1.79680335521698,
            -0.1707322597503662,
            -0.8925008773803711,
            4.997235298156738,
            1.40409255027771,
            2.221437931060791,
            2.756770610809326,
            1.2652673721313477,
            -0.5985602140426636,
            -2.4919281005859375,
            1.3432751893997192,
            -2.4139087200164795,
            -3.1131792068481445,
            -2.73344349861145,
            -0.1984560489654541,
            1.355539083480835,
            1.3192975521087646,
            1.1106505393981934,
            1.2605187892913818,
            3.2238597869873047,
            0.7308523654937744,
            0.612596869468689,
            1.0760302543640137,
            1.667834997177124,
            3.4798715114593506,
            0.7184731960296631,
            3.684109926223755,
            -0.740869402885437,
            1.163245439529419,
            -2.6225106716156006,
            -0.9916133880615234,
            -1.9719717502593994,
            1.229880690574646,
            3.1914498805999756,
            1.4133169651031494,
            -1.6315298080444336,
            -4.794067859649658,
            -3.070913314819336,
            -3.8516461849212646,
            -3.3269896507263184,
            -1.040572166442871,
            1.9224660396575928,
            2.8938727378845215,
            -0.0684410035610199,
            -2.390977382659912,
            1.7615752220153809,
            -2.274188995361328,
            0.22521352767944336,
            -2.844912528991699,
            -3.9402852058410645,
            0.8750535249710083,
            -0.7674605250358582,
            -3.3782057762145996,
            -3.6616005897521973,
            4.706241607666016,
            -1.9912376403808594,
            -0.3512361943721771,
            -3.573354721069336,
            0.7766718864440918,
            2.2033190727233887,
            -0.9380595684051514,
            1.7654428482055664,
            0.4580008387565613,
            0.19308272004127502,
            2.8257880210876465,
            2.264751672744751,
            -2.564673662185669,
            2.5884766578674316,
            3.95590877532959,
            3.633729934692383,
            -0.579552412033081,
            3.3940906524658203,
            -1.6081573963165283,
            -0.23799759149551392,
            0.2665300965309143,
            2.8401966094970703,
            -2.5280497074127197,
            2.740466594696045,
            3.6567368507385254,
            -0.9855324029922485,
            -1.6570360660552979,
            -2.7471461296081543,
            3.517974376678467,
            -0.3135818839073181,
            1.4881280660629272,
            -4.833804607391357,
            -0.8183320760726929,
            -3.959724187850952,
            -1.1340166330337524,
            0.44690799713134766,
            1.899842619895935,
            -2.9789583683013916,
            -0.972135603427887,
            -0.2176308035850525,
            4.354274272918701,
            2.256171226501465,
            3.4575271606445312,
            0.562752366065979,
            -3.6371042728424072,
            -1.7978476285934448,
            -0.6944760680198669,
            0.567351222038269,
            1.5559072494506836,
            -0.47165465354919434,
            5.2256293296813965,
            -4.328920364379883,
            0.7855710387229919,
            -2.9936118125915527,
            0.6603407859802246,
            0.8303953409194946,
            -1.9647551774978638,
            -0.2620009183883667,
            -3.246103286743164,
            -0.8932747840881348,
            2.319645881652832,
            3.1730995178222656,
            -0.02513819932937622,
            2.0806074142456055,
            4.313248157501221,
            -0.5684664249420166,
            3.802903175354004,
            1.0705244541168213,
            3.5670738220214844,
            -0.017885852605104446,
            -3.54604172706604,
            2.6519601345062256,
            -0.7345308661460876,
            -0.2041912078857422,
            -1.3981000185012817,
            9.472460746765137,
            -2.03438401222229,
            -2.677644968032837,
            -6.317800521850586,
            -0.503184974193573,
            -2.778395652770996,
            -2.114439010620117,
            3.644538164138794,
            -1.5256717205047607,
            -1.6851544380187988,
            3.3705930709838867,
            -4.594155788421631,
            2.5787854194641113,
            -0.21804668009281158,
            0.3547142744064331,
            4.863680839538574,
            -3.694319725036621,
            4.380571365356445,
            -1.7629773616790771,
            1.2971065044403076,
            -4.5698747634887695,
            3.8040454387664795,
            0.059507742524147034,
            0.12356656789779663,
            0.3737820088863373,
            3.344834327697754,
            0.3599741458892822,
            0.2608071565628052,
            -6.487699508666992,
            -3.5566134452819824,
            -3.284329652786255,
            -3.669832706451416,
            1.7250685691833496,
            -2.5382657051086426,
            -1.5876505374908447,
            3.655442953109741,
            5.012771129608154,
            1.2839751243591309,
            -2.6798622608184814,
            -1.1202328205108643,
            3.0966851711273193,
            -3.0900745391845703,
            -2.1272354125976562,
            1.5035789012908936,
            -4.191754341125488,
            -3.1175696849823,
            -2.4635136127471924,
            -7.673346996307373,
            -1.4228256940841675,
            0.4065244495868683,
            2.8107757568359375,
            2.590831995010376,
            5.11581563949585,
            0.050908803939819336,
            -2.220865488052368,
            1.212035894393921,
            3.0616235733032227,
            4.497495174407959,
            -1.1994144916534424,
            -0.48883703351020813,
            1.7023037672042847,
            1.706421136856079,
            -2.275825023651123,
            3.3128762245178223,
            -1.7058141231536865,
            2.0988571643829346,
            -2.2475428581237793,
            -2.189054250717163,
            -1.4270904064178467,
            0.45173561573028564,
            2.697678565979004,
            2.84922194480896,
            0.6318219304084778,
            -2.1466000080108643,
            -1.004419207572937,
            2.666074514389038,
            -2.949831485748291,
            8.384909629821777,
            0.32397395372390747,
            -0.4903982877731323,
            -0.9404658079147339,
            3.8987011909484863,
            -1.1707109212875366,
            -2.0427815914154053,
            3.589684009552002,
            -4.112922668457031,
            -4.705568313598633,
            -1.4684691429138184,
            0.507195234298706,
            -0.35442066192626953,
            -3.373932361602783,
            -1.4296483993530273,
            -1.3682433366775513,
            1.4124795198440552,
            -4.525518417358398,
            5.017762184143066,
            -0.8387999534606934,
            0.24489960074424744,
            -1.5284390449523926,
            2.727022886276245,
            -2.384335994720459,
            0.4965791702270508,
            0.7678078413009644,
            -0.5442482233047485,
            1.2257212400436401,
            -2.43418550491333,
            -0.5119411945343018,
            -1.08820641040802,
            -2.7641634941101074,
            -0.08019766211509705,
            2.600388765335083,
            3.01838755607605,
            1.5389918088912964,
            -5.9182329177856445,
            -2.239893674850464,
            -0.7495362758636475,
            5.0309367179870605,
            -5.133127212524414,
            -2.4093246459960938,
            2.4437203407287598,
            3.813114643096924,
            4.503209114074707,
            2.7209744453430176,
            3.579805374145508,
            1.9184478521347046,
            3.1815390586853027,
            2.478933811187744,
            -0.04394535720348358,
            -1.1075048446655273,
            -0.5356895923614502,
            0.07548666000366211,
            2.981938362121582,
            1.233699083328247,
            0.06262075901031494,
            -0.4062270522117615,
            -1.5095558166503906,
            2.7820727825164795,
            0.21274276077747345,
            -2.1550230979919434,
            5.166825771331787,
            4.5308685302734375,
            1.6250569820404053,
            -2.356654644012451,
            -2.264296531677246,
            -3.0460586547851562,
            -2.0163164138793945,
            -4.723579406738281,
            0.9381855726242065,
            -2.5448999404907227,
            0.6990997791290283,
            -1.2841371297836304,
            -1.2125011682510376,
            0.5277934074401855,
            -0.42597585916519165,
            -0.8010571002960205,
            -1.8923012018203735,
            -1.1546704769134521,
            1.9290030002593994,
            -0.6554474830627441,
            3.7298202514648438,
            -5.3609466552734375,
            -0.2587122321128845,
            0.7127293944358826,
            5.576327323913574,
            5.603541374206543,
            1.5904452800750732,
            3.3279950618743896,
            -4.828322410583496,
            -1.077376127243042,
            -0.2670380473136902,
            3.596029281616211,
            3.8313980102539062,
            -2.5975284576416016,
            1.6280667781829834,
            -0.15063604712486267,
            -0.22249829769134521,
            -1.2895734310150146,
            1.375847339630127,
            2.5220582485198975,
            1.6444019079208374,
            -5.011916160583496,
            0.016932129859924316,
            -0.7275545597076416,
            1.8266839981079102,
            0.0901915431022644,
            -0.5353661179542542,
            1.4966869354248047,
            -1.2890257835388184,
            -4.870977878570557,
            1.6501988172531128,
            -0.5903566479682922,
            -0.5774011611938477,
            1.0126354694366455,
            3.044893741607666,
            -0.3396231234073639,
            1.362863302230835,
            2.410043954849243,
            3.4442880153656006,
            -3.738981246948242,
            -1.7937290668487549,
            2.1176323890686035,
            5.165393829345703,
            0.4306660294532776,
            -1.9784178733825684,
            0.3434864282608032,
            -2.201462984085083,
            -6.0567193031311035,
            4.293363571166992,
            -0.4012995958328247,
            2.7012295722961426,
            3.7244648933410645,
            1.7465391159057617,
            -2.36209774017334,
            -0.5060006976127625,
            -2.427905321121216,
            -4.209155559539795,
            -3.6141257286071777,
            -1.9323590993881226,
            -0.058184556663036346,
            -3.5839860439300537,
            -0.6559445858001709,
            2.1092844009399414,
            -4.231022834777832,
            2.3732714653015137,
            0.3157806396484375,
            1.6484551429748535,
            -1.6077518463134766,
            -3.7912116050720215,
            -0.777032196521759,
            -3.02193546295166,
            4.118409156799316,
            -2.6659188270568848,
            0.883916974067688,
            3.892867088317871,
            3.7321412563323975,
            3.996880054473877,
            5.825573921203613,
            2.397005319595337,
            -0.17773091793060303,
            -0.7989181280136108,
            5.394541263580322,
            0.4094676375389099,
            -3.271084785461426,
            2.3633651733398438,
            2.552865982055664,
            2.675358295440674,
            16.921144485473633,
            0.8994269371032715,
            -0.0005370974540710449,
            -1.8854167461395264,
            -2.06563138961792,
            -4.183104038238525,
            -1.304819107055664,
            0.5263110399246216,
            2.5561137199401855,
            1.9997308254241943,
            -1.8439927101135254,
            -4.584356784820557,
            0.37127596139907837,
            2.4609618186950684,
            -3.710930347442627,
            0.6904103755950928,
            -1.471073031425476,
            -0.3089551031589508,
            -4.345184326171875,
            -0.05297739803791046,
            -1.2995452880859375,
            0.4633629620075226,
            -4.334443092346191,
            -1.6581332683563232,
            -1.5945680141448975,
            3.679030656814575,
            1.7213743925094604,
            3.5428385734558105,
            -4.301367282867432,
            1.637365460395813,
            -0.9848064184188843,
            -0.022028475999832153,
            1.0954012870788574,
            2.352193593978882,
            -2.2835233211517334,
            2.2059378623962402,
            3.9490208625793457,
            -4.049412727355957,
            3.4784233570098877,
            2.570736885070801,
            -1.713871955871582,
            -0.41496002674102783,
            -4.934776306152344,
            0.3223573863506317,
            -1.1510379314422607,
            1.2268657684326172,
            0.6637454032897949,
            -1.8162455558776855,
            1.3454383611679077,
            6.233546257019043,
            2.9759750366210938,
            1.5253984928131104,
            -2.9309563636779785,
            0.4018848240375519,
            0.23024113476276398,
            0.20431271195411682,
            4.152612686157227,
            -4.5082855224609375,
            1.372398853302002,
            -2.0589187145233154,
            -0.9184532761573792,
            -1.0764045715332031,
            -1.5891506671905518,
            -4.380876541137695,
            -0.5198878049850464,
            -0.6944500803947449,
            -4.636857032775879,
            2.596126079559326,
            2.8904809951782227,
            0.8107824325561523,
            3.0759835243225098,
            -0.300481915473938,
            0.5308247208595276,
            -2.8383901119232178,
            -1.7925925254821777,
            -5.64472770690918,
            1.1238038539886475,
            -2.6449344158172607,
            1.186174988746643,
            4.307961463928223,
            -2.659987449645996,
            3.091885566711426,
            -2.9464845657348633,
            -2.169386148452759,
            2.7264370918273926,
            -3.3353545665740967,
            5.2431206703186035,
            0.8240110874176025,
            0.1408812403678894,
            4.161562919616699,
            1.4106374979019165,
            -1.0607329607009888,
            3.0503482818603516,
            4.135445594787598,
            0.227020263671875,
            -6.853719234466553,
            -4.564828872680664,
            -3.2722725868225098,
            -6.5280985832214355,
            -3.4540457725524902,
            4.074640274047852,
            2.912747383117676,
            3.9540343284606934,
            -0.930296778678894,
            -0.4284721612930298,
            0.4430125057697296,
            -3.36208438873291,
            -7.081043243408203,
            -5.435996055603027,
            -2.2875614166259766,
            4.093394756317139,
            -1.3591103553771973,
            -2.8551201820373535,
            -0.3975798785686493,
            1.5811079740524292,
            -0.9630179405212402,
            0.5401461720466614,
            3.1044344902038574,
            1.439438819885254,
            5.199985027313232,
            -0.5871967077255249,
            -1.9283556938171387,
            -0.6502929925918579,
            -2.9159250259399414,
            -1.5324196815490723,
            -2.0951223373413086,
            1.7993420362472534,
            -2.6580629348754883,
            -0.9324725866317749,
            -0.9477112293243408,
            1.1929506063461304,
            -1.9830037355422974,
            0.10945230722427368,
            -0.09571382403373718,
            -1.3696471452713013,
            0.6526319980621338,
            0.25695329904556274,
            3.440617322921753,
            -0.14350004494190216,
            2.351165533065796,
            -0.7361051440238953,
            -2.5909781455993652,
            -1.0498378276824951,
            8.30359935760498,
            0.9015269875526428,
            -0.14083677530288696,
            1.9516030550003052,
            -0.8504171371459961,
            -3.535337448120117,
            0.16470086574554443,
            1.3008484840393066,
            0.5642125606536865,
            0.47106310725212097,
            0.5265243053436279,
            -1.9059001207351685,
            -3.366882801055908
        ]
    },
    "authors": [
        {
            "authorId": "48506411",
            "name": "Xu Han"
        },
        {
            "authorId": "2148904862",
            "name": "Zhengyan Zhang"
        },
        {
            "authorId": "46649145",
            "name": "Ning Ding"
        },
        {
            "authorId": "2116405624",
            "name": "Yuxian Gu"
        },
        {
            "authorId": "2111312892",
            "name": "Xiao Liu"
        },
        {
            "authorId": "4140493",
            "name": "Yuqi Huo"
        },
        {
            "authorId": "40125294",
            "name": "J. Qiu"
        },
        {
            "authorId": "2146643767",
            "name": "Liang Zhang"
        },
        {
            "authorId": "2114924784",
            "name": "Wentao Han"
        },
        {
            "authorId": "1730108",
            "name": "Minlie Huang"
        },
        {
            "authorId": "2160884081",
            "name": "Qin Jin"
        },
        {
            "authorId": "37510256",
            "name": "Yanyan Lan"
        },
        {
            "authorId": "2152797548",
            "name": "Yang Liu"
        },
        {
            "authorId": "49293587",
            "name": "Zhiyuan Liu"
        },
        {
            "authorId": "1776220",
            "name": "Zhiwu Lu"
        },
        {
            "authorId": "1767521",
            "name": "Xipeng Qiu"
        },
        {
            "authorId": "35119829",
            "name": "Ruihua Song"
        },
        {
            "authorId": "2109541439",
            "name": "Jie Tang"
        },
        {
            "authorId": "153693432",
            "name": "Ji-rong Wen"
        },
        {
            "authorId": "2118572640",
            "name": "Jinhui Yuan"
        },
        {
            "authorId": "2542603",
            "name": "Wayne Xin Zhao"
        },
        {
            "authorId": "145254043",
            "name": "Jun Zhu"
        }
    ],
    "references": [
        {
            "paperId": "6f0aba8102d63938ce0b48ec23ff5ddd8110f2e8",
            "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification"
        },
        {
            "paperId": "8b954e1654c6b759a957fd11e66c111e6105fb3f",
            "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"
        },
        {
            "paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530",
            "title": "A Survey of Transformers"
        },
        {
            "paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
            "title": "CogView: Mastering Text-to-Image Generation via Transformers"
        },
        {
            "paperId": "da454295392cf4caaa39cc465734237ffe55392f",
            "title": "PTR: Prompt Tuning with Rules for Text Classification"
        },
        {
            "paperId": "fe9cd5bcca161289b0e3da0f49114dcccf62eaa1",
            "title": "Token-Aware Virtual Adversarial Training in Natural Language Understanding"
        },
        {
            "paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f",
            "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"
        },
        {
            "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
        },
        {
            "paperId": "72dd63d67588a42fc817bbb8d655b397f67425df",
            "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"
        },
        {
            "paperId": "238eb420c472bfdb1b4d34f9f53abec51f307a6b",
            "title": "FastMoE: A Fast Mixture-of-Expert Training System"
        },
        {
            "paperId": "5d8e85d12c218f56f3c50351fa2ddc3da098b31c",
            "title": "Prototypical Representation Learning for Relation Extraction"
        },
        {
            "paperId": "e2af3fc33a80b86beed4d38f6247a8ca6f883e25",
            "title": "Controllable Generation from Pre-trained Language Models via Inverse Prompting"
        },
        {
            "paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17",
            "title": "GPT Understands, Too"
        },
        {
            "paperId": "f7db2e2b181df60be90fd602b1aa306e4ca8b342",
            "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training"
        },
        {
            "paperId": "9ed25f101f19ea735ca300848948ed64064b97ca",
            "title": "Random Feature Attention"
        },
        {
            "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "title": "Learning Transferable Visual Models From Natural Language Supervision"
        },
        {
            "paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
            "title": "Zero-Shot Text-to-Image Generation"
        },
        {
            "paperId": "040ad14a2c97e51510889ae6a0c3c23b29da801d",
            "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models"
        },
        {
            "paperId": "33ce3cd897a3473973f338c154f3fe5c1175643c",
            "title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations"
        },
        {
            "paperId": "12b71736392209b4292471b7da0aed71ba2aa545",
            "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"
        },
        {
            "paperId": "77423d2370ad0377559fa08c166520267d4ace6f",
            "title": "Red Alarm for Pre-trained Models: Universal Vulnerabilities by Neuron-Level Backdoor Attacks"
        },
        {
            "paperId": "fdacf2a732f55befdc410ea927091cad3b791f13",
            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
        },
        {
            "paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
            "title": "Making Pre-trained Language Models Better Few-shot Learners"
        },
        {
            "paperId": "66d8573a7b63055c58c3a8321062d87078fb5fce",
            "title": "ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora"
        },
        {
            "paperId": "25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b",
            "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning"
        },
        {
            "paperId": "a1a6192eaea76170a78d6b219e445ef4f876be9b",
            "title": "Generating Adversarial Examples in Chinese Texts Using Sentence-Pieces"
        },
        {
            "paperId": "d4aa7141e00f5cd6cf560cfea2df63352f21d437",
            "title": "Towards a Universal Continuous Knowledge Base"
        },
        {
            "paperId": "cc50f846ed7222698d130cddbc58ed4d547914ed",
            "title": "CPM: A Large-scale Generative Chinese Pre-trained Language Model"
        },
        {
            "paperId": "0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d",
            "title": "Exploring Simple Siamese Representation Learning"
        },
        {
            "paperId": "02806b916d2c341d5a5ac7dc3c19e3f2363d402f",
            "title": "Know What You Don't Need: Single-Shot Meta-Pruning for Attention Heads"
        },
        {
            "paperId": "2310d893abf4ec900cb9e0c5da58284a37329780",
            "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"
        },
        {
            "paperId": "d5ed0d819bece4c08c2131bd9b8b4f9c40223ea1",
            "title": "Further Analysis of Outlier Detection with Deep Generative Models"
        },
        {
            "paperId": "3ee955bfb656e30a337b22a1149b5ecc91a91217",
            "title": "Language Models are Open Knowledge Graphs"
        },
        {
            "paperId": "7eda139d737eea10fc1d95364327a41ec0cee4a4",
            "title": "CoLAKE: Contextualized Language and Knowledge Embedding"
        },
        {
            "paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
            "title": "Rethinking Attention with Performers"
        },
        {
            "paperId": "097210dc65924f8ce59523faf444e635523dc714",
            "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"
        },
        {
            "paperId": "6b989b8327db3a7212141c59c1569f0219775058",
            "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks"
        },
        {
            "paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9",
            "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"
        },
        {
            "paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
            "title": "Efficient Transformers: A Survey"
        },
        {
            "paperId": "8656e5a885613f29f0f2f35589d865baeb1317a6",
            "title": "Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation"
        },
        {
            "paperId": "0c1cb3ce072e0b593ddce096be5520a05e65c75e",
            "title": "Variance-reduced Language Pretraining via a Mask Proposal Network"
        },
        {
            "paperId": "f76f3bbc3072394c5c3ed9e481861696ff970873",
            "title": "A Large-Scale Chinese Short-Text Conversation Dataset"
        },
        {
            "paperId": "311909621177c397c6b7099beff32332124f7d46",
            "title": "On Learning Universal Representations Across Languages"
        },
        {
            "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
            "title": "Big Bird: Transformers for Longer Sequences"
        },
        {
            "paperId": "4ceff7472c04ee6d76bce89d61ba4b445d8dbf74",
            "title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"
        },
        {
            "paperId": "0e012c2bd18236445cfbc6e3e409eb02df4691fe",
            "title": "Can neural networks acquire a structural bias from raw linguistic data?"
        },
        {
            "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
            "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
        },
        {
            "paperId": "bcbac71ac64cd6a6aaae41e37ebe960f508ab741",
            "title": "Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge"
        },
        {
            "paperId": "9b1933038680b13c06b60dfe810e96a3a0ef9d37",
            "title": "Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation"
        },
        {
            "paperId": "1882f194cb43828852cc052887671e55a80f945a",
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
        },
        {
            "paperId": "70af4173983eccc0beac29ed4602bf9db5568b92",
            "title": "PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning"
        },
        {
            "paperId": "6f68e1bb253925d8431588555d3010419f322e04",
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
        },
        {
            "paperId": "8668fd1cb5cab820f8b2a136b2ef4adfad6c4dc1",
            "title": "Knowledge-Aware Language Model Pretraining"
        },
        {
            "paperId": "2a81f6bf76bcb70244aa40217ff316025971bd0f",
            "title": "Graph Optimal Transport for Cross-Domain Alignment"
        },
        {
            "paperId": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e",
            "title": "Self-Supervised Learning: Generative or Contrastive"
        },
        {
            "paperId": "368c72c2298e5f8276398b2cb198702281eac4f8",
            "title": "Rethinking Pre-training and Self-training"
        },
        {
            "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
            "title": "Linformer: Self-Attention with Linear Complexity"
        },
        {
            "paperId": "dfb93b3072bc7d4fb3a53072fc04e28f057b1d4e",
            "title": "M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training"
        },
        {
            "paperId": "04ef54bd467d5e03dee7b0be601cf06d420bffa0",
            "title": "Emergent linguistic structure in artificial neural networks trained by self-supervision"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "title": "End-to-End Object Detection with Transformers"
        },
        {
            "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
        },
        {
            "paperId": "91ac65431b2dc46919e1673fde67671c29446812",
            "title": "When BERT Plays the Lottery, All Tickets Are Winning"
        },
        {
            "paperId": "cdf766403e365643ac4dfdf9e10df8da1b75b63f",
            "title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT"
        },
        {
            "paperId": "9b539d413393047b28bb7be9b195f142aaf7a80e",
            "title": "Recipes for Building an Open-Domain Chatbot"
        },
        {
            "paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271",
            "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"
        },
        {
            "paperId": "f670f8e0ba95a0a39e7e7e1d08f6e839fc4b1093",
            "title": "Train No Evil: Selective Masking for Task-guided Pre-training"
        },
        {
            "paperId": "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
            "title": "MPNet: Masked and Permuted Pre-training for Language Understanding"
        },
        {
            "paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d",
            "title": "ETC: Encoding Long and Structured Inputs in Transformers"
        },
        {
            "paperId": "016368185723d0ec99aafa4b5927300590d0647f",
            "title": "Entities as Experts: Sparse Memory Access with Entity Supervision"
        },
        {
            "paperId": "1c8aea2bfb61f4661b6907018a5a8bca390900dd",
            "title": "PALM: Pre-training an Autoencoding&autoregressive Language Model for Context-conditioned Generation"
        },
        {
            "paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0",
            "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"
        },
        {
            "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
            "title": "Longformer: The Long-Document Transformer"
        },
        {
            "paperId": "10467a1466aeec246ac0a577bfc311ec4de110de",
            "title": "Alternating Language Modeling for Cross-Lingual Pre-Training"
        },
        {
            "paperId": "dc0ce66f5ab4c5173cdef951649044e4c4c05076",
            "title": "BERT-ATTACK: Adversarial Attack against BERT Using BERT"
        },
        {
            "paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a",
            "title": "Pre-trained models for natural language processing: A survey"
        },
        {
            "paperId": "657329c633709dd1ac34a30d57341b186b1a47c2",
            "title": "Efficient Content-Based Sparse Attention with Routing Transformers"
        },
        {
            "paperId": "7cb7b80e50fd5418b7f47326e60ce32c9a0f6b38",
            "title": "SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via Smart Swapping"
        },
        {
            "paperId": "1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0",
            "title": "XGPT: Cross-modal Generative Pre-Training for Image Captioning"
        },
        {
            "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
            "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"
        },
        {
            "paperId": "a9e6222e71dd101d444b7192b3a0636c71edb0a4",
            "title": "Differentiable Reasoning over a Virtual Knowledge Base"
        },
        {
            "paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d",
            "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"
        },
        {
            "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
            "title": "A Simple Framework for Contrastive Learning of Visual Representations"
        },
        {
            "paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56",
            "title": "REALM: Retrieval-Augmented Language Model Pre-Training"
        },
        {
            "paperId": "80376bdec5f534be78ba82821f540590ebce5559",
            "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"
        },
        {
            "paperId": "0bce17fdfd4872b16c97a89516a24a092cdc3616",
            "title": "Blank Language Models"
        },
        {
            "paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86",
            "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"
        },
        {
            "paperId": "50be7d2858523d0e63174d974f380349fca0d666",
            "title": "Parsing as Pretraining"
        },
        {
            "paperId": "7cf8510d5905bd8a63f1e098e05ab591d689e0fd",
            "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction"
        },
        {
            "paperId": "d08463bd665589d04619f04dbde84183ffcf2e63",
            "title": "Towards a Human-like Open-Domain Chatbot"
        },
        {
            "paperId": "f18fa3728868af6c44bb1dc3e913925abc37b5c1",
            "title": "Further Boosting BERT-based Models by Duplicating Existing Layers: Some Intriguing Phenomena inside BERT"
        },
        {
            "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
            "title": "Scaling Laws for Neural Language Models"
        },
        {
            "paperId": "a9fd5511b42206a27748f373e0fdb7eb76a23055",
            "title": "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data"
        },
        {
            "paperId": "495da6f19baa09c6db3697d839e10432cdc25934",
            "title": "Multilingual Denoising Pre-training for Neural Machine Translation"
        },
        {
            "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "title": "Reformer: The Efficient Transformer"
        },
        {
            "paperId": "c6a84615bc36486cd0170f8a3e1b7e5ec8f5344e",
            "title": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation"
        },
        {
            "paperId": "c7fc1cac162c0e2a934704184c7554fd6b6253f0",
            "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model"
        },
        {
            "paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"
        },
        {
            "paperId": "7931e3d484b365fae76819967a8fe519d5f85ee6",
            "title": "Conditional"
        },
        {
            "paperId": "9915315f5cae822e98c94382ce3b0a6f9a7f8e5e",
            "title": "12-in-1: Multi-Task Vision and Language Representation Learning"
        },
        {
            "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        },
        {
            "paperId": "a75649771901a4881b44c0ceafa469fcc6e6f968",
            "title": "How Can We Know What Language Models Know?"
        },
        {
            "paperId": "f67fcbb1aec92ae293998ddfd904f61a31bef334",
            "title": "Inducing Relational Knowledge from BERT"
        },
        {
            "paperId": "01f2b214962997260020279bd1fd1f8f372249d4",
            "title": "Evaluating Commonsense in Pre-trained Language Models"
        },
        {
            "paperId": "ba8215e77f35b0d947c7cec39c45df4516e93421",
            "title": "Do Attention Heads in BERT Track Syntactic Dependencies?"
        },
        {
            "paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
            "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"
        },
        {
            "paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc",
            "title": "Momentum Contrast for Unsupervised Visual Representation Learning"
        },
        {
            "paperId": "317d2ac530e1db49229d6c442f50722db85afbb7",
            "title": "Understanding Multi-Head Attention in Abstractive Summarization"
        },
        {
            "paperId": "2bd5b4aed18400bf1a1cc866d9b8d931aa047290",
            "title": "E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"
        },
        {
            "paperId": "348be8e64565a3df80d743f0580b63d3fbb49f35",
            "title": "SentiLARE: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis"
        },
        {
            "paperId": "8a9a798c56fc83858d7ace0352606d73aeaa204d",
            "title": "Adversarial Language Games for Advanced Natural Language Intelligence"
        },
        {
            "paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
            "title": "Unsupervised Cross-lingual Representation Learning at Scale"
        },
        {
            "paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d",
            "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"
        },
        {
            "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
            "paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8",
            "title": "PipeDream: generalized pipeline parallelism for DNN training"
        },
        {
            "paperId": "76c929af6735cdff2c4badc9a9c8f39d15ea3e70",
            "title": "A generic communication scheduler for distributed DNN training acceleration"
        },
        {
            "paperId": "f1e8a99a7e17d449559b26ee0db2e8f1f47ce7ad",
            "title": "SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "b85d339e49399966d629973c889e8edfca56517c",
            "title": "A Mutual Information Maximization Perspective of Language Representation Learning"
        },
        {
            "paperId": "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
            "title": "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3",
            "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"
        },
        {
            "paperId": "df92434acb9d9cc77478259a33eccf04144e57ac",
            "title": "Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "5f134bfc7131ef5494cb15cb45971489af3d1fbe",
            "title": "Project"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290",
            "title": "UNITER: UNiversal Image-TExt Representation Learning"
        },
        {
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout"
        },
        {
            "paperId": "6648b4db5f12c30941ea78c695e77aded19672bb",
            "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"
        },
        {
            "paperId": "aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47",
            "title": "Cross-Lingual Natural Language Generation via Pre-Training"
        },
        {
            "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding"
        },
        {
            "paperId": "06a73ad09664435f8b3cd90293f4e05a047cf375",
            "title": "K-BERT: Enabling Language Representation with Knowledge Graph"
        },
        {
            "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
        },
        {
            "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
            "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09",
            "title": "Knowledge Enhanced Contextual Word Representations"
        },
        {
            "paperId": "65f788fb964901e3f1149a0a53317535ca85ed7d",
            "title": "Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"
        },
        {
            "paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39",
            "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"
        },
        {
            "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
            "title": "Language Models as Knowledge Bases?"
        },
        {
            "paperId": "f98e135986414cccf29aec593d547c0656e4d82c",
            "title": "Commonsense Knowledge Mining from Pretrained Models"
        },
        {
            "paperId": "6c1beae31b92c70b42ebeb99e5598d73bff6eea5",
            "title": "NEZHA: Neural Contextualized Representation for Chinese Language Understanding"
        },
        {
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression"
        },
        {
            "paperId": "4aa6298b606941a282d735fa3143da293199d2ca",
            "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"
        },
        {
            "paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103",
            "title": "Revealing the Dark Secrets of BERT"
        },
        {
            "paperId": "79c93274429d6355959f1e4374c2147bb81ea649",
            "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"
        },
        {
            "paperId": "3caf34532597683c980134579b156cd0d7db2f40",
            "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP"
        },
        {
            "paperId": "2bc1c8bd00bbf7401afcb5460277840fd8bab029",
            "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"
        },
        {
            "paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
            "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"
        },
        {
            "paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
            "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"
        },
        {
            "paperId": "356645552f8f40adf5a99b4e3a69f47699399010",
            "title": "Quantity doesn\u2019t buy quality syntax with neural language models"
        },
        {
            "paperId": "b82153bf85d5d1edd3f170aace830e5328ca9ed0",
            "title": "Fusion of Detected Objects in Text for Visual Question Answering"
        },
        {
            "paperId": "cc02386375b1262c3a1d5525154eaea24c761d15",
            "title": "Do Neural Language Representations Learn Physical Commonsense?"
        },
        {
            "paperId": "a0e49f65b6847437f262c59d0d399255101d0b75",
            "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"
        },
        {
            "paperId": "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
            "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"
        },
        {
            "paperId": "335613303ebc5eac98de757ed02a56377d99e03a",
            "title": "What Does BERT Learn about the Structure of Language?"
        },
        {
            "paperId": "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2",
            "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "64e87487a87532778352a358209935fef48300d7",
            "title": "OAG: Toward Linking Large-scale Heterogeneous Entity Graphs"
        },
        {
            "paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc",
            "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"
        },
        {
            "paperId": "f3b89e9a2b8ce1b6058e6984c3556bc2dded0938",
            "title": "Probing Neural Network Comprehension of Natural Language Arguments"
        },
        {
            "paperId": "adce96bdd3f3093a90933f288106b67f20e094f6",
            "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks"
        },
        {
            "paperId": "bf442ab269074665a68e4dbbe19e4efc97862541",
            "title": "Large Memory Layers with Product Keys"
        },
        {
            "paperId": "71f551f0352b91ab4725c498c68610655d3b5578",
            "title": "Inducing Syntactic Trees from BERT Representations"
        },
        {
            "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
            "paperId": "2ff41a463a374b138bb5a012e5a32bc4beefec20",
            "title": "Pre-Training with Whole Word Masking for Chinese BERT"
        },
        {
            "paperId": "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
            "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction"
        },
        {
            "paperId": "95a251513853c6032bdecebd4b74e15795662986",
            "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"
        },
        {
            "paperId": "4af09143735210777281b66997ec12994dbb43d4",
            "title": "Matching the Blanks: Distributional Similarity for Relation Learning"
        },
        {
            "paperId": "165d51a547cd920e6ac55660ad5c404dcb9562ed",
            "title": "Open Sesame: Getting inside BERT\u2019s Linguistic Knowledge"
        },
        {
            "paperId": "809cc93921e4698bde891475254ad6dfba33d03b",
            "title": "How Multilingual is Multilingual BERT?"
        },
        {
            "paperId": "455a8838cde44f288d456d01c76ede95b56dc675",
            "title": "A Structural Probe for Finding Syntax in Word Representations"
        },
        {
            "paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
            "title": "Efficient Training of BERT by Progressively Stacking"
        },
        {
            "paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583",
            "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
        },
        {
            "paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde",
            "title": "ERNIE: Enhanced Language Representation with Informative Entities"
        },
        {
            "paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
            "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"
        },
        {
            "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
            "title": "BERT Rediscovers the Classical NLP Pipeline"
        },
        {
            "paperId": "5728919676a85553b3c3063626c220fe7a5634e4",
            "title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale"
        },
        {
            "paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88",
            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"
        },
        {
            "paperId": "145b8b5d99a2beba6029418ca043585b90138d12",
            "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"
        },
        {
            "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
            "title": "Are Sixteen Heads Really Better than One?"
        },
        {
            "paperId": "21da617a0f79aabf94272107184606cefe90ab75",
            "title": "Generating Long Sequences with Sparse Transformers"
        },
        {
            "paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
            "title": "ERNIE: Enhanced Representation through Knowledge Integration"
        },
        {
            "paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a",
            "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"
        },
        {
            "paperId": "bc789aef715498e79a74f857fa090ece9e383bf1",
            "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"
        },
        {
            "paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028",
            "title": "SciBERT: A Pretrained Language Model for Scientific Text"
        },
        {
            "paperId": "f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
            "title": "Linguistic Knowledge and Transferability of Contextual Representations"
        },
        {
            "paperId": "403227333329b36183004f04db72362b604adef3",
            "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning"
        },
        {
            "paperId": "2a31319e73d4486716168b65cdf7559baeda18ce",
            "title": "Star-Transformer"
        },
        {
            "paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c",
            "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"
        },
        {
            "paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"
        },
        {
            "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
            "title": "Cross-lingual Language Model Pretraining"
        },
        {
            "paperId": "efeab0dcdb4c1cce5e537e57745d84774be99b9a",
            "title": "Assessing BERT's Syntactic Abilities"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "f86f1748d1b6d22870f4347fd5d65314ba800583",
            "title": "Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off"
        },
        {
            "paperId": "4152d2c8585f7e3f85d3b3d84036171de104cbd7",
            "title": "Rethinking ImageNet Pre-Training"
        },
        {
            "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"
        },
        {
            "paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f",
            "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"
        },
        {
            "paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5",
            "title": "XNLI: Evaluating Cross-lingual Sentence Representations"
        },
        {
            "paperId": "634c083444e11c89f30c93a2986cb43db35ca304",
            "title": "Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering"
        },
        {
            "paperId": "7dd7198bb8a61dd22879068e8c4619b32f0470f8",
            "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning"
        },
        {
            "paperId": "f971658ab845d7573c4bbb760d5e7e5332025254",
            "title": "Beyond Data and Model Parallelism for Deep Neural Networks"
        },
        {
            "paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0",
            "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"
        },
        {
            "paperId": "155b7782dbd713982a4133df3aee7adfd0b6b304",
            "title": "Unsupervised Feature Learning via Non-parametric Instance Discrimination"
        },
        {
            "paperId": "c41516420ddbd0f29e010ca259a74c1fc2da0466",
            "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties"
        },
        {
            "paperId": "804fb9542f4f56e264dd2df57c255a9a2011c00f",
            "title": "Adversarially Robust Generalization Requires More Data"
        },
        {
            "paperId": "2229ac756f89c3db017293918548555734d2f891",
            "title": "TicTac: Accelerating Distributed Deep Learning with Communication Scheduling"
        },
        {
            "paperId": "d8c09661b1bebfb690f0566167c87d64c5628d73",
            "title": "Demystifying Parallel and Distributed Deep Learning"
        },
        {
            "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "title": "Deep Contextualized Word Representations"
        },
        {
            "paperId": "f6a4bf043af1a9ec7f104a7b7ab56806b241ceda",
            "title": "Model compression via distillation and quantization"
        },
        {
            "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "title": "Universal Language Model Fine-tuning for Text Classification"
        },
        {
            "paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
            "title": "Graph Attention Networks"
        },
        {
            "paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
            "title": "Mixed Precision Training"
        },
        {
            "paperId": "03928c2fa119c8f52cf28beba39459786091a06a",
            "title": "ZhuSuan: A Library for Bayesian Deep Learning"
        },
        {
            "paperId": "e6e64043c66b83a8787a69a70d7c0a85fd9d23c3",
            "title": "Scaling SGD Batch Size to 32K for ImageNet Training"
        },
        {
            "paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7",
            "title": "Learned in Translation: Contextualized Word Vectors"
        },
        {
            "paperId": "acd87843a451d18b4dc6474ddce1ae946429eaf1",
            "title": "Wasserstein Generative Adversarial Networks"
        },
        {
            "paperId": "231af7dc01a166cac3b5b01ca05778238f796e41",
            "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5",
            "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"
        },
        {
            "paperId": "77d30cf9a34fb6b50979c6a68863099da9a060ad",
            "title": "Residual Attention Network for Image Classification"
        },
        {
            "paperId": "cd8a9914d50b0ac63315872530274d158d6aff09",
            "title": "Modeling Relational Data with Graph Convolutional Networks"
        },
        {
            "paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586",
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
        },
        {
            "paperId": "54ddb00fa691728944fd8becea90a373d21597cf",
            "title": "Understanding deep learning requires rethinking generalization"
        },
        {
            "paperId": "d821ce08da6c0084d5eacbdf65e25556bc1b9bc3",
            "title": "Does String-Based Neural MT Learn Source Syntax?"
        },
        {
            "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
            "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
        },
        {
            "paperId": "36eff562f65125511b5dfab68ce7f7a943c27478",
            "title": "Semi-Supervised Classification with Graph Convolutional Networks"
        },
        {
            "paperId": "e44da7d8c71edcc6e575fa7faadd5e75785a7901",
            "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks"
        },
        {
            "paperId": "53d43ccc593bf44e9aa52e3971df1b9dd396e30d",
            "title": "Probing for semantic evidence of composition by means of simple classification tasks"
        },
        {
            "paperId": "59761abc736397539bdd01ad7f9d91c8607c0457",
            "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "4954fa180728932959997a4768411ff9136aac81",
            "title": "TensorFlow: A system for large-scale machine learning"
        },
        {
            "paperId": "e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b",
            "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning"
        },
        {
            "paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654",
            "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"
        },
        {
            "paperId": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194",
            "title": "Dynamic Memory Networks for Visual and Textual Question Answering"
        },
        {
            "paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
            "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "d7ce5665a72c0b607f484c1b448875f02ddfac3b",
            "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning"
        },
        {
            "paperId": "d15e9a0aa68aeb1cdb8429d9db0d343178f2d377",
            "title": "What\u2019s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation"
        },
        {
            "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
            "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
        },
        {
            "paperId": "dbb6ded623159c867fbeca0772db7b2eb9489523",
            "title": "Spatial Transformer Networks"
        },
        {
            "paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
        },
        {
            "paperId": "2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1",
            "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question"
        },
        {
            "paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6",
            "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"
        },
        {
            "paperId": "020928259a848b91b7c36062e4bbc6df6398c02e",
            "title": "Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model"
        },
        {
            "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
            "title": "VQA: Visual Question Answering"
        },
        {
            "paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628",
            "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
            "title": "Conditional Random Fields as Recurrent Neural Networks"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "b7cf49e30355633af2db19f35189410c8515e91f",
            "title": "Deep Learning with Limited Numerical Precision"
        },
        {
            "paperId": "f01fc808592ea7c473a69a6e7484040a435f36d9",
            "title": "Long-term recurrent convolutional networks for visual recognition and description"
        },
        {
            "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "title": "Show and tell: A neural image caption generator"
        },
        {
            "paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
            "title": "Fully convolutional networks for semantic segmentation"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd",
            "title": "Deeply-Supervised Nets"
        },
        {
            "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "title": "Going deeper with convolutions"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
            "paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc",
            "title": "Findings of the 2014 Workshop on Statistical Machine Translation"
        },
        {
            "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "title": "Microsoft COCO: Common Objects in Context"
        },
        {
            "paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
            "title": "A Convolutional Neural Network for Modelling Sentences"
        },
        {
            "paperId": "1109b663453e78a59e4f66446d71720ac58cec25",
            "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
        },
        {
            "paperId": "99c970348b8f70ce23d6641e201904ea49266b6e",
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
            "title": "Linguistic Regularities in Continuous Space Word Representations"
        },
        {
            "paperId": "92b79eab68909c885306a10ffeb31c0b742aff92",
            "title": "\u201cOSCAR\u201d"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
            "paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4",
            "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"
        },
        {
            "paperId": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972",
            "title": "A Survey on Transfer Learning"
        },
        {
            "paperId": "8492070dc4031ed825e95e4803781752bb5e909f",
            "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning"
        },
        {
            "paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
            "title": "Why Does Unsupervised Pre-training Help Deep Learning?"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "f9f4693cef7634badcf31d51be8085758d7cf6f2",
            "title": "Aleatory or epistemic? Does it matter?"
        },
        {
            "paperId": "ffc5a9610df0341369aa75c0331ef021de0a02a9",
            "title": "Transferred Dimensionality Reduction"
        },
        {
            "paperId": "cebbd6e489d03410ff178ffe2efce3451ea28790",
            "title": "Knowledge transfer via multiple model local structure mapping"
        },
        {
            "paperId": "c6d014a8528e6476616936b983886b3fcde5a762",
            "title": "Self-taught clustering"
        },
        {
            "paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"
        },
        {
            "paperId": "10d10df314c1b58f5c83629e73a35185876cd4e2",
            "title": "Multi-task Gaussian Process Prediction"
        },
        {
            "paperId": "33c83eb6ccdf6e34f1431e5aa9df6ae5646ae919",
            "title": "Co-clustering based classification for out-of-domain documents"
        },
        {
            "paperId": "0fbf373d25f2710e2e4eaf64e00e071f734ad84f",
            "title": "Mapping and Revising Markov Logic Networks for Transfer Learning"
        },
        {
            "paperId": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "title": "Self-taught learning: transfer learning from unlabeled data"
        },
        {
            "paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "title": "A Fast Learning Algorithm for Deep Belief Nets"
        },
        {
            "paperId": "47f5682448cdc0b650b54e7f59d22d72f4976c2d",
            "title": "Domain Adaptation for Statistical Classifiers"
        },
        {
            "paperId": "214d59739d221230aac4d9eeb8cd14131f2f815d",
            "title": "Domain adaptation for statistical classifiers"
        },
        {
            "paperId": "b4299baa815ca5a815a70fba94a9f6f2b42fff19",
            "title": "A High-Performance Semi-Supervised Learning Method for Text Chunking"
        },
        {
            "paperId": "e219a61354d972a28954e655a7c53373508a08b6",
            "title": "Regularized multi--task learning"
        },
        {
            "paperId": "73e1c4a1152a75ec7310adfb4b8daea16d627bc7",
            "title": "Learning to learn with the informative vector machine"
        },
        {
            "paperId": "6d5965a76f88a8ebab4fc9c43a3ae2630628966a",
            "title": "Learning and evaluating classifiers under sample selection bias"
        },
        {
            "paperId": "86c97493c2c7245afefd0336824eb48669923ddd",
            "title": "Time constraints and resource sharing in adults' working memory spans."
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "235723a15c86c369c99a42e7b666dfe156ad2cba",
            "title": "Improving predictive inference under covariate shift by weighting the log-likelihood function"
        },
        {
            "paperId": "3e06c979b01b1c235017495d7d3a2769bb6a81bc",
            "title": "Learning to Learn: Introduction and Overview"
        },
        {
            "paperId": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "title": "Learning long-term dependencies with gradient descent is difficult"
        },
        {
            "paperId": "2a6e82a9b3752b737887fc6fcf2b7583441711c9",
            "title": "Below the Surface: Analogical Similarity and Retrieval Competition in Reminding"
        },
        {
            "paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56",
            "title": "Adaptive Mixtures of Local Experts"
        },
        {
            "paperId": "35a2b204ee6da98232674728e82f7fedd9f46931",
            "title": "Working memory"
        },
        {
            "paperId": "62edcc7d7f76dc9b12c9ed6f6864ba1045577452",
            "title": "Some Tests of the Decay Theory of Immediate Memory"
        },
        {
            "paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd",
            "title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": "da5d78b3e3a1544fde98fba86088e1215e97cbe8",
            "title": "All NLP Tasks Are Generation Tasks: A General Pretraining Framework"
        },
        {
            "paperId": "50068fbea4d1cafcf4c99873ab272c701c08dfcb",
            "title": "OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models"
        },
        {
            "paperId": "bbca889e21fcc71f6808eceeaf4833c9d594a078",
            "title": "Reproducibility Report: Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness"
        },
        {
            "paperId": "77e73174e606c0820a52a940088832b32d9a033e",
            "title": "Efficient Large-Scale Language Model Training on GPU Clusters"
        },
        {
            "paperId": null,
            "title": "OneFlow Deep Learning"
        },
        {
            "paperId": "a5fa6e7565dca654eab9372ace4b1ba7f63655f7",
            "title": "CogLTX: Applying BERT to Long Texts"
        },
        {
            "paperId": "08588107b9e37f9601bb5c801aa46b918cc3c8ec",
            "title": "A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters"
        },
        {
            "paperId": "6803804dea9133c5aa6e496acc84c201992c7e93",
            "title": "Merlin: A GPU Accelerated Recommendation Framework"
        },
        {
            "paperId": "9c9fafc3105325428fe6f6ef58709be433510b2f",
            "title": "Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning"
        },
        {
            "paperId": null,
            "title": "Cross-Modal"
        },
        {
            "paperId": null,
            "title": "Electra: Pre-training text encoders as discriminators rather than generators"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
            "title": "Efficient BackProp"
        },
        {
            "paperId": null,
            "title": "Multi-task feature learning"
        },
        {
            "paperId": "43e490bd52b8f9d25934a82dd20e42917d0a16e4",
            "title": "Word-level Textual Adversarial Attacking as Combinatorial Optimization"
        },
        {
            "paperId": null,
            "title": "MindSpore Deep Learning Framework"
        },
        {
            "paperId": null,
            "title": "Autoprompt : Eliciting knowledge from language models with automatically generated prompts"
        },
        {
            "paperId": null,
            "title": "Pritam Damania, and Soumith Chintala. 2020d. Pytorch distributed: Experiences on accelerating data parallel training"
        }
    ]
}