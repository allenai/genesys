{
    "paperId": "da398dd57a31a663eb572ec85db7cdfe2f70dc99",
    "externalIds": {
        "ArXiv": "1602.03032",
        "DBLP": "conf/icml/DanihelkaWUKG16",
        "MAG": "2265796482",
        "CorpusId": 8239112
    },
    "title": "Associative Long Short-Term Memory",
    "abstract": "We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.",
    "venue": "International Conference on Machine Learning",
    "year": 2016,
    "referenceCount": 20,
    "citationCount": 167,
    "influentialCitationCount": 17,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work investigates a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters, which creates redundant copies of stored information, which enables retrieval with reduced noise."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1841008",
            "name": "Ivo Danihelka"
        },
        {
            "authorId": "89504302",
            "name": "Greg Wayne"
        },
        {
            "authorId": "2825051",
            "name": "Benigno Uria"
        },
        {
            "authorId": "2583391",
            "name": "Nal Kalchbrenner"
        },
        {
            "authorId": "1753223",
            "name": "Alex Graves"
        }
    ],
    "references": [
        {
            "paperId": "e9c771197a6564762754e48c1daafb066f449f2e",
            "title": "Unitary Evolution Recurrent Neural Networks"
        },
        {
            "paperId": "5b8364c21155d3d2cd38ea4c8b8580beba9a3250",
            "title": "An Empirical Exploration of Recurrent Network Architectures"
        },
        {
            "paperId": "e837b79de602c69395498c1fbbe39bbb4e6f75ad",
            "title": "Learning to Transduce with Unbounded Memory"
        },
        {
            "paperId": "c9d7afd65b2127e6f0651bc7e13eeec1897ac8dd",
            "title": "Reinforcement Learning Neural Turing Machines"
        },
        {
            "paperId": "5259755f9c100e220ffaa7e08439c5d34be7757a",
            "title": "Reinforcement Learning Neural Turing Machines - Revised"
        },
        {
            "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "title": "End-To-End Memory Networks"
        },
        {
            "paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82",
            "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de",
            "title": "Neural Turing Machines"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
            "title": "Generating Sequences With Recurrent Neural Networks"
        },
        {
            "paperId": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "title": "Speech recognition with deep recurrent neural networks"
        },
        {
            "paperId": "93c20e38c85b69fc2d2eb314b3c1217913f7db11",
            "title": "Generating Text with Recurrent Neural Networks"
        },
        {
            "paperId": "425931e434f6b370cc6cdd2db58873843def7d7f",
            "title": "Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors"
        },
        {
            "paperId": "c3577312cb178cc93459bda92e37076e1fa9af88",
            "title": "Holographic Reduced Representation: Distributed Representation for Cognitive Structures"
        },
        {
            "paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "title": "Learning to Forget: Continual Prediction with LSTM"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "2ae5a5507253aa3cada113d41d35fada1e84555f",
            "title": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
        },
        {
            "paperId": null,
            "title": "The human knowledge compression contest"
        }
    ]
}