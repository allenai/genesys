{
    "paperId": "424aef7340ee618132cc3314669400e23ad910ba",
    "externalIds": {
        "MAG": "2951403397",
        "DBLP": "conf/iclr/InanKS17",
        "ArXiv": "1611.01462",
        "CorpusId": 7443908
    },
    "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
    "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",
    "venue": "International Conference on Learning Representations",
    "year": 2016,
    "referenceCount": 36,
    "citationCount": 375,
    "influentialCitationCount": 32,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a novel theoretical framework that facilitates better learning in language modeling, and shows that this framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2901820",
            "name": "Hakan Inan"
        },
        {
            "authorId": "2308992",
            "name": "Khashayar Khosravi"
        },
        {
            "authorId": "2166511",
            "name": "R. Socher"
        }
    ],
    "references": [
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "49899bd9e5a7f59aa14e6d21ed501e3c3acd5852",
            "title": "LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition"
        },
        {
            "paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559",
            "title": "Using the Output Embedding to Improve Language Models"
        },
        {
            "paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
            "title": "Recurrent Highway Networks"
        },
        {
            "paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa",
            "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"
        },
        {
            "paperId": "9ed9bff37ec952134564b3b2a022b7aba9479ff2",
            "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism"
        },
        {
            "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
            "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"
        },
        {
            "paperId": "1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52",
            "title": "A Neural Attention Model for Abstractive Sentence Summarization"
        },
        {
            "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "title": "Character-Aware Neural Language Models"
        },
        {
            "paperId": "ae23eda2faddd824480de2d90638795f797cc66e",
            "title": "Learning with a Wasserstein Loss"
        },
        {
            "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
            "title": "Distilling the Knowledge in a Neural Network"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97",
            "title": "Recurrent Neural Network Regularization"
        },
        {
            "paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7",
            "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"
        },
        {
            "paperId": "533ee188324b833e059cb59b654e6160776d5812",
            "title": "How to Construct Deep Recurrent Neural Networks"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4",
            "title": "Context dependent recurrent neural network language model"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "bd7d93193aad6c4b71cc8942e808753019e87706",
            "title": "Three new graphical models for statistical language modelling"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "fa1d573a466ea6ab189a21036bdcb9bb1e923bba",
            "title": "Cryptosporidium Parvum Genome Project"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "bcf377585fa0751b8bb983ccc7cf0866826eebec",
            "title": "Numerical methods for computing angles between linear subspaces"
        },
        {
            "paperId": "bcd857d75841aa3e92cd4284a8818aba9f6c0c3f",
            "title": "Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS"
        },
        {
            "paperId": "5762b7deff7e95febe193196d548379ff34b34f1",
            "title": "Improved Learning through Augmenting the Loss"
        },
        {
            "paperId": "e4351041d25c272a008bcd5765868dc3a28fe470",
            "title": "Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks"
        },
        {
            "paperId": "2a76c2121eee30af82a24058b4e149f05bcda911",
            "title": "Language modeling with sum-product networks"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "02fd44cf2e1728a58f37263af75271611895af79",
            "title": "Advances in Neural Information Processing"
        },
        {
            "paperId": "e6647feeaafa97f2b888b906232bcaed38c5bd0d",
            "title": "Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France"
        },
        {
            "paperId": null,
            "title": "Essai sur la g\u00b4eom\u00b4etrie `a n dimensions"
        },
        {
            "paperId": "2d61f694852f536034b4091197f12ffb97de2f44",
            "title": "Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France"
        },
        {
            "paperId": null,
            "title": "k . k Fr"
        },
        {
            "paperId": null,
            "title": "Calculate the projection of either one of U and V onto the other"
        }
    ]
}