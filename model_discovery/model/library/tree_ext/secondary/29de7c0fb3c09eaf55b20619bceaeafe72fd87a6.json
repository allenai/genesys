{
    "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
    "externalIds": {
        "MAG": "2798664956",
        "ArXiv": "1805.04833",
        "ACL": "P18-1082",
        "DBLP": "journals/corr/abs-1805-04833",
        "DOI": "10.18653/v1/P18-1082",
        "CorpusId": 44134226
    },
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2018,
    "referenceCount": 33,
    "citationCount": 1377,
    "influentialCitationCount": 192,
    "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P18-1082.pdf",
        "status": "HYBRID"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work collects a large dataset of 300K human-written stories paired with writing prompts from an online forum that enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "144270981",
            "name": "Angela Fan"
        },
        {
            "authorId": "35084211",
            "name": "M. Lewis"
        },
        {
            "authorId": "2921469",
            "name": "Yann Dauphin"
        }
    ],
    "references": [
        {
            "paperId": "cf8a30a987cda5f7544064e607c15d7a7229b7d3",
            "title": "Toward Automated Story Generation with Markov Chain Monte Carlo Methods and Deep Neural Networks"
        },
        {
            "paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9",
            "title": "Generating Wikipedia by Summarizing Long Sequences"
        },
        {
            "paperId": "92a0cf2085013da3fe1fea2090d1bbabcabbf5be",
            "title": "Hierarchical Text Generation and Planning for Strategic Dialogue"
        },
        {
            "paperId": "33125ec92a0b4b1687ccd153762d6275668e3d09",
            "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models"
        },
        {
            "paperId": "3740df4c6e7144e2c1bc441e18fa4a996c9d57b9",
            "title": "Story Generation from Sequence of Independent Short Descriptions"
        },
        {
            "paperId": "13395213d47f78672ab4e81573f2b0fa0cfc8c6d",
            "title": "Challenges in Data-to-Document Generation"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "47a26b0c5d27b99da175e0a719f42d707f97ec3d",
            "title": "Event Representations for Automated Story Generation with Deep Neural Nets"
        },
        {
            "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
            "title": "Convolutional Sequence to Sequence Learning"
        },
        {
            "paperId": "7f9135f3584e4e1715b2990a4f389c94af0313a5",
            "title": "Generating Long and Diverse Responses with Neural Conversation Models"
        },
        {
            "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
            "title": "Language Modeling with Gated Convolutional Networks"
        },
        {
            "paperId": "7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786",
            "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models"
        },
        {
            "paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc",
            "title": "Unsupervised Pretraining for Sequence to Sequence Learning"
        },
        {
            "paperId": "e4dd95c4341ec7d14317a3d97022773a0822906c",
            "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "e2dba792360873aef125572812f3673b1a85d850",
            "title": "Enriching Word Vectors with Subword Information"
        },
        {
            "paperId": "1a58b67e521a4ef9ffaa1ce16d81edda3c24183b",
            "title": "Writing Stories with Help from Recurrent Neural Networks"
        },
        {
            "paperId": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "title": "Exploring the Limits of Language Modeling"
        },
        {
            "paperId": "651e5bcc14f14605a879303e97572a27ea8c7956",
            "title": "A Diversity-Promoting Objective Function for Neural Conversation Models"
        },
        {
            "paperId": "1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52",
            "title": "A Neural Attention Model for Abstractive Sentence Summarization"
        },
        {
            "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "title": "Skip-Thought Vectors"
        },
        {
            "paperId": "b21c78a62fbb945a19ae9a8935933711647e7d70",
            "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents"
        },
        {
            "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "title": "End-To-End Memory Networks"
        },
        {
            "paperId": "5fcd41ca42659ff792fc8ee7d535156e8e69f987",
            "title": "On Using Monolingual Corpora in Neural Machine Translation"
        },
        {
            "paperId": "229ec55602143271867682d181ec35f2e43e06e8",
            "title": "Chinese Poetry Generation with Recurrent Neural Networks"
        },
        {
            "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
            "title": "On the importance of initialization and momentum in deep learning"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": null,
            "title": "Raquel Urtasun, and Sanja Fidler"
        },
        {
            "paperId": null,
            "title": "normalization 1 e \u2212 7 , 4 decoder self-attention heads."
        },
        {
            "paperId": null,
            "title": "embedding size 256, l2 nomalization 1 e \u2212 7 , 4 decoder self-attention heads. 9.3 Ensemble: Conv seq2seq + self-attention"
        },
        {
            "paperId": null,
            "title": "Two different Conv seq2seq models were trained and ensembled together by averaging with equal weights"
        }
    ]
}