{
    "paperId": "13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0",
    "externalIds": {
        "DBLP": "journals/corr/abs-2210-09221",
        "ArXiv": "2210.09221",
        "DOI": "10.48550/arXiv.2210.09221",
        "CorpusId": 252918331
    },
    "title": "Vision Transformers provably learn spatial structure",
    "abstract": "Vision Transformers (ViTs) have achieved comparable or superior performance than Convolutional Neural Networks (CNNs) in computer vision. This empirical breakthrough is even more remarkable since, in contrast to CNNs, ViTs do not embed any visual inductive bias of spatial locality. Yet, recent works have shown that while minimizing their training loss, ViTs specifically learn spatially localized patterns. This raises a central question: how do ViTs learn these patterns by solely minimizing their training loss using gradient-based methods from random initialization? In this paper, we provide some theoretical justification of this phenomenon. We propose a spatially structured dataset and a simplified ViT model. In this model, the attention matrix solely depends on the positional encodings. We call this mechanism the positional attention mechanism. On the theoretical side, we consider a binary classification task and show that while the learning problem admits multiple solutions that generalize, our model implicitly learns the spatial structure of the dataset while generalizing: we call this phenomenon patch association. We prove that patch association helps to sample-efficiently transfer to downstream datasets that share the same structure as the pre-training one but differ in the features. Lastly, we empirically verify that a ViT with positional attention performs similarly to the original one on CIFAR-10/100, SVHN and ImageNet.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "referenceCount": 76,
    "citationCount": 48,
    "influentialCitationCount": 4,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2210.09221",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a spatially structured dataset and a simplified ViT model and proposes a mechanism that implicitly learns the spatial structure of the dataset while generalizing, and proves that patch association helps to sample-efficiently transfer to downstream datasets that share the same structure as the pre-training one but differ in the features."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -1.8258339166641235,
            -1.4867180585861206,
            2.854639768600464,
            3.8931610584259033,
            -1.8114155530929565,
            0.6858874559402466,
            9.222100257873535,
            -0.23822230100631714,
            1.4902479648590088,
            -1.4808619022369385,
            -0.6570371985435486,
            6.250416278839111,
            0.4632617235183716,
            0.26072922348976135,
            -4.334556579589844,
            -1.3785091638565063,
            -1.6722006797790527,
            1.207638144493103,
            2.8743045330047607,
            2.8326737880706787,
            -0.010012563318014145,
            1.212265968322754,
            -1.759103775024414,
            -1.260398507118225,
            -3.1240317821502686,
            -0.32050493359565735,
            1.6585185527801514,
            1.3889800310134888,
            -4.247924327850342,
            3.3581175804138184,
            0.9339395761489868,
            -6.085021018981934,
            8.208901405334473,
            -2.512143611907959,
            0.7557271122932434,
            0.24842816591262817,
            2.906165838241577,
            5.443299770355225,
            -6.225295543670654,
            0.27415868639945984,
            -1.4818577766418457,
            3.663118362426758,
            1.4058363437652588,
            1.06186842918396,
            -0.9707789421081543,
            1.4212589263916016,
            -3.1606826782226562,
            3.003275156021118,
            -0.21899628639221191,
            2.1232738494873047,
            0.5737038850784302,
            3.852494716644287,
            -1.19419264793396,
            -0.7381778359413147,
            -1.418304681777954,
            -2.1321287155151367,
            -0.301421195268631,
            4.189678192138672,
            2.2342467308044434,
            -0.6547401547431946,
            2.3731191158294678,
            1.1870957612991333,
            0.08083352446556091,
            0.8234404921531677,
            6.13609504699707,
            -6.25567626953125,
            1.9503012895584106,
            5.105556488037109,
            -0.5258725881576538,
            -1.2008906602859497,
            -1.207869052886963,
            -5.188051700592041,
            0.33511579036712646,
            -0.7755674719810486,
            -2.9435346126556396,
            -0.27642732858657837,
            -0.5467721819877625,
            -6.076935768127441,
            -0.6826695203781128,
            0.270129531621933,
            -2.8996968269348145,
            3.0886471271514893,
            -2.1160619258880615,
            0.3498850464820862,
            4.619718551635742,
            -4.830883026123047,
            -0.6453421115875244,
            -1.561350703239441,
            -2.331834554672241,
            -0.9890349507331848,
            -2.459351062774658,
            0.5994670391082764,
            2.3021535873413086,
            2.928595542907715,
            -7.0497212409973145,
            3.5985817909240723,
            1.3947747945785522,
            0.022861719131469727,
            -0.21257087588310242,
            -0.7021567821502686,
            3.7945556640625,
            -1.5341720581054688,
            4.178351879119873,
            0.11305303871631622,
            3.2835538387298584,
            -3.1712241172790527,
            -0.16028517484664917,
            -0.624038815498352,
            2.0546863079071045,
            -5.881532669067383,
            -3.282142400741577,
            1.766022801399231,
            -1.344091773033142,
            -0.23905949294567108,
            0.07135039567947388,
            0.699028730392456,
            3.931152820587158,
            0.16031834483146667,
            1.9012694358825684,
            3.800642490386963,
            -0.6533740758895874,
            -3.8130574226379395,
            -5.8431010246276855,
            3.6197094917297363,
            -1.3493150472640991,
            2.5326201915740967,
            0.9377734065055847,
            3.4346885681152344,
            -0.8799015283584595,
            -5.552705764770508,
            3.507059335708618,
            -5.149520397186279,
            1.2179588079452515,
            0.3874126970767975,
            1.109844446182251,
            2.966583013534546,
            -1.3007330894470215,
            -0.44840455055236816,
            -4.4786787033081055,
            0.9456032514572144,
            2.2581892013549805,
            3.4827730655670166,
            -0.8476026058197021,
            2.0050973892211914,
            0.4275023937225342,
            3.087447166442871,
            -0.5013478994369507,
            5.397480010986328,
            4.562398910522461,
            5.436460018157959,
            3.005949020385742,
            -3.2866880893707275,
            0.540467381477356,
            4.391074180603027,
            -0.5143752098083496,
            2.1835241317749023,
            -7.592434406280518,
            2.8441386222839355,
            -2.383686065673828,
            3.213193655014038,
            0.2766050696372986,
            0.9005417823791504,
            -10.532767295837402,
            1.397614598274231,
            4.672915458679199,
            -4.40595006942749,
            -0.9449312686920166,
            0.1225641518831253,
            -0.6477192640304565,
            5.066132545471191,
            1.571975588798523,
            2.898684501647949,
            0.8813174366950989,
            4.934469699859619,
            0.4886947572231293,
            2.3455371856689453,
            2.4756906032562256,
            -1.6136493682861328,
            -1.612231731414795,
            -0.012152314186096191,
            -2.7668678760528564,
            -1.9407458305358887,
            -5.02541971206665,
            -0.23114660382270813,
            -2.7233564853668213,
            -2.49442458152771,
            -2.0023231506347656,
            0.7255401611328125,
            -3.171133518218994,
            -2.8733532428741455,
            -0.30160805583000183,
            0.10058017075061798,
            4.390514373779297,
            6.413440227508545,
            0.39553433656692505,
            0.5661538243293762,
            1.3370496034622192,
            -0.9528036117553711,
            -1.171755313873291,
            4.713412761688232,
            3.332125663757324,
            0.37229418754577637,
            -2.2851195335388184,
            -3.1654038429260254,
            2.124051570892334,
            2.058539867401123,
            -2.4529216289520264,
            1.8477047681808472,
            2.1368939876556396,
            -0.25992104411125183,
            -3.4417686462402344,
            1.2866145372390747,
            -1.3963487148284912,
            3.5408337116241455,
            -5.988966464996338,
            -0.825549840927124,
            -4.74807071685791,
            1.4135009050369263,
            5.102930068969727,
            -1.3197922706604004,
            1.9391368627548218,
            -0.7805150151252747,
            -1.9361093044281006,
            -3.0260276794433594,
            1.4445796012878418,
            -1.4996057748794556,
            3.825944185256958,
            4.738759517669678,
            1.573718786239624,
            5.065071105957031,
            -1.5403231382369995,
            -4.747673988342285,
            1.5837619304656982,
            -2.876918077468872,
            -5.227099895477295,
            -3.4236209392547607,
            -3.9525065422058105,
            -1.4190428256988525,
            -3.8316407203674316,
            -1.022615671157837,
            6.816516876220703,
            0.1460566520690918,
            1.6375837326049805,
            2.6552491188049316,
            -1.2508620023727417,
            -1.6349523067474365,
            -5.533735275268555,
            0.9628573656082153,
            4.297948360443115,
            -1.234717845916748,
            -0.17382636666297913,
            -1.2941954135894775,
            3.039928674697876,
            -0.7894257307052612,
            1.0922722816467285,
            1.0558785200119019,
            2.9695932865142822,
            2.1214370727539062,
            1.7489196062088013,
            -0.12532520294189453,
            -0.7728182673454285,
            1.9088921546936035,
            1.4777334928512573,
            1.1482568979263306,
            -1.7105472087860107,
            -0.8437051773071289,
            -3.5859603881835938,
            1.005352258682251,
            -2.5129470825195312,
            3.9036383628845215,
            0.5277262330055237,
            0.7210092544555664,
            -2.026937961578369,
            -5.16407585144043,
            -1.724877119064331,
            -1.2135088443756104,
            -4.6489362716674805,
            -2.4117274284362793,
            3.2867555618286133,
            2.0623843669891357,
            -0.36864566802978516,
            0.6066088676452637,
            -2.116976499557495,
            -0.18239456415176392,
            -0.5893599987030029,
            -7.284269332885742,
            -3.2235755920410156,
            -0.7009162902832031,
            0.609011173248291,
            -0.30765479803085327,
            -3.5880942344665527,
            4.257120609283447,
            -4.065347194671631,
            1.7161343097686768,
            0.08821950852870941,
            3.706397533416748,
            4.179948806762695,
            4.0173749923706055,
            -2.0863747596740723,
            0.6486572027206421,
            -2.402043581008911,
            0.8702734708786011,
            4.487506866455078,
            -3.0189521312713623,
            1.4431012868881226,
            2.862459659576416,
            -0.7089391946792603,
            -2.8453500270843506,
            0.7462629079818726,
            -2.2118310928344727,
            -0.6065542101860046,
            -1.9050359725952148,
            1.316868543624878,
            -2.950942277908325,
            -3.661675214767456,
            -4.088596820831299,
            1.3016631603240967,
            0.6616159677505493,
            -1.649911642074585,
            4.396426200866699,
            1.5806554555892944,
            4.181864261627197,
            -2.335360288619995,
            -3.8515615463256836,
            -2.1453375816345215,
            -0.6774347424507141,
            5.440291404724121,
            -0.4506387412548065,
            1.7957265377044678,
            3.552522897720337,
            2.311722755432129,
            3.2781248092651367,
            2.1991398334503174,
            2.5961828231811523,
            1.3734132051467896,
            -4.2882819175720215,
            1.4526691436767578,
            0.10707163065671921,
            0.7479159832000732,
            5.399020195007324,
            1.0778931379318237,
            7.185125350952148,
            1.4312174320220947,
            -0.1126699447631836,
            2.7930498123168945,
            -0.48448294401168823,
            1.32231867313385,
            -5.104784965515137,
            1.9326921701431274,
            -3.2470273971557617,
            -0.7418478727340698,
            0.6901170015335083,
            1.4990792274475098,
            -4.810529708862305,
            0.9052128791809082,
            4.467432975769043,
            4.151640892028809,
            0.7232494354248047,
            3.860959768295288,
            2.489356517791748,
            -0.662838876247406,
            -0.5198523998260498,
            0.6302378177642822,
            -1.7816585302352905,
            -1.9596766233444214,
            -1.9503309726715088,
            10.399574279785156,
            -0.014264672994613647,
            -1.5205119848251343,
            -3.236809492111206,
            -7.4156494140625,
            -0.23829317092895508,
            0.84079909324646,
            4.361071586608887,
            -2.653837203979492,
            -1.9674097299575806,
            1.81207275390625,
            -3.8164608478546143,
            2.6828198432922363,
            1.2919807434082031,
            0.16394208371639252,
            4.694604396820068,
            -0.012401588261127472,
            5.186169624328613,
            -0.14348196983337402,
            0.9164054989814758,
            -4.611246585845947,
            1.6423864364624023,
            2.444279193878174,
            3.1374049186706543,
            -2.884376049041748,
            4.482599258422852,
            0.2879084348678589,
            -0.7061159610748291,
            -3.8132853507995605,
            -4.123069763183594,
            -6.305339336395264,
            -4.591893196105957,
            -2.2945830821990967,
            -0.6527438163757324,
            -2.0021400451660156,
            5.324804306030273,
            3.646181583404541,
            2.0523126125335693,
            -3.079444646835327,
            -2.8700926303863525,
            5.981952667236328,
            0.975812554359436,
            -0.44054991006851196,
            0.871383786201477,
            -3.9393908977508545,
            -0.4815436899662018,
            -1.576704740524292,
            -2.0159287452697754,
            2.36439847946167,
            -1.7026593685150146,
            -1.827152132987976,
            2.817678928375244,
            4.049542427062988,
            2.9914236068725586,
            -0.21821878850460052,
            5.419239044189453,
            4.637405872344971,
            1.016786813735962,
            -0.6145949959754944,
            3.6478559970855713,
            -0.31894397735595703,
            3.4163155555725098,
            -0.4534415006637573,
            0.9841955900192261,
            -1.9876574277877808,
            4.562572002410889,
            -2.270918846130371,
            2.275815486907959,
            -3.1328158378601074,
            3.212639808654785,
            2.4252982139587402,
            1.4743276834487915,
            2.1215083599090576,
            0.8186353445053101,
            -2.241006851196289,
            1.9053428173065186,
            0.3124304413795471,
            5.1395111083984375,
            -2.188595771789551,
            -2.159294366836548,
            0.6847753524780273,
            -0.06506982445716858,
            -0.3045438230037689,
            -6.061799049377441,
            0.7273555994033813,
            -4.768077373504639,
            -1.3859134912490845,
            -0.09493875503540039,
            2.280465602874756,
            1.5015504360198975,
            -0.09745016694068909,
            -2.8279659748077393,
            0.394536554813385,
            -0.541877806186676,
            -4.392354965209961,
            3.0817415714263916,
            0.7829622030258179,
            3.9192018508911133,
            3.083625078201294,
            6.02523136138916,
            -1.3919782638549805,
            2.159707546234131,
            -3.29972243309021,
            1.2867155075073242,
            4.3712944984436035,
            1.4461530447006226,
            0.00897189974784851,
            -2.0097451210021973,
            -0.04955517500638962,
            -2.7346110343933105,
            1.72060227394104,
            3.2790403366088867,
            0.012444943189620972,
            -2.075894832611084,
            -3.187944173812866,
            -1.764568567276001,
            3.097456455230713,
            -0.6729658842086792,
            -3.3490686416625977,
            2.9185121059417725,
            5.25417947769165,
            2.803776741027832,
            4.607888221740723,
            2.719308376312256,
            -0.24565677344799042,
            1.719641923904419,
            3.0223453044891357,
            -0.751998245716095,
            4.463243007659912,
            2.498277187347412,
            -2.352386713027954,
            3.6131327152252197,
            -0.2905748188495636,
            -1.3586372137069702,
            0.7971681356430054,
            -5.031248569488525,
            -2.4603981971740723,
            -2.2869017124176025,
            -4.658592700958252,
            2.4874019622802734,
            3.054950714111328,
            1.1382479667663574,
            -1.3091200590133667,
            -0.2812715172767639,
            -2.018907070159912,
            0.6035929918289185,
            -7.71059513092041,
            -0.6951746940612793,
            0.2042486071586609,
            2.2441153526306152,
            2.714540719985962,
            -2.861508369445801,
            -2.033390760421753,
            1.3184421062469482,
            0.25593137741088867,
            2.507446527481079,
            -2.2390520572662354,
            1.8754987716674805,
            2.081648826599121,
            0.03748651593923569,
            -7.590585708618164,
            3.6117751598358154,
            0.4824293851852417,
            3.225586414337158,
            4.983133316040039,
            4.343887805938721,
            2.774726390838623,
            -2.663722038269043,
            -3.4958481788635254,
            -5.373651027679443,
            0.14835700392723083,
            5.297577857971191,
            -0.5214928984642029,
            -1.2407708168029785,
            0.30968618392944336,
            0.4987390637397766,
            -2.2296876907348633,
            0.9442884922027588,
            -3.111830711364746,
            2.8786892890930176,
            -5.032310485839844,
            -1.5321643352508545,
            0.10867160558700562,
            0.906033992767334,
            1.9604382514953613,
            -1.9373388290405273,
            0.8724489212036133,
            -4.297884941101074,
            -0.9967197179794312,
            -1.9374988079071045,
            -0.6756052374839783,
            -3.0064663887023926,
            -2.726487398147583,
            1.3421800136566162,
            -0.46933573484420776,
            -0.31403812766075134,
            1.2070649862289429,
            4.172144889831543,
            -0.9496232271194458,
            0.7296785116195679,
            3.0099167823791504,
            1.749932885169983,
            -1.529916524887085,
            -6.224603652954102,
            0.216461643576622,
            -4.861237049102783,
            -3.4938547611236572,
            1.5745515823364258,
            1.953197956085205,
            2.9217724800109863,
            5.2197794914245605,
            4.2183356285095215,
            -2.6907923221588135,
            3.9307188987731934,
            -3.5506772994995117,
            -0.6522423028945923,
            -1.1139622926712036,
            -7.825282573699951,
            3.6140317916870117,
            -4.558985233306885,
            -1.9327526092529297,
            1.3716049194335938,
            -2.9652199745178223,
            -1.0324559211730957,
            0.9581472873687744,
            -3.440225601196289,
            3.5537264347076416,
            -6.247272491455078,
            -4.8134260177612305,
            -2.9862308502197266,
            4.784573078155518,
            -1.4682295322418213,
            -0.4422042667865753,
            4.022319316864014,
            3.5929667949676514,
            2.446934700012207,
            4.502630710601807,
            1.7337532043457031,
            -2.1751582622528076,
            1.6153101921081543,
            0.37780481576919556,
            1.5174336433410645,
            -0.9336117506027222,
            0.944942831993103,
            1.090258240699768,
            -2.5549240112304688,
            15.668002128601074,
            -1.0534262657165527,
            -1.4211783409118652,
            -4.395274639129639,
            2.0530762672424316,
            -3.3532841205596924,
            2.183650493621826,
            1.1766939163208008,
            2.1016135215759277,
            4.1707868576049805,
            -1.223229169845581,
            -5.954346656799316,
            0.7298588156700134,
            1.5277340412139893,
            -3.799518585205078,
            -2.7864630222320557,
            -1.5399483442306519,
            -0.2070533037185669,
            -0.43746936321258545,
            -0.29796484112739563,
            -4.007743835449219,
            1.12027108669281,
            -0.0996989905834198,
            1.213496208190918,
            -0.20377230644226074,
            2.913435935974121,
            -2.508615016937256,
            2.273512125015259,
            -5.174755096435547,
            2.057175874710083,
            2.2623095512390137,
            1.0694063901901245,
            6.035335540771484,
            -2.221634864807129,
            -2.7512240409851074,
            1.8340837955474854,
            7.163249969482422,
            1.696865200996399,
            4.77591609954834,
            -1.3148233890533447,
            1.0593786239624023,
            0.3563729524612427,
            -4.9414591789245605,
            1.3905314207077026,
            -0.38260552287101746,
            -0.15008237957954407,
            1.9658092260360718,
            -2.880037784576416,
            -1.108770728111267,
            2.0965447425842285,
            1.541713833808899,
            0.9898391962051392,
            0.0736321210861206,
            2.6240432262420654,
            2.5052332878112793,
            3.666137218475342,
            -0.1342054307460785,
            -0.028323635458946228,
            0.425275057554245,
            -0.705030083656311,
            0.014432251453399658,
            3.6124842166900635,
            -4.936274528503418,
            -5.198987007141113,
            -2.0550289154052734,
            2.3728034496307373,
            -6.647614002227783,
            1.604665756225586,
            0.3859884440898895,
            -1.5225801467895508,
            4.333227157592773,
            0.7923099398612976,
            1.3962624073028564,
            -3.801567554473877,
            -2.959254741668701,
            -8.317607879638672,
            -1.3752552270889282,
            0.16275440156459808,
            0.8569855690002441,
            4.644505023956299,
            -5.825774669647217,
            1.2253031730651855,
            -3.4366109371185303,
            -0.1794980764389038,
            5.322300910949707,
            -0.7453057765960693,
            1.0155259370803833,
            -1.2373826503753662,
            1.561048984527588,
            3.272733449935913,
            0.41492342948913574,
            -1.6073379516601562,
            -0.052142709493637085,
            -1.0540268421173096,
            4.913431644439697,
            -6.020665168762207,
            -4.474583625793457,
            -3.4802281856536865,
            -5.845733642578125,
            -3.919832706451416,
            7.076723098754883,
            2.4068775177001953,
            0.26382893323898315,
            -5.425247669219971,
            -0.8487145304679871,
            -1.7294330596923828,
            1.7555150985717773,
            -5.7074785232543945,
            -5.720843315124512,
            -4.133999824523926,
            4.67257833480835,
            -4.410027027130127,
            2.224761724472046,
            -1.140700101852417,
            -0.7649621963500977,
            -0.3228985369205475,
            -0.09867344796657562,
            3.5912907123565674,
            3.170332908630371,
            3.674421787261963,
            0.32709217071533203,
            -0.1900307536125183,
            -3.721858501434326,
            -3.5109550952911377,
            -1.2508255243301392,
            0.6588853597640991,
            1.4220988750457764,
            -0.19527573883533478,
            0.34310802817344666,
            -6.502236366271973,
            0.05753205716609955,
            -2.5114407539367676,
            -1.039438009262085,
            -0.9671962857246399,
            -4.732882499694824,
            -1.8924533128738403,
            2.3811326026916504,
            2.0652413368225098,
            -4.128317832946777,
            3.1018729209899902,
            3.2934865951538086,
            -0.26291829347610474,
            -0.048645198345184326,
            7.262452125549316,
            -3.1666007041931152,
            -1.2908759117126465,
            -3.6816351413726807,
            -1.0337402820587158,
            -1.5089843273162842,
            -0.8715102672576904,
            -0.9572781324386597,
            -1.097132682800293,
            5.518379211425781,
            2.1086459159851074,
            -2.557866334915161,
            -1.5250602960586548
        ]
    },
    "authors": [
        {
            "authorId": "47009988",
            "name": "Samy Jelassi"
        },
        {
            "authorId": "2068668635",
            "name": "Michael E. Sander"
        },
        {
            "authorId": "152244300",
            "name": "Yuan-Fang Li"
        }
    ],
    "references": [
        {
            "paperId": "33d97417aa0e5c6fc4a58716003b02c09736e782",
            "title": "Towards Understanding Mixture of Experts in Deep Learning"
        },
        {
            "paperId": "ea55230102f589d37f71992b97ba3dfe38fcbadc",
            "title": "Towards understanding how momentum improves generalization in deep learning"
        },
        {
            "paperId": "3425495ee3b6ead009f35aeb70edeac4e6eb2d10",
            "title": "Patches Are All You Need?"
        },
        {
            "paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737",
            "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"
        },
        {
            "paperId": "1d9cefa796b0c7340b1af35356e93b3715a39973",
            "title": "Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization"
        },
        {
            "paperId": "39b492db00faead70bc3f4fb4b0364d94398ffdb",
            "title": "Do Vision Transformers See Like Convolutional Neural Networks?"
        },
        {
            "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
            "title": "Program Synthesis with Large Language Models"
        },
        {
            "paperId": "0e9ac2cfc5a3ecb66eeace720901390f7809ba0a",
            "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"
        },
        {
            "paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65",
            "title": "CMT: Convolutional Neural Networks Meet Vision Transformers"
        },
        {
            "paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0",
            "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"
        },
        {
            "paperId": "50879b5ee22766811e15a5c243f9764d245e1ad8",
            "title": "Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions"
        },
        {
            "paperId": "f864d4d2267abba15eb43db54f58286aef78292b",
            "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
        },
        {
            "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
            "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
        },
        {
            "paperId": "ac78850445bdeb4d3f8a273297916d6e6a90b2fc",
            "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning"
        },
        {
            "paperId": "03db529f0bfae6d0b64b0feef565196327fe8d50",
            "title": "Intriguing Properties of Vision Transformers"
        },
        {
            "paperId": "5e4f03f68c6867d850f457dc5cc36738e5dff6c1",
            "title": "Vision Transformers are Robust Learners"
        },
        {
            "paperId": "0768aba7d87ddda3482fd7892b189f84711ede47",
            "title": "Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet"
        },
        {
            "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
            "title": "Emerging Properties in Self-Supervised Vision Transformers"
        },
        {
            "paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae",
            "title": "An Empirical Study of Training Self-Supervised Vision Transformers"
        },
        {
            "paperId": "d2a3bb6356d439146cd8d8e72dc728a1e3d93e7f",
            "title": "Understanding Robustness of Transformers for Image Classification"
        },
        {
            "paperId": "610b302950a19acef1c45456111dcd495f638c18",
            "title": "ConViT: improving vision transformers with soft convolutional inductive biases"
        },
        {
            "paperId": "4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506",
            "title": "Approximating How Single Head Attention Learns"
        },
        {
            "paperId": "52f32e4340989a4fec9d4bcc62cbd724aa6daa9c",
            "title": "Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm"
        },
        {
            "paperId": "cec7872b194aadf54140578b9be52939eb1112e9",
            "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"
        },
        {
            "paperId": "58c74cec28f3416b9a1d308bb2d6519d21d53ab0",
            "title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "255e6239bcc51047d020d41ce0179c1270f3c22f",
            "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"
        },
        {
            "paperId": "901b546ae60d1e3b6cfe80f19f0786321e701bf4",
            "title": "Why are Adaptive Methods Good for Attention Models?"
        },
        {
            "paperId": "0e012c2bd18236445cfbc6e3e409eb02df4691fe",
            "title": "Can neural networks acquire a structural bias from raw linguistic data?"
        },
        {
            "paperId": "ee0dbe241f87e5161fd0eb9714f38a003a2b97f3",
            "title": "Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK"
        },
        {
            "paperId": "6349901133797ab211cfd8c8dfcdc57828f20dfb",
            "title": "A Mathematical Theory of Attention"
        },
        {
            "paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510",
            "title": "Infinite attention: NNGP and NTK for deep attention networks"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "title": "End-to-End Object Detection with Transformers"
        },
        {
            "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
            "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "71022c0c51f1e06384ff211467d04230dee96f51",
            "title": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss"
        },
        {
            "paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62",
            "title": "Deep Learning for Symbolic Mathematics"
        },
        {
            "paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614",
            "title": "On the Relationship between Self-Attention and Convolutional Layers"
        },
        {
            "paperId": "36451fe94b7b1f978b8301cf3f7305566bd3b454",
            "title": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"
        },
        {
            "paperId": "3a982f7e82bedb82bd2b53cf2da1d08012ed5179",
            "title": "Kernel and Rich Regimes in Overparametrized Models"
        },
        {
            "paperId": "3f46ac38812f9f0f99ff1edddd85d71a84da4497",
            "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks"
        },
        {
            "paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d",
            "title": "Stand-Alone Self-Attention in Vision Models"
        },
        {
            "paperId": "217a85f667778567d7ffc8b56060783caf5803b0",
            "title": "Implicit Regularization in Deep Matrix Factorization"
        },
        {
            "paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c",
            "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"
        },
        {
            "paperId": "27ac832ee83d8b5386917998a171a0257e2151e2",
            "title": "Attention Augmented Convolutional Networks"
        },
        {
            "paperId": "3dfb0a18ab5a5413c50d911e49b3c83b1a9383a3",
            "title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization"
        },
        {
            "paperId": "67a97032fd3ad81cda45e1e5d4a1a7d851494525",
            "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks"
        },
        {
            "paperId": "f723eb3e7159f07b97464c8d947d15e78612abe4",
            "title": "AutoAugment: Learning Augmentation Policies from Data"
        },
        {
            "paperId": "48f0b0ab8ccdca861e374f26c3e54e1720cfb22e",
            "title": "Norm matters: efficient and accurate normalization schemes in deep networks"
        },
        {
            "paperId": "8b4b861583f698e89c8cd9e198aad86809a71de7",
            "title": "Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "3299aee7a354877e43339d06abb967af2be8b872",
            "title": "Don't Decay the Learning Rate, Increase the Batch Size"
        },
        {
            "paperId": "11adc8bd35bd897502f9b5452ab7ac668ec9b0fb",
            "title": "The Implicit Bias of Gradient Descent on Separable Data"
        },
        {
            "paperId": "4feef0fd284feb1233399b400eb897f59ec92755",
            "title": "mixup: Beyond Empirical Risk Minimization"
        },
        {
            "paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341",
            "title": "Random Erasing Data Augmentation"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5",
            "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"
        },
        {
            "paperId": "1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c",
            "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning"
        },
        {
            "paperId": "8501e330d78391f4e690886a8eb8fac867704ea6",
            "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks"
        },
        {
            "paperId": "6783b727e7de7d6d826e765e48406aacf103e63d",
            "title": "SGD Learns the Conjugate Kernel Class of the Network"
        },
        {
            "paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81",
            "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"
        },
        {
            "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
            "title": "Deep Networks with Stochastic Depth"
        },
        {
            "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "title": "Identity Mappings in Deep Residual Networks"
        },
        {
            "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "title": "Rethinking the Inception Architecture for Computer Vision"
        },
        {
            "paperId": "6fe02ad979baad659f04c3376a77dbb2cb4699a5",
            "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks"
        },
        {
            "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "581528b2215e017eba96ef4ee16d33a74645755f",
            "title": "Neocognitron for handwritten digit recognition"
        },
        {
            "paperId": null,
            "title": "Adam is no better than normalized SGD: Dissecting how adaptivity improves GAN performance, 2022"
        },
        {
            "paperId": null,
            "title": "The implicit and explicit regularization effects of dropout, 2020"
        },
        {
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting"
        },
        {
            "paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda",
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"
        },
        {
            "paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "title": "Learning Multiple Layers of Features from Tiny Images"
        }
    ]
}