{
    "paperId": "988f2bc5fccbea00a23ecea2da112982d397a3dd",
    "externalIds": {
        "DBLP": "conf/acl/AhmedSM19",
        "MAG": "2952802110",
        "ACL": "P19-1030",
        "DOI": "10.18653/v1/P19-1030",
        "CorpusId": 196186973
    },
    "title": "You Only Need Attention to Traverse Trees",
    "abstract": "In recent NLP research, a topic of interest is universal sentence encoding, sentence representations that can be used in any supervised task. At the word sequence level, fully attention-based models suffer from two problems: a quadratic increase in memory consumption with respect to the sentence length and an inability to capture and use syntactic information. Recursive neural nets can extract very good syntactic information by traversing a tree structure. To this end, we propose Tree Transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention. Evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and LSTM-based models as well as tree-structured LSTMs. Ablation studies to find whether positional information is inherently encoded in the trees and which type of attention is suitable for doing the recursive traversal are provided.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "referenceCount": 31,
    "citationCount": 22,
    "influentialCitationCount": 3,
    "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1030.pdf",
        "status": "HYBRID"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Tree Transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention, and gets noteworthy results compared to the standard transformer and LSTM-based models."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "9336386",
            "name": "Mahtab Ahmed"
        },
        {
            "authorId": "51151096",
            "name": "Muhammad Rifayat Samee"
        },
        {
            "authorId": "144549038",
            "name": "Robert E. Mercer"
        }
    ],
    "references": [
        {
            "paperId": "952af139e6a49c5b6490663be967d312c438334d",
            "title": "Grammar Induction with Neural Language Models: An Unusual Replication"
        },
        {
            "paperId": "a76706d350b8c483a3aff73e61b91d15b5687335",
            "title": "Universal Sentence Encoder"
        },
        {
            "paperId": "ad31866da7f14ae21bd38df0a3b1ffd1a1438122",
            "title": "An efficient framework for learning sentence representations"
        },
        {
            "paperId": "3861ae2a6bdd2a759c2d901a6583e63a216bc2fc",
            "title": "Weighted Transformer Network for Machine Translation"
        },
        {
            "paperId": "f1cbf097ce436f7304a1984f4a29ab41f75ebfe3",
            "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon"
        },
        {
            "paperId": "8f46c21fef31a4cdf7b1808e67171466a9317882",
            "title": "Do latent tree learning models identify meaningful structure in sentences?"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
            "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data"
        },
        {
            "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
            "title": "A Structured Self-attentive Sentence Embedding"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2",
            "title": "Hierarchical Attention Networks for Document Classification"
        },
        {
            "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "title": "A Decomposable Attention Model for Natural Language Inference"
        },
        {
            "paperId": "f93a0a3e8a3e6001b4482430254595cf737697fa",
            "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention"
        },
        {
            "paperId": "bb93bc031fede3d53ee01a1b66ca3b24fc8a10d7",
            "title": "SICK through the SemEval glasses. Lesson learned from the evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment"
        },
        {
            "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "title": "Deep Residual Learning for Image Recognition"
        },
        {
            "paperId": "452059171226626718eb677358836328f884298e",
            "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"
        },
        {
            "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "title": "Skip-Thought Vectors"
        },
        {
            "paperId": "e0945081b5b87187a53d4329cf77cd8bff635795",
            "title": "Highway Networks"
        },
        {
            "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
        },
        {
            "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "title": "GloVe: Global Vectors for Word Representation"
        },
        {
            "paperId": "11ec56898a9e7f401a2affe776b5297bd4e25025",
            "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment"
        },
        {
            "paperId": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
            "title": "The Stanford CoreNLP Natural Language Processing Toolkit"
        },
        {
            "paperId": "0ca7d208ff8d81377e0eaa9723820aeae7a7322d",
            "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "27e38351e48fe4b7da2775bf94341738bc4da07e",
            "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces"
        },
        {
            "paperId": "ae5e6c6f5513613a161b2c85563f9708bf2e9178",
            "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection"
        },
        {
            "paperId": "9c0ddf74f87d154db88d79c640578c1610451eec",
            "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"
        },
        {
            "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
        },
        {
            "paperId": "7acfdc905f734abf966aed58abb983bc015ff7fe",
            "title": "Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources"
        },
        {
            "paperId": null,
            "title": "Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil"
        }
    ]
}