{
    "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
    "externalIds": {
        "DBLP": "conf/icml/LeM14",
        "ArXiv": "1405.4053",
        "MAG": "2949547296",
        "CorpusId": 2407601
    },
    "title": "Distributed Representations of Sentences and Documents",
    "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
    "venue": "International Conference on Machine Learning",
    "year": 2014,
    "referenceCount": 42,
    "citationCount": 8881,
    "influentialCitationCount": 1067,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Paragraph Vector is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents, and its construction gives the algorithm the potential to overcome the weaknesses of bag-of-words models."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2827616",
            "name": "Quoc V. Le"
        },
        {
            "authorId": "2047446108",
            "name": "Tomas Mikolov"
        }
    ],
    "references": [
        {
            "paperId": "50d53cc562225549457cbc782546bfbe1ac6f0cf",
            "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion"
        },
        {
            "paperId": "4aa4069693bee00d1b0759ca3df35e59284e9845",
            "title": "DeViSE: A Deep Visual-Semantic Embedding Model"
        },
        {
            "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
            "paperId": "0d3233d858660aff451a6c2561a05378ed09725a",
            "title": "Bilingual Word Embeddings for Phrase-Based Machine Translation"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "0157dcd6122c20b5afc359a799b2043453471f7f",
            "title": "Exploiting Similarities among Languages for Machine Translation"
        },
        {
            "paperId": "43c0b8309d05102aa75980f6cd53e2e77f222a17",
            "title": "Modeling Documents with Deep Boltzmann Machines"
        },
        {
            "paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
            "title": "Linguistic Regularities in Continuous Space Word Representations"
        },
        {
            "paperId": "07020264ff1bc81fe85e652db9ded7f30195be01",
            "title": "Combining Heterogeneous Models for Measuring Relational Similarity"
        },
        {
            "paperId": "607cca37c1429b7380df35b3f761ae1499aa84ab",
            "title": "Multi-Step Regression Learning for Compositional Distributional Semantics"
        },
        {
            "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
            "title": "Efficient Estimation of Word Representations in Vector Space"
        },
        {
            "paperId": "d1b78d136e9e6be0aeb814027f0f3fd843606155",
            "title": "A Neural Autoregressive Topic Model"
        },
        {
            "paperId": "5e9fa46f231c59e6573f9a116f77f53703347659",
            "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification"
        },
        {
            "paperId": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa",
            "title": "Improving Word Representations via Global Context and Multiple Word Prototypes"
        },
        {
            "paperId": "2e12a485325776d3c23eae2b488d4812d86b4052",
            "title": "Training Restricted Boltzmann Machines on Word Observations"
        },
        {
            "paperId": "ae5e6c6f5513613a161b2c85563f9708bf2e9178",
            "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection"
        },
        {
            "paperId": "cfa2646776405d50533055ceb1b7f050e9014dcb",
            "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"
        },
        {
            "paperId": "2063745d08868c928455f422202b72146a1960fb",
            "title": "Compositional Matrix-Space Models for Sentiment Analysis"
        },
        {
            "paperId": "9c0ddf74f87d154db88d79c640578c1610451eec",
            "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"
        },
        {
            "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
            "title": "Learning Word Vectors for Sentiment Analysis"
        },
        {
            "paperId": "bc1022b031dc6c7019696492e8116598097a8c12",
            "title": "Natural Language Processing (Almost) from Scratch"
        },
        {
            "paperId": "745d86adca56ec50761591733e157f84cfb19671",
            "title": "Composition in Distributional Models of Semantics"
        },
        {
            "paperId": "8d9da542a6aa92fece5dfb7eecfb44ae7de0f664",
            "title": "Estimating Linear Models for Compositional Distributional Semantics"
        },
        {
            "paperId": "8492070dc4031ed825e95e4803781752bb5e909f",
            "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning"
        },
        {
            "paperId": "48257a889a9aa61998ae20fa52b25d90c441f63a",
            "title": "Large-scale image retrieval with compressed Fisher vectors"
        },
        {
            "paperId": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics"
        },
        {
            "paperId": "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
            "title": "A Scalable Hierarchical Distributed Language Model"
        },
        {
            "paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"
        },
        {
            "paperId": "23694b6d61668e62bb11f17c1d75dde3b4951948",
            "title": "Fisher Kernels on Visual Vocabularies for Image Categorization"
        },
        {
            "paperId": "6af58c061f2e4f130c3b795c21ff0c7e3903278f",
            "title": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales"
        },
        {
            "paperId": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "title": "Accurate Unlexicalized Parsing"
        },
        {
            "paperId": "28209ce8d0ac1cf4ceea3eeddf4630e1032fa0ef",
            "title": "A neural probabilistic language model"
        },
        {
            "paperId": "e45c2420e6dc59ba6d357fb0c996ebf43c861560",
            "title": "Exploiting Generative Models in Discriminative Classifiers"
        },
        {
            "paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "title": "Finding Structure in Time"
        },
        {
            "paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "title": "Learning representations by back-propagating errors"
        },
        {
            "paperId": "4131146c9c3d56ef41ad2bf3a6976c1cce680ccd",
            "title": "Recursive deep learning for natural language processing and computer vision"
        },
        {
            "paperId": "96364af2d208ea75ca3aeb71892d2f7ce7326b55",
            "title": "Statistical Language Models Based on Neural Networks"
        },
        {
            "paperId": "5eb1a272f9933a11d113cf63fe659e073942bce5",
            "title": "Neural Probabilistic Language Models"
        },
        {
            "paperId": "c19fbefdeead6a4154a22a9c8551a18b1530033a",
            "title": "Hierarchical Probabilistic Neural Network Language Model"
        },
        {
            "paperId": "749ce8ccd9453d1b34901143cddf5f9bee2977cf",
            "title": "Learning representations by back-propagation errors, nature"
        },
        {
            "paperId": "decd9bc0385612bdf936928206d83730718e737e",
            "title": "Distributional Structure"
        },
        {
            "paperId": null,
            "title": "Every sentence in the dataset has a label which goes from very negative to very positive in the scale from 0.0 to 1.0"
        }
    ]
}