{
    "paperId": "078cf910a125919b2fe1ae0dcdc34400f3073d0e",
    "externalIds": {
        "MAG": "2066396564",
        "DBLP": "journals/iandc/KilianS96",
        "DOI": "10.1006/inco.1996.0062",
        "CorpusId": 7111824
    },
    "title": "The Dynamic Universality of Sigmoidal Neural Networks",
    "abstract": "We investigate the computational power of recurrent neural networks that apply the sigmoid activation function?(x)=2/(1+e?x)]?1. These networks are extensively used in automatic learning of non-linear dynamical behavior. We show that in the noiseless model, there exists a universal architecture that can be used to compute any recursive (Turing) function. This is the first result of its kind for the sigmoid activation function; previous techniques only applied to linearized and truncated version of this function. The significance of our result, besides the proving technique itself, lies in the popularity of the sigmoidal function both in engineering applications of artificial neural networks and in biological modelling. Our techniques can be applied to a much more general class of \u201csigmoidal-like\u201d activation functions, suggesting that Turing universality is a relatively common property of recurrent neural network models.",
    "venue": "Information and Computation",
    "year": 1996,
    "referenceCount": 14,
    "citationCount": 133,
    "influentialCitationCount": 7,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The techniques can be applied to a much more general class of \u201csigmoidal-like\u201d activation functions, suggesting that Turing universality is a relatively common property of recurrent neural network models."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "144747292",
            "name": "J. Kilian"
        },
        {
            "authorId": "2797623",
            "name": "H. Siegelmann"
        }
    ],
    "references": [
        {
            "paperId": "93aaf126443643fb0835df896ab07b523f2c9613",
            "title": "Analog computation via neural networks"
        },
        {
            "paperId": "17594df98c222217a11510dd454ba52a5a737378",
            "title": "On the computational power of neural nets"
        },
        {
            "paperId": "872cdc269f3cb59f8a227818f35041415091545f",
            "title": "Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks"
        },
        {
            "paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "title": "Finding Structure in Time"
        },
        {
            "paperId": "bd46c1b5948abe04e565a8bae6454da63a1b021e",
            "title": "Finite State Automata and Simple Recurrent Networks"
        },
        {
            "paperId": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
        },
        {
            "paperId": "79914a9898941527c5a3f4d1d456b733a029699d",
            "title": "Computational power of neural networks"
        },
        {
            "paperId": null,
            "title": "Neural networks analog computation via real weights : Analog computational complexity , Theoret"
        },
        {
            "paperId": "7509b472cbe7b1fe71a8fccf60f34cc873d1ab63",
            "title": "Turing computability with neural nets"
        },
        {
            "paperId": "090c5a5df345ab60c41d6de02b3e366e1a27cf43",
            "title": "A logical calculus of the ideas immanent in nervous activity"
        },
        {
            "paperId": null,
            "title": "``On Connectionist Models of Natural Language Processing,'"
        },
        {
            "paperId": "41a88a490d7ba9e383ecb16c4290083413a08258",
            "title": "Introduction to Automata Theory, Languages and Computation"
        },
        {
            "paperId": "1c771595221341d5b0647921b26e229fc7887395",
            "title": "Computation: Finite and Infinite Machines"
        },
        {
            "paperId": null,
            "title": "Universal neural networks, manuscript"
        }
    ]
}