{
    "paperId": "d193675b92fbfbf22ed82fda35cd2e73587e33bd",
    "externalIds": {
        "DBLP": "journals/corr/abs-2306-12929",
        "ArXiv": "2306.12929",
        "DOI": "10.48550/arXiv.2306.12929",
        "CorpusId": 259224568
    },
    "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
    "abstract": "Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a\"no-op\"or just a partial update of the residual. To achieve the exact zeros needed in the attention matrix for a no-update, the input to the softmax is pushed to be larger and larger during training, causing outliers in other parts of the network. Based on these observations, we propose two simple (independent) modifications to the attention mechanism - clipped softmax and gated attention. We empirically show that models pre-trained using our methods learn significantly smaller outliers while maintaining and sometimes even improving the floating-point task performance. This enables us to quantize transformers to full INT8 quantization of the activations without any additional effort. We demonstrate the effectiveness of our methods on both language models (BERT, OPT) and vision transformers.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "referenceCount": 80,
    "citationCount": 45,
    "influentialCitationCount": 6,
    "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2306.12929",
        "status": "CLOSED"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is empirically show that models pre-trained using the proposed methods learn significantly smaller outliers while maintaining and sometimes even improving the floating-point task performance, which enables us to quantize transformers to full INT8 quantization of the activations without any additional effort."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.184375286102295,
            -0.5970242023468018,
            0.7390176057815552,
            3.591907262802124,
            1.2194530963897705,
            -0.32484161853790283,
            3.7906041145324707,
            -2.5899441242218018,
            0.9425151944160461,
            -0.4047130346298218,
            -2.0154354572296143,
            4.426300048828125,
            1.270999789237976,
            1.5531855821609497,
            -5.006564140319824,
            -1.3439315557479858,
            -1.3901641368865967,
            1.5928617715835571,
            5.495051860809326,
            3.714245080947876,
            -1.7454822063446045,
            -0.3308401107788086,
            -4.895204067230225,
            1.781629204750061,
            -2.2370195388793945,
            0.514296293258667,
            -2.0597827434539795,
            -1.397085189819336,
            -3.0631415843963623,
            1.550253987312317,
            -1.210123896598816,
            -2.821953296661377,
            5.571071147918701,
            -3.310539722442627,
            3.2500669956207275,
            0.6444544792175293,
            0.32069045305252075,
            7.115793228149414,
            -3.0754611492156982,
            -1.3949084281921387,
            0.8521299362182617,
            1.5408189296722412,
            -0.37648141384124756,
            -0.2587243616580963,
            -2.2295286655426025,
            2.551358938217163,
            2.341169595718384,
            3.442173719406128,
            -0.642655074596405,
            4.469855308532715,
            4.938951015472412,
            1.3740640878677368,
            -2.177760362625122,
            0.4146265387535095,
            -2.5436787605285645,
            -3.946495532989502,
            3.1921167373657227,
            -1.0553690195083618,
            0.8488696813583374,
            -2.6193056106567383,
            4.912989616394043,
            5.328928470611572,
            1.8486628532409668,
            0.4716501235961914,
            3.65092396736145,
            0.628455400466919,
            -0.13999024033546448,
            4.114434242248535,
            1.8604077100753784,
            1.5268402099609375,
            0.8491983413696289,
            -7.463335990905762,
            3.0244877338409424,
            0.3498130738735199,
            -6.135187149047852,
            1.5845835208892822,
            0.2580527067184448,
            -8.754679679870605,
            -0.697094738483429,
            -3.2891650199890137,
            -1.1162962913513184,
            4.37086296081543,
            -2.712617874145508,
            4.801671028137207,
            2.565502405166626,
            -1.0813941955566406,
            -3.2933905124664307,
            -1.264672040939331,
            1.963517427444458,
            -3.227477550506592,
            -2.3589723110198975,
            0.954672634601593,
            -0.5960794687271118,
            1.3331869840621948,
            -6.205506324768066,
            -0.9141743779182434,
            -0.8615874648094177,
            -2.525068521499634,
            -2.6232082843780518,
            2.0621914863586426,
            1.9656407833099365,
            -0.6841388940811157,
            2.716625928878784,
            -1.4835455417633057,
            3.1677591800689697,
            -4.354435920715332,
            -1.571545958518982,
            0.1009904146194458,
            0.7608078718185425,
            -2.0983998775482178,
            -1.4674098491668701,
            3.394244432449341,
            -2.3047232627868652,
            0.8079133033752441,
            -1.6559803485870361,
            -0.9640817046165466,
            -2.4763338565826416,
            1.2263660430908203,
            1.2657337188720703,
            1.5277719497680664,
            -1.693287968635559,
            -1.2826350927352905,
            -5.223415851593018,
            -1.5933691263198853,
            -0.37035834789276123,
            3.3617334365844727,
            -0.6865828037261963,
            2.8690547943115234,
            0.6212822794914246,
            -1.5547605752944946,
            1.6839748620986938,
            -4.54482889175415,
            2.465771198272705,
            -0.25107046961784363,
            2.830334186553955,
            5.586336135864258,
            -2.6254048347473145,
            0.9652789235115051,
            -2.8508148193359375,
            -0.7400616407394409,
            1.634031057357788,
            0.03546246886253357,
            4.30084228515625,
            1.4961198568344116,
            1.8603479862213135,
            4.989286422729492,
            0.7771522402763367,
            1.82478928565979,
            0.9693690538406372,
            7.224079608917236,
            3.3560166358947754,
            -2.272089719772339,
            2.973912239074707,
            3.8619351387023926,
            2.2957210540771484,
            3.20664119720459,
            -3.5940592288970947,
            2.451568365097046,
            -0.9695043563842773,
            0.9462630748748779,
            -1.0919781923294067,
            0.149483323097229,
            -10.958291053771973,
            -3.253096580505371,
            5.1256937980651855,
            -2.8771066665649414,
            -1.6700184345245361,
            2.943589687347412,
            -0.902942955493927,
            2.6522634029388428,
            -0.9882841110229492,
            2.580725908279419,
            -0.07128128409385681,
            4.121316909790039,
            4.438930034637451,
            7.895390510559082,
            3.0647010803222656,
            -5.676565647125244,
            -4.3627848625183105,
            -1.3663337230682373,
            -1.931066632270813,
            -1.6393693685531616,
            -6.316380500793457,
            1.0291805267333984,
            -3.3225841522216797,
            -3.6619551181793213,
            -2.059450626373291,
            -2.2768054008483887,
            -2.2744975090026855,
            -2.98946475982666,
            1.229763388633728,
            0.9887861609458923,
            3.57151198387146,
            4.162363052368164,
            2.6705994606018066,
            -1.4381353855133057,
            2.3572206497192383,
            1.3860588073730469,
            -5.164949893951416,
            -2.89831280708313,
            3.1780269145965576,
            1.2787930965423584,
            -1.095677375793457,
            0.5418886542320251,
            2.569847583770752,
            0.7891578674316406,
            -4.0958428382873535,
            2.0529088973999023,
            4.865549087524414,
            1.308704137802124,
            2.0677006244659424,
            -2.3523309230804443,
            -2.553492307662964,
            1.9811410903930664,
            -5.5127644538879395,
            -2.5852179527282715,
            -6.375110626220703,
            5.009400844573975,
            3.465594530105591,
            2.960968494415283,
            -2.0984230041503906,
            1.1570167541503906,
            0.31628161668777466,
            -2.93904709815979,
            3.5875096321105957,
            -0.6306477189064026,
            2.2273120880126953,
            -0.6919656991958618,
            2.23905086517334,
            1.4062455892562866,
            -0.5048797726631165,
            -4.7698564529418945,
            1.8747864961624146,
            -2.892324209213257,
            -7.27591609954834,
            -1.8123979568481445,
            -4.2338972091674805,
            1.7021714448928833,
            -0.02369600534439087,
            -1.251274585723877,
            6.691644668579102,
            1.3748195171356201,
            2.6920595169067383,
            3.154294490814209,
            2.070484161376953,
            -6.918421745300293,
            1.206986904144287,
            -0.21631211042404175,
            1.0784356594085693,
            -4.575838088989258,
            -1.799320101737976,
            -0.03389623761177063,
            4.221902847290039,
            -2.199918746948242,
            5.021499156951904,
            2.2493767738342285,
            3.5754458904266357,
            -1.6923282146453857,
            0.6246112585067749,
            1.440671443939209,
            -1.6501295566558838,
            7.69930362701416,
            4.426771640777588,
            2.263218879699707,
            -0.4363822340965271,
            -1.80662202835083,
            -4.687686920166016,
            -0.874417245388031,
            1.2331064939498901,
            3.644911289215088,
            0.3246019184589386,
            0.8171643614768982,
            -2.1482279300689697,
            -3.6801295280456543,
            -1.4998482465744019,
            -5.436298847198486,
            -1.4355721473693848,
            -0.13008078932762146,
            2.0655179023742676,
            4.492483139038086,
            2.4725539684295654,
            -1.476295828819275,
            -2.81217885017395,
            0.8110573291778564,
            -1.5748291015625,
            0.2885701060295105,
            -1.8940231800079346,
            0.7429680824279785,
            -0.044513821601867676,
            -1.6025251150131226,
            -4.036561012268066,
            2.856586456298828,
            -3.644412040710449,
            -2.446523427963257,
            -2.6539039611816406,
            1.8087643384933472,
            5.278104305267334,
            -1.1267797946929932,
            0.7923202514648438,
            -4.155673027038574,
            1.6566262245178223,
            1.5909521579742432,
            3.689086675643921,
            0.4516794979572296,
            0.4539399743080139,
            2.61136531829834,
            -0.4483899176120758,
            -2.7627017498016357,
            -2.548539161682129,
            -5.027223587036133,
            -2.080972671508789,
            -1.6355595588684082,
            5.000203609466553,
            -5.477459907531738,
            1.5894193649291992,
            -1.9058737754821777,
            2.2578186988830566,
            -0.6674488186836243,
            -0.42579635977745056,
            4.149054527282715,
            -1.387194275856018,
            0.2536652088165283,
            -4.268608093261719,
            -5.177563190460205,
            -2.8758127689361572,
            -1.4064669609069824,
            4.309159755706787,
            4.555274963378906,
            -3.6681907176971436,
            4.43975305557251,
            0.18008673191070557,
            4.244893550872803,
            5.020955562591553,
            2.2004339694976807,
            0.017798244953155518,
            -4.773001670837402,
            1.2725129127502441,
            -0.7660118937492371,
            -0.5887891054153442,
            4.043725967407227,
            -1.5679434537887573,
            3.9149796962738037,
            1.5523158311843872,
            0.7151919603347778,
            0.19320765137672424,
            0.27550992369651794,
            4.8054094314575195,
            2.416919708251953,
            0.43206536769866943,
            -1.7939220666885376,
            -1.5751887559890747,
            1.6268525123596191,
            3.7350945472717285,
            -4.763278007507324,
            2.5171890258789062,
            4.301219940185547,
            3.8242526054382324,
            0.05792778730392456,
            1.538611888885498,
            3.5180535316467285,
            1.4163132905960083,
            -0.25880467891693115,
            1.2505314350128174,
            -2.6035866737365723,
            -3.395904779434204,
            -3.47480845451355,
            13.110980033874512,
            -2.3498005867004395,
            0.43831419944763184,
            -3.499866008758545,
            -0.5322688817977905,
            -2.3570010662078857,
            -4.9993815422058105,
            3.0156872272491455,
            -0.7826669216156006,
            -1.487510323524475,
            -0.20582985877990723,
            -4.056638717651367,
            2.426314353942871,
            -0.9309345483779907,
            1.4450323581695557,
            6.4969635009765625,
            3.09208083152771,
            3.922893524169922,
            -2.8945717811584473,
            1.2847235202789307,
            -3.7981865406036377,
            2.546726703643799,
            2.156407594680786,
            -1.3029534816741943,
            -3.9866156578063965,
            2.146629810333252,
            1.5903819799423218,
            1.7062735557556152,
            -2.2542977333068848,
            -4.998246192932129,
            -4.412519454956055,
            -2.587606430053711,
            0.4146946668624878,
            1.2809736728668213,
            1.2474654912948608,
            -0.6821939945220947,
            6.526961326599121,
            6.126956939697266,
            -6.3287553787231445,
            -1.164093017578125,
            4.760727882385254,
            -0.2342299222946167,
            0.2972670793533325,
            2.028979778289795,
            -4.570982456207275,
            -3.7039265632629395,
            0.7678086757659912,
            -2.373041868209839,
            3.4065804481506348,
            -2.1677980422973633,
            2.203592538833618,
            5.015761852264404,
            2.4861361980438232,
            2.885472059249878,
            -2.181457757949829,
            3.5652499198913574,
            4.502506256103516,
            0.9433383345603943,
            -4.03067684173584,
            0.6873515844345093,
            3.5329606533050537,
            0.9712411761283875,
            -3.5270168781280518,
            2.50120210647583,
            -3.4372899532318115,
            5.002226829528809,
            -2.9072344303131104,
            -0.6938973665237427,
            -0.2001657485961914,
            2.4268364906311035,
            1.0890295505523682,
            0.44320881366729736,
            1.2792729139328003,
            -0.515872597694397,
            2.158752918243408,
            4.5777668952941895,
            -1.585105061531067,
            6.252943992614746,
            -1.3421423435211182,
            1.9718024730682373,
            1.6203629970550537,
            3.4814229011535645,
            -1.3368523120880127,
            -5.25799036026001,
            1.247572898864746,
            -2.5657846927642822,
            -2.0353472232818604,
            -2.933380126953125,
            1.3589602708816528,
            1.4484484195709229,
            -2.6266398429870605,
            -1.9184374809265137,
            -1.9586749076843262,
            -1.8637876510620117,
            -3.111238479614258,
            2.4758076667785645,
            -1.4714678525924683,
            -2.1413230895996094,
            1.2128576040267944,
            2.8882532119750977,
            -0.9885296821594238,
            -5.878460884094238,
            -1.085739254951477,
            1.438666820526123,
            1.928824782371521,
            1.4891796112060547,
            -3.0562775135040283,
            -0.01564502716064453,
            0.9265080690383911,
            -1.8391575813293457,
            2.9965498447418213,
            2.1110692024230957,
            -1.7932655811309814,
            -6.184717178344727,
            -4.155692100524902,
            0.6143361926078796,
            2.3393044471740723,
            -1.9214105606079102,
            -1.61758553981781,
            5.9883270263671875,
            5.1572089195251465,
            3.7494513988494873,
            3.463235855102539,
            3.2331881523132324,
            -0.3520934581756592,
            1.2674212455749512,
            5.608522891998291,
            -2.8098974227905273,
            0.6147739887237549,
            0.35759174823760986,
            -4.7069220542907715,
            2.537642002105713,
            0.9495847821235657,
            1.3030571937561035,
            -4.6701202392578125,
            -6.317772388458252,
            0.06941716372966766,
            -3.143906831741333,
            -3.403484344482422,
            2.5938189029693604,
            1.4802346229553223,
            2.8738396167755127,
            -3.4870550632476807,
            -1.6624040603637695,
            1.7049070596694946,
            1.6552197933197021,
            -6.3221282958984375,
            3.072690725326538,
            0.32960987091064453,
            0.6993381977081299,
            4.073861122131348,
            -0.4586760401725769,
            0.44264477491378784,
            2.3089375495910645,
            -0.9855560064315796,
            -1.7550957202911377,
            -1.8169673681259155,
            5.155527114868164,
            0.367769330739975,
            1.4632426500320435,
            -4.084043502807617,
            1.4531865119934082,
            -0.48800623416900635,
            3.8847568035125732,
            6.7323384284973145,
            3.3153574466705322,
            3.244534969329834,
            -1.6442463397979736,
            -0.7664257287979126,
            -4.466227054595947,
            0.4320985674858093,
            4.809467315673828,
            -5.840785026550293,
            1.3724873065948486,
            1.2099415063858032,
            3.8260788917541504,
            -2.9647908210754395,
            3.295464515686035,
            -2.8165459632873535,
            2.973332166671753,
            -3.873729705810547,
            -2.256361484527588,
            -4.947145462036133,
            0.8681640625,
            -0.4619116485118866,
            -0.16272848844528198,
            1.3063712120056152,
            -3.0275299549102783,
            -3.02756667137146,
            -2.109675168991089,
            -4.5706467628479,
            -1.5957434177398682,
            -1.9689762592315674,
            2.8607258796691895,
            -0.24221596121788025,
            -1.5984718799591064,
            -0.6698353290557861,
            4.40393590927124,
            -4.4031267166137695,
            -0.006452970206737518,
            -1.2465107440948486,
            3.3894057273864746,
            -1.521677851676941,
            -6.117751121520996,
            1.7487413883209229,
            -0.8460990190505981,
            -0.04267463460564613,
            3.409114360809326,
            2.420304775238037,
            3.821286678314209,
            1.1964616775512695,
            3.063748359680176,
            -1.4250518083572388,
            1.134006142616272,
            -0.21107059717178345,
            -2.50854754447937,
            -2.106095790863037,
            -4.402895927429199,
            1.3242993354797363,
            -5.713866233825684,
            -4.7052717208862305,
            -1.296204686164856,
            -4.2393646240234375,
            -2.0417685508728027,
            -0.4298376142978668,
            -2.093862771987915,
            -0.09094545245170593,
            -4.783751487731934,
            -2.605105400085449,
            -1.2158291339874268,
            2.883089065551758,
            -3.3183059692382812,
            1.0906403064727783,
            3.8186206817626953,
            3.331808567047119,
            3.939450263977051,
            5.3130998611450195,
            2.7693545818328857,
            -3.1687612533569336,
            -1.1815481185913086,
            0.9040578007698059,
            0.8702999353408813,
            -2.2045388221740723,
            0.08142265677452087,
            1.3577455282211304,
            4.142515182495117,
            15.237510681152344,
            1.3747483491897583,
            0.12191329896450043,
            -2.6405539512634277,
            2.4484317302703857,
            -7.533346176147461,
            -1.259209156036377,
            2.3809008598327637,
            1.3904529809951782,
            1.186898946762085,
            -1.5576163530349731,
            -1.790679931640625,
            1.491498589515686,
            0.04053854942321777,
            0.3008686304092407,
            -2.265354871749878,
            -2.7814035415649414,
            3.142758846282959,
            -2.6626791954040527,
            -0.29596078395843506,
            -1.8540501594543457,
            3.950852155685425,
            -2.1518054008483887,
            0.7325135469436646,
            -0.9694865942001343,
            0.89776611328125,
            1.5306637287139893,
            3.928717613220215,
            -4.929009437561035,
            1.4528254270553589,
            4.124852657318115,
            -0.31924742460250854,
            0.09144371747970581,
            -0.09059756994247437,
            -4.186465263366699,
            6.380674839019775,
            0.845583975315094,
            -2.330819845199585,
            3.409478187561035,
            3.271022319793701,
            -2.799543857574463,
            0.8514166474342346,
            -5.898593425750732,
            1.8684308528900146,
            -0.8898578882217407,
            -0.5915753245353699,
            2.0158350467681885,
            -1.9007811546325684,
            -3.517282009124756,
            4.221778869628906,
            0.7090439200401306,
            -0.5645186305046082,
            -0.04171180725097656,
            0.32611462473869324,
            1.2239859104156494,
            4.262686252593994,
            0.636228084564209,
            -1.5733921527862549,
            0.9541146755218506,
            0.2137012779712677,
            4.136704444885254,
            1.7633535861968994,
            0.16430777311325073,
            -2.35760235786438,
            -0.11477947235107422,
            0.44285640120506287,
            -5.125097751617432,
            3.245150089263916,
            4.000646591186523,
            2.27820086479187,
            4.246013164520264,
            1.1134731769561768,
            -3.0366528034210205,
            -1.7358503341674805,
            -0.7544317245483398,
            -5.187420845031738,
            2.1586103439331055,
            0.9042636752128601,
            -0.8949052095413208,
            8.083318710327148,
            -5.7290449142456055,
            3.478849411010742,
            -2.9966306686401367,
            1.8526222705841064,
            6.4499664306640625,
            -4.720959663391113,
            7.1915082931518555,
            -2.4437198638916016,
            -0.9972181916236877,
            6.5944108963012695,
            -1.6089751720428467,
            -0.5257319808006287,
            3.7262558937072754,
            2.795933485031128,
            3.6966655254364014,
            -2.571025848388672,
            -3.8974249362945557,
            -4.628670692443848,
            -4.631099224090576,
            -5.798684120178223,
            8.372184753417969,
            5.9578094482421875,
            4.3398942947387695,
            -2.825540065765381,
            -0.5196326971054077,
            -1.7991560697555542,
            -0.8051562309265137,
            -3.954460382461548,
            -5.184503078460693,
            -1.3369112014770508,
            3.569575786590576,
            -4.003454208374023,
            -3.076916217803955,
            0.8235159516334534,
            0.22559374570846558,
            -0.16714143753051758,
            -0.628620445728302,
            4.7559943199157715,
            1.6959242820739746,
            3.8413965702056885,
            0.38205498456954956,
            -2.33958101272583,
            -4.646703243255615,
            -3.3540029525756836,
            -4.969364166259766,
            3.8945841789245605,
            1.4997131824493408,
            -1.4933557510375977,
            -0.6062506437301636,
            -2.0566632747650146,
            -0.132973313331604,
            -0.08118393272161484,
            -3.941206455230713,
            -2.271634578704834,
            -2.808438301086426,
            -3.1551260948181152,
            1.3665478229522705,
            3.971196174621582,
            -4.108151435852051,
            2.017714262008667,
            3.096254825592041,
            -2.0252480506896973,
            -5.419228553771973,
            9.311473846435547,
            -2.074730396270752,
            -0.37287473678588867,
            -3.5438406467437744,
            -1.3007732629776,
            -4.4494452476501465,
            -1.2373639345169067,
            -0.39377260208129883,
            -1.0367177724838257,
            4.364396572113037,
            -4.013704299926758,
            0.9338268041610718,
            -3.223940134048462
        ]
    },
    "authors": [
        {
            "authorId": "2112207572",
            "name": "Yelysei Bondarenko"
        },
        {
            "authorId": "41229153",
            "name": "Markus Nagel"
        },
        {
            "authorId": "83133279",
            "name": "Tijmen Blankevoort"
        }
    ],
    "references": [
        {
            "paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9",
            "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
        },
        {
            "paperId": "3f2a55671ea1993893143811c3aa0b23323c0a52",
            "title": "FP8 versus INT8 for efficient deep learning inference"
        },
        {
            "paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1",
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"
        },
        {
            "paperId": "f7655ff2b53e4c87645d85297f994d33e7e4a92b",
            "title": "Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6",
            "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
        },
        {
            "paperId": "3f6243097a58e386aea1215fed4f372dee07a100",
            "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
            "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models"
        },
        {
            "paperId": "d2f63b56fc6bc373f5c023454c2b253326962865",
            "title": "DeiT III: Revenge of the ViT"
        },
        {
            "paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
            "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
        },
        {
            "paperId": "cddf40e579a596d0110b260313adf43470617c4c",
            "title": "Datasets: A Community Library for Natural Language Processing"
        },
        {
            "paperId": "8a0a7170977cf5c94d9079b351562077b78df87a",
            "title": "A White Paper on Neural Network Quantization"
        },
        {
            "paperId": "773472a423d176fedfc46ba03287ed41995c99f7",
            "title": "How Do BERT Embeddings Organize Linguistic Knowledge?"
        },
        {
            "paperId": "5a09edeb26f9f116f2c0503cd020f38fb943f79b",
            "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers"
        },
        {
            "paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325",
            "title": "Going deeper with Image Transformers"
        },
        {
            "paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e",
            "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
        },
        {
            "paperId": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3",
            "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"
        },
        {
            "paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8",
            "title": "I-BERT: Integer-only BERT Quantization"
        },
        {
            "paperId": "1aa219b2069de522081980d2d9d764958197e545",
            "title": "Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "31a68965ea4af87295cb83d8fe72e31f651a4ee1",
            "title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming"
        },
        {
            "paperId": "0c0dfe47afcec2e229015f3c8f213d4c88e86b28",
            "title": "Up or Down? Adaptive Rounding for Post-Training Quantization"
        },
        {
            "paperId": "8cc855854384755336aa05c5376ea455137bfbce",
            "title": "LSQ+: Improving low-bit quantization through learnable offsets and better initialization"
        },
        {
            "paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39",
            "title": "Training with Quantization Noise for Extreme Model Compression"
        },
        {
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
        },
        {
            "paperId": "9807032064b278b72611ddbe504a3ce396d33517",
            "title": "Robust Quantization: One Model to Rule Them All"
        },
        {
            "paperId": "0acc25f3993bd9431418ae275aa12a536b740b77",
            "title": "ZeroQ: A Novel Zero Shot Quantization Framework"
        },
        {
            "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        },
        {
            "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "c95383f251a62c63217586059c67f63507c3e839",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39",
            "title": "Randaugment: Practical automated data augmentation with a reduced search space"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103",
            "title": "Revealing the Dark Secrets of BERT"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
            "paperId": "d77123b54dcc8014949584ab624e97298617bcad",
            "title": "Data-Free Quantization Through Weight Equalization and Bias Correction"
        },
        {
            "paperId": "95a251513853c6032bdecebd4b74e15795662986",
            "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"
        },
        {
            "paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c",
            "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"
        },
        {
            "paperId": "dc160709bbe528b506a37ead334f60d258413357",
            "title": "Learned Step Size Quantization"
        },
        {
            "paperId": "47a1edfb88f5b4a7ba1e9f6aed327f67f942f6d6",
            "title": "Low-bit Quantization of Neural Networks for Efficient Inference"
        },
        {
            "paperId": "63521e29aacc8c07bcb8476389f9b0cc247802bd",
            "title": "Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization"
        },
        {
            "paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235",
            "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"
        },
        {
            "paperId": "f789425a7af1d012675118d7d10cd50afad09074",
            "title": "Post training 4-bit quantization of convolutional networks for rapid-deployment"
        },
        {
            "paperId": "3d8b62c060f8444907e7c975c6ae590373b51ed4",
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294",
            "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"
        },
        {
            "paperId": "2ec7156913117949ab933f27f492d0149bc0031f",
            "title": "Learning Sparse Neural Networks through L0 Regularization"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "4feef0fd284feb1233399b400eb897f59ec92755",
            "title": "mixup: Beyond Empirical Risk Minimization"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "54ddb00fa691728944fd8becea90a373d21597cf",
            "title": "Understanding deep learning requires rethinking generalization"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f",
            "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7",
            "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"
        },
        {
            "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
            "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
        },
        {
            "paperId": "b7cf49e30355633af2db19f35189410c8515e91f",
            "title": "Deep Learning with Limited Numerical Precision"
        },
        {
            "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "title": "Going deeper with convolutions"
        },
        {
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge"
        },
        {
            "paperId": "947620a1854655ed91a86b90d12695e05be85983",
            "title": "1.1 Computing's energy problem (and what we can do about it)"
        },
        {
            "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "8264257f573696fc0a1ef7531c825041832197f8",
            "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases"
        },
        {
            "paperId": "81051b830a4f5606106765902a51ba281c9230f9",
            "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"
        },
        {
            "paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312",
            "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "paperId": null,
            "title": "busters: Outlier dimensions that disrupt transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP"
        },
        {
            "paperId": null,
            "title": "Electra: Pre-training text encoders as discriminators rather than generators"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "Pytorch image models"
        },
        {
            "paperId": null,
            "title": "using the linear G adds the compute overhead between 3% and 8% , depending on the model"
        },
        {
            "paperId": null,
            "title": "Accelerate: Training and inference at scale made simple, efficient and adaptable"
        },
        {
            "paperId": null,
            "title": "Attention layer #10, data sequence #88"
        }
    ]
}