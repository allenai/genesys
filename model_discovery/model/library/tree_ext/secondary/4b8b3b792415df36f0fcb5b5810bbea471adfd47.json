{
    "paperId": "4b8b3b792415df36f0fcb5b5810bbea471adfd47",
    "externalIds": {
        "MAG": "2583687993",
        "ArXiv": "1701.08718",
        "DBLP": "journals/corr/GulcehreCB17",
        "CorpusId": 12749186
    },
    "title": "Memory Augmented Neural Networks with Wormhole Connections",
    "abstract": "Recent empirical results on long-term dependency tasks have shown that neural networks augmented with an external memory can learn the long-term dependency tasks more easily and achieve better generalization than vanilla recurrent neural networks (RNN). We suggest that memory augmented neural networks can reduce the effects of vanishing gradients by creating shortcut (or wormhole) connections. Based on this observation, we propose a novel memory augmented neural network model called TARDIS (Temporal Automatic Relation Discovery in Sequences). The controller of TARDIS can store a selective set of embeddings of its own previous hidden states into an external memory and revisit them as and when needed. For TARDIS, memory acts as a storage for wormhole connections to the past to propagate the gradients more effectively and it helps to learn the temporal dependencies. The memory structure of TARDIS has similarities to both Neural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but both read and write operations of TARDIS are simpler and more efficient. We use discrete addressing for read/write operations which helps to substantially to reduce the vanishing gradient problem with very long sequences. Read and write operations in TARDIS are tied with a heuristic once the memory becomes full, and this makes the learning problem simpler when compared to NTM or D-NTM type of architectures. We provide a detailed analysis on the gradient propagation in general for MANNs. We evaluate our models on different long-term dependency tasks and report competitive results in all of them.",
    "venue": "arXiv.org",
    "year": 2017,
    "referenceCount": 50,
    "citationCount": 60,
    "influentialCitationCount": 7,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is suggested that memory augmented neural networks can reduce the effects of vanishing gradients by creating shortcut (or wormhole) connections to propagate the gradients more effectively and it helps to learn the temporal dependencies."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "1854385",
            "name": "\u00c7aglar G\u00fcl\u00e7ehre"
        },
        {
            "authorId": "144631588",
            "name": "A. Chandar"
        },
        {
            "authorId": "1751762",
            "name": "Yoshua Bengio"
        }
    ],
    "references": [
        {
            "paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
            "title": "Improving Neural Language Models with a Continuous Cache"
        },
        {
            "paperId": "29e944711a354c396fad71936f536e83025b6ce0",
            "title": "Categorical Reparameterization with Gumbel-Softmax"
        },
        {
            "paperId": "515a21e90117941150923e559729c59f5fdade1c",
            "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"
        },
        {
            "paperId": "be8c6c69f3e357bfad2987e45b62cff7e7474378",
            "title": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes"
        },
        {
            "paperId": "784ee73d5363c711118f784428d1ab89f019daa5",
            "title": "Hybrid computing using a neural network with dynamic external memory"
        },
        {
            "paperId": "563783de03452683a9206e85fe6d661714436686",
            "title": "HyperNetworks"
        },
        {
            "paperId": "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2",
            "title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference"
        },
        {
            "paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
            "title": "Hierarchical Multiscale Recurrent Neural Networks"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
            "title": "Recurrent Highway Networks"
        },
        {
            "paperId": "906ac7584faf8ead6004be4cc5122320c87df59c",
            "title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes"
        },
        {
            "paperId": "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5",
            "title": "Natural Language Comprehension with the EpiReader"
        },
        {
            "paperId": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105",
            "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"
        },
        {
            "paperId": "c17b6f2d9614878e3f860c187f72a18ffb5aabb6",
            "title": "Hierarchical Memory Networks"
        },
        {
            "paperId": "bbd0e204f48a45735e1065c8b90b298077b73192",
            "title": "One-shot Learning with Memory-Augmented Neural Networks"
        },
        {
            "paperId": "6b570069f14c7588e066f7138e1f21af59d62e61",
            "title": "Theano: A Python framework for fast computation of mathematical expressions"
        },
        {
            "paperId": "952454718139dba3aafc6b3b67c4f514ac3964af",
            "title": "Recurrent Batch Normalization"
        },
        {
            "paperId": "cf76789618f5db929393c1187514ce6c3502c3cd",
            "title": "Recurrent Dropout without Memory Loss"
        },
        {
            "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
            "title": "Long Short-Term Memory-Networks for Machine Reading"
        },
        {
            "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
            "title": "Neural GPUs Learn Algorithms"
        },
        {
            "paperId": "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45",
            "title": "Reasoning about Entailment with Neural Attention"
        },
        {
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference"
        },
        {
            "paperId": "17f5c7411eeeeedf25b0db99a9130aa353aee4ba",
            "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models"
        },
        {
            "paperId": "1b69dad5f287c402befe157fc755fa4f34d14e81",
            "title": "On Singular Value Inequalities for the Sum of Two Matrices"
        },
        {
            "paperId": "e837b79de602c69395498c1fbbe39bbb4e6f75ad",
            "title": "Learning to Transduce with Unbounded Memory"
        },
        {
            "paperId": "6e565308c8081e807709cb4a917443b737e6cdb4",
            "title": "Large-scale Simple Question Answering with Memory Networks"
        },
        {
            "paperId": "c9d7afd65b2127e6f0651bc7e13eeec1897ac8dd",
            "title": "Reinforcement Learning Neural Turing Machines"
        },
        {
            "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "title": "End-To-End Memory Networks"
        },
        {
            "paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82",
            "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"
        },
        {
            "paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
            "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de",
            "title": "Neural Turing Machines"
        },
        {
            "paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39",
            "title": "Memory Networks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
        },
        {
            "paperId": "5522764282c85aea422f1c4dc92ff7e0ca6987bc",
            "title": "A Clockwork RNN"
        },
        {
            "paperId": "331f0fb3b6176c6e463e0401025b04f6ace9ccd3",
            "title": "Neural Variational Inference and Learning in Belief Networks"
        },
        {
            "paperId": "533ee188324b833e059cb59b654e6160776d5812",
            "title": "How to Construct Deep Recurrent Neural Networks"
        },
        {
            "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
            "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
        },
        {
            "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "title": "On the difficulty of training recurrent neural networks"
        },
        {
            "paperId": "93c20e38c85b69fc2d2eb314b3c1217913f7db11",
            "title": "Generating Text with Recurrent Neural Networks"
        },
        {
            "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
            "title": "Long Short-Term Memory"
        },
        {
            "paperId": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "title": "Learning long-term dependencies with gradient descent is difficult"
        },
        {
            "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "title": "Building a Large Annotated Corpus of English: The Penn Treebank"
        },
        {
            "paperId": "be1fed9544830df1137e72b1d2396c40d3e18365",
            "title": "A Cache-Based Natural Language Model for Speech Recognition"
        },
        {
            "paperId": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb",
            "title": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"
        },
        {
            "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
        },
        {
            "paperId": "781d202358e9bb65985cc9da49825e0a4af60ba4",
            "title": "Chronesthesia: Conscious awareness of subjective time."
        },
        {
            "paperId": "43009d5666f8ce225352462868b11dfaa3a8d48b",
            "title": "Incremental sequence learning"
        },
        {
            "paperId": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "title": "Untersuchungen zu dynamischen neuronalen Netzen"
        }
    ]
}