{
    "paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
    "externalIds": {
        "MAG": "1498436455",
        "DOI": "10.1038/323533a0",
        "CorpusId": 205001834
    },
    "title": "Learning representations by back-propagating errors",
    "abstract": null,
    "venue": "Nature",
    "year": 1986,
    "referenceCount": 2,
    "citationCount": 25529,
    "influentialCitationCount": 787,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
    },
    "embedding": null,
    "authors": [
        {
            "authorId": "2681887",
            "name": "D. Rumelhart"
        },
        {
            "authorId": "1695689",
            "name": "Geoffrey E. Hinton"
        },
        {
            "authorId": "2116648700",
            "name": "Ronald J. Williams"
        }
    ],
    "references": [
        {
            "paperId": "4bb08f30bb2c83334b21fbbc68c3f2622d4fb04b",
            "title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
        },
        {
            "paperId": "cccc0a4817fd5f6d8758c66b4065a23897d49f1d",
            "title": "Principles of neurodynamics"
        }
    ]
}