{
    "paperId": "385c363ea8e450f362d389f401beaeb5b42a0022",
    "externalIds": {
        "DBLP": "conf/icml/ZhaiLLBR0GS23",
        "ArXiv": "2303.06296",
        "DOI": "10.48550/arXiv.2303.06296",
        "CorpusId": 257496258
    },
    "title": "Stabilizing Transformer Training by Preventing Attention Entropy Collapse",
    "abstract": "Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\\textit{entropy collapse}$. As a remedy, we propose $\\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with $\\sigma$Reparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer {to competitive performance} without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at \\url{https://github.com/apple/ml-sigma-reparam}.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 71,
    "citationCount": 30,
    "influentialCitationCount": 5,
    "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2303.06296",
        "status": "GREEN"
    },
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work investigates the training dynamics of Transformers by examining the evolution of the attention layers, and shows that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training a Vision Transformer without warmup, weight decay, layer normalization or adaptive optimizers."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -4.423778533935547,
            -0.845220685005188,
            4.969202041625977,
            3.9538426399230957,
            0.06917795538902283,
            -1.3114070892333984,
            3.705446720123291,
            -3.5302062034606934,
            2.333890676498413,
            -1.7968909740447998,
            -2.655489444732666,
            5.07493257522583,
            0.7541648745536804,
            0.7557913064956665,
            -2.7805557250976562,
            -2.422816753387451,
            -4.09025764465332,
            1.5206525325775146,
            1.494568109512329,
            0.9041824340820312,
            -0.5183656811714172,
            1.9496045112609863,
            -4.720468044281006,
            -1.2400113344192505,
            -1.4521838426589966,
            -1.195380449295044,
            4.139264106750488,
            -1.8002865314483643,
            -2.988546371459961,
            -0.8437278270721436,
            -1.2378491163253784,
            -7.025331497192383,
            6.065831184387207,
            -1.2827677726745605,
            1.8925970792770386,
            -0.6921253204345703,
            -0.9196968078613281,
            9.494293212890625,
            -2.515507459640503,
            -3.6876838207244873,
            -1.6188437938690186,
            1.688584566116333,
            0.013689607381820679,
            1.5660881996154785,
            0.6689855456352234,
            1.085568904876709,
            4.343032360076904,
            4.936130046844482,
            -1.7871713638305664,
            5.716489791870117,
            5.472679138183594,
            0.7420797348022461,
            -2.1543240547180176,
            2.126911163330078,
            -0.29798394441604614,
            -0.7880895733833313,
            0.7736371755599976,
            -1.6207468509674072,
            1.1864385604858398,
            0.053271323442459106,
            4.164525032043457,
            1.0370709896087646,
            1.0688751935958862,
            -4.350776672363281,
            0.6447125673294067,
            0.27301469445228577,
            -2.564271926879883,
            3.4544870853424072,
            2.4274260997772217,
            3.61222243309021,
            -0.5119820833206177,
            -8.700506210327148,
            0.39042848348617554,
            0.7303280830383301,
            -5.529189109802246,
            0.26450783014297485,
            -0.5398409962654114,
            -9.56804084777832,
            -0.3794412612915039,
            -1.028661847114563,
            -1.825084924697876,
            2.567201852798462,
            0.2841578722000122,
            0.2586871087551117,
            5.1525726318359375,
            -1.1887309551239014,
            -0.3155527114868164,
            -0.64183509349823,
            3.4836339950561523,
            -1.0945589542388916,
            -1.0391249656677246,
            0.45280012488365173,
            -0.1021498292684555,
            4.750246524810791,
            -3.5170540809631348,
            -0.05327107012271881,
            0.39033815264701843,
            0.3584720492362976,
            -3.8443174362182617,
            -0.15831798315048218,
            1.6141750812530518,
            0.4312513470649719,
            2.6126182079315186,
            0.16368913650512695,
            2.2332491874694824,
            -2.2270495891571045,
            -3.5311360359191895,
            -0.5668222308158875,
            1.4528632164001465,
            -1.6617045402526855,
            -2.6402969360351562,
            3.4125332832336426,
            -2.4069595336914062,
            -1.7133724689483643,
            -2.2249364852905273,
            -5.091729164123535,
            -3.163119316101074,
            2.433426856994629,
            3.2050063610076904,
            3.473397731781006,
            -2.401724338531494,
            0.02051994949579239,
            -2.338754177093506,
            1.1421245336532593,
            -1.4916646480560303,
            2.675776481628418,
            -0.22517478466033936,
            4.458282947540283,
            0.7833694219589233,
            -3.0810129642486572,
            -0.43597352504730225,
            -0.022018074989318848,
            0.17476585507392883,
            -0.7593675255775452,
            5.728204727172852,
            2.4024062156677246,
            -2.424126386642456,
            3.7222886085510254,
            -2.2171268463134766,
            -1.5992199182510376,
            0.7338107228279114,
            2.453291177749634,
            0.8974460363388062,
            0.03177496790885925,
            5.25882625579834,
            2.7893755435943604,
            -1.5761181116104126,
            1.643609642982483,
            2.244633913040161,
            4.208668231964111,
            0.09727713465690613,
            -1.1794604063034058,
            0.02466335892677307,
            4.159689903259277,
            1.3485503196716309,
            3.040144205093384,
            -3.331294536590576,
            3.69624924659729,
            -3.6586780548095703,
            -0.8537030816078186,
            -0.8314688801765442,
            -0.3380007743835449,
            -11.625770568847656,
            2.0087227821350098,
            3.017120122909546,
            -3.0336453914642334,
            -1.0742353200912476,
            0.23233306407928467,
            -0.7548307180404663,
            3.0265684127807617,
            0.994804859161377,
            2.632425308227539,
            1.1880090236663818,
            1.556238055229187,
            4.102872371673584,
            7.033397197723389,
            3.091787815093994,
            -4.843843936920166,
            -2.0630624294281006,
            0.5514477491378784,
            -4.369077682495117,
            -4.519543170928955,
            -4.8837666511535645,
            0.9815141558647156,
            -3.0888638496398926,
            -3.5877814292907715,
            -3.3162946701049805,
            -2.855827808380127,
            -2.549067974090576,
            -1.5988168716430664,
            2.09641695022583,
            0.1872696876525879,
            6.366121768951416,
            5.954440116882324,
            2.1766648292541504,
            0.8839429616928101,
            0.31740447878837585,
            1.654063105583191,
            -0.6757224202156067,
            -1.170713186264038,
            2.855851411819458,
            4.059289455413818,
            -1.966421127319336,
            -3.6918654441833496,
            2.682878255844116,
            0.12974312901496887,
            -3.6486458778381348,
            4.3867998123168945,
            3.039154291152954,
            -0.20223796367645264,
            2.9023778438568115,
            -1.1853808164596558,
            -3.9590046405792236,
            -1.1441690921783447,
            -7.107254981994629,
            -1.1474690437316895,
            -6.2972025871276855,
            4.500824451446533,
            0.9271381497383118,
            1.1754443645477295,
            -0.6946909427642822,
            0.7757525444030762,
            -1.8320648670196533,
            -3.01423978805542,
            0.20658835768699646,
            -1.8105823993682861,
            4.241090297698975,
            0.3478880822658539,
            -0.6593875288963318,
            1.1752766370773315,
            -0.32264775037765503,
            -3.442101001739502,
            2.1758744716644287,
            -0.31808188557624817,
            -4.117865085601807,
            -3.1947672367095947,
            -4.668994426727295,
            -0.745189905166626,
            -4.072528839111328,
            -1.4754488468170166,
            5.9952168464660645,
            0.5003563165664673,
            0.6394541263580322,
            7.001137733459473,
            2.3608062267303467,
            -0.6097952723503113,
            -0.5041360855102539,
            0.8475373983383179,
            -1.3733205795288086,
            -4.0769572257995605,
            -1.3153843879699707,
            0.9599337577819824,
            4.605751037597656,
            -5.522271156311035,
            4.026293754577637,
            -0.018028318881988525,
            1.0466521978378296,
            -1.4366244077682495,
            2.614753484725952,
            -0.6099413633346558,
            -0.577173113822937,
            4.500988483428955,
            0.11945413053035736,
            4.3363237380981445,
            0.05485051870346069,
            -2.7750742435455322,
            -3.0009779930114746,
            -3.765397071838379,
            0.2211698591709137,
            0.6492624282836914,
            -0.2681114375591278,
            2.213005781173706,
            -3.6488804817199707,
            -4.617276191711426,
            -2.544285774230957,
            -4.731266498565674,
            -2.7726290225982666,
            0.5034129619598389,
            5.9500837326049805,
            3.415778636932373,
            -3.146695613861084,
            -2.3098433017730713,
            0.3154919147491455,
            2.4151535034179688,
            0.624549388885498,
            -4.04428243637085,
            -2.9616260528564453,
            2.615699291229248,
            0.6358194351196289,
            0.09759604930877686,
            -5.003938674926758,
            3.039499521255493,
            -4.361820697784424,
            -0.07737113535404205,
            0.0323815792798996,
            2.9518237113952637,
            1.166512131690979,
            -1.0082521438598633,
            0.13415369391441345,
            -4.223341464996338,
            -3.475285530090332,
            -1.1168105602264404,
            4.967020511627197,
            2.494415283203125,
            3.365034580230713,
            2.959360122680664,
            2.050546407699585,
            -4.273157119750977,
            -2.0114121437072754,
            -4.5003485679626465,
            -3.375915288925171,
            1.6829040050506592,
            4.185980796813965,
            -2.421290397644043,
            1.0216902494430542,
            -1.1740844249725342,
            -0.8126556277275085,
            0.2489084154367447,
            -1.873958945274353,
            2.7719762325286865,
            -2.1645429134368896,
            -1.0963387489318848,
            -5.084573745727539,
            -7.052520751953125,
            1.9020670652389526,
            -3.642768621444702,
            6.220511436462402,
            2.8155064582824707,
            -2.1902246475219727,
            2.3252782821655273,
            -0.08943787217140198,
            2.2234232425689697,
            1.6865081787109375,
            1.4240014553070068,
            0.7446907162666321,
            -4.621945381164551,
            -1.4119017124176025,
            0.6479095220565796,
            0.598084568977356,
            3.521554946899414,
            -0.6006960868835449,
            3.1888720989227295,
            3.6913928985595703,
            -1.7844493389129639,
            -4.567781448364258,
            0.9034146070480347,
            -0.1171879768371582,
            -4.920456409454346,
            -1.1172566413879395,
            2.1004722118377686,
            -2.193582534790039,
            0.9627113938331604,
            1.8346320390701294,
            -1.6959909200668335,
            2.6587884426116943,
            3.2863497734069824,
            2.82348370552063,
            1.4730329513549805,
            1.3166331052780151,
            4.590135097503662,
            -1.7499943971633911,
            -2.2974298000335693,
            2.058328151702881,
            -3.287780523300171,
            -0.736871600151062,
            1.00619637966156,
            9.9668607711792,
            -0.1263553500175476,
            -2.0155818462371826,
            -8.60633659362793,
            -1.8411377668380737,
            -0.21657121181488037,
            -3.44286847114563,
            4.177945613861084,
            -0.9075776934623718,
            -3.127166509628296,
            1.6037527322769165,
            -4.782477378845215,
            3.2844035625457764,
            0.7330368757247925,
            -1.1052969694137573,
            4.028799057006836,
            4.795848846435547,
            5.243311882019043,
            -4.209821701049805,
            -1.5693050622940063,
            -2.1757946014404297,
            1.8763221502304077,
            1.7156291007995605,
            -0.7926313281059265,
            -1.2026923894882202,
            1.718648910522461,
            0.003960072994232178,
            1.9095628261566162,
            -2.9087791442871094,
            -3.0168070793151855,
            -2.63663911819458,
            -6.751762390136719,
            2.292945146560669,
            4.416601181030273,
            -3.9012162685394287,
            4.930841445922852,
            2.930021286010742,
            3.6117939949035645,
            -1.6905832290649414,
            -2.962563991546631,
            7.285150051116943,
            0.9208904504776001,
            3.4663450717926025,
            2.323176383972168,
            -3.774535894393921,
            -1.5622159242630005,
            -0.5464749336242676,
            -2.8837969303131104,
            3.172999858856201,
            -2.2343766689300537,
            -1.3576695919036865,
            3.8220582008361816,
            5.163321018218994,
            3.6712100505828857,
            -2.6734485626220703,
            2.790792465209961,
            6.912457466125488,
            3.106569528579712,
            -2.9755771160125732,
            3.2481393814086914,
            3.730433702468872,
            4.361867427825928,
            1.0432021617889404,
            -0.6053344011306763,
            -3.006882667541504,
            6.063403606414795,
            -5.8587188720703125,
            -2.2259111404418945,
            2.5958218574523926,
            6.013766765594482,
            -0.17081882059574127,
            0.7584465742111206,
            1.58585524559021,
            -1.3643988370895386,
            4.036742687225342,
            3.4203555583953857,
            -2.8188812732696533,
            4.137304306030273,
            -0.11115600168704987,
            -1.9445027112960815,
            0.5688833594322205,
            1.4190449714660645,
            1.1763471364974976,
            -3.448014497756958,
            1.339937448501587,
            -4.663700103759766,
            -4.935569763183594,
            0.22430109977722168,
            -1.9867637157440186,
            0.9884209632873535,
            -3.6416096687316895,
            -3.393303394317627,
            2.137274742126465,
            1.8455815315246582,
            -2.1769518852233887,
            1.5105204582214355,
            -1.8546791076660156,
            2.3808817863464355,
            0.8017184734344482,
            6.316222667694092,
            -1.2013410329818726,
            -3.5810580253601074,
            -6.5266218185424805,
            0.022481411695480347,
            0.6170923709869385,
            -1.3786420822143555,
            -2.5383050441741943,
            -7.074941635131836,
            2.595573663711548,
            -0.7012406587600708,
            2.656822681427002,
            4.3228559494018555,
            1.2660982608795166,
            -7.010440349578857,
            -4.369554042816162,
            -1.920060634613037,
            1.8061213493347168,
            1.8701255321502686,
            -3.6346123218536377,
            5.4278974533081055,
            7.429710388183594,
            0.8518810272216797,
            2.9889750480651855,
            -0.2532031536102295,
            -1.0524126291275024,
            -0.060759514570236206,
            1.7216603755950928,
            -0.602041482925415,
            5.187798976898193,
            1.1554374694824219,
            -1.3914544582366943,
            0.6911773085594177,
            1.5615429878234863,
            -1.4348301887512207,
            0.06468427181243896,
            1.685145378112793,
            2.1350631713867188,
            -0.8153460621833801,
            -4.542937278747559,
            4.426946640014648,
            1.2479538917541504,
            0.9752980470657349,
            -0.6743011474609375,
            -0.5799791812896729,
            -4.838567733764648,
            4.637673854827881,
            -9.308540344238281,
            3.3518524169921875,
            -0.39759525656700134,
            -1.6362991333007812,
            0.2834451198577881,
            -0.9777167439460754,
            -1.2210808992385864,
            1.6598892211914062,
            -0.4400882124900818,
            2.450650215148926,
            -0.4922525882720947,
            4.234158992767334,
            -1.3794231414794922,
            1.1731681823730469,
            -3.527386426925659,
            1.0917906761169434,
            -0.015131533145904541,
            3.6657633781433105,
            8.060647010803223,
            6.864193439483643,
            3.147695541381836,
            -1.2230600118637085,
            -1.5186805725097656,
            -3.4979681968688965,
            4.27294921875,
            2.194748640060425,
            -6.147862911224365,
            0.01189742237329483,
            4.387027263641357,
            0.20655474066734314,
            3.0100457668304443,
            -4.742775917053223,
            -1.0665125846862793,
            2.8357386589050293,
            -0.7283573150634766,
            -1.3214010000228882,
            -4.125148773193359,
            -1.2747349739074707,
            2.4161033630371094,
            -0.8980197310447693,
            1.3365153074264526,
            -4.779231548309326,
            -2.7830605506896973,
            1.3347501754760742,
            -4.585513114929199,
            0.44009923934936523,
            -1.5278408527374268,
            4.094866752624512,
            4.3238019943237305,
            -0.6438264846801758,
            -0.6723523736000061,
            5.087124824523926,
            -3.5696887969970703,
            -1.7810248136520386,
            -1.6532533168792725,
            1.1509968042373657,
            -1.8688552379608154,
            -0.02298194169998169,
            0.73277348279953,
            -3.255734443664551,
            -1.3484985828399658,
            -1.6285521984100342,
            2.611088752746582,
            6.611111640930176,
            3.689967393875122,
            2.4744386672973633,
            -4.806853294372559,
            0.8643875122070312,
            1.6410493850708008,
            -3.7858543395996094,
            -1.9929252862930298,
            -2.907137155532837,
            4.579893112182617,
            -6.684485912322998,
            -6.648472785949707,
            -0.011769980192184448,
            -1.2995541095733643,
            -1.1150600910186768,
            2.8983945846557617,
            -0.38499534130096436,
            -0.8660633563995361,
            -2.7710437774658203,
            0.196821391582489,
            -1.235630989074707,
            4.196316719055176,
            2.6984949111938477,
            -3.706819772720337,
            2.269594192504883,
            3.8697938919067383,
            1.8370107412338257,
            4.154193878173828,
            0.7586172819137573,
            -1.9277467727661133,
            3.0969510078430176,
            1.5826464891433716,
            2.1491317749023438,
            -2.057793378829956,
            0.9547682404518127,
            3.2915759086608887,
            4.005455493927002,
            15.874327659606934,
            0.606666088104248,
            0.3316497206687927,
            -0.7668209671974182,
            2.0661721229553223,
            -5.4339518547058105,
            -1.837797999382019,
            2.608334541320801,
            1.1710586547851562,
            -1.4707999229431152,
            -5.057931423187256,
            -0.025706004351377487,
            3.552356004714966,
            -1.3218642473220825,
            -0.4964684247970581,
            -1.531968355178833,
            -3.280930519104004,
            1.7519762516021729,
            -1.4537454843521118,
            -1.2525742053985596,
            -1.9538145065307617,
            2.3282034397125244,
            3.2086315155029297,
            3.3774657249450684,
            0.29520469903945923,
            3.2189319133758545,
            -2.439056396484375,
            1.988948941230774,
            -4.499510288238525,
            2.285633087158203,
            1.8851605653762817,
            3.1538896560668945,
            -2.065272331237793,
            1.0645651817321777,
            -3.9805166721343994,
            2.098740816116333,
            5.147265434265137,
            1.7424633502960205,
            5.449958324432373,
            3.2984604835510254,
            -2.9263787269592285,
            -0.023569047451019287,
            -5.860487461090088,
            -0.6769052743911743,
            1.428286075592041,
            -0.07251361757516861,
            2.667210578918457,
            -3.3080859184265137,
            -4.385592460632324,
            2.1352527141571045,
            2.57965087890625,
            3.2366385459899902,
            -3.1591691970825195,
            2.8387017250061035,
            0.8636611700057983,
            4.574294090270996,
            0.7937245965003967,
            1.093116044998169,
            0.7236002683639526,
            5.188610553741455,
            -1.1411247253417969,
            -0.12156882882118225,
            -2.817079544067383,
            -4.4451165199279785,
            1.310611605644226,
            0.43847665190696716,
            -2.7693099975585938,
            5.344091415405273,
            0.7532639503479004,
            2.3708064556121826,
            6.767675399780273,
            -1.3299206495285034,
            1.0551586151123047,
            -2.870633840560913,
            -3.673069477081299,
            -5.008760452270508,
            1.8982858657836914,
            0.013914182782173157,
            -1.3606318235397339,
            7.047881603240967,
            -5.6837615966796875,
            1.9539400339126587,
            -4.493844032287598,
            3.960451126098633,
            6.803727149963379,
            -1.5790680646896362,
            3.500183582305908,
            -0.010173305869102478,
            -3.01440691947937,
            4.431777000427246,
            -0.22387132048606873,
            1.3309003114700317,
            0.5228869915008545,
            0.9996969699859619,
            3.8733744621276855,
            0.8896068930625916,
            -3.9351673126220703,
            -4.4885382652282715,
            -3.8905911445617676,
            -6.13697624206543,
            6.009778022766113,
            1.0837937593460083,
            1.1329609155654907,
            -1.059395670890808,
            0.4725947976112366,
            -1.82464599609375,
            1.239054560661316,
            -7.935848236083984,
            -4.968003749847412,
            -1.7012213468551636,
            2.649419069290161,
            -3.5867362022399902,
            -4.857024669647217,
            -1.0576215982437134,
            -3.0072429180145264,
            -0.2798858880996704,
            -3.8490848541259766,
            5.882536888122559,
            3.8290343284606934,
            3.4889438152313232,
            1.5833520889282227,
            -1.7088756561279297,
            -3.357283592224121,
            -1.4670302867889404,
            -1.9383032321929932,
            5.037864685058594,
            -0.06593114137649536,
            -2.5335278511047363,
            4.068330764770508,
            0.5058860182762146,
            4.841019153594971,
            -1.7501543760299683,
            -6.031252861022949,
            -5.21327018737793,
            -5.935511112213135,
            -1.0931119918823242,
            2.091367721557617,
            2.0885672569274902,
            -3.8628175258636475,
            -1.6440387964248657,
            4.688360691070557,
            -2.4293785095214844,
            -5.5415120124816895,
            9.318059921264648,
            -1.2973747253417969,
            2.293548583984375,
            -3.107041597366333,
            -0.29401785135269165,
            -2.495072841644287,
            -3.6626152992248535,
            1.2255988121032715,
            3.202033281326294,
            6.912693023681641,
            -2.018040180206299,
            -2.777531623840332,
            -2.7617738246917725
        ]
    },
    "authors": [
        {
            "authorId": "2443456",
            "name": "Shuangfei Zhai"
        },
        {
            "authorId": "145827700",
            "name": "T. Likhomanenko"
        },
        {
            "authorId": "1762320",
            "name": "Etai Littwin"
        },
        {
            "authorId": "46254693",
            "name": "Dan Busbridge"
        },
        {
            "authorId": "16092809",
            "name": "Jason Ramapuram"
        },
        {
            "authorId": "48378494",
            "name": "Yizhe Zhang"
        },
        {
            "authorId": "3016273",
            "name": "Jiatao Gu"
        },
        {
            "authorId": "49158771",
            "name": "J. Susskind"
        }
    ],
    "references": [
        {
            "paperId": "37c1a63152c219ee8f630ecdea18f782074389e9",
            "title": "CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet"
        },
        {
            "paperId": "84f6ab620eb7b112e5b2ca64b305970894e679c1",
            "title": "Adaptive Gradient Methods at the Edge of Stability"
        },
        {
            "paperId": "5eeb80dc67590422db64ca95ec0aded24799cfb6",
            "title": "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse"
        },
        {
            "paperId": "d2f63b56fc6bc373f5c023454c2b253326962865",
            "title": "DeiT III: Revenge of the ViT"
        },
        {
            "paperId": "a6e231db70a6774abc995afc0201671e0dab4d3a",
            "title": "Masked Siamese Networks for Label-Efficient Learning"
        },
        {
            "paperId": "0b0d7d87c58d41b92d907347b778032be5966f60",
            "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"
        },
        {
            "paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca",
            "title": "DeepNet: Scaling Transformers to 1,000 Layers"
        },
        {
            "paperId": "00754af9cd0eaf613a99b13fb19e422b09beeee4",
            "title": "Robust Training of Neural Networks using Scale Invariant Architectures"
        },
        {
            "paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
            "title": "Masked Autoencoders Are Scalable Vision Learners"
        },
        {
            "paperId": "2582a04918f6fe62dc142f2fca9ca0bb0b1d7895",
            "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization"
        },
        {
            "paperId": "83474f6510e0b985595f936d233321e131966bae",
            "title": "A Loss Curvature Perspective on Training Instability in Deep Learning"
        },
        {
            "paperId": "2a805d0e1b067444a554c5169d189fa1f649f411",
            "title": "Scaling Vision Transformers"
        },
        {
            "paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f",
            "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"
        },
        {
            "paperId": "42a7015e48a1e00b70ebb442a82afb4b10017c0b",
            "title": "When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations"
        },
        {
            "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
            "title": "Emerging Properties in Self-Supervised Vision Transformers"
        },
        {
            "paperId": "0d5406775fab3e71848908327fb5504df5f60f92",
            "title": "ImageNet-21K Pretraining for the Masses"
        },
        {
            "paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae",
            "title": "An Empirical Study of Training Self-Supervised Vision Transformers"
        },
        {
            "paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e",
            "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"
        },
        {
            "paperId": "026bb8a1066f50ddc8797e1341353603149a8cb8",
            "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability"
        },
        {
            "paperId": "2b8088253e2378fce001a090fe923b81e8dedf25",
            "title": "RepVGG: Making VGG-style ConvNets Great Again"
        },
        {
            "paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
            "title": "Shortformer: Better Language Modeling using Shorter Inputs"
        },
        {
            "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
            "title": "Training data-efficient image transformers & distillation through attention"
        },
        {
            "paperId": "62ce4d65335c32844247e0ecf3be6de1ccb924b2",
            "title": "Rethinking Evaluation in ASR: Are Our Models Robust Enough?"
        },
        {
            "paperId": "7f0c7c324675179f0e32c160d99c7066c7ab30ae",
            "title": "slimIPL: Language-Model-Free Iterative Pseudo-Labeling"
        },
        {
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
        },
        {
            "paperId": "a2cd073b57be744533152202989228cb4122270a",
            "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"
        },
        {
            "paperId": "b5271f4522fd72e335535c5f65d3afc01d1cb2bd",
            "title": "Very Deep Transformers for Neural Machine Translation"
        },
        {
            "paperId": "e00484961fb2f30d2d48a5f9853fa3ebab140cac",
            "title": "Improving Transformer Optimization Through Better Initialization"
        },
        {
            "paperId": "8d908042f139575d6688c745e94156c9df6eae07",
            "title": "Understanding the Difficulty of Training Transformers"
        },
        {
            "paperId": "e52051204cb1179584f3b008c9d38848b52c1f28",
            "title": "ReZero is All You Need: Fast Convergence at Large Depth"
        },
        {
            "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
            "title": "A Simple Framework for Contrastive Learning of Visual Representations"
        },
        {
            "paperId": "6e5d89c2b3b5ead2c3ab389534de62a28c1e8e6e",
            "title": "PyHessian: Neural Networks Through the Lens of the Hessian"
        },
        {
            "paperId": "703685e969fed715e13937c11d7ecc5cc7c4dfd0",
            "title": "Transformers without Tears: Improving the Normalization of Self-Attention"
        },
        {
            "paperId": "b0fae9fbb4e580d92395eabafe73e317ae6510e3",
            "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition"
        },
        {
            "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
            "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"
        },
        {
            "paperId": "409b9abbeecbc91d00187e73d441f556c8c6b598",
            "title": "An Investigation into Neural Net Optimization via Hessian Eigenvalue Density"
        },
        {
            "paperId": "96c82727dd5a80fef93007f888bb8569feb6bd85",
            "title": "Fixup Initialization: Residual Learning Without Normalization"
        },
        {
            "paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299",
            "title": "Adaptive Input Representations for Neural Language Modeling"
        },
        {
            "paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
            "title": "Self-Attention with Relative Position Representations"
        },
        {
            "paperId": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
            "title": "Spectral Normalization for Generative Adversarial Networks"
        },
        {
            "paperId": "1e3d18beaf3921f561e1b999780f29f2b23f3b7d",
            "title": "Large Batch Training of Convolutional Networks"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "title": "Pointer Sentinel Mixture Models"
        },
        {
            "paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559",
            "title": "Using the Output Embedding to Improve Language Models"
        },
        {
            "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
            "title": "Layer Normalization"
        },
        {
            "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
            "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"
        },
        {
            "paperId": "34038d9424ce602d7ac917a4e582d977725d4393",
            "title": "Librispeech: An ASR corpus based on public domain audio books"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "a0117b72a4f68d0a134e24f674ca7fd0b42663b7",
            "title": "Causality"
        },
        {
            "paperId": null,
            "title": "Results for MT on WMT\u201917 English-German data for post-LN, with or without additional \u03c3Reparam, with or without residual rescaling (\u2018DeepNorm"
        },
        {
            "paperId": null,
            "title": "DeepNorm, solution (it uses post-LN and rescale residual connections depending on"
        },
        {
            "paperId": null,
            "title": "can recover from attention entropy collapse (happens in encoder layers) and nicely converge, 100L-100L suffers from it and can diverge"
        },
        {
            "paperId": null,
            "title": "The goal of this section is to understand how varying the model depth for the well-established recipes affects the training stability"
        },
        {
            "paperId": null,
            "title": "We also observed the same trend of decreasing model performance with increasing the model depth. Attention entropy is nicely bounded across all depths similarly to ASR11"
        },
        {
            "paperId": null,
            "title": "Our main stability experiments use 40 epochs of learning rate warmup, matching the setting of Chen et al."
        },
        {
            "paperId": null,
            "title": "Reduced Learning Rate Warmup"
        },
        {
            "paperId": null,
            "title": "2021) pointed out by Vimal Thilak. All investigations, experiments and related analysis done by Dan Busbridge"
        },
        {
            "paperId": null,
            "title": "2021) the authors noted that the learning rate warmup period needed extending from its typical"
        },
        {
            "paperId": null,
            "title": "50L-50L and 100L-100L, are unable to train and we observe the same vanishing gradients problem as reported by Wang et al"
        },
        {
            "paperId": null,
            "title": "2020a) are recent works that proposed to rescale residual connections"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "For our iterative method we use the implementation of Lanczos from CuPy"
        },
        {
            "paperId": "a59da4639436f582e483347a4833e7659fd3e598",
            "title": "CuPy : A NumPy-Compatible Library for NVIDIA GPU Calculations"
        },
        {
            "paperId": "2e0f0bbe0a630c36ccd42e3b1b62eeb5e8dc4541",
            "title": "Praktische Verfahren der Gleichungsaufl\u00f6sung ."
        },
        {
            "paperId": "261a056f8b21918e8616a429b2df6e1d5d33be41",
            "title": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks"
        },
        {
            "paperId": "845ee9838c1f5bf63b7db2c95ec5d27af14a4e02",
            "title": "Connectionist Temporal Classi\ufb01cation: Labelling Unsegmented Sequences with Recurrent Neural Networks"
        },
        {
            "paperId": null,
            "title": "2022) results to probe for entropy collapse phenomenon. All later experiments, Section 4.3 and Appendix F, with deep transformers and deepnorm are done by Tatiana Likhomanenko"
        },
        {
            "paperId": null,
            "title": "In vision we initialize the \u03c3Reparam \u03b3 term using the first singular value, computed with the SVD at weight initialization. We then use one power iteration for all further updates"
        },
        {
            "paperId": null,
            "title": "Automatic Speech Recognition All speech recognition experiments are done by Tatiana Likhomanenko. Shuangfei Zhai and Jason Ramapuram advised to have also large scale results"
        },
        {
            "paperId": null,
            "title": "Language Modeling Initial implementation and preliminary results on applicability of \u03c3Reparam to the language modeling, Appendix G, are done by Yizhe Zhang with help from Jiatao Gu"
        }
    ]
}