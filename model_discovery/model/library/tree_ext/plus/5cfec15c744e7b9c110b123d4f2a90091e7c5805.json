{
    "paperId": "5cfec15c744e7b9c110b123d4f2a90091e7c5805",
    "externalIds": {
        "ArXiv": "2408.12637",
        "CorpusId": 271947166
    },
    "title": "Building and better understanding vision-language models: insights and future directions",
    "abstract": "The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.",
    "venue": "",
    "year": 2024,
    "referenceCount": 213,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper walks through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -0.28752607107162476,
            -2.6941099166870117,
            -1.8446375131607056,
            4.020193099975586,
            -1.3416459560394287,
            3.033851146697998,
            3.2069523334503174,
            1.6335320472717285,
            -2.817760467529297,
            -0.07373791933059692,
            -3.0267059803009033,
            4.799294471740723,
            -2.9214446544647217,
            1.5612339973449707,
            -1.1041712760925293,
            -0.8988035917282104,
            1.9858301877975464,
            -2.0047647953033447,
            5.322482585906982,
            1.1094800233840942,
            -0.24052482843399048,
            0.41373229026794434,
            -1.9796223640441895,
            -1.7561850547790527,
            -3.2235422134399414,
            -3.3899075984954834,
            2.180875778198242,
            3.4073476791381836,
            -1.7290843725204468,
            0.4999101758003235,
            -0.22982749342918396,
            -2.3347103595733643,
            3.701185941696167,
            -0.5871911644935608,
            1.1649954319000244,
            -0.8654874563217163,
            0.20790672302246094,
            5.829978942871094,
            -2.610668182373047,
            -2.5892226696014404,
            0.9980328679084778,
            -2.3936266899108887,
            1.5758370161056519,
            -0.8543202877044678,
            1.0436809062957764,
            0.4269896149635315,
            -1.415229082107544,
            0.2577207088470459,
            -0.563524067401886,
            -1.0063997507095337,
            -1.3176558017730713,
            3.050708532333374,
            2.305894374847412,
            2.6451210975646973,
            -2.352076530456543,
            -0.05634942650794983,
            2.453516960144043,
            2.246290922164917,
            2.1787824630737305,
            -2.857612133026123,
            3.2873644828796387,
            6.285335540771484,
            0.6273210644721985,
            1.8597948551177979,
            4.333127975463867,
            -4.772549629211426,
            -3.686042308807373,
            3.6176092624664307,
            2.194272994995117,
            1.2750942707061768,
            -5.566856384277344,
            -5.166074752807617,
            -1.0708916187286377,
            -1.0646685361862183,
            -1.968142032623291,
            0.17003273963928223,
            -1.577826738357544,
            -1.920879602432251,
            -2.0490121841430664,
            -2.2079358100891113,
            1.7078841924667358,
            1.958093523979187,
            -0.4882545471191406,
            0.8909158706665039,
            3.508338212966919,
            -0.707256555557251,
            -5.8151960372924805,
            0.1262972056865692,
            -0.012689292430877686,
            -3.2260451316833496,
            -1.3689448833465576,
            -1.229250192642212,
            1.230541467666626,
            -0.7771780490875244,
            -1.7667431831359863,
            0.19918781518936157,
            0.7229136228561401,
            -3.738285541534424,
            -2.450644016265869,
            -2.7916760444641113,
            5.524694442749023,
            -1.479824185371399,
            1.0813530683517456,
            -1.9137275218963623,
            3.325394630432129,
            -3.8291008472442627,
            3.7218806743621826,
            0.31630396842956543,
            3.2416229248046875,
            -0.11486643552780151,
            -0.9460886716842651,
            2.156135082244873,
            -0.7230443954467773,
            -0.18833917379379272,
            -2.8158349990844727,
            0.8063305616378784,
            1.7326829433441162,
            1.3399521112442017,
            -5.0371856689453125,
            2.642749309539795,
            1.0596230030059814,
            -0.3533504009246826,
            -4.64737606048584,
            1.8871636390686035,
            5.041086673736572,
            -0.06710273027420044,
            -1.0020047426223755,
            1.4277184009552002,
            -0.17699414491653442,
            -1.9466633796691895,
            1.9572350978851318,
            -0.7440686225891113,
            4.304803848266602,
            -3.475123882293701,
            0.4096655249595642,
            0.34375759959220886,
            -3.746157646179199,
            1.9932796955108643,
            -1.110647439956665,
            0.4365023970603943,
            2.109053611755371,
            6.120979309082031,
            1.126802682876587,
            1.4608197212219238,
            -2.128227710723877,
            1.6294358968734741,
            -0.5839053392410278,
            -3.1539289951324463,
            3.1626124382019043,
            4.66541862487793,
            3.3513643741607666,
            -2.9381039142608643,
            -1.11713445186615,
            0.8077870607376099,
            1.7711825370788574,
            5.0205078125,
            -4.89963436126709,
            1.827467441558838,
            -4.144692420959473,
            -1.192849040031433,
            -2.318638801574707,
            1.9317578077316284,
            -12.377270698547363,
            1.9919129610061646,
            4.14610481262207,
            -4.57903528213501,
            -0.4680257737636566,
            2.548063278198242,
            1.139545202255249,
            2.838711977005005,
            2.6651782989501953,
            1.9474633932113647,
            0.670520544052124,
            4.170280933380127,
            0.6399279832839966,
            3.828202247619629,
            -1.9022847414016724,
            -4.289862155914307,
            -1.7014216184616089,
            -1.3533782958984375,
            -0.6926536560058594,
            -1.1181445121765137,
            -6.200870513916016,
            2.5493154525756836,
            -4.166750907897949,
            -4.565223217010498,
            -0.6674560308456421,
            0.20483525097370148,
            -0.09839344024658203,
            0.4576451778411865,
            -0.2634185552597046,
            -1.402647852897644,
            8.290940284729004,
            7.060693264007568,
            3.8776049613952637,
            1.7144513130187988,
            5.245012283325195,
            5.809874534606934,
            -0.4467499256134033,
            3.9957191944122314,
            2.773411273956299,
            -2.6200432777404785,
            -2.0662930011749268,
            -3.9252521991729736,
            3.796435832977295,
            2.115097999572754,
            -3.1758944988250732,
            2.0365147590637207,
            1.9342787265777588,
            -0.813549280166626,
            0.934553861618042,
            -3.34152889251709,
            -2.428220510482788,
            3.862879753112793,
            -1.1001007556915283,
            -1.2336513996124268,
            -5.24660587310791,
            2.90093994140625,
            5.650043487548828,
            0.7010495662689209,
            -0.943362295627594,
            0.8149659633636475,
            -2.3967881202697754,
            -3.062035083770752,
            0.0033917129039764404,
            -3.6105332374572754,
            -1.330660104751587,
            2.7923436164855957,
            -1.8347482681274414,
            3.031747341156006,
            -2.6828057765960693,
            -5.857504367828369,
            -1.277238368988037,
            -0.05387992411851883,
            -6.720049858093262,
            -0.8590332865715027,
            -6.883966445922852,
            0.0882875919342041,
            0.0812409520149231,
            -1.982090950012207,
            4.325775146484375,
            3.7836813926696777,
            2.9877026081085205,
            4.678106307983398,
            3.3010735511779785,
            -1.7782862186431885,
            -2.505643606185913,
            0.3686894178390503,
            2.0373916625976562,
            0.10272815823554993,
            -0.34691497683525085,
            0.40559160709381104,
            1.5583789348602295,
            -2.209979772567749,
            -2.535566806793213,
            -1.8806531429290771,
            2.7447714805603027,
            0.665740966796875,
            -1.5576106309890747,
            -0.18004897236824036,
            2.00795578956604,
            4.7720627784729,
            -2.355684757232666,
            3.6806163787841797,
            -3.0755038261413574,
            0.33573615550994873,
            -3.0407416820526123,
            -0.24487870931625366,
            -4.162607192993164,
            2.794637680053711,
            3.0160646438598633,
            0.015313774347305298,
            0.5957383513450623,
            -2.2767410278320312,
            -1.8429405689239502,
            -6.4071760177612305,
            -1.8709102869033813,
            -1.160888433456421,
            1.6055877208709717,
            0.18479657173156738,
            0.2957514524459839,
            0.24242126941680908,
            0.44732964038848877,
            -3.50502610206604,
            -1.8564263582229614,
            -3.884169101715088,
            -2.2479238510131836,
            -0.8547301888465881,
            -0.41637712717056274,
            -2.9663119316101074,
            -2.436429500579834,
            4.979826927185059,
            -1.2606992721557617,
            0.8172892332077026,
            -2.4287381172180176,
            2.709277391433716,
            4.336848258972168,
            -1.15742826461792,
            -1.4648655652999878,
            1.1579134464263916,
            -1.5409255027770996,
            5.045699119567871,
            2.302053451538086,
            -1.4019854068756104,
            0.8332545757293701,
            3.0608608722686768,
            -0.21534371376037598,
            1.3989933729171753,
            0.9795202016830444,
            -0.5065975785255432,
            -3.4226675033569336,
            0.5518335700035095,
            1.414054274559021,
            -5.557314872741699,
            -0.862724781036377,
            2.8889245986938477,
            2.176431894302368,
            -1.494277000427246,
            -1.3729403018951416,
            3.679966688156128,
            2.038623332977295,
            0.8513156175613403,
            -4.24149227142334,
            -1.8529689311981201,
            -3.8328957557678223,
            0.11768411099910736,
            0.7676911950111389,
            3.6718239784240723,
            -6.14694356918335,
            2.314270496368408,
            0.966507077217102,
            3.7663533687591553,
            3.2414870262145996,
            0.20672404766082764,
            -2.0949411392211914,
            -3.887859344482422,
            1.278376817703247,
            -2.1383790969848633,
            2.868237018585205,
            -0.9498160481452942,
            2.4907848834991455,
            4.5508036613464355,
            -3.084230422973633,
            0.3236616849899292,
            2.7893757820129395,
            0.15608826279640198,
            3.776376724243164,
            0.9197816848754883,
            -0.9081019163131714,
            -2.809868812561035,
            1.2655978202819824,
            3.81996488571167,
            3.980236530303955,
            -0.18568885326385498,
            2.3281588554382324,
            4.133266925811768,
            0.6482994556427002,
            0.9658126831054688,
            -0.7295767664909363,
            0.3552146553993225,
            0.884358286857605,
            -1.8617562055587769,
            4.727422714233398,
            2.147475481033325,
            1.1875104904174805,
            0.4960976243019104,
            11.864853858947754,
            -2.252253293991089,
            -0.679729163646698,
            -4.388073921203613,
            -1.822402000427246,
            -3.1378159523010254,
            0.05705040693283081,
            3.2601559162139893,
            -4.009648323059082,
            -3.201992988586426,
            -2.343867540359497,
            -6.752384185791016,
            2.6249752044677734,
            2.985924243927002,
            -2.5075926780700684,
            5.712919235229492,
            -3.8456428050994873,
            3.495624542236328,
            3.188624382019043,
            2.566056251525879,
            -3.522766351699829,
            3.7179436683654785,
            2.466947317123413,
            1.5220556259155273,
            -1.3015658855438232,
            3.5573039054870605,
            1.7222390174865723,
            3.0231332778930664,
            -3.9662294387817383,
            -3.7126400470733643,
            -2.5789480209350586,
            -3.7989840507507324,
            2.5253214836120605,
            0.23898202180862427,
            1.2791643142700195,
            1.5278384685516357,
            3.19935941696167,
            4.448983669281006,
            -3.238800048828125,
            -1.420050859451294,
            1.0314915180206299,
            1.2685728073120117,
            -3.4027116298675537,
            1.6751821041107178,
            -1.860584020614624,
            -2.482820510864258,
            -4.395324230194092,
            -1.2635505199432373,
            -3.297729969024658,
            0.6668052077293396,
            1.6412291526794434,
            1.8758604526519775,
            2.028926372528076,
            1.4793862104415894,
            -3.422452449798584,
            1.990903377532959,
            3.499427556991577,
            0.8690651059150696,
            -0.5974165201187134,
            0.7538188099861145,
            2.8692588806152344,
            2.443836212158203,
            -2.1253557205200195,
            4.983857154846191,
            -0.5953776240348816,
            2.5817136764526367,
            -1.4546452760696411,
            -1.5919058322906494,
            -0.48782646656036377,
            2.109671115875244,
            0.24702784419059753,
            3.161738395690918,
            1.8782867193222046,
            -7.8403778076171875,
            1.1961978673934937,
            4.649492263793945,
            -1.924830436706543,
            4.283226490020752,
            2.175903081893921,
            3.1951475143432617,
            -1.2857990264892578,
            0.9358986616134644,
            -1.0316978693008423,
            -0.16987451910972595,
            5.110233783721924,
            -1.1189863681793213,
            -3.21260142326355,
            0.19517207145690918,
            0.7582102417945862,
            0.9467990398406982,
            -3.061612606048584,
            -0.25382375717163086,
            1.6590662002563477,
            0.28964221477508545,
            -4.932192802429199,
            3.5093138217926025,
            -0.7953344583511353,
            1.6421738862991333,
            0.9249593019485474,
            2.7603960037231445,
            -0.15277069807052612,
            -0.305142879486084,
            -1.3562086820602417,
            1.5898404121398926,
            5.992295742034912,
            1.0366129875183105,
            -4.619995594024658,
            1.8637620210647583,
            -0.5641859769821167,
            2.2080893516540527,
            3.508350372314453,
            0.31894445419311523,
            1.8151910305023193,
            -3.800546884536743,
            -1.5952372550964355,
            0.2427346557378769,
            1.636028528213501,
            -3.038300037384033,
            0.07453584671020508,
            2.8966267108917236,
            3.733415365219116,
            1.590584397315979,
            3.5962677001953125,
            1.9112813472747803,
            -0.18539446592330933,
            1.5562231540679932,
            2.022728681564331,
            -0.93556147813797,
            2.880758762359619,
            -1.4572452306747437,
            -5.157818794250488,
            1.647025465965271,
            0.7520541548728943,
            1.1911966800689697,
            0.48835688829421997,
            -5.148441314697266,
            -1.1385748386383057,
            0.5341840982437134,
            -2.0342764854431152,
            5.165970325469971,
            4.458742141723633,
            -1.5634722709655762,
            1.8215575218200684,
            -2.2083868980407715,
            0.1736462116241455,
            -1.6701079607009888,
            -5.72811222076416,
            -1.0796759128570557,
            -2.110267162322998,
            -1.295844316482544,
            4.457301139831543,
            -0.9922010898590088,
            -1.9141873121261597,
            1.212468147277832,
            -0.5017202496528625,
            -1.0381135940551758,
            1.4198126792907715,
            0.8003286123275757,
            2.42828106880188,
            -0.22928479313850403,
            -3.899578809738159,
            2.4929299354553223,
            0.5547438859939575,
            6.199233055114746,
            3.0767505168914795,
            4.5669450759887695,
            1.017601728439331,
            -2.9166905879974365,
            -3.5794057846069336,
            -2.5741496086120605,
            0.8230513334274292,
            1.097104549407959,
            -2.9558210372924805,
            -2.6003546714782715,
            -1.2414504289627075,
            0.4150739014148712,
            3.2682039737701416,
            5.414112567901611,
            1.2623634338378906,
            2.4194421768188477,
            -2.8750674724578857,
            -1.5227957963943481,
            0.6246059536933899,
            1.9964203834533691,
            1.3449903726577759,
            -1.1945561170578003,
            -0.6451597213745117,
            0.048693180084228516,
            -0.7821370959281921,
            0.23271432518959045,
            -1.6906263828277588,
            -2.7937734127044678,
            0.44866204261779785,
            2.1412172317504883,
            3.296720027923584,
            -2.0357110500335693,
            3.6962852478027344,
            3.2342641353607178,
            -1.513681411743164,
            0.4146403968334198,
            -0.6749135255813599,
            3.746722936630249,
            1.9898850917816162,
            -0.6929481625556946,
            0.2461884319782257,
            -4.924551010131836,
            1.8370213508605957,
            0.724689781665802,
            0.3693162202835083,
            3.4032583236694336,
            2.6869611740112305,
            0.018101662397384644,
            -3.731715202331543,
            -0.14911305904388428,
            -4.012264251708984,
            -0.49256977438926697,
            -0.4507470726966858,
            -2.4592318534851074,
            0.65760338306427,
            -1.642174243927002,
            -1.5939478874206543,
            1.8263107538223267,
            -3.9425201416015625,
            1.0631805658340454,
            3.0505270957946777,
            -2.4072227478027344,
            -0.8409172296524048,
            -4.125677108764648,
            0.1478596031665802,
            -4.223776817321777,
            3.8641018867492676,
            2.278766632080078,
            0.7040451169013977,
            2.778330087661743,
            1.9896135330200195,
            2.2149949073791504,
            2.2598323822021484,
            1.195672869682312,
            -1.9533439874649048,
            -2.2017335891723633,
            -0.8612468242645264,
            -1.3111467361450195,
            0.1433088183403015,
            -0.9000892639160156,
            -2.4327638149261475,
            -0.6468116044998169,
            18.221586227416992,
            -0.8365786671638489,
            1.8660273551940918,
            -2.683490753173828,
            -3.631150722503662,
            -3.578758716583252,
            0.7761186361312866,
            0.48152899742126465,
            2.706404209136963,
            3.9054973125457764,
            1.6646463871002197,
            -4.192495346069336,
            -1.0284661054611206,
            3.778385639190674,
            -5.356271743774414,
            -0.33831119537353516,
            -1.2079370021820068,
            1.2500996589660645,
            -0.11393071711063385,
            1.4667704105377197,
            -1.4673742055892944,
            -2.528658866882324,
            -5.313365459442139,
            -4.332114219665527,
            -0.7609997987747192,
            4.920159339904785,
            3.8814475536346436,
            3.8466787338256836,
            -4.530543327331543,
            -0.38949841260910034,
            -2.434267520904541,
            2.3304359912872314,
            5.775753021240234,
            -0.38952964544296265,
            -1.3715569972991943,
            2.8131015300750732,
            1.898341417312622,
            -3.5671143531799316,
            -0.3862095773220062,
            2.6274189949035645,
            -0.11234274506568909,
            2.018486738204956,
            -3.149764060974121,
            0.7953658103942871,
            -3.042731285095215,
            2.5920753479003906,
            2.0063834190368652,
            -3.833026885986328,
            -1.811591386795044,
            6.0874481201171875,
            1.266469955444336,
            -1.1295907497406006,
            -3.9713869094848633,
            0.8265548348426819,
            2.3664369583129883,
            0.20012056827545166,
            -0.3605494201183319,
            -5.598320007324219,
            0.9711505174636841,
            -2.487612724304199,
            2.4631595611572266,
            0.3819643259048462,
            -2.692659378051758,
            -4.133204460144043,
            -2.5378992557525635,
            0.058196038007736206,
            -3.178224563598633,
            2.9258697032928467,
            3.243162155151367,
            -0.6615064740180969,
            2.607553720474243,
            0.37141185998916626,
            1.2129136323928833,
            -5.558913230895996,
            -0.24452047049999237,
            -3.761990547180176,
            -0.4380422532558441,
            -1.873263955116272,
            3.6071207523345947,
            4.015190124511719,
            -3.6801095008850098,
            3.7513821125030518,
            -3.9995670318603516,
            -4.565828323364258,
            2.6925225257873535,
            -3.106363296508789,
            1.1070683002471924,
            2.628477096557617,
            -0.26501524448394775,
            1.1314737796783447,
            -1.2163162231445312,
            0.7846775054931641,
            4.13303804397583,
            2.0861501693725586,
            1.0082006454467773,
            -5.03656005859375,
            -0.5717711448669434,
            0.25414443016052246,
            -5.056299686431885,
            -0.7225749492645264,
            4.548633575439453,
            5.096255302429199,
            1.7888394594192505,
            -3.2105135917663574,
            -0.6275023818016052,
            -0.3507782220840454,
            -0.4713059067726135,
            -6.1628313064575195,
            -3.494750499725342,
            -2.366692543029785,
            4.676816940307617,
            -3.4158997535705566,
            -0.3399357199668884,
            0.192470520734787,
            3.169706344604492,
            -1.7873822450637817,
            -0.7456002235412598,
            -0.007169745862483978,
            2.6047260761260986,
            1.5510711669921875,
            -0.5100213289260864,
            -1.1831295490264893,
            -0.3737053871154785,
            -1.7957072257995605,
            -1.3100954294204712,
            -1.3469324111938477,
            -1.2298333644866943,
            -1.3015203475952148,
            -2.2088570594787598,
            -2.8926422595977783,
            1.1886646747589111,
            -2.5490574836730957,
            0.45461851358413696,
            0.9937734603881836,
            2.417032241821289,
            1.642317533493042,
            -0.9874491095542908,
            1.0031614303588867,
            -2.204540729522705,
            5.115498065948486,
            0.07633492350578308,
            -3.019803524017334,
            -0.8642683625221252,
            8.51334285736084,
            -3.771786689758301,
            -1.017645239830017,
            -4.306147575378418,
            -0.9543377161026001,
            -1.5066174268722534,
            1.4894565343856812,
            2.0792770385742188,
            1.398820400238037,
            1.0935875177383423,
            2.740786075592041,
            -3.07844877243042,
            -0.025864124298095703
        ]
    },
    "authors": [
        {
            "authorId": "2172404846",
            "name": "Hugo Laurenccon"
        },
        {
            "authorId": "2316862322",
            "name": "Andr'es Marafioti"
        },
        {
            "authorId": "2285868436",
            "name": "Victor Sanh"
        },
        {
            "authorId": "2220831421",
            "name": "L\u00e9o Tronchon"
        }
    ],
    "references": [
        {
            "paperId": "140331a0807e2bb9d9544e7b354a116ddc922454",
            "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models"
        },
        {
            "paperId": "1a71f7b216b710b936da666027014adb83af8e7a",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer"
        },
        {
            "paperId": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
            "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone"
        },
        {
            "paperId": "6520557cc3bfd198f960cc8cb6151c3474321bd8",
            "title": "The Llama 3 Herd of Models"
        },
        {
            "paperId": "1ab3c15e56a91c6a7b3c22c7004f1f79e7f4ed8c",
            "title": "VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document Understanding"
        },
        {
            "paperId": "24a91683b4011fd24d9593ae9a48783234bc9819",
            "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models"
        },
        {
            "paperId": "1f54fdd67a6ffa99384c9c20746d839391da3834",
            "title": "MAVIS: Mathematical Visual Instruction Tuning"
        },
        {
            "paperId": "8c7a4db172bd66674f4f84e3fbb6ef79191d3943",
            "title": "PaliGemma: A versatile 3B VLM for transfer"
        },
        {
            "paperId": "334d7fba900eab258cd4fbb5152539e83678b9c4",
            "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild"
        },
        {
            "paperId": "0cc05e1acc178cfa4b26cb1b0271497363d4147f",
            "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output"
        },
        {
            "paperId": "b83a9e35c3aeeb37708e362473c7617d59b815b5",
            "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale"
        },
        {
            "paperId": "50e69faefa9a78afb3068f7ca15df07e7ac7b319",
            "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model"
        },
        {
            "paperId": "258ef0ba8ebb4209a6e24fdad2480a8a06244254",
            "title": "MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens"
        },
        {
            "paperId": "1b5262c26fdb79d7313360ec0af650e7f4a0f638",
            "title": "From Pixels to Prose: A Large Dataset of Dense Image Captions"
        },
        {
            "paperId": "5d3eacfd4e44dd5fa88026edc5848f8bd5efea0e",
            "title": "OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text"
        },
        {
            "paperId": "87a108853c92a969fda26d0fd91bbd1d70706a98",
            "title": "The Evolution of Multimodal Model Architectures"
        },
        {
            "paperId": "a0a2012b851a796ba7c4702aab855a462a225815",
            "title": "RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness"
        },
        {
            "paperId": "a7853ac0766ce2668fb37d3dbe5ccea476c90047",
            "title": "Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models"
        },
        {
            "paperId": "53a803388e83ae89261624099d7be4287ace67cb",
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
        },
        {
            "paperId": "00a3703fd076bddfa1ac14786f66e91ccd043140",
            "title": "ImageInWords: Unlocking Hyper-Detailed Image Descriptions"
        },
        {
            "paperId": "ce68430823b79dd3d478c505cc2761f03cf72b30",
            "title": "What matters when building vision-language models?"
        },
        {
            "paperId": "63455557c532abc09fa9a229d2fa7f9bf98b8ae4",
            "title": "MANTIS: Interleaved Multi-Image Instruction Tuning"
        },
        {
            "paperId": "887306ee7ebd9eebd7faea24106ec8e8f1a50987",
            "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"
        },
        {
            "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
            "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
        },
        {
            "paperId": "f9fc7b5ebd5fe30257abb03ad87d8138eeeb28d9",
            "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models"
        },
        {
            "paperId": "01ae1c181dcb5117491affae728065e5e62bf074",
            "title": "InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD"
        },
        {
            "paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f",
            "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"
        },
        {
            "paperId": "8a9e11addba791860a9dbf15de75cc28d2cf844c",
            "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?"
        },
        {
            "paperId": "b38845e9adbeeeab37519a2fc30e899411b4a36a",
            "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models"
        },
        {
            "paperId": "75b2ae5ee35611ecfbd3dc2c3d0799cfb4fd98e4",
            "title": "InternLM2 Technical Report"
        },
        {
            "paperId": "4c4396c227c68bb89e1b937a7dadda73d41db33b",
            "title": "Improved Baselines for Data-efficient Perceptual Augmentation of LLMs"
        },
        {
            "paperId": "15ef0417570a190241caac0615eabdff11abb4de",
            "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding"
        },
        {
            "paperId": "c4985c30261c0c99eeafd97af59a301c253ad84c",
            "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs"
        },
        {
            "paperId": "6675bcf6dc97c87da7afda223938ec7e51ecc3b2",
            "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"
        },
        {
            "paperId": "29af66d6eefbb0117c0bdfbe1eef84727b70d3b6",
            "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset"
        },
        {
            "paperId": "b842b83a7ff5dff8e3b83915d8c15423b6085728",
            "title": "Gemma: Open Models Based on Gemini Research and Technology"
        },
        {
            "paperId": "6c6f63643eb97a8c309a881e4ba21461454478fe",
            "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models"
        },
        {
            "paperId": "1c304466269aac9044840b8d3b3f8a2f7abf3b12",
            "title": "Synth2: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings"
        },
        {
            "paperId": "b14e5138d4d1f3577b2390541dc7b730a41bb651",
            "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding"
        },
        {
            "paperId": "0fce243964da0ec358152f226b21432e5a658917",
            "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
        },
        {
            "paperId": "1531c25b0103308beee55d85b1b5301f592303be",
            "title": "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding"
        },
        {
            "paperId": "18e7ab056c16928d8f9539509a4b366889106d97",
            "title": "StarCoder 2 and The Stack v2: The Next Generation"
        },
        {
            "paperId": "6a6751f59c5dbc80823b3cf47c3aaae063991b86",
            "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models"
        },
        {
            "paperId": "ad2be51acf42f686a8d1de92d7435d84274ee62d",
            "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math"
        },
        {
            "paperId": "2eb70bf96e170f34d72f0023fc0c4f6fb06b6009",
            "title": "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter"
        },
        {
            "paperId": "da053e2a4ba1b244940c8f2cad5dcdf0d730f85f",
            "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"
        },
        {
            "paperId": "f86f71cfd2e9682a56d7334736a7b8a0b1c70b45",
            "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models"
        },
        {
            "paperId": "a091bf215c716a146140f81c751712db628c8e20",
            "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"
        },
        {
            "paperId": "cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3",
            "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"
        },
        {
            "paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32",
            "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
        },
        {
            "paperId": "9b093787f9be3d480cd11f8ec6ca5b0e44050d6d",
            "title": "Solving olympiad geometry without human demonstrations"
        },
        {
            "paperId": "6c64ddd2190909de2c680dd18abc9b92e80c39f9",
            "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action"
        },
        {
            "paperId": "4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5",
            "title": "Generative Multimodal Models are In-Context Learners"
        },
        {
            "paperId": "608a2b333fd8262e8c918f36c5700bafd3ea3cdd",
            "title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning"
        },
        {
            "paperId": "3713112311efbcf785de17fa86e5bf42e4360f77",
            "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model"
        },
        {
            "paperId": "2b14d9e190022e388476ebb24eb1a84349ca0de4",
            "title": "Silkie: Preference Distillation for Large Visual Language Models"
        },
        {
            "paperId": "1608e505c3e749a00ce0c56e0c2d53e0e9ae7fe4",
            "title": "CogAgent: A Visual Language Model for GUI Agents"
        },
        {
            "paperId": "2141ed804636a1cf339d606cd03fd3b3e9582133",
            "title": "VILA: On Pre-training for Visual Language Models"
        },
        {
            "paperId": "4f5654ec1dfc04478be42d03eee8e6db6bd9ca14",
            "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM"
        },
        {
            "paperId": "0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8",
            "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback"
        },
        {
            "paperId": "b50d19c5c298f6562c3b3c6c3822a351bdc89260",
            "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"
        },
        {
            "paperId": "f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560",
            "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"
        },
        {
            "paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
            "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"
        },
        {
            "paperId": "bf14244669d5505f63343d4365d99d24aa6c5e82",
            "title": "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models"
        },
        {
            "paperId": "441bada9aa6dfd1f94d45d20e0f7eb060d59dd30",
            "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks"
        },
        {
            "paperId": "2313afae52d98e569da2dedbf14daf9efc74e7cf",
            "title": "CogVLM: Visual Expert for Pretrained Language Models"
        },
        {
            "paperId": "68e0e789b5147b1e7d028c7a825650075f4e26bf",
            "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger"
        },
        {
            "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
            "title": "Mistral 7B"
        },
        {
            "paperId": "1606ca3e8d4cc409df635e494dc34d678152480a",
            "title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning"
        },
        {
            "paperId": "69b90bd79bb0fc87d39180161926964ae9dd7cbc",
            "title": "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model"
        },
        {
            "paperId": "124d4d374fbef2016fa9880489871a58a7450644",
            "title": "Improved Baselines with Visual Instruction Tuning"
        },
        {
            "paperId": "62e633f4b5cf8bc573e496602d3aa6e5919bbe61",
            "title": "Improving Automatic VQA Evaluation Using Large Language Models"
        },
        {
            "paperId": "8946891e94831adc8cddb0d32311cce2445c96d2",
            "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"
        },
        {
            "paperId": "9486179c5ab6e0cff0ef919c34080a882bb9fe9f",
            "title": "AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ"
        },
        {
            "paperId": "10bd38673951f5d7729568284093cbd80482ab16",
            "title": "Vision Transformers Need Registers"
        },
        {
            "paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
            "title": "Qwen Technical Report"
        },
        {
            "paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
            "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"
        },
        {
            "paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
            "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
        },
        {
            "paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9",
            "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"
        },
        {
            "paperId": "4b4a329e54325e80be50cdc77e274c6e9fd5ade4",
            "title": "Nougat: Neural Optical Understanding for Academic Documents"
        },
        {
            "paperId": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
            "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
        },
        {
            "paperId": "7fbc502441d66daf1f53765d5d86a8dfba9ab0ce",
            "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"
        },
        {
            "paperId": "4309d572a37d655779f9dce6a2c98c66334132de",
            "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"
        },
        {
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "b37b1dc72b1882858f5120f2cd6883134089a6ed",
            "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
        },
        {
            "paperId": "918617dbc02fa4df1999599bcf967acd2ea84d71",
            "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution"
        },
        {
            "paperId": "a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3",
            "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"
        },
        {
            "paperId": "d5d936b142eb81826002888230649c805532630e",
            "title": "VisText: A Benchmark for Semantically Rich Chart Captioning"
        },
        {
            "paperId": "341d516cc858dc92ba14a788ef40d0559b5a2b26",
            "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations"
        },
        {
            "paperId": "948e8cfae92c2004f2dd5c9316f5972f8baaea21",
            "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"
        },
        {
            "paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b",
            "title": "Textbooks Are All You Need"
        },
        {
            "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
            "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
        },
        {
            "paperId": "d47524cd5c3c4b57af2e5a29f6f91c420310f236",
            "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning"
        },
        {
            "paperId": "6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8",
            "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"
        },
        {
            "paperId": "3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a",
            "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model"
        },
        {
            "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
            "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
        },
        {
            "paperId": "8c7846c9805834dbe2fb0c8f48253b8d65b79d6a",
            "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"
        },
        {
            "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
            "title": "LIMA: Less Is More for Alignment"
        },
        {
            "paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
            "title": "Evaluating Object Hallucination in Large Vision-Language Models"
        },
        {
            "paperId": "f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",
            "title": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"
        },
        {
            "paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd",
            "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
        },
        {
            "paperId": "f9570989919338079088270a9cf1a7afc8db8093",
            "title": "DataComp: In search of the next generation of multimodal datasets"
        },
        {
            "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
            "title": "Visual Instruction Tuning"
        },
        {
            "paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
            "title": "DINOv2: Learning Robust Visual Features without Supervision"
        },
        {
            "paperId": "df958800014d310b6df34ad83d771314d68fbb2d",
            "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text"
        },
        {
            "paperId": "7bf72a3b5fbac8bc0f461780810fbc781c28ef53",
            "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society"
        },
        {
            "paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba",
            "title": "Sigmoid Loss for Language Image Pre-Training"
        },
        {
            "paperId": "a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf",
            "title": "EVA-CLIP: Improved Training Techniques for CLIP at Scale"
        },
        {
            "paperId": "c5b2243baf88a00db2d4e4f9edb33cde08eb153f",
            "title": "eP-ALM: Efficient Perceptual Augmentation of Language Models"
        },
        {
            "paperId": "9ff9ea4a504874a04b674002f090a650a1efe9a0",
            "title": "On the De-duplication of LAION-2B"
        },
        {
            "paperId": "638b08154fbb71fd34db2aae6cb40045577fe0de",
            "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6",
            "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"
        },
        {
            "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
            "title": "PaLM-E: An Embodied Multimodal Language Model"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",
            "title": "Language Is Not All You Need: Aligning Perception with Language Models"
        },
        {
            "paperId": "6173520a1eb2814d067e8c5fd16212b7cbf6ee78",
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs"
        },
        {
            "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
        },
        {
            "paperId": "dc7af5ce33330085c70c5ed9ee4c34ab9df3b4de",
            "title": "MapQA: A Dataset for Question Answering on Choropleth Maps"
        },
        {
            "paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
            "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"
        },
        {
            "paperId": "1f86bf1e334200ec0481349255559fbfe7a33caa",
            "title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting"
        },
        {
            "paperId": "e1484706c0fab932fc9804df328044b3cb2f110d",
            "title": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"
        },
        {
            "paperId": "3e565c544a8639cc9c7568833e484d7610f5e5d4",
            "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"
        },
        {
            "paperId": "d3135733aa39dec20ce72aa138589dda27c8406d",
            "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"
        },
        {
            "paperId": "74cc3d340039c67bdabaef090d1386fe2c5376ca",
            "title": "CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning"
        },
        {
            "paperId": "0b7e9b6b588baa3f24fdde06feec26a067aa74bd",
            "title": "MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"
        },
        {
            "paperId": "47a67e76ed84260ff19f7a948d764005d1edf1c9",
            "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"
        },
        {
            "paperId": "354b48677e314ef2f47512c5a81723cfd17dd05d",
            "title": "Visual Spatial Reasoning"
        },
        {
            "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
            "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
        },
        {
            "paperId": "b611c501269224702d1a9942c8600a31ec66ab28",
            "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"
        },
        {
            "paperId": "6289874e3b499593dddd51ae229062fe07d30b56",
            "title": "OCR-IDL: OCR Annotations for Industry Document Library Dataset"
        },
        {
            "paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca",
            "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"
        },
        {
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
            "paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99",
            "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
        },
        {
            "paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02",
            "title": "FLAVA: A Foundational Language And Vision Alignment Model"
        },
        {
            "paperId": "da4261a957eaa96bf626e9641ef68ebed1d5333f",
            "title": "RedCaps: web-curated image-text data created by the people, for the people"
        },
        {
            "paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df",
            "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"
        },
        {
            "paperId": "9bcf3b43f2323a194036cc52c6878a9b1dc7e058",
            "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"
        },
        {
            "paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
            "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"
        },
        {
            "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
            "title": "Finetuned Language Models Are Zero-Shot Learners"
        },
        {
            "paperId": "99053e3a708fc27709c9dab33110dc98b187c158",
            "title": "FinQA: A Dataset of Numerical Reasoning over Financial Data"
        },
        {
            "paperId": "a1364257028332760208827cd0c7af08d91e058b",
            "title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation"
        },
        {
            "paperId": "638107136cd71057855a1d393c07923394b0e40f",
            "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning"
        },
        {
            "paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d",
            "title": "Multimodal Few-Shot Learning with Frozen Language Models"
        },
        {
            "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "title": "LoRA: Low-Rank Adaptation of Large Language Models"
        },
        {
            "paperId": "b3213c84a6ff7a2f11099de783c93166e4fc02a4",
            "title": "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"
        },
        {
            "paperId": "fb1c90806fc5ec72987f58110aa255edbce6620d",
            "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"
        },
        {
            "paperId": "d5611a92619548e7f2af5adb04070574c0dacac1",
            "title": "InfographicVQA"
        },
        {
            "paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
            "title": "Perceiver: General Perception with Iterative Attention"
        },
        {
            "paperId": "98e565fa06f6c7bf7c46833b5106b26dc45130c4",
            "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"
        },
        {
            "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "title": "Learning Transferable Visual Models From Natural Language Supervision"
        },
        {
            "paperId": "394be105b87e9bfe72c20efe6338de10604e1a11",
            "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"
        },
        {
            "paperId": "f05126c1a792ea64a7af0c8c68b03bcddec5b297",
            "title": "VisualMRC: Machine Reading Comprehension on Document Images"
        },
        {
            "paperId": "9a2a5bb18d6252711e124c0029fcf002d62f1795",
            "title": "Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model"
        },
        {
            "paperId": null,
            "title": "Transformers: State-of-the-Art Natural Language Processing"
        },
        {
            "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
            "title": "Measuring Massive Multitask Language Understanding"
        },
        {
            "paperId": "b40bfcf339de3f0dba08fabb2b58b9368ff4c51a",
            "title": "DocVQA: A Dataset for VQA on Document Images"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "3f6570fd55dc5855f93a56150e6d99c7944a1c1e",
            "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes"
        },
        {
            "paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e",
            "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"
        },
        {
            "paperId": "fc0b46a0f3720e6c29c1a913aaa3de4a0699f713",
            "title": "PathVQA: 30000+ Questions for Medical Visual Question Answering"
        },
        {
            "paperId": "439369de9514e41e0f03fed552d8f6e5aebf51b2",
            "title": "Connecting Vision and Language with Localized Narratives"
        },
        {
            "paperId": "c496f00f2b85b496abdc36e4dc4105541248fd3c",
            "title": "PlotQA: Reasoning over Scientific Plots"
        },
        {
            "paperId": "1097cf8cf5961589ff693b069002e7181e24e631",
            "title": "OCR-VQA: Visual Question Answering by Reading Text in Images"
        },
        {
            "paperId": "d0818dac77eee5b970736e57a478bcedfb1b15fe",
            "title": "KVQA: Knowledge-Aware Visual Question Answering"
        },
        {
            "paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70",
            "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"
        },
        {
            "paperId": "0033346700dc450ac22c9b704eab0e906d868662",
            "title": "Scene Text Visual Question Answering"
        },
        {
            "paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907",
            "title": "Towards VQA Models That Can Read"
        },
        {
            "paperId": "d0bfd3cb732471a0843a39d2d047caf60a844466",
            "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing"
        },
        {
            "paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c",
            "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"
        },
        {
            "paperId": "cf336d272a30d6ad6141db67faa64deb8791cd61",
            "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs"
        },
        {
            "paperId": "634161e4759616dbe06f0b1465999d3df122f366",
            "title": "TallyQA: Answering Complex Counting Questions"
        },
        {
            "paperId": "7062b5de5fddb298823cf8969c7dfa6165ea933e",
            "title": "Learning to Describe Differences Between Pairs of Similar Images"
        },
        {
            "paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0",
            "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"
        },
        {
            "paperId": "7289a240c9425bc7cad87b3b835e5f0cac22f488",
            "title": "DVQA: Understanding Data Visualizations via Question Answering"
        },
        {
            "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
            "title": "Decoupled Weight Decay Regularization"
        },
        {
            "paperId": "cb6be69c67b0b15ebbda89a126f4dd62a4d32958",
            "title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning"
        },
        {
            "paperId": "cbd569036fc72ae7ff747350b91816440282596b",
            "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"
        },
        {
            "paperId": "c071a1ad68310fed7f0876b6f01cb7b135043bc3",
            "title": "Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension"
        },
        {
            "paperId": "8ff54aa8045b1e30c348cf2ca42259c946cd7a9e",
            "title": "Search-based Neural Structured Learning for Sequential Question Answering"
        },
        {
            "paperId": "03eb382e04cca8cca743f7799070869954f1402a",
            "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"
        },
        {
            "paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
            "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
        },
        {
            "paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c",
            "title": "Aggregated Residual Transformations for Deep Neural Networks"
        },
        {
            "paperId": "e18ec2c9f0b4a817b8cf0435822bbc879d7db698",
            "title": "A Diagram is Worth a Dozen Images"
        },
        {
            "paperId": "def584565d05d6a8ba94de6621adab9e301d375d",
            "title": "Visual7W: Grounded Question Answering in Images"
        },
        {
            "paperId": "b41e95c8c97846d5ca4c11ef79d7814499cc9663",
            "title": "Compositional Semantic Parsing on Semi-Structured Tables"
        },
        {
            "paperId": "62a956d7600b10ca455076cd56e604dfd106072a",
            "title": "Exploring Models and Data for Image Question Answering"
        },
        {
            "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
            "title": "VQA: Visual Question Answering"
        },
        {
            "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "title": "Adam: A Method for Stochastic Optimization"
        },
        {
            "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "title": "Microsoft COCO: Common Objects in Context"
        },
        {
            "paperId": "44040913380206991b1991daf1192942e038fe31",
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"
        },
        {
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database"
        },
        {
            "paperId": "f988f03428af2516af403077d609bbca3fcf8097",
            "title": "Transcription"
        },
        {
            "paperId": "04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59",
            "title": "The IAM-database: an English sentence database for offline handwriting recognition"
        },
        {
            "paperId": "05aa63683f7d027c18f560d4472ac87c1ea754fe",
            "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only"
        },
        {
            "paperId": null,
            "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm"
        },
        {
            "paperId": null,
            "title": "From scarcity to efficiency: Improving clip training via visual-enriched captions"
        },
        {
            "paperId": null,
            "title": "Introducing our multimodal models"
        },
        {
            "paperId": null,
            "title": "Identifying and eliminating csam in generative ml training data and models"
        },
        {
            "paperId": null,
            "title": "Laion coco: 600m synthetic captions from laion2b-en"
        },
        {
            "paperId": null,
            "title": "Image-text pair dataset"
        },
        {
            "paperId": null,
            "title": "Pali: Scaling language-image learning in 100+ languages"
        },
        {
            "paperId": "8b55402ffee2734bfc7d5d7595500916e1ef04e8",
            "title": "nocaps: novel object captioning at scale"
        },
        {
            "paperId": "18f9a6045ba01cb079c4fa49a630d71bbd27cd92",
            "title": "Descriptor : A dataset of clinically generated visual questions and answers about radiology images"
        },
        {
            "paperId": "cfee1826dd4743eab44c6e27a0cc5970effa4d80",
            "title": "Improving Image Generation with Better Captions"
        },
        {
            "paperId": "de8ba9b01c9ab7cbabf5c33b80b7bbc618857627",
            "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku"
        },
        {
            "paperId": "edae8ef52171e6147eb8293ccb8cc1ee26625e17",
            "title": "OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework"
        },
        {
            "paperId": null,
            "title": "Teknium (2023)"
        },
        {
            "paperId": null,
            "title": "Table understanding A dataset"
        },
        {
            "paperId": null,
            "title": "Document understanding Understanding documents from images"
        },
        {
            "paperId": null,
            "title": "Reasoning with chain-of-thought In Meteor (Lee et al., 2024)"
        },
        {
            "paperId": null,
            "title": "We thank Amy Sartran for helping to integrate Idefics3 into the Transformers library (Wolf et al., 2020 Merve Noyan for building a demo for the model, and Leandro von Werra for reviewing this paper"
        }
    ]
}