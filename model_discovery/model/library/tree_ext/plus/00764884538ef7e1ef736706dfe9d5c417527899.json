{
    "paperId": "00764884538ef7e1ef736706dfe9d5c417527899",
    "externalIds": {
        "ArXiv": "2408.03130",
        "CorpusId": 271720097
    },
    "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations",
    "abstract": "Large language models are ubiquitous in natural language processing because they can adapt to new tasks without retraining. However, their sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.",
    "venue": "",
    "year": 2024,
    "referenceCount": 83,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations, to explore each method in-depth and highlight its unique challenges and practical applications."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.469982624053955,
            -1.2970563173294067,
            -3.365391731262207,
            2.777794599533081,
            1.980989933013916,
            -0.5920265913009644,
            2.1197240352630615,
            1.266242504119873,
            -2.4217240810394287,
            1.6677435636520386,
            -3.121030330657959,
            3.144611120223999,
            -2.0886340141296387,
            0.11094862222671509,
            -2.405283212661743,
            2.1835947036743164,
            -2.322106122970581,
            0.23997637629508972,
            5.227697372436523,
            3.093825101852417,
            -0.5355602502822876,
            0.8109117150306702,
            -2.3816654682159424,
            -0.31431329250335693,
            0.3667263388633728,
            -2.796403408050537,
            3.2589733600616455,
            0.33979713916778564,
            0.5127537250518799,
            2.4965579509735107,
            -0.4106166958808899,
            -2.2171194553375244,
            4.459031105041504,
            -6.478589057922363,
            2.3922502994537354,
            -3.455090284347534,
            -3.6916990280151367,
            7.357013702392578,
            -2.2520947456359863,
            0.24240337312221527,
            -1.540107011795044,
            0.8484693169593811,
            1.790027141571045,
            0.1417994499206543,
            -0.39283716678619385,
            2.6828956604003906,
            1.0160170793533325,
            1.22025465965271,
            1.3410693407058716,
            0.5722308158874512,
            2.637636661529541,
            2.392292022705078,
            2.3794918060302734,
            1.2851814031600952,
            0.4928765296936035,
            0.058849409222602844,
            1.1319646835327148,
            1.3642382621765137,
            2.4315953254699707,
            -3.2914862632751465,
            3.324129581451416,
            3.3229899406433105,
            -1.0967741012573242,
            2.458322286605835,
            4.364673614501953,
            0.08814451098442078,
            -3.532810926437378,
            3.5966789722442627,
            3.453906297683716,
            1.153635859489441,
            -2.908566474914551,
            -2.017960786819458,
            1.3531205654144287,
            0.44059324264526367,
            -1.9606246948242188,
            -1.241110920906067,
            -1.5969185829162598,
            -6.220618724822998,
            0.4205177426338196,
            -2.175938129425049,
            -0.14425654709339142,
            -1.0750231742858887,
            1.829258680343628,
            6.918071269989014,
            1.2416870594024658,
            -2.1925463676452637,
            -1.928234577178955,
            0.3783801794052124,
            0.6595972180366516,
            -1.921006679534912,
            -1.9251766204833984,
            -1.7147560119628906,
            -1.2812435626983643,
            5.761980056762695,
            -3.2664904594421387,
            -3.5997507572174072,
            3.6272099018096924,
            1.153790831565857,
            -4.676136016845703,
            1.3050487041473389,
            4.456268310546875,
            -0.18593762814998627,
            1.104844093322754,
            2.3973541259765625,
            5.836640357971191,
            -5.160430908203125,
            2.8987314701080322,
            2.193459987640381,
            -0.6195886135101318,
            -0.10100960731506348,
            -1.2423056364059448,
            0.9800733327865601,
            -0.1123131513595581,
            -0.3638190031051636,
            -0.5500065684318542,
            -3.2433135509490967,
            -2.964916706085205,
            2.5161004066467285,
            -3.707101821899414,
            3.4646637439727783,
            1.6769828796386719,
            -1.7486298084259033,
            -0.27622491121292114,
            -1.9165740013122559,
            3.9624905586242676,
            1.6768394708633423,
            -0.6705198287963867,
            -3.0661120414733887,
            -0.19489911198616028,
            -2.9430441856384277,
            1.6068981885910034,
            -1.524226188659668,
            5.569417953491211,
            -1.7174652814865112,
            0.23828497529029846,
            0.7753827571868896,
            -4.347232818603516,
            2.8215842247009277,
            0.531224250793457,
            0.12476515769958496,
            -1.3429787158966064,
            6.2587127685546875,
            -4.161345958709717,
            -1.9182956218719482,
            -0.7180121541023254,
            1.8057235479354858,
            1.9345943927764893,
            -0.04163700342178345,
            0.131186842918396,
            4.670492172241211,
            3.077009677886963,
            -6.568453311920166,
            0.7856700420379639,
            3.1117424964904785,
            -0.5200847387313843,
            5.229372978210449,
            -1.8068275451660156,
            2.1007039546966553,
            -0.19356226921081543,
            2.3418707847595215,
            0.26592326164245605,
            1.7918531894683838,
            -7.445436954498291,
            -0.5238137245178223,
            1.590837836265564,
            -3.4816174507141113,
            -3.7109692096710205,
            1.6580770015716553,
            -1.2101900577545166,
            5.937354564666748,
            1.6760749816894531,
            1.6832380294799805,
            1.1757440567016602,
            3.518573760986328,
            1.6794832944869995,
            1.1859381198883057,
            0.6296587586402893,
            -1.9359116554260254,
            -0.28814932703971863,
            0.183207705616951,
            -0.9643590450286865,
            0.6681152582168579,
            -6.685426235198975,
            -1.1136829853057861,
            -4.1531171798706055,
            1.1238582134246826,
            -0.3103035092353821,
            0.9389561414718628,
            0.3357692062854767,
            4.051938533782959,
            -2.142179012298584,
            -1.5403268337249756,
            2.420405626296997,
            5.327308654785156,
            5.0404276847839355,
            0.7515479326248169,
            2.0022170543670654,
            4.233292579650879,
            -1.6325678825378418,
            -0.209003746509552,
            2.235349178314209,
            0.4898759126663208,
            -0.39584171772003174,
            -3.0453391075134277,
            4.06657600402832,
            1.7725460529327393,
            -2.8538005352020264,
            3.433682918548584,
            0.394874632358551,
            0.8615642189979553,
            2.6025280952453613,
            2.677245616912842,
            -3.57395076751709,
            2.7899088859558105,
            -1.7637686729431152,
            -1.6870942115783691,
            -8.177606582641602,
            0.9551445245742798,
            5.205571174621582,
            3.1017565727233887,
            -1.64235258102417,
            1.7471396923065186,
            1.514011025428772,
            -3.6736326217651367,
            -2.7766952514648438,
            -3.6035537719726562,
            1.4724383354187012,
            -1.672948956489563,
            -2.0568687915802,
            -0.7652315497398376,
            -3.0078742504119873,
            -5.775406837463379,
            0.6510167121887207,
            -0.5116410255432129,
            -6.683011054992676,
            -0.6018883585929871,
            -3.2051501274108887,
            1.8447344303131104,
            -1.9972538948059082,
            -1.324387550354004,
            4.063952922821045,
            2.0830955505371094,
            0.22321787476539612,
            4.090277671813965,
            4.529580593109131,
            -0.32049769163131714,
            -4.726729393005371,
            0.17719748616218567,
            0.8883696794509888,
            -2.8024370670318604,
            -1.3466359376907349,
            -1.9138731956481934,
            0.5989087224006653,
            -1.7002816200256348,
            1.7159476280212402,
            3.010951042175293,
            0.8044114112854004,
            -2.200493335723877,
            1.7427644729614258,
            2.645613670349121,
            0.11696511507034302,
            3.2851948738098145,
            1.134947419166565,
            7.38329553604126,
            -2.42232084274292,
            -4.889059543609619,
            -3.8606443405151367,
            -1.5435197353363037,
            -2.9240026473999023,
            3.0079216957092285,
            0.7307211756706238,
            -0.3964695334434509,
            0.017669498920440674,
            -3.4573426246643066,
            -2.309359550476074,
            -7.747049331665039,
            -2.219660997390747,
            -1.3843746185302734,
            1.3991596698760986,
            0.6778637170791626,
            1.599106788635254,
            -3.854707956314087,
            -1.4460501670837402,
            -0.11611390113830566,
            0.2221829891204834,
            -3.25233793258667,
            0.6462486982345581,
            1.9191635847091675,
            0.46721699833869934,
            -2.9963784217834473,
            -3.812870502471924,
            4.455110549926758,
            -4.928478240966797,
            -3.2066054344177246,
            -2.5541188716888428,
            2.598102569580078,
            3.4475252628326416,
            -2.0990591049194336,
            -1.5598526000976562,
            -1.539736270904541,
            0.5328552722930908,
            2.8308658599853516,
            4.491216659545898,
            0.9693060517311096,
            1.3294165134429932,
            3.4054923057556152,
            -0.9534189701080322,
            -2.775545120239258,
            0.90730881690979,
            -2.8199644088745117,
            -3.231851816177368,
            -0.6568982601165771,
            2.2815122604370117,
            -5.525850772857666,
            2.2221784591674805,
            3.4919700622558594,
            3.194288969039917,
            1.0758136510849,
            0.09718379378318787,
            4.093294620513916,
            -1.252500057220459,
            -0.18987759947776794,
            -2.024322032928467,
            -0.03377673029899597,
            -4.777925491333008,
            1.966459035873413,
            0.9495538473129272,
            2.4947562217712402,
            -3.084857225418091,
            3.654080390930176,
            -0.9752638936042786,
            5.121723651885986,
            2.9703264236450195,
            1.234679937362671,
            -2.6411263942718506,
            -6.100053787231445,
            -0.2151709944009781,
            -1.3229373693466187,
            3.388596773147583,
            -0.5771209597587585,
            1.3427962064743042,
            4.468689918518066,
            -1.3169045448303223,
            -0.23338055610656738,
            -1.2812519073486328,
            0.4355889856815338,
            3.348267078399658,
            -2.148721218109131,
            -0.5409021973609924,
            -2.280524730682373,
            -1.4748647212982178,
            -1.4599261283874512,
            2.9185500144958496,
            -0.15046218037605286,
            2.4317331314086914,
            1.8040919303894043,
            -3.670156002044678e-05,
            1.0684669017791748,
            -2.4892921447753906,
            0.7004314661026001,
            -0.6385822296142578,
            0.4091096520423889,
            0.14908891916275024,
            2.9843595027923584,
            2.716668128967285,
            -2.780632495880127,
            9.719295501708984,
            -2.0010170936584473,
            0.24377650022506714,
            -4.101696014404297,
            -1.2242910861968994,
            -5.793637275695801,
            -4.613219261169434,
            3.0054566860198975,
            -1.1927193403244019,
            -1.945576786994934,
            -0.35762858390808105,
            -7.29667854309082,
            2.2380802631378174,
            0.7999164462089539,
            0.16403520107269287,
            1.592806100845337,
            -2.502094030380249,
            2.2649331092834473,
            -1.8939764499664307,
            0.12176431715488434,
            -2.05342435836792,
            0.2847689092159271,
            -0.3588520288467407,
            0.9408588409423828,
            0.05252128839492798,
            0.6593115925788879,
            1.045562744140625,
            4.249845504760742,
            -3.679893970489502,
            0.24782532453536987,
            -2.170189380645752,
            -3.1465468406677246,
            0.011403992772102356,
            2.7172727584838867,
            0.960766077041626,
            -1.480676293373108,
            4.168820381164551,
            5.823735237121582,
            -2.981661081314087,
            0.7387273907661438,
            4.068509578704834,
            -0.553787112236023,
            -3.596775531768799,
            -2.7126407623291016,
            -3.146827220916748,
            -1.018451452255249,
            -1.3279919624328613,
            -6.277066230773926,
            -1.0106123685836792,
            0.06838935613632202,
            2.699558973312378,
            1.0245780944824219,
            1.182060718536377,
            1.8508236408233643,
            0.12861710786819458,
            2.085400104522705,
            5.975955486297607,
            2.9842522144317627,
            1.7018542289733887,
            0.18961095809936523,
            1.071908712387085,
            0.9258908033370972,
            -1.3085254430770874,
            4.009340763092041,
            -1.5410648584365845,
            2.5451819896698,
            -4.390405654907227,
            -2.473167657852173,
            1.4168492555618286,
            3.8024330139160156,
            1.82988703250885,
            1.0516818761825562,
            1.7228293418884277,
            -4.790028095245361,
            0.7334702014923096,
            2.7717490196228027,
            -3.4336910247802734,
            4.396989822387695,
            -0.48277705907821655,
            2.7674875259399414,
            1.631331205368042,
            2.894244432449341,
            -1.400589108467102,
            -4.062811851501465,
            1.5116239786148071,
            -2.5694127082824707,
            -1.877004623413086,
            -2.352466583251953,
            -1.0408377647399902,
            1.624321699142456,
            -3.0906457901000977,
            -0.6787219047546387,
            1.667729377746582,
            1.5638673305511475,
            -0.9555065035820007,
            3.8927154541015625,
            -0.7015575766563416,
            -0.23826536536216736,
            1.057473063468933,
            0.2796776294708252,
            -3.324338912963867,
            -4.580181121826172,
            -2.316133975982666,
            2.1910691261291504,
            -0.36289751529693604,
            0.4382091164588928,
            -2.34165096282959,
            0.16724199056625366,
            -1.5179109573364258,
            0.20615729689598083,
            3.974456310272217,
            0.5180940628051758,
            0.7615019083023071,
            -3.7919321060180664,
            -1.5297931432724,
            -0.1563255339860916,
            2.851853609085083,
            -3.1364035606384277,
            -0.34286248683929443,
            5.324907302856445,
            1.667737603187561,
            2.095885992050171,
            -0.5101562738418579,
            -2.300499677658081,
            -0.1812421977519989,
            1.9093780517578125,
            1.6711194515228271,
            0.5286083221435547,
            1.2940540313720703,
            -1.7958625555038452,
            -5.085904121398926,
            3.9539899826049805,
            0.8423548340797424,
            2.555790901184082,
            2.5008726119995117,
            -4.413496494293213,
            -0.17066946625709534,
            -0.6596773266792297,
            -1.4135175943374634,
            6.733180046081543,
            4.151403903961182,
            -0.34748005867004395,
            -6.471689701080322,
            -0.30117112398147583,
            1.1515209674835205,
            1.2697659730911255,
            -6.993828773498535,
            3.0626330375671387,
            0.6853730082511902,
            0.21762467920780182,
            3.130178451538086,
            -0.3932734429836273,
            -4.098576068878174,
            0.8659778237342834,
            -2.9872188568115234,
            1.9887372255325317,
            -0.08826303482055664,
            -0.34203436970710754,
            0.25588881969451904,
            1.1008731126785278,
            -1.7429393529891968,
            0.5276280045509338,
            0.21504704654216766,
            4.717642784118652,
            6.377293586730957,
            5.06463098526001,
            1.784954309463501,
            -1.774529218673706,
            0.7061525583267212,
            -1.010575294494629,
            2.170711040496826,
            2.9749698638916016,
            -3.410365343093872,
            -1.9558309316635132,
            -2.21431565284729,
            0.32810088992118835,
            1.816816806793213,
            4.386961936950684,
            -0.07471072673797607,
            4.628495216369629,
            -5.784883975982666,
            1.3774003982543945,
            -1.3867552280426025,
            3.014023780822754,
            2.9536051750183105,
            -1.7827770709991455,
            3.739715576171875,
            1.4264394044876099,
            -2.5473804473876953,
            -1.82079017162323,
            -0.4142764210700989,
            -1.0620253086090088,
            -0.5744184851646423,
            4.302115440368652,
            1.0356916189193726,
            1.4464383125305176,
            0.08249211311340332,
            3.39508318901062,
            0.4434836208820343,
            -0.47574472427368164,
            -2.83444881439209,
            2.4903931617736816,
            1.7066956758499146,
            -1.328663945198059,
            0.3723681569099426,
            -1.8477659225463867,
            3.3982367515563965,
            -2.4472904205322266,
            2.358189344406128,
            1.2405861616134644,
            1.8117852210998535,
            0.4434880018234253,
            -1.5923500061035156,
            -2.737124443054199,
            -1.0248242616653442,
            -1.8635843992233276,
            -4.028088569641113,
            -6.116372108459473,
            -2.064943552017212,
            -2.254127025604248,
            -2.347172737121582,
            3.021496295928955,
            -7.8308424949646,
            -0.8714601397514343,
            2.50968599319458,
            -2.2327022552490234,
            -0.21049003303050995,
            -0.9862924814224243,
            1.1314468383789062,
            -2.7999777793884277,
            3.3171591758728027,
            0.8802788853645325,
            0.2361660599708557,
            2.5014538764953613,
            -2.0534653663635254,
            3.9124417304992676,
            4.119553565979004,
            0.21888485550880432,
            0.6178905963897705,
            -1.9399420022964478,
            2.7382991313934326,
            -0.4682772159576416,
            -0.6912937164306641,
            -1.2645881175994873,
            -3.4029955863952637,
            1.7587801218032837,
            11.086627960205078,
            0.6770924925804138,
            -2.5171079635620117,
            -2.258427381515503,
            -3.3122265338897705,
            -2.556661367416382,
            -0.6393555402755737,
            0.3823692798614502,
            -1.799950122833252,
            0.5752992630004883,
            -0.3571600914001465,
            -1.0628066062927246,
            0.8582676649093628,
            1.8337533473968506,
            -2.460181713104248,
            -0.17404893040657043,
            -1.7138659954071045,
            2.6689722537994385,
            0.7276015877723694,
            1.204169750213623,
            0.850362241268158,
            0.8839185237884521,
            -2.665339469909668,
            -2.944765567779541,
            -1.936082363128662,
            2.5502357482910156,
            3.661339282989502,
            5.257746696472168,
            -3.1253254413604736,
            -0.7601205110549927,
            2.1500678062438965,
            1.4367718696594238,
            -2.4862236976623535,
            2.1492624282836914,
            -0.9054455161094666,
            5.547417640686035,
            2.990227699279785,
            0.3765087127685547,
            0.11517107486724854,
            -0.19149404764175415,
            -0.9621164798736572,
            1.4355494976043701,
            0.643557071685791,
            0.460948646068573,
            -5.173468112945557,
            -1.1568989753723145,
            0.881453275680542,
            -1.4462924003601074,
            -3.9239258766174316,
            3.6439571380615234,
            -2.3102171421051025,
            -1.3947527408599854,
            -2.0917723178863525,
            0.42621520161628723,
            0.27004197239875793,
            -0.981387197971344,
            -3.395801544189453,
            -3.118152618408203,
            0.1674395203590393,
            -3.032499313354492,
            2.086108684539795,
            -1.0368620157241821,
            0.9607482552528381,
            -0.8927177786827087,
            -2.469813823699951,
            -0.1929095983505249,
            -2.592555522918701,
            2.9868602752685547,
            0.05439433455467224,
            0.33198267221450806,
            2.1876134872436523,
            0.1131051778793335,
            -3.1435346603393555,
            0.36865925788879395,
            2.7812228202819824,
            -4.713486194610596,
            3.023611545562744,
            -0.6738800406455994,
            -1.9806857109069824,
            7.529593467712402,
            -0.63874351978302,
            3.4554572105407715,
            -1.7838549613952637,
            -0.3079298734664917,
            2.8864645957946777,
            -4.28262996673584,
            2.5941314697265625,
            1.414488673210144,
            -3.5334696769714355,
            6.142982006072998,
            -0.8530731797218323,
            -1.1162152290344238,
            5.210507392883301,
            3.433577537536621,
            0.7423782348632812,
            -3.6443052291870117,
            -1.72588312625885,
            -4.2783098220825195,
            -2.811093807220459,
            -0.1413826048374176,
            4.268726825714111,
            5.156484603881836,
            -0.5550127029418945,
            -3.9308366775512695,
            0.20875364542007446,
            0.010720759630203247,
            -1.7608990669250488,
            -6.217798233032227,
            -3.4376869201660156,
            -1.309873104095459,
            3.317487955093384,
            -4.634644031524658,
            -2.535813331604004,
            0.6100419759750366,
            3.6945858001708984,
            -2.992745876312256,
            2.5006442070007324,
            -1.2994794845581055,
            2.076852560043335,
            1.3281842470169067,
            1.6826653480529785,
            -0.880527138710022,
            0.0691765546798706,
            -2.8316688537597656,
            -1.884865403175354,
            -2.392956495285034,
            0.7191849946975708,
            -1.6789425611495972,
            -2.7350103855133057,
            3.2323765754699707,
            2.8633170127868652,
            -1.6892398595809937,
            -3.5958292484283447,
            0.1419582962989807,
            0.4962771534919739,
            -2.458723545074463,
            1.5965949296951294,
            -0.28087860345840454,
            -0.8613983392715454,
            1.7765165567398071,
            3.7384657859802246,
            -2.593736171722412,
            -1.6828362941741943,
            8.047861099243164,
            1.4567806720733643,
            1.0815403461456299,
            -2.575216054916382,
            0.2745124101638794,
            -2.001875400543213,
            0.5874704122543335,
            1.5582408905029297,
            -2.2371883392333984,
            2.2269678115844727,
            -2.907553195953369,
            1.942779302597046,
            -1.3606016635894775
        ]
    },
    "authors": [
        {
            "authorId": "2314918537",
            "name": "Leo Donisch"
        },
        {
            "authorId": "28544288",
            "name": "Sigurd Schacht"
        },
        {
            "authorId": "2305847311",
            "name": "Carsten Lanquillon"
        }
    ],
    "references": [
        {
            "paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey"
        },
        {
            "paperId": "fdc53c2c10742464087c0525f77e32604827a21d",
            "title": "Efficient Streaming Language Models with Attention Sinks"
        },
        {
            "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
            "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
        },
        {
            "paperId": "eb2c2330177f765038a2b17e2ee3498965865797",
            "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
        },
        {
            "paperId": "0b0debb710366cdff461938c80763eace1651af6",
            "title": "Code Llama: Open Foundation Models for Code"
        },
        {
            "paperId": "aeb9454987c3f85563cf7a5d2cb7f3d502d3398d",
            "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "a57ef1f5c3af185af79751855b8033b7fc6d89b3",
            "title": "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes"
        },
        {
            "paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6",
            "title": "A Simple and Effective Pruning Approach for Large Language Models"
        },
        {
            "paperId": "58fce438f817b46d37d072f8af7dfa4fd2dcd866",
            "title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization"
        },
        {
            "paperId": "f5359f596e0306599b4aa4157e6fe03567b35c01",
            "title": "Knowledge Distillation of Large Language Models"
        },
        {
            "paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df",
            "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"
        },
        {
            "paperId": "aa44b28b7c4c4a56d1f59ab4669215b667822c25",
            "title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models"
        },
        {
            "paperId": "b463f5dd8ed6871ddeac03754c1c2d99547b08fe",
            "title": "LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"
        },
        {
            "paperId": "ea75117f34b168a20f2a4309ac2eb685ca6b1436",
            "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance"
        },
        {
            "paperId": "5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35",
            "title": "The False Promise of Imitating Proprietary LLMs"
        },
        {
            "paperId": "758985395372f5378fcf036094195b2848e13a21",
            "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning"
        },
        {
            "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
        },
        {
            "paperId": "a10843d1349fff8d2a7d9722f800802187fef67f",
            "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"
        },
        {
            "paperId": "02529b2666a536053a2e2940de5b28de36fd594b",
            "title": "Lion: Adversarial Distillation of Proprietary Large Language Models"
        },
        {
            "paperId": "017010b941d902a467f6d329ae5e74fd67e67912",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"
        },
        {
            "paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
            "title": "StarCoder: may the source be with you!"
        },
        {
            "paperId": "aad167be3c902388ea625da4117fcae4325b8b7d",
            "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"
        },
        {
            "paperId": "56fa65d8dc41708082f9b2ef7752c49cee9ebe01",
            "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation"
        },
        {
            "paperId": "389ec3e8902a5dcfcde1adec735854e93f845937",
            "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"
        },
        {
            "paperId": "8f48c75e1354c88a84a67abb60789083c12e5037",
            "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation"
        },
        {
            "paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e",
            "title": "Full Stack Optimization of Transformer Inference: a Survey"
        },
        {
            "paperId": "ae3ac5509c445327a23431409624a1333aa825b0",
            "title": "What Matters In The Structured Pruning of Generative Language Models?"
        },
        {
            "paperId": "fbd49b25bdab98c171af49962a41139c73dacbde",
            "title": "Specializing Smaller Language Models towards Multi-Step Reasoning"
        },
        {
            "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"
        },
        {
            "paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44",
            "title": "Large Language Models Are Reasoning Teachers"
        },
        {
            "paperId": "f9ad1fffa1cc76fd5db3ff758c0839492c5147c4",
            "title": "In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models"
        },
        {
            "paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1",
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"
        },
        {
            "paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e",
            "title": "Teaching Small Language Models to Reason"
        },
        {
            "paperId": "8fd462f6248d5e3f1b6602697c09489086b5655f",
            "title": "Distilling Reasoning Capabilities into Smaller Language Models"
        },
        {
            "paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644",
            "title": "Fast Inference from Transformers via Speculative Decoding"
        },
        {
            "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
        },
        {
            "paperId": "a2d2bbe4c542173662a444b33b76c66992697830",
            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions"
        },
        {
            "paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6",
            "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
        },
        {
            "paperId": "7d29a84a589aa5655e5d3fed8d725ea472816599",
            "title": "Explanations from Large Language Models Make Small Reasoners Better"
        },
        {
            "paperId": "003c08471fe579d98e82cf5c0cac03897403fb55",
            "title": "FP8 Quantization: The Power of the Exponent"
        },
        {
            "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
        },
        {
            "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a",
            "title": "Emergent Abilities of Large Language Models"
        },
        {
            "paperId": "e03609f2587f690867e7ea0bedaf0db25282c548",
            "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
        },
        {
            "paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
        },
        {
            "paperId": "72b989a52a5cc2eee44bba29e8d225ce7bc07666",
            "title": "Decoupled Knowledge Distillation"
        },
        {
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback"
        },
        {
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
            "paperId": "111e39b157b19ce5afb4abec031e8d0224d0ecd0",
            "title": "Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length"
        },
        {
            "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
            "title": "Evaluating Large Language Models Trained on Code"
        },
        {
            "paperId": "503503ec4395ab0e36d4f0a190772f7785649319",
            "title": "Towards Fully 8-bit Integer Inference for the Transformer Model"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
        },
        {
            "paperId": "c9d3c181d999b0e11c6e4c51b3f9aefd01489e0f",
            "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation"
        },
        {
            "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
            "title": "Longformer: The Long-Document Transformer"
        },
        {
            "paperId": "0c25bdcaa64997f466fff5afcb530bc19b0d9fa1",
            "title": "Explanations:"
        },
        {
            "paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"
        },
        {
            "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        },
        {
            "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
            "paperId": "ce106590145e89ea4b621c99665862967ccf5dac",
            "title": "Q8BERT: Quantized 8Bit BERT"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "01baca4fa7ad5d28b95f6f72fe33de9a34633bc6",
            "title": "Dissecting the NVidia Turing T4 GPU via Microbenchmarking"
        },
        {
            "paperId": "9a1093af92d315def21b90918faf08665157051a",
            "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
            "paperId": "4954fa180728932959997a4768411ff9136aac81",
            "title": "TensorFlow: A system for large-scale machine learning"
        },
        {
            "paperId": "31c36d445367ba204244bb74893c5654e31c3869",
            "title": "cuDNN: Efficient Primitives for Deep Learning"
        },
        {
            "paperId": "d770060812fb646b3846a7d398a3066145b5e3c8",
            "title": "Do Deep Nets Really Need to be Deep?"
        },
        {
            "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
        },
        {
            "paperId": "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9",
            "title": "Model compression"
        },
        {
            "paperId": "2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c",
            "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
        },
        {
            "paperId": "72c03b873e8c5cd86b15bf73186df341da4731c9",
            "title": "Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": "d2e4fb59b2bf04a0b324db22ed6caf97ffd8f56a",
            "title": "IEEE Standard for Floating-Point Arithmetic"
        },
        {
            "paperId": "da27e46316ae257f258516d7c51e256f0c4fad06",
            "title": "BORN AGAIN TREES"
        },
        {
            "paperId": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "title": "Optimal Brain Damage"
        },
        {
            "paperId": null,
            "title": "\u201cHow to train your (compressed) large language model,\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cOpen llm leaderboard,\u201d"
        },
        {
            "paperId": null,
            "title": "Quantization - Neural Network Distiller"
        },
        {
            "paperId": null,
            "title": "NVIDIA Blogs: TensorFloat-32 Accelerates AI Training HPC upto 20x"
        }
    ]
}