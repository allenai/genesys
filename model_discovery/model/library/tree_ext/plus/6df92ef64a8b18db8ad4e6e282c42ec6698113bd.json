{
    "paperId": "6df92ef64a8b18db8ad4e6e282c42ec6698113bd",
    "externalIds": {
        "DBLP": "journals/corr/abs-2407-11239",
        "ArXiv": "2407.11239",
        "DOI": "10.48550/arXiv.2407.11239",
        "CorpusId": 271218569
    },
    "title": "From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients",
    "abstract": "Modern Large Language Models (LLMs) are composed of matrices with billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Being significantly large, such matrices can often be expressed in low-rank format with potential to relax resource requirements. Unlike prior works which focus on developing novel matrix decomposition algorithms, in this work we first study the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices. Our findings reveal that different layers exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, we present Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail distribution of singular values to identify a suitable rank reduction ratio for matrices within LLMs. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. Our gradient perspective and extensive experiments illustrate that LRCs tend to have better finetuning capabilities and can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. For example, finetuning a 50\\% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs (WeLore) can outperform its full finetuning with ~3x better throughput and ~0.6x GPU requirement. Our codes are available at \\url{https://github.com/VITA-Group/welore}",
    "venue": "arXiv.org",
    "year": 2024,
    "referenceCount": 47,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work first study the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices, and presents Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.8799381256103516,
            0.530322790145874,
            -0.33658960461616516,
            3.070391893386841,
            -1.6109439134597778,
            -0.38055384159088135,
            1.5970888137817383,
            -2.3248472213745117,
            -0.6866758465766907,
            3.19287109375,
            0.916210412979126,
            4.63435697555542,
            1.2474044561386108,
            -1.0034966468811035,
            -3.504086971282959,
            -1.993327021598816,
            -2.040731906890869,
            1.8104066848754883,
            4.667272090911865,
            0.720058798789978,
            -4.157421112060547,
            -1.3359755277633667,
            -3.61991024017334,
            4.4372687339782715,
            -3.787999391555786,
            -1.2918742895126343,
            0.4711347222328186,
            0.7145374417304993,
            3.8469767570495605,
            -2.1504197120666504,
            -0.03065037727355957,
            -5.355025291442871,
            6.536272048950195,
            -7.037237644195557,
            3.6990795135498047,
            -6.814875602722168,
            -2.3896265029907227,
            5.568658351898193,
            -3.2347888946533203,
            -0.4614909887313843,
            1.8471148014068604,
            -0.9422262907028198,
            2.717395782470703,
            1.72727632522583,
            0.043796420097351074,
            1.4301049709320068,
            0.7755721807479858,
            1.6991688013076782,
            0.8161964416503906,
            2.6202392578125,
            0.7282630205154419,
            -2.1197571754455566,
            3.052389621734619,
            -0.4921681582927704,
            0.11045390367507935,
            -2.606445550918579,
            1.1230003833770752,
            -0.32663029432296753,
            -0.6574326157569885,
            -1.918602705001831,
            3.620056629180908,
            2.485443115234375,
            -0.7611397504806519,
            1.7993652820587158,
            1.3896876573562622,
            -1.238044261932373,
            -2.867023229598999,
            8.379451751708984,
            0.26559096574783325,
            0.09783458709716797,
            0.7539594173431396,
            -4.0773701667785645,
            1.0799952745437622,
            0.3980165123939514,
            -5.432681560516357,
            2.181380271911621,
            1.5407383441925049,
            -8.122007369995117,
            -1.9065196514129639,
            -2.208387613296509,
            0.19469831883907318,
            1.1968309879302979,
            -2.1091151237487793,
            1.4836218357086182,
            4.4890456199646,
            -0.2830009162425995,
            -2.2702651023864746,
            0.8424651622772217,
            4.042187690734863,
            -2.725458860397339,
            -0.3290504515171051,
            1.4714446067810059,
            -0.3739755153656006,
            1.9907084703445435,
            -2.8298356533050537,
            1.8790268898010254,
            -0.13693931698799133,
            -0.7341468930244446,
            -1.9517911672592163,
            0.8041058778762817,
            1.382478952407837,
            0.3780286908149719,
            1.7267735004425049,
            -0.45511120557785034,
            2.5044233798980713,
            -2.2099523544311523,
            -2.227194309234619,
            1.3173224925994873,
            1.9264781475067139,
            -1.9406954050064087,
            -0.4039689302444458,
            1.8402549028396606,
            -1.0022614002227783,
            -0.622429370880127,
            -3.618295192718506,
            -4.629417419433594,
            -2.556644916534424,
            -1.1014989614486694,
            -2.2158076763153076,
            4.213531494140625,
            -4.516070365905762,
            -0.30401331186294556,
            -5.1531662940979,
            3.678922176361084,
            -0.7697097063064575,
            4.0484771728515625,
            -1.7112867832183838,
            -1.058302640914917,
            -1.7234556674957275,
            -1.6975654363632202,
            0.08120566606521606,
            -1.6960668563842773,
            2.8182127475738525,
            -0.421573281288147,
            4.562316417694092,
            2.967966079711914,
            -4.19083309173584,
            2.319326400756836,
            -2.476675510406494,
            -2.0146725177764893,
            3.704315185546875,
            2.7156503200531006,
            2.188199043273926,
            -1.0825539827346802,
            -1.860799789428711,
            2.249101161956787,
            3.2465689182281494,
            4.021383285522461,
            -1.253273844718933,
            6.7326483726501465,
            3.761035680770874,
            -4.54660177230835,
            1.3151867389678955,
            0.21836555004119873,
            0.303276002407074,
            2.6078882217407227,
            -5.469674587249756,
            -0.05074280500411987,
            1.6197527647018433,
            -0.25977736711502075,
            0.948607861995697,
            1.1414002180099487,
            -9.841872215270996,
            -1.260028600692749,
            4.1978864669799805,
            0.29778504371643066,
            -1.8315497636795044,
            -0.5487450361251831,
            -1.455686092376709,
            3.882208824157715,
            -1.837415099143982,
            2.2287535667419434,
            2.583101511001587,
            2.811028480529785,
            0.9272763729095459,
            5.370043754577637,
            0.6033221483230591,
            -0.7316136360168457,
            -3.226881504058838,
            0.6257146596908569,
            -0.3407644033432007,
            -2.607959270477295,
            -7.280409336090088,
            3.054173469543457,
            -6.14484977722168,
            -0.41208457946777344,
            -1.3607499599456787,
            -3.232246160507202,
            -2.169332504272461,
            0.6254788637161255,
            -1.0024404525756836,
            0.2693594992160797,
            3.455751895904541,
            4.118133068084717,
            1.3911532163619995,
            1.4113683700561523,
            5.314912796020508,
            -0.031648993492126465,
            -3.9010000228881836,
            2.4697766304016113,
            4.861181259155273,
            -0.14465057849884033,
            0.5617722272872925,
            0.6539193391799927,
            2.428395986557007,
            0.2212693691253662,
            -1.5763124227523804,
            3.743030548095703,
            2.202343225479126,
            2.722576141357422,
            1.546955943107605,
            0.6665983200073242,
            -1.2259204387664795,
            0.49271029233932495,
            -3.887270927429199,
            -2.649794816970825,
            -3.054021120071411,
            3.164008617401123,
            2.1654086112976074,
            0.4939841628074646,
            -0.25635212659835815,
            -4.4804582595825195,
            2.681278705596924,
            -4.697617530822754,
            1.4690721035003662,
            -5.03372859954834,
            2.23641300201416,
            0.46258118748664856,
            0.10139727592468262,
            -0.08284151554107666,
            0.6859428286552429,
            -4.560985088348389,
            -2.7490978240966797,
            0.05120900273323059,
            -6.0340576171875,
            -2.705566883087158,
            -3.4314146041870117,
            3.737454891204834,
            0.9881563782691956,
            -1.239916443824768,
            5.387495040893555,
            2.1268200874328613,
            1.5487302541732788,
            3.620971202850342,
            2.8045153617858887,
            0.28923583030700684,
            -1.3908379077911377,
            3.5756211280822754,
            0.6158568859100342,
            -2.401043176651001,
            -1.3627269268035889,
            -4.63714599609375,
            2.2472550868988037,
            -5.116559982299805,
            2.9450080394744873,
            4.289240837097168,
            3.522352695465088,
            -0.8406927585601807,
            2.645625114440918,
            0.7647719979286194,
            0.266620397567749,
            0.6181385517120361,
            2.18902850151062,
            5.78765869140625,
            -0.6045994758605957,
            -0.5609195232391357,
            -3.972346544265747,
            -1.955984354019165,
            -1.450765609741211,
            -1.5317264795303345,
            3.346804618835449,
            1.1382783651351929,
            -1.0238547325134277,
            -4.418152809143066,
            -2.4479479789733887,
            -7.7474470138549805,
            -2.993778705596924,
            -2.496417999267578,
            1.634073257446289,
            3.6629278659820557,
            2.6690945625305176,
            -3.2274091243743896,
            -3.2416563034057617,
            -0.9604971408843994,
            -1.1423864364624023,
            -0.6409804821014404,
            -2.6673638820648193,
            -1.5340012311935425,
            1.2175980806350708,
            -0.33686912059783936,
            -0.8745699524879456,
            1.3590620756149292,
            -1.2627334594726562,
            -3.40814208984375,
            -1.9726426601409912,
            3.532971143722534,
            3.794161319732666,
            -0.41302889585494995,
            -1.1019937992095947,
            1.669844627380371,
            1.0442724227905273,
            3.2318198680877686,
            2.8427109718322754,
            0.31645265221595764,
            0.5783581733703613,
            5.3770060539245605,
            0.4303938150405884,
            -3.133251905441284,
            1.2491217851638794,
            -3.4838311672210693,
            -4.663334369659424,
            -2.646151304244995,
            5.213316917419434,
            -4.471973896026611,
            1.0260889530181885,
            -0.7034214735031128,
            2.826303720474243,
            0.17025578022003174,
            0.22452759742736816,
            1.5255801677703857,
            0.013361811637878418,
            1.0400035381317139,
            -5.835060119628906,
            1.4574607610702515,
            -5.066642761230469,
            1.49276602268219,
            4.242276191711426,
            5.322805404663086,
            -2.005545139312744,
            2.17559552192688,
            -0.8690320253372192,
            8.289583206176758,
            5.009284496307373,
            -1.7012044191360474,
            -2.7029991149902344,
            -4.308114528656006,
            1.3521065711975098,
            0.3973626494407654,
            0.47577571868896484,
            2.8931939601898193,
            0.30640316009521484,
            4.933140277862549,
            0.5194503664970398,
            2.6227235794067383,
            -5.547358989715576,
            0.8139389157295227,
            0.2056601494550705,
            -3.2052369117736816,
            -0.9482275247573853,
            0.5235031843185425,
            1.714174509048462,
            -0.23498180508613586,
            4.310786724090576,
            -1.2985174655914307,
            -0.36513379216194153,
            3.631575107574463,
            3.0170764923095703,
            0.46769165992736816,
            0.5455076694488525,
            0.5687355995178223,
            2.0297136306762695,
            1.9162964820861816,
            2.6936800479888916,
            1.7550833225250244,
            3.7662758827209473,
            0.14832353591918945,
            10.911642074584961,
            0.18698328733444214,
            2.6048967838287354,
            -1.4116694927215576,
            -2.0162553787231445,
            -2.61617374420166,
            -1.2781457901000977,
            0.4901597797870636,
            -4.35003137588501,
            -1.7932953834533691,
            1.7123186588287354,
            -3.1002469062805176,
            2.684826374053955,
            1.9244660139083862,
            -0.038672953844070435,
            4.848696231842041,
            0.051246896386146545,
            0.00508706271648407,
            -0.595190167427063,
            1.1819956302642822,
            -1.422813892364502,
            2.773470401763916,
            0.06803902983665466,
            3.010265588760376,
            -1.115840196609497,
            -0.3233245611190796,
            -1.7296562194824219,
            3.1329188346862793,
            -3.8468148708343506,
            1.0664093494415283,
            -3.0950493812561035,
            -4.797454833984375,
            -0.5715596675872803,
            4.2947893142700195,
            3.0540449619293213,
            -0.605230450630188,
            5.013186454772949,
            5.83817195892334,
            -4.41800594329834,
            -1.019087314605713,
            2.6563429832458496,
            -2.9002232551574707,
            -1.9664138555526733,
            0.9197044372558594,
            -3.6291134357452393,
            -4.933768272399902,
            -0.03479403257369995,
            -4.3778886795043945,
            2.5680837631225586,
            -0.9725430011749268,
            1.9565436840057373,
            1.00126051902771,
            1.638948678970337,
            5.275308609008789,
            0.4751194715499878,
            2.0076241493225098,
            4.9456987380981445,
            0.7458136081695557,
            1.9474960565567017,
            1.319922685623169,
            3.2314724922180176,
            2.571145534515381,
            -0.14048051834106445,
            2.111497163772583,
            1.5221576690673828,
            4.626609802246094,
            -2.8142220973968506,
            -0.9829257726669312,
            2.1647307872772217,
            2.7203660011291504,
            0.22876796126365662,
            1.0545834302902222,
            0.21220499277114868,
            2.6861777305603027,
            1.4662253856658936,
            4.378031253814697,
            -3.066972017288208,
            3.0866751670837402,
            1.738987684249878,
            2.467796802520752,
            3.2215890884399414,
            1.1313114166259766,
            -2.0366873741149902,
            -3.551095724105835,
            3.2462081909179688,
            -3.3685803413391113,
            -3.4266748428344727,
            -0.9464189410209656,
            -1.5527592897415161,
            -2.495223045349121,
            -2.6973888874053955,
            -1.7677834033966064,
            0.12639200687408447,
            1.2652099132537842,
            -3.926295280456543,
            5.042147636413574,
            -1.0610427856445312,
            0.12529510259628296,
            0.930192232131958,
            -1.200912356376648,
            -2.737053632736206,
            -3.7437171936035156,
            -0.3884134888648987,
            1.8242794275283813,
            2.3891024589538574,
            -2.731393814086914,
            0.13271673023700714,
            -0.31518757343292236,
            -2.0079593658447266,
            -0.582908570766449,
            2.9948697090148926,
            0.3764292001724243,
            -0.11403030157089233,
            -8.027929306030273,
            -3.2338225841522217,
            -0.7321244478225708,
            3.2579870223999023,
            -4.621820449829102,
            -2.8624424934387207,
            5.9260334968566895,
            2.9926066398620605,
            1.302900791168213,
            3.14969539642334,
            1.6479517221450806,
            -0.3251791000366211,
            -0.7855581641197205,
            2.999307632446289,
            1.4566084146499634,
            2.8204212188720703,
            -1.6777925491333008,
            -6.397983551025391,
            0.7868967056274414,
            2.096381425857544,
            -1.0048848390579224,
            0.41850459575653076,
            -5.782773494720459,
            -2.4734926223754883,
            -1.2266196012496948,
            -3.1034908294677734,
            4.068363666534424,
            3.1778724193573,
            -0.2080150842666626,
            -1.6311007738113403,
            -2.795772075653076,
            -1.7975047826766968,
            1.7290687561035156,
            -5.876490592956543,
            1.0736747980117798,
            0.5614308714866638,
            1.7057503461837769,
            1.592674970626831,
            -2.7728161811828613,
            0.13075000047683716,
            3.913090229034424,
            -3.1882376670837402,
            1.1360622644424438,
            -0.5975743532180786,
            2.8643109798431396,
            -2.340550422668457,
            0.907525360584259,
            -1.547743320465088,
            2.527754306793213,
            -0.6039904356002808,
            5.337543964385986,
            5.644900798797607,
            5.554208278656006,
            3.4031455516815186,
            -1.2588093280792236,
            -2.271590232849121,
            -4.107831001281738,
            0.15380121767520905,
            2.8562488555908203,
            -4.851226806640625,
            -2.7566871643066406,
            -2.5617358684539795,
            2.4512929916381836,
            0.8912268280982971,
            1.7318534851074219,
            -1.1886508464813232,
            0.8727545738220215,
            -3.762878656387329,
            -0.11387574672698975,
            -5.383841514587402,
            2.650766372680664,
            -0.05019932985305786,
            -0.5812103748321533,
            3.111340045928955,
            -0.1106424331665039,
            -0.21239055693149567,
            2.2219769954681396,
            -1.859800100326538,
            -1.1158369779586792,
            -4.336863994598389,
            3.6149184703826904,
            3.085090160369873,
            -2.272977828979492,
            0.8712679743766785,
            -0.41370508074760437,
            -2.084716796875,
            -5.161438465118408,
            -4.116175651550293,
            2.2067677974700928,
            1.3233609199523926,
            -1.807051181793213,
            0.6723102927207947,
            -2.8085904121398926,
            -0.8114364743232727,
            0.783367931842804,
            0.8848118782043457,
            2.749783992767334,
            3.787116527557373,
            3.381380558013916,
            -3.8325490951538086,
            -1.519955039024353,
            0.618477463722229,
            -3.574522018432617,
            -5.583029747009277,
            -3.9433090686798096,
            1.1992146968841553,
            -2.1936588287353516,
            -1.3675124645233154,
            -1.5061848163604736,
            -4.265739917755127,
            -1.8762181997299194,
            1.8583014011383057,
            -5.519738674163818,
            -0.59928959608078,
            -3.30361270904541,
            0.006375372409820557,
            -2.236611843109131,
            1.3366687297821045,
            -1.7337028980255127,
            -3.1707191467285156,
            -1.1323049068450928,
            1.9398092031478882,
            4.31024169921875,
            8.068496704101562,
            0.5205962657928467,
            -2.8739945888519287,
            -1.6290481090545654,
            0.3004622459411621,
            0.676050066947937,
            -1.5419752597808838,
            -2.529934883117676,
            1.3154335021972656,
            2.2817769050598145,
            16.51013946533203,
            -3.1536355018615723,
            1.26212477684021,
            -1.0977284908294678,
            1.1899291276931763,
            -3.3559165000915527,
            -3.8697187900543213,
            -0.08234104514122009,
            0.85699462890625,
            1.5797404050827026,
            -2.2686595916748047,
            -1.2446357011795044,
            1.8147541284561157,
            1.1805475950241089,
            1.0609707832336426,
            -0.10635925829410553,
            -1.6441106796264648,
            3.0243008136749268,
            -4.279012680053711,
            -1.1194305419921875,
            0.8912736773490906,
            0.11196261644363403,
            0.7820839881896973,
            -0.07762989401817322,
            -1.1611865758895874,
            0.8465875387191772,
            1.3955111503601074,
            2.2516167163848877,
            -4.796795845031738,
            0.4343516230583191,
            2.230156898498535,
            3.5229361057281494,
            -1.4455163478851318,
            1.4849932193756104,
            -0.761821448802948,
            2.573159694671631,
            1.8222135305404663,
            -0.10361960530281067,
            2.270019054412842,
            2.567939043045044,
            -1.156273365020752,
            0.7117698192596436,
            -2.5892882347106934,
            3.1485755443573,
            -4.824618339538574,
            -2.4320778846740723,
            0.10971999168395996,
            -5.057478904724121,
            -1.4931654930114746,
            2.0013654232025146,
            -3.1166722774505615,
            0.21882852911949158,
            -3.3892617225646973,
            -0.21426230669021606,
            2.9370369911193848,
            3.8719706535339355,
            -2.2366480827331543,
            2.336205244064331,
            3.812981367111206,
            6.229213714599609,
            1.0054525136947632,
            -1.1627802848815918,
            -1.3678323030471802,
            -2.9498729705810547,
            -0.6389588713645935,
            2.3335061073303223,
            -1.8742719888687134,
            -1.5448353290557861,
            1.3930745124816895,
            2.575758218765259,
            3.7878053188323975,
            -0.03319384157657623,
            0.6460217833518982,
            -1.444341778755188,
            2.1843631267547607,
            -2.340883255004883,
            3.6724042892456055,
            1.7551178932189941,
            -0.4157394766807556,
            8.727684020996094,
            -2.9440581798553467,
            0.04149618744850159,
            -2.6880712509155273,
            -3.090034008026123,
            5.750051975250244,
            -1.6111979484558105,
            6.582125663757324,
            0.7984851598739624,
            -0.21059110760688782,
            3.9188103675842285,
            -1.6033399105072021,
            0.8323377966880798,
            0.9285441040992737,
            5.086514472961426,
            5.3683881759643555,
            -1.2759721279144287,
            -3.005056142807007,
            -3.0336484909057617,
            -5.4134521484375,
            -1.386022925376892,
            4.695672988891602,
            4.188158988952637,
            0.0761101245880127,
            -1.4648164510726929,
            1.7867920398712158,
            1.1257315874099731,
            -3.8432633876800537,
            -4.510555744171143,
            -3.2549264430999756,
            -1.5083065032958984,
            1.2029520273208618,
            -3.6294901371002197,
            -2.023087739944458,
            -1.6519644260406494,
            2.374096393585205,
            -1.195220708847046,
            2.809526205062866,
            1.5811344385147095,
            1.040716528892517,
            5.031259536743164,
            1.349678874015808,
            -1.973828911781311,
            -1.91849946975708,
            0.08265683054924011,
            1.1724175214767456,
            1.5781750679016113,
            -1.7184406518936157,
            -0.907158613204956,
            -1.6916162967681885,
            -0.8842443227767944,
            -0.025007426738739014,
            0.558578610420227,
            -0.568298876285553,
            0.5757932662963867,
            -1.258109450340271,
            -3.6538686752319336,
            1.1708738803863525,
            0.4522610902786255,
            -1.5155751705169678,
            1.8889551162719727,
            4.447822570800781,
            -3.273393154144287,
            -2.2294254302978516,
            8.36407470703125,
            -0.3627811372280121,
            3.2087740898132324,
            -3.217552661895752,
            -3.9862160682678223,
            -3.0845396518707275,
            0.05070027709007263,
            -0.11747899651527405,
            -1.3155807256698608,
            2.4432716369628906,
            1.1075165271759033,
            -1.127514123916626,
            -2.8060836791992188
        ]
    },
    "authors": [
        {
            "authorId": "2253397454",
            "name": "A. Jaiswal"
        },
        {
            "authorId": "2254142682",
            "name": "Lu Yin"
        },
        {
            "authorId": "2109338656",
            "name": "Zhenyu (Allen) Zhang"
        },
        {
            "authorId": "2255081092",
            "name": "Shiwei Liu"
        },
        {
            "authorId": "2310775826",
            "name": "Jiawei Zhao"
        },
        {
            "authorId": "2249538771",
            "name": "Yuandong Tian"
        },
        {
            "authorId": "2284563898",
            "name": "Zhangyang Wang"
        }
    ],
    "references": [
        {
            "paperId": "be12e6806e9190c3954cb34af7a6923b65cfedba",
            "title": "LoRA Learns Less and Forgets Less"
        },
        {
            "paperId": "ee4014497ccf2f65d6e05d3956b0e6b0c7369bae",
            "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"
        },
        {
            "paperId": "4706711dc4e1e16424db9f454a1f3b092b972785",
            "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression"
        },
        {
            "paperId": "c1fa6255cc9fc3128f74befc7855e255bc7a2c6e",
            "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
        },
        {
            "paperId": "241eefc1bb11e693e0fef6977a65a0a822fb8f5e",
            "title": "LoRA+: Efficient Low Rank Adaptation of Large Models"
        },
        {
            "paperId": "da053e2a4ba1b244940c8f2cad5dcdf0d730f85f",
            "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"
        },
        {
            "paperId": "840f759d110aea21ebbfe0523c435e11a2759aba",
            "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
        },
        {
            "paperId": "dd3f133837adbc5164046906e174726af9ff5f21",
            "title": "SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing"
        },
        {
            "paperId": "5122bd001ec67d80543abe284bf7e0bf31da45d5",
            "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning"
        },
        {
            "paperId": "5c7a21e9262b62f0a27fefdc8b1270dfdcbd3912",
            "title": "The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction"
        },
        {
            "paperId": "39ad56e1e94c71b5366ac0b41d48b5b7bba7d64a",
            "title": "Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models"
        },
        {
            "paperId": "a1bcf68d6ed2fec1ecaf16b67f2d19bc20c00ee6",
            "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models"
        },
        {
            "paperId": "4c387de9f5d32b9990cd1a006fef17101ce98d4c",
            "title": "Tied-LoRA: Enhancing parameter efficiency of LoRA with Weight Tying"
        },
        {
            "paperId": "faa4c46e1cbd99e486c7dc2881e024b79967961b",
            "title": "S-LoRA: Serving Thousands of Concurrent LoRA Adapters"
        },
        {
            "paperId": "43017a16dfe593c09533c5fb3c3612c83761a98a",
            "title": "Matrix Compression via Randomized Low Rank and Low Precision Factorization"
        },
        {
            "paperId": "faab24bc6cd4a4dea6e82420d145f08445c05fc7",
            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"
        },
        {
            "paperId": "4e13ecf80443a4135d516b7ba77eca82b5c6d347",
            "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple"
        },
        {
            "paperId": "8ec117feff6ee10e3b20a19ac101fee5c99e14d7",
            "title": "LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression"
        },
        {
            "paperId": "8ce219059d777c2333ee21cb2af2aad71275c98f",
            "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"
        },
        {
            "paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6",
            "title": "A Simple and Effective Pruning Approach for Large Language Models"
        },
        {
            "paperId": "bc8428e270a5474cabfaff578d44955f757ccacd",
            "title": "LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation"
        },
        {
            "paperId": "86bb2a8cf04991a99a21dc7462a861214002d805",
            "title": "InRank: Incremental Low-Rank Learning"
        },
        {
            "paperId": "7e61acdfd6939532a628f3bed13658976d6b6fa4",
            "title": "Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models"
        },
        {
            "paperId": "9d460930d9b5d12a65ff2b3efa23047ec75fbca1",
            "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter"
        },
        {
            "paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df",
            "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"
        },
        {
            "paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9",
            "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
        },
        {
            "paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2",
            "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"
        },
        {
            "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
        },
        {
            "paperId": "a10843d1349fff8d2a7d9722f800802187fef67f",
            "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"
        },
        {
            "paperId": "a06a4a38668c4737ab2ce80badc177ea3f520456",
            "title": "Cuttlefish: Low-Rank Model Training without All the Tuning"
        },
        {
            "paperId": "fdacdbc6a00eeb42efe7f81848b0bc09be5ca997",
            "title": "Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!"
        },
        {
            "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"
        },
        {
            "paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6",
            "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
        },
        {
            "paperId": "d151ced8700d84a2efe411a234a4cb2c595e8ca9",
            "title": "Language model compression with weighted low-rank factorization"
        },
        {
            "paperId": "5faa744dcc28cbbdd9bd67eb703320c6e2d85e52",
            "title": "Black-box Prompt Learning for Pre-trained Language Models"
        },
        {
            "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "title": "LoRA: Low-Rank Adaptation of Large Language Models"
        },
        {
            "paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d",
            "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"
        },
        {
            "paperId": "253bafbd4f7082d87ee5f333223e2a0d49613e36",
            "title": "AutoFreeze: Automatically Freezing Model Blocks to Accelerate Fine-tuning"
        },
        {
            "paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "title": "WARP: Word-level Adversarial ReProgramming"
        },
        {
            "paperId": "f582f8c666b7c7f65691c4175b6607aad751530d",
            "title": "Accelerated Dynamic MRI Using Kernel-Based Low Rank Constraint"
        },
        {
            "paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c",
            "title": "Parameter-Efficient Transfer Learning for NLP"
        },
        {
            "paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d",
            "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"
        },
        {
            "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
            "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
        },
        {
            "paperId": "a7484ac305e3746c9e612d558345d9664bd436de",
            "title": "FreezeOut: Accelerate Training by Progressively Freezing Layers"
        },
        {
            "paperId": "fec3b4c089158d1920af1e59a8b2b979aaeccdfb",
            "title": "Multidimensional Compressed Sensing MRI Using Tensor Decomposition-Based Sparsifying Transform"
        },
        {
            "paperId": "7a22c2061d7161a2966b6a50abdb01a97c785ea0",
            "title": "Accelerated Dynamic MRI Exploiting Sparsity and Low-Rank Structure: k-t SLR"
        },
        {
            "paperId": null,
            "title": "Stack more layers differently: High-rank training through low-rank updates"
        }
    ]
}