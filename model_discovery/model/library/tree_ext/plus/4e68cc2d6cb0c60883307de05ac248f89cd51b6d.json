{
    "paperId": "4e68cc2d6cb0c60883307de05ac248f89cd51b6d",
    "externalIds": {
        "DBLP": "journals/corr/abs-2407-20018",
        "ArXiv": "2407.20018",
        "DOI": "10.48550/arXiv.2407.20018",
        "CorpusId": 271533993
    },
    "title": "Efficient Training of Large Language Models on Distributed Infrastructures: A Survey",
    "abstract": "Large Language Models (LLMs) like GPT and LLaMA are revolutionizing the AI industry with their sophisticated capabilities. Training these models requires vast GPU clusters and significant computing time, posing major challenges in terms of scalability, efficiency, and reliability. This survey explores recent advancements in training systems for LLMs, including innovations in training infrastructure with AI accelerators, networking, storage, and scheduling. Additionally, the survey covers parallelism strategies, as well as optimizations for computation, communication, and memory in distributed LLM training. It also includes approaches of maintaining system reliability over extended training periods. By examining current innovations and future directions, this survey aims to provide valuable insights towards improving LLM training systems and tackling ongoing challenges. Furthermore, traditional digital circuit-based computing systems face significant constraints in meeting the computational demands of LLMs, highlighting the need for innovative solutions such as optical computing and optical networks.",
    "venue": "arXiv.org",
    "year": 2024,
    "referenceCount": 392,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This survey explores recent advancements in training systems for LLMs, including innovations in training infrastructure with AI accelerators, networking, storage, and scheduling, as well as optimizations for computation, communication, and memory in distributed LLM training."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -1.7940276861190796,
            -0.1500491499900818,
            -2.9125053882598877,
            6.880290985107422,
            -0.9431915283203125,
            0.7891225814819336,
            3.1164374351501465,
            2.3787789344787598,
            -2.9708619117736816,
            2.2295420169830322,
            -0.9027433395385742,
            1.4398174285888672,
            2.3214573860168457,
            -2.3673150539398193,
            -4.918522834777832,
            -0.5777814984321594,
            -1.4440107345581055,
            0.26289209723472595,
            5.896149635314941,
            2.9832234382629395,
            -2.4965736865997314,
            0.4881385862827301,
            0.5984485745429993,
            3.048123359680176,
            -2.1784005165100098,
            2.454500675201416,
            0.6253457069396973,
            -1.5322097539901733,
            -0.9022222757339478,
            0.6616100072860718,
            -0.013364478945732117,
            -2.9997506141662598,
            4.751193046569824,
            -7.012460708618164,
            6.5096235275268555,
            -2.8785789012908936,
            -2.2951242923736572,
            6.684423923492432,
            -4.882143497467041,
            0.23640456795692444,
            2.001372814178467,
            -2.1067121028900146,
            -1.4465781450271606,
            -0.9185035824775696,
            3.348745822906494,
            2.730281352996826,
            2.0229456424713135,
            -0.35332992672920227,
            4.669125080108643,
            2.21048641204834,
            1.7821717262268066,
            0.6245119571685791,
            0.7227880358695984,
            -1.8396129608154297,
            -1.737489938735962,
            1.500356912612915,
            -0.33963704109191895,
            1.402347207069397,
            3.9181900024414062,
            -2.406334400177002,
            4.7317657470703125,
            4.880764961242676,
            -1.595002293586731,
            0.7592172026634216,
            2.722601890563965,
            -0.5570164322853088,
            -0.9444159269332886,
            -1.0455026626586914,
            2.985995292663574,
            0.4759763777256012,
            -3.129887580871582,
            -2.8720245361328125,
            2.856511116027832,
            0.3362138271331787,
            -3.452699899673462,
            1.8719985485076904,
            -2.199568271636963,
            -4.917393684387207,
            -0.8822953701019287,
            -6.633647441864014,
            2.576524257659912,
            3.7412657737731934,
            -1.500281572341919,
            0.7127214074134827,
            6.142107009887695,
            -4.512408256530762,
            -0.2995038330554962,
            0.5798408389091492,
            1.8032357692718506,
            -1.068791389465332,
            0.6078863143920898,
            0.26757949590682983,
            -1.1447389125823975,
            0.44509491324424744,
            -0.73548424243927,
            -0.45268169045448303,
            0.4028415083885193,
            -1.4419740438461304,
            1.8971952199935913,
            1.2691372632980347,
            1.5916783809661865,
            0.9445613026618958,
            0.5382984280586243,
            -0.21933072805404663,
            3.428506374359131,
            -3.532062530517578,
            -1.6305129528045654,
            2.2184510231018066,
            -1.4781392812728882,
            -0.06893056631088257,
            -0.056792452931404114,
            4.231706142425537,
            -0.8719208240509033,
            -2.112640857696533,
            -2.7231812477111816,
            -3.810807228088379,
            -0.612030029296875,
            -1.3334803581237793,
            -2.3830747604370117,
            2.7739806175231934,
            -1.1023168563842773,
            -0.9956635236740112,
            -1.0074371099472046,
            2.041497230529785,
            2.418870449066162,
            1.0167206525802612,
            0.6091442108154297,
            -0.32546329498291016,
            -0.4457818269729614,
            -3.2755932807922363,
            0.547247052192688,
            -2.4862213134765625,
            2.449134349822998,
            -4.045190811157227,
            -2.7591686248779297,
            -0.2724933922290802,
            -4.560286521911621,
            0.6537968516349792,
            -1.2270522117614746,
            2.851656436920166,
            0.3559861183166504,
            6.579702854156494,
            0.8506828546524048,
            -0.38726454973220825,
            -1.1956621408462524,
            1.610241413116455,
            5.213426113128662,
            1.1470162868499756,
            -2.0585923194885254,
            2.755760431289673,
            3.702836036682129,
            -0.735623300075531,
            0.10206559300422668,
            1.1258562803268433,
            0.7923036217689514,
            2.0483109951019287,
            -1.1404751539230347,
            1.5700830221176147,
            -1.8262879848480225,
            2.108569622039795,
            1.5964168310165405,
            0.6738619208335876,
            -9.82265853881836,
            -2.133732318878174,
            4.233429908752441,
            -2.792262315750122,
            -4.41201639175415,
            3.121725559234619,
            0.33707278966903687,
            3.718318462371826,
            -1.897273302078247,
            3.8458821773529053,
            3.248387336730957,
            2.523721218109131,
            1.9059512615203857,
            0.6190031170845032,
            1.4995837211608887,
            -1.8712166547775269,
            -3.568018913269043,
            -0.20950794219970703,
            -3.8156676292419434,
            2.941006660461426,
            -2.940957546234131,
            1.8656108379364014,
            -3.1787261962890625,
            -3.149768829345703,
            -2.2970190048217773,
            -1.3156557083129883,
            -0.738933801651001,
            3.001760244369507,
            -0.8105133771896362,
            -0.07408091425895691,
            1.3047916889190674,
            2.7031025886535645,
            2.296266555786133,
            -2.4994025230407715,
            3.827969551086426,
            4.524041175842285,
            -2.4812917709350586,
            2.193586826324463,
            3.4592292308807373,
            -0.6069480180740356,
            0.9210797548294067,
            -2.677021026611328,
            3.492736339569092,
            -0.6064815521240234,
            -2.1393094062805176,
            1.1595648527145386,
            1.1049731969833374,
            0.7535420656204224,
            1.980596661567688,
            -0.5940858125686646,
            -2.5662593841552734,
            2.195253849029541,
            -1.0339200496673584,
            -2.702660322189331,
            -5.887941360473633,
            5.681819915771484,
            6.3274359703063965,
            2.003854274749756,
            1.1177325248718262,
            -1.7126057147979736,
            -2.4333035945892334,
            -3.2991013526916504,
            1.8664864301681519,
            -1.3472524881362915,
            1.974118947982788,
            0.7259963750839233,
            1.5684635639190674,
            0.5342878103256226,
            -3.0299670696258545,
            -8.38121509552002,
            1.2769068479537964,
            -1.6676852703094482,
            -2.9161694049835205,
            -2.5407519340515137,
            -0.388691246509552,
            -2.1293253898620605,
            -3.1475181579589844,
            0.7005994319915771,
            0.2598673403263092,
            4.570152282714844,
            1.046341061592102,
            3.3434560298919678,
            2.036036252975464,
            -1.309584617614746,
            0.3501253128051758,
            0.4937126934528351,
            0.2532818913459778,
            -3.21150803565979,
            -1.8983086347579956,
            -2.0721006393432617,
            4.529455184936523,
            0.7627763152122498,
            1.6177709102630615,
            3.4837899208068848,
            2.650425434112549,
            -3.956228494644165,
            1.5916938781738281,
            -0.5755734443664551,
            -3.603510856628418,
            6.442665100097656,
            4.5069804191589355,
            4.380831718444824,
            -1.4719539880752563,
            -1.227041482925415,
            -3.0785374641418457,
            1.2504997253417969,
            0.9288645386695862,
            1.3956286907196045,
            4.679056167602539,
            1.2026629447937012,
            -1.846177339553833,
            -5.460340976715088,
            -1.9813889265060425,
            -8.164913177490234,
            -2.645658016204834,
            -3.0095419883728027,
            -0.6518959403038025,
            4.143012046813965,
            4.270894527435303,
            -5.040886402130127,
            -0.9876350164413452,
            0.48874619603157043,
            0.1934434324502945,
            -2.063570737838745,
            -0.29514268040657043,
            -0.4917027950286865,
            0.648658037185669,
            0.17541155219078064,
            -1.3199554681777954,
            0.8038784861564636,
            -2.165222644805908,
            -2.612208843231201,
            -1.7024564743041992,
            -0.36870628595352173,
            2.0191609859466553,
            0.277880996465683,
            -2.1756720542907715,
            -1.2893314361572266,
            -3.215583324432373,
            2.376002073287964,
            3.235492706298828,
            -0.49541646242141724,
            0.017647922039031982,
            2.3651371002197266,
            -1.933681607246399,
            -4.941989898681641,
            -0.19695442914962769,
            -2.9302120208740234,
            1.4273583889007568,
            -0.01791536808013916,
            4.493222236633301,
            -3.858719825744629,
            -1.0023689270019531,
            1.5044934749603271,
            2.3581061363220215,
            -0.07490599155426025,
            -2.320793628692627,
            4.012767791748047,
            -0.6959772706031799,
            0.1862863302230835,
            -1.856552243232727,
            -2.2724082469940186,
            -3.0033183097839355,
            1.9417251348495483,
            0.9097999334335327,
            0.43428874015808105,
            -1.4123070240020752,
            6.231775283813477,
            1.6463954448699951,
            4.959342956542969,
            1.3390802145004272,
            2.384984016418457,
            -2.339846134185791,
            -2.8204946517944336,
            -2.688969135284424,
            -2.901824951171875,
            0.35620710253715515,
            -5.334876537322998,
            1.9694679975509644,
            5.702897071838379,
            1.5290699005126953,
            2.0943613052368164,
            -1.187342882156372,
            0.15086281299591064,
            1.842050552368164,
            -0.943871259689331,
            1.4080390930175781,
            0.04678663611412048,
            -0.03690981864929199,
            -0.7127832174301147,
            3.5968945026397705,
            0.9937345385551453,
            0.60682213306427,
            4.07659912109375,
            0.4445866346359253,
            3.0227386951446533,
            0.3285280466079712,
            1.0232044458389282,
            -2.500382423400879,
            3.239744186401367,
            4.380972862243652,
            0.5031321048736572,
            2.2949280738830566,
            -1.4797078371047974,
            12.709630966186523,
            -1.7168700695037842,
            1.1852023601531982,
            -3.549771308898926,
            -2.2262754440307617,
            -4.489089488983154,
            -4.317564010620117,
            0.6008987426757812,
            -0.18153735995292664,
            -0.7246093153953552,
            0.9434230923652649,
            -4.819920063018799,
            -1.3884016275405884,
            0.09003869444131851,
            -0.7978435754776001,
            4.363510608673096,
            -3.3772854804992676,
            1.8028037548065186,
            -3.0604875087738037,
            1.7228158712387085,
            -1.3022452592849731,
            -0.9148741960525513,
            -0.4877937436103821,
            1.3144506216049194,
            -1.7729976177215576,
            3.2956807613372803,
            -0.9402541518211365,
            3.412726402282715,
            -0.7878594398498535,
            -4.056919574737549,
            0.3232532739639282,
            -0.555831253528595,
            0.4905950427055359,
            1.400710105895996,
            -2.4892020225524902,
            -3.1227645874023438,
            5.77700138092041,
            6.221738815307617,
            -4.622437000274658,
            3.9047904014587402,
            1.814406394958496,
            1.9364125728607178,
            -1.7905892133712769,
            -1.5125150680541992,
            -4.722194671630859,
            -1.562863826751709,
            -4.088489532470703,
            -5.224575996398926,
            -1.0781599283218384,
            -5.935509204864502,
            0.5697667598724365,
            0.17245274782180786,
            -0.24479931592941284,
            -1.7395522594451904,
            -0.3809399902820587,
            0.9110461473464966,
            4.304112911224365,
            3.5141639709472656,
            -2.6446805000305176,
            0.27052611112594604,
            1.1180498600006104,
            3.826613426208496,
            0.2681756019592285,
            4.303765296936035,
            0.27239274978637695,
            -0.006210923194885254,
            -2.9017996788024902,
            1.1648340225219727,
            0.33403727412223816,
            2.156895160675049,
            -0.7618709802627563,
            0.6592668294906616,
            5.331246376037598,
            -1.0305607318878174,
            -0.47810840606689453,
            5.304155349731445,
            -3.62795352935791,
            3.714365005493164,
            0.43216586112976074,
            2.336294174194336,
            -2.2655398845672607,
            -1.0238711833953857,
            -3.536201000213623,
            0.8418086767196655,
            1.0904688835144043,
            -2.3584561347961426,
            2.3348541259765625,
            -4.888941764831543,
            1.5124447345733643,
            1.0081284046173096,
            -1.8306840658187866,
            -1.5737621784210205,
            1.3805766105651855,
            -0.5080037117004395,
            0.5185359716415405,
            5.758203029632568,
            -0.15760520100593567,
            2.253758430480957,
            1.2912625074386597,
            -0.7783684730529785,
            2.5385985374450684,
            -5.807665824890137,
            2.5377602577209473,
            2.2800350189208984,
            0.764604926109314,
            -3.124284267425537,
            -0.9139890074729919,
            1.9445366859436035,
            -4.299144744873047,
            -1.0450518131256104,
            4.3888983726501465,
            -0.39185836911201477,
            1.5208746194839478,
            -4.813265323638916,
            -0.0858801007270813,
            2.9266557693481445,
            -0.18965160846710205,
            -4.679474830627441,
            -1.3922648429870605,
            4.758024215698242,
            2.397974729537964,
            1.9248030185699463,
            -0.49387475848197937,
            -0.25116631388664246,
            0.640702486038208,
            -2.7027392387390137,
            2.486696243286133,
            -4.802797317504883,
            1.1004525423049927,
            0.4016839563846588,
            -3.9738497734069824,
            2.2264811992645264,
            2.270127773284912,
            2.7803573608398438,
            -3.3024349212646484,
            -5.293678283691406,
            0.017231762409210205,
            -1.6658360958099365,
            -3.87367844581604,
            7.894952774047852,
            2.869938850402832,
            -1.1008765697479248,
            -3.526441812515259,
            3.6490073204040527,
            0.8792790770530701,
            -1.1511200666427612,
            -5.661617279052734,
            2.1903252601623535,
            0.6531634330749512,
            3.3080618381500244,
            0.42209577560424805,
            1.489985466003418,
            -1.7846759557724,
            -0.31930291652679443,
            -2.0456087589263916,
            0.5422191619873047,
            -0.4888530969619751,
            -0.5392853617668152,
            3.025205135345459,
            2.7714293003082275,
            -2.2273852825164795,
            3.109835624694824,
            2.989262104034424,
            2.3575615882873535,
            3.6609740257263184,
            1.6469266414642334,
            -1.0423455238342285,
            -5.591633319854736,
            1.5678105354309082,
            -1.8944333791732788,
            2.976241111755371,
            2.104092836380005,
            -5.503016471862793,
            -2.3892805576324463,
            -3.485322952270508,
            1.4949172735214233,
            -1.251810073852539,
            3.45432186126709,
            -0.9149830341339111,
            1.3465096950531006,
            -0.7634190320968628,
            0.31400424242019653,
            -2.3273825645446777,
            1.5839877128601074,
            4.045803070068359,
            1.5416409969329834,
            -0.0035318732261657715,
            -0.7520228624343872,
            -1.3062266111373901,
            -4.602595806121826,
            -1.892273187637329,
            0.32746219635009766,
            -0.9292665719985962,
            4.122128486633301,
            2.142920970916748,
            -0.3541516661643982,
            -0.3536619544029236,
            6.084222793579102,
            -3.9832756519317627,
            -1.786163091659546,
            -0.13038954138755798,
            1.9921244382858276,
            0.3054126501083374,
            -0.2177751064300537,
            1.8693265914916992,
            -1.1808691024780273,
            3.5391321182250977,
            -0.01793956756591797,
            -0.9706509709358215,
            1.920487403869629,
            2.4776055812835693,
            2.4430646896362305,
            -1.026371955871582,
            0.35899055004119873,
            -2.576143741607666,
            -4.956536293029785,
            -0.5277880430221558,
            -2.6638007164001465,
            -1.2908599376678467,
            -1.3037835359573364,
            -1.01274573802948,
            2.2575371265411377,
            -4.148432731628418,
            1.511866807937622,
            1.1960029602050781,
            -1.2632255554199219,
            -1.2361936569213867,
            -3.105607509613037,
            1.0883195400238037,
            -3.118201732635498,
            -0.2746995687484741,
            -1.3232698440551758,
            0.2362651824951172,
            2.0326077938079834,
            -0.7418920993804932,
            5.454761981964111,
            0.8485370874404907,
            0.5530333518981934,
            -2.849489212036133,
            -1.9989873170852661,
            0.3761095404624939,
            0.3036368787288666,
            2.478148937225342,
            0.6172353029251099,
            -3.8770761489868164,
            0.5621528625488281,
            14.50446891784668,
            -2.821798324584961,
            -2.7024383544921875,
            -2.707254409790039,
            -1.3224644660949707,
            -4.064608097076416,
            -1.2609968185424805,
            0.8712078332901001,
            1.6550321578979492,
            2.289654016494751,
            0.36096081137657166,
            -0.20085543394088745,
            2.5728917121887207,
            2.1637394428253174,
            -0.8220616579055786,
            4.821223258972168,
            0.13665014505386353,
            1.5673723220825195,
            -2.349332809448242,
            2.19234299659729,
            1.4063650369644165,
            -0.20849651098251343,
            0.7844011187553406,
            -4.559340953826904,
            -1.8200634717941284,
            3.1173248291015625,
            4.885824680328369,
            1.6269569396972656,
            0.5789070129394531,
            2.587491035461426,
            3.453702926635742,
            4.216681480407715,
            1.4773043394088745,
            1.5528626441955566,
            -2.9004855155944824,
            2.458564519882202,
            0.7792165279388428,
            1.2374238967895508,
            1.8322577476501465,
            4.0012922286987305,
            -2.179873466491699,
            -2.8848657608032227,
            -1.202417016029358,
            -1.4330544471740723,
            -3.0799694061279297,
            2.105238437652588,
            0.5726149082183838,
            -0.9378266930580139,
            -3.371735095977783,
            2.032881259918213,
            -3.456408739089966,
            -4.669815540313721,
            -0.8030370473861694,
            -1.2074980735778809,
            -0.4498087763786316,
            2.6635046005249023,
            0.444050669670105,
            0.5928144454956055,
            2.437570333480835,
            -5.324029922485352,
            2.526275157928467,
            -2.3807482719421387,
            -0.7622663378715515,
            -3.5484695434570312,
            -3.0156874656677246,
            5.471138954162598,
            -1.8807988166809082,
            0.5054847002029419,
            -0.9741781949996948,
            2.0986263751983643,
            0.684670090675354,
            1.43294358253479,
            2.0884580612182617,
            -2.055461883544922,
            0.10427647829055786,
            -2.267859935760498,
            5.017914772033691,
            -3.4352951049804688,
            -2.0433669090270996,
            5.909265518188477,
            -0.4725022315979004,
            4.785422325134277,
            -0.5846995711326599,
            -1.4625341892242432,
            2.0968644618988037,
            -2.852374315261841,
            5.005687236785889,
            1.8937073945999146,
            -1.9488900899887085,
            2.9358465671539307,
            -0.5798001885414124,
            -0.8954434990882874,
            3.1550655364990234,
            1.3280584812164307,
            1.9934725761413574,
            -2.4472477436065674,
            -5.064840316772461,
            -3.8703699111938477,
            -2.0439319610595703,
            -2.363666296005249,
            5.405517101287842,
            4.106963157653809,
            0.7246530055999756,
            -2.897413730621338,
            0.3397015333175659,
            -0.5250564813613892,
            0.07814003527164459,
            -7.489793300628662,
            -1.9190075397491455,
            -0.7805033922195435,
            4.329790115356445,
            -6.041259288787842,
            -1.737675666809082,
            0.994599461555481,
            -1.7141870260238647,
            -1.0078644752502441,
            2.2736685276031494,
            0.6751291751861572,
            0.6795672178268433,
            -1.0731003284454346,
            1.6587612628936768,
            -0.6020249128341675,
            1.136757731437683,
            -3.760141372680664,
            0.477517306804657,
            -2.364949941635132,
            -0.9871933460235596,
            0.5782737731933594,
            -2.36277437210083,
            1.3813279867172241,
            -0.20327657461166382,
            -2.611845016479492,
            -3.9458882808685303,
            1.4332225322723389,
            -1.7134346961975098,
            -2.093628406524658,
            0.5944816470146179,
            -3.485971450805664,
            -0.3936323821544647,
            -0.5838955640792847,
            3.24410080909729,
            -2.5849008560180664,
            2.3233892917633057,
            7.835599899291992,
            0.36926978826522827,
            1.1256883144378662,
            -0.2791154980659485,
            -2.626981258392334,
            2.599449634552002,
            1.1383723020553589,
            1.8370178937911987,
            -3.6161859035491943,
            0.697441577911377,
            0.18514832854270935,
            0.8600148558616638,
            -3.345583438873291
        ]
    },
    "authors": [
        {
            "authorId": "2268410645",
            "name": "Jiangfei Duan"
        },
        {
            "authorId": "2257086624",
            "name": "Shuo Zhang"
        },
        {
            "authorId": "2200108715",
            "name": "Zerui Wang"
        },
        {
            "authorId": "2196217535",
            "name": "Lijuan Jiang"
        },
        {
            "authorId": "2313633991",
            "name": "Wenwen Qu"
        },
        {
            "authorId": "2150570711",
            "name": "Qi Hu"
        },
        {
            "authorId": "2263696698",
            "name": "Guoteng Wang"
        },
        {
            "authorId": "2257054500",
            "name": "Qizhen Weng"
        },
        {
            "authorId": "146948229",
            "name": "Hang Yan"
        },
        {
            "authorId": "2283503847",
            "name": "Xingcheng Zhang"
        },
        {
            "authorId": "2256661980",
            "name": "Xipeng Qiu"
        },
        {
            "authorId": "2258618409",
            "name": "Dahua Lin"
        },
        {
            "authorId": "2114783855",
            "name": "Yonggang Wen"
        },
        {
            "authorId": "2279869979",
            "name": "Xin Jin"
        },
        {
            "authorId": "2146333441",
            "name": "Tianwei Zhang"
        },
        {
            "authorId": "2300809135",
            "name": "Peng Sun"
        }
    ],
    "references": [
        {
            "paperId": "15d37231657883a1e7022e1e84b5188a94800eca",
            "title": "Alibaba HPN: A Data Center Network for Large Language Model Training"
        },
        {
            "paperId": "fca1ad2e38d3c6784b3eab0629a8e62df8698e3a",
            "title": "A Multidimensional Communication Scheduling Method for Hybrid Parallel DNN Training"
        },
        {
            "paperId": "5d1ca53a0b41f4f5c960cd9997556d7180d6b88d",
            "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision"
        },
        {
            "paperId": "b149bcbe4bf29cfd1c89543a5d04f020619c88e8",
            "title": "The infrastructure powering IBM's Gen AI model development"
        },
        {
            "paperId": "353539616338018f4a381b685e719fbf64aba37c",
            "title": "WallFacer: Guiding Transformer Model Training Out of the Long-Context Dark Forest with N-body Problem"
        },
        {
            "paperId": "196aac9b36f1a109dfe3df2c0c1f608978efaf2f",
            "title": "Challenging the Need for Packet Spraying in Large-Scale Distributed Training"
        },
        {
            "paperId": "dcf97a6c0f7947aaee1912f3f1ac7c8806703f95",
            "title": "Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training"
        },
        {
            "paperId": "a79c67e5ecddc025ccf705c7b7a40e8844f83a6a",
            "title": "LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism"
        },
        {
            "paperId": "9b1ef16bb9c7a8e8ce952e724cdaa0478f4b5681",
            "title": "GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism"
        },
        {
            "paperId": "758a107293bd962b8363dde2f8dd149f0c2ddfa9",
            "title": "A Heuristic for Periodic Memory Allocation with Little Fragmentation to Train Neural Networks"
        },
        {
            "paperId": "86937462753b62499eca8cd06dae870b51a60032",
            "title": "ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation"
        },
        {
            "paperId": "08f8e417faf7a02f2d42c853bfedc21938e2aad2",
            "title": "FastPersist: Accelerating Model Checkpointing in Deep Learning"
        },
        {
            "paperId": "8082e08349f789fb5fe1e45caede6da118c00b37",
            "title": "Optimizing Large Model Training through Overlapped Activation Recomputation"
        },
        {
            "paperId": "1c1851f37d2e80af9507a2514fd4c9b91c76cf6b",
            "title": "Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach"
        },
        {
            "paperId": "e6dbc9545be9b0cda0925862d355a177511d3b11",
            "title": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training"
        },
        {
            "paperId": "cba8438b8cf288ff7d36fe2439037a3c887a0a3a",
            "title": "DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models"
        },
        {
            "paperId": "773597e8acb8c9ee2a4603e0bd40bb74b7fba871",
            "title": "MPMoE: Memory Efficient MoE for Pre-Trained Models With Adaptive Pipeline Parallelism"
        },
        {
            "paperId": "dedcfd974eb7a8ab3d02b42561e4448883f0215a",
            "title": "System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"
        },
        {
            "paperId": "dc2108f1fc59c51179a5aaa11a826437a8214f5c",
            "title": "Pipeline Parallelism with Controllable Memory"
        },
        {
            "paperId": "3c6f8b14a1ed05c7228a37c32968c23a133bf28f",
            "title": "SlipStream: Adapting Pipelines for Distributed Training of Large DNNs Amid Failures"
        },
        {
            "paperId": "a941c7948377201abceecc9bcecb7fad09becb9f",
            "title": "MOSS: An Open Conversational Large Language Model"
        },
        {
            "paperId": "9deda73da446cbb21550abaa62b41f5a3acd92fc",
            "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
        },
        {
            "paperId": "28f59093730d88719b96041f9544c73671f798bd",
            "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI"
        },
        {
            "paperId": "53a803388e83ae89261624099d7be4287ace67cb",
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
        },
        {
            "paperId": "a509670659e8b054e2b7d1b6f8a0bc722398fa62",
            "title": "PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation"
        },
        {
            "paperId": "3bffaa1da8778405673d174b8bbf28f83d5916d1",
            "title": "Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning"
        },
        {
            "paperId": "2d6bd2c05c05691c8f30eef54e01eab750295f19",
            "title": "AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning"
        },
        {
            "paperId": "83858f08aef77ced91428207b8891a82e7087cd2",
            "title": "ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling"
        },
        {
            "paperId": "e28e266c675acbf131c18af5f6f438a4a1d576a2",
            "title": "Just-In-Time Checkpointing: Low Cost Error Recovery from Deep Learning Training Failures"
        },
        {
            "paperId": "bbe2e5cbcb7abaed960708f10fe1c160663b7293",
            "title": "Aceso: Efficient Parallel DNN Training through Iterative Bottleneck Alleviation"
        },
        {
            "paperId": "5fb9124b0e817986e8687c322214a828a71b79ce",
            "title": "ML Training with Cloud GPU Shortages: Is Cross-Region the Answer?"
        },
        {
            "paperId": "3dcaf781cde9d8fcf9e5870db6baeeeac1a5c303",
            "title": "Large-scale photonic chiplet Taichi empowers 160-TOPS/W artificial general intelligence"
        },
        {
            "paperId": "75b2ae5ee35611ecfbd3dc2c3d0799cfb4fd98e4",
            "title": "InternLM2 Technical Report"
        },
        {
            "paperId": "8046d516d395df434ff2b0c65c15f55e8e85dfe5",
            "title": "A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters"
        },
        {
            "paperId": "916b4926cda574dc3f9486bb9994b6f2788dd800",
            "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey"
        },
        {
            "paperId": "01f6de9e8670b613f4eccf0a699acb45673c19c5",
            "title": "Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances"
        },
        {
            "paperId": "388df4f4d6577c92a43a9a578ebed844a1b4cc17",
            "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization"
        },
        {
            "paperId": "49ac8b961a896fda01cdf450d4ad2c5b74dec5e8",
            "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers"
        },
        {
            "paperId": "cf93c04b73d50ba097151ee3e2a9f47fda4a8525",
            "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences"
        },
        {
            "paperId": "d8599a04f738a42f5d7a2c9bc598c2c7b74669c5",
            "title": "Scattered Mixture-of-Experts Implementation"
        },
        {
            "paperId": "fb0207353fe923efd26a4fedafde56cd8eda1173",
            "title": "Characterization of Large Language Model Development in the Datacenter"
        },
        {
            "paperId": "67aa2bbc59fc6ec26083f263775a72f5cee4a66d",
            "title": "Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU"
        },
        {
            "paperId": "1113d9329463048a5b969583a8dd228027264cae",
            "title": "SWattention: designing fast and memory-efficient attention for a new Sunway Supercomputer"
        },
        {
            "paperId": "0390f8b6c0b2d377530b385bf5b1c0099eb7d70b",
            "title": "Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System"
        },
        {
            "paperId": "7946c2c8dadbb9e7f5f5c6f341dcdfcde77bcf67",
            "title": "High-Speed Data Communication With Advanced Networks in Large Language Model Training"
        },
        {
            "paperId": "d68e93cad6a038f233cba5c7f3ca5448a1744d55",
            "title": "WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset"
        },
        {
            "paperId": "63167c30b06aa6c3d76e09065ced0412090d6c3b",
            "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
        },
        {
            "paperId": "6dc5c6190dfbe55c8b45b7b23800614c21e5b51c",
            "title": "MegaScale: Scaling Large Language Model Training to More Than 10, 000 GPUs"
        },
        {
            "paperId": "f7ba2bb86f0804e627fcf9a89039360ecca80445",
            "title": "MLTCP: Congestion Control for DNN Training"
        },
        {
            "paperId": "061a3e91fa84b382847df502220e6930f7cfef7d",
            "title": "SuperBench: Improving Cloud AI Infrastructure Reliability with Proactive Validation"
        },
        {
            "paperId": "8ffd570001c45eba70075e25e6637fcdce9f8419",
            "title": "T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives"
        },
        {
            "paperId": "883b5a6cbbf3c499c8402204a657abf1e836d310",
            "title": "Deep Learning Workload Scheduling in GPU Datacenters: A Survey"
        },
        {
            "paperId": "b12775806f607aa2415680630037f4ede509bc95",
            "title": "PartIR: Composing SPMD Partitioning Strategies for Machine Learning"
        },
        {
            "paperId": "38bb1b247ee019fb3d2e0cb53fa4e067970bbf91",
            "title": "GMLake: Efficient and Transparent GPU Memory Defragmentation for Large-scale DNN Training with Virtual Memory Stitching"
        },
        {
            "paperId": "cfabacfc676ea8804b5acbab169f2df5e5866d4d",
            "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models"
        },
        {
            "paperId": "2712a7c0a8275bd0db91a61790a9e7a7aa7e74b8",
            "title": "HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis"
        },
        {
            "paperId": "411114f989a3d1083d90afd265103132fee94ebe",
            "title": "Mixtral of Experts"
        },
        {
            "paperId": "efc5e94635a850ede9c1f8dbce65d5dc536f3bfb",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference"
        },
        {
            "paperId": "6cdf32647b158b6308aac7a650f756a27227f0f7",
            "title": "Unicron: Economizing Self-Healing LLM Training at Scale"
        },
        {
            "paperId": "13261129251c9e8891cff02c3aee15c4df6a5630",
            "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"
        },
        {
            "paperId": "d0ad9d1c928e8a119f300e8dd88165060ad8c35e",
            "title": "Optimizing Distributed Training on Frontier for Large Language Models"
        },
        {
            "paperId": "18a74a76d2ed55df12544c55c43458457a081fd5",
            "title": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training"
        },
        {
            "paperId": "32cfcf171e0eccd1ccf50c5e1dc427ee539cef24",
            "title": "A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library"
        },
        {
            "paperId": "8f6a87e17e0987ff639638d4538e74c28f9f2b8e",
            "title": "Perseus: Reducing Energy Bloat in Large Model Training"
        },
        {
            "paperId": "5851121df5ce46be5faea265c868ec0beabfce96",
            "title": "Efficient Large Language Models: A Survey"
        },
        {
            "paperId": "7e88160ab2b3c9224ce0b9a66bec1740728984bf",
            "title": "Zero Bubble Pipeline Parallelism"
        },
        {
            "paperId": "8820369cbf53c520924eb84f27f21fb646eb9937",
            "title": "Tessel: Boosting Distributed Execution of Large DNN Models via Flexible Schedule Search"
        },
        {
            "paperId": "4ea5ca620122e6a9a2b000444d36491cebf49c7c",
            "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"
        },
        {
            "paperId": "8b418947454affb80ba47454b65af7e8fea705ae",
            "title": "DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines"
        },
        {
            "paperId": "ade22704be8a0fc3730d320cc7934b2ccbcd97e4",
            "title": "Striped Attention: Faster Ring Attention for Causal Transformers"
        },
        {
            "paperId": "aef4ee743e6e61aec316ceaa301515f048020921",
            "title": "Just-in-time Quantization with Processing-In-Memory for Efficient ML Training"
        },
        {
            "paperId": "c07ce0954743d93e0e954abbc16f7f132b1ad2b8",
            "title": "A Cost-Efficient Failure-Tolerant Scheme for Distributed DNN Training"
        },
        {
            "paperId": "58fc40ba1aaf3694f9c44425524bdc9a2c3aa35b",
            "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication"
        },
        {
            "paperId": "57c655bec92a3df3ee2a9be0a209a7350fa8a25b",
            "title": "Coop: Memory is not a Commodity"
        },
        {
            "paperId": "1094b385d05c36ccf83a110c8b22318e4f6e72f6",
            "title": "AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training"
        },
        {
            "paperId": "985e4584d8dce5ce1c75860828c92d65cbd58590",
            "title": "ChipNeMo: Domain-Adapted LLMs for Chip Design"
        },
        {
            "paperId": "25067493e2a969217e6c8b8115462184390bb2bd",
            "title": "Prophet: Fine-grained Load Balancing for Parallel Training of Large-scale MoE Models"
        },
        {
            "paperId": "eaea6533a6945450a167b4b15688dc0f240fc0ee",
            "title": "ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout"
        },
        {
            "paperId": "334e8b7597b69da621c9114e3f695c6315c9267e",
            "title": "FP8-LM: Training FP8 Large Language Models"
        },
        {
            "paperId": "a2eba36b34833621fa70bcc63ba239846bfd529a",
            "title": "GEMINI: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints"
        },
        {
            "paperId": "71de9b9cb83dcf53a86c7a3cb3ff7b36b3917978",
            "title": "Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling"
        },
        {
            "paperId": "c31b8b8b1d1ed299b465cf70ba9c3ad9e023e407",
            "title": "Fault-Tolerant Hybrid-Parallel Training at Scale with Reliable and Efficient In-memory Checkpointing"
        },
        {
            "paperId": "358b96ce1653ec489e7bf565c0f1afad81e502d0",
            "title": "BitNet: Scaling 1-bit Transformers for Large Language Models"
        },
        {
            "paperId": "9d78505dd333b4bc77b75b65313ab9f96bcfe198",
            "title": "Microscaling Data Formats for Deep Learning"
        },
        {
            "paperId": "ffa343b5dce9e82f00fbf380f8b343963109f8ec",
            "title": "TRANSOM: An Efficient Fault-Tolerant System for Training LLMs"
        },
        {
            "paperId": "6a433d3cd43c22cbe23b700b9a1e0ee5cf631a8e",
            "title": "Rethinking Memory and Communication Cost for Efficient Large Language Model Training"
        },
        {
            "paperId": "a941874be310395c5778d4f1710ad0635795ad6b",
            "title": "A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators"
        },
        {
            "paperId": "8511ea96d61593de57cbc2e996910e5cb3dbfe84",
            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training"
        },
        {
            "paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"
        },
        {
            "paperId": "e4011238f0aee0f1baebea9b1bea0bd85371826f",
            "title": "Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors"
        },
        {
            "paperId": "e97addc2c9d137ca53a73d41ad59083c1a4cf214",
            "title": "Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates"
        },
        {
            "paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
            "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
        },
        {
            "paperId": "660380b17d3a37d8132f2e6dcb5cb47092e5b7d1",
            "title": "FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs"
        },
        {
            "paperId": "a32476f93be0e8707cc1b99c2f506e60d61715a4",
            "title": "Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models"
        },
        {
            "paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
            "title": "Instruction Tuning for Large Language Models: A Survey"
        },
        {
            "paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310",
            "title": "A Survey on Model Compression for Large Language Models"
        },
        {
            "paperId": "02d4096c030d052e1866d52fbc3b83480e1ed9f5",
            "title": "Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques"
        },
        {
            "paperId": "dd278797cae2d4ca9725d98a4e0f73b637990381",
            "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"
        },
        {
            "paperId": "c095361942179c96cae0c30248fb6ff80b0bc390",
            "title": "CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters"
        },
        {
            "paperId": "e77c553b71704196b2ea010651dc8f4c42c3e735",
            "title": "A Survey on Auto-Parallelism of Large-Scale Deep Learning Training"
        },
        {
            "paperId": "83d60df5c515be6a10b71fb615c224f6ea94ab6e",
            "title": "PipePar: Enabling fast DNN pipeline parallel training in heterogeneous GPU clusters"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0",
            "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
        },
        {
            "paperId": "3e5741ee9cfd23c79d2af2e209ebb1b57da96e2a",
            "title": "Improving Automatic Parallel Training via Balanced Memory Workload Optimization"
        },
        {
            "paperId": "7505337d88e4d36e2e37891c542947a2e3c9e009",
            "title": "Training Transformers with 4-bit Integers"
        },
        {
            "paperId": "1ca5618423c64f0656d13c2bc0d387cc2006f7b2",
            "title": "ZeRO++: Extremely Efficient Collective Communication for Giant Model Training"
        },
        {
            "paperId": "bb28712eb3b32bca5ce123bf85d81aaf0e2e037f",
            "title": "Evaluation of pre-training large language models on leadership-class supercomputers"
        },
        {
            "paperId": "de431ca980507e70ef35a14a48ef635747b8edcc",
            "title": "AMD InstinctTM MI250X Accelerator enabled by Elevated Fanout Bridge Advanced Packaging Architecture"
        },
        {
            "paperId": "e8d6dc483b439c1e5ab839e86794ba301dabed88",
            "title": "Automated Tensor Model Parallelism with Overlapped Communication for Efficient Foundation Model Training"
        },
        {
            "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
        },
        {
            "paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200",
            "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"
        },
        {
            "paperId": "479f22891a50a553ae46ca32e7bc7e195d9293fc",
            "title": "PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining"
        },
        {
            "paperId": "a8e32285ff09afaa3fcea50a06a421d5b9fe578d",
            "title": "SiloD: A Co-design of Caching and Scheduling for Deep Learning Clusters"
        },
        {
            "paperId": "114cc72d93c73b97ba03ed8c4e4a8d937b344607",
            "title": "An Efficient 2D Method for Training Super-Large Deep Learning Models"
        },
        {
            "paperId": "ed734578bf467a51b72ea8d0da2b81e1a6364346",
            "title": "NVIDIA Hopper H100 GPU: Scaling Performance"
        },
        {
            "paperId": "2d2b3fa757d7a839d6154b709f779366e624b903",
            "title": "Fold3D: Rethinking and Parallelizing Computational and Communicational Tasks in the Training of Large DNN Models"
        },
        {
            "paperId": "e61462184a6dce9259e76c9069c5747a420b3c0d",
            "title": "SDPipe: A Semi-Decentralized Framework for Heterogeneity-aware Pipeline-parallel Training"
        },
        {
            "paperId": "4c1faaf1573902bdfee39fd1089d38882e7bd5eb",
            "title": "An Empirical Study on Quality Issues of Deep Learning Platform"
        },
        {
            "paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da",
            "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
        },
        {
            "paperId": "dbbc5003af690799fa4fe6330fb795311cde106f",
            "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"
        },
        {
            "paperId": "ece77610adfb0fb162dd22ef694f2777393c319a",
            "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster"
        },
        {
            "paperId": "7c25adf2ddb35df05a61c697da97efb8583d77df",
            "title": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"
        },
        {
            "paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
            "title": "A Survey of Large Language Models"
        },
        {
            "paperId": "443c1bef6a7dc3db941375ae76451c884ceffb8a",
            "title": "A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training"
        },
        {
            "paperId": "d7e00702bbb5a0cccc97033f0405b634ae9e2d3c",
            "title": "Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent"
        },
        {
            "paperId": "3822cb0a089a66f2ad88e7960fcae6ebdc4e4427",
            "title": "DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining"
        },
        {
            "paperId": "a34384389f74b7b2c31c696b0db0bf813e8bb301",
            "title": "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training"
        },
        {
            "paperId": "3599a236f285af48782fc30b1341d13ec7320735",
            "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"
        },
        {
            "paperId": "6c148b4273eeab70ca4499400d5fcf34d813b957",
            "title": "Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning Model Training"
        },
        {
            "paperId": "fc2f87333527ea30f9bdf7391572509c2a6f79c3",
            "title": "THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression"
        },
        {
            "paperId": "bea8f5d6cf70402b21721b7ddf458286ef7af775",
            "title": "Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models"
        },
        {
            "paperId": "2365de6a5d1085f6225fc42a7cb47d28eaf71985",
            "title": "Tensor Movement Orchestration in Multi-GPU Training Systems"
        },
        {
            "paperId": "eef3b5edb30c6b68ccc1d69dea83946d282899e6",
            "title": "Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion"
        },
        {
            "paperId": "43cefce076df7ee54505fd78a8a97129c0f6d36b",
            "title": "MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism"
        },
        {
            "paperId": "5278b81db686b4d36143941bff1c683bea963a63",
            "title": "SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"
        },
        {
            "paperId": "58d6ec0dec4952e93be7cf72c1cebb0216eac9df",
            "title": "Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers"
        },
        {
            "paperId": "d5fe97309afdf0da633e04b5da4212a054661ecf",
            "title": "Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs"
        },
        {
            "paperId": "3995c7a3704db252cfcce18ef56562a89f4cf693",
            "title": "ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning"
        },
        {
            "paperId": "cba1fb71b14d211639dbabb2d4cfd4a2295c985f",
            "title": "AutoDDL: Automatic Distributed Deep Learning With Near-Optimal Bandwidth Cost"
        },
        {
            "paperId": "3692f4df9d11af68f9b9c9a526667db3f99e552c",
            "title": "Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models"
        },
        {
            "paperId": "736973165f98105fec3729b7db414ae4d80fcbeb",
            "title": "Scalable Diffusion Models with Transformers"
        },
        {
            "paperId": "16de2006e2960ba410772c6b6d460b83c0a5cc4b",
            "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning"
        },
        {
            "paperId": "be157d55b4afd5be9c81619d75aa4897f5e201e4",
            "title": "Elixir: Train a Large Language Model on a Small GPU Cluster"
        },
        {
            "paperId": "43014fc85c4860487336579ec98f509fec1803f7",
            "title": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"
        },
        {
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
        },
        {
            "paperId": "e2df6ae1b3485449364ce2a5356ab09600fc3632",
            "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"
        },
        {
            "paperId": "1a6a7fe065e42365515ccf7d0b5d1225b8088464",
            "title": "STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training"
        },
        {
            "paperId": "6d088e8d785a57b50c2b0e465e2460e09ced48d7",
            "title": "Accelerating Distributed MoE Training and Inference with Lina"
        },
        {
            "paperId": "347bb85574a0152d7273245c4e657965fb325213",
            "title": "ALCOP: Automatic Load-Compute Pipelining in Deep Learning Compiler for AI-GPUs"
        },
        {
            "paperId": "1e2fd6d64e7eea23d713c98bcc8664c1cf052d35",
            "title": "AMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness"
        },
        {
            "paperId": "22b58dce1a13382418b8372bbd50ed3b2533f899",
            "title": "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"
        },
        {
            "paperId": "1ad7a323cebbd186a6f3c9a1f0caf0af94ce91bd",
            "title": "HammingMesh: A Network Topology for Large-Scale Deep Learning"
        },
        {
            "paperId": "baa467a4dccf87bc7e2c5a4ea6fd5e401d962d39",
            "title": "AutoPipe: A Fast Pipeline Parallelism Approach with Balanced Partitioning and Micro-batch Slicing"
        },
        {
            "paperId": "0437f76a8e6607116659f16c7e4e656851fdd9d5",
            "title": "HPH: Hybrid Parallelism on Heterogeneous Clusters for Accelerating Large-scale DNNs Training"
        },
        {
            "paperId": "92ffac92cdb5ae7fce881b4d65996458dfe7f241",
            "title": "Multi-resource interleaving for deep learning training"
        },
        {
            "paperId": "c419c631aa4c271b009d53ef26f94675c87cbf04",
            "title": "Nvidia Hopper GPU: Scaling Performance"
        },
        {
            "paperId": "3deca3e77cfddb11feb4783f8acb2ceb860800e5",
            "title": "Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training"
        },
        {
            "paperId": "d5cfc82ac7ad6c5e9c91ef18bba6d2979b632443",
            "title": "Merak: An Efficient Distributed DNN Training Framework With Automated 3D Parallelism for Giant Foundation Models"
        },
        {
            "paperId": "2e700ff36108119f5ed19a53bd2eaa22b42ec3d8",
            "title": "Tutel: Adaptive Mixture-of-Experts at Scale"
        },
        {
            "paperId": "6b117a8dcaa161562b0a69afbb9811e11afb5b3e",
            "title": "Decentralized Training of Foundation Models in Heterogeneous Environments"
        },
        {
            "paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336",
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
        },
        {
            "paperId": "e8327b84e4c8c631bb11443ad13af18c9f5ee634",
            "title": "MoESys: A Distributed and Efficient Mixture-of-Experts Training and Inference System for Internet Services"
        },
        {
            "paperId": "cfbfb9eecacce184706f79bda3d90406d806b2ae",
            "title": "Enabling Fast and Flexible Distributed Deep Learning with Programmable Switches"
        },
        {
            "paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96",
            "title": "Reducing Activation Recomputation in Large Transformer Models"
        },
        {
            "paperId": "c183510bc7b86ee0ed7ecc6fb0698a43bd0e2379",
            "title": "Fluid: Dataset Abstraction and Elastic Acceleration for Cloud-native Deep Learning Training Jobs"
        },
        {
            "paperId": "ba377c911b1d601cbe6a7f10f89e5b6175adcd98",
            "title": "TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting"
        },
        {
            "paperId": "cd733ce920e055415e9a9a7d90d3ec89f8750866",
            "title": "MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud"
        },
        {
            "paperId": "5ce7d930b87a16ae9a7b5f789181a6e021f4df5d",
            "title": "Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs"
        },
        {
            "paperId": "f2c9db0279f9acc32d3315fe8daa4dca28fe60a2",
            "title": "The Cerebras CS-2: Designing an AI Accelerator around the World's Largest 2.6 Trillion Transistor Chip"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "533beb3cc4b99f05ac2f3baae90e1bab07fd93a2",
            "title": "Out-of-order backprop: an effective scheduling technique for deep learning"
        },
        {
            "paperId": "0dab58e476f3f0e6f580a295f7c4756c86f1f198",
            "title": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"
        },
        {
            "paperId": "512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352",
            "title": "Pathways: Asynchronous Distributed Dataflow for ML"
        },
        {
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback"
        },
        {
            "paperId": "f756e993e6eba8d7c3db74838a348ed3a79bc1cc",
            "title": "Nvidia Hopper GPU and Grace CPU Highlights"
        },
        {
            "paperId": "7171c5f56543c51ea584ed9edfe8ccd33375167d",
            "title": "AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures"
        },
        {
            "paperId": "f74b387136d1d57a791c6b7c9d823577a47516aa",
            "title": "Harmony: Overcoming the hurdles of GPU memory capacity to train massive DNN models on commodity servers"
        },
        {
            "paperId": "2d9c43e1133f17c73408774840c93382f3cc8f84",
            "title": "TopoOpt: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs"
        },
        {
            "paperId": "91b3f376f063dd59060ea5b95e8cb828adec36d7",
            "title": "GC3: An Optimizing Compiler for GPU Collective Communication"
        },
        {
            "paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8",
            "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"
        },
        {
            "paperId": "8584a1f52ec8d31f028159c3bc56c68d600c2c1a",
            "title": "The Exascale Era is Upon Us: The Frontier supercomputer may be the first to reach 1,000,000,000,000,000,000 operations per second"
        },
        {
            "paperId": "53c3940f35b8b45d55ed49056282e1961954513d",
            "title": "Self-attention Does Not Need $O(n^2)$ Memory"
        },
        {
            "paperId": "ee042a3e299a32c413532e64603de8d3ddb6aa87",
            "title": "Automap: Towards Ergonomic Automated Parallelism for ML Models"
        },
        {
            "paperId": "cb1cc03b04079b89a1f42d736cba4d2ee3e4f7c3",
            "title": "TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches"
        },
        {
            "paperId": "43332a71939ae7f3bd4756cba2c5ef0763b5cfac",
            "title": "Varuna: scalable, low-cost training of massive deep learning models"
        },
        {
            "paperId": "f12975df157fe4462054ba1484dd4b4e5aaf5a17",
            "title": "Large-Scale Deep Learning Optimizations: A Comprehensive Survey"
        },
        {
            "paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9",
            "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"
        },
        {
            "paperId": "3186b9dd1331b647cf3304d185c248ea7ec9ad1b",
            "title": "OneFlow: Redesign the Distributed Deep Learning Framework from Scratch"
        },
        {
            "paperId": "e930b3dc75f9cc36aa78aa640eee639b708a92bd",
            "title": "Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning"
        },
        {
            "paperId": "ccbe83578d44cb90a79aee05588e4a3323d4403d",
            "title": "Scalable Fully Pipelined Hardware Architecture for In-Network Aggregated AllReduce Communication"
        },
        {
            "paperId": "ac35dffd21c16b02e140a36726b3a21d266cab0f",
            "title": "Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters"
        },
        {
            "paperId": "78a5b29d3bdeaf297aee08cab6a8b4f59763481b",
            "title": "ACCL: Architecting Highly Scalable Distributed Training Systems With Highly Efficient Collective Communication Library"
        },
        {
            "paperId": "10c0a1d3519dcc7b876d21d614f49d82467c9dc3",
            "title": "Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management"
        },
        {
            "paperId": "c0c36424f691c9d934aee7b9c6c6c86429a57b45",
            "title": "SiP-ML: high-bandwidth optical network interconnects for machine learning training"
        },
        {
            "paperId": "10f3ca78e194552427ebe9173b19d1b910469e27",
            "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"
        },
        {
            "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
            "title": "Evaluating Large Language Models Trained on Code"
        },
        {
            "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "title": "LoRA: Low-Rank Adaptation of Large Language Models"
        },
        {
            "paperId": "659b95b28a9f3b6cfa3bd06fa7bd004165db5663",
            "title": "Tesseract: Parallelize the Tensor Parallelism Efficiently"
        },
        {
            "paperId": "d8df456f790381f4ddb388be24a546625bd75ee2",
            "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"
        },
        {
            "paperId": "16e623059ffccab60f4c35be028a2d4f10933515",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective"
        },
        {
            "paperId": "91b29761840442005da39bc258e2298b528f31aa",
            "title": "Breaking the computation and communication abstraction barrier in distributed machine learning workloads"
        },
        {
            "paperId": "509b16378deec0fb6bbec1d7aeb32a4bdeedddb1",
            "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs"
        },
        {
            "paperId": "d037f3df7bcad91c18f394855054abf990f7bb40",
            "title": "Switches for HIRE: resource scheduling for data center in-network computing"
        },
        {
            "paperId": "72dd63d67588a42fc817bbb8d655b397f67425df",
            "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"
        },
        {
            "paperId": "774591fdd988eaaff3917e7c5171d044b0843e63",
            "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"
        },
        {
            "paperId": "238eb420c472bfdb1b4d34f9f53abec51f307a6b",
            "title": "FastMoE: A Fast Mixture-of-Expert Training System"
        },
        {
            "paperId": "040ad14a2c97e51510889ae6a0c3c23b29da801d",
            "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models"
        },
        {
            "paperId": "7a485356e538cf5d259912f41a7e54d2370397ca",
            "title": "3.2 The A100 Datacenter GPU and Ampere Architecture"
        },
        {
            "paperId": "12b71736392209b4292471b7da0aed71ba2aa545",
            "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"
        },
        {
            "paperId": "fdacf2a732f55befdc410ea927091cad3b791f13",
            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
        },
        {
            "paperId": "6c2294891e79e4f5eb7f000d586cbf0e0ed2d3b5",
            "title": "RoCC: robust congestion control for RDMA"
        },
        {
            "paperId": "2b2056bf5763e32811a69769fa8c223160125f9e",
            "title": "DNNFusion: accelerating deep neural networks execution with advanced operator fusion"
        },
        {
            "paperId": "7b829d3e23a6da339e254b6592a2a9ddd1ae526f",
            "title": "NetReduce: RDMA-Compatible In-Network Reduction for Distributed DNN Training Acceleration"
        },
        {
            "paperId": "2adcdc9e9e81147499e1372f992d25ac6265fb29",
            "title": "Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"
        },
        {
            "paperId": "461a7bf0c14df0fb08d3c59bdec491778a861270",
            "title": "Synthesizing optimal collective algorithms"
        },
        {
            "paperId": "2f4d6d3748ac6822711fe0bbd4cf6d2e66fa6613",
            "title": "Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads"
        },
        {
            "paperId": "dc9b09b688b4bfc116d1d55df3cdec399c768eb9",
            "title": "Google's Training Chips Revealed: TPUv2 and TPUv3"
        },
        {
            "paperId": "5b1809dce04d222900cc64f39f31e45f65bef6f3",
            "title": "Swift: Delay is Simple and Effective for Congestion Control in the Datacenter"
        },
        {
            "paperId": "725264948d7b6946259af5b8d966e996b9570f99",
            "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"
        },
        {
            "paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d",
            "title": "DAPPLE: a pipelined data parallel approach for training large models"
        },
        {
            "paperId": "488128bc81bb96ecdfdff8ca79fb793308d05285",
            "title": "Preemptive All-reduce Scheduling for Expediting Distributed DNN Training"
        },
        {
            "paperId": "1882f194cb43828852cc052887671e55a80f945a",
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
        },
        {
            "paperId": "7208279034eba55f9d4aaab9525f5484b434d019",
            "title": "Efficient Algorithms for Device Placement of DNN Graph Operators"
        },
        {
            "paperId": "ab5f0004c5f3317689e8457e1c8d8390ccbee522",
            "title": "A domain-specific supercomputer for training deep neural networks"
        },
        {
            "paperId": "cfa6e7ac8bef5b3aadcdc7a27d2a9e9d508b3322",
            "title": "Is Network the Bottleneck of Distributed Training?"
        },
        {
            "paperId": "09bda461aa4911d0513e8e46dd39a4113947e450",
            "title": "Ansor : Generating High-Performance Tensor Programs for Deep Learning"
        },
        {
            "paperId": "9d9dbb4487aca2b62ca3659446d7010ac65aa642",
            "title": "HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism"
        },
        {
            "paperId": "5e2df05a1a45b57d2fc0748bccc45865b9da6913",
            "title": "DeepFreeze: Towards Scalable Asynchronous Checkpointing of Deep Learning Models"
        },
        {
            "paperId": "c9060df13d2e7f06b277a2893590aebb653c5995",
            "title": "Automatic Cross-Replica Sharding of Weight Update in Data-Parallel Training"
        },
        {
            "paperId": "96a3afc3e3fd6ef727b6aea846f54f59283a473e",
            "title": "TensorOpt: Exploring the Tradeoffs in Distributed DNN Training with Auto-Parallelism"
        },
        {
            "paperId": "3b2a4cda4a1b9a4c2b2e478f4778719dca52c3fb",
            "title": "Balancing efficiency and fairness in heterogeneous GPU clusters for deep learning"
        },
        {
            "paperId": "d31b6e2991f494373eb27744b097471ffb319c2b",
            "title": "Quiver: An Informed Storage Cache for Deep Learning"
        },
        {
            "paperId": "6a5c0fc737b6fbd6672fc4265b5e0ca38de17416",
            "title": "Training Large Neural Networks with Constant Memory using a New Execution Algorithm"
        },
        {
            "paperId": "65aa97fd4f52f863bbce2fe8a88f762b90086020",
            "title": "2.2 AMD Chiplet Architecture for High-Performance Server and Desktop Products"
        },
        {
            "paperId": "cd410e078d091c567a1fb3438f705ac5d75009ec",
            "title": "EFLOPS: Algorithm and System Co-Design for a High Performance Distributed Training Platform"
        },
        {
            "paperId": "524f89a2a8f2a260e60f895076a9bd324df0cf25",
            "title": "AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators"
        },
        {
            "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
            "title": "Scaling Laws for Neural Language Models"
        },
        {
            "paperId": "5c36a76d181d9dd48923ba68f8b78f20db037af1",
            "title": "GradientFlow: Optimizing Network Performance for Large-Scale Distributed DNN Training"
        },
        {
            "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        },
        {
            "paperId": "dc52b09089704ebd6f471177474bc29741c50023",
            "title": "Fast Transformer Decoding: One Write-Head is All You Need"
        },
        {
            "paperId": "1e009f755503bffd7644fcd0a45939c54b838b37",
            "title": "BlueConnect: Decomposing all-reduce for deep learning on heterogeneous network hierarchy"
        },
        {
            "paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8",
            "title": "PipeDream: generalized pipeline parallelism for DNN training"
        },
        {
            "paperId": "76c929af6735cdff2c4badc9a9c8f39d15ea3e70",
            "title": "A generic communication scheduler for distributed DNN training acceleration"
        },
        {
            "paperId": "80b362efee95c1759c6dab9219eb77ca3ee44475",
            "title": "TASO: optimizing deep learning computation with automatic generation of graph substitutions"
        },
        {
            "paperId": "fd431005d26100f5453590080683cbae9dc1189f",
            "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization"
        },
        {
            "paperId": "00c957711b12468cb38424caccdf5291bb354033",
            "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"
        },
        {
            "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
        },
        {
            "paperId": "382b54869bb92039a7f7c56de67d70fb2421f604",
            "title": "HPCC: high precision congestion control"
        },
        {
            "paperId": "e9343b8322ea4bd2630a81209fe7fc86efbf4d3a",
            "title": "I/O Characterization and Performance Evaluation of BeeGFS for Deep Learning"
        },
        {
            "paperId": "4c3ae07aab62ad4aaa7448be0a0ed834b624b5a0",
            "title": "Themis: Fair and Efficient GPU Cluster Scheduling for Machine Learning Workloads"
        },
        {
            "paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f",
            "title": "Triton: an intermediate language and compiler for tiled neural network computations"
        },
        {
            "paperId": "f238364cff7c2723a2319d42a30323a780926cb5",
            "title": "Accelerating Distributed Reinforcement learning with In-Switch Computing"
        },
        {
            "paperId": "e65c84e2778d7b13b7541e6b14ff790b624a24ec",
            "title": "A Study of BFLOAT16 for Deep Learning Training"
        },
        {
            "paperId": "90cbe7f340a8de92143e5b464e6e963bb95f6129",
            "title": "Priority-based Parameter Propagation for Distributed DNN Training"
        },
        {
            "paperId": "6b39bea0ae1720dcbc1ed19ffa697114c4d356c4",
            "title": "Scalable Deep Learning on Distributed Infrastructures"
        },
        {
            "paperId": "d25626a852bd6df7502721e54b4330e7566f49ac",
            "title": "High performance Monte Carlo simulation of ising model on TPU clusters"
        },
        {
            "paperId": "46bb5162da1ad9b3591a9e056550135e7a50bc2b",
            "title": "Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect"
        },
        {
            "paperId": "ca513657a09369be8cec83fda6a5948e744c78a6",
            "title": "Scaling Distributed Machine Learning with In-Network Aggregation"
        },
        {
            "paperId": "a97a38f0c25685b1b486968b2b457f2263af4999",
            "title": "SwitchAgg: A Further Step Towards In-Network Computation"
        },
        {
            "paperId": "cc14e2e99ef12b01ecb1e869b46b9eb50e2179bd",
            "title": "HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array"
        },
        {
            "paperId": "9a1093af92d315def21b90918faf08665157051a",
            "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers"
        },
        {
            "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"
        },
        {
            "paperId": "7f12f77cd04dab6116a6ea1db73bf9425d94d8af",
            "title": "Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash"
        },
        {
            "paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f",
            "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"
        },
        {
            "paperId": "3c07aa4b70ee763b1c2f4b1e745deb5444dc6de5",
            "title": "Automatic Full Compilation of Julia Programs and ML Models to Cloud TPUs"
        },
        {
            "paperId": "0606676f16d581fa453f6b7b8a14fc7c4af8d025",
            "title": "Gandiva: Introspective Cluster Scheduling for Deep Learning"
        },
        {
            "paperId": "a82fc0115c1802d48d352b35595204738fad84f0",
            "title": "Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes"
        },
        {
            "paperId": "7dd7198bb8a61dd22879068e8c4619b32f0470f8",
            "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning"
        },
        {
            "paperId": "dd97a44fdc5923d1d3fd9d7c3dc300fb6f4f04ed",
            "title": "Spotlight: Optimizing Device Placement for Training Deep Neural Networks"
        },
        {
            "paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd",
            "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"
        },
        {
            "paperId": "2229ac756f89c3db017293918548555734d2f891",
            "title": "TicTac: Accelerating Distributed Deep Learning with Communication Scheduling"
        },
        {
            "paperId": "e2c8726d092aea573e69f5b0a2654225883cfacf",
            "title": "Horovod: fast and easy distributed deep learning in TensorFlow"
        },
        {
            "paperId": "3ea088eae8637530d1108065acab244f3b6c280d",
            "title": "Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks"
        },
        {
            "paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
            "title": "Mixed Precision Training"
        },
        {
            "paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
            "title": "Proximal Policy Optimization Algorithms"
        },
        {
            "paperId": "bfbd10ebffc9494423770a5bd30ebd0f9cbce66d",
            "title": "Device Placement Optimization with Reinforcement Learning"
        },
        {
            "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "title": "Attention is All you Need"
        },
        {
            "paperId": "079932bf6ff8b99c899172ba60071818f6b5dfcb",
            "title": "Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters"
        },
        {
            "paperId": "d5ea74717b3176aa4bd2963aae00dfb0406a2a5b",
            "title": "Dragonfly+: Low Cost Topology for Scaling Datacenters"
        },
        {
            "paperId": "b6c13c6b26b5e39f427727365a1ab59151cb4fdb",
            "title": "ECN or Delay: Lessons Learnt from Analysis of DCQCN and TIMELY"
        },
        {
            "paperId": "c0c352b314e0d972e7eabd35e435789791d407cc",
            "title": "Scalable Hierarchical Aggregation Protocol (SHArP): A Hardware Architecture for Efficient Data Reduction"
        },
        {
            "paperId": "60ddf74dd5b443c3bfb59fe876b42f9d6112c4fb",
            "title": "RDMA over Commodity Ethernet at Scale"
        },
        {
            "paperId": "4954fa180728932959997a4768411ff9136aac81",
            "title": "TensorFlow: A system for large-scale machine learning"
        },
        {
            "paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e",
            "title": "Training Deep Nets with Sublinear Memory Cost"
        },
        {
            "paperId": "07367703f587dbc3313cc613289c4330cebe5c8c",
            "title": "Congestion Control for Large-Scale RDMA Deployments"
        },
        {
            "paperId": "327a02b19a60319cc35be860ad0259a5c1aef920",
            "title": "TIMELY: RTT-based Congestion Control for the Datacenter"
        },
        {
            "paperId": "9e9191ff842fd86dbb4ae118bfb831ad8b0c532a",
            "title": "NetAgg: Using Middleboxes for Application-specific On-path Aggregation in Data Centres"
        },
        {
            "paperId": "d48e362d8c25179cf05f737d64d070c64c57186b",
            "title": "Tachyon: Reliable, Memory Speed Storage for Cluster Computing Frameworks"
        },
        {
            "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
            "paperId": "463e7d2a62b1f15cc2daba34230297827e7c6757",
            "title": "P4: programming protocol-independent packet processors"
        },
        {
            "paperId": "ff3cb3a14ce0d248cdf4b63d8be3c8b220f36a5f",
            "title": "The MVAPICH Project: Evolution and Sustainability of an Open Source Production Quality MPI Library for HPC"
        },
        {
            "paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235",
            "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"
        },
        {
            "paperId": "4d23db55e6671a82c95dacec33b2967a4b8b677d",
            "title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"
        },
        {
            "paperId": "b0c6c04c406daa61e61016c446d81bbb4b2cd5d3",
            "title": "On the impact of packet spraying in data center networks"
        },
        {
            "paperId": "e764048eb0d28e0351584475e9c44c215fce76d3",
            "title": "Camdoop: Exploiting In-network Aggregation for Big Data Applications"
        },
        {
            "paperId": "3ffae44b736b6dbd6d715977bb6381297dd94304",
            "title": "Communication-Optimal Parallel 2.5D Matrix Multiplication and LU Factorization Algorithms"
        },
        {
            "paperId": "5b8bcb3d2b8e2f4111c61003abbfbb5963187870",
            "title": "Jellyfish: Networking Data Centers Randomly"
        },
        {
            "paperId": "677f55089039398ad9f23288fd89e77ff8380de3",
            "title": "The development of Mellanox/NVIDIA GPUDirect over InfiniBand\u2014a new model for GPU to GPU communications"
        },
        {
            "paperId": "8ce4c0ee315d86f32ec7354ccdf8d8996e8ee270",
            "title": "The Hadoop Distributed File System"
        },
        {
            "paperId": "fe33ba23625e0039b6bddf69a63f43dfe22928b1",
            "title": "BCube: a high performance, server-centric network architecture for modular data centers"
        },
        {
            "paperId": "6f4e48c2a5de9337d147ebbb7d0ff0e555adceca",
            "title": "Bandwidth optimal all-reduce algorithms for clusters of workstations"
        },
        {
            "paperId": "bd2e28376aca5a8ef1cf6068a3aeb1ca6d0e1abd",
            "title": "Dcell: a scalable and fault-tolerant network structure for data centers"
        },
        {
            "paperId": "b81350c2601441dcd526e656a590ff354dedb220",
            "title": "Technology-Driven, Highly-Scalable Dragonfly Topology"
        },
        {
            "paperId": "c5de8202da04a88a0122ac2c21780cd9ec3db43b",
            "title": "Ceph: a scalable, high-performance distributed file system"
        },
        {
            "paperId": "b27542559edf6cc867afdc1637fe36b9a4e250e6",
            "title": "Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation"
        },
        {
            "paperId": "ebc09b04a900afc6c3cf53a4b7ff6035f33f02b2",
            "title": "MPICH2: A New Start for MPI Implementations"
        },
        {
            "paperId": "cfa5b563d50efc83271883407c98b73a39f959d6",
            "title": "Analysis of an Equal-Cost Multi-Path Algorithm"
        },
        {
            "paperId": "1aa8ad634d1879af9b5ac34b44ecc3de8debd276",
            "title": "A three-dimensional approach to parallel matrix multiplication"
        },
        {
            "paperId": "6326e8ab3d2c2b179c26b1aa368fae7e42a34ed0",
            "title": "SUMMA: scalable universal matrix multiplication algorithm"
        },
        {
            "paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56",
            "title": "Adaptive Mixtures of Local Experts"
        },
        {
            "paperId": "fa5c7464479bd56f6d4c2c6f42463d37a65686da",
            "title": "Interconnection Networks"
        },
        {
            "paperId": "148c1c241f4b4bd3848b6c0d6ff99e90a534dfce",
            "title": "A study of non-blocking switching networks"
        },
        {
            "paperId": "5271353b3eeccd342536818b59105d04d70942da",
            "title": "Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism"
        },
        {
            "paperId": "1b5de61ad031cd382436829b6dbd3f47f26dbde1",
            "title": "Resiliency at Scale: Managing Google's TPUv4 Machine Learning Supercomputer"
        },
        {
            "paperId": "d8a144b524524091006ea64e77e402e1740a3252",
            "title": "DISTMM: Accelerating Distributed Multimodal Model Training"
        },
        {
            "paperId": "38687b782053e029ce3773fabed85b9fdeea93ca",
            "title": "When In-Network Computing Meets Distributed Machine Learning"
        },
        {
            "paperId": "9540e390fa481fa4130733445f240365c22c2367",
            "title": "Towards Domain-Specific Network Transport for Distributed DNN Training"
        },
        {
            "paperId": "adf7b1ed6b4d4ae346888278b7496d55e8e3f13f",
            "title": "PUZZLE: Efficiently Aligning Large Language Models through Light-Weight Context Switch"
        },
        {
            "paperId": "b1d674e82f7579fdf2fb8de1e772ee8610bc2034",
            "title": "nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training"
        },
        {
            "paperId": "e51578b433b7cf7cb92febd4fcd8b78035f08ff9",
            "title": "Breadth-First Pipeline Parallelism"
        },
        {
            "paperId": "aa6ddad0a84eaa004e49142981d05c5f36cc585e",
            "title": "Hanayo: Harnessing Wave-Like Pipeline Parallelism for Enhanced Large Model Training Efficiency"
        },
        {
            "paperId": "33489921dac9fed095332ea13326043c9911a6d3",
            "title": "Welder: Scheduling Deep Learning Memory Access via Tile-graph"
        },
        {
            "paperId": "283e56e843edc2ab599b0cf8218b936a89a11875",
            "title": "BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models"
        },
        {
            "paperId": "7889cc4e9565c9e7bd725509d7da4597e6a9b576",
            "title": "CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks"
        },
        {
            "paperId": "6597b53ea7f7c1909870c9be6ed54695ec36d3b6",
            "title": "Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters"
        },
        {
            "paperId": "cb542f525a3a91bc8d0d3fe69235f59e75822fd4",
            "title": "Better Together: Jointly Optimizing ML Collective Scheduling and Execution Planning using SYNDICATE"
        },
        {
            "paperId": "f4945c90c509f9fcc3a416371d4c75c110cbcb66",
            "title": "EnvPipe: Performance-preserving DNN Training Framework for Saving Energy"
        },
        {
            "paperId": "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8",
            "title": "Blockwise Parallel Transformers for Large Context Models"
        },
        {
            "paperId": "ac771182d1780c863954243809d1e144433919f9",
            "title": "Aligning Large Language Models with Human: A Survey"
        },
        {
            "paperId": "8f4e626c2783b0fada28d206c3584ed6674ba53f",
            "title": "Beware of Fragmentation: Scheduling GPU-Sharing Workloads with Fragmentation Gradient Descent"
        },
        {
            "paperId": "5c6a17850c9ad6bf6dc8992ec598cd932ce42208",
            "title": "SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization"
        },
        {
            "paperId": null,
            "title": "Gaudi training platform white paper"
        },
        {
            "paperId": "ee2c6440d67a7193d4aec6c51f5eb535f637a45b",
            "title": "mCAP: Memory-Centric Partitioning for Large-Scale Pipeline-Parallel DNN Training"
        },
        {
            "paperId": "7250b2053d05687664029c29c2f1225b48c7ee45",
            "title": "ROLLER: Fast and Efficient Tensor Compilation for Deep Learning"
        },
        {
            "paperId": "08041ba871ab80f2f63eddd5ae6f0b6443c0306f",
            "title": "Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training"
        },
        {
            "paperId": "da37fb0a9071d1ceda0c7e359a049c6c7e627a01",
            "title": "Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization"
        },
        {
            "paperId": "5af975364d0a228e316278575cadbe41f03b48e4",
            "title": "Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters"
        },
        {
            "paperId": "ca1402619a80c140c650961d899319c2928744f0",
            "title": "Piper: Multidimensional Planner for DNN Parallelization"
        },
        {
            "paperId": "d87ceea3f5d0582997840a107f2bd6c5a1aadbde",
            "title": "In-network Aggregation for Shared Machine Learning Clusters"
        },
        {
            "paperId": "f99dc2d2f20082e7638c6ad31503df7ab0b59828",
            "title": "Facebook's Tectonic Filesystem: Efficiency from Exascale"
        },
        {
            "paperId": "c90a5375329ea537d4ac266297b1c5cca89af19b",
            "title": "ATP: In-network Aggregation for Multi-tenant Learning"
        },
        {
            "paperId": null,
            "title": "Deepspeed autotuning"
        },
        {
            "paperId": null,
            "title": "\u201c { CheckFreq } : Frequent, { Fine-Grained }{ DNN } checkpointing,\u201d"
        },
        {
            "paperId": "57bc3e71a889013981b191b6431bec3dc45eba9f",
            "title": "AntMan: Dynamic Scaling on GPU Clusters for Deep Learning"
        },
        {
            "paperId": null,
            "title": "\u201cPytorch distributed: Experiences on accelerating data parallel training,\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cDynamic tensor remateri-alization,\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cPlink: Discovering and exploiting datacenter network locality for efficient cloud-based distributed training,\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cInfiniband product guide,\u201d"
        },
        {
            "paperId": "e6cc6a7bd4db3e7604bae6a654ec29aa8542dafc",
            "title": "Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks"
        },
        {
            "paperId": "8d2d560bf1c4c6930d2d1411e48b642bd5b81179",
            "title": "Tiresias: A GPU Cluster Manager for Distributed Deep Learning"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "parallelism for deep neural networks"
        },
        {
            "paperId": null,
            "title": "for DNN training workloads"
        },
        {
            "paperId": null,
            "title": "Massively scale your deep learning training with nccl 2.4"
        },
        {
            "paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603",
            "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"
        },
        {
            "paperId": null,
            "title": "\u201cJax: composable transformations of python+ numpy programs,\u201d"
        },
        {
            "paperId": null,
            "title": "framework for emerging { AI } applications"
        },
        {
            "paperId": null,
            "title": "Doubling all2all performance with nvidia collective communication library 2.12"
        },
        {
            "paperId": null,
            "title": "Nvidia dgx-2: The world\u2019s most powerful system for the most complex ai challenges"
        },
        {
            "paperId": null,
            "title": "Rccl library"
        },
        {
            "paperId": "f5121d0f20ab249d16e8f3473f9179cbe5499ef1",
            "title": "NVIDIA DGX-1 System Architecture White paper"
        },
        {
            "paperId": null,
            "title": "\u201cRoce vs. iwarp competitive analysis,\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cNvidia data center gpu manager (dcgm),\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cUsing pci express\u00ae as the primary system interconnect in multiroot compute, storage, communications and embedded systems,\u201d"
        },
        {
            "paperId": "551adcf26986bfa27af744b588dbaaff20c7932f",
            "title": "Lustre: Building a File System for 1,000-node Clusters"
        },
        {
            "paperId": "8b52bb4cb8a5ae2ffb573ba33ccc5455069be910",
            "title": "An Introduction to the InfiniBand Architecture"
        },
        {
            "paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "title": "Gradient-based learning applied to document recognition"
        },
        {
            "paperId": "a40614033be2c4f6dd17bcd420a044f6381874c9",
            "title": "MPI: A message - passing interface standard"
        },
        {
            "paperId": "f9c2ee112cd9fa53ace7b75393cdd8baa72f678e",
            "title": "A cellular computer to implement the kalman filter algorithm"
        },
        {
            "paperId": null,
            "title": "LlamaTeam"
        },
        {
            "paperId": "1d27a56a8133f947a5a0217b00241d26f585f834",
            "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."
        },
        {
            "paperId": null,
            "title": "\u201cMegtaichi: Dynamic tensor-based memory"
        },
        {
            "paperId": null,
            "title": "\u201cOp-timized network architectures for large language model training with billions of parameters,\u201d"
        },
        {
            "paperId": "ceecad72381aca9dae1ab24c46f014301dfe8b44",
            "title": "This paper is included in the Proceedings of the 2022 USENIX Annual Technical Conference. Whale: Efficient Giant Model Training over Heterogeneous GPUs"
        },
        {
            "paperId": null,
            "title": "\u201cThe promise and peril of generative ai,\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cTrl - transformer reinforcement learning.\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cGemini: a family of highly capable multimodal models,\u201d"
        },
        {
            "paperId": null,
            "title": "(2023) Pytorch expandable segments"
        },
        {
            "paperId": null,
            "title": "\u201c { Check-N-Run } : A checkpointing system for training deep learning recommendation models,\u201d"
        },
        {
            "paperId": null,
            "title": "(2023) Taking stock of new data center computing paradigm"
        },
        {
            "paperId": null,
            "title": "\u201cTorchsnapshor: A performant, memory-efficient checkpointing library for pytorch applications, designed with large, complex distributed workloads in mind.\u201d"
        },
        {
            "paperId": null,
            "title": "\u201cUnlocking the power of inline { Floating-Point } operations on programmable switches,\u201d"
        },
        {
            "paperId": null,
            "title": "Building Meta\u2019s GenAI Infrastructure"
        },
        {
            "paperId": null,
            "title": "\u201c { MLaaS } in the wild: Workload analysis and scheduling in { Large-Scale } heterogeneous { GPU } clusters,\u201d"
        },
        {
            "paperId": null,
            "title": "802.1qbb \u2013 priority-based flow control"
        },
        {
            "paperId": null,
            "title": "\u201cAnalysis of large-scale multi-tenant GPU clusters"
        },
        {
            "paperId": null,
            "title": "\u201cRay: A distributed"
        },
        {
            "paperId": "684d6a042808a97b6779336f3e3b06a34b1a6a6d",
            "title": "This paper is included in the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation."
        },
        {
            "paperId": null,
            "title": "\u201cunsloth - finetune llama 3, mistral, phi-3 & gemma 2-5x faster with 80"
        },
        {
            "paperId": null,
            "title": "Architectural Specifications for RDMA over TCP/IP"
        },
        {
            "paperId": null,
            "title": "\u201cAccl: Architecting highly scalable dis-41"
        },
        {
            "paperId": null,
            "title": "Megatron context parallelism"
        }
    ]
}