{
    "paperId": "64b34d20d217a2db702716d924396fef555abddc",
    "externalIds": {
        "ArXiv": "2408.15040",
        "CorpusId": 271963268
    },
    "title": "A Survey of Large Language Models for European Languages",
    "abstract": "Large Language Models (LLMs) have gained significant attention due to their high performance on a wide range of natural language tasks since the release of ChatGPT. The LLMs learn to understand and generate language by training billions of model parameters on vast volumes of text data. Despite being a relatively new field, LLM research is rapidly advancing in various directions. In this paper, we present an overview of LLM families, including LLaMA, PaLM, GPT, and MoE, and the methods developed to create and enhance LLMs for official European Union (EU) languages. We provide a comprehensive summary of common monolingual and multilingual datasets used for pretraining large language models.",
    "venue": "",
    "year": 2024,
    "referenceCount": 156,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An overview of LLM families, including LLaMA, PaLM, GPT, and MoE, and the methods developed to create and enhance LLMs for official European Union languages are presented."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -4.695226192474365,
            -1.047235131263733,
            -0.39953070878982544,
            3.800471544265747,
            1.3767144680023193,
            -0.9171075820922852,
            5.861583709716797,
            -1.2035419940948486,
            -1.4553751945495605,
            5.415676593780518,
            -0.2187667042016983,
            3.613699197769165,
            0.2667582035064697,
            0.0746387243270874,
            -0.2748984098434448,
            -0.45355159044265747,
            1.89774489402771,
            -4.8227338790893555,
            4.355998992919922,
            0.06941938400268555,
            -3.0490314960479736,
            0.6613489985466003,
            -1.3087109327316284,
            -2.1813807487487793,
            -0.1698085069656372,
            -2.2895712852478027,
            5.338112831115723,
            4.069389343261719,
            -1.3147639036178589,
            -0.9225441217422485,
            -1.9464111328125,
            -3.212097644805908,
            0.9763618111610413,
            -5.6388139724731445,
            -0.44776302576065063,
            -2.1167821884155273,
            -3.277156352996826,
            6.052677631378174,
            -1.22450852394104,
            -1.6534936428070068,
            -1.1854760646820068,
            -2.09858775138855,
            2.2143054008483887,
            3.8182501792907715,
            3.810152530670166,
            1.4025293588638306,
            1.5191981792449951,
            1.1202839612960815,
            -1.0586812496185303,
            -0.1133989691734314,
            4.320414066314697,
            -1.366558313369751,
            1.7927650213241577,
            4.2772393226623535,
            1.8516610860824585,
            1.4362967014312744,
            -0.5892136096954346,
            -0.20760899782180786,
            2.4863083362579346,
            -4.935569763183594,
            3.4924843311309814,
            9.21392822265625,
            -0.74664306640625,
            1.2164599895477295,
            2.411745309829712,
            -0.8957693576812744,
            -6.817257881164551,
            2.5863900184631348,
            -0.17148852348327637,
            1.0394055843353271,
            -1.1051822900772095,
            -5.336496829986572,
            -0.31757718324661255,
            1.2559508085250854,
            -0.5554314255714417,
            -0.319724440574646,
            -1.5077736377716064,
            -5.497845649719238,
            1.471592903137207,
            -0.4479316473007202,
            -2.1833643913269043,
            -1.6526926755905151,
            1.7011597156524658,
            1.2227370738983154,
            4.666338920593262,
            -0.6945447325706482,
            -7.1282854080200195,
            2.1449737548828125,
            2.891083240509033,
            -3.0166029930114746,
            0.03904023766517639,
            -0.7113403677940369,
            0.7723993062973022,
            4.673023223876953,
            -2.100351572036743,
            -1.9015474319458008,
            2.972862482070923,
            0.22790789604187012,
            -1.836836576461792,
            -1.75162935256958,
            4.386133193969727,
            -1.8167144060134888,
            -0.5199642181396484,
            3.3268561363220215,
            3.809577226638794,
            -5.281281471252441,
            1.8092364072799683,
            -1.3787858486175537,
            3.086804151535034,
            -2.2038204669952393,
            -2.3562676906585693,
            2.128715991973877,
            -1.9169390201568604,
            -2.1169161796569824,
            0.6460527181625366,
            -2.462963104248047,
            -1.0264720916748047,
            2.907665729522705,
            -1.4668115377426147,
            2.5981104373931885,
            -0.43670934438705444,
            -1.5596741437911987,
            -1.2831292152404785,
            -1.6015111207962036,
            2.1833486557006836,
            3.870342493057251,
            -0.9279295206069946,
            0.5017557740211487,
            -1.0416128635406494,
            -3.193242311477661,
            2.510223150253296,
            1.9253664016723633,
            5.7227983474731445,
            -1.95115327835083,
            3.0008387565612793,
            3.2054882049560547,
            -3.682065010070801,
            1.4935353994369507,
            2.1081032752990723,
            -0.42035114765167236,
            3.8282647132873535,
            4.801016807556152,
            1.210233211517334,
            -0.7834848761558533,
            -1.696608066558838,
            -0.6084126234054565,
            1.4579790830612183,
            -0.25379252433776855,
            -0.8518384695053101,
            4.2332963943481445,
            3.3043861389160156,
            -3.1796813011169434,
            0.22067350149154663,
            -0.6293545365333557,
            -2.4387316703796387,
            5.288950443267822,
            -5.690235137939453,
            1.092045545578003,
            -0.22380331158638,
            0.7110962867736816,
            -2.2277848720550537,
            2.5295040607452393,
            -9.851197242736816,
            -2.6935839653015137,
            1.9794936180114746,
            -3.663816213607788,
            0.6560257077217102,
            1.4136170148849487,
            0.9647507667541504,
            1.3709160089492798,
            -0.8644759654998779,
            -0.2704890966415405,
            0.38059982657432556,
            1.7428743839263916,
            0.7879756093025208,
            2.0965254306793213,
            1.4227612018585205,
            -1.1637458801269531,
            -3.833127975463867,
            0.3841152787208557,
            -0.5290793776512146,
            -2.500225782394409,
            -6.340662956237793,
            -0.951450765132904,
            -6.712383270263672,
            -1.7849496603012085,
            -1.9033737182617188,
            -3.384636402130127,
            4.48847770690918,
            -2.3291218280792236,
            -0.1325441598892212,
            -0.29344436526298523,
            3.797657012939453,
            7.2727370262146,
            0.13728660345077515,
            2.046921968460083,
            3.6799156665802,
            5.320911407470703,
            -3.1616387367248535,
            -1.1263926029205322,
            2.717787265777588,
            -1.2590141296386719,
            -1.3844435214996338,
            -2.6941609382629395,
            3.9221959114074707,
            2.2095699310302734,
            -4.483328819274902,
            2.7213659286499023,
            2.2347469329833984,
            -0.7825270295143127,
            1.4715808629989624,
            -1.7586249113082886,
            -3.6613941192626953,
            1.901753306388855,
            -0.1014336347579956,
            -2.8835980892181396,
            -4.208375930786133,
            0.45500195026397705,
            2.9232773780822754,
            -0.00813990831375122,
            -2.0665175914764404,
            1.0855275392532349,
            -0.10960736870765686,
            -3.416264295578003,
            0.9461234211921692,
            -5.964517593383789,
            -0.8013395667076111,
            -0.0051688551902771,
            -0.5380805730819702,
            -2.603950262069702,
            0.08267435431480408,
            -1.871161699295044,
            -0.8087477087974548,
            0.6841828227043152,
            -5.2607574462890625,
            0.06376063823699951,
            -5.18181037902832,
            2.884597063064575,
            0.6359074711799622,
            -3.1205248832702637,
            1.874311923980713,
            1.9283783435821533,
            1.360811710357666,
            4.084968566894531,
            2.2400951385498047,
            -1.8739097118377686,
            -2.1936323642730713,
            1.210615873336792,
            -1.79616117477417,
            -1.334956169128418,
            -3.4691946506500244,
            0.539455235004425,
            3.1225481033325195,
            -0.5895058512687683,
            -0.3326597511768341,
            0.7000679969787598,
            3.4876513481140137,
            0.9473215341567993,
            0.48563334345817566,
            3.480799674987793,
            1.3457311391830444,
            5.497770309448242,
            -2.031245470046997,
            2.780550241470337,
            -4.245999813079834,
            -1.2444037199020386,
            -3.37725567817688,
            -1.0225656032562256,
            -2.2786247730255127,
            5.715178489685059,
            3.122502326965332,
            -0.33903762698173523,
            1.0653393268585205,
            -3.647566556930542,
            -0.47231218218803406,
            -5.051916122436523,
            -0.9511715769767761,
            -2.489515781402588,
            3.5368847846984863,
            -1.483363389968872,
            2.074141263961792,
            -4.098616123199463,
            -1.5010054111480713,
            -2.0095584392547607,
            0.9186069965362549,
            -3.9372124671936035,
            -3.0046286582946777,
            -1.0416984558105469,
            -1.8954999446868896,
            -6.837512016296387,
            -0.10952877253293991,
            2.381059169769287,
            -1.5770515203475952,
            -4.921848773956299,
            -1.678055763244629,
            0.21582072973251343,
            3.5208895206451416,
            2.0815889835357666,
            0.14382897317409515,
            0.4947943091392517,
            1.1047395467758179,
            6.227234363555908,
            4.87982177734375,
            -1.1473979949951172,
            1.310347318649292,
            -0.1508563756942749,
            0.9832932949066162,
            -2.0254263877868652,
            4.594106197357178,
            -3.998034954071045,
            -2.1312899589538574,
            2.3247289657592773,
            2.349416732788086,
            -4.236240386962891,
            3.063155174255371,
            2.3441789150238037,
            2.1028716564178467,
            1.283473253250122,
            -5.38715124130249,
            0.9205390810966492,
            2.1753387451171875,
            1.5270378589630127,
            -5.329179286956787,
            -1.615211009979248,
            -5.137029647827148,
            1.9881161451339722,
            1.5383812189102173,
            0.34697574377059937,
            -3.3558266162872314,
            3.6650969982147217,
            0.7406978607177734,
            4.046015739440918,
            0.497860312461853,
            2.5447351932525635,
            -2.7773237228393555,
            -3.3060011863708496,
            -3.381446361541748,
            -2.960092067718506,
            4.689087390899658,
            -1.5356987714767456,
            3.7099721431732178,
            3.6346280574798584,
            1.001317024230957,
            1.2335947751998901,
            1.144618034362793,
            1.4269601106643677,
            2.700119972229004,
            2.618626117706299,
            2.51393985748291,
            -1.035412073135376,
            1.4438567161560059,
            1.8780567646026611,
            4.239076137542725,
            -0.6929404735565186,
            0.988216757774353,
            2.514249801635742,
            2.4640767574310303,
            -0.04593920707702637,
            0.8150822520256042,
            0.22071129083633423,
            -0.16991686820983887,
            -0.4846203327178955,
            5.254390716552734,
            2.903461456298828,
            3.5703983306884766,
            -1.8412528038024902,
            9.57414436340332,
            -5.438371658325195,
            2.44806170463562,
            -6.04481315612793,
            -1.486331582069397,
            -2.2530126571655273,
            -4.284958362579346,
            4.80430793762207,
            -0.8850582838058472,
            -2.0598480701446533,
            -1.925994634628296,
            -7.795676231384277,
            0.6704587936401367,
            2.8214292526245117,
            -1.5763921737670898,
            0.9717676639556885,
            -3.706990957260132,
            -1.8995006084442139,
            0.657644510269165,
            3.954291820526123,
            -2.882248640060425,
            1.2192144393920898,
            -0.897146463394165,
            0.06871327757835388,
            -1.703696370124817,
            -0.7295213341712952,
            -1.0942044258117676,
            -0.7992319464683533,
            -3.3167006969451904,
            1.5914498567581177,
            -0.7781475782394409,
            -0.5256797075271606,
            2.0136938095092773,
            3.980015277862549,
            0.6117855310440063,
            -1.9685920476913452,
            1.9176783561706543,
            -0.07163017988204956,
            -3.5041158199310303,
            1.2921347618103027,
            2.451552629470825,
            -1.2042675018310547,
            -2.698988914489746,
            -1.574965238571167,
            -3.2240395545959473,
            -4.672022342681885,
            0.060659706592559814,
            -4.229837417602539,
            -0.38937991857528687,
            1.2742977142333984,
            1.8162944316864014,
            4.079593658447266,
            1.0639252662658691,
            0.2322186529636383,
            -2.393012523651123,
            2.3813271522521973,
            6.489514350891113,
            0.36238592863082886,
            0.9469708204269409,
            0.1051037609577179,
            0.9059687256813049,
            2.1751818656921387,
            -2.659250020980835,
            1.9907150268554688,
            1.4914722442626953,
            3.06803035736084,
            -0.9825478792190552,
            -3.108628034591675,
            -0.20421916246414185,
            -0.09112739562988281,
            0.3832571804523468,
            2.7416086196899414,
            3.4073727130889893,
            -2.376166343688965,
            0.9546710252761841,
            2.9694862365722656,
            -1.4287927150726318,
            3.6314754486083984,
            0.8058478236198425,
            4.491254806518555,
            2.935117483139038,
            1.6629557609558105,
            -1.0467602014541626,
            0.14213109016418457,
            5.281313896179199,
            -3.850166082382202,
            -1.2974313497543335,
            -2.2937021255493164,
            0.5390678644180298,
            1.373363733291626,
            -2.2092390060424805,
            2.6743903160095215,
            0.06896066665649414,
            1.6772913932800293,
            -0.023026734590530396,
            2.0541040897369385,
            -3.869706630706787,
            -0.256288081407547,
            0.9336050748825073,
            -2.0515713691711426,
            -1.6030346155166626,
            -2.5230422019958496,
            -1.4917950630187988,
            3.9486076831817627,
            -1.048166036605835,
            -0.19884884357452393,
            -2.1904168128967285,
            3.8880789279937744,
            -1.2313166856765747,
            2.5960230827331543,
            3.2221927642822266,
            -3.2172293663024902,
            0.7854382395744324,
            -5.660247802734375,
            -1.9150481224060059,
            1.011642336845398,
            3.7486634254455566,
            -3.971161365509033,
            -1.8436733484268188,
            3.570910930633545,
            1.1025981903076172,
            0.11811202764511108,
            3.4806861877441406,
            -1.8562262058258057,
            -0.3998251259326935,
            5.423101902008057,
            2.972414016723633,
            0.5522315502166748,
            -2.108229398727417,
            -4.233213901519775,
            -5.689631938934326,
            0.5406509637832642,
            1.0111253261566162,
            -1.2859227657318115,
            2.446320056915283,
            -6.930080413818359,
            -0.08332473039627075,
            0.6343560218811035,
            -1.906425952911377,
            7.2414140701293945,
            4.286478519439697,
            -2.201488733291626,
            -2.431621551513672,
            -0.09889388084411621,
            1.862248420715332,
            0.38159406185150146,
            -4.7734375,
            3.207454204559326,
            -0.9123470187187195,
            -0.1385980248451233,
            0.9987866878509521,
            -5.547285079956055,
            0.12120050191879272,
            -2.1127562522888184,
            -1.8622510433197021,
            -0.14724844694137573,
            -1.8576418161392212,
            4.536717414855957,
            1.8928879499435425,
            -1.16163170337677,
            1.5883967876434326,
            -4.8588547706604,
            0.07887256145477295,
            5.0209784507751465,
            3.5394749641418457,
            1.5691190958023071,
            4.0986247062683105,
            -1.2732172012329102,
            2.01454496383667,
            -4.2269487380981445,
            -0.7737131118774414,
            1.7622737884521484,
            -2.0439252853393555,
            -1.8935664892196655,
            -0.9384255409240723,
            1.2222836017608643,
            2.131669521331787,
            5.421123504638672,
            0.82124924659729,
            0.9740263819694519,
            -3.9211130142211914,
            -2.983633518218994,
            -1.3757132291793823,
            0.6845811605453491,
            2.3446359634399414,
            -0.016567528247833252,
            2.421781063079834,
            4.083220481872559,
            -3.3944852352142334,
            -1.5716149806976318,
            -0.5598155856132507,
            -1.9965343475341797,
            -1.495490312576294,
            3.9548375606536865,
            3.1523146629333496,
            -0.461647629737854,
            2.6333119869232178,
            1.1491880416870117,
            2.67039155960083,
            0.26229146122932434,
            -1.4357457160949707,
            3.405815362930298,
            1.74374520778656,
            -4.789146900177002,
            0.4492252469062805,
            -2.8390727043151855,
            3.4178285598754883,
            1.276674509048462,
            1.664893627166748,
            2.7564351558685303,
            4.745919227600098,
            0.3861019015312195,
            0.040738821029663086,
            -2.198045015335083,
            -2.3229668140411377,
            0.0729019045829773,
            0.7105525732040405,
            -2.497797727584839,
            0.1692773401737213,
            -0.3642072379589081,
            -1.936861515045166,
            1.5817158222198486,
            -3.897970676422119,
            0.4388277530670166,
            2.1098906993865967,
            -1.4526903629302979,
            -1.3560993671417236,
            -2.557934045791626,
            0.6399010419845581,
            -4.206526279449463,
            2.972024917602539,
            3.000082015991211,
            -0.4964159429073334,
            4.726814270019531,
            -0.23294416069984436,
            5.142816543579102,
            0.5672207474708557,
            1.3311171531677246,
            0.7654577493667603,
            -1.8179712295532227,
            3.8130264282226562,
            -1.3824691772460938,
            -0.06977644562721252,
            -1.6259254217147827,
            -2.1906628608703613,
            3.94866943359375,
            11.823296546936035,
            1.6579591035842896,
            -1.3941981792449951,
            2.870814800262451,
            -4.593253135681152,
            -2.1287779808044434,
            -3.250974655151367,
            0.15640705823898315,
            0.7648918032646179,
            -0.1422971785068512,
            -1.634577989578247,
            0.14842712879180908,
            -2.310953378677368,
            3.618694305419922,
            -3.6167612075805664,
            -1.4910681247711182,
            -0.26756197214126587,
            2.592428207397461,
            0.420560747385025,
            -1.1116255521774292,
            -1.8692939281463623,
            2.736665725708008,
            -1.3156360387802124,
            -4.901330947875977,
            -1.2364706993103027,
            3.848496437072754,
            5.36983585357666,
            3.1626434326171875,
            -3.976954221725464,
            -1.5653777122497559,
            -1.5029635429382324,
            3.7488155364990234,
            -2.6235146522521973,
            1.0900084972381592,
            -0.6133442521095276,
            1.8070532083511353,
            1.4076759815216064,
            -5.587323188781738,
            0.7114235162734985,
            1.188414216041565,
            -2.2232513427734375,
            -0.2563375234603882,
            -3.831185817718506,
            3.0097835063934326,
            -4.0247697830200195,
            1.0385879278182983,
            1.9101510047912598,
            -1.231631875038147,
            0.2456250786781311,
            4.612697601318359,
            0.4741370677947998,
            -4.882288932800293,
            -0.49195587635040283,
            -1.5116243362426758,
            2.517995834350586,
            -4.590847015380859,
            0.3746042847633362,
            -1.080264925956726,
            2.1434717178344727,
            -2.950223922729492,
            2.686039447784424,
            -1.5103888511657715,
            0.14713457226753235,
            -2.5648305416107178,
            -3.2292985916137695,
            -0.1276177167892456,
            -6.547557830810547,
            2.950627565383911,
            1.4522390365600586,
            -0.4585304260253906,
            1.9374256134033203,
            0.9968286156654358,
            -1.8277744054794312,
            -1.1839828491210938,
            -0.8237841129302979,
            -0.41570112109184265,
            1.3476016521453857,
            -3.6574926376342773,
            -2.2584683895111084,
            4.586204528808594,
            1.1531506776809692,
            1.6223723888397217,
            -1.9911420345306396,
            -3.8706555366516113,
            -0.35521337389945984,
            0.9016777276992798,
            3.529026985168457,
            1.6714098453521729,
            -0.9212371110916138,
            2.8876142501831055,
            -2.87093186378479,
            -0.728032648563385,
            5.9786834716796875,
            4.664274215698242,
            -0.6627199053764343,
            -3.352867603302002,
            -1.1970491409301758,
            0.20644959807395935,
            -3.4521889686584473,
            -1.4481873512268066,
            5.408579349517822,
            3.0134005546569824,
            0.0576939582824707,
            -4.717562675476074,
            1.1737476587295532,
            -0.5734124183654785,
            -0.8892800807952881,
            -4.077212333679199,
            -3.092486619949341,
            -1.3247222900390625,
            3.5073001384735107,
            -1.334172248840332,
            -0.9742375016212463,
            0.9034507274627686,
            2.191826343536377,
            -0.4125977158546448,
            3.9240167140960693,
            0.780005693435669,
            0.2592405080795288,
            0.8005725145339966,
            1.3303900957107544,
            -3.4572205543518066,
            -1.660587191581726,
            -6.107278823852539,
            -2.811994791030884,
            -2.0924181938171387,
            2.8706154823303223,
            -1.21180260181427,
            -2.252920627593994,
            -0.26526200771331787,
            4.442088603973389,
            0.5053537487983704,
            0.7149406671524048,
            1.4438254833221436,
            0.721055269241333,
            -1.6360528469085693,
            1.29643976688385,
            1.1370564699172974,
            1.6057522296905518,
            3.4614367485046387,
            1.1212513446807861,
            -0.8880527019500732,
            -1.549910545349121,
            5.503359794616699,
            -1.5472023487091064,
            -2.750977039337158,
            -3.9938812255859375,
            0.2418062388896942,
            -0.26962071657180786,
            1.1586313247680664,
            1.18120276927948,
            1.9542253017425537,
            -2.58897066116333,
            1.0336792469024658,
            -1.7564095258712769,
            -2.2297921180725098
        ]
    },
    "authors": [
        {
            "authorId": "2317012538",
            "name": "Wazir Ali"
        },
        {
            "authorId": "1708916",
            "name": "S. Pyysalo"
        }
    ],
    "references": [
        {
            "paperId": "1d4c48335d841014d0145256c3c4e7f6c426b8fb",
            "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models"
        },
        {
            "paperId": "53a803388e83ae89261624099d7be4287ace67cb",
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
        },
        {
            "paperId": "6002f0d76189ad99ed3cb2d9945e8be869b96244",
            "title": "Understanding LLMs Requires More Than Statistical Generalization"
        },
        {
            "paperId": "0b5439b6c22f4e48ed34cce1409057a358033c81",
            "title": "A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law"
        },
        {
            "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
            "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
        },
        {
            "paperId": "ab8d436e1792907294cb7707c94b3eb66ec0da17",
            "title": "Poro 34B and the Blessing of Multilinguality"
        },
        {
            "paperId": "6fd5dbea7588ee6bca703aa3fea9a487006dba29",
            "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order"
        },
        {
            "paperId": "428600c0586bd017199690277a1ba1ce8ffe351b",
            "title": "A New Massive Multilingual Dataset for High-Performance Language Technologies"
        },
        {
            "paperId": "5aa4e4d90cac81f8ec7001ae25356d75f02efbb1",
            "title": "Datasets for Large Language Models: A Comprehensive Survey"
        },
        {
            "paperId": "5e20c344594321a80d5dde9f6c3015ebb6c121e7",
            "title": "Gl\u00f3rIA: A Generative and Open Large Language Model for Portuguese"
        },
        {
            "paperId": "d931011e7a9baaa26c2385141aba38615c3edcac",
            "title": "Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish"
        },
        {
            "paperId": "a1f76db91c0debcf93ae9889736bce8470902113",
            "title": "Large Language Models: A Survey"
        },
        {
            "paperId": "341103678d9def1add23ad12a28e31a985ea50cd",
            "title": "CroissantLLM: A Truly Bilingual French-English Language Model"
        },
        {
            "paperId": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
            "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"
        },
        {
            "paperId": "411114f989a3d1083d90afd265103132fee94ebe",
            "title": "Mixtral of Experts"
        },
        {
            "paperId": "c6cb23bb92a9b6a36614a153efb24decaa146e83",
            "title": "Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task"
        },
        {
            "paperId": "f7c03462d669e3991bbdee8e28abccf0dccbcd1f",
            "title": "Language Resources for Dutch Large Language Modelling"
        },
        {
            "paperId": "cf1fd110df9fec1d820032a5264514d88a56b06b",
            "title": "LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language"
        },
        {
            "paperId": "2c0312c604f9f7638bb4533b39e0ae81e7f6ab12",
            "title": "The Falcon Series of Open Language Models"
        },
        {
            "paperId": "40f6a33f2e540aada8bc11eab1419bda84a39abf",
            "title": "GreekT5: A Series of Greek Sequence-to-Sequence Models for News Summarization"
        },
        {
            "paperId": "1a849f298dcd49fe99ebca6749c0564585aa3018",
            "title": "FinGPT: Large Generative Models for a Small Language"
        },
        {
            "paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
            "title": "Mistral 7B"
        },
        {
            "paperId": "1f0903b12d50a7b71abe9b5001a2b494c4597143",
            "title": "BERT models for Brazilian Portuguese: Pretraining, evaluation and tokenization analysis"
        },
        {
            "paperId": "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b",
            "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages"
        },
        {
            "paperId": "420daaec9d0e3bbf3c4d945e0c63ca5aa217373b",
            "title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset"
        },
        {
            "paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47",
            "title": "Explainability for Large Language Models: A Survey"
        },
        {
            "paperId": "bde98ab6970961b6a1bde2ab4c5ce83f7060b3ef",
            "title": "Cabrita: closing the gap for foreign languages"
        },
        {
            "paperId": "79926aa63d4daee6af06a8e9a7c2480b31cb7ed9",
            "title": "Spanish Pre-trained BERT Model and Evaluation Data"
        },
        {
            "paperId": "a7ff4d1a89baa5007b3c9ee46492aaf88dfc257f",
            "title": "Camoscio: an Italian Instruction-tuned LLaMA"
        },
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
            "paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729",
            "title": "A Comprehensive Overview of Large Language Models"
        },
        {
            "paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf",
            "title": "A Survey on Evaluation of Large Language Models"
        },
        {
            "paperId": "b69969bd91206a9f8f3d474df20caa500904143d",
            "title": "Fauno: The Italian Large Language Model that will leave you senza parole!"
        },
        {
            "paperId": "b6d6c33298b852cf63edac233deca70530d69a2a",
            "title": "PaLM 2 Technical Report"
        },
        {
            "paperId": "176e8c49c32f4c219857a1853998770927a598c6",
            "title": "SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish"
        },
        {
            "paperId": "736c0a291a09b0b4537ca3493361604001c49085",
            "title": "Domain Adaptation Speech-to-Text for Low-Resource European Portuguese Using Deep Learning"
        },
        {
            "paperId": "c30a4fb51684a8f4b23d899c9fc8c99653d3b0aa",
            "title": "GreekBART: The First Pretrained Greek Sequence-to-Sequence Model"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6",
            "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "aa4c05b46d5813b5e46bdea3c1531c02a3302afd",
            "title": "BART-IT: An Efficient Sequence-to-Sequence Model for Italian Text Summarization"
        },
        {
            "paperId": "7c6fcb3f1577b2385342d054b94df7924a5cdd13",
            "title": "ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information"
        },
        {
            "paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
        },
        {
            "paperId": "1bb6d5761903c7ac978188ae36e2648905e95dc5",
            "title": "Transcending Scaling Laws with 0.1% Extra Compute"
        },
        {
            "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
            "title": "GLM-130B: An Open Bilingual Pre-trained Model"
        },
        {
            "paperId": "914254fac74a2da051cccf6ca16afcaad416a079",
            "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"
        },
        {
            "paperId": "00d9a73c54053a32e2aba92b53fc3e6bc71d3238",
            "title": "Sequence-to-sequence pretraining for a less-resourced Slovenian language"
        },
        {
            "paperId": "99be6dc17a7fd399f4af80c4c1cd7ee5247591a1",
            "title": "Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese"
        },
        {
            "paperId": "267dcb61f72f48dadd60a3a770493c0c9f70be65",
            "title": "Evaluation of Transfer Learning for Polish with a Text-to-Text Model"
        },
        {
            "paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8",
            "title": "UL2: Unifying Language Learning Paradigms"
        },
        {
            "paperId": "95a7ab38505b328325195a6f412f61ce3cf173dc",
            "title": "A Lite Romanian BERT: ALR-BERT"
        },
        {
            "paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
            "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
            "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1",
            "title": "Training Compute-Optimal Large Language Models"
        },
        {
            "paperId": "55b9a2ade0a49e9cf10b71528d69dfee4e826025",
            "title": "Cedille: A large autoregressive French language model"
        },
        {
            "paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19",
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
        },
        {
            "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
            "title": "LaMDA: Language Models for Dialog Applications"
        },
        {
            "paperId": "645a317c9305207e95d03b5756a65e7e850f32d5",
            "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus"
        },
        {
            "paperId": "a3184d40d390793232c99c89b57b8f65c16320b2",
            "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"
        },
        {
            "paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14",
            "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"
        },
        {
            "paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
            "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"
        },
        {
            "paperId": "3e95ca099185478fc81b55cb491e03284f60c664",
            "title": "RoGPT2: Romanian GPT2 for Text Generation"
        },
        {
            "paperId": "92bc69500c16fce47bcfd06ada14ffc4a7e8ddca",
            "title": "PAGnol: An Extra-Large French Generative Model"
        },
        {
            "paperId": "09ed8c5a4b9b9e516c61415ce99ccd9b3170a1f7",
            "title": "SlovakBERT: Slovak Masked Language Model"
        },
        {
            "paperId": "21230f72afae4f19f59c4ce9c099075e7ecfe77c",
            "title": "gaBERT \u2014 an Irish Language Model"
        },
        {
            "paperId": "2132eac5628bc200de226b51f1dfb82423ff1d24",
            "title": "MarIA: Spanish Language Models"
        },
        {
            "paperId": "319b84be7a843250bc81d7086f79a4126d550277",
            "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"
        },
        {
            "paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d",
            "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"
        },
        {
            "paperId": "0934952763f1549e75b142261e73f2b27a2f495b",
            "title": "RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model"
        },
        {
            "paperId": "dc70b180329a6ffead5e48093fb5a551955047c4",
            "title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish"
        },
        {
            "paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f",
            "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"
        },
        {
            "paperId": "8985c0f900dfa9bf1ab6bcda594195f6c754dc94",
            "title": "Czert \u2013 Czech BERT-like Model for Language Representation"
        },
        {
            "paperId": "fdacf2a732f55befdc410ea927091cad3b791f13",
            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
        },
        {
            "paperId": "f0520b991c1a16449a95ff98771d3cf86ed71428",
            "title": "GottBERT: a pure German Language Model"
        },
        {
            "paperId": "218ddb2d8182d197dd2cff40220179d7867c3589",
            "title": "RoBERT \u2013 A Romanian BERT Model"
        },
        {
            "paperId": "03221b7d65cd8437a55aa571d23b96a806ba1511",
            "title": "EstBERT: A Pretrained Language-Specific BERT for Estonian"
        },
        {
            "paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a",
            "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"
        },
        {
            "paperId": "61f42619eca0afaf2cff9a899bbdea88182885f8",
            "title": "German\u2019s Next Language Model"
        },
        {
            "paperId": "d5f0cf6f006f954316ce43aa945d6786808e8715",
            "title": "LVBERT: Transformer-Based Model for Latvian Language Understanding"
        },
        {
            "paperId": "0bd100685a492501d3f5f82a383edb3b5ded0f06",
            "title": "GREEK-BERT: The Greeks visiting Sesame Street"
        },
        {
            "paperId": "214a0eede75f546b631d6d28871bd9028a66fc46",
            "title": "Playing with Words at the National Library of Sweden - Making a Swedish BERT"
        },
        {
            "paperId": "1882f194cb43828852cc052887671e55a80f945a",
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
        },
        {
            "paperId": "e34fdd75a78d08e1ff901ed466e6b1aaf4915750",
            "title": "FlauBERT : des mod\u00e8les de langue contextualis\u00e9s pr\u00e9-entra\u00een\u00e9s pour le fran\u00e7ais (FlauBERT : Unsupervised Language Model Pre-training for French)"
        },
        {
            "paperId": "82453548b97f78ab2cdb9a8626ff858db9ce5a82",
            "title": "Pre-training Polish Transformer-based Language Models at Scale"
        },
        {
            "paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583",
            "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
        },
        {
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners"
        },
        {
            "paperId": "b0079a39983f9fd85bdd74679121b86a232f8ab5",
            "title": "Towards an Open Platform for Legal Information"
        },
        {
            "paperId": "73cb8b7a02d1b9c9f3e5c21d25752cb4115fa83d",
            "title": "The Danish Gigaword Corpus"
        },
        {
            "paperId": "6866cbd623b34985debbde08046d51b8ccc0e620",
            "title": "Gigafida 2.0: The Reference Corpus of Written Standard Slovene"
        },
        {
            "paperId": "9f051d215c3330f8068bb834f7b48792f796a672",
            "title": "The siParl corpus of Slovene parliamentary proceedings"
        },
        {
            "paperId": "11c73c1bd2c2424e7c7b9aa2944e1d3a3f579ffb",
            "title": "GePpeTto Carves Italian into a Language Model"
        },
        {
            "paperId": "634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b",
            "title": "RobBERT: a Dutch RoBERTa-based Language Model"
        },
        {
            "paperId": "477d66dcd2c08243dcc69822d6da7ec06393773a",
            "title": "Multilingual is not enough: BERT for Finnish"
        },
        {
            "paperId": "b61c6405f4de381758e8b52a20313554d68a9d85",
            "title": "CamemBERT: a Tasty French Language Model"
        },
        {
            "paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c",
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"
        },
        {
            "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
            "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
            "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
            "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
        },
        {
            "paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8",
            "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"
        },
        {
            "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
            "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
            "paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88",
            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"
        },
        {
            "paperId": "145b8b5d99a2beba6029418ca043585b90138d12",
            "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"
        },
        {
            "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
            "title": "Cross-lingual Language Model Pretraining"
        },
        {
            "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
            "paperId": "8c207ece66e0a63627869c49fb37c6811072539b",
            "title": "The brWaC Corpus: A New Open Resource for Brazilian Portuguese"
        },
        {
            "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
            "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
            "paperId": "ac79361b8c129ab8acc954a49c50a30f5ed01f37",
            "title": "emLam - a Hungarian Language Modeling baseline"
        },
        {
            "paperId": "a1474df97f5da895befe5f21e3a482ee5b605041",
            "title": "SYN v4: large corpus of written Czech"
        },
        {
            "paperId": "e11edb4201007530c3692814a155b22f78a0d659",
            "title": "OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles"
        },
        {
            "paperId": "dc66194b0b977aeabafa2f37f7fa8a455a6f2642",
            "title": "Towards Universal Web Parsebanks"
        },
        {
            "paperId": "7bb79a8c174a17a7d494478f81147212720ca387",
            "title": "The Hungarian Gigaword Corpus"
        },
        {
            "paperId": "9d748af3521b51648a91a7e807db4d5270d2e240",
            "title": "Automatic Speech Recognition: A Survey"
        },
        {
            "paperId": "1b560f892432fb853d233c92f9294640bc91de3c",
            "title": "Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages"
        },
        {
            "paperId": "773d8c08d0a3733e39002522dc37404f7fc87508",
            "title": "hrWaC and slWac: Compiling Web Corpora for Croatian and Slovene"
        },
        {
            "paperId": "8ec7a7b28f5237fa429e380f266f33548f3cff59",
            "title": "The WaCky wide web: a collection of very large linguistically processed web-crawled corpora"
        },
        {
            "paperId": "47af0f93948efac676fcde8150c337578352c7ca",
            "title": "Evaluation"
        },
        {
            "paperId": "0303e0216078f298cce8ec1681e3dd0c446774ee",
            "title": "Introduction to Modern Information Retrieval (2nd edition)"
        },
        {
            "paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "title": "A Neural Probabilistic Language Model"
        },
        {
            "paperId": "8f617510cfe7f8ba24b238f43b135aca60c1b7b1",
            "title": "Relevance weighting for combining multi-domain data for n-gram language modeling"
        },
        {
            "paperId": "c54ab91fd71ed5061b750cd7064d7d6973fa0ad8",
            "title": "Transformer-Based Language Models for Bulgarian"
        },
        {
            "paperId": "8a930572177545e7394ba5cd03e9342142da564e",
            "title": "Better Quality Pre-training Data and T5 Models for African Languages"
        },
        {
            "paperId": "11905e54f33e1959de44a8895872a2d8c9e7ef9f",
            "title": "BERTabaporu: Assessing a Genre-Specific Language Model for Portuguese NLP"
        },
        {
            "paperId": "bcfebfda58753c7bcc293def2c67b49b59d9d6e1",
            "title": "Proceedings of the 9th Italian Conference on Computational Linguistics, Venice, Italy, November 30 - December 2, 2023"
        },
        {
            "paperId": "f4b82e4f2746b1332ba4eb09427df1becf7e0fa6",
            "title": "Mono- and Multilingual GPT-3 Models for Hungarian"
        },
        {
            "paperId": "a3837000307940d66b28b617735c59aea585c360",
            "title": "Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish"
        },
        {
            "paperId": "f0b8389a9f9f0630255bf623eeb0749c444e4b39",
            "title": "SloBERTa: Slovene monolingual large pretrained masked language model"
        },
        {
            "paperId": null,
            "title": "Jurassic-1: Technical details and evaluation. White Paper"
        },
        {
            "paperId": null,
            "title": "Natural language processing methods for language modeling. PhD thesis"
        },
        {
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
            "paperId": null,
            "title": "Kallas: Estional National Corpus"
        },
        {
            "paperId": "5d9f4e04e1e966812b37a4a5dfaca1072d0c29ea",
            "title": "csTenTen17, a Recent Czech Web Corpus"
        },
        {
            "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
            "paperId": "1117d2e195f215b852af9f44690e5397bf6e4e20",
            "title": "OPUS \u2013 parallel corpora for everyone"
        },
        {
            "paperId": null,
            "title": "(cid:32)Lazi\u00b4nski"
        },
        {
            "paperId": null,
            "title": "Czes: LINDAT/CLARIAH-CZ. digital library at the Institute of Formal and Applied Linguistics (\u00b4UFAL), Faculty"
        },
        {
            "paperId": null,
            "title": "W2c\u2013web to corpus"
        },
        {
            "paperId": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "title": "Recurrent neural network based language model"
        },
        {
            "paperId": "694b3c58712deefb59502847ba1b52b192c413e5",
            "title": "Europarl: A Parallel Corpus for Statistical Machine Translation"
        },
        {
            "paperId": "a15ee7533f1afe363982a49bc8e8effe9fc22d88",
            "title": "Archiving the Greek Web"
        },
        {
            "paperId": "6ff7417332eb1af6d2b3383e1009d37986e32be2",
            "title": "Modern Information Retrieval : A Brief Overview"
        },
        {
            "paperId": "c1e3f2d537e50e0d5263e4731ab6c7983acd6687",
            "title": "Prediction and Entropy of Printed English"
        },
        {
            "paperId": null,
            "title": "ELECTRA: pre-training text encoders as discriminators rather than generators"
        },
        {
            "paperId": null,
            "title": "GPT Neo 1.3B pre-trained on cleaned Dutch mC4"
        },
        {
            "paperId": null,
            "title": "Rotex"
        },
        {
            "paperId": null,
            "title": "proceedings.neurips.cc/paper/2019/hash"
        },
        {
            "paperId": null,
            "title": "Gpt-2 large for finnish"
        },
        {
            "paperId": null,
            "title": "Awesome NLP Resources for Hungarian"
        },
        {
            "paperId": null,
            "title": "AI@Meta: Llama 3 model card"
        },
        {
            "paperId": null,
            "title": "Robbert-2023: Keeping dutch language models up-to-date at a lower cost thanks to model conversion"
        },
        {
            "paperId": null,
            "title": "A generative pre-trained transformer model for finnish (2022)"
        },
        {
            "paperId": null,
            "title": "Gemini: A family of highly capable multimodal models"
        },
        {
            "paperId": null,
            "title": "LEOLM: IGNITING GERMAN-LANGUAGE"
        }
    ]
}