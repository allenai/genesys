{
    "paperId": "d40631a850c21607a5b1cb63efc7bf4ba1ab1fe0",
    "externalIds": {
        "ArXiv": "2408.08632",
        "CorpusId": 271892136
    },
    "title": "A Survey on Benchmarks of Multimodal Large Language Models",
    "abstract": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to support the development of MLLMs better. For more details, please visit our GitHub repository: https://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.",
    "venue": "",
    "year": 2024,
    "referenceCount": 148,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "openAccessPdf": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on perception and understanding, recognition and reasoning, specific domains, key capabilities, and other modalities, concludes that evaluation should be regarded as a crucial discipline to support the development of MLLMs better."
    },
    "embedding": {
        "model": "specter_v1",
        "vector": [
            -3.3794631958007812,
            -0.44045233726501465,
            -1.3598933219909668,
            4.026671409606934,
            -0.31761014461517334,
            1.7198249101638794,
            5.1345295906066895,
            2.25044584274292,
            -2.5319957733154297,
            1.5195422172546387,
            -4.30142879486084,
            3.958167552947998,
            -3.4664645195007324,
            2.44736647605896,
            -4.340088844299316,
            -1.7591477632522583,
            0.11311888694763184,
            -1.2016613483428955,
            6.800061225891113,
            -0.8752664923667908,
            -0.4112062454223633,
            2.73122239112854,
            -1.350466012954712,
            -1.1835006475448608,
            -1.2614772319793701,
            -2.335782766342163,
            3.937492609024048,
            4.381120681762695,
            -0.8733752369880676,
            2.9605047702789307,
            -0.5494898557662964,
            -4.5680413246154785,
            2.895265817642212,
            -2.6240313053131104,
            0.9812508821487427,
            -2.577218770980835,
            -2.3105971813201904,
            3.1957645416259766,
            -0.7105881571769714,
            -1.5157532691955566,
            0.5677075386047363,
            0.7878932952880859,
            3.0566983222961426,
            0.722557544708252,
            0.794548511505127,
            0.9174221754074097,
            -0.886091947555542,
            -0.011301904916763306,
            -0.6711389422416687,
            1.9651196002960205,
            1.490305781364441,
            -0.437921941280365,
            2.0011091232299805,
            2.5893659591674805,
            -2.4712862968444824,
            0.48310586810112,
            1.696098804473877,
            3.065545082092285,
            2.541048288345337,
            -0.08411026000976562,
            5.6479997634887695,
            5.842583179473877,
            -0.4230164885520935,
            2.457278251647949,
            2.8066606521606445,
            -0.43876588344573975,
            -2.3715476989746094,
            2.229616641998291,
            1.1624611616134644,
            1.1753571033477783,
            0.7195627093315125,
            -4.399179935455322,
            0.5344334244728088,
            2.735177993774414,
            -1.879511833190918,
            -0.4945637881755829,
            0.1333189308643341,
            -5.846898078918457,
            -0.758330762386322,
            -0.2947593629360199,
            -1.1519129276275635,
            2.6363890171051025,
            0.9606385231018066,
            3.298999786376953,
            4.9322919845581055,
            0.06521294265985489,
            -5.292146682739258,
            2.0963308811187744,
            1.2409383058547974,
            -4.6268510818481445,
            -2.331505537033081,
            -1.3961671590805054,
            -0.5798165202140808,
            1.9463191032409668,
            -1.813078761100769,
            -0.35401925444602966,
            -0.09037065505981445,
            -3.095552682876587,
            -4.041522979736328,
            -3.090320587158203,
            1.0461485385894775,
            -1.0247443914413452,
            -0.9850382804870605,
            3.617480754852295,
            5.305499076843262,
            -5.051671504974365,
            2.630380630493164,
            0.29446011781692505,
            2.0119338035583496,
            -0.3799734115600586,
            -3.601893663406372,
            3.7454919815063477,
            -2.4557409286499023,
            -1.4702528715133667,
            1.2967716455459595,
            -0.6621261835098267,
            -1.4683794975280762,
            4.115647792816162,
            -2.920236110687256,
            7.273036479949951,
            -1.6598966121673584,
            -1.0375354290008545,
            -2.691685676574707,
            0.615325927734375,
            4.991539001464844,
            1.9109148979187012,
            1.7739207744598389,
            0.14914071559906006,
            0.8805644512176514,
            -4.020758628845215,
            0.638867199420929,
            0.6701064109802246,
            4.371647834777832,
            -2.86210298538208,
            1.5151276588439941,
            3.6006827354431152,
            -3.5418624877929688,
            0.7774155139923096,
            0.31354260444641113,
            -0.4243299663066864,
            2.234529972076416,
            5.3736982345581055,
            -0.5863789319992065,
            -0.6852269768714905,
            1.133813500404358,
            -0.03679513931274414,
            1.5975903272628784,
            -0.0826725959777832,
            2.2334232330322266,
            4.830522537231445,
            2.45280122756958,
            -5.184442043304443,
            -2.116105318069458,
            0.13448446989059448,
            0.17537373304367065,
            4.521871566772461,
            -3.1734988689422607,
            1.7449276447296143,
            -1.8353281021118164,
            0.426980584859848,
            0.4919934868812561,
            2.369584560394287,
            -11.905487060546875,
            -1.8168747425079346,
            -1.791980266571045,
            -5.746528148651123,
            -0.6731105446815491,
            2.921194553375244,
            0.07180790603160858,
            -0.24896830320358276,
            -2.3035054206848145,
            2.2741894721984863,
            1.6368530988693237,
            5.5465898513793945,
            -1.5186880826950073,
            1.3941601514816284,
            0.09828805923461914,
            -2.126053810119629,
            -1.6828662157058716,
            0.8119255304336548,
            -1.5415122509002686,
            -0.8201354146003723,
            -7.089664459228516,
            0.8179471492767334,
            -1.2470930814743042,
            -1.038261890411377,
            0.3342759907245636,
            -1.1715024709701538,
            0.8014634847640991,
            -2.298171043395996,
            -1.237293004989624,
            0.7728254795074463,
            7.037281036376953,
            7.2707414627075195,
            3.72501277923584,
            2.4020814895629883,
            1.577582597732544,
            1.2819465398788452,
            -3.479522466659546,
            1.079190731048584,
            2.4703707695007324,
            -1.153123140335083,
            -2.023005247116089,
            -4.269301414489746,
            5.31770133972168,
            5.106190204620361,
            -4.214530944824219,
            2.7749111652374268,
            3.2701547145843506,
            0.6479036808013916,
            1.7086329460144043,
            1.297893762588501,
            -2.0977437496185303,
            2.5647449493408203,
            -0.37752774357795715,
            -1.8207460641860962,
            -4.698611736297607,
            1.240429162979126,
            1.8365569114685059,
            3.5486743450164795,
            -1.4698779582977295,
            1.7823818922042847,
            -0.3775976300239563,
            -4.342185020446777,
            3.626150131225586,
            -4.358119964599609,
            0.25787049531936646,
            0.8525273203849792,
            0.34633973240852356,
            -0.7814611196517944,
            -2.178734540939331,
            -4.740918159484863,
            -2.2727952003479004,
            -2.631638526916504,
            -6.606564521789551,
            1.2891592979431152,
            -3.5735461711883545,
            1.6739897727966309,
            -0.9741421937942505,
            -0.3310050964355469,
            2.592003345489502,
            3.377351760864258,
            1.9095561504364014,
            4.100313186645508,
            -0.44704148173332214,
            -1.586953043937683,
            -1.9078426361083984,
            1.970629096031189,
            0.7076225876808167,
            -0.5415698885917664,
            -0.8946104049682617,
            -0.7387795448303223,
            3.425542116165161,
            -1.7682803869247437,
            -1.3119275569915771,
            -1.3473882675170898,
            1.3110580444335938,
            1.3707101345062256,
            -0.9149916768074036,
            0.7264203429222107,
            3.615938901901245,
            5.368942737579346,
            3.624094009399414,
            2.089942693710327,
            -0.8407201170921326,
            0.279246985912323,
            -3.03853702545166,
            -0.14176011085510254,
            -3.4082045555114746,
            4.370631217956543,
            3.968733787536621,
            0.947878360748291,
            2.508171558380127,
            -1.9125803709030151,
            -3.577058792114258,
            -5.969241619110107,
            -2.9330763816833496,
            -4.2427568435668945,
            2.7110118865966797,
            -0.6979964971542358,
            1.1571344137191772,
            -0.13329538702964783,
            0.2754669785499573,
            -1.0092318058013916,
            1.9446160793304443,
            -3.8601174354553223,
            -3.0038094520568848,
            -0.22714370489120483,
            0.550706684589386,
            -6.009176254272461,
            -0.7604582905769348,
            2.6601107120513916,
            -2.9052987098693848,
            -0.4826047420501709,
            -1.763303518295288,
            -0.6149434447288513,
            3.748452663421631,
            -1.5438259840011597,
            -1.403165340423584,
            -1.002392292022705,
            -0.48600202798843384,
            3.584764242172241,
            4.621804714202881,
            -1.2368589639663696,
            0.8108179569244385,
            4.035522937774658,
            0.5929384231567383,
            -0.9412686824798584,
            0.9825711250305176,
            -6.209930419921875,
            -0.8740811347961426,
            0.775917649269104,
            3.237122058868408,
            -5.092397212982178,
            -0.9953039884567261,
            1.4572045803070068,
            5.22245454788208,
            0.46932724118232727,
            -2.3790321350097656,
            0.4431752860546112,
            1.7622690200805664,
            -1.8248947858810425,
            -4.914396286010742,
            -3.434462070465088,
            -4.687287330627441,
            0.4865463376045227,
            1.1975919008255005,
            2.2306528091430664,
            -4.096010684967041,
            -1.2124097347259521,
            3.3106653690338135,
            5.237990856170654,
            1.6138558387756348,
            1.6375057697296143,
            -1.6030237674713135,
            -3.381140947341919,
            -0.3994826078414917,
            -2.7933313846588135,
            1.9932674169540405,
            -1.5329005718231201,
            0.7982100248336792,
            5.305103302001953,
            -1.7582018375396729,
            -1.529669165611267,
            1.6189615726470947,
            1.8494763374328613,
            2.0506038665771484,
            -3.2348525524139404,
            -1.1368415355682373,
            -3.578913927078247,
            1.0198228359222412,
            1.3452943563461304,
            2.547905921936035,
            -3.3220813274383545,
            2.4324772357940674,
            0.7067961692810059,
            0.4375835955142975,
            -2.2171928882598877,
            1.363809585571289,
            1.0082550048828125,
            -3.841681480407715,
            1.5235092639923096,
            3.6129066944122314,
            2.79634952545166,
            -2.8692755699157715,
            -1.2101333141326904,
            10.388455390930176,
            -5.231371879577637,
            1.5684479475021362,
            -7.072441577911377,
            -1.0382156372070312,
            -5.217753887176514,
            -5.727433204650879,
            4.340321063995361,
            -2.467890977859497,
            -4.6973958015441895,
            -5.4036407470703125,
            -7.096053600311279,
            2.607818603515625,
            3.0063562393188477,
            -2.424992084503174,
            4.201102256774902,
            -0.4299488961696625,
            0.7810521721839905,
            2.751279354095459,
            3.9092392921447754,
            -2.696157693862915,
            0.9173357486724854,
            0.44978904724121094,
            -0.5215146541595459,
            -0.3035203516483307,
            1.139215350151062,
            -1.6814064979553223,
            0.41118329763412476,
            -0.1724083423614502,
            -1.0441722869873047,
            -2.309751033782959,
            -2.1351587772369385,
            2.237057685852051,
            2.5062942504882812,
            2.502798557281494,
            -0.2445831596851349,
            4.0384721755981445,
            2.6582698822021484,
            -2.8958916664123535,
            -0.18344274163246155,
            0.43762868642807007,
            1.7572261095046997,
            -2.657989978790283,
            -0.21070677042007446,
            -2.975060224533081,
            -2.789757251739502,
            -0.7228520512580872,
            -3.868288278579712,
            -2.492791175842285,
            2.1259591579437256,
            -0.008235007524490356,
            1.119036316871643,
            0.280266135931015,
            -0.14300400018692017,
            -3.682157516479492,
            2.4543094635009766,
            4.291411399841309,
            3.672125816345215,
            0.0803859680891037,
            2.4197354316711426,
            0.47371625900268555,
            0.20145192742347717,
            -2.1594557762145996,
            3.1883115768432617,
            2.405052661895752,
            1.988185167312622,
            -1.2164618968963623,
            -5.035773277282715,
            1.1857131719589233,
            1.0443768501281738,
            0.976284384727478,
            3.1132450103759766,
            1.754302978515625,
            -3.6897215843200684,
            2.511298418045044,
            1.2379902601242065,
            -2.8455657958984375,
            3.383883476257324,
            0.7105318307876587,
            2.6513209342956543,
            2.035921096801758,
            -0.26109203696250916,
            -3.4678783416748047,
            -2.292868137359619,
            5.811021327972412,
            -2.0989928245544434,
            -2.529132843017578,
            -2.258561134338379,
            -0.31385505199432373,
            2.9686124324798584,
            -3.4723169803619385,
            2.515103816986084,
            0.9055162668228149,
            0.20004689693450928,
            -2.9348721504211426,
            -0.07240676879882812,
            0.0997561514377594,
            4.313137531280518,
            0.2137729525566101,
            0.1053299605846405,
            -1.5582466125488281,
            -4.331586837768555,
            -3.827484369277954,
            -0.3876429796218872,
            2.568666934967041,
            1.8379915952682495,
            -1.8881821632385254,
            1.0356627702713013,
            0.3656311631202698,
            1.7347301244735718,
            3.6351685523986816,
            -1.0889133214950562,
            1.2049744129180908,
            -2.0949368476867676,
            -1.9635601043701172,
            2.4879913330078125,
            2.648951768875122,
            -3.649664878845215,
            -2.54705548286438,
            4.193225860595703,
            1.9155323505401611,
            2.0031490325927734,
            3.3534505367279053,
            -0.5059228539466858,
            0.11202344298362732,
            3.858583450317383,
            2.028423309326172,
            1.3564624786376953,
            -2.2704360485076904,
            -2.690763473510742,
            -8.56174087524414,
            2.1579439640045166,
            -1.0806994438171387,
            0.4016801714897156,
            2.656437635421753,
            -5.23227596282959,
            1.113591194152832,
            0.31781893968582153,
            -0.752717137336731,
            7.928325653076172,
            4.343770980834961,
            -0.7283550500869751,
            0.664527177810669,
            0.8233004212379456,
            0.3819364309310913,
            0.7517287731170654,
            -3.94331693649292,
            4.541553020477295,
            -2.3347363471984863,
            -0.08494493365287781,
            3.4512665271759033,
            -1.7837989330291748,
            -1.1157457828521729,
            2.873044729232788,
            -0.860501766204834,
            -1.543057918548584,
            -2.167171001434326,
            1.879561185836792,
            1.856450080871582,
            -0.22153803706169128,
            -2.938419818878174,
            0.14126849174499512,
            -0.6596749424934387,
            4.899920463562012,
            3.8547043800354004,
            4.68646240234375,
            3.6513118743896484,
            -0.04552370309829712,
            0.4308345913887024,
            -4.063355445861816,
            0.47116637229919434,
            4.21905517578125,
            -2.0304558277130127,
            -3.152086019515991,
            -0.792304277420044,
            -0.9496897459030151,
            0.8274838328361511,
            5.90886116027832,
            2.511847734451294,
            4.1195831298828125,
            -5.209235668182373,
            0.853290855884552,
            -0.16687940061092377,
            0.32229673862457275,
            2.0615198612213135,
            -1.4759453535079956,
            3.1625328063964844,
            2.581662654876709,
            -2.7291653156280518,
            0.3038688600063324,
            -0.8425912857055664,
            -0.15701434016227722,
            -0.21900561451911926,
            2.375406265258789,
            1.8754444122314453,
            -1.2871867418289185,
            0.21346637606620789,
            4.668682098388672,
            -1.9879488945007324,
            1.334312915802002,
            -2.4905529022216797,
            3.0250282287597656,
            0.502164363861084,
            -1.8708808422088623,
            -0.0050566792488098145,
            -4.8939666748046875,
            0.4986642003059387,
            0.1480960100889206,
            -0.44012391567230225,
            4.776968955993652,
            0.6233633756637573,
            -0.985754132270813,
            -0.8448258638381958,
            -1.9904985427856445,
            -2.1649739742279053,
            0.42244952917099,
            0.29733389616012573,
            -0.7734633684158325,
            -0.42870596051216125,
            1.4497361183166504,
            -2.317577838897705,
            2.299842357635498,
            -3.814903736114502,
            0.919687807559967,
            2.282813310623169,
            -2.077288866043091,
            0.013297617435455322,
            -3.5894393920898438,
            -0.1041836142539978,
            -5.865634918212891,
            2.5907459259033203,
            5.819075584411621,
            -0.9583211541175842,
            2.0419042110443115,
            -0.488089382648468,
            4.633049488067627,
            3.1626596450805664,
            0.4857211709022522,
            -2.557826042175293,
            -2.0112996101379395,
            2.998300075531006,
            -1.938378095626831,
            -0.39267078042030334,
            -0.23692023754119873,
            -1.1702864170074463,
            2.329374313354492,
            15.994919776916504,
            3.7252864837646484,
            -0.5641065835952759,
            0.74637770652771,
            -0.90938401222229,
            -2.4531502723693848,
            0.011528998613357544,
            0.6155107021331787,
            -3.3356502056121826,
            0.30558907985687256,
            -2.5805392265319824,
            -3.3499059677124023,
            -2.7693071365356445,
            4.930980682373047,
            -2.0113980770111084,
            0.3848377466201782,
            -1.846528172492981,
            0.21013933420181274,
            0.5783119201660156,
            0.24833565950393677,
            0.8439284563064575,
            3.606747627258301,
            -4.223891258239746,
            -3.2814266681671143,
            -0.7152752876281738,
            0.9071837663650513,
            2.6095001697540283,
            2.2174127101898193,
            -5.012897968292236,
            3.2310452461242676,
            -0.6907808184623718,
            2.4716219902038574,
            0.9227404594421387,
            -1.567201018333435,
            -3.805811882019043,
            2.305096387863159,
            3.8324480056762695,
            -2.0115180015563965,
            -1.4749133586883545,
            0.1278257668018341,
            -1.0711288452148438,
            0.40252748131752014,
            -6.555981159210205,
            3.800992250442505,
            -2.746246337890625,
            0.5437283515930176,
            2.733992576599121,
            -1.9095029830932617,
            -0.33000341057777405,
            5.720948696136475,
            -1.8852238655090332,
            -2.593104600906372,
            0.09501475095748901,
            -0.6382977962493896,
            2.893259286880493,
            -2.477213144302368,
            -0.35038602352142334,
            -2.7362940311431885,
            2.429767370223999,
            -1.9547052383422852,
            2.068282127380371,
            1.3317716121673584,
            -1.100144386291504,
            -2.7293944358825684,
            -2.5822975635528564,
            -0.6114798188209534,
            -1.73429274559021,
            3.29297137260437,
            1.5075069665908813,
            1.4604626893997192,
            3.8052706718444824,
            0.8084156513214111,
            -2.0010855197906494,
            0.39225924015045166,
            -0.4964374303817749,
            -4.851527214050293,
            1.047629475593567,
            -0.2000449150800705,
            -0.05934309959411621,
            4.42795467376709,
            -1.5193639993667603,
            5.523590564727783,
            -1.0537471771240234,
            -0.7287639379501343,
            2.8764760494232178,
            -2.67281436920166,
            3.3595130443573,
            0.2682037055492401,
            -0.32946962118148804,
            3.230562686920166,
            0.50155109167099,
            -1.4440357685089111,
            4.868259429931641,
            2.4316771030426025,
            0.12595224380493164,
            -3.9753963947296143,
            -3.689150810241699,
            0.019146740436553955,
            -4.37335205078125,
            -2.5677475929260254,
            5.818999767303467,
            2.5085740089416504,
            1.9956657886505127,
            -3.942474603652954,
            1.4594625234603882,
            -0.45460841059684753,
            0.3549416661262512,
            -6.661937713623047,
            -1.587121844291687,
            -1.8630638122558594,
            4.049493789672852,
            -1.9671683311462402,
            -1.554513692855835,
            -1.5386204719543457,
            0.8914459943771362,
            -1.4900604486465454,
            2.797936201095581,
            -0.4211823344230652,
            0.7196969985961914,
            1.211392879486084,
            0.09126392006874084,
            -0.5722165107727051,
            0.19339919090270996,
            -2.536036968231201,
            -0.9836728572845459,
            -1.176969051361084,
            -0.49498724937438965,
            -0.9193507432937622,
            -2.3637351989746094,
            -1.0910847187042236,
            3.4028820991516113,
            -2.421703815460205,
            -2.5412399768829346,
            1.6833341121673584,
            0.3381481170654297,
            -1.8030641078948975,
            1.1024543046951294,
            4.034907817840576,
            0.04911111295223236,
            4.564290523529053,
            0.83529132604599,
            -4.741545677185059,
            -1.1610589027404785,
            7.652559280395508,
            -2.516570568084717,
            -2.433476209640503,
            -4.474808692932129,
            -0.6333340406417847,
            -3.14860200881958,
            0.6772098541259766,
            2.008673667907715,
            0.9322382211685181,
            -0.11081324517726898,
            -0.225412517786026,
            -3.7860631942749023,
            -2.6537351608276367
        ]
    },
    "authors": [
        {
            "authorId": "2316588022",
            "name": "Jian Li"
        },
        {
            "authorId": "2316478695",
            "name": "Weiheng Lu"
        },
        {
            "authorId": "2320150406",
            "name": "Hao Fei"
        },
        {
            "authorId": "2320188215",
            "name": "Meng Luo"
        },
        {
            "authorId": "2320151098",
            "name": "Ming Dai"
        },
        {
            "authorId": "2320151388",
            "name": "Min Xia"
        },
        {
            "authorId": "2267492043",
            "name": "Yizhang Jin"
        },
        {
            "authorId": "2066402135",
            "name": "Zhenye Gan"
        },
        {
            "authorId": "2320150586",
            "name": "Ding Qi"
        },
        {
            "authorId": null,
            "name": "Chaoyou Fu"
        },
        {
            "authorId": "2320151117",
            "name": "Ying Tai"
        },
        {
            "authorId": "2320189104",
            "name": "Wankou Yang"
        },
        {
            "authorId": "2628601",
            "name": "Yabiao Wang"
        },
        {
            "authorId": "2302227792",
            "name": "Chengjie Wang"
        }
    ],
    "references": [
        {
            "paperId": "b1b62d324e2cae75cc75538ec50a86edfa60229c",
            "title": "Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models"
        },
        {
            "paperId": "b8560ff7b9bbb88163c6eb7125db8316e7cf0572",
            "title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models"
        },
        {
            "paperId": "de5e83068f04b9e8c3acb5180f78ce78f9970ed0",
            "title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images"
        },
        {
            "paperId": "c9b2bf3736edc4c6b160ecb92e7fcdcb0fb62ff7",
            "title": "LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description"
        },
        {
            "paperId": "13a03ba21fd6e3c23efe143da070e2d461007540",
            "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding"
        },
        {
            "paperId": "d97d7c977da22d94c7c82b57e5fa43d379c26ced",
            "title": "Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist"
        },
        {
            "paperId": "fd36c54ae4f81f1fca80c52125a14bee3a0b6711",
            "title": "ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities"
        },
        {
            "paperId": "02f41dc0b9dd7361aa90ff160952ba77fe9550cd",
            "title": "MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs"
        },
        {
            "paperId": "58ee9e1c426166a5451a1ce13e1186f7d6baacfd",
            "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models"
        },
        {
            "paperId": "d02b9420df66330620f8853d19de610c98d2e1c1",
            "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding"
        },
        {
            "paperId": "7c752d0a7b2cb33295b50efc7a03afb88bed77b5",
            "title": "GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs"
        },
        {
            "paperId": "879905229dd629570165ab92cfa4045d7de0cbe5",
            "title": "Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models"
        },
        {
            "paperId": "cdda2961164d5788079eeb94684969baa18b01c1",
            "title": "Seeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questions"
        },
        {
            "paperId": "c198ca6a79fc1ce5071a5d28b8b35d14060bc1a6",
            "title": "Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study"
        },
        {
            "paperId": "0abc653fe34ce293b6a2db3bd59341e6fd1b3dc6",
            "title": "Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models"
        },
        {
            "paperId": "de103054dfeec44c2643b5e82707a81aa3737375",
            "title": "Automated Multi-level Preference for MLLMs"
        },
        {
            "paperId": "efc50d7ee5bcd8039ba5d686d7d943be3ff199b0",
            "title": "Efficient Multimodal Large Language Models: A Survey"
        },
        {
            "paperId": "efbd5505812dab88b45ed6320ae4484c90d3c47a",
            "title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots"
        },
        {
            "paperId": "10d3c1cfb98f4e547f579a30b31f2361297983f1",
            "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI"
        },
        {
            "paperId": "b9a64f44a2de148a8db8aa6c91584b00d4717cf0",
            "title": "ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction"
        },
        {
            "paperId": "3a09a5046d9707bb53646329d28920d025a4a8c8",
            "title": "DesignProbe: A Graphic Design Benchmark for Multimodal Large Language Models"
        },
        {
            "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
            "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
        },
        {
            "paperId": "415c594667bb94f086ea980f897c4f62f9a22d18",
            "title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models"
        },
        {
            "paperId": "1b93eba7e1f1ca764d94909cd7a936ab12002eb4",
            "title": "DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation"
        },
        {
            "paperId": "8484a2af6c1276f9fa11d4091fcab415808ced1d",
            "title": "VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?"
        },
        {
            "paperId": "627a5edf93091a4a50c9501c5ae5541fde393fa3",
            "title": "JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks"
        },
        {
            "paperId": "2af15dd679b9c3c91030e2bf2047abbca0194d94",
            "title": "M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models"
        },
        {
            "paperId": "3c026fbab985ad0af6bfe4818b94c29a286fa4b7",
            "title": "Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want"
        },
        {
            "paperId": "8a9e11addba791860a9dbf15de75cc28d2cf844c",
            "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?"
        },
        {
            "paperId": "9f3bc73b27d83a15baf81a4221322c04c522ff2b",
            "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models"
        },
        {
            "paperId": "ee8c1a46c90f1261c23479e15c6bed7f67ad8943",
            "title": "Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning"
        },
        {
            "paperId": "fd614b1a2e97bfa1a449b888b9a6edde7a0f36df",
            "title": "VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning"
        },
        {
            "paperId": "e198314dbbe1c98957b6e6eea3c91ab47f7decfd",
            "title": "CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model"
        },
        {
            "paperId": "c41ff6575a0eb5942212de2cfdb09396bcb3601e",
            "title": "NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models"
        },
        {
            "paperId": "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2",
            "title": "Visual Hallucinations of Multi-modal Large Language Models"
        },
        {
            "paperId": "3fd4a8f70f60c9cf945762d1a4c3f70b90496e7f",
            "title": "MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms"
        },
        {
            "paperId": "73d57469550b92e1dce5debdddae725f2a582581",
            "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts"
        },
        {
            "paperId": "a3d418b4e35a02e4306505ab660a6bcd44c3c752",
            "title": "Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"
        },
        {
            "paperId": "57a16e741016638dfd5b5f4c8a839c65ef83b817",
            "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension"
        },
        {
            "paperId": "3e75ac0db303e9acdf3ba4b524a1f69c22824ac8",
            "title": "A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs"
        },
        {
            "paperId": "b7b5b8fd1645f432f9099873c0983ba3407c756f",
            "title": "SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models"
        },
        {
            "paperId": "21f1a99feff322b2c2af8c4239eb86ffe64b613b",
            "title": "SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark"
        },
        {
            "paperId": "0370f0e8459bc687c6adaaff2e34de35bb480d81",
            "title": "The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs"
        },
        {
            "paperId": "19e909f88b8b9b0635bd6e441094e1738c3bba9a",
            "title": "Unified Hallucination Detection for Multimodal Large Language Models"
        },
        {
            "paperId": "4d5262260022d8baff65242c6eb879d184447d5f",
            "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning"
        },
        {
            "paperId": "e0702a22e0841c54ab865b4996d7b07af192a3e1",
            "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"
        },
        {
            "paperId": "a07de765c59755fa9653276f0b5ccb68f77d7660",
            "title": "AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception"
        },
        {
            "paperId": "f48b95576bd50a69937d8bbff0cc42bdb20e49f5",
            "title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception"
        },
        {
            "paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0",
            "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"
        },
        {
            "paperId": "c672ec79f55cef8f7a32cd8dddfa981b893f1567",
            "title": "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"
        },
        {
            "paperId": "c60305f2a719c0ab5427a1f55304293ce18cd2e1",
            "title": "EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning"
        },
        {
            "paperId": "d77bc1a237b67c57b0c1b99b4802e703747a9688",
            "title": "BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"
        },
        {
            "paperId": "1c5d887ec6c4d57bfee29fa632a96a75129a50ce",
            "title": "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models"
        },
        {
            "paperId": "eca8a3e6383e3618e0bc984382e08c09be3cca6c",
            "title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding"
        },
        {
            "paperId": "1a5a79b393b3f00eb5a47243ee031ad799d2f641",
            "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"
        },
        {
            "paperId": "b037bb09aa162d8a543e64ec777ca0edc732d2af",
            "title": "Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models"
        },
        {
            "paperId": "cb76f7fc35ff289fdf1cf50d9cfe1493342a0dec",
            "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering"
        },
        {
            "paperId": "d2d2cf9bb166f4e54d3199bc7488314206a23fda",
            "title": "Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs"
        },
        {
            "paperId": "4da938af4eb8e40857f30a3ef612ef4f2eeea300",
            "title": "ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models"
        },
        {
            "paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
            "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"
        },
        {
            "paperId": "ad13b213681b6f634bc83a264df246e83dd9a9d9",
            "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"
        },
        {
            "paperId": "18a3ec6c7aca5fc6e21455db46b6aaff22bc1a35",
            "title": "Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"
        },
        {
            "paperId": "1fa674b4e1c82cf648868be2568637fdfb5fb1b3",
            "title": "What's \"up\" with vision-language models? Investigating their struggle with spatial reasoning"
        },
        {
            "paperId": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "title": "HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models"
        },
        {
            "paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
            "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"
        },
        {
            "paperId": "593b42c628b49937bcd7f7c4a7d54d5f97e6b414",
            "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision"
        },
        {
            "paperId": "786294f4008732a5dac9895a8507bc4c80450075",
            "title": "Dynamic-Superb: Towards a Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark For Speech"
        },
        {
            "paperId": "bb1083425517bdac8d9a6438fcf5032543acb20e",
            "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models"
        },
        {
            "paperId": "656a6b3c0348d69cf9f98f95cbf68046941a4f29",
            "title": "EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"
        },
        {
            "paperId": "d6c2523ab97416c2692cbbeab082ed1790e8e55e",
            "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"
        },
        {
            "paperId": "cb712ab2b3bbaef6bff02efa7295ea420b58f654",
            "title": "Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"
        },
        {
            "paperId": "4309d572a37d655779f9dce6a2c98c66334132de",
            "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"
        },
        {
            "paperId": "ebddfdc5d845a788e8062eddbbf7a335737cb99b",
            "title": "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"
        },
        {
            "paperId": "c7a7104df3db13737a865ede2be8146990fa4026",
            "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"
        },
        {
            "paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c",
            "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"
        },
        {
            "paperId": "50c1414fe41d0cb9db6f0933c9319aa124beac5d",
            "title": "Contextual Object Detection with Multimodal Large Language Models"
        },
        {
            "paperId": "ee156428803c5bd6e7372f6b27d74bcf88390db3",
            "title": "NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario"
        },
        {
            "paperId": "6a5525c316b9be7909c433a79e090ed731425083",
            "title": "What Makes for Good Visual Tokenizers for Large Language Models?"
        },
        {
            "paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
            "title": "Evaluating Object Hallucination in Large Vision-Language Models"
        },
        {
            "paperId": "848e690a62c327e1210532d58a6b914097cac763",
            "title": "On the Hidden Mystery of OCR in Large Multimodal Models"
        },
        {
            "paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd",
            "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
        },
        {
            "paperId": "d48cb91b9e555194f7494c4d4bb9815021d3ee45",
            "title": "VideoChat: Chat-Centric Video Understanding"
        },
        {
            "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
            "title": "Visual Instruction Tuning"
        },
        {
            "paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
            "title": "DINOv2: Learning Robust Visual Features without Supervision"
        },
        {
            "paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba",
            "title": "Sigmoid Loss for Language Image Pre-Training"
        },
        {
            "paperId": "fc8988585c6846fdeee33b34779a6a87b92c3e86",
            "title": "Equivariant Similarity for Vision-Language Foundation Models"
        },
        {
            "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
            "title": "GPT-4 Technical Report"
        },
        {
            "paperId": "57e849d0de13ed5f91d086936296721d4ff75a75",
            "title": "LLaMA: Open and Efficient Foundation Language Models"
        },
        {
            "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"
        },
        {
            "paperId": "1436ee75f8dded44de157cf778a96bdaf6b20a76",
            "title": "Hierarchical multimodal transformers for Multi-Page DocVQA"
        },
        {
            "paperId": "77937788fdbdefe4dd775075b5768c03eaaac008",
            "title": "AVQA: A Dataset for Audio-Visual Question Answering on Videos"
        },
        {
            "paperId": "d3135733aa39dec20ce72aa138589dda27c8406d",
            "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"
        },
        {
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback"
        },
        {
            "paperId": "816fde2349e35cd58dd3945227d2df8be8c8a3f0",
            "title": "VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena"
        },
        {
            "paperId": "a9906a42c77444a7a4573ef19d208bec2534ba78",
            "title": "Multimodal"
        },
        {
            "paperId": "fb1c90806fc5ec72987f58110aa255edbce6620d",
            "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"
        },
        {
            "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
            "title": "Emerging Properties in Self-Supervised Vision Transformers"
        },
        {
            "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "title": "Learning Transferable Visual Models From Natural Language Supervision"
        },
        {
            "paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e",
            "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"
        },
        {
            "paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70",
            "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"
        },
        {
            "paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907",
            "title": "Towards VQA Models That Can Read"
        },
        {
            "paperId": "d0bfd3cb732471a0843a39d2d047caf60a844466",
            "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing"
        },
        {
            "paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c",
            "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"
        },
        {
            "paperId": "4921243268c81d0d6db99053a9d004852225a622",
            "title": "Object Hallucination in Image Captioning"
        },
        {
            "paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c",
            "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"
        },
        {
            "paperId": "03eb382e04cca8cca743f7799070869954f1402a",
            "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"
        },
        {
            "paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
            "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
        },
        {
            "paperId": "def584565d05d6a8ba94de6621adab9e301d375d",
            "title": "Visual7W: Grounded Question Answering in Images"
        },
        {
            "paperId": "0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
            "title": "Explicit Knowledge-based Reasoning for Visual Question Answering"
        },
        {
            "paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6",
            "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"
        },
        {
            "paperId": "95335e01a62dab09cbeabf41f7c09e8f6cf4551a",
            "title": "Learning to answer questions"
        },
        {
            "paperId": "870ab6fdcc60899747e3893c6108f0377eb0a8d2",
            "title": "Benchmarking"
        },
        {
            "paperId": "d97951108dc85389aedda2395467f218f624f70c",
            "title": "Benchmark"
        },
        {
            "paperId": "e871b4e11bf8c5d8126b13b03bd4958e9467e32d",
            "title": "A Family Is"
        },
        {
            "paperId": "e7eed0967523609d9b16c0abc61fde7c3a199ce0",
            "title": "IN RELATION"
        },
        {
            "paperId": null,
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
        },
        {
            "paperId": null,
            "title": "Fact-based"
        },
        {
            "paperId": null,
            "title": "Video-mme"
        },
        {
            "paperId": "920731b589af90a5b79236f4939ac117bbb939f2",
            "title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models"
        },
        {
            "paperId": "13bd881530d003403325cd2de57d8af42acd0d81",
            "title": "A HIGH-RESOLUTION"
        },
        {
            "paperId": null,
            "title": ": A benchmark for complex visual"
        },
        {
            "paperId": null,
            "title": "Charting gaps in realistic chart understanding in"
        },
        {
            "paperId": null,
            "title": "Ii-bench: An image implication"
        },
        {
            "paperId": null,
            "title": ": A situated video reasoning benchmark with aligned"
        },
        {
            "paperId": null,
            "title": "Open foundation models by 01"
        },
        {
            "paperId": null,
            "title": "Does your multi-modal llm truly see the diagrams"
        },
        {
            "paperId": null,
            "title": "Towards end-to-end embodied decision making"
        },
        {
            "paperId": null,
            "title": "A benchmark for question answering about"
        },
        {
            "paperId": null,
            "title": "Empowering remote sensing with vgi-enhanced large multimodal language"
        },
        {
            "paperId": null,
            "title": ": A fine-grained multimodal knowledge editing benchmark emphasizing modality"
        },
        {
            "paperId": null,
            "title": "A large-scale synthetic multi-turn question-answering"
        },
        {
            "paperId": null,
            "title": ": Modularization empowers"
        },
        {
            "paperId": null,
            "title": "Advancing multimodal chart understanding with large-scale instruction"
        },
        {
            "paperId": null,
            "title": "diagnostic dataset for temporal concept"
        },
        {
            "paperId": null,
            "title": "Are multimodal llms eligible as the brain"
        },
        {
            "paperId": null,
            "title": ": Evaluate multimodal llms with unlabeled"
        },
        {
            "paperId": null,
            "title": "Parameter-free llava extension from images to videos"
        },
        {
            "paperId": null,
            "title": ": An open-source small language"
        },
        {
            "paperId": null,
            "title": "A-okvqa"
        },
        {
            "paperId": null,
            "title": "Amber: An llm-free multi-dimensional benchmark for mllms"
        },
        {
            "paperId": null,
            "title": "M 3 cot: A novel benchmark for multi-domain multi-step multi-modal"
        },
        {
            "paperId": null,
            "title": "Claude 3.5 sonnet, 2024"
        },
        {
            "paperId": null,
            "title": "Evaluating mathematical reasoning of foundation models in"
        },
        {
            "paperId": null,
            "title": "multi-modal mobile device"
        },
        {
            "paperId": null,
            "title": "From dense token to sparse memory for"
        },
        {
            "paperId": null,
            "title": "Unibench"
        },
        {
            "paperId": null,
            "title": "Is your multi-modal model"
        },
        {
            "paperId": null,
            "title": "Gemini: a family of highly capable multimodal models"
        }
    ]
}