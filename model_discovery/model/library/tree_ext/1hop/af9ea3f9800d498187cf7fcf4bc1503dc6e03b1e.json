{
    "acronym": "af9ea3f9800d498187cf7fcf4bc1503dc6e03b1e",
    "title": "Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers",
    "seed_ids": [
        "metaformer",
        "fnet",
        "f8a9990ca21f57b3637ed003254da0817c1a8e15",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "af9ea3f9800d498187cf7fcf4bc1503dc6e03b1e",
    "abstract": "Convolution Neural Networks (CNNs) are widely used in medical image analysis, but their performance degrade when the magnification of testing images differ from the training images. The inability of CNNs to generalize across magnification scales can result in sub-optimal performance on external datasets. This study aims to evaluate the robustness of various deep learning architectures in the analysis of breast cancer histopathological images with varying magnification scales at training and testing stages. Here we explore and compare the performance of multiple deep learning architectures, including CNN-based ResNet and MobileNet, self-attention-based Vision Transformers and Swin Transformers, and token-mixing models, such as FNet, ConvMixer, MLP-Mixer, and WaveMix. The experiments are conducted using the BreakHis dataset, which contains breast cancer histopathological images at varying magnification levels. We show that performance of WaveMix is invariant to the magnification of training and testing data and can provide stable and good classification accuracy. These evaluations are critical in identifying deep learning architectures that can robustly handle changes in magnification scale, ensuring that scale changes across anatomical structures do not disturb the inference results.",
    "authors": [
        "Pranav Jeevan",
        "N. Kurian",
        "A. Sethi"
    ],
    "venue": "International Joint Conference on Biomedical Engineering Systems and Technologies",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that performance of WaveMix is invariant to the magnification of training and testing data and can provide stable and good classification accuracy, which is critical in identifying deep learning architectures that can robustly handle changes in magnification scale."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}