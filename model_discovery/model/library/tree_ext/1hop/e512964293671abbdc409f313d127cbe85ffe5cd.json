{
    "acronym": "e512964293671abbdc409f313d127cbe85ffe5cd",
    "title": "GENIE : Large Scale Pre-training for Generation with Diffusion Model",
    "seed_ids": [
        "diffuseq",
        "2c6ac935c826002976722ca8d3319f691975687e",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "25db56fc85fe15625c3375064a35e908ba6dfd2a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e512964293671abbdc409f313d127cbe85ffe5cd",
    "abstract": "In this paper, we propose a large-scale language pre-training for text GEN eration using d I ffusion mod E l, which is named GENIE . GENIE is a pre-training sequence-to-sequence text generation model which combines Transformer and diffusion. The diffusion model accepts the latent information from the encoder, which is used to guide the denoising of the current time step. After multiple such denoise iterations, the diffusion model can restore the Gaussian noise to the diverse output text which is controlled by the input text. More-over, such architecture design also allows us to adopt large scale pre-training on the GENIE . We propose a novel pre-training method named continuous paragraph denoise based on the characteristics of the diffusion model. Extensive experiments on the XS UM , CNN/D AILY M AIL , and G IGAWORD benchmarks shows that GENIE can achieves comparable performance with various strong baselines, especially after pre-training, the generation quality of GENIE is greatly improved. We have also conduct a lot of experiments on the generation diversity and parameter impact of GENIE . The code for GENIE will be made publicly available.",
    "authors": [
        "Zheng-Wen Lin",
        "Yeyun Gong",
        "Yelong Shen",
        "Tong Wu",
        "Zhihao Fan",
        "Chen Lin",
        "Weizhu Chen",
        "Nan Duan"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel pre-training method named continuous paragraph denoise based on the characteristics of the diffusion model is proposed, which can restore the Gaussian noise to the diverse output text which is controlled by the input text."
    },
    "citationCount": 19,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}