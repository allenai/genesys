{
    "acronym": "30f75d952a5d1e4d4a245963da55f0b4606c2182",
    "title": "GCformer: An Efficient Solution for Accurate and Scalable Long-Term Multivariate Time Series Forecasting",
    "seed_ids": [
        "s4",
        "sgconv",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "defecf3dc299214f4cb76a093c6eed2297eaa46f",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "3bc53c49ae68adacf2d5be2fa795bcb879e2717a",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9",
        "f7890aaf985b001e9226da375f8c2289a323db90"
    ],
    "s2id": "30f75d952a5d1e4d4a245963da55f0b4606c2182",
    "abstract": "Transformer-based models have emerged as promising tools for time series forecasting. However, these models cannot make accurate prediction for long input time series. On the one hand, they failed to capture long-range dependency within time series data. On the other hand, the long input sequence usually leads to large model size and high time complexity. To address these limitations, we present GCformer, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals. A cohesive framework for a global convolution kernel has been introduced, utilizing three distinct parameterization methods. The selected structured convolutional kernel in the global branch has been specifically crafted with sublinear complexity, thereby allowing for the efficient and effective processing of lengthy and noisy input signals. Empirical studies on six benchmark datasets demonstrate that GCformer outperforms state-of-the-art methods, reducing MSE error in multivariate time series benchmarks by 4.38% and model parameters by 61.92%. In particular, the global convolutional branch can serve as a plug-in block to enhance the performance of other models, with an average improvement of 31.93%, including various recently published Transformer-based models. Our code is publicly available at https://github.com/Yanjun-Zhao/GCformer.",
    "authors": [
        "Yanjun Zhao",
        "Ziqing Ma",
        "Tian Zhou",
        "Liang Sun",
        "M. Ye",
        "Yi Qian"
    ],
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "GCformer is presented, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals, and can serve as a plug-in block to enhance the performance of other models, with an average improvement of 31.93%."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}