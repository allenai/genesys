{
    "acronym": "eaf3a64b637b3699678374a2afe8dde05d17a5cc",
    "title": "Improved Transformer for High-Resolution GANs",
    "seed_ids": [
        "axialattn",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "eaf3a64b637b3699678374a2afe8dde05d17a5cc",
    "abstract": "Attention-based models, exemplified by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difficult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 30.83 and 2.95 on unconditional ImageNet $128 \\times 128$ and FFHQ $256 \\times 256$, respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions. Our code is made publicly available at https://github.com/google-research/hit-gan",
    "authors": [
        "Long Zhao",
        "Zizhao Zhang",
        "Ting Chen",
        "Dimitris N. Metaxas",
        "Han Zhang"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The proposed HiT is an important milestone for generators in GANs which are completely free of convolutions and has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images."
    },
    "citationCount": 76,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}