{
    "acronym": "cc11b047c41a7d33dccfaad50ef468c2d21e8d73",
    "title": "Inquisition: A Reformed Question-Answering System",
    "seed_ids": [
        "reformer",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1"
    ],
    "s2id": "cc11b047c41a7d33dccfaad50ef468c2d21e8d73",
    "abstract": "Transformer-based models are incredibly powerful, but are also incredibly expensive to train and utilize for inference at scale. In this project, we apply memory-and compute-reduction techniques (such using LSH attention with chunking and RevTransformer blocks) to a popular transformer-based NLP QA model (QANet) to explore whether more efficient models can achieve similar performance to the more expensive ones highlighted in most modern research papers. We have implemented the base QANet model from scratch, and will discuss the impact of replacing core subnetworks within the model with more resource-efficient implementations using the techniques described in the Reformer paper",
    "authors": [
        "Andrew Gaut",
        "Shannon Yan",
        "Zeb Mehring"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This project applies memory-and compute-reduction techniques to a popular transformer-based NLP QA model (QANet) to explore whether more efficient models can achieve similar performance to the more expensive ones highlighted in most modern research papers."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}