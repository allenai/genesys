{
    "acronym": "386b192f7082c8c2bda285b9be424d321a1e9497",
    "title": "Dual Path Transformer with Partition Attention",
    "seed_ids": [
        "reformer",
        "flashattn",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "6c22336873706b1cf5205ac6bd2432aa69d97821",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "386b192f7082c8c2bda285b9be424d321a1e9497",
    "abstract": "This paper introduces a novel attention mechanism, called dual attention, which is both efficient and effective. The dual attention mechanism consists of two parallel components: local attention generated by Convolutional Neural Networks (CNNs) and long-range attention generated by Vision Transformers (ViTs). To address the high computational complexity and memory footprint of vanilla Multi-Head Self-Attention (MHSA), we introduce a novel Multi-Head Partition-wise Attention (MHPA) mechanism. The partition-wise attention approach models both intra-partition and inter-partition attention simultaneously. Building on the dual attention block and partition-wise attention mechanism, we present a hierarchical vision backbone called DualFormer. We evaluate the effectiveness of our model on several computer vision tasks, including image classification on ImageNet, object detection on COCO, and semantic segmentation on Cityscapes. Specifically, the proposed DualFormer-XS achieves 81.5\\% top-1 accuracy on ImageNet, outperforming the recent state-of-the-art MPViT-XS by 0.6\\% top-1 accuracy with much higher throughput.",
    "authors": [
        "Zhengkai Jiang",
        "Liang Liu",
        "Jiangning Zhang",
        "Yabiao Wang",
        "Mingang Chen",
        "Chengjie Wang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces a novel attention mechanism, called dual attention, which is both efficient and effective, and presents a hierarchical vision backbone called DualFormer, which achieves 81.5\\% top-1 accuracy on ImageNet, outperforming the recent state-of-the-art MPViT-XS by 0.6%, and evaluates the effectiveness of the model on several computer vision tasks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}