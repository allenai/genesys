{
    "acronym": "44b7adbd196e69c8771734aa8c9af5fd69c04370",
    "title": "BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model",
    "seed_ids": [
        "alibi",
        "2dfb9171e180dcb0af23d305e024d43d311708ab",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "bb15f3727f827a3cb88b5d3ca48415c09b40a88f",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "44b7adbd196e69c8771734aa8c9af5fd69c04370",
    "abstract": "We introduce the Bittensor Language Model, called\"BTLM-3B-8K\", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \\textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity. On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: https://huggingface.co/cerebras/btlm-3b-8k-base.",
    "authors": [
        "Nolan Dey",
        "Daria Soboleva",
        "Faisal Al-Khateeb",
        "Bowen Yang",
        "Ribhu Pathria",
        "Hemant Khachane",
        "Shaheer Muhammad",
        "Zhiming Chen",
        "Robert Myers",
        "Jacob Robert Steeves",
        "Natalia Vassilieva",
        "Marvin Tom",
        "Joel Hestness"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Bittensor Language Model is introduced, a new state-of-the-art 3 billion parameter open-source language model that needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices."
    },
    "citationCount": 10,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}