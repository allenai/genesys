{
    "acronym": "eeab3b4fcef51c8a7595ca1de2c9308ee070e3bd",
    "title": "Controllable Text Generation in the Instruction-Tuning Era",
    "seed_ids": [
        "gpt3",
        "062bbb6474309d3c42397d8ab808505a91ca6ef2",
        "e7ad6cbea96dea94c06faa2b5a4c2563648d7d16",
        "6db13f58ff662eefa823a660fa86faf8ddf75533",
        "12daedd80a4f860960c5b50314d09d6827f4fd4a",
        "f1e56def812bc398d1b2b8c9a7ea6a623abd38e5",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "4a6a65968a8eb8c09ffb57a7774ddabb596565b1",
        "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "824d15233236e804b40ad827a3b076e07b2c3d3b"
    ],
    "s2id": "eeab3b4fcef51c8a7595ca1de2c9308ee070e3bd",
    "abstract": "While most research on controllable text generation has focused on steering base Language Models, the emerging instruction-tuning and prompting paradigm offers an alternate approach to controllability. We compile and release ConGenBench, a testbed of 17 different controllable generation tasks, using a subset of it to benchmark the performance of 9 different baselines and methods on Instruction-tuned Language Models. To our surprise, we find that prompting-based approaches outperform controllable text generation methods on most datasets and tasks, highlighting a need for research on controllable text generation with Instruction-tuned Language Models in specific. Prompt-based approaches match human performance on most stylistic tasks while lagging on structural tasks, foregrounding a need to study more varied constraints and more challenging stylistic tasks. To facilitate such research, we provide an algorithm that uses only a task dataset and a Large Language Model with in-context capabilities to automatically generate a constraint dataset. This method eliminates the fields dependence on pre-curated constraint datasets, hence vastly expanding the range of constraints that can be studied in the future.",
    "authors": [
        "D. Ashok",
        "Barnab\u00e1s P\u00f3czos"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An algorithm that uses only a task dataset and a Large Language Model with in-context capabilities to automatically generate a constraint dataset, which eliminates the fields dependence on pre-curated constraint datasets, hence vastly expanding the range of constraints that can be studied in the future."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}