{
    "acronym": "f4d546b9cd5681430de63e7d8739dc2d50045fb4",
    "title": "CacheGen: Fast Context Loading for Language Model Applications",
    "seed_ids": [
        "longformer",
        "routingtransformer",
        "2b05686607991a39aead43f371fd7ea2b08195f5",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "e586a4591ba0303b769f2c07cbddaf1899cb72e4",
        "cbbc2cc774c50b0b19922185b80e9ce90b7cd2f6",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "dbc368bc8b49347dd27679894524fa62f88492c9",
        "465471bb5bf1a945549d6291c2d23367966b4957",
        "a7ca1bce0af7fe4703f5c3296db2dcc8dc112f20",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "12809bcb734beafeb47876f42e7b438e27fe99fe",
        "f75d05e759447c2aedb7097728f29f9a520d9bc1",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "93d3e45395117e21214d404c8753b578c29266d1",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "f4d546b9cd5681430de63e7d8739dc2d50045fb4",
    "abstract": "As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing ( e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching ( e.g., key-value features consume orders of magnitude larger bandwidth than the text context). This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts\u2019 key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a tailored arithmetic coder, taking advantage of the KV features\u2019 distributional properties, such as locality across tokens. Furthermore, CacheGen minimizes the total delay in fetching and processing a context by using a controller that determines when to load the context as compressed KV features or raw text and picks the appropriate compression level if loaded as KV features. We test CacheGen on three models of various sizes and three datasets of different context lengths. Compared to recent methods that handle long contexts, CacheGen reduces bandwidth usage by 3.7-4.3 \u00d7 and the total delay in fetching and processing contexts by 2.7-3 \u00d7 while maintaining similar LLM performance on various tasks as loading the text contexts.",
    "authors": [
        "Yuhan Liu",
        "Han-Chiang Li",
        "Kuntai Du",
        "Jiayi Yao",
        "Yihua Cheng",
        "Yuyang Huang",
        "Shan Lu",
        "Michael Maire",
        "Henry Hoffmann",
        "Ari Holtzman",
        "Ganesh Ananthanarayanan",
        "Junchen Jiang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "CacheGen reduces the bandwidth needed for transmitting long contexts\u2019 key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations and minimizes the total delay in fetching and processing a context."
    },
    "citationCount": 12,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}