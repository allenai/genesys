{
    "acronym": "b9be0e630f95cad585706cbbd0385cc2af4c7c8a",
    "title": "Revisiting Structured Variational Autoencoders",
    "seed_ids": [
        "s5",
        "31b27d90c2d5732386dd3a46a7d8336c0555c6fe",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51"
    ],
    "s2id": "b9be0e630f95cad585706cbbd0385cc2af4c7c8a",
    "abstract": "Structured variational autoencoders (SVAEs) combine probabilistic graphical model priors on latent variables, deep neural networks to link latent variables to observed data, and structure-exploiting algorithms for approximate posterior inference. These models are particularly appealing for sequential data, where the prior can capture temporal dependencies. However, despite their conceptual elegance, SVAEs have proven difficult to implement, and more general approaches have been favored in practice. Here, we revisit SVAEs using modern machine learning tools and demonstrate their advantages over more general alternatives in terms of both accuracy and efficiency. First, we develop a modern implementation for hardware acceleration, parallelization, and automatic differentiation of the message passing algorithms at the core of the SVAE. Second, we show that by exploiting structure in the prior, the SVAE learns more accurate models and posterior distributions, which translate into improved performance on prediction tasks. Third, we show how the SVAE can naturally handle missing data, and we leverage this ability to develop a novel, self-supervised training approach. Altogether, these results show that the time is ripe to revisit structured variational autoencoders.",
    "authors": [
        "Yixiu Zhao",
        "Scott W. Linderman"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "By exploiting structure in the prior, the SVAE learns more accurate models and posterior distributions, which translate into improved performance on prediction tasks, and this ability to develop a novel, self-supervised training approach is leveraged."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}