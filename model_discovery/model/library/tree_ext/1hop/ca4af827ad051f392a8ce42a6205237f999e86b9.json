{
    "acronym": "ca4af827ad051f392a8ce42a6205237f999e86b9",
    "title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs",
    "seed_ids": [
        "flagembedding",
        "2b8439f319dfa73df62ca8957ff6d0c1f3c7a73c",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "b6346f9fa093b8e85df712485a2b851b9f680dac",
        "73290ecbec2f38d1d647ddef1ada69cee41725b3",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "f51497f463566581874c941353dd9d80069c5b77",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ca4af827ad051f392a8ce42a6205237f999e86b9",
    "abstract": "Managing long texts is challenging for large language models (LLMs) due to limited context window sizes. This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers under long-context settings. We initially conceptualize the process as a streamlined encoder-decoder framework where the weights-shared encoder and decoder respectively encapsulate a context segment into memories and leverage these memories to predict outputs of the subsequent segment. Subsequently, by treating our memory-enhanced transformers as fully-connected recurrent neural networks (RNNs), we refine the training process using the Truncated Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative incremental optimization techniques. These techniques not only diminish time complexity but also address the bias in gradient computation through an unbiased optimization process. UIO-LLMs successfully handle long context, such as extending the context window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters, while keeping the inference cost nearly linear as context length increases.",
    "authors": [
        "Wenhao Li",
        "Mingbao Lin",
        "Yunshan Zhong",
        "Shuicheng Yan",
        "Rongrong Ji"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "UIO-LLMs successfully handle long context, such as extending the context window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters, while keeping the inference cost nearly linear as context length increases."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}