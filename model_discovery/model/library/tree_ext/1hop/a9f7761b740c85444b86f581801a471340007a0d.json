{
    "acronym": "a9f7761b740c85444b86f581801a471340007a0d",
    "title": "scELMo: Embeddings from Language Models are Good Learners for Single-cell Data Analysis",
    "seed_ids": [
        "gpt2",
        "9f79520c8fdea54dfb1416f91854a240a95ef690",
        "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "44279244407a64431810f982be6d0c7da4429dd7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a9f7761b740c85444b86f581801a471340007a0d",
    "abstract": "Various Foundation Models (FMs) have been built based on the pre-training and fine-tuning framework to analyze single-cell data with different degrees of success. In this manuscript, we propose a method named scELMo (Single-cell Embedding from Language Models), to analyze single cell data that utilizes Large Language Models (LLMs) as a generator for both the description of metadata information and the embeddings for such descriptions. We combine the embeddings from LLMs with the raw data under the zero-shot learning framework to further extend its function by using the fine-tuning framework to handle different tasks. We demonstrate that scELMo is capable of cell clustering, batch effect correction, and cell-type annotation without training a new model. Moreover, the fine-tuning framework of scELMo can help with more challenging tasks including in-silico treatment analysis or modeling perturbation. scELMo has a lighter structure and lower requirement for resources. Moreover, it is comparable to recent largescale FMs (i.e. scGPT [1], Geneformer [2]) based on our evaluations, suggesting a promising path for developing domain-specific FMs.",
    "authors": [
        "Tianyu Liu",
        "Tianqi Chen",
        "W. Zheng",
        "Xiao Luo",
        "Hongyu Zhao"
    ],
    "venue": "bioRxiv",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This manuscript proposes a method named scELMo (Single-cell Embedding from Language Models), to analyze single cell data that utilizes Large Language Models (LLMs) as a generator for both the description of metadata information and the embeddings for such descriptions."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}