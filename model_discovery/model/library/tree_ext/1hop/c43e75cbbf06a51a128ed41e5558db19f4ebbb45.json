{
    "acronym": "c43e75cbbf06a51a128ed41e5558db19f4ebbb45",
    "title": "Context Compression for Auto-regressive Transformers with Sentinel Tokens",
    "seed_ids": [
        "longformer",
        "sparsetransformer",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c43e75cbbf06a51a128ed41e5558db19f4ebbb45",
    "abstract": "The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.",
    "authors": [
        "Siyu Ren",
        "Qi Jia",
        "Kenny Q. Zhu"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}