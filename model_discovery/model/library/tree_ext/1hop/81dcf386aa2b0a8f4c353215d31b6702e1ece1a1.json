{
    "acronym": "81dcf386aa2b0a8f4c353215d31b6702e1ece1a1",
    "title": "Regularizing Transformers With Deep Probabilistic Layers",
    "seed_ids": [
        "transformerxl",
        "6e8f35c6d54acb14109c9b792a62609eac8a7b5e",
        "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
        "62dc8ddb4907db4b889c5e93673d9b3c189d1f25",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "81dcf386aa2b0a8f4c353215d31b6702e1ece1a1",
    "abstract": null,
    "authors": [
        "Aurora Cobo Aguilera",
        "P. Olmos",
        "Antonio Art\u00e9s-Rodr\u00edguez",
        "Fernando P'erez-Cruz"
    ],
    "venue": "Neural Networks",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experimental result demonstrates that the inclusion of deep generative models within Transformer-based architectures such as BERT, RoBERTa, or XLM-R can bring more versatile models, able to generalize better and achieve improved imputation score in tasks such as SST-2 and TREC or even impute missing/noisy words with richer text."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}