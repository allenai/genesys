{
    "acronym": "892397050de6f68c6ea11e9ed3fd091e42aa34f4",
    "title": "Towards Long-Term Time-Series Forecasting: Feature, Pattern, and Distribution",
    "seed_ids": [
        "reformer",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "892397050de6f68c6ea11e9ed3fd091e42aa34f4",
    "abstract": "Long-term time-series forecasting (LTTF) has become a pressing demand in many applications, such as wind power supply planning. Transformer models have been adopted to deliver high prediction capacity because of the high computational self-attention mechanism. Though one could lower the complexity of Transformers by inducing the sparsity in point-wise self-attentions for LTTF, the limited information utilization prohibits the model from exploring the complex dependencies comprehensively. To this end, we propose an efficient Transformer-based model, named Conformer, which differentiates itself from existing methods for LTTF in three aspects: (i) an encoder-decoder architecture incorporating a linear complexity without sacrificing information utilization is proposed on top of sliding-window attention and Stationary and Instant Recurrent Network (SIRN); (ii) a module derived from the normalizing flow is devised to further improve the information utilization by inferring the outputs with the latent variables in SIRN directly; (iii) the inter-series correlation and temporal dynamics in time-series data are modeled explicitly to fuel the downstream self-attention mechanism. Extensive experiments on seven real-world datasets demonstrate that Conformer outperforms the state-of-the-art methods on LTTF and generates reliable prediction results with uncertainty quantification.",
    "authors": [
        "Yan Li",
        "Xin Lu",
        "Haoyi Xiong",
        "Jian Tang",
        "Jian Su",
        "Bo Jin",
        "D. Dou"
    ],
    "venue": "IEEE International Conference on Data Engineering",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An efficient Transformer-based model, named Conformer, is proposed, which differentiates itself from existing methods for LTTF in three aspects and outperforms the state-of-the-art methods on LTTF and generates reliable prediction results with uncertainty quantification."
    },
    "citationCount": 15,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}