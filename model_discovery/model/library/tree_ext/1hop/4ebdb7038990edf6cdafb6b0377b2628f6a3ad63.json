{
    "acronym": "4ebdb7038990edf6cdafb6b0377b2628f6a3ad63",
    "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
    "seed_ids": [
        "roformer",
        "ssdlm",
        "c97df0846d6973602c18ec8049f9b026e491c088",
        "a8e6a8480543efdfd5189a19c6c54a40d2cc6efe",
        "42e726e2ea5bbb946001947d1a5b31ccc6b7aef9",
        "0b9770a377b3f96cef9f268cee1791d39a0d4893",
        "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "0a33c8d2e83b2d8d611698b52805232567724817",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4"
    ],
    "s2id": "4ebdb7038990edf6cdafb6b0377b2628f6a3ad63",
    "abstract": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
    "authors": [
        "Zhijun Liu",
        "Shuai Wang",
        "Sho Inoue",
        "Qibing Bai",
        "Haizhou Li"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes encoding audio as vector sequences in continuous space and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT) and indicates that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}