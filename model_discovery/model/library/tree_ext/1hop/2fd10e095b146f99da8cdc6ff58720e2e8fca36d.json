{
    "acronym": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
    "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
    "seed_ids": [
        "transformerxl",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "f51497f463566581874c941353dd9d80069c5b77",
        "830995ef17cc291c13f42dfd9f462137de1d2179",
        "f4238bd2385a52413ccbacfd9e409a650235bd13"
    ],
    "s2id": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
    "abstract": "Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",
    "authors": [
        "Tao Lei"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling that exhibits strong modeling capacity and training efficiency and suggests jointly leveragingFast recurrence with little attention as a promising direction for accelerating model training and inference."
    },
    "citationCount": 41,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}