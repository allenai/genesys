{
    "acronym": "28863db0ed342b2a465a718fe835ccbd2e54c3e3",
    "title": "STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction",
    "seed_ids": [
        "hopfield",
        "3f3c01adbdd433d515c19ac8cf6c61c905f0061a",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "eb95b02edfaeb28f528b5ee8b705388bb9a933be",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "562bf6d0aac2c6362086ef4c80503de8ea56b340",
        "f6390beca54411b06f3bde424fb983a451789733",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "28863db0ed342b2a465a718fe835ccbd2e54c3e3",
    "abstract": "We present STanHop-Net (Sparse Tandem Hopfield Network) for multivariate time series prediction with memory-enhanced capabilities. At the heart of our approach is STanHop, a novel Hopfield-based neural network block, which sparsely learns and stores both temporal and cross-series representations in a data-dependent fashion. In essence, STanHop sequentially learn temporal representation and cross-series representation using two tandem sparse Hopfield layers. In addition, StanHop incorporates two additional external memory modules: a Plug-and-Play module and a Tune-and-Play module for train-less and task-aware memory-enhancements, respectively. They allow StanHop-Net to swiftly respond to certain sudden events. Methodologically, we construct the StanHop-Net by stacking STanHop blocks in a hierarchical fashion, enabling multi-resolution feature extraction with resolution-specific sparsity. Theoretically, we introduce a sparse extension of the modern Hopfield model (Generalized Sparse Modern Hopfield Model) and show that it endows a tighter memory retrieval error compared to the dense counterpart without sacrificing memory capacity. Empirically, we validate the efficacy of our framework on both synthetic and real-world settings.",
    "authors": [
        "Dennis Wu",
        "Jerry Yao-Chieh Hu",
        "Weijian Li",
        "Bo-Yu Chen",
        "Han Liu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Theoretically, this work introduces a sparse extension of the modern Hopfield model (Generalized Sparse Modern Hopfield Model) and shows that it endows a tighter memory retrieval error compared to the dense counterpart without sacrificing memory capacity."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}