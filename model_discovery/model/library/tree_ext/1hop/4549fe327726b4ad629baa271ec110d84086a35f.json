{
    "acronym": "4549fe327726b4ad629baa271ec110d84086a35f",
    "title": "Exploring the limits of decoder-only models trained on public speech recognition corpora",
    "seed_ids": [
        "gpt3",
        "0c4f46e4dcae5527018e6432fb60cfe8c3354e97"
    ],
    "s2id": "4549fe327726b4ad629baa271ec110d84086a35f",
    "abstract": "The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.",
    "authors": [
        "Ankit Gupta",
        "G. Saon",
        "Brian Kingsbury"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}