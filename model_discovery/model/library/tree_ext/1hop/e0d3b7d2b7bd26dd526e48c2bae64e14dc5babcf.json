{
    "acronym": "e0d3b7d2b7bd26dd526e48c2bae64e14dc5babcf",
    "title": "Long-Range Transformers for Dynamic Spatiotemporal Forecasting",
    "seed_ids": [
        "performer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "f4566761fe39c4b5273d696d9bc3f4195c9325bb",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "2a31319e73d4486716168b65cdf7559baeda18ce",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "e0d3b7d2b7bd26dd526e48c2bae64e14dc5babcf",
    "abstract": "Multivariate time series forecasting focuses on predicting future values based on historical context. State-of-the-art sequence-to-sequence models rely on neural attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables. In contrast, methods based on graph neural networks explicitly model variable relationships. However, these methods often rely on predefined graphs that cannot change over time and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep. Our work addresses these problems by translating multivariate forecasting into a\"spatiotemporal sequence\"formulation where each Transformer input token represents the value of a single variable at a given time. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, achieves competitive results on benchmarks from traffic forecasting to electricity demand and weather prediction while learning spatiotemporal relationships purely from data.",
    "authors": [
        "J. Grigsby",
        "Zhe Wang",
        "Yanjun Qi"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work addresses multivariate forecasting into a \"spatiotemporal sequence\"formulation where each Transformer input token represents the value of a single variable at a given time and Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence."
    },
    "citationCount": 55,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}