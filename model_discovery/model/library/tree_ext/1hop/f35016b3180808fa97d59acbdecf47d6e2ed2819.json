{
    "acronym": "f35016b3180808fa97d59acbdecf47d6e2ed2819",
    "title": "Rethinking Vision Transformers for MobileNet Size and Speed",
    "seed_ids": [
        "metaformer",
        "ec139916edd6feb9b3cb3a0325ca96e21dbb0147",
        "066c143b427571fb5568f2c581ea9066478d2e55",
        "dd1139cfc609c2f3263d02e97176d5275caebc0a",
        "bf6ce546c589fa8054b3972b266532664914bd21",
        "6b6ffb94626e672caffafc77097491d9ee7a8682",
        "7d2a78a1f713b71c3a337247d042c5c2f0b2da84",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "f35016b3180808fa97d59acbdecf47d6e2ed2819",
    "abstract": "With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously. The proposed models, EfficientFormerV2, achieve 3.5% higher top-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and parameters. This work demonstrate that properly designed and optimized vision transformers can achieve high performance even with MobileNet-level size and speed 1.",
    "authors": [
        "Yanyu Li",
        "Ju Hu",
        "Yang Wen",
        "Georgios Evangelidis",
        "Kamyar Salahi",
        "Yanzhi Wang",
        "S. Tulyakov",
        "Jian Ren"
    ],
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work revisits the design choices of ViTs and proposes a novel supernet with low latency and high parameter efficiency, and introduces a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously."
    },
    "citationCount": 74,
    "influentialCitationCount": 14,
    "code": null,
    "description": null,
    "url": null
}