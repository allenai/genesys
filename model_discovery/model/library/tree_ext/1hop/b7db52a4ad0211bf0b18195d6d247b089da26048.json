{
    "acronym": "b7db52a4ad0211bf0b18195d6d247b089da26048",
    "title": "The ART of LLM Refinement: Ask, Refine, and Trust",
    "seed_ids": [
        "gpt3",
        "538288d24bdad73d831dfed44b706958287ed318",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "23447f473cd240494b0a20ea008038aaef7e3391",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2"
    ],
    "s2id": "b7db52a4ad0211bf0b18195d6d247b089da26048",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations and self-improve?A popular concept, referred to as *self-refinement*, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with a refinement strategy called *ART: Ask, Refine, and Trust*, which *asks* necessary questions to decide when an LLM should *refine* its output, and uses it to affirm or deny *trust* in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), *ART* achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We believe that *ART* with smaller models, making refinement decisions can be a cost-effective alternative to fine-tuning LLMs.",
    "authors": [
        "K. Shridhar",
        "Koustuv Sinha",
        "Andrew Cohen",
        "Tianlu Wang",
        "Ping Yu",
        "Ramakanth Pasunuru",
        "Mrinmaya Sachan",
        "Jason Weston",
        "Asli Celikyilmaz"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a reasoning with a refinement strategy called *ART: Ask, Refine, and Trust*, which asks necessary questions to decide when an LLM should *refine* its output, and uses it to affirm or deny *trust* in its refinement by ranking the refinement and the initial prediction."
    },
    "citationCount": 11,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}