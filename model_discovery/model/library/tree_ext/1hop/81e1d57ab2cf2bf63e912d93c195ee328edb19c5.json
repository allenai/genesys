{
    "acronym": "81e1d57ab2cf2bf63e912d93c195ee328edb19c5",
    "title": "Empirical Analysis of Dialogue Relation Extraction with Large Language Models",
    "seed_ids": [
        "gpt3",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "81e1d57ab2cf2bf63e912d93c195ee328edb19c5",
    "abstract": "Dialogue relation extraction (DRE) aims to extract relations between two arguments within a dialogue, which is more challenging than standard RE due to the higher person pronoun frequency and lower information density in dialogues. However, existing DRE methods still suffer from two serious issues: (1) hard to capture long and sparse multi-turn information, and (2) struggle to extract golden relations based on partial dialogues, which motivates us to discover more effective methods that can alleviate the above issues. We notice that the rise of large language models (LLMs) has sparked considerable interest in evaluating their performance across diverse tasks. To this end, we initially investigate the capabilities of different LLMs in DRE, considering both proprietary models and open-source models. Interestingly, we discover that LLMs significantly alleviate two issues in existing DRE methods. Generally, we have following findings: (1) scaling up model size substantially boosts the overall DRE performance and achieves exceptional results, tackling the difficulty of capturing long and sparse multi-turn information; (2) LLMs encounter with much smaller performance drop from entire dialogue setting to partial dialogue setting compared to existing methods; (3) LLMs deliver competitive or superior performances under both full-shot and few-shot settings compared to current state-of-the-art; (4) LLMs show modest performances on inverse relations but much stronger improvements on general relations, and they can handle dialogues of various lengths especially for longer sequences.",
    "authors": [
        "Guozheng Li",
        "Zijie Xu",
        "Ziyu Shang",
        "Jiajun Liu",
        "Ke Ji",
        "Yikai Guo"
    ],
    "venue": "Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Interestingly, it is discovered that LLMs significantly alleviate two issues in existing DRE methods, including hard to capture long and sparse multi-turn information and struggle to extract golden relations based on partial dialogues."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}