{
    "acronym": "5b92aa83a6516b52d0ebca749bee184a99048877",
    "title": "Generating Factual Documents by Synthesizing Knowledge Sources",
    "seed_ids": [
        "memcompress",
        "83fac78857c7e65fe10a11a798674dd3cd259c1d",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "830995ef17cc291c13f42dfd9f462137de1d2179",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5b92aa83a6516b52d0ebca749bee184a99048877",
    "abstract": "From youth, humans can read and process large amounts of information to write 1 articles, book reports, and conduct deep conversation. Existing large-scale language 2 models are yet incapable of such meaningful generation. We propose a knowledge-3 grounded document writing task for pre-training an encoder-decoder language 4 model to enable such knowledge synthesis. We will pre-train a model on networks 5 of knowledge-grounded documents from encyclopedias and news, leveraging high-6 quality source citations common in these \ufb01elds. We present the datasets that we 7 have collected thus far, methods for large-context knowledge grounded synthesis, 8 and preliminary results indicating the applicability of our framework. 9",
    "authors": [],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A knowledge-3 grounded document writing task for pre-training an encoder-decoder language 4 model to enable knowledge synthesis, and preliminary results indicating the applicability of the framework."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}