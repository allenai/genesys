{
    "acronym": "b854589889608edcc45409c27f528c91c8c9e7b2",
    "title": "Social Learning: Towards Collaborative Learning with Large Language Models",
    "seed_ids": [
        "gpt3",
        "ada81a4de88a6ce474df2e2446ad11fea480616e",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b854589889608edcc45409c27f528c91c8c9e7b2",
    "abstract": "We introduce the framework of\"social learning\"in the context of large language models (LLMs), whereby models share knowledge with each other in a privacy-aware manner using natural language. We present and evaluate two approaches for knowledge transfer between LLMs. In the first scenario, we allow the model to generate abstract prompts aiming to teach the task. In our second approach, models transfer knowledge by generating synthetic examples. We evaluate these methods across diverse datasets and quantify memorization as a proxy for privacy loss. These techniques inspired by social learning yield promising results with low memorization of the original data. In particular, we show that performance using these methods is comparable to results with the use of original labels and prompts. Our work demonstrates the viability of social learning for LLMs, establishes baseline approaches and highlights several unexplored areas for future work.",
    "authors": [
        "Amirkeivan Mohtashami",
        "Florian Hartmann",
        "Sian Gooding",
        "Luk\u00e1s Zilka",
        "Matt Sharifi",
        "B. A. Y. Arcas"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}