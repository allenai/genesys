{
    "acronym": "2042cec0e9d1883f6c1c55c021b20da45effed11",
    "title": "Exploring Deep Transfer Learning Techniques for Alzheimer\u2019s Dementia Detection",
    "seed_ids": [
        "longformer",
        "228896a28b18ef3ad956dd8000b5ea3b8a0bfd7e",
        "eb6309dd0e70fe68ba2c97629f9091eae3cbf8b4",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "09e2c7adbed37440d4a339852cfa34e5b660f768",
        "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1"
    ],
    "s2id": "2042cec0e9d1883f6c1c55c021b20da45effed11",
    "abstract": "Examination of speech datasets for detecting dementia, collected via various speech tasks, has revealed links between speech and cognitive abilities. However, the speech dataset available for this research is extremely limited because the collection process of speech and baseline data from patients with dementia in clinical settings is expensive. In this paper, we study the spontaneous speech dataset from a recent ADReSS challenge, a Cookie Theft Picture (CTP) dataset with balanced groups of participants in age, gender, and cognitive status. We explore state-of-the-art deep transfer learning techniques from image, audio, speech, and language domains. We envision that one advantage of transfer learning is to eliminate the design of handcrafted features based on the tasks and datasets. Transfer learning further mitigates the limited dementia-relevant speech data problem by inheriting knowledge from similar but much larger datasets. Specifically, we built a variety of transfer learning models using commonly employed MobileNet (image), YAMNet (audio), Mockingjay (speech), and BERT (text) models. Results indicated that the transfer learning models of text data showed significantly better performance than those of audio data. Performance gains of the text models may be due to the high similarity between the pre-training text dataset and the CTP text dataset. Our multi-modal transfer learning introduced a slight improvement in accuracy, demonstrating that audio and text data provide limited complementary information. Multi-task transfer learning resulted in limited improvements in classification and a negative impact in regression. By analyzing the meaning behind the Alzheimer's disease (AD)/non-AD labels and Mini-Mental State Examination (MMSE) scores, we observed that the inconsistency between labels and scores could limit the performance of the multi-task learning, especially when the outputs of the single-task models are highly consistent with the corresponding labels/scores. In sum, we conducted a large comparative analysis of varying transfer learning models focusing less on model customization but more on pre-trained models and pre-training datasets. We revealed insightful relations among models, data types, and data labels in this research area.",
    "authors": [
        "Youxiang Zhu",
        "Xiaohui Liang",
        "J. Batsis",
        "R. Roth"
    ],
    "venue": "Frontiers of Computer Science",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A large comparative analysis of varying transfer learning models focusing less on model customization but more on pre-trained models and pre-training datasets revealed insightful relations among models, data types, and data labels in this research area."
    },
    "citationCount": 29,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}