{
    "acronym": "722bd6f31934b5772d4d3f4425d3f8d5981fafa9",
    "title": "Improving CTC-Based Speech Recognition Via Knowledge Transferring from Pre-Trained Language Models",
    "seed_ids": [
        "gpt2",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "722bd6f31934b5772d4d3f4425d3f8d5981fafa9",
    "abstract": "Recently, end-to-end automatic speech recognition models based on connectionist temporal classification (CTC) have achieved impressive results, especially when fine-tuned from wav2vec2.0 models. Due to the conditional independence assumption, CTC-based models are always weaker than attention-based encoder-decoder models and require the assistance of external language models (LMs). To solve this issue, we propose two knowledge transferring methods that leverage pre-trained LMs, such as BERT and GPT2, to improve CTC-based models. The first method is based on representation learning, in which the CTC-based models use the representation produced by BERT as an auxiliary learning target. The second method is based on joint classification learning, which combines GPT2 for text modeling with a hybrid CTC/attention architecture. Experiment on AISHELL-1 corpus yields a character error rate (CER) of 4.2% on the test set. When compared to the vanilla CTC-based models fine-tuned from the wav2vec2.0 models, our knowledge transferring method reduces CER by 16.1% relatively without external LMs.",
    "authors": [
        "Keqi Deng",
        "Songjun Cao",
        "Yike Zhang",
        "Long Ma",
        "Gaofeng Cheng",
        "Ji Xu",
        "Pengyuan Zhang"
    ],
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes two knowledge transferring methods that leverage pre-trained LMs, such as BERT and GPT2, to improve CTC-based models and proposes a joint classification learning method, which combines G PT2 for text modeling with a hybrid CTC/attention architecture."
    },
    "citationCount": 21,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}