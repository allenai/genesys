{
    "acronym": "7c65c43c6b6052282e22d7a57458e959ceed3a5a",
    "title": "cdsBERT - Extending Protein Language Models with Codon Awareness",
    "seed_ids": [
        "gpt",
        "c064c79e3026f81e5043cd5b0f4264b4d43336e6",
        "c10a283ed8c5d27cf3394e60a457a276029043cd"
    ],
    "s2id": "7c65c43c6b6052282e22d7a57458e959ceed3a5a",
    "abstract": "Recent advancements in Protein Language Models (pLMs) have enabled high-throughput analysis of proteins through primary sequence alone. At the same time, newfound evidence illustrates that codon usage bias is remarkably predictive and can even change the final structure of a protein. Here, we explore these findings by extending the traditional vocabulary of pLMs from amino acids to codons to encapsulate more information inside CoDing Sequences (CDS). We build upon traditional transfer learning techniques with a novel pipeline of token embedding matrix seeding, masked language modeling, and student-teacher knowledge distillation, called MELD. This transformed the pretrained ProtBERT into cdsBERT; a pLM with a codon vocabulary trained on a massive corpus of CDS. Interestingly, cdsBERT variants produced a highly biochemically relevant latent space, outperforming their amino acid-based counterparts on enzyme commission number prediction. Further analysis revealed that synonymous codon token embeddings moved distinctly in the embedding space, showcasing unique additions of information across broad phylogeny inside these traditionally \u201csilent\u201d mutations. This embedding movement correlated significantly with average usage bias across phylogeny. Future fine-tuned organism-specific codon pLMs may potentially have a more significant increase in codon usage fidelity. This work enables an exciting potential in using the codon vocabulary to improve current state-of-the-art structure and function prediction that necessitates the creation of a codon pLM foundation model alongside the addition of high-quality CDS to large-scale protein databases.",
    "authors": [
        "Logan Hallee",
        "Nikolaos Rafailidis",
        "Jason P. Gleghorn"
    ],
    "venue": "bioRxiv",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work transformed the pretrained ProtBERT into cdsBERT; a pLM with a codon vocabulary trained on a massive corpus of CDS, and revealed that synonymous codon token embeddings moved distinctly in the embedding space, showcasing unique additions of information across broad phylogeny inside these traditionally \u201csilent\u201d mutations."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}