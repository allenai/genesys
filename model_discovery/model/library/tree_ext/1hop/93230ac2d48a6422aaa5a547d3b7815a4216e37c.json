{
    "acronym": "93230ac2d48a6422aaa5a547d3b7815a4216e37c",
    "title": "Domain-Specific Adaptation for Enhanced Gait Recognition in Practical Scenarios",
    "seed_ids": [
        "settransformer"
    ],
    "s2id": "93230ac2d48a6422aaa5a547d3b7815a4216e37c",
    "abstract": "Gait recognition is a burgeoning field within biometric recognition which utilizes computer vision technology to extract silhouette images or body skeletons to identify users by leveraging the unique walking patterns of individuals. However, with its huge potential for user identification in diverse settings especially in security and surveillance applications, it faces challenges in transitioning from controlled datasets to real-world applications. In the regime of silhouette-based models, the most challenging covariate is associated with varying viewing angles, which has often been a bottleneck to achieving optimal accuracy for practical application in a real-world situation. Addressing this challenge, this paper introduces a novel domain adaptation technique tailored for gait recognition for practical applications, capitalising on expansive dataset pretraining and precise fine-tuning on targeted, smaller datasets pertaining to specific camera views. Our deep dive reveals that models adopting this adaptive training approach, especially when fine-tuned with viewing angles mirroring the test domain, witness a significant boost in pure cross-domain performance. Moreover, in a stride towards practical gait recognition, we present Asilla-Office\u2014a non-synthetic dataset captured in an indoor office simulating real walking patterns of people in a real application environment. With its roots in real-world challenges, Asilla-Office is poised to be an initial benchmark, promoting research reflecting genuine application needs. In-depth experiments show that our domain-adapted fine-tuning approach trumps traditional single-staged training, marking a notable leap of more than 11% in Rank-1 accuracy on the new Asilla-Office dataset. In the spirit of fostering community-driven progress, the Asilla-Office dataset will be made publicly available.",
    "authors": [
        "Nitish Jaiswal",
        "Vi Duc Huan",
        "Felix Limanta",
        "Koichi Shinoda",
        "Masahiro Wakasa"
    ],
    "venue": "International Conference on Image, Video and Signal Processing",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "In-depth experiments show that the domain-adapted fine-tuning approach trumps traditional single-staged training, marking a notable leap of more than 11% in Rank-1 accuracy on the new Asilla-Office dataset."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}