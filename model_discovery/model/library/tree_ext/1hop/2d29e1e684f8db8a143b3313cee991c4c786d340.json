{
    "acronym": "2d29e1e684f8db8a143b3313cee991c4c786d340",
    "title": "EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training",
    "seed_ids": [
        "gpt",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "2d29e1e684f8db8a143b3313cee991c4c786d340",
    "abstract": "Although pre-trained language models have remarkably enhanced the generation ability of dialogue systems, open-domain Chinese dialogue systems are still limited by the dialogue data and the model size compared with English ones. In this paper, we propose EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters. To build this model, we collect the largest Chinese dialogue dataset named WDC-Dialogue from various public social media. This dataset contains 1.4B context-response pairs and is used as the pre-training corpus of EVA. Extensive experiments on automatic and human evaluation show that EVA outperforms other Chinese pre-trained dialogue models especially in the multi-turn interaction of human-bot conversations.",
    "authors": [
        "Hao Zhou",
        "Pei Ke",
        "Zheng Zhang",
        "Yuxian Gu",
        "Yinhe Zheng",
        "Chujie Zheng",
        "Yida Wang",
        "Chen Henry Wu",
        "Hao Sun",
        "Xiaocong Yang",
        "Bosi Wen",
        "Xiaoyan Zhu",
        "Minlie Huang",
        "Jie Tang"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters is proposed and extensive experiments on automatic and human evaluation show that EVA outperforms other ChinesePre- trained dialogue models especially in the multi-turn interaction of human-bot conversations."
    },
    "citationCount": 52,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}