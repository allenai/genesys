{
    "acronym": "325f47fc20929532937165228f7f2fa29bbc040b",
    "title": "T OPIC -A WARE C ONTEXTUALIZED T RANSFORMERS",
    "seed_ids": [
        "transformerxl",
        "3df83a60f55c64b40e6dbcd99cf9f67894a0736e",
        "f51497f463566581874c941353dd9d80069c5b77",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "325f47fc20929532937165228f7f2fa29bbc040b",
    "abstract": "Training on disjoint fixed-length segments, Transformers successfully transform static word embeddings into contextualized word representations. However, they often restrict the context of a token to the segment it resides in and hence neglect the flow of contextual information across segments, failing to capture longer-term dependencies beyond the predefined segment length. This paper uses a probabilistic deep topic model to provide contextualized embeddings at both the token and segment levels. It also introduces a contextual next-word embedding guided topic attention module, injecting contextualized topic information into Transformerbased architectures. The proposed method not only captures global semantic coherence of all segments and word concurrence patterns, but also enriches the representation of each token by adapting it to its local context, which goes beyond the segment it resides in and can be flexibly defined according to the target task while maintaining control over memory footprint and computational time. Experiments on various corpora show that adding only a few extra parameters, the proposed topic-aware contextualized transformers consistently outperform their conventional counterparts, and can be used to generate coherent sentences and paragraphs.",
    "authors": [],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A probabilistic deep topic model is used to provide contextualized embeddings at both the token and segment levels, and a contextual next-word embedding guided topic attention module is introduced, injecting contextualized topic information into Transformerbased architectures."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}