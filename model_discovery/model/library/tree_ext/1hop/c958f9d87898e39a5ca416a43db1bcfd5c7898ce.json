{
    "acronym": "c958f9d87898e39a5ca416a43db1bcfd5c7898ce",
    "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
    "seed_ids": [
        "bert",
        "3cbd663640482ea8ffb6d7f128642b8757dfb35e",
        "d77942dde428c5a7dfe0b7991f4cbdc5d1f1f42d",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "c958f9d87898e39a5ca416a43db1bcfd5c7898ce",
    "abstract": "Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract\"important information\", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of\"important information\". We evaluate the meta-model under the few-shot adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.",
    "authors": [
        "Letian Peng",
        "Zilong Wang",
        "Feng Yao",
        "Zihan Wang",
        "Jingbo Shang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Results confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}