{
    "acronym": "3c6f2e0c5ff5dff6151c3e6489378a53318a75b4",
    "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
    "seed_ids": [
        "compressivetransformer",
        "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "f51497f463566581874c941353dd9d80069c5b77",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad"
    ],
    "s2id": "3c6f2e0c5ff5dff6151c3e6489378a53318a75b4",
    "abstract": "As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.",
    "authors": [
        "Xin Men",
        "Mingyu Xu",
        "Qingyu Zhang",
        "Bingning Wang",
        "Hongyu Lin",
        "Yaojie Lu",
        "Xianpei Han",
        "Weipeng Chen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality, and proposes a straightforward pruning approach: layer removal, in which the redundant layers in LLMs are deleted based on their BI scores."
    },
    "citationCount": 25,
    "influentialCitationCount": 8,
    "code": null,
    "description": null,
    "url": null
}