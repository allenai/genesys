{
    "acronym": "fc92ed618ded1d1463f7ec736093f420b05827db",
    "title": "Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs",
    "seed_ids": [
        "gpt2",
        "9fa9d5dd481400b2f3904b33d542d70a6affccb9",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fc92ed618ded1d1463f7ec736093f420b05827db",
    "abstract": "Large transformer-based language models have achieved incredible success at various tasks which require narrative comprehension, including story completion, answering questions about stories, and generating stories ex nihilo. However, due to the limitations of finite context windows, these language models struggle to produce or understand stories longer than several thousand tokens. In order to mitigate the document length limitations that come with finite context windows, we introduce a novel architecture that augments story processing with an external dynamic knowledge graph. In contrast to static commonsense knowledge graphs which hold information about the real world, these dynamic knowledge graphs reflect facts extracted from the story being processed. Our architecture uses these knowledge graphs to create information-rich prompts which better facilitate story comprehension than prompts composed only of story text. We apply our architecture to the tasks of question answering and story completion. To complement this line of research, we introduce two long-form question answering tasks, LF-SQuAD and LF-QUOREF, in which the document length exceeds the size of the language model's context window, and introduce a story completion evaluation method that bypasses the stochastic nature of language model generation. We demonstrate broad improvement over typical prompt formulation methods for both question answering and story completion using GPT-2, GPT-3 and XLNet.",
    "authors": [
        "Berkeley Andrus",
        "Yeganeh Nasiri",
        "Shilong Cui",
        "Benjamin Cullen",
        "Nancy Fulda"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a novel architecture that augments story processing with an external dynamic knowledge graph, and introduces a story completion evaluation method that bypasses the stochastic nature of language model generation."
    },
    "citationCount": 25,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}