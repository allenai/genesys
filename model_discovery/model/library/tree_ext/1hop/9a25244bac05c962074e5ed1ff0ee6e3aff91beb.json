{
    "acronym": "9a25244bac05c962074e5ed1ff0ee6e3aff91beb",
    "title": "Language Models Encode Collaborative Signals in Recommendation",
    "seed_ids": [
        "bert",
        "88d9b2006ea8807767ce01a3c9f680683c36c5f5",
        "13965d8d68217308ff8c7e738ced637739c6a1b8",
        "a1a05b4c7e8adfe15345885da40c73c12f189e39",
        "ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "11628f656257e75e46447ac21cdaa86c4b340a0a",
        "cbf3bf8f541f5b446c59c8deacbcc18527768c75",
        "df602516e28a9ef0ef665ed0aef551984d8d770d"
    ],
    "s2id": "9a25244bac05c962074e5ed1ff0ee6e3aff91beb",
    "abstract": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields. However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to the prevailing understanding that LMs and traditional recommender models learn two distinct representation spaces due to a huge gap in language and behavior modeling objectives, this work rethinks such understanding and explores extracting a recommendation space directly from the language representation space. Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance. This outcome suggests the homomorphism between the language representation space and an effective recommendation space, implying that collaborative signals may indeed be encoded within advanced LMs. Motivated by these findings, we propose a simple yet effective collaborative filtering (CF) model named AlphaRec, which utilizes language representations of item textual metadata (e.g., titles) instead of traditional ID-based embeddings. Specifically, AlphaRec is comprised of three main components: a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function, making it extremely easy to implement and train. Our empirical results show that AlphaRec outperforms leading ID-based CF models on multiple datasets, marking the first instance of such a recommender with text embeddings achieving this level of performance. Moreover, AlphaRec introduces a new language-representation-based CF paradigm with several desirable advantages: being easy to implement, lightweight, rapid convergence, superior zero-shot recommendation abilities in new domains, and being aware of user intention.",
    "authors": [
        "Leheng Sheng",
        "An Zhang",
        "Yi Zhang",
        "Yuxin Chen",
        "Xiang Wang",
        "Tat-Seng Chua"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Surprisingly, the findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance, suggesting the homomorphism between the language representation space and an effective recommendation space, implying that collaborative signals may indeed be encoded within advanced LMs."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}