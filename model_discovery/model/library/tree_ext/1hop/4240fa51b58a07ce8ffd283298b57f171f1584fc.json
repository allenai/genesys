{
    "acronym": "4240fa51b58a07ce8ffd283298b57f171f1584fc",
    "title": "Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%",
    "seed_ids": [
        "transformer",
        "gpt2",
        "76e8218f657c77c38da44daaed5bb54ab727a8fc",
        "7e3864e2ab94cd5bc54830392dcdb49a927f1ba6",
        "0c474b03e8fd385b08a40df22934c9d9b180ffb7",
        "512fdb53eb341bc56769728274615c9bbe5bc23d",
        "32b3553d7dc8a263c63d32eeec2916d1647ab178",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4240fa51b58a07ce8ffd283298b57f171f1584fc",
    "abstract": "In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models. Code and models are available at https://github.com/zh460045050/VQGAN-LC.",
    "authors": [
        "Lei Zhu",
        "Fangyun Wei",
        "Yanye Lu",
        "Dong Chen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99% and demonstrating the superior performance of the model over its counterparts across a variety of tasks."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}