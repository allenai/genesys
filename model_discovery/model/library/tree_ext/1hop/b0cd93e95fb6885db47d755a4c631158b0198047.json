{
    "acronym": "b0cd93e95fb6885db47d755a4c631158b0198047",
    "title": "DeLighT: Very Deep and Light-weight Transformer",
    "seed_ids": [
        "transformerxl",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "841d43cf4015042a4ee45745c5b6f2c59c184da5",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "16c844fd4d97f3c6eb38b0d6527c87d184efedc3"
    ],
    "s2id": "b0cd93e95fb6885db47d755a4c631158b0198047",
    "abstract": "We introduce a very deep and light-weight transformer, DeLighT, that delivers similar or better performance than transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using DExTra, a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on machine translation and language modeling tasks show that DeLighT matches the performance of baseline Transformers with significantly fewer parameters. On the WMT'14 En-Fr high resource dataset, DeLighT requires 1.8 times fewer parameters and 2 times fewer operations and achieves better performance (+0.4 BLEU score) than baseline transformers. On the WMT'16 En-Ro low resource dataset, DeLighT delivers similar performance with 2.8 times fewer parameters than baseline transformers.",
    "authors": [
        "Sachin Mehta",
        "Marjan Ghazvininejad",
        "Srini Iyer",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A very deep and light-weight transformer, DeLighT, that delivers similar or better performance than transformer-based models with significantly fewer parameters, and that matches the performance of baseline Transformers with significantly less parameters."
    },
    "citationCount": 30,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}