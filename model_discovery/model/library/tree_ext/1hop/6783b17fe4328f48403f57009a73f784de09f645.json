{
    "acronym": "6783b17fe4328f48403f57009a73f784de09f645",
    "title": "XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters",
    "seed_ids": [
        "gpt",
        "83edcfbb206ddad38a971d605da09390604248ea",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "15a9fecbcae22e6029714c3ba71469b51e65586a"
    ],
    "s2id": "6783b17fe4328f48403f57009a73f784de09f645",
    "abstract": "Recently, with the popularity of ChatGPT, large-scale language models have experienced rapid development. However, there is a scarcity of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By integrating general and domain-specific knowledge, as well as combining the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.",
    "authors": [
        "Xuanyu Zhang",
        "Qing Yang",
        "Dongliang Xu"
    ],
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture, and proposes a novel training method called hybrid-tuning to mitigate catastrophic forgetting."
    },
    "citationCount": 49,
    "influentialCitationCount": 14,
    "code": null,
    "description": null,
    "url": null
}