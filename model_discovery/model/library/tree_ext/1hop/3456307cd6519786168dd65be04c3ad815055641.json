{
    "acronym": "3456307cd6519786168dd65be04c3ad815055641",
    "title": "Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "3456307cd6519786168dd65be04c3ad815055641",
    "abstract": "Massive textual content has enabled rapid advances in natural language modeling. The use of pre-trained deep neural language models has significantly improved natural language understanding tasks. However, the extent to which these systems can be applied to content generation is unclear. While a few informal studies have claimed that these models can generate \u2018high quality\u2019 readable content, there is no prior study on analyzing the generated content from these models based on sampling and fine-tuning hyperparameters. We conduct an in-depth comparison of several language models for open-ended story generation from given prompts. Using a diverse set of automated metrics, we compare the performance of transformer-based generative models \u2013 OpenAI\u2019s GPT2 (pre-trained and fine-tuned) and Google\u2019s pre-trained Transformer-XL and XLNet to human-written textual references. Studying inter-metric correlation along with metric ranking reveals interesting insights \u2013 the high correlation between the readability scores and word usage in the text. A study of the statistical significance and empirical evaluations between the scores (human and machine-generated) at higher sampling hyperparameter combinations ( $t=\\{0.75, 1.0\\}$ , $k=\\{100, 150, 250\\}$ ) reveal that the top pre-trained and fine-tuned models generated samples condition well on the prompt with an increased occurrence of unique and difficult words. The GPT2-medium model fine-tuned on the 1024 Byte-pair Encoding (BPE) tokenized version of the dataset along with pre-trained Transformer-XL models generated samples close to human written content on three metrics: prompt-based overlap, coherence, and variation in sentence length. A study of overall model stability and performance shows that fine-tuned GPT2 language models have the least deviation in metric scores from human performance.",
    "authors": [
        "Avisha Das",
        "Rakesh M. Verma"
    ],
    "venue": "IEEE Access",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An in-depth comparison of several language models for open-ended story generation from given prompts shows that fine-tuned GPT2 language models have the least deviation in metric scores from human performance."
    },
    "citationCount": 14,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}