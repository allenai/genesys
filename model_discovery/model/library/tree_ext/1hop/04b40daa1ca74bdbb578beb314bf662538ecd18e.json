{
    "acronym": "04b40daa1ca74bdbb578beb314bf662538ecd18e",
    "title": "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders",
    "seed_ids": [
        "transformerxl",
        "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3"
    ],
    "s2id": "04b40daa1ca74bdbb578beb314bf662538ecd18e",
    "abstract": "Pre-trained text encoders have drawn sustaining attention in natural language processing (NLP) and shown their capability in obtaining promising results in different tasks. Recent studies illustrated that external self-supervised signals (or knowledge extracted by unsupervised learning, such as n-grams) are beneficial to provide useful semantic evidence for understanding languages such as Chinese, so as to improve the performance on various downstream tasks accordingly. To further enhance the encoders, in this paper, we propose to pre-train n-gram-enhanced encoders with a large volume of data and advanced techniques for training. Moreover, we try to extend the encoder to different languages as well as different domains, where it is confirmed that the same architecture is applicable to these varying circumstances and new state-of-the-art performance is observed from a long list of NLP tasks across languages and domains.",
    "authors": [
        "Yan Song",
        "Tong Zhang",
        "Yonggang Wang",
        "Kai-Fu Lee"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes to pre-train n-gram-enhanced encoders with a large volume of data and advanced techniques for training and extends the encoder to different languages as well as different domains, where it is confirmed that the same architecture is applicable to these varying circumstances."
    },
    "citationCount": 37,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}