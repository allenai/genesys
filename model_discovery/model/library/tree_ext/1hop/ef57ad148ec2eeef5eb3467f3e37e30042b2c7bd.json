{
    "acronym": "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
    "title": "Generating Representative Headlines for News Stories",
    "seed_ids": [
        "memcompress",
        "7cc730da554003dda77796d2cb4f06da5dfd5592",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
    "abstract": "Millions of news articles are published online every day, which can be overwhelming for readers to follow. Grouping articles that are reporting the same event into news stories is a common way of assisting readers in their news consumption. However, it remains a challenging research problem to efficiently and effectively generate a representative headline for each story. Automatic summarization of a document set has been studied for decades, while few studies have focused on generating representative headlines for a set of articles. Unlike summaries, which aim to capture most information with least redundancy, headlines aim to capture information jointly shared by the story articles in short length and exclude information specific to each individual article. In this work, we study the problem of generating representative headlines for news stories. We develop a distant supervision approach to train large-scale generation models without any human annotation. The proposed approach centers on two technical components. First, we propose a multi-level pre-training framework that incorporates massive unlabeled corpus with different quality-vs.-quantity balance at different levels. We show that models trained within the multi-level pre-training framework outperform those only trained with human-curated corpus. Second, we propose a novel self-voting-based article attention layer to extract salient information shared by multiple articles. We show that models that incorporate this attention layer are robust to potential noises in news stories and outperform existing baselines on both clean and noisy datasets. We further enhance our model by incorporating human labels, and show that our distant supervision approach significantly reduces the demand on labeled data. Finally, to serve the research community, we publish the first manually curated benchmark dataset on headline generation for news stories, NewSHead, which contains 367K stories (each with 3-5 articles), 6.5 times larger than the current largest multi-document summarization dataset.",
    "authors": [
        "Xiaotao Gu",
        "Yuning Mao",
        "Jiawei Han",
        "Jialu Liu",
        "Hongkun Yu",
        "You Wu",
        "Cong Yu",
        "Daniel Finnie",
        "Jiaqi Zhai",
        "Nicholas Zukoski"
    ],
    "venue": "The Web Conference",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The first manually curated benchmark dataset on headline generation for news stories, NewSHead, is published, which contains 367K stories, 6.5 times larger than the current largest multi-document summarization dataset."
    },
    "citationCount": 59,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}