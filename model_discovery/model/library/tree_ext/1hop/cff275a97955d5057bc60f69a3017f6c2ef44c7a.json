{
    "acronym": "cff275a97955d5057bc60f69a3017f6c2ef44c7a",
    "title": "Benchmarking Neural Decoding Backbones towards Enhanced On-edge iBCI Applications",
    "seed_ids": [
        "mamba",
        "rwkv4",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51"
    ],
    "s2id": "cff275a97955d5057bc60f69a3017f6c2ef44c7a",
    "abstract": "Traditional invasive Brain-Computer Interfaces (iBCIs) typically depend on neural decoding processes conducted on workstations within laboratory settings, which prevents their everyday usage. Implementing these decoding processes on edge devices, such as the wearables, introduces considerable challenges related to computational demands, processing speed, and maintaining accuracy. This study seeks to identify an optimal neural decoding backbone that boasts robust performance and swift inference capabilities suitable for edge deployment. We executed a series of neural decoding experiments involving nonhuman primates engaged in random reaching tasks, evaluating four prospective models, Gated Recurrent Unit (GRU), Transformer, Receptance Weighted Key Value (RWKV), and Selective State Space model (Mamba), across several metrics: single-session decoding, multi-session decoding, new session fine-tuning, inference speed, calibration speed, and scalability. The findings indicate that although the GRU model delivers sufficient accuracy, the RWKV and Mamba models are preferable due to their superior inference and calibration speeds. Additionally, RWKV and Mamba comply with the scaling law, demonstrating improved performance with larger data sets and increased model sizes, whereas GRU shows less pronounced scalability, and the Transformer model requires computational resources that scale prohibitively. This paper presents a thorough comparative analysis of the four models in various scenarios. The results are pivotal in pinpointing an optimal backbone that can handle increasing data volumes and is viable for edge implementation. This analysis provides essential insights for ongoing research and practical applications in the field.",
    "authors": [
        "Zhou Zhou",
        "Guohang He",
        "Zheng Zhang",
        "Luziwei Leng",
        "Qinghai Guo",
        "Jianxing Liao",
        "Xuan Song",
        "Ran Cheng"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A thorough comparative analysis of the four prospective models, Gated Recurrent Unit, Transformer, Receptance Weighted Key Value, and Selective State Space model, indicates that although the GRU model delivers sufficient accuracy, the RWKV and Mamba models are preferable due to their superior inference and calibration speeds."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}