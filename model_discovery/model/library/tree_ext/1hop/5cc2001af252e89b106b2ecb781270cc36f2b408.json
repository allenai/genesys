{
    "acronym": "5cc2001af252e89b106b2ecb781270cc36f2b408",
    "title": "How Long Is Enough? Exploring the Optimal Intervals of Long-Range Clinical Note Language Modeling",
    "seed_ids": [
        "longformer",
        "transformerxl",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5cc2001af252e89b106b2ecb781270cc36f2b408",
    "abstract": "Large pre-trained language models (LMs) have been widely adopted in biomedical and clinical domains, introducing many powerful LMs such as bio-lm and BioELECTRA. However, the applicability of these methods to real clinical use cases is hindered, due to the limitation of pre-trained LMs in processing long textual data with thousands of words, which is a common length for a clinical note. In this work, we explore long-range adaptation from such LMs with Longformer, allowing the LMs to capture longer clinical notes context. We conduct experiments on three n2c2 challenges datasets and a longitudinal clinical dataset from Hong Kong Hospital Authority electronic health record (EHR) system to show the effectiveness and generalizability of this concept, achieving ~10% F1-score improvement. Based on our experiments, we conclude that capturing a longer clinical note interval is beneficial to the model performance, but there are different cut-off intervals to achieve the optimal performance for different target variables.",
    "authors": [
        "Samuel Cahyawijaya",
        "Bryan Wilie",
        "Holy Lovenia",
        "Huang Zhong",
        "Mingqian Zhong",
        "Yuk-Yu Nancy Ip",
        "Pascale Fung"
    ],
    "venue": "International Workshop on Health Text Mining and Information Analysis",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work explores long-range adaptation from large pre-trained language models with Longformer, allowing the LMs to capture longer clinical notes context and concludes that capturing a longer clinical note interval is beneficial to the model performance."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}