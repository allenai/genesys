{
    "acronym": "6073b70ccda321f80bec80eaad01765fa8381419",
    "title": "TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings",
    "seed_ids": [
        "gpt2",
        "bert",
        "diffusionlm",
        "diffuseq",
        "selfcondembdiffu",
        "1206b05eae5a06ba662ae79fb291b50e359c4f42",
        "4c26a1c6e573c772e1634b1747d7e9a3b33dce52",
        "54b6e5dcef733c151adef0ac06430f63cb301a36",
        "020a50f6a7154850ac81e3cde69ad8198ded6751",
        "a1186d7d9a9ef258c76afef1177e4f348061a537",
        "bb7e779c9360a94dd2779c2468fe06b82de7af59",
        "2c6ac935c826002976722ca8d3319f691975687e",
        "0b9770a377b3f96cef9f268cee1791d39a0d4893",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "492a655a67e6ec7423a968cedb70eec0cdbc8e98",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6073b70ccda321f80bec80eaad01765fa8381419",
    "abstract": "Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority over existing non-autoregressive models.",
    "authors": [
        "Alexander Shabalin",
        "Viacheslav Meshchaninov",
        "Tingir Badmaev",
        "Dmitry Molchanov",
        "Grigory Bartosh",
        "Sergey Markov",
        "Dmitry Vetrov"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel approach named Text Encoding Diffusion Model (TEncDM), which trains the model in the space of the language model encodings, and proposes to use a Transformer-based decoder that utilizes contextual information for text reconstruction."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}