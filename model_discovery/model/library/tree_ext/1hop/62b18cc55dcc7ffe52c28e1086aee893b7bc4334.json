{
    "acronym": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "seed_ids": [
        "mamba",
        "retnet",
        "deltanet",
        "tnn",
        "rfa",
        "lineartransformer",
        "based",
        "flashattn",
        "flash",
        "hyenadistill",
        "1d4c48335d841014d0145256c3c4e7f6c426b8fb",
        "46732358e98ce6be0c564ae11f71d556a64b4c35",
        "157ed5647da39a7f5d33a84a90414b2a9e97e301",
        "f4a0c4154203808f362e4678f3741b3d317fdc82",
        "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
        "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "3e8d4062ec4353ff2701c7769336dbdb97f8814c",
        "7294c426b8a95975ca932eaf8f700acdd3d950b2",
        "e10ee483325f590b1e139dfafdb03edeb2e1766a",
        "ade22704be8a0fc3730d320cc7934b2ccbcd97e4",
        "5c104f905fcacf390270f619f232a2ba4eb873f2",
        "434d751d355d7a7c20efa570e785c76286245e77",
        "d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797",
        "cb0ac335adda4ceef9987cbcbca9129e71c37f0a",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "d2d0371158803df93a249c9f7237ffd79b875816",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "54155c2977a977bf129849455dcae3a2b79b3f41",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "a128b1c47e6842605fb95bceae930d2135fc38fc",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "f6d8beb02771791d628f7e0773d8906261ce707c",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
        "e2ee883fca5f8f32a1dfa2dc06c742d57f2c38b9",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "e0cbbca02b332f398c6639b3bea0613f79166220",
        "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
        "16e623059ffccab60f4c35be028a2d4f10933515",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "f51497f463566581874c941353dd9d80069c5b77",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "88695b5bb6462872ce1dd946cff00dd6ebabf2d9",
        "dc48bc1a4d81e0f37603013fd2a95644dc233bd0",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28"
    ],
    "s2id": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
    "abstract": "Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.",
    "authors": [
        "Songlin Yang",
        "Bailin Wang",
        "Yikang Shen",
        "Rameswar Panda",
        "Yoon Kim"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments."
    },
    "citationCount": 44,
    "influentialCitationCount": 9,
    "code": null,
    "description": null,
    "url": null
}