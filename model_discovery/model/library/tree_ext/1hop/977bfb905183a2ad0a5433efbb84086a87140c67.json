{
    "acronym": "977bfb905183a2ad0a5433efbb84086a87140c67",
    "title": "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models",
    "seed_ids": [
        "gpt",
        "7c7f7f191aa7ceb083e7f7295f89ce400192220f",
        "5d74e5fe624aa81c94a6ecf74436fce214ebd0b2",
        "6cfd71d6f3cbe63a0af95b0622a6dd7387ab6acf",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "977bfb905183a2ad0a5433efbb84086a87140c67",
    "abstract": "With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slow-downs or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and are difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer , designed to strike an optimal balance between performance and efficiency in PPI for Trans-former models. By implementing knowledge distillation techniques, we successfully eliminate the high-cost exponential and maximum operations in PPI without sacrificing model performance. Additionally, we have developed a suite of efficient SMPC protocols that utilize segmented polynomials and Goldschmidt\u2019s method to handle other complex nonlinear functions within PPI, such as GeLU, LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer in performance, showing improvements of 5 . 6% and 24 . 2% for BERT BASE and BERT LARGE , respectively. In terms of efficiency, SecFormer is 3.4 and 3.2 times faster than Puma , demonstrating its effectiveness and speed",
    "authors": [
        "Jinglong Luo",
        "Yehong Zhang",
        "Jiaqi Zhang",
        "Xin Mu",
        "Wendy Hui Wang",
        "Yue Yu",
        "Zenglin Xu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An advanced optimization framework called SecFormer is introduced, designed to strike an optimal balance between performance and efficiency in PPI for Trans-former models, and a suite of efficient SMPC protocols that utilize segmented polynomials and Goldschmidt\u2019s method to handle other complex nonlinear functions within PPI."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}