{
    "acronym": "2235420149c8f839e29d4cf8df1894d7a9f3de37",
    "title": "Multilingual Large Language Models and Curse of Multilinguality",
    "seed_ids": [
        "gpt3",
        "bert",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "2235420149c8f839e29d4cf8df1894d7a9f3de37",
    "abstract": "Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.",
    "authors": [
        "Daniil Gurgurov",
        "Tanja B\u00e4umel",
        "Tatiana Anikina"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects, and explores the unique features of different model types: encoder-only, decoder-only, and encoder-decoder models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}