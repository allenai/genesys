{
    "acronym": "5d8fec20573bc89ec72151277974919c85d8bdfd",
    "title": "Position Interpolation Improves ALiBi Extrapolation",
    "seed_ids": [
        "pi",
        "44b7adbd196e69c8771734aa8c9af5fd69c04370",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9dc624d7258d1a56117ca720aea953ce46b66b21"
    ],
    "s2id": "5d8fec20573bc89ec72151277974919c85d8bdfd",
    "abstract": "Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.",
    "authors": [
        "Faisal Al-Khateeb",
        "Nolan Dey",
        "Daria Soboleva",
        "Joel Hestness"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi) and finds position interpolations significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}