{
    "acronym": "e2f8864c3e40298513ca320de0012818ce092bea",
    "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
    "seed_ids": [
        "gpt2",
        "91b95b98cc1a7e974e62d0b8295d3b974b94aa0e",
        "33be243ac9dd8723e6267dea45fd6a6172d4f6a5",
        "fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "0e3d6a7c9c04cf3ba9c902724548846a5ade04b4",
        "e4f82c0a13cae6739239ae0c25a554b6daff35af",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "20db5ac6e88e2457c82856354a2e5d521482f360",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e2f8864c3e40298513ca320de0012818ce092bea",
    "abstract": "Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.",
    "authors": [
        "Gyeongman Kim",
        "Doohyuk Jang",
        "Eunho Yang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 2,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}