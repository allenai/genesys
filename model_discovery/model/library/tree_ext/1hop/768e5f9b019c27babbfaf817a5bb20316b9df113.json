{
    "acronym": "768e5f9b019c27babbfaf817a5bb20316b9df113",
    "title": "Streaming Transformer-based Acoustic Models Using Self-attention with Augmented Memory",
    "seed_ids": [
        "transformerxl",
        "09e2c7adbed37440d4a339852cfa34e5b660f768",
        "f51497f463566581874c941353dd9d80069c5b77",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "768e5f9b019c27babbfaf817a5bb20316b9df113",
    "abstract": "Transformer-based acoustic modeling has achieved great suc-cess for both hybrid and sequence-to-sequence speech recogni-tion. However, it requires access to the full sequence, and thecomputational cost grows quadratically with respect to the in-put sequence length. These factors limit its adoption for stream-ing applications. In this work, we proposed a novel augmentedmemory self-attention, which attends on a short segment of theinput sequence and a bank of memories. The memory bankstores the embedding information for all the processed seg-ments. On the librispeech benchmark, our proposed methodoutperforms all the existing streamable transformer methods bya large margin and achieved over 15% relative error reduction,compared with the widely used LC-BLSTM baseline. Our find-ings are also confirmed on some large internal datasets.",
    "authors": [
        "Chunyang Wu",
        "Yongqiang Wang",
        "Yangyang Shi",
        "Ching-feng Yeh",
        "Frank Zhang"
    ],
    "venue": "Interspeech",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposed a novel augmentedmemory self-attention, which attends on a short segment of the input sequence and a bank of memories, which stores the embedding information for all the processed seg-ments."
    },
    "citationCount": 54,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}