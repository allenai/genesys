{
    "acronym": "a6e08a27da2e1436a9d6e6fb42bb352589a56bbe",
    "title": "Modality Translation through Conditional Encoder-Decoder",
    "seed_ids": [
        "classfreediffu",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d"
    ],
    "s2id": "a6e08a27da2e1436a9d6e6fb42bb352589a56bbe",
    "abstract": "With the recent rise of multi-modal learning, many of the developed models are task-specific, and, as a consequence, lack generalizability when applied to other types of down-stream tasks. One of the representative models that overcomes this issue of generalizability is CLIP, which attacks downstream tasks using cosine similarity metric. However, CLIP has shown relatively low cosine similarity between text and image vector representations. For this reason, we aim to develop a new approach that more accurately maps the hyperplanes of text and image embeddings, and thus, achieves a high-quality text-image modality translation. To this end, we propose a new conditional encoder-decoder model that maps a latent space of one modality given another modality as a condition. We observe that our model is a general method that can be used with various latent encoders and decoders, which are not limited to multi-modal models. Experiments show that conditional encoder-decoder achieves comparable results with the previous state-of-the-art on several downstream tasks.",
    "authors": [
        "Hyunsoo Lee",
        "Yoonsang Lee",
        "Maria Pak",
        "Jinri Kim"
    ],
    "venue": "",
    "year": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new conditional encoder-decoder model is proposed that maps a latent space of one modality given another modality as a condition, and can be used with various latent encoders and decoders, which are not limited to multi-modal models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}