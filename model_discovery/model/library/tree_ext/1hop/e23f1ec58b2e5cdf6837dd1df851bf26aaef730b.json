{
    "acronym": "e23f1ec58b2e5cdf6837dd1df851bf26aaef730b",
    "title": "nanoT5: Fast & Simple Pre-training and Fine-tuning of T5 Models with Limited Resources",
    "seed_ids": [
        "flashattn",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d",
        "5e52d654fd31f04c1bd884cd5480e6af8c95ad50",
        "87c5b281fa43e6f27191b20a8dd694eda1126336"
    ],
    "s2id": "e23f1ec58b2e5cdf6837dd1df851bf26aaef730b",
    "abstract": "State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community\u2019s demand for more user-friendly T5 (Encoder-Decoder) implementations. We make our contributions, including configurations, codebase, pre-training insights, and pre-trained models, available to the public.",
    "authors": [
        "Piotr Nawrot"
    ],
    "venue": "NLPOSS",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "N nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models, is presented, hoping to widen the accessibility to language modelling research and cater to the community\u2019s demand for more user-friendly T5 (Encoder-Decoder) implementations."
    },
    "citationCount": 6,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}