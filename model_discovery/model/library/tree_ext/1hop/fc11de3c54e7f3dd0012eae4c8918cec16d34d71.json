{
    "acronym": "fc11de3c54e7f3dd0012eae4c8918cec16d34d71",
    "title": "Does Transformer Interpretability Transfer to RNNs?",
    "seed_ids": [
        "mamba",
        "edd1dc1e8d7989f36c0e54f69f4aeb5e597edc8b",
        "44b7adbd196e69c8771734aa8c9af5fd69c04370",
        "026b3396a63ed5772329708b7580d633bb86bec9"
    ],
    "s2id": "fc11de3c54e7f3dd0012eae4c8918cec16d34d71",
    "abstract": "Recent advances in recurrent neural network architectures, such as Mamba and RWKV, have enabled RNNs to match or exceed the performance of equal-size transformers in terms of language modeling perplexity and downstream evaluations, suggesting that future systems may be built on completely new architectures. In this paper, we examine if selected interpretability methods originally designed for transformer language models will transfer to these up-and-coming recurrent architectures. Specifically, we focus on steering model outputs via contrastive activation addition, on eliciting latent predictions via the tuned lens, and eliciting latent knowledge from models fine-tuned to produce false outputs under certain conditions. Our results show that most of these techniques are effective when applied to RNNs, and we show that it is possible to improve some of them by taking advantage of RNNs' compressed state.",
    "authors": [
        "Gonccalo Paulo",
        "Thomas Marshall",
        "Nora Belrose"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper examines if selected interpretability methods originally designed for transformer language models will transfer to these up-and-coming recurrent architectures, and focuses on steering model outputs via contrastive activation addition, on eliciting latent predictions via the tuned lens, and eliciting latent knowledge from models fine-tuned to produce false outputs under certain conditions."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}