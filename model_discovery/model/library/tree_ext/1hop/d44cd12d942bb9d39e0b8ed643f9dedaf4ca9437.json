{
    "acronym": "d44cd12d942bb9d39e0b8ed643f9dedaf4ca9437",
    "title": "Temporal Self-Attentional and Adaptive Graph Convolutional Mixed Model for Sleep Staging",
    "seed_ids": [
        "transformer",
        "b0de1d5fe394226cec0a59d783ab739eb52da76f"
    ],
    "s2id": "d44cd12d942bb9d39e0b8ed643f9dedaf4ca9437",
    "abstract": "Evaluating sleep quality through reliable sleep staging is of paramount importance. Although many studies reached fair performances in sleep stage classification, effectively leveraging the spatial\u2013temporal characteristics derived from multichannel brain recordings remains challenging. We develop a novel temporal self-attentional and adaptive graph convolutional mixed model (TS-AGCMM), comprising a feature extraction module (FEM), dynamic time warping (DTW)-based attention module, temporal context module (TCM), and adaptive graph convolutional module (AGCM) in this study. First, the FEM enables capturing representative information from raw data. Then, the DTW-based attention module utilizes a dynamic programming algorithm to enhance the spatial information expression ability of extracted features. The TCM includes multihead attention mechanisms that effectively capture temporal dependencies. In particular, we employ an attention module named normalization-based attention module (NAM), which utilizes the contributing factors of weights to suppress less salient information. Meanwhile, the AGCM can obtain optimal spatial functional connections between polysomnography (PSG) channels, which benefit from the adaptive learning property of the adjacency matrix. Finally, we fuse the temporal and spatial features by concat operation to obtain the prediction results. We utilize the Montreal archive of sleep studies (MASS) and ISRUC-S3 to assess TS-AGCMM. The TS-AGCMM exhibits performance comparable to other currently available approaches as per our results, achieving an accuracy of 89.1% and 81.2%, a macroaveraging F1-score of 84.7% and 79.5%, as well as a Cohen\u2019s kappa coefficient of 83.9% and 75.8% on the two databases, respectively.",
    "authors": [
        "Ziyang Chen",
        "Wenbin Shi",
        "Xianchao Zhang",
        "C. Yeh"
    ],
    "venue": "IEEE Sensors Journal",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel temporal self-attentional and adaptive graph convolutional mixed model (TS-AGCMM), comprising a feature extraction module (FEM), dynamic time warping-based attention module, temporal context module (TCM), and adaptive graph convolutional module (AGCM) is developed in this study."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}