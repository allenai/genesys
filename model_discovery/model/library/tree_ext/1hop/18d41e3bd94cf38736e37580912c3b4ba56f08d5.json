{
    "acronym": "18d41e3bd94cf38736e37580912c3b4ba56f08d5",
    "title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks",
    "seed_ids": [
        "transformerxl",
        "lighdynconv"
    ],
    "s2id": "18d41e3bd94cf38736e37580912c3b4ba56f08d5",
    "abstract": "Attention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms.",
    "authors": [
        "Xizhou Zhu",
        "Dazhi Cheng",
        "Zheng Zhang",
        "Stephen Lin",
        "Jifeng Dai"
    ],
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules, yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding."
    },
    "citationCount": 311,
    "influentialCitationCount": 15,
    "code": null,
    "description": null,
    "url": null
}