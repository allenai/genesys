{
    "acronym": "524ec3ffde9a3dacddf926d136f398aa541fbbf2",
    "title": "Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification",
    "seed_ids": [
        "transformer",
        "bigbird",
        "077f3c382d0dce221cf6aaef0e7185a249b71b9f",
        "786077a54eee75d0fd74b8565f91b9386a6344cd",
        "84daddd294fa3cc12596b5785f81c2a153d2fb1d",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "524ec3ffde9a3dacddf926d136f398aa541fbbf2",
    "abstract": "Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.",
    "authors": [
        "Tengfei Liu",
        "Yongli Hu",
        "Junbin Gao",
        "Yanfeng Sun",
        "Baocai Yin"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The results show that the proposed Hierarchical Multi-modal Transformer outperforms state-of-the-art single-modality and multi-modality methods in cross-modal long document classification."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}