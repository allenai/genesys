{
    "acronym": "207a8328ab05d3a23f4d2882e70819bb1ad91b10",
    "title": "ReadOnce Transformers: Reusable Representations of Text for Transformers",
    "seed_ids": [
        "longformer",
        "e77c3097d7605b1b8d61c13617b6fdceac59f9d8",
        "e7c698bdace380f7183dedbe657686f1885f615c",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "d9f6ada77448664b71128bb19df15765336974a6"
    ],
    "s2id": "207a8328ab05d3a23f4d2882e70819bb1ad91b10",
    "abstract": "We present ReadOnce Transformers, an approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text. The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. This leads to faster training and evaluation of models. Additionally, we extend standard text-to-text transformer models to Representation+Text-to-text models, and evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and long-document summarization. Our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models, while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models.",
    "authors": [
        "Shih-Ting Lin",
        "Ashish Sabharwal",
        "Tushar Khot"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The ReadOnce Transformers approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text, resulting in a 2x-5x speedup compared to standard text-to-text models, and allows existing language models to handle longer documents without the need for designing new pre-trained models."
    },
    "citationCount": 3,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}