{
    "acronym": "708450b22ed062da7fa577e10088f25023b1437c",
    "title": "MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks",
    "seed_ids": [
        "gpt3",
        "31d65e179b1d00484154b3525d93846dd82f23d8",
        "6d23532a1e9a8116041fd5aac6b0ef8ddd6d8171",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "708450b22ed062da7fa577e10088f25023b1437c",
    "abstract": "Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",
    "authors": [
        "Allen Nie",
        "Yuhui Zhang",
        "Atharva Amdekar",
        "Chris Piech",
        "Tatsunori Hashimoto",
        "Tobias Gerstenberg"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work uncovers LLMs implicit tendencies and shows to what extent these align with human intuitions, using a dataset of stories from 24 cognitive science papers and a system to annotate each story with the factors they investigated."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}