{
    "acronym": "0b5a346035f65c598a33e51972ca6de54302d801",
    "title": "Measure More, Question More: Experimental Studies on Transformer-based Language Models and Complement Coercion",
    "seed_ids": [
        "gpt2",
        "5a2263092f49540fd0e049050a96882ff29b00c3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "0b5a346035f65c598a33e51972ca6de54302d801",
    "abstract": "Transformer-based language models have shown strong performance on an array of natural language understanding tasks. However, the question of how these models react to implicit meaning has been largely unexplored. We investigate this using the complement coercion phenomenon, which involves sentences like\"The student finished the book about sailing\"where the action\"reading\"is implicit. We compare LMs' surprisal estimates at various critical sentence regions in sentences with and without implicit meaning. Effects associated with recovering implicit meaning were found at a critical region other than where sentences minimally differ. We then use follow-up experiments to factor out potential confounds, revealing different perspectives that offer a richer and more accurate picture.",
    "authors": [
        "Yuling Gu"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work compares LMs' surprisal estimates at various critical sentence regions in sentences with and without implicit meaning, and finds effects associated with recovering implicit meaning were found at a critical region other than where sentences minimally differ."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}