{
    "acronym": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25",
    "title": "A Tensorized Transformer for Language Modeling",
    "seed_ids": [
        "transformerxl"
    ],
    "s2id": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25",
    "abstract": "Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",
    "authors": [
        "Xindian Ma",
        "Peng Zhang",
        "Shuai Zhang",
        "Nan Duan",
        "Yuexian Hou",
        "D. Song",
        "M. Zhou"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD) with tensor train decomposition is proposed, which can not only largely compress the model parameters but also obtain performance improvements."
    },
    "citationCount": 140,
    "influentialCitationCount": 11,
    "code": null,
    "description": null,
    "url": null
}