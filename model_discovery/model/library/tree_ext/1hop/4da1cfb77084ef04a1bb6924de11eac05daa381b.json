{
    "acronym": "4da1cfb77084ef04a1bb6924de11eac05daa381b",
    "title": "Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation",
    "seed_ids": [
        "gpt2",
        "7eba731a7fd8de712b7b79b5af41a6e2d4dbd191",
        "8cd4054b41936ba0889edc26be8969c3dc8491d8",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4da1cfb77084ef04a1bb6924de11eac05daa381b",
    "abstract": "In this paper, we propose a variational autoencoder with disentanglement priors, VAE-Dprior, for task-specific natural language generation with none or a handful of task-specific labeled examples. In order to tackle compositional generalization across tasks, our model performs disentangled representation learning by introducing a conditional prior for the latent content space and another conditional prior for the latent label space. Both types of priors satisfy a novel property called \\epsilon-disentangled. We show both empirically and theoretically that the novel priors can disentangle representations even without specific regularizations as in the prior work. The content prior enables directly sampling diverse content representations from the content space learned from the seen tasks, and fuse them with the representations of novel tasks for generating semantically diverse texts in the low-resource settings. Our extensive experiments demonstrate the superior performance of our model over competitive baselines in terms of i) data augmentation in continuous zero/few-shot learning, and ii) text style transfer in the few-shot setting.",
    "authors": [
        "Zhuang Li",
        "Lizhen Qu",
        "Qiongkai Xu",
        "Tongtong Wu",
        "Tianyang Zhan",
        "Gholamreza Haffari"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a variational autoencoder with disentanglement priors, VAE-Dprior, for task-specific natural language generation with none or a handful of task- specific labeled examples, and shows both empirically and theoretically that the novel priors can disentangle representations even without specific regularizations as in the prior work."
    },
    "citationCount": 2,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}