{
    "acronym": "8a8dc735939f75d0329926fe3de817203a47cb2f",
    "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
    "seed_ids": [
        "gpt3",
        "16d83e930a4dab2d49f5d276838ddce79df3f787",
        "83edcfbb206ddad38a971d605da09390604248ea",
        "a214eea51a026b048a52949042ef8afd7f3a1715",
        "017cfe49fce1cb77bda2e1791f7203f5ebac0ac7",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "7a15950dc71079285a4eaf195de5aadd87c41b40"
    ],
    "s2id": "8a8dc735939f75d0329926fe3de817203a47cb2f",
    "abstract": "State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.",
    "authors": [
        "Shreyas Chaudhari",
        "Pranjal Aggarwal",
        "Vishvak Murahari",
        "Tanmay Rajpurohit",
        "A. Kalyan",
        "Karthik Narasimhan",
        "A. Deshpande",
        "Bruno Castro da Silva"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward and improves the understanding of the role of reward models and methods for their training."
    },
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}