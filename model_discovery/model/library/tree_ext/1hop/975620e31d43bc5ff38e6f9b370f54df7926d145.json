{
    "acronym": "975620e31d43bc5ff38e6f9b370f54df7926d145",
    "title": "CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model",
    "seed_ids": [
        "bert",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "975620e31d43bc5ff38e6f9b370f54df7926d145",
    "abstract": "Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.",
    "authors": [
        "Shang-Hsuan Chiang",
        "Ssu-Cheng Wang",
        "Yao-Chung Fan"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper investigates cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation and shows that the PLM-enhanced model brings a substantial performance improvement."
    },
    "citationCount": 9,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}