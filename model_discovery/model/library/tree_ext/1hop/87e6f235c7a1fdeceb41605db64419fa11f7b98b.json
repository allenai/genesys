{
    "acronym": "87e6f235c7a1fdeceb41605db64419fa11f7b98b",
    "title": "Couplformer: Rethinking Vision Transformer with Coupling Attention Map",
    "seed_ids": [
        "linformer",
        "reformer",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "b1c39d042fdf8f00a407b0df734764beb6c3b062",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "87e6f235c7a1fdeceb41605db64419fa11f7b98b",
    "abstract": "With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.",
    "authors": [
        "Hai Lan",
        "Xihao Wang",
        "Xian Wei"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel memory economy attention mechanism named Couplformer is proposed, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information and can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}