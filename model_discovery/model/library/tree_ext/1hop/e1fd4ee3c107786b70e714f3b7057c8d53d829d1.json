{
    "acronym": "e1fd4ee3c107786b70e714f3b7057c8d53d829d1",
    "title": "A Data Set for the Analysis of Text Quality Dimensions in Summarization Evaluation",
    "seed_ids": [
        "memcompress"
    ],
    "s2id": "e1fd4ee3c107786b70e714f3b7057c8d53d829d1",
    "abstract": "Automatic evaluation of summarization focuses on developing a metric to represent the quality of the resulting text. However, text qualityis represented in a variety of dimensions ranging from grammaticality to readability and coherence. In our work, we analyze the depen-dencies between a variety of quality dimensions on automatically created multi-document summaries and which dimensions automaticevaluation metrics such as ROUGE, PEAK or JSD are able to capture. Our results indicate that variants of ROUGE are correlated tovarious quality dimensions and that some automatic summarization methods achieve higher quality summaries than others with respectto individual summary quality dimensions. Our results also indicate that differentiating between quality dimensions facilitates inspectionand fine-grained comparison of summarization methods and its characteristics. We make the data from our two summarization qualityevaluation experiments publicly available in order to facilitate the future development of specialized automatic evaluation methods.",
    "authors": [
        "Margot Mieskes",
        "E. Menc\u00eda",
        "Tim Kronsbein"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work analyzes the depen-dencies between a variety of quality dimensions on automatically created multi-document summaries and which dimensions automaticevaluation metrics such as ROUGE, PEAK or JSD are able to capture."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}