{
    "acronym": "6533d1a7fb4dfe9241d7e6cc6ff7ee6751e0cc03",
    "title": "TransformerFTC: Scaling Low-Dimensional Transformers for Higher Performance",
    "seed_ids": [
        "transformerxl",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "f51497f463566581874c941353dd9d80069c5b77",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "6533d1a7fb4dfe9241d7e6cc6ff7ee6751e0cc03",
    "abstract": "Transformers have achieved high performance in neural machine translation (NMT), with deep models\u2014those with numerous layers\u2014being the most successful. However, such models are not only slow to train but are also computationally expensive. In this work, we present T RANSFORMER FTC, a Transformer architecture that utilizes subsampling methods which allow for training deep Transformers at a lower computational cost. Speci\ufb01cally, the Transformer layers are divided into a series of blocks , with each block operating over a pooled sequence of shorter length ( time compression ) and/or of a lower feature dimension ( feature compression ). Validating on the IWSLT\u201914 German-English dataset, we achieve performance equivalent to a vanilla Transformer with a 25% training speedup. Our results suggest that compressing the upper layers of a Transformer are a promising strategy for model ef\ufb01ciency.",
    "authors": [
        "V. Krishna"
    ],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents T RANSFORMER FTC, a Transformer architecture that utilizes subsampling methods which allow for training deep Transformers at a lower computational cost and suggests that compressing the upper layers of a Transformer are a promising strategy for model ef\ufb01ciency."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}