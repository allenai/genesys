{
    "acronym": "e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286",
    "title": "A Survey on Contextual Embeddings",
    "seed_ids": [
        "gpt",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "d56c1fc337fb07ec004dc846f80582c327af717c",
        "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "e2587eddd57bc4ba286d91b27c185083f16f40ee",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
        "f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286",
    "abstract": "Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.",
    "authors": [
        "Qi Liu",
        "Matt J. Kusner",
        "Phil Blunsom"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "In this survey, existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses are reviewed."
    },
    "citationCount": 131,
    "influentialCitationCount": 8,
    "code": null,
    "description": null,
    "url": null
}