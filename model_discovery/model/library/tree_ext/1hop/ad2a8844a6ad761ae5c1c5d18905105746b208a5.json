{
    "acronym": "ad2a8844a6ad761ae5c1c5d18905105746b208a5",
    "title": "Finetuning Transformer Models to Build ASAG System",
    "seed_ids": [
        "gpt",
        "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "96a3461e5f05973db7aec78afdb763304bd2e195",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ad2a8844a6ad761ae5c1c5d18905105746b208a5",
    "abstract": "Research towards creating systems for automatic grading of student answers to quiz and exam questions in educational settings has been ongoing since 1966 (Burrows et al. 2015). Over the years, the problem was divided into many categories. Among them, grading text answers were divided into short answer grading, and essay grading. The goal of this work was to develop an ML-based short answer grading system. I hence built a system which uses finetuning on Roberta Large Model pretrained on STS benchmark dataset and have also created an interface to show the production readiness of the system. I evaluated the performance of the system on the Mohler extended dataset (2011) and SciEntsBank Dataset (converted). The developed system achieved a Pearson\u2019s Correlation of 0.82 and RMSE of 0.7 on the Mohler Dataset which beats the state-of-the-art performance on this dataset (correlation of 0.805 and RMSE of 0.793). Additionally, Pearson\u2019s Correlation of 0.79 and RMSE of 0.56 was achieved on the SciEntsBank Dataset, which only reconfirms the robustness of the system. A few observations during achieving these results included usage of batch size of 1 produced better results than using batch size of 16/32 and using huber loss as loss function performed well on this regression task. The system was tried and tested on train and validation splits using various random seeds and still has been tweaked to achieve a minimum of 0.76 of correlation and 15% RMSE on any dataset. The code for the system has been saved at https://github.com/mithunthakkar26/NLP-Projects",
    "authors": [
        "Mithun Thakkar"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The goal of this work was to develop an ML-based short answer grading system which uses finetuning on Roberta Large Model pretrained on STS benchmark dataset and also created an interface to show the production readiness of the system."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}