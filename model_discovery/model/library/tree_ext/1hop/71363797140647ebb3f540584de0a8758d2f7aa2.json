{
    "acronym": "71363797140647ebb3f540584de0a8758d2f7aa2",
    "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision",
    "seed_ids": [
        "gmlp",
        "f75cddf2d42ed01b34686704eb3504becef67442",
        "c38ba200201655f8cb9594cddd52daa8b6f4f335",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "71363797140647ebb3f540584de0a8758d2f7aa2",
    "abstract": "An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper. Different from MLP-Mixer, where the global spatial feature is encoded for information flow through matrix transposition and one token-mixing MLP, we pay more attention to the local features interaction. By axially shifting channels of the feature map, AS-MLP is able to obtain the information flow from different axial directions, which captures the local dependencies. Such an operation enables us to utilize a pure MLP architecture to achieve the same local receptive field as CNN-like architecture. We can also design the receptive field size and dilation of blocks of AS-MLP, etc, in the same spirit of convolutional neural networks. With the proposed AS-MLP architecture, our model obtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the ImageNet-1K dataset. Such a simple yet effective architecture outperforms all MLP-based architectures and achieves competitive performance compared to the transformer-based architectures (e.g., Swin Transformer) even with slightly lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be applied to the downstream tasks (e.g., object detection and semantic segmentation). The experimental results are also impressive. Our proposed AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the ADE20K dataset, which is competitive compared to the transformer-based architectures. Our AS-MLP establishes a strong baseline of MLP-based architecture. Code is available at https://github.com/svip-lab/AS-MLP.",
    "authors": [
        "Dongze Lian",
        "Zehao Yu",
        "Xing Sun",
        "Shenghua Gao"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An Axial Shifted MLP architecture (AS-MLP) is proposed, which is the first MLP-based architecture to be applied to the downstream tasks and achieves competitive performance compared to the transformer-based architectures even with slightly lower FLOPs."
    },
    "citationCount": 161,
    "influentialCitationCount": 22,
    "code": null,
    "description": null,
    "url": null
}