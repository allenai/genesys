{
    "acronym": "2b5e40c9c6c76569714b902a53838cb80ce89a26",
    "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
    "seed_ids": [
        "gpt2",
        "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
        "d28c18a3c2a0afdc0a8634d18345af8d36e1f948",
        "68e686817f2c33cd09ba3805fa082348f18affd9",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2b5e40c9c6c76569714b902a53838cb80ce89a26",
    "abstract": "The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known\"power phase\"but also the previously unobserved\"pre-power phase\". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of\"pre-learned data size\"into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io.",
    "authors": [
        "Haowei Lin",
        "Baizhou Huang",
        "Haotian Ye",
        "Qinyu Chen",
        "Zihao Wang",
        "Sujian Li",
        "Jianzhu Ma",
        "Xiaojun Wan",
        "James Zou",
        "Yitao Liang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work forms this resource-constrained selection task into predicting fine-tuning performance and introduces the concept of \"pre-learned data size\" into the Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}