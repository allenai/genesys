{
    "acronym": "735c3eff45b5cabcd9efa2c0451a8fd36f840769",
    "title": "UnitNorm: Rethinking Normalization for Transformers in Time Series",
    "seed_ids": [
        "transformer",
        "d078b6e88bc4749f4eb83f289537a88b4aaf54e6",
        "892397050de6f68c6ea11e9ed3fd091e42aa34f4",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a"
    ],
    "s2id": "735c3eff45b5cabcd9efa2c0451a8fd36f840769",
    "abstract": "Normalization techniques are crucial for enhancing Transformer models' performance and stability in time series analysis tasks, yet traditional methods like batch and layer normalization often lead to issues such as token shift, attention shift, and sparse attention. We propose UnitNorm, a novel approach that scales input vectors by their norms and modulates attention patterns, effectively circumventing these challenges. Grounded in existing normalization frameworks, UnitNorm's effectiveness is demonstrated across diverse time series analysis tasks, including forecasting, classification, and anomaly detection, via a rigorous evaluation on 6 state-of-the-art models and 10 datasets. Notably, UnitNorm shows superior performance, especially in scenarios requiring robust attention mechanisms and contextual comprehension, evidenced by significant improvements by up to a 1.46 decrease in MSE for forecasting, and a 4.89% increase in accuracy for classification. This work not only calls for a reevaluation of normalization strategies in time series Transformers but also sets a new direction for enhancing model performance and stability. The source code is available at https://anonymous.4open.science/r/UnitNorm-5B84.",
    "authors": [
        "Nan Huang",
        "C. K\u00fcmmerle",
        "Xiang Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Grounded in existing normalization frameworks, UnitNorm is proposed, a novel approach that scales input vectors by their norms and modulates attention patterns, effectively circumventing challenges in time series Transformers."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}