{
    "acronym": "78bb909c314784ff345fe2bea997e74ca08ee0e5",
    "title": "A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture",
    "seed_ids": [
        "blockbert",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "78bb909c314784ff345fe2bea997e74ca08ee0e5",
    "abstract": "Transformer-based language models have become the de-facto standard model for various natural language processing (NLP) applications given the superior algorithmic performances. Processing a transformer-based language model on a conventional accelerator induces the memory wall problem, and the ReRAM-based accelerator is a promising solution to this problem. However, due to the characteristics of the self-attention mechanism and the ReRAM-based accelerator, the pipeline hazard arises when processing the transformer-based language model on the ReRAM-based accelerator. This hazard issue greatly increases the overall execution time. In this article, we propose a framework to resolve the hazard issue. First, we propose the concept of window self-attention to reduce the attention computation scope by analyzing the properties of the self-attention mechanism. After that, we present a window-size search algorithm, which finds an optimal window size set according to the target application/algorithmic performance. We also suggest a hardware design that exploits the advantages of the proposed algorithm optimization on the general ReRAM-based accelerator. The proposed work successfully alleviates the hazard issue while maintaining the algorithmic performance, leading to a $5.8\\times $ speedup over the provisioned baseline. It also delivers up to $39.2\\times /643.2\\times $ speedup/higher energy efficiency over GPU, respectively.",
    "authors": [
        "Myeonggu Kang",
        "Hyein Shin",
        "L. Kim"
    ],
    "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article proposes the concept of window self-attention to reduce the attention computation scope by analyzing the properties of the self-ATTention mechanism, and presents a window-size search algorithm, which finds an optimal window size set according to the target application/algorithmic performance."
    },
    "citationCount": 9,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}