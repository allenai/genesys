{
    "acronym": "88051a6dce3b67541d8096647da2f6d31daa9e9a",
    "title": "Latent Relation Language Models",
    "seed_ids": [
        "transformerxl",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "88051a6dce3b67541d8096647da2f6d31daa9e9a",
    "abstract": "In this paper, we propose Latent Relation Language Models (LRLMs), a class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations. This model has a number of attractive properties: it not only improves language modeling performance, but is also able to annotate the posterior probability of entity spans for a given text through relations. Experiments demonstrate empirical improvements over both word-based language models and a previous approach that incorporates knowledge graph information. Qualitative analysis further demonstrates the proposed model's ability to learn to predict appropriate relations in context. 1",
    "authors": [
        "Hiroaki Hayashi",
        "Zecong Hu",
        "Chenyan Xiong",
        "Graham Neubig"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations is proposed, able to annotate the posterior probability of entity spans for a given text through relations."
    },
    "citationCount": 36,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}