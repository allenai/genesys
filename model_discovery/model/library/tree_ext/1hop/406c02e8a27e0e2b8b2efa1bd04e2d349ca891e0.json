{
    "acronym": "406c02e8a27e0e2b8b2efa1bd04e2d349ca891e0",
    "title": "SMR: State Memory Replay for Long Sequence Modeling",
    "seed_ids": [
        "mamba",
        "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
        "661e8d555c4424b5953f17434f2ba910bfcf3afe",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "406c02e8a27e0e2b8b2efa1bd04e2d349ca891e0",
    "abstract": "Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.",
    "authors": [
        "Biqing Qi",
        "Junqi Gao",
        "Kaiyan Zhang",
        "Dong Li",
        "Jianxing Liu",
        "Ligang Wu",
        "Bowen Zhou"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel non-recursive non-uniform sample processing strategy, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data, enables SSMs to stably model varying sampling points."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}