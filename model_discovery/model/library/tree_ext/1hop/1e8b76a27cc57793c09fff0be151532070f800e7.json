{
    "acronym": "1e8b76a27cc57793c09fff0be151532070f800e7",
    "title": "RWKV-CLIP: A Robust Vision-Language Representation Learner",
    "seed_ids": [
        "transformer",
        "4d84441e7f296d42d6493bb5a7b16d36e479b354",
        "51f38bd957fa863022feb5878fa1ba3bea6657cf",
        "026b3396a63ed5772329708b7580d633bb86bec9"
    ],
    "s2id": "1e8b76a27cc57793c09fff0be151532070f800e7",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from websites. This paper further explores CLIP from the perspectives of data and model architecture. To address the prevalence of noisy data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to synthesize and refine content from web-based texts, synthetic captions, and detection tags. Furthermore, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Comprehensive experiments across various model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust and efficient vision-language representation learner, it achieves state-of-the-art performance in several downstream tasks, including linear probe, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP",
    "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Xiang An",
        "Ziyong Feng",
        "Dongnan Liu",
        "Weidong Cai",
        "Jiankang Deng"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "RWKV-CLIP is proposed, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs and achieves state-of-the-art performance in several downstream tasks, including linear probe, zero-shot classification, and zero-shot image-text retrieval."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}