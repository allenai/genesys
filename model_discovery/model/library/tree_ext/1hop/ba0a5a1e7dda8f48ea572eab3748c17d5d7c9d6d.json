{
    "acronym": "ba0a5a1e7dda8f48ea572eab3748c17d5d7c9d6d",
    "title": "LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task",
    "seed_ids": [
        "bert",
        "17455350c5e541e66284765e5463293ce77d790d"
    ],
    "s2id": "ba0a5a1e7dda8f48ea572eab3748c17d5d7c9d6d",
    "abstract": "Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF layer to try to model the inter-dependencies between adjacent utterances more effectively. In the official leaderboard for the task, our architecture was ranked 8th, achieving an F1-score of 0.1759 on the leaderboard.",
    "authors": [
        "Suyash Vardhan Mathur",
        "Akshett Rai Jindal",
        "Hardik Mittal",
        "Manish Shrivastava"
    ],
    "venue": "International Workshop on Semantic Evaluation",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes models that tackle the Multimodal Emotion Cause Analysis in Conversations task as an utterance labeling and a sequence labeling problem and performs a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF layer to try to model the inter-dependencies between adjacent utterances more effectively."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}