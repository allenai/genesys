{
    "acronym": "92f86c59a221752560e72466b2e1026e83bfdc12",
    "title": "A novel approach to attention mechanism using kernel functions: Kerformer",
    "seed_ids": [
        "performer",
        "bigbird",
        "longformer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "dd618e8ece2d2a248487522c987df685258c047b",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "92f86c59a221752560e72466b2e1026e83bfdc12",
    "abstract": "Artificial Intelligence (AI) is driving advancements across various fields by simulating and enhancing human intelligence. In Natural Language Processing (NLP), transformer models like the Kerformer, a linear transformer based on a kernel approach, have garnered success. However, traditional attention mechanisms in these models have quadratic calculation costs linked to input sequence lengths, hampering efficiency in tasks with extended orders. To tackle this, Kerformer introduces a nonlinear reweighting mechanism, transforming maximum attention into feature-based dot product attention. By exploiting the non-negativity and non-linear weighting traits of softmax computation, separate non-negativity operations for Query(Q) and Key(K) computations are performed. The inclusion of the SE Block further enhances model performance. Kerformer significantly reduces attention matrix time complexity from O(N2) to O(N), with N representing sequence length. This transformation results in remarkable efficiency and scalability gains, especially for prolonged tasks. Experimental results demonstrate Kerformer's superiority in terms of time and memory consumption, yielding higher average accuracy (83.39%) in NLP and vision tasks. In tasks with long sequences, Kerformer achieves an average accuracy of 58.94% and exhibits superior efficiency and convergence speed in visual tasks. This model thus offers a promising solution to the limitations posed by conventional attention mechanisms in handling lengthy tasks.",
    "authors": [
        "Yao Gan",
        "Yan-yun Fu",
        "Deyong Wang",
        "Yongming Li"
    ],
    "venue": "Frontiers Neurorobotics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Kerformer introduces a nonlinear reweighting mechanism, transforming maximum attention into feature-based dot product attention, which results in remarkable efficiency and scalability gains, especially for prolonged tasks."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}