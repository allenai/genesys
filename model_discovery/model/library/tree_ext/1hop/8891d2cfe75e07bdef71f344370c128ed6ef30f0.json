{
    "acronym": "8891d2cfe75e07bdef71f344370c128ed6ef30f0",
    "title": "Fairness and Bias in Multimodal AI: A Survey",
    "seed_ids": [
        "gpt2",
        "2f89071c59b71effc434f9eed8697e3152f7acd4",
        "ef4eb30579533e33395472b3cebd3f75432da38d",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "b04493c0865c166d00f65bb78f6476b466bbe512",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8891d2cfe75e07bdef71f344370c128ed6ef30f0",
    "abstract": "The importance of addressing fairness and bias in artificial intelligence (AI) systems cannot be over-emphasized. Mainstream media has been awashed with news of incidents around stereotypes and bias in many of these systems in recent years. In this survey, we fill a gap with regards to the minimal study of fairness and bias in Large Multimodal Models (LMMs) compared to Large Language Models (LLMs), providing 50 examples of datasets and models along with the challenges affecting them; we identify a new category of quantifying bias (preuse), in addition to the two well-known ones in the literature: intrinsic and extrinsic; we critically discuss the various ways researchers are addressing these challenges. Our method involved two slightly different search queries on Google Scholar, which revealed that 33,400 and 538,000 links are the results for the terms\"Fairness and bias in Large Multimodal Models\"and\"Fairness and bias in Large Language Models\", respectively. We believe this work contributes to filling this gap and providing insight to researchers and other stakeholders on ways to address the challenge of fairness and bias in multimodal A!.",
    "authors": [
        "Tosin P. Adewumi",
        "Lama Alkhaled",
        "Namrata Gurung",
        "G. V. Boven",
        "Irene Pagliai"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This survey fills a gap with regards to the minimal study of fairness and bias in Large Multimodal Models (LMMs) compared to Large Language Models (LLMs), providing 50 examples of datasets and models along with the challenges affecting them; identifies a new category of quantifying bias (preuse); and critically discusses the various ways researchers are addressing these challenges."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}