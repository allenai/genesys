{
    "acronym": "a6238191cc43c41a544caadcf8b00947293cf099",
    "title": "Character-centric Story Visualization via Visual Planning and Token Alignment",
    "seed_ids": [
        "gpt2",
        "2d9ae4c167510ed78803735fc57ea67c3cc55a35",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a6238191cc43c41a544caadcf8b00947293cf099",
    "abstract": "Story visualization advances the traditional text-to-image generation by enabling multiple image generation based on a complete story. This task requires machines to 1) understand long text inputs, and 2) produce a globally consistent image sequence that illustrates the contents of the story. A key challenge of consistent story visualization is to preserve characters that are essential in stories. To tackle the challenge, we propose to adapt a recent work that augments VQ-VAE with a text-to-visual-token (transformer) architecture. Specifically, we modify the text-to-visual-token module with a two-stage framework: 1) character token planning model that predicts the visual tokens for characters only; 2) visual token completion model that generates the remaining visual token sequence, which is sent to VQ-VAE for finalizing image generations. To encourage characters to appear in the images, we further train the two-stage framework with a character-token alignment objective. Extensive experiments and evaluations demonstrate that the proposed method excels at preserving characters and can produce higher quality image sequences compared with the strong baselines.",
    "authors": [
        "Hong Chen",
        "Rujun Han",
        "Te-Lin Wu",
        "Hideki Nakayama",
        "Nanyun Peng"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes to adapt a recent work that augments VQ-VAE with a text-to-visual-token (transformer) architecture that excels at preserving characters and can produce higher quality image sequences compared with the strong baselines."
    },
    "citationCount": 16,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}