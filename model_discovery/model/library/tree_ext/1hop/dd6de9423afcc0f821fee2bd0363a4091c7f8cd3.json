{
    "acronym": "dd6de9423afcc0f821fee2bd0363a4091c7f8cd3",
    "title": "Distribution Augmentation for Generative Modeling",
    "seed_ids": [
        "sparsetransformer",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "dd6de9423afcc0f821fee2bd0363a4091c7f8cd3",
    "abstract": "We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmentation functions to data and, importantly, conditions the generative model on the speci\ufb01c function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures and problem domains.",
    "authors": [
        "Heewoo Jun",
        "R. Child",
        "Mark Chen",
        "John Schulman",
        "A. Ramesh",
        "Alec Radford",
        "I. Sutskever"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "DistAug is presented, a simple and powerful method of regularizing generative models and, importantly, conditions the generative model on the speci\ufb01c function used, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning."
    },
    "citationCount": 55,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}