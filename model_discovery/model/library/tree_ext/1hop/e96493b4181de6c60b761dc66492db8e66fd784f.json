{
    "acronym": "e96493b4181de6c60b761dc66492db8e66fd784f",
    "title": "Long Document Summarization with Top-down and Bottom-up Inference",
    "seed_ids": [
        "bigbird",
        "longformer",
        "routingtransformer",
        "24b951275a7a42ef36aca8352caaf6f4cd6238d2",
        "34042e2680e475510a1030b54165a81534ad88d3",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "01b15017ac59b8d6f2ce3598c4a7d6358c211426",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0"
    ],
    "s2id": "e96493b4181de6c60b761dc66492db8e66fd784f",
    "abstract": "Text summarization aims to condense long documents and retain key information. Critical to the success of a summarization model is the faithful inference of latent representations of words or tokens in the source documents. Most recent models infer the latent representations with a transformer encoder, which is purely bottom-up and thus does not capture long-distance context well. Also, self-attention-based models face the challenge of quadratic complexity with respect to sequence length. We propose a method to improve summarization models on these two aspects. Our method assumes a hierarchical latent structure of a document where the top-level captures the long range dependency at a coarser time scale and the bottom token level preserves the details. Critically, our method enables token representations to be updated in both a bottom-up and top-down manner. In the bottom-up pass, token representations are inferred with local self-attention to leverage its efficiency. Top-down correction is then applied to allow tokens to capture global context. We demonstrate the effectiveness on a diverse set of summarization datasets, including narrative, conversational, scientific documents and news. Our model achieves state-of-the-art performance on a wide range of long document summarization benchmarks, compared to recent efficient transformers. We show that our model can summarize an entire book and achieve competitive performance using 0.27% parameters and much less training data, compared to a recent GPT-3-based model. These results indicate the general applicability and benefits of the framework.",
    "authors": [
        "Bo Pang",
        "Erik Nijkamp",
        "Wojciech Kryscinski",
        "S. Savarese",
        "Yingbo Zhou",
        "Caiming Xiong"
    ],
    "venue": "Findings",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This model achieves state-of-the-art performance on a wide range of long document summarization benchmarks, compared to recent efficient transformers, and can summarize an entire book and achieve competitive performance using 0.27% parameters and much less training data."
    },
    "citationCount": 40,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}