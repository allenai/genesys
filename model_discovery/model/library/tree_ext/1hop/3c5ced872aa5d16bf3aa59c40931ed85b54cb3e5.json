{
    "acronym": "3c5ced872aa5d16bf3aa59c40931ed85b54cb3e5",
    "title": "FMamba: Mamba based on Fast-attention for Multivariate Time-series Forecasting",
    "seed_ids": [
        "transformer",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d"
    ],
    "s2id": "3c5ced872aa5d16bf3aa59c40931ed85b54cb3e5",
    "abstract": "In multivariate time-series forecasting (MTSF), extracting the temporal correlations of the input sequences is crucial. While popular Transformer-based predictive models can perform well, their quadratic computational complexity results in inefficiency and high overhead. The recently emerged Mamba, a selective state space model, has shown promising results in many fields due to its strong temporal feature extraction capabilities and linear computational complexity. However, due to the unilateral nature of Mamba, channel-independent predictive models based on Mamba cannot attend to the relationships among all variables in the manner of Transformer-based models. To address this issue, we combine fast-attention with Mamba to introduce a novel framework named FMamba for MTSF. Technically, we first extract the temporal features of the input variables through an embedding layer, then compute the dependencies among input variables via the fast-attention module. Subsequently, we use Mamba to selectively deal with the input features and further extract the temporal dependencies of the variables through the multi-layer perceptron block (MLP-block). Finally, FMamba obtains the predictive results through the projector, a linear layer. Experimental results on eight public datasets demonstrate that FMamba can achieve state-of-the-art performance while maintaining low computational overhead.",
    "authors": [
        "Shusen Ma",
        "Yu Kang",
        "Peng Bai",
        "Yunan Zhao"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work combines fast-attention with Mamba to introduce a novel framework named FMamba for MTSF, which can achieve state-of-the-art performance while maintaining low computational overhead."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}