{
    "acronym": "78a08ff468e1f69bcc6463dad95f9476f3177f48",
    "title": "Manifold-Preserving Transformers are Effective for Short-Long Range Encoding",
    "seed_ids": [
        "lineartransformer",
        "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "ca4ecf116a9b97ce525a01f3f51117877688ddf5",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1"
    ],
    "s2id": "78a08ff468e1f69bcc6463dad95f9476f3177f48",
    "abstract": "Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual information. Transformers usually project tokens onto sparse manifolds and fail to preserve mathematical equivalence among the token representations. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens. We propose a simple alternative to dot-product attention to ensure Lipschitz continuity. This allows TransJect to learn injective mappings to transform token representations to different manifolds with similar topology and preserve Euclidean distance between every pair of tokens in subsequent layers. Evaluations across multiple benchmark short- and long-sequence classification tasks show maximum improvements of 6.8% and 5.9%, respectively, over the variants of Transformers. Additionally, TransJect displays 79% better performance than Transformer on the language modeling task. We further highlight the shortcomings of multi-head self-attention from the statistical physics viewpoint. Although multi-head self-attention was incepted to learn different abstraction levels within the networks, our empirical analyses suggest that different attention heads learn randomly and unorderly. In contrast, TransJect adapts a mixture of experts for regularization; these experts are more orderly and balanced and learn different sparse representations from the input sequences. TransJect exhibits very low entropy and can be efficiently scaled to larger depths.",
    "authors": [
        "Ayan Sengupta",
        "Md. Shad Akhtar",
        "Tanmoy Chakraborty"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens and proposes a simple alternative to dot-product attention to ensure Lipschitz continuity, and highlights the shortcomings of multi-head self-attention from the statistical physics viewpoint."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}