{
    "acronym": "f11044596cf2eaf59f83d82b8167b16ba6a08617",
    "title": "Emergent Agentic Transformer from Chain of Hindsight Experience",
    "seed_ids": [
        "gpt",
        "bfe6fd05f09647b001c7eb6e333a95c881c88344",
        "860bc4f071f35d6d8529a52c2c1858d030779a6a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "f11044596cf2eaf59f83d82b8167b16ba6a08617",
    "abstract": "Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple AI areas. In reinforcement learning (RL), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards. Our method consists of relabelling target return of each trajectory to the maximum total reward among in sequence of trajectories and training an autoregressive model to predict actions conditioning on past states, actions, rewards, target returns, and task completion tokens, the resulting model, Agentic Transformer (AT), can learn to improve upon itself both at training and test time. As we show on D4RL and ExoRL benchmarks, to the best our knowledge, this is the first time that a simple transformer-based model performs competitively with both temporal-difference and imitation-learning-based approaches, even from sub-optimal data. Our Agentic Transformer also shows a promising scaling trend that bigger models consistently improve results.",
    "authors": [
        "Hao Liu",
        "P. Abbeel"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This is the first time that a simple transformer-based model performs competitively with both temporal-difference and imitation-learning-based approaches, even from sub-optimal data, and shows a promising scaling trend that bigger models consistently improve results."
    },
    "citationCount": 19,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}