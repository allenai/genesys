{
    "acronym": "ee1fdc0e4391f2ef7d3222d47d8677c574fcd95c",
    "title": "ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT",
    "seed_ids": [
        "gpt2",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ee1fdc0e4391f2ef7d3222d47d8677c574fcd95c",
    "abstract": "In this paper, we investigate the use of data obtained from prompting a large generative language model, ChatGPT, to generate synthetic training data with the aim of augmenting data in low resource scenarios. We show that with appropriate task-specific ChatGPT prompts, we outperform the most popular existing approaches for such data augmentation. Furthermore, we investigate methodologies for evaluating the similarity of the augmented data generated from ChatGPT with the aim of validating and assessing the quality of the data generated.",
    "authors": [
        "S. Ubani",
        "S. Polat",
        "Rodney D. Nielsen"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that with appropriate task-specific ChatGPT prompts, the use of data obtained from prompting a large generative language model,ChatGPT, to generate synthetic training data with the aim of augmenting data in low resource scenarios outperform the most popular existing approaches for such data augmentation."
    },
    "citationCount": 32,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}