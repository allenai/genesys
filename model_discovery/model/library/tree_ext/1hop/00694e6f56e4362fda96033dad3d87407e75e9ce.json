{
    "acronym": "00694e6f56e4362fda96033dad3d87407e75e9ce",
    "title": "Parameter Sharing Decoder Pair for Auto Composing",
    "seed_ids": [
        "gpt",
        "83b56c3c7a61767bd88d85796aa5dbc4976912c3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "00694e6f56e4362fda96033dad3d87407e75e9ce",
    "abstract": "Auto Composing is an active and appealing research area in the past few years, and lots of efforts have been put into inventing more robust models to solve this problem. With the fast evolution of deep learning techniques, some deep neural network-based language models are becoming dominant. Notably, the transformer structure has been proven to be very efficient and promising in modeling texts. However, the transformer-based language models usually contain huge number of parameters and the size of the model is usually too large to put in production for some storage limited applications. In this paper, we propose a parameter sharing decoder pair (PSDP), which reduces the number of parameters dramatically and at the same time maintains the capability of generating understandable and reasonable compositions. Works created by the proposed model are presented to demonstrate the effectiveness of the model.",
    "authors": [
        "Xu Zhao"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a parameter sharing decoder pair (PSDP), which reduces the number of parameters dramatically and at the same time maintains the capability of generating understandable and reasonable compositions."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}