{
    "acronym": "5d733042a5185a97eb80e4c665eaeaf71f9d1bba",
    "title": "A Dataset for Answering Time-Sensitive Questions",
    "seed_ids": [
        "bigbird",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2"
    ],
    "s2id": "5d733042a5185a97eb80e4c665eaeaf71f9d1bba",
    "abstract": "Time is an important dimension in our physical world. Lots of facts can evolve with respect to time. For example, the U.S. President might change every four years. Therefore, it is important to consider the time dimension and empower the existing QA models to reason over time. However, the existing QA datasets contain rather few time-sensitive questions, hence not suitable for diagnosing or benchmarking the model's temporal reasoning capability. In order to promote research in this direction, we propose to construct a time-sensitive QA dataset. The dataset is constructed by 1) mining time-evolving facts from WikiData and aligning them to their corresponding Wikipedia page, 2) employing crowd workers to verify and calibrate these noisy facts, 3) generating question-answer pairs based on the annotated time-sensitive facts. Our dataset poses challenges in the aspect of both temporal understanding and temporal reasoning. We evaluate different SoTA long-document QA systems like BigBird and FiD on our dataset. The best-performing model FiD can only achieve 46\\% accuracy, still far behind the human performance of 87\\%. We demonstrate that these models are still lacking the ability to perform consistent temporal reasoning. Therefore, we believe that our dataset could serve as a benchmark to develop NLP models more sensitive to temporal shifts. The dataset and code are released in~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.",
    "authors": [
        "Wenhu Chen",
        "Xinyi Wang",
        "W. Wang"
    ],
    "venue": "NeurIPS Datasets and Benchmarks",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The dataset is constructed by mining time-evolving facts from WikiData and aligning them to their corresponding Wikipedia page, and generating question-answer pairs based on the annotated time-sensitive facts to serve as a benchmark to develop NLP models more sensitive to temporal shifts."
    },
    "citationCount": 75,
    "influentialCitationCount": 16,
    "code": null,
    "description": null,
    "url": null
}