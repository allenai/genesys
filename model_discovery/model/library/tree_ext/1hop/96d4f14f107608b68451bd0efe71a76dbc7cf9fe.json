{
    "acronym": "96d4f14f107608b68451bd0efe71a76dbc7cf9fe",
    "title": "Sparse-MLP: A Fully-MLP Architecture with Conditional Computation",
    "seed_ids": [
        "gmlp",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "96d4f14f107608b68451bd0efe71a76dbc7cf9fe",
    "abstract": "Mixture-of-Experts (MoE) with sparse conditional computation has been proved an effective architecture for scaling attention-based models to more parameters with comparable computation cost. In this paper, we propose Sparse-MLP, scaling the recent MLP-Mixer model with sparse MoE layers, to achieve a more computation-ef\ufb01cient architecture. We replace a subset of dense MLP blocks in the MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two stages of MoE layers: one with MLP experts mixing information within channels along image patch dimension, one with MLP experts mixing information within patches along the channel dimension. Besides, to reduce computational cost in routing and improve expert capacity, we design Re-represent layers in each Sparse block. These layers are to re-scale image representations by two simple but effective linear transformations. When pre-training on ImageNet-1k with MoCo v3 algorithm, our models can outperform dense MLP models by 2.5% on ImageNet Top-1 accuracy with fewer parameters and computational cost. On small-scale downstream image classi\ufb01cation tasks, i.e., Cifar10 and Cifar100, our Sparse-MLP can still achieve better performance than baselines.",
    "authors": [
        "Yuxuan Lou",
        "Fuzhao Xue",
        "Zangwei Zheng",
        "Yang You"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper replaces a subset of dense MLP blocks in the MLP-Mixer model with Sparse blocks, to achieve a more computation-ef\ufb01cient architecture and design Re-represent layers in each Sparse block to reduce computational cost in routing and improve expert capacity."
    },
    "citationCount": 16,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}