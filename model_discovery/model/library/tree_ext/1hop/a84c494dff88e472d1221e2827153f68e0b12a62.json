{
    "acronym": "a84c494dff88e472d1221e2827153f68e0b12a62",
    "title": "DoS: Abstractive text summarization based on pretrained model with document sharing",
    "seed_ids": [
        "gpt2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a84c494dff88e472d1221e2827153f68e0b12a62",
    "abstract": "In this paper, an abstractive text summarization method with document sharing is proposed. It consists of a pretrained model and self-attention mechanism on multi-document. We call it DoS mechanism. By applying the mechanism to the single-document text summarization task, the model can absorb information from multiple documents, thus enhancing its effectiveness of the model. We compared the results with several models. The experimental results show that the pre-trained model with modified attention provides the best results, where the values of Rouge-l, Rouge-2, and Rouge-L are 41.3%, 27.4%, and 38.0%, respectively. Evaluations on the LCSTS demonstrate that our model outperforms the baseline model. Subsequent analysis showed that our model was able to generate higherquality summaries.",
    "authors": [
        "Xingxing Ding",
        "Ruo Wang",
        "Zhong Zheng",
        "Xuan Liu",
        "Quan Zhu",
        "Ruiqun Li",
        "Wanru Du",
        "Siyuan Shen"
    ],
    "venue": "2022 4th International Conference on Intelligent Information Processing (IIP)",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An abstractive text summarization method with document sharing that consists of a pretrained model and self-attention mechanism on multi-document and can absorb information from multiple documents, thus enhancing its effectiveness of the model."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}