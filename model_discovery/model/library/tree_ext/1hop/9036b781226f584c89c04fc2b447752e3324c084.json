{
    "acronym": "9036b781226f584c89c04fc2b447752e3324c084",
    "title": "Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks",
    "seed_ids": [
        "transformer",
        "gpt3",
        "lineartransformer",
        "resurrectrnn",
        "917479a7a72ee7c1fb320c14d770e30ef322ef28",
        "46732358e98ce6be0c564ae11f71d556a64b4c35",
        "2a53d07a399c47151a7b440dacfb3673b9c4e753",
        "46fb606d4059611d95ffc534bef6ad593c2281a3",
        "26e6cd121c5fdb147df83cb848e4813c926737c8",
        "917096f28209ef90c9e6363cf49438341120af5e",
        "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "b3caabbae4b7c3b842086b21940ce9d5b25d476f",
        "77b046c5d568b329a927cfc895ea2e6c8f43ff43",
        "e29abe559e5d7521afe37e9bd1b5de1b4e1af98a",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "d98b5c1d0f9a4e39dc79ea7a3f74e54789df5e13",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "a128b1c47e6842605fb95bceae930d2135fc38fc",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "6f68e1bb253925d8431588555d3010419f322e04"
    ],
    "s2id": "9036b781226f584c89c04fc2b447752e3324c084",
    "abstract": "Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.",
    "authors": [
        "Jerome Sieber",
        "Carmen Amo Alonso",
        "A. Didier",
        "M. Zeilinger",
        "Antonio Orvieto"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}