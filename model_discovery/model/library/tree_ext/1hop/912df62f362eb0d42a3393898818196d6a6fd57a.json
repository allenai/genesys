{
    "acronym": "912df62f362eb0d42a3393898818196d6a6fd57a",
    "title": "Controlled Text Generation via Language Model Arithmetic",
    "seed_ids": [
        "gpt2",
        "0b0debb710366cdff461938c80763eace1651af6",
        "5a3a04af4935302f0871bf14a4b573d477ce96be",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "6db13f58ff662eefa823a660fa86faf8ddf75533",
        "473e55ded26b201b834ae73966474299528ec48d",
        "12daedd80a4f860960c5b50314d09d6827f4fd4a",
        "f1e56def812bc398d1b2b8c9a7ea6a623abd38e5",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "912df62f362eb0d42a3393898818196d6a6fd57a",
    "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at https://github.com/eth-sri/language-model-arithmetic.",
    "authors": [
        "Jasper Dekoninck",
        "Marc Fischer",
        "Luca Beurer-Kellner",
        "Martin T. Vechev"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets, and demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction."
    },
    "citationCount": 10,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}