{
    "acronym": "42048a3c606504998a552d6a164bf28775b43c26",
    "title": "Moirai: Towards Optimal Placement for Distributed Inference on Heterogeneous Devices",
    "seed_ids": [
        "gpt3",
        "8e3b715d8ba39dcaad768635d25ead67daef0b92"
    ],
    "s2id": "42048a3c606504998a552d6a164bf28775b43c26",
    "abstract": "The escalating size of Deep Neural Networks (DNNs) has spurred a growing research interest in hosting and serving DNN models across multiple devices. A number of studies have been reported to partition a DNN model across devices, providing device placement solutions. The methods appeared in the literature, however, either suffer from poor placement performance due to the exponential search space or miss an optimal placement as a consequence of the reduced search space with limited heuristics. Moreover, these methods have ignored the runtime inter-operator optimization of a computation graph when coarsening the graph, which degrades the end-to-end inference performance. This paper presents Moirai that better exploits runtime inter-operator fusion in a model to render a coarsened computation graph, reducing the search space while maintaining the inter-operator optimization provided by inference backends. Moirai also generalizes the device placement algorithm from multiple perspectives by considering inference constraints and device heterogeneity.Extensive experimental evaluation with 11 large DNNs demonstrates that Moirai outperforms the state-of-the-art counterparts, i.e., Placeto, m-SCT, and GETF, up to 4.28$\\times$ in reduction of the end-to-end inference latency. Moirai code is anonymously released at \\url{https://github.com/moirai-placement/moirai}.",
    "authors": [
        "Beibei Zhang",
        "Hongwei Zhu",
        "Feng Gao",
        "Zhihui Yang",
        "Xiaoyang Sean Wang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents Moirai, a DNN model partitioner that better exploits runtime inter-operator fusion in a model to render a coarsened computation graph, reducing the search space while maintaining the inter- operator optimization provided by inference backends."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}