{
    "acronym": "2a31319e73d4486716168b65cdf7559baeda18ce",
    "title": "Star-Transformer",
    "seed_ids": [
        "gpt",
        "transformerxl",
        "universaltrans"
    ],
    "s2id": "2a31319e73d4486716168b65cdf7559baeda18ce",
    "abstract": "Although Transformer has achieved great successes on many NLP tasks, its heavy structure with fully-connected attention connections leads to dependencies on large training data. In this paper, we present Star-Transformer, a lightweight alternative by careful sparsification. To reduce model complexity, we replace the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node. Thus, complexity is reduced from quadratic to linear, while preserving the capacity to capture both local composition and long-range dependency. The experiments on four tasks (22 datasets) show that Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets.",
    "authors": [
        "Qipeng Guo",
        "Xipeng Qiu",
        "Pengfei Liu",
        "Yunfan Shao",
        "X. Xue",
        "Zheng Zhang"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Star-Transformer replaces the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node, and complexity is reduced from quadratic to linear, while preserving the capacity to capture both local composition and long-range dependency."
    },
    "citationCount": 236,
    "influentialCitationCount": 19,
    "code": null,
    "description": null,
    "url": null
}