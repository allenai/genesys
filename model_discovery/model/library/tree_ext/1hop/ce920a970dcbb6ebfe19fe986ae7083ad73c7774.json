{
    "acronym": "ce920a970dcbb6ebfe19fe986ae7083ad73c7774",
    "title": "MLP-BASED ARCHITECTURE WITH VARIABLE LENGTH INPUT FOR AUTOMATIC SPEECH RECOGNITION",
    "seed_ids": [
        "gmlp",
        "fnet",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "09e2c7adbed37440d4a339852cfa34e5b660f768"
    ],
    "s2id": "ce920a970dcbb6ebfe19fe986ae7083ad73c7774",
    "abstract": "We propose multi-layer perceptron (MLP)-based architectures suitable for variable length input. Recently, several such architectures that do not rely on selfattention have been proposed for image classification. They achieve performance competitive with that of transformer-based architectures, albeit with a simpler structure and low computational cost. They split an image into patches and mix information by applying MLPs within and across patches alternately. Due to the use of MLPs, such a model can only be used for inputs of a fixed, pre-defined size. However, many types of data are naturally variable in length, for example, acoustic signals. We propose three approaches to extend MLP-based architectures for use with sequences of arbitrary length. In all of them, we start by splitting the signal into contiguous tokens of fixed size (equivalent to patches in images). Naturally, the number of tokens is variable. The two first approaches use a gating mechanism that mixes local information across tokens in a shift-invariant and length-agnostic way. One uses a depthwise convolution to derive the gate values, while the other relies on shifting tokens. The final approach explores non-gated mixing using a circular convolution applied in the Fourier domain. We evaluate the proposed architectures on an automatic speech recognition task with the Librispeech and Tedlium2 corpora. Compared to Transformer, our proposed architecture reduces the WER by 1.2/0.3% on Librispeech test-clean/test-other set, and 1.6/1.6% on Tedlium2 dev/test set, using only 86.4% of the parameters. In addition, a hybrid of our proposed architecture and self-attention module reduces the WER by 1.9/3.4% on Librispeech test-clean/test-other set, and 1.8/1.6% on Tedlium2 dev/test set, using only 75.3% of the parameters.",
    "authors": [],
    "venue": "",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Three approaches to extend MLP-based architectures suitable for variable length input using a gating mechanism that mixes local information across tokens in a shift-invariant and length-agnostic way are proposed."
    },
    "citationCount": 10,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}