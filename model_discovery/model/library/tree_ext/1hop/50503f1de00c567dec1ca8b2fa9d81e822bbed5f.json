{
    "acronym": "50503f1de00c567dec1ca8b2fa9d81e822bbed5f",
    "title": "Do Efficient Transformers Really Save Computation?",
    "seed_ids": [
        "gpt3",
        "lineartransformer",
        "blockbert",
        "sparsetransformer",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "f2f68ed280d27bd25d61782224f8a465db8f43bd",
        "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
        "e82e3f4347674b75c432cb80604d38ee630d4bf6",
        "ac2e15fbfe3ea338725f5d33d17a5a687609c431",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "92173d081b15824d22a9ef070e118744ceee8052",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "3694381e74445a8b9f8cb8d373e39626e47191b5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "50503f1de00c567dec1ca8b2fa9d81e822bbed5f",
    "abstract": "As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.",
    "authors": [
        "Kai Yang",
        "Jan Ackermann",
        "Zhenyu He",
        "Guhao Feng",
        "Bohang Zhang",
        "Yunzhen Feng",
        "Qiwei Ye",
        "Di He",
        "Liwei Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper aims to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer, and identifies a class of DP problems for which these models can be more efficient than the standard Transformer."
    },
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}