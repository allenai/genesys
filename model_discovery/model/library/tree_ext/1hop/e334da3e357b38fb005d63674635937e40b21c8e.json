{
    "acronym": "e334da3e357b38fb005d63674635937e40b21c8e",
    "title": "On the Mathematical Relationship Between Contextual Probability and N400 Amplitude",
    "seed_ids": [
        "gpt2",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "eaee0b647d336c6fc8b844812675ec35cddf14a1",
        "4061a9941fa0ff106e884272d9ed753650417ec4",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "ebc0acee471e61fd7d51c8720c404a53b7a9e29b",
        "ea0ea0da2e774bc0b73c4470de6cbd1fc979b265",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e334da3e357b38fb005d63674635937e40b21c8e",
    "abstract": "Abstract Accounts of human language comprehension propose different mathematical relationships between the contextual probability of a word and how difficult it is to process, including linear, logarithmic, and super-logarithmic ones. However, the empirical evidence favoring any of these over the others is mixed, appearing to vary depending on the index of processing difficulty used and the approach taken to calculate contextual probability. To help disentangle these results, we focus on the mathematical relationship between corpus-derived contextual probability and the N400, a neural index of processing difficulty. Specifically, we use 37 contemporary transformer language models to calculate the contextual probability of stimuli from 6 experimental studies of the N400, and test whether N400 amplitude is best predicted by a linear, logarithmic, super-logarithmic, or sub-logarithmic transformation of the probabilities calculated using these language models, as well as combinations of these transformed metrics. We replicate the finding that on some datasets, a combination of linearly and logarithmically-transformed probability can predict N400 amplitude better than either metric alone. In addition, we find that overall, the best single predictor of N400 amplitude is sub-logarithmically-transformed probability, which for almost all language models and datasets explains all the variance in N400 amplitude otherwise explained by the linear and logarithmic transformations. This is a novel finding that is not predicted by any current theoretical accounts, and thus one that we argue is likely to play an important role in increasing our understanding of how the statistical regularities of language impact language comprehension.",
    "authors": [
        "J. Michaelov",
        "Benjamin Bergen"
    ],
    "venue": "Open Mind",
    "year": 2024,
    "tldr": null,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}