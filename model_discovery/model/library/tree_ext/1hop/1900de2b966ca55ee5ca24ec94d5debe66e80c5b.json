{
    "acronym": "1900de2b966ca55ee5ca24ec94d5debe66e80c5b",
    "title": "Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism",
    "seed_ids": [
        "nystromformer",
        "linformer",
        "bigbird",
        "longformer",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1"
    ],
    "s2id": "1900de2b966ca55ee5ca24ec94d5debe66e80c5b",
    "abstract": "Transformers are becoming the mainstream solutions for various tasks like NLP and Computer vision. Despite their success, the high complexity of the attention mechanism hinders them from being applied to latency-sensitive tasks. One opportunity to accelerate the attention mechanism is leveraging the sparsity in the attention weight matrix. However, due to the dilemma between \"dynamic\" and \"fine-grained\", previous studies fail to achieve speedup on GPUs under moderate sequence lengths. They also require costly retraining to recover accuracy. In this paper, we present DFSS, the first GPU-friendly dynamic fine-grained pruning mechanism, to address this dilemma. DFSS dynamically prunes the full attention score matrix to N:M fine-grained structured sparse pattern. Our key insight is that on the dynamic side, N:M sparsity is friendly to pruning and encoding the sparse matrix on GPU. On the fine-grained side, it always preserves the dominant entries in each row. We develop a dynamic sampled dense-dense matrix multiplication kernel, first of its kind, that multiplies the query and key matrices, prunes the result, and encodes the compressed sparse matrix without overhead. Compared with previous studies, DFSS achieves speedup in arbitrary sequence lengths. It only takes a few fine-tuning epochs to reach on-par accuracy with full attention mechanism. We provide both theoretical and empirical evidence to demonstrate DFSS is a good approximation of the full attention mechanism. We evaluate the 1:2 and 2:4 sparsity under different settings and achieve 1.38 ~ 1.86\u00d7 speedups over the full-attention on A100 GPU. On tasks from various domains with sequence lengths from 384 to 4096, its accuracy is on par with the full attention after only a couple of finetuning epochs from the dense pre-trained model.",
    "authors": [
        "Zhaodong Chen",
        "Yuying Quan",
        "Zheng Qu",
        "L. Liu",
        "Yufei Ding",
        "Yuan Xie"
    ],
    "venue": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "DFSS is the first GPU-friendly dynamic fine-grained pruning mechanism, and a dynamic sampled dense-dense matrix multiplication kernel is developed, first of its kind, that multiplies the query and key matrices, prunes the result, and encodes the compressed sparse matrix without overhead."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}