{
    "acronym": "2463dedcd8e3fe806e561b88899ca593400d7019",
    "title": "Design of Analog-AI Hardware Accelerators for Transformer-based Language Models (Invited)",
    "seed_ids": [
        "gpt3"
    ],
    "s2id": "2463dedcd8e3fe806e561b88899ca593400d7019",
    "abstract": "Analog Non-Volatile Memory-based accelerators offer high-throughput and energy-efficient Multiply-Accumulate operations for the large Fully-Connected layers that dominate Transformer-based Large Language Models. We describe architectural, wafer-scale testing, chip-demo, and hardware-aware training efforts towards such accelerators, and quantify the unique raw-throughput and latency benefits of Fully-(rather than Partially-) Weight-Stationary systems.",
    "authors": [
        "G. W. Burr",
        "H. Tsai",
        "W. Simon",
        "I. Boybat",
        "S. Ambrogio",
        "C.-E. Ho",
        "Z.-W. Liou",
        "M. Rasch",
        "J. B\u00a8uchel",
        "P. Narayanan",
        "T. Gordon",
        "S. Jain",
        "T. M. Levin",
        "K. Hosokawa",
        "M. L. Gallo",
        "H. Smith",
        "M. Ishii",
        "Y. Kohda",
        "A. Chen",
        "C. Mackin",
        "A. Fasoli",
        "K. E. Maghraoui",
        "R. Muralidhar",
        "A. Okazaki",
        "C.-T. Chen",
        "M. M. Frank",
        "C. Lammie",
        "A. Vasilopoulos",
        "A. Friz",
        "J. Luquin",
        "S. Teehan",
        "I. Ahsan",
        "A. Sebastian",
        "V. Narayanan"
    ],
    "venue": "International Electron Devices Meeting",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work describes architectural, wafer-scale testing, chip-demo, and hardware-aware training efforts towardsalog Non-Volatile Memory-based accelerators, and quantifies the unique raw-throughput and latency benefits of Fully-(rather than Partially-) Weight-Stationary systems."
    },
    "citationCount": 1,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}