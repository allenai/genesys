{
    "acronym": "1794bc353c94e8d708476132eb326fe3af51c2e6",
    "title": "Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning",
    "seed_ids": [
        "gpt",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
        "9405cc0d6169988371b2755e573cc28650d14dfe",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "1794bc353c94e8d708476132eb326fe3af51c2e6",
    "abstract": "Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. These changes can be observable, such as movements, manipulations, and transformations of the objects in the scene -- these are reflected in conventional video captioning. However, unlike images, actions in videos are also inherently linked to social and commonsense aspects such as intentions (why the action is taking place), attributes (such as who is doing the action, on whom, where, using what etc.) and effects (how the world changes due to the action, the effect of the action on other agents). Thus for video understanding, such as when captioning videos or when answering question about videos, one must have an understanding of these commonsense aspects. We present the first work on generating \\textit{commonsense} captions directly from videos, in order to describe latent aspects such as intentions, attributes, and effects. We present a new dataset \"Video-to-Commonsense (V2C)\" that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. We finetune our commonsense generation models on the V2C-QA task where we ask questions about the latent aspects in the video. Both the generation task and the QA task can be used to enrich video captions.",
    "authors": [
        "Zhiyuan Fang",
        "Tejas Gokhale",
        "Pratyay Banerjee",
        "Chitta Baral",
        "Yezhou Yang"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents the first work on generating commonsense captions directly from videos, in order to describe latent aspects such as intentions, attributes, and effects, and finetune their commonsense generation models on the V2C-QA task where they ask questions about the latent aspects in the video."
    },
    "citationCount": 53,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}