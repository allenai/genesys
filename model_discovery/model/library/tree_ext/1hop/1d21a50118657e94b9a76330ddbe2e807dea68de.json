{
    "acronym": "1d21a50118657e94b9a76330ddbe2e807dea68de",
    "title": "Contrastive Document Representation Learning with Graph Attention Networks",
    "seed_ids": [
        "bigbird",
        "longformer",
        "eb184e970c40f6b06dee499e2f6aed7205c8d5d9",
        "2c953a3c378b40dadf2e3fb486713c8608b8e282",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "1d21a50118657e94b9a76330ddbe2e807dea68de",
    "abstract": "Recent progress in pretrained Transformer-based language models has shown great success in learning contextual representation of text. However, due to the quadratic self-attention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks.",
    "authors": [
        "Peng Xu",
        "Xinchi Chen",
        "Xiaofei Ma",
        "Zhiheng Huang",
        "Bing Xiang"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings, which allows the model to leverage the high-level semantic structure of the document."
    },
    "citationCount": 8,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}