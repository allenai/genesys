{
    "acronym": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
    "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
    "seed_ids": [
        "gpt2",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "47e15941c8b157873c8264e4bf50318d1ba5cd18",
        "92173d081b15824d22a9ef070e118744ceee8052",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
    "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
    "authors": [
        "Xuezhi Wang",
        "Jason Wei",
        "D. Schuurmans",
        "Quoc Le",
        "E. Chi",
        "Denny Zhou"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
    },
    "citationCount": 1830,
    "influentialCitationCount": 310,
    "code": null,
    "description": null,
    "url": null
}