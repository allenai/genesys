{
    "acronym": "0227ea56aadef401ee668ab63ed49b2471f5caa0",
    "title": "Towards Long Form Audio-visual Video Understanding",
    "seed_ids": [
        "longformer",
        "lstransformer",
        "131ba9932572c92155874db93626cf299659254e",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "0227ea56aadef401ee668ab63ed49b2471f5caa0",
    "abstract": "We live in a world filled with never-ending streams of multimodal information. As a more natural recording of the real scenario, long form audio-visual videos are expected as an important bridge for better exploring and understanding the world. In this paper, we propose the multisensory temporal event localization task in long form videos and strive to tackle the associated challenges. To facilitate this study, we first collect a large-scale Long Form Audio-visual Video (LFAV) dataset with 5,175 videos and an average video length of 210 seconds. Each collected video is elaborately annotated with diversified modality-aware events, in a long-range temporal sequence. We then propose an event-centric framework for localizing multisensory events as well as understanding their relations in long form videos. It includes three phases in different levels: snippet prediction phase to learn snippet features, event extraction phase to extract event-level features, and event interaction phase to study event relations. Experiments demonstrate that the proposed method, utilizing the new LFAV dataset, exhibits considerable effectiveness in localizing multiple modality-aware events within long form videos. We hope that our newly collected dataset and novel approach serve as a cornerstone for furthering research in the realm of long form audio-visual video understanding. Project page: https://gewu-lab.github.io/LFAV/.",
    "authors": [
        "Wenxuan Hou",
        "Guangyao Li",
        "Yapeng Tian",
        "Di Hu"
    ],
    "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An event-centric framework for localizing multisensory events as well as understanding their relations in long form videos and demonstrating considerable effectiveness in localizing multiple modality-aware events within long form videos is proposed."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}