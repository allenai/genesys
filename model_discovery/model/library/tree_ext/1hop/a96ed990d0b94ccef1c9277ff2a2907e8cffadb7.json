{
    "acronym": "a96ed990d0b94ccef1c9277ff2a2907e8cffadb7",
    "title": "LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking",
    "seed_ids": [
        "transformer",
        "42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a96ed990d0b94ccef1c9277ff2a2907e8cffadb7",
    "abstract": "Deep learning models, particularly those based on transformers, often employ numerous stacked structures, which possess identical architectures and perform similar functions. While effective, this stacking paradigm leads to a substantial increase in the number of parameters, posing challenges for practical applications. In today's landscape of increasingly large models, stacking depth can even reach dozens, further exacerbating this issue. To mitigate this problem, we introduce LORS (LOw-rank Residual Structure). LORS allows stacked modules to share the majority of parameters, requiring a much smaller number of unique ones per module to match or even surpass the performance of using entirely distinct ones, thereby significantly reducing parameter usage. We validate our method by applying it to the stacked decoders of a query-based object detector, and conduct extensive experiments on the widely used MS COCO dataset. Experimental results demonstrate the effectiveness of our method, as even with a 70\\% reduction in the parameters of the decoder, our method still enables the model to achieve comparable or",
    "authors": [
        "Jialin Li",
        "Qiang Nie",
        "Weifu Fu",
        "Yuhuan Lin",
        "Guangpin Tao",
        "Yong Liu",
        "Chengjie Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "LORS (LOw-rank Residual Structure) allows stacked modules to share the majority of parameters, requiring a much smaller number of unique ones per module to match or even surpass the performance of using entirely distinct ones, thereby significantly reducing parameter usage."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}