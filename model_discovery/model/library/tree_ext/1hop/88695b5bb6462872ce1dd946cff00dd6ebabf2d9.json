{
    "acronym": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9",
    "title": "Scaling TransNormer to 175 Billion Parameters",
    "seed_ids": [
        "lineartransformer",
        "flash",
        "2f0203386f3dcbffb47c9f7fe2d19d373d9dda2f",
        "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "54155c2977a977bf129849455dcae3a2b79b3f41",
        "ac608a4a6b19b3208e560eee5daadb3cc18638a2",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "bb15f3727f827a3cb88b5d3ca48415c09b40a88f",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "86c8d930b492a4f9cadc6c60aecdaaded49acc86",
        "ca444821352a4bd91884413d8070446e2960715a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9",
    "abstract": "We present TransNormerLLM, the \ufb01rst linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and ef\ufb01ciency. TransNormerLLM evolves from the previous linear attention architecture TransNormer [32] by making advanced modi\ufb01cations that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, and inference acceleration and stabilization. Speci\ufb01cally, we use LRPE [34] together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20% . Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior ef\ufb01ciency during both training and inference stages. Scalability is at the heart of our model\u2019s design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics. Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure data quality and relevance, we implement a new self-cleaning strategy to",
    "authors": [
        "Zhen Qin",
        "Dong Li",
        "Weigao Sun",
        "Weixuan Sun",
        "Xuyang Shen",
        "Xiaodong Han",
        "Yunshen Wei",
        "Baohong Lv",
        "Fei Yuan",
        "Xiao Luo",
        "Yu Qiao",
        "Yiran Zhong"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A robust inference algorithm is developed that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior ef\ufb01ciency during both training and inference stages."
    },
    "citationCount": 15,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}