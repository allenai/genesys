{
    "acronym": "2dda6da7375bf5e8bcf60f87b17ba10757f3bc57",
    "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
    "seed_ids": [
        "mamba",
        "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "95b2cb3f4a765014ce025afe5679660982554e6c",
        "b3caabbae4b7c3b842086b21940ce9d5b25d476f",
        "9bfd79af22a25c57becad12964034f0dd5ef53e6",
        "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "a7d68b1702af08ce4dbbf2cd0b083e744ae5c6be",
        "59b7448f816908cfb49a2ab5e63b2fa5786387f7",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "5eda60d4940d4185df45c5703e103458171d465d",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "322cfc0af8b29eff694bc80bca5e456b35e400cc",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "2dda6da7375bf5e8bcf60f87b17ba10757f3bc57",
    "abstract": "Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new challenges when adapting SSMs to graph-structured data, and present four required and one optional steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE and SE. We further provide theoretical justification for the power of GMNs. Experiments demonstrate that despite much less computational cost, GMNs attain an outstanding performance in long-range, small-scale, large-scale, and heterophilic benchmark datasets.",
    "authors": [
        "Ali Behrouz",
        "Farnoosh Hashemi"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary for good performance in practice, and theoretical justification for the power of GMNs is provided."
    },
    "citationCount": 30,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}