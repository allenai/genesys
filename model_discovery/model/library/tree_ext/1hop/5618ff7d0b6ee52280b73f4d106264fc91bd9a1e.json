{
    "acronym": "5618ff7d0b6ee52280b73f4d106264fc91bd9a1e",
    "title": "HyperMixer: An MLP-based Green AI Alternative to Transformers",
    "seed_ids": [
        "metaformer",
        "gmlp",
        "485c08025157973bb52a935c6aa3bee74f990c01",
        "71363797140647ebb3f540584de0a8758d2f7aa2",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "cec7872b194aadf54140578b9be52939eb1112e9",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c"
    ],
    "s2id": "5618ff7d0b6ee52280b73f4d106264fc91bd9a1e",
    "abstract": "Transformer-based architectures are the model of choice for natural language understanding, but they come at a signi\ufb01cant cost, as they have quadratic complexity in the input length and can be dif\ufb01cult to tune. In the pursuit of Green AI , we investigate simple MLP-based architectures. We \ufb01nd that existing architectures such as MLPMixer, which achieves to-ken mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natu-ral language understanding. In this paper, we propose a simple variant, HyperMixer , which forms the token mixing MLP dynamically us-ing hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, Hy-perMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.",
    "authors": [
        "Florian Mai",
        "Arnaud Pannatier",
        "Fabio Fehr",
        "Haolin Chen",
        "Fran\u00e7ois Marelli",
        "F. Fleuret",
        "J. Henderson"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a simple variant, HyperMixer, which forms the token mixing MLP dynamically us-ing hypernetworks, and demonstrates that this model performs better than alternative MLP-based models, and on par with Transformers."
    },
    "citationCount": 8,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}