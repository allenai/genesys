{
    "acronym": "86e7122a7359f32329cd52dc34c425e6810693d8",
    "title": "Training GPT-2 to represent two Romantic-era authors: challenges, evaluations and pitfalls",
    "seed_ids": [
        "gpt",
        "b18e50747e88953e86046a7bec5afc17bf2649a0",
        "83b56c3c7a61767bd88d85796aa5dbc4976912c3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "86e7122a7359f32329cd52dc34c425e6810693d8",
    "abstract": "Poetry generation within style constraints has many creative challenges, despite the recent advances in Trans-former models for text generation. We study 1) how overfitting of various versions of GPT-2 models affects the quality of the generated text, and 2) which model is better at generating text in a specific style. For that purpose, we propose a novel setup for text evaluation with neural networks. Our GPT-2 models are trained on datasets of collected works of the two Romantic-era po-ets: Byron and Shelley. With some models, overfitting manifests by producing malformed samples, with others, the samples are always well-formed, but contain increasingly higher levels of n-grams duplicated from the original corpus. This behaviour can lead to incorrect evaluations of generated text because the plagia-rised output can deceive neural network classifiers and even human judges. To determine which model is better at preserving style before it becomes overfitted, we conduct two series of experiments with BERT-based classifiers. Overall, our results provide a novel way of selecting the right models for fine-tuning on a specific dataset, while highlighting the pitfalls that come with overfitting, like reordering and replicating text, towards more credible creative text generation.",
    "authors": [
        "Piotr Sawicki",
        "M. Grzes",
        "Anna K. Jordanous",
        "Daniel Brown",
        "Max Peeperkorn"
    ],
    "venue": "International Conference on Innovative Computing and Cloud Computing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel way of selecting the right models for fine-tuning on a specific dataset is proposed, while highlighting the pitfalls that come with overfitting, like reordering and replicating text, towards more credible creative text generation."
    },
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}