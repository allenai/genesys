{
    "acronym": "b8a0c4d42ab90126cf6bcfc762c2622d8fc39495",
    "title": "AltChart: Enhancing VLM-based Chart Summarization Through Multi-Pretext Tasks",
    "seed_ids": [
        "bert",
        "52f166d02d7b4367fc9ee30692810b35fbd2f21f"
    ],
    "s2id": "b8a0c4d42ab90126cf6bcfc762c2622d8fc39495",
    "abstract": "Chart summarization is a crucial task for blind and visually impaired individuals as it is their primary means of accessing and interpreting graphical data. Crafting high-quality descriptions is challenging because it requires precise communication of essential details within the chart without vision perception. Many chart analysis methods, however, produce brief, unstructured responses that may contain significant hallucinations, affecting their reliability for blind people. To address these challenges, this work presents three key contributions: (1) We introduce the AltChart dataset, comprising 10,000 real chart images, each paired with a comprehensive summary that features long-context, and semantically rich annotations. (2) We propose a new method for pretraining Vision-Language Models (VLMs) to learn fine-grained chart representations through training with multiple pretext tasks, yielding a performance gain with ${\\sim}2.5\\%$. (3) We conduct extensive evaluations of four leading chart summarization models, analyzing how accessible their descriptions are. Our dataset and codes are publicly available on our project page: https://github.com/moured/AltChart.",
    "authors": [
        "Omar Moured",
        "Jiaming Zhang",
        "M. Sarfraz",
        "Rainer Stiefelhagen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces the AltChart dataset, comprising 10,000 real chart images, each paired with a comprehensive summary that features long-context, and semantically rich annotations, and proposes a new method for pretraining Vision-Language Models to learn fine-grained chart representations through training with multiple pretext tasks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}