{
    "acronym": "a399fe831a4bd1bb56b763895609fcfc9a5d0c71",
    "title": "Learning Long Sequences in Spiking Neural Networks",
    "seed_ids": [
        "spikegpt",
        "s4",
        "s4d",
        "resurrectrnn",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "d9964ab436eefd21f923a4bc833c6b66692c7f00",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "9f52317ea9c5a6804b978987ff2a6557f98b5b2c",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "2ace8667f2b331001136391cae237d50c0db6383",
        "11df9ac34655f4ad746e4db39c49f928f0cbd201",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd"
    ],
    "s2id": "a399fe831a4bd1bb56b763895609fcfc9a5d0c71",
    "abstract": "Spiking neural networks (SNNs) take inspiration from the brain to enable energy-efficient computations. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on modern sequential tasks, as they inherit limitations from recurrent neural networks (RNNs), with the added challenge of training with non-differentiable binary spiking activations. However, a recent renewed interest in efficient alternatives to Transformers has given rise to state-of-the-art recurrent architectures named state space models (SSMs). This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling. Results suggest that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark. It is also shown that SSM-based SNNs can outperform current state-of-the-art SNNs with fewer parameters on sequential image classification. Finally, a novel feature mixing layer is introduced, improving SNN accuracy while challenging assumptions about the role of binary activations in SNNs. This work paves the way for deploying powerful SSM-based architectures, such as large language models, to neuromorphic hardware for energy-efficient long-range sequence modelling.",
    "authors": [
        "Matei Ioan Stan",
        "Oliver Rhodes"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling, and suggests that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}