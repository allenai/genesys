{
    "acronym": "ebc4fcf5a402e0c3cae73e108e1efd8cd5145fdf",
    "title": "POLCA: Power Oversubscription in LLM Cloud Providers",
    "seed_ids": [
        "gpt2",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ebc4fcf5a402e0c3cae73e108e1efd8cd5145fdf",
    "abstract": "Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow. We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the average and peak power utilization in LLM clusters for inference should not be very high. Our deductions align with the data from production LLM clusters, revealing that inference workloads offer substantial headroom for power oversubscription. However, the stringent set of telemetry and controls that GPUs offer in a virtualized environment, makes it challenging to have a reliable and robust power oversubscription mechanism. We propose POLCA, our framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters. Using open-source models to replicate the power patterns observed in production, we simulate POLCA and demonstrate that we can deploy 30% more servers in the same GPU cluster for inference, with minimal performance loss",
    "authors": [
        "Pratyush Patel",
        "Esha Choukse",
        "Chaojie Zhang",
        "\u00cd\u00f1igo Goiri",
        "Brijesh Warrier",
        "Nithish Mahalingam",
        "R. Bianchini"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "PolicCA, the framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters, is proposed and it is demonstrated that you can deploy 30% more servers in the same GPU cluster for inference, with minimal performance loss."
    },
    "citationCount": 6,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}