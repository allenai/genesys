{
    "acronym": "d2fe7536b347a7039a241bb60d507880ada686e8",
    "title": "OASum: Large-Scale Open Domain Aspect-based Summarization",
    "seed_ids": [
        "longformer",
        "3bcea238b0c323d8f891829714bbe6e8a3de894c",
        "1487b51b327028576bc480120be96ef84efa6723",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "13850aabbe8b70e99560a42504773a76ab5a4f47",
        "addd2d86d19c1e7c8854e827fb2656a50c250440",
        "6e6a2fe517b33e1f29d761ae31fb37ddccb9a213",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "d2fe7536b347a7039a241bb60d507880ada686e8",
    "abstract": "Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users' interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, contains relatively small-scale instances, or includes only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia.org and automatically create a high-quality, large-scale open-domain aspect-based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 million Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization generation. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine-tuning results show that the model pre-trained on our corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model. Our dataset and pre-trained checkpoints are publicly available.",
    "authors": [
        "Xianjun Yang",
        "Kaiqiang Song",
        "Sangwoo Cho",
        "Xiaoyang Wang",
        "Xiaoman Pan",
        "Linda Petzold",
        "Dong Yu"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Zero/few-shot and fine-tuning results show that the model pre-trained on the authors' corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model."
    },
    "citationCount": 16,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}