{
    "acronym": "41c50865c456ea30b07466b3b970d8a6cde08426",
    "title": "Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition",
    "seed_ids": [
        "cosformer",
        "performer",
        "rfa",
        "linformer",
        "flash",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28"
    ],
    "s2id": "41c50865c456ea30b07466b3b970d8a6cde08426",
    "abstract": "Cued Speech (CS) is a pure visual coding method used by hearing-impaired people that combines lip reading with several specific hand shapes to make the spoken language visible. Automatic CS recognition (ACSR) seeks to transcribe visual cues of speech into text, which can help hearing-impaired people to communicate effectively. The visual information of CS contains lip reading and hand cueing, thus the fusion of them plays an important role in ACSR. However, most previous fusion methods struggle to capture the global dependency present in long sequence inputs of multi-modal CS data. As a result, these methods generally fail to learn the effective cross-modal relationships that contribute to the fusion. Recently, attention-based transformers have been a prevalent idea for capturing the global dependency over the long sequence in multi-modal fusion, but existing multi-modal fusion transformers suffer from both poor recognition accuracy and inefficient computation for the ACSR task. To address these problems, we develop a novel computation and parameter efficient multi-modal fusion transformer by proposing a novel Token-Importance-Aware Attention mechanism (TIAA), where a token utilization rate (TUR) is formulated to select the important tokens from the multi-modal streams. More precisely, TIAA firstly models the modality-specific fine-grained temporal dependencies over all tokens of each modality, and then learns the efficient cross-modal interaction for the modality-shared coarse-grained temporal dependencies over the important tokens of different modalities. Besides, a light-weight gated hidden projection is designed to control the feature flows of TIAA. The resulting model, named Economical Cued Speech Fusion Transformer (EcoCued), achieves state-of-the-art performance on all existing CS datasets (i.e., Mandarin Chinese, French, and British CS), compared with existing transformer-based fusion methods and ACSR fusion methods. Notably, our method dramatically reduces the computational complexity from $\\mathcal {O}(T^{2})$ to $\\mathcal {O}(T)$. We will release the source code and data as open source.",
    "authors": [
        "Lei Liu",
        "Li Liu",
        "Haizhou Li"
    ],
    "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel computation and parameter efficient multi-modal fusion transformer is developed by proposing a novel Token-Importance-Aware Attention mechanism (TIAA), where a token utilization rate (TUR) is formulated to select the important tokens from the multi-modal streams."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}