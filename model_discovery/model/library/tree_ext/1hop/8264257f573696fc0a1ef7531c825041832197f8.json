{
    "acronym": "8264257f573696fc0a1ef7531c825041832197f8",
    "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases",
    "seed_ids": [
        "flashattn",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8264257f573696fc0a1ef7531c825041832197f8",
    "abstract": "Improving the deployment e\ufb03ciency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be e\ufb00ective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this work, we fully investigate the feasibility of using INT4 quantization for language models, and show that using INT4 introduces no or negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a signi\ufb01cant accuracy drop for decoder-only models. To materialize the performance gain using INT4, we develop a highly-optimized end-to-end INT4 encoder inference pipeline supporting di\ufb00erent quantization strategies. Our INT4 pipeline is 8 . 5 \u00d7 faster for latency-oriented scenarios and up to 3 \u00d7 for throughput-oriented scenarios compared to the inference of FP16, and improves the SOTA BERT INT8 performance from FasterTransformer by up to 1 . 7 \u00d7 . We also provide insights into the failure cases when applying INT4 to decoder-only models, and further explore the compatibility of INT4 quantization with other compression techniques, like pruning and layer reduction.",
    "authors": [
        "Xiaoxia Wu",
        "Cheng Li",
        "Reza Yazdani Aminabadi",
        "Z. Yao",
        "Yuxiong He"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work fully investigate the feasibility of usingINT4 quantization for language models, and shows that using INT4 introduces no or negligible accuracy degradation for encoder-only and encoder -decoder models, but causes a signi\ufb01cant accuracy drop for decoder- only models."
    },
    "citationCount": 22,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}