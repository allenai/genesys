{
    "acronym": "fbbc45e85d46aaffe660efddad9a7c6280a9fad6",
    "title": "Technical Report on Neural Language Models and Few-Shot Learning for Systematic Requirements Processing in MDSE",
    "seed_ids": [
        "gpt",
        "caacf9dddc1857ebb8e02b520bb5635b44e55c88",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fbbc45e85d46aaffe660efddad9a7c6280a9fad6",
    "abstract": "Systems engineering, in particular in the automotive domain, needs to cope with the massively increasing numbers of requirements that arise during the development process. To guarantee a high product quality and make sure that functional safety standards such as ISO26262 are fulfilled, the exploitation of potentials of model-driven systems engineering in the form of automatic analyses, consistency checks, and tracing mechanisms is indispensable. However, the language in which requirements are written, and the tools needed to operate on them, are highly individual and require domain-specific tailoring. This hinders automated processing of requirements as well as the linking of requirements to models. Introducing formal requirement notations in existing projects leads to the challenge of translating masses of requirements and process changes on the one hand and to the necessity of the corresponding training for the requirements engineers. In this paper, based on the analysis of an open-source set of automotive requirements, we derive domain-specific language constructs helping us to avoid ambiguities in requirements and increase the level of formality. The main contribution is the adoption and evaluation of few-shot learning with large pretrained language models for the automated translation of informal requirements to structured languages such as a requirement DSL. We show that support sets of less than ten translation examples can suffice to few-shot train a language model to incorporate keywords and implement syntactic rules into informal natural language requirements.",
    "authors": [
        "Vincent Bertram",
        "Miriam Bo\u00df",
        "Evgeny Kusmenko",
        "I. Nachmann",
        "Bernhard Rumpe",
        "Danilo Trotta",
        "Louis Wachtmeister"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that support sets of less than ten translation examples can suffice to few-shot train a language model to incorporate keywords and implement syntactic rules into informal natural language requirements and increase the level of formality in requirements."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}