{
    "acronym": "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
    "title": "Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces",
    "seed_ids": [
        "performer",
        "bigbird",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "59b7448f816908cfb49a2ab5e63b2fa5786387f7",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "6c761cfdb031701072582e434d8f64d436255da6",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78"
    ],
    "s2id": "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
    "abstract": "Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.",
    "authors": [
        "Chloe X. Wang",
        "Oleksii Tsepa",
        "Jun Ma",
        "Bo Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Graph-Mamba is introduced, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism, leading to a substantial improvement in predictive performance."
    },
    "citationCount": 44,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}