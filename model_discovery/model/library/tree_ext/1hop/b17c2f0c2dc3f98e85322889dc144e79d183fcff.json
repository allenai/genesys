{
    "acronym": "b17c2f0c2dc3f98e85322889dc144e79d183fcff",
    "title": "Scaling Laws from the Data Manifold Dimension",
    "seed_ids": [
        "gpt",
        "d28c18a3c2a0afdc0a8634d18345af8d36e1f948",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b17c2f0c2dc3f98e85322889dc144e79d183fcff",
    "abstract": "When data is plentiful, the test loss achieved by well-trained neural networks scales as a power-law L \u221d N \u2212 \u03b1 in the number of network parameters N . This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are e\ufb00ectively just performing regression on a data manifold of intrinsic dimension d . This simple theory predicts that the scaling exponents \u03b1 \u2248 4 /d for cross-entropy and mean-squared error losses. We con\ufb01rm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of d and \u03b1 by dialing the properties of random teacher networks. We also test the theory with CNN image classi\ufb01ers on several datasets and with GPT-type language models.",
    "authors": [
        "Utkarsh Sharma"
    ],
    "venue": "Journal of machine learning research",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a simple theory that predicts that the scaling exponents \u03b1 \u2248 4 /d for cross-entropy and mean-squared error losses in well-trained neural networks scales as a power-law L \u221d N \u2212 \u03b1 in the number of network parameters N."
    },
    "citationCount": 26,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}