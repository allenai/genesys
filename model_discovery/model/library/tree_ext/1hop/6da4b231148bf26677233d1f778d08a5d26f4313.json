{
    "acronym": "6da4b231148bf26677233d1f778d08a5d26f4313",
    "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference",
    "seed_ids": [
        "longformer",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "8382402fe166df3de499dac182e42baa51335926",
        "5744f56d3253bd7c4341d36de40a93fceaa266b3",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "efe4902a39c8ef332058ae7d156a6bffcd3c1341",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "6da4b231148bf26677233d1f778d08a5d26f4313",
    "abstract": "Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs\u2019 inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT.",
    "authors": [
        "Deming Ye",
        "Yankai Lin",
        "Yufei Huang",
        "Maosong Sun"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A dynamic token reduction approach to accelerate PLMs\u2019 inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation and achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs."
    },
    "citationCount": 48,
    "influentialCitationCount": 10,
    "code": null,
    "description": null,
    "url": null
}