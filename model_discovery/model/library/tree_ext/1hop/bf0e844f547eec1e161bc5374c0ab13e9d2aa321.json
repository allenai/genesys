{
    "acronym": "bf0e844f547eec1e161bc5374c0ab13e9d2aa321",
    "title": "Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model",
    "seed_ids": [
        "transformerxl",
        "d02bdac2e1abafcb0116862eb358da72a189fdfa"
    ],
    "s2id": "bf0e844f547eec1e161bc5374c0ab13e9d2aa321",
    "abstract": "In this work, we introduce a simple yet efficient post-processing model for automatic speech recognition. Our model has Transformer-based encoder-decoder architecture which \"translates\" acoustic model output into grammatically and semantically correct text. We investigate different strategies for regularizing and optimizing the model and show that extensive data augmentation and the initialization with pretrained weights are required to achieve good performance. On the LibriSpeech benchmark, our method demonstrates significant improvement in word error rate over the baseline acoustic model with greedy decoding, especially on much noisier dev-other and test-other portions of the evaluation dataset. Our model also outperforms baseline with 6-gram language model re-scoring and approaches the performance of re-scoring with Transformer-XL neural language model.",
    "authors": [
        "Oleksii Hrinchuk",
        "Mariya Popova",
        "Boris Ginsburg"
    ],
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a simple yet efficient post-processing model for automatic speech recognition which has Transformer-based encoder-decoder architecture which \"translates\" acoustic model output into grammatically and semantically correct text."
    },
    "citationCount": 73,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}