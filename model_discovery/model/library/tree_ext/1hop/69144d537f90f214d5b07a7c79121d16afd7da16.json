{
    "acronym": "69144d537f90f214d5b07a7c79121d16afd7da16",
    "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
    "seed_ids": [
        "diffusionlm",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "69144d537f90f214d5b07a7c79121d16afd7da16",
    "abstract": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{https://github.com/Shark-NLP/DiffuSeq}",
    "authors": [
        "Shansan Gong",
        "Mukai Li",
        "Jiangtao Feng",
        "Zhiyong Wu",
        "Lingpeng Kong"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Upon extensive evaluation over a wide range of Seq2Seq tasks, DiffuSeq is found achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models."
    },
    "citationCount": 200,
    "influentialCitationCount": 40,
    "code": null,
    "description": null,
    "url": null
}