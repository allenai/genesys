{
    "acronym": "94dbc06cd2c891f5cebe1f6af392c1a6b2b35a1b",
    "title": "How to Compute the Probability of a Word",
    "seed_ids": [
        "gpt2",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "eaee0b647d336c6fc8b844812675ec35cddf14a1",
        "60332206dd6a64c6361e78614c65076cc43f069f",
        "ebc0acee471e61fd7d51c8720c404a53b7a9e29b",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "94dbc06cd2c891f5cebe1f6af392c1a6b2b35a1b",
    "abstract": "Language models (LMs) estimate the probability distribution over sequences of natural language; these distributions are crucial for computing perplexity and surprisal in linguistics research. While we are usually concerned with measuring these values for words, most LMs operate over subwords. Despite seemingly straightforward, accurately computing probabilities over one unit given probabilities over the other requires care. Indeed, we show here that many recent linguistic studies have been incorrectly computing these values. This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that correcting the widespread bug in probability computations affects measured outcomes in sentence comprehension and lexical optimisation analyses.",
    "authors": [
        "Tiago Pimentel",
        "Clara Meister"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}