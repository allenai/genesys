{
    "acronym": "5b7cc3c440147f13f032570213eab40eca8d70c1",
    "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
    "seed_ids": [
        "gpt3",
        "1c0b3679919cd0531973fced1a1eb49745d9332d",
        "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "3fb0731538c59f8520a309996a0567b58965f0fe",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "8345d757e9127eff382d5285fef99312eaf283cd",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "6ff68b34a5f78bdd14437fe5a79aebbc42c26467",
        "0371c6376d08d7368e4feea161b4909032b59a35",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5b7cc3c440147f13f032570213eab40eca8d70c1",
    "abstract": "Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps.",
    "authors": [
        "Daixuan Cheng",
        "Yuxian Gu",
        "Shaohan Huang",
        "Junyu Bi",
        "Minlie Huang",
        "Furu Wei"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs and enables Llama3-8B to be comparable to or even outperform Llama3-70B."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}