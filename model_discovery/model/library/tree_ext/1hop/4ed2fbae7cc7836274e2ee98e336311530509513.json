{
    "acronym": "4ed2fbae7cc7836274e2ee98e336311530509513",
    "title": "Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering",
    "seed_ids": [
        "longt5",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "ec307b17f193b14292206b65a1bcc95bfd8f02ed",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "4ed2fbae7cc7836274e2ee98e336311530509513",
    "abstract": "We train a language model (LM) to robustly answer multistep questions by generating and answering sub-questions. We propose Chain-of-Questions, a framework that trains a model to generate sub-questions and sub-answers one at a time by leveraging human annotated question decomposition meaning representation (QDMR). The key technical challenge is that QDMR only contains sub-questions but not answers to those sub-questions, so we treat sub-answers as latent variables and optimize them using a novel dynamic mixture of Hard-EM and MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods by 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA adversarial set, thus demonstrating the effectiveness and robustness of our framework.",
    "authors": [
        "Wang Zhu",
        "Jesse Thomason",
        "Robin Jia"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The proposed Chain-of-Questions framework, a framework that trains a model to generate sub-questions and sub-answers one at a time by leveraging human annotated question decomposition meaning representation (QDMR), greatly outperforms strong neuro-symbolic methods and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA adversarial set."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}