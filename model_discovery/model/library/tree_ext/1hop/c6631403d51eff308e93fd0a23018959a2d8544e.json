{
    "acronym": "c6631403d51eff308e93fd0a23018959a2d8544e",
    "title": "Training language models for deeper understanding improves brain alignment",
    "seed_ids": [
        "bigbird",
        "longt5",
        "8cef169a76fc8ff2971ff3b6832b5de885d37ad4",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "c6631403d51eff308e93fd0a23018959a2d8544e",
    "abstract": "Building systems that achieve a deeper understanding of language is one of the central goals of natural language processing (NLP). Towards this goal, recent works have begun to train language models on narrative datasets which require extracting the most critical information by integrating across long contexts. However, it is still an open question whether these models are learning a deeper understanding of the text",
    "authors": [
        "Khai Loong Aw",
        "Mariya Toneva"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work trains language models on narrative datasets which require extracting the most critical information by integrating across long contexts, but it is still an open question whether these models are learning a deeper understanding of the text."
    },
    "citationCount": 10,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}