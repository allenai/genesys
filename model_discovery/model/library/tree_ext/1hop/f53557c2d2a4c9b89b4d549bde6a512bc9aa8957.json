{
    "acronym": "f53557c2d2a4c9b89b4d549bde6a512bc9aa8957",
    "title": "CABINET: Content Relevance based Noise Reduction for Table Question Answering",
    "seed_ids": [
        "gpt3",
        "bert",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "395aae6e7a79e5760457ca38e868acc970016230",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "f53557c2d2a4c9b89b4d549bde6a512bc9aa8957",
    "abstract": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.",
    "authors": [
        "Sohan Patnaik",
        "Heril Changwal",
        "Milan Aggarwal",
        "Sumita Bhatia",
        "Yaman Kumar",
        "Balaji Krishnamurthy"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets."
    },
    "citationCount": 5,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}