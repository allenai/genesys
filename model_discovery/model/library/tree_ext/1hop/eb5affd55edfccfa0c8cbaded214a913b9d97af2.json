{
    "acronym": "eb5affd55edfccfa0c8cbaded214a913b9d97af2",
    "title": "Enhancing diversity for logical table\u2010to\u2010text generation with mixture of experts",
    "seed_ids": [
        "gpt2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "eb5affd55edfccfa0c8cbaded214a913b9d97af2",
    "abstract": "Logical table\u2010to\u2010text generation is a task within the realm of natural language generation (NLG) that aims to generate coherent and logically faithful sentences based on tables. Unlike conventional NLG tasks, this task demands not only surface\u2010level fluency but also a high degree of logic\u2010level fidelity in the generated outputs. Current table\u2010to\u2010text systems grapple with various quality issues, such as repetitive generation, insufficient reasoning and limited complexity. Therefore, we introduce LogicMoE, a dedicated Mixture\u2010of\u2010Experts (MoE) model tailored for logical table\u2010to\u2010text generation. The primary objective of LogicMoE is to enrich the diversity of generated sentences from both semantic and logical perspectives. In particular, each expert within the model serves as a specialized generator responsible for generating sentences of a specific logical type. Additionally, we propose and employ novel evaluation metrics to comprehensively assess the diversity of generated outputs. Our experimental results showcase LogicMoE's superiority with absolute improvements of 0.8 and 2.2 in BLEU\u20103 over the strong baselines on LogicNLG and Logic2Text datasets, respectively, driving the state\u2010of\u2010the\u2010art performance to a new level. Furthermore, we highlight its inherent advantages in terms of diversity and controllability, signifying its potential to spearhead advancements in logical table\u2010to\u2010text generation applications.",
    "authors": [
        "Jiehui Wu",
        "Mengshu Hou"
    ],
    "venue": "Expert Syst. J. Knowl. Eng.",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "LogicalMoE, a dedicated Mixture\u2010of\u2010Experts (MoE) model tailored for logical table\u2010to\u2010text generation, is introduced, highlighting its inherent advantages in terms of diversity and controllability, signifying its potential to spearhead advancements in logical table\u2010to\u2010text generation applications."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}