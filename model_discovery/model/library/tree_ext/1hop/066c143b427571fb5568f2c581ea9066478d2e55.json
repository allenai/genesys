{
    "acronym": "066c143b427571fb5568f2c581ea9066478d2e55",
    "title": "Separable Self-attention for Mobile Vision Transformers",
    "seed_ids": [
        "reformer",
        "dd1139cfc609c2f3263d02e97176d5275caebc0a",
        "cec7872b194aadf54140578b9be52939eb1112e9",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "066c143b427571fb5568f2c581ea9066478d2e55",
    "abstract": "Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires $O(k^2)$ time complexity with respect to the number of tokens (or patches) $k$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. $O(k)$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running $3.2\\times$ faster on a mobile device. Our source code is available at: \\url{https://github.com/apple/ml-cvnets}",
    "authors": [
        "Sachin Mehta",
        "Mohammad Rastegari"
    ],
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A separable self-attention method with linear complexity, making it a good choice for resource-constrained devices, and the improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection."
    },
    "citationCount": 131,
    "influentialCitationCount": 31,
    "code": null,
    "description": null,
    "url": null
}