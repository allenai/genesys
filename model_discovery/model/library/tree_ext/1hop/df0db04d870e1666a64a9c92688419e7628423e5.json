{
    "acronym": "df0db04d870e1666a64a9c92688419e7628423e5",
    "title": "Is Knowledge All Large Language Models Needed for Causal Reasoning?",
    "seed_ids": [
        "gpt3",
        "0a94fbb5e1c93513523f00e75d672ef4553861f9",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "88051a6dce3b67541d8096647da2f6d31daa9e9a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "df0db04d870e1666a64a9c92688419e7628423e5",
    "abstract": "This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes ``do-operators\"for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs' causal reasoning ability mainly depends on the context and domain-specific knowledge provided. In the absence of such knowledge, LLMs can still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations. This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.",
    "authors": [
        "Hengrui Cai",
        "Shengjie Liu",
        "Rui Song"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel causal attribution model is proposed that utilizes ``do-operators\" for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes."
    },
    "citationCount": 4,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}