{
    "acronym": "61a5ae33fc34efcdbc710004554ec57e607ce75e",
    "title": "ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation",
    "seed_ids": [
        "neurallegal",
        "f6245b3e6270e4dc2e279c4b728030523dffcff4",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "61a5ae33fc34efcdbc710004554ec57e607ce75e",
    "abstract": "An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process. For such a system to be practically useful, predictions by the system should be explainable. To promote research in developing such a system, we introduce ILDC (Indian Legal Documents Corpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. Based on ILDC, we propose the task of Court Judgment Prediction and Explanation (CJPE). The task requires an automated system to predict an explainable outcome of a case. We experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability. Our best prediction model has an accuracy of 78% versus 94% for human legal experts, pointing towards the complexity of the prediction task. The analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments, pointing towards scope for future research.",
    "authors": [
        "Vijit Malik",
        "Rishabh Sanjay",
        "S. Nigam",
        "Kripabandhu Ghosh",
        "S. Guha",
        "Arnab Bhattacharya",
        "Ashutosh Modi"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A battery of baseline models for case predictions and a hierarchical occlusion based model for explainability are experimented and a significant difference in the point of view of the algorithm and legal experts for explaining the judgments is revealed, pointing towards scope for future research."
    },
    "citationCount": 97,
    "influentialCitationCount": 20,
    "code": null,
    "description": null,
    "url": null
}