{
    "acronym": "955f90930d48750e7239478b4eed440eb84131cd",
    "title": "FourierFormer: Transformer Meets Generalized Fourier Integral Theorem",
    "seed_ids": [
        "deltanet",
        "flowformer",
        "260b9388c90b497218b591a9a0e2742b7e0951e5",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "48af9b314181b04edcc0b7224ffe4689036b755f",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "72f207c777e4a17180cc54ccc6a743d5f43227af",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "05b22d6ec2cff81bcfbac2a6cf67bc1e9ef0f60a",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "955f90930d48750e7239478b4eed440eb84131cd",
    "abstract": "Multi-head attention empowers the recent success of transformers, the state-of-the-art models that have achieved remarkable success in sequence modeling and beyond. These attention mechanisms compute the pairwise dot products between the queries and keys, which results from the use of unnormalized Gaussian kernels with the assumption that the queries follow a mixture of Gaussian distribution. There is no guarantee that this assumption is valid in practice. In response, we \ufb01rst interpret attention in transformers as a nonparametric kernel regression. We then propose the FourierFormer, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels. Different from the dot-product kernels, where we need to choose a good covariance matrix to capture the dependency of the features of data, the generalized Fourier integral kernels can automatically capture such dependency and remove the need to tune the covariance matrix. We theoretically prove that our proposed Fourier integral kernels can ef\ufb01-ciently approximate any key and query distributions. Compared to the conventional transformers with dot-product attention, FourierFormers attain better accuracy and reduce the redundancy between attention heads. We empirically corroborate the advantages of FourierFormers over the baseline transformers in a variety of practical applications including language modeling and image classi\ufb01cation.",
    "authors": [
        "T. Nguyen",
        "Minh Pham",
        "Tam Nguyen",
        "Khai Nguyen",
        "S. Osher",
        "Nhat Ho"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The FourierFormer is proposed, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels, which can automatically capture the dependency of the features of data and remove the need to tune the covariance matrix."
    },
    "citationCount": 24,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}