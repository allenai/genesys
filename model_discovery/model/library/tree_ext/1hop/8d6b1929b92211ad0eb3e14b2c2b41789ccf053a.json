{
    "acronym": "8d6b1929b92211ad0eb3e14b2c2b41789ccf053a",
    "title": "Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models",
    "seed_ids": [
        "transformerxl",
        "84476fdf6ead3553f4493dff8e02308439d6222b",
        "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
        "84898960f68fa78296a102edc8ac81739f9a9408",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8d6b1929b92211ad0eb3e14b2c2b41789ccf053a",
    "abstract": "Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the Gaussian function to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different RPEs.",
    "authors": [
        "Anlin Qu",
        "Jianwei Niu",
        "Shasha Mo"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper investigates the potential problems in Shaw-R PE and XL-RPE, and proposes two novel RPEs called Low-level Fine-grained High-level Coarse- grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) R PE."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}