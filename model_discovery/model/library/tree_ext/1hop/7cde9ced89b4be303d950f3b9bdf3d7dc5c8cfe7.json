{
    "acronym": "7cde9ced89b4be303d950f3b9bdf3d7dc5c8cfe7",
    "title": "A General-Purpose Multilingual Document Encoder",
    "seed_ids": [
        "longformer",
        "9166caa474031b62bacad8a920db8308e6a15120",
        "4a78f23be51f314240d17fa0a340785ee88bf032",
        "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
        "cc2d1a66c701e9be9124eb95431b6c3dd79a61d3"
    ],
    "s2id": "7cde9ced89b4be303d950f3b9bdf3d7dc5c8cfe7",
    "abstract": "Massively multilingual pretrained transformers (MMTs) have tremendously pushed the state of the art on multilingual NLP and cross-lingual transfer of NLP models in particular. While a large body of work leveraged MMTs to mine parallel data and induce bilingual document embeddings, much less effort has been devoted to training general-purpose (massively) multilingual document encoder that can be used for both supervised and unsupervised document-level tasks. In this work, we pretrain a massively multilingual document encoder as a hierarchical transformer model (HMDE) in which a shallow document transformer contextualizes sentence representations produced by a state-of-the-art pretrained multilingual sentence encoder. We leverage Wikipedia as a readily available source of comparable documents for creating training data, and train HMDE by means of a cross-lingual contrastive objective, further exploiting the category hierarchy of Wikipedia for creation of difficult negatives. We evaluate the effectiveness of HMDE in two arguably most common and prominent cross-lingual document-level tasks: (1) cross-lingual transfer for topical document classification and (2) cross-lingual document retrieval. HMDE is significantly more effective than (i) aggregations of segment-based representations and (ii) multilingual Longformer. Crucially, owing to its massively multilingual lower transformer, HMDE successfully generalizes to languages unseen in document-level pretraining. We publicly release our code and models at https://github.com/ogaloglu/pre-training-multilingual-document-encoders .",
    "authors": [
        "Onur Galoglu",
        "Robert Litschko",
        "Goran Glavas"
    ],
    "venue": "MRL",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work pretrain a massively multilingual document encoder as a hierarchical transformer model (HMDE) in which a shallow document transformer contextualizes sentence representations produced by a state-of-the-art pretrained multilingual sentence encoder."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}