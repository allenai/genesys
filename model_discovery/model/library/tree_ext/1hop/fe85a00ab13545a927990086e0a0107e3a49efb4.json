{
    "acronym": "fe85a00ab13545a927990086e0a0107e3a49efb4",
    "title": "Graph Kernel Attention Transformers",
    "seed_ids": [
        "performer",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "77706ee4cbdbb23345da22af37bc1b9f5ec8f110",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04"
    ],
    "s2id": "fe85a00ab13545a927990086e0a0107e3a49efb4",
    "abstract": "We introduce a new class of graph neural networks (GNNs), by combining several concepts that were so far studied independently - graph kernels, attention-based networks with structural priors and more recently, ef\ufb01cient Transformers architectures applying small memory footprint implicit attention methods via low rank decomposition techniques. The goal of the paper is twofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much more expressive than SOTA GNNs as capable of modeling longer-range dependencies within a single layer. Consequently, they can use more shallow architecture design. Furthermore, GKAT attention layers scale linearly rather than quadratically in the number of nodes of the input graphs, even when those graphs are dense, requiring less compute than their regular graph attention counterparts. They achieve it by applying new classes of graph kernels admitting random feature map decomposition via random walks on graphs. As a byproduct of the introduced techniques, we obtain a new class of learnable graph sketches, called graphots , compactly encoding topological graph properties as well as nodes\u2019 features. We conducted exhaustive empirical comparison of our method with nine different GNN classes on tasks ranging from motif detection through social network classi\ufb01cation to bioinformatics challenges, showing consistent gains coming from GKATs.",
    "authors": [
        "K. Choromanski",
        "Han Lin",
        "Haoxian Chen",
        "Jack Parker-Holder"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Graph Kernel Attention Transformers (or GKATs) are much more expressive than SOTA GNNs as capable of modeling longer-range dependencies within a single layer, and can use more shallow architecture design."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}