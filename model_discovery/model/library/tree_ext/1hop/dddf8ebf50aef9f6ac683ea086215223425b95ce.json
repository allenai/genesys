{
    "acronym": "dddf8ebf50aef9f6ac683ea086215223425b95ce",
    "title": "Base of RoPE Bounds Context Length",
    "seed_ids": [
        "transformer",
        "pi",
        "yarn",
        "roformer",
        "93765da00ae8f14bb7675b95fc55a8a57fff091f",
        "6b597704044b71cbf5c224a441eb5d803445ac1c",
        "b085968c4362fb286ad6c5ef71a5db9630da0498",
        "434d751d355d7a7c20efa570e785c76286245e77",
        "539fadfb615ef84c240f4741061c44eeda540091",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "d3de0bac5703825796c240bfab8dc3c8e0a90222",
        "5735e49e501c8e51e9be4079592e46e047747b03",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "6edccbd83a9aae204785d4821f97855677c33866",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "e3aa232577bb427b1f3a34acbdef84bd85734042"
    ],
    "s2id": "dddf8ebf50aef9f6ac683ea086215223425b95ce",
    "abstract": "Position embedding is a core component of current Large Language Models (LLMs). Rotary position embedding (RoPE), a technique that encodes the position information with a rotation matrix, has been the de facto choice for position embedding in many LLMs, such as the Llama series. RoPE has been further utilized to extend long context capability, which is roughly based on adjusting the \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD) problems in position embedding. However, in this paper, we find that LLMs may obtain a superficial long-context ability based on the OOD theory. We revisit the role of RoPE in LLMs and propose a novel property of long-term decay, we derive that the \\textit{base of RoPE bounds context length}: there is an absolute lower bound for the base value to obtain certain context length capability. Our work reveals the relationship between context length and RoPE base both theoretically and empirically, which may shed light on future long context training.",
    "authors": [
        "Xin Men",
        "Mingyu Xu",
        "Bingning Wang",
        "Qingyu Zhang",
        "Hongyu Lin",
        "Xianpei Han",
        "Weipeng Chen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper revisits the role of RoPE in LLMs and proposes a novel property of long-term decay, and derives that the base of RoPE bounds context length, indicating that there is an absolute lower bound for the base value to obtain certain context length capability."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}