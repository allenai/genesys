{
    "acronym": "24615e613d4551f7aaa0557befa0a8bc403f39cd",
    "title": "Stateful Memory-Augmented Transformers for Dialogue Modeling",
    "seed_ids": [
        "transformerxl",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "24615e613d4551f7aaa0557befa0a8bc403f39cd",
    "abstract": "Transformer encoder-decoder models have shown impressive performance in dialogue modeling. However, as Transformers are in-ef\ufb01cient in processing long sequences, dialogue history length often needs to be truncated. To address this problem, we propose a new memory-augmented Transformer that is compatible with existing pre-trained encoder-decoder models and enables ef\ufb01cient preservation of history information. It incorporates a separate memory module alongside the pre-trained Transformer to effectively interchange information between the memory states and the current input context. We evaluate our model on three dialogue datasets and two language modeling datasets. Experimental results show that our method has achieved superior ef\ufb01ciency and performance compared to other pre-trained Transformer baselines.",
    "authors": [
        "Qingyang Wu",
        "Zhou Yu"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new memory-augmented Transformer is proposed that is compatible with existing pre-trained encoder-decoder models and enables ef\ufb01cient preservation of history information and incorporates a separate memory module alongside the pre-trained Transformer to effectively interchange information between the memory states and the current input context."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}