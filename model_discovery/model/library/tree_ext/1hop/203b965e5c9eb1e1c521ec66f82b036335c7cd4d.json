{
    "acronym": "203b965e5c9eb1e1c521ec66f82b036335c7cd4d",
    "title": "Shifted Chunk Transformer for Spatio-Temporal Representational Learning",
    "seed_ids": [
        "reformer"
    ],
    "s2id": "203b965e5c9eb1e1c521ec66f82b036335c7cd4d",
    "abstract": "Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation. Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models,e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51.",
    "authors": [
        "Xuefan Zha",
        "Wentao Zhu",
        "Tingxun Lv",
        "Sen Yang",
        "Ji Liu"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A shifted chunk Transformer with pure self-attention blocks that can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip and outperforms previous state-of-the-art approaches onKinetics-400, Kinetics-600, UCF101, and HMDB51."
    },
    "citationCount": 24,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}