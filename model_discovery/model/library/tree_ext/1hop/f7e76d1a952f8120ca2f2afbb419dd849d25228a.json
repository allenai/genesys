{
    "acronym": "f7e76d1a952f8120ca2f2afbb419dd849d25228a",
    "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding",
    "seed_ids": [
        "gqa",
        "mqa",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45"
    ],
    "s2id": "f7e76d1a952f8120ca2f2afbb419dd849d25228a",
    "abstract": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV) caching, but can lead to major memory bottlenecks as model size, batch size, and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV) sharing, a novel approach extending KV sharing across transformer layers to reduce memory usage beyond what was possible with Multi-Query Attention (MQA) and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA. These results highlight MLKV's potential for efficient deployment of transformer models at scale. We provide code at https://github.com/zaydzuhri/pythia-mlkv",
    "authors": [
        "Zayd Muhammad Kawakibi Zuhri",
        "Muhammad Farid Adilazuarda",
        "Ayu Purwarianti",
        "Alham Fikri Aji"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}