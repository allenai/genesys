{
    "acronym": "21ddc4fc3551619b8a64db6ae124acc72aaae2c2",
    "title": "PointMamba: A Simple State Space Model for Point Cloud Analysis",
    "seed_ids": [
        "s4",
        "mamba",
        "31fdba3a68f286894f025e734a277e2ce94dd84c",
        "e287e523762f4ef0748f263af2626c20ec29e07a",
        "0a32e6ff6eaac83ff325bae4557a8362222979aa",
        "3af7273d7ca20c0c63cbaa47e60b058840835052",
        "26e6cd121c5fdb147df83cb848e4813c926737c8",
        "2dda6da7375bf5e8bcf60f87b17ba10757f3bc57",
        "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
        "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "745594bd0dc3e9dc86f74e100cd2c98ed36256c0",
        "128fc3e518a9616cd2780d974d32b4ef47ba5901",
        "bb6644a9f5920abfc1fa008f366a9ff48468e063",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "a30ac45ac5b7bd2148d3fb80ee7f3c29724e3170",
        "ca444821352a4bd91884413d8070446e2960715a",
        "34103f1294844c447fe8872bf5c3ab1c7ce32103",
        "9226ae23b95b3f6891461e086d910ffeb7ac448a",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "21ddc4fc3551619b8a64db6ae124acc72aaae2c2",
    "abstract": "Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose PointMamba, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our method leverages space-filling curves for effective point tokenization and adopts an extremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. The code is available at https://github.com/LMD0311/PointMamba.",
    "authors": [
        "Dingkang Liang",
        "Xin Zhou",
        "Xinyu Wang",
        "Xingkui Zhu",
        "Wei Xu",
        "Zhikang Zou",
        "Xiaoqing Ye",
        "Xiang Bai"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes PointMamba, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks, and employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs."
    },
    "citationCount": 30,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}