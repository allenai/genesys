{
    "acronym": "9e3e56957e249cdebdd8673fd1174980ed694560",
    "title": "Efficient Beam Tree Recursion",
    "seed_ids": [
        "flash",
        "contrnn",
        "2fc074288f66711e4ee37350d364e74c1c401163",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "70a10d95e968158c2a862af217186c74c44b5e25",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "e528466e2aff981511d4ca6e063211297c0b4175",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "4ecf4356c3b451b16780788a3f94e422d4deeda5",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "9a618cca0d2fc78db1be1aed70517401cb3f3859"
    ],
    "s2id": "9e3e56957e249cdebdd8673fd1174980ed694560",
    "abstract": "Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.",
    "authors": [
        "Jishnu Ray Chowdhury",
        "Cornelia Caragea"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "These proposals standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}