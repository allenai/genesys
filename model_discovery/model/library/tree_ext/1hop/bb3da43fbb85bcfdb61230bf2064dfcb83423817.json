{
    "acronym": "bb3da43fbb85bcfdb61230bf2064dfcb83423817",
    "title": "Bilinear Attention Linear + Softmax Class Probabilities General Purpose Transformer Representation Target Entity Representation Post Conditioning Process Tokens Positional embedding Contextual vectors + Linear + Softmax Class Probabilities",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "e2587eddd57bc4ba286d91b27c185083f16f40ee",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "bb3da43fbb85bcfdb61230bf2064dfcb83423817",
    "abstract": "Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities\u2019 interactions. While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still untested. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained transformers, and find that these approaches underperform even simple baselines. We show that much stronger results can be attained by restructuring the input to guide the transformer model to focus on a particular entity. Second, we assess the degree to which transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-ofthe-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate entity or process state.1",
    "authors": [
        "Aditya Gupta",
        "Greg Durrett"
    ],
    "venue": "",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper tests standard lightweight approaches for prediction with pre-trained transformers, and finds that these approaches underperform even simple baselines, and shows that much stronger results can be attained by restructuring the input to guide the transformer model to focus on a particular entity."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}