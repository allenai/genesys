{
    "acronym": "46ae8535a3dbe388d3ac25c85c4a57724c98a775",
    "title": "How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad",
    "seed_ids": [
        "elgllm",
        "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6",
        "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b",
        "66d98dc2aad17c03532dbae21d05f098257cc2e2",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "92173d081b15824d22a9ef070e118744ceee8052",
        "e528466e2aff981511d4ca6e063211297c0b4175",
        "ed535e93d5b5a8b689e861e9c6083a806d1535c2",
        "49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "46ae8535a3dbe388d3ac25c85c4a57724c98a775",
    "abstract": "Can Transformers predict new syllogisms by composing established ones? More generally, what type of targets can be learned by such models from scratch? Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective. This paper puts forward the notion of 'distribution locality' to capture when weak learning is efficiently achievable by regular Transformers, where the locality measures the least number of tokens required in addition to the tokens histogram to correlate nontrivially with the target. As shown experimentally and theoretically under additional assumptions, distributions with high locality cannot be learned efficiently. In particular, syllogisms cannot be composed on long chains. Furthermore, we show that (i) an agnostic scratchpad cannot help to break the locality barrier, (ii) an educated scratchpad can help if it breaks the locality at each step, (iii) a notion of 'inductive scratchpad' can both break the locality and improve the out-of-distribution generalization, e.g., generalizing to almost double input size for some arithmetic tasks.",
    "authors": [
        "Emmanuel Abbe",
        "Samy Bengio",
        "Aryo Lotfi",
        "Colin Sandon",
        "Omid Saremi"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The notion of 'distribution locality' is put forward to capture when weak learning is efficiently achievable by regular Transformers, where the locality measures the least number of tokens required in addition to the tokens histogram to correlate nontrivially with the target."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}