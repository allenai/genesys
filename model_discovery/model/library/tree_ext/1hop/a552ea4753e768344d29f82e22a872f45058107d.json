{
    "acronym": "a552ea4753e768344d29f82e22a872f45058107d",
    "title": "Vim-F: Visual State Space Model Benefiting from Learning in the Frequency Domain",
    "seed_ids": [
        "mamba",
        "5867382590f9f0ff8caf15804d20bde10845b2d2",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "dd1139cfc609c2f3263d02e97176d5275caebc0a",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "a552ea4753e768344d29f82e22a872f45058107d",
    "abstract": "In recent years, State Space Models (SSMs) with efficient hardware-aware designs, known as the Mamba deep learning models, have made significant progress in modeling long sequences such as language understanding. Therefore, building efficient and general-purpose visual backbones based on SSMs is a promising direction. Compared to traditional convolutional neural networks (CNNs) and Vision Transformers (ViTs), the performance of Vision Mamba (ViM) methods is not yet fully competitive. To enable SSMs to process image data, ViMs typically flatten 2D images into 1D sequences, inevitably ignoring some 2D local dependencies, thereby weakening the model's ability to interpret spatial relationships from a global perspective. We use Fast Fourier Transform (FFT) to obtain the spectrum of the feature map and add it to the original feature map, enabling ViM to model a unified visual representation in both frequency and spatial domains. The introduction of frequency domain information enables ViM to have a global receptive field during scanning. We propose a novel model called Vim-F, which employs pure Mamba encoders and scans in both the frequency and spatial domains. Moreover, we question the necessity of position embedding in ViM and remove it accordingly in Vim-F, which helps to fully utilize the efficient long-sequence modeling capability of ViM. Finally, we redesign a patch embedding for Vim-F, leveraging a convolutional stem to capture more local correlations, further improving the performance of Vim-F. Code is available at: \\url{https://github.com/yws-wxs/Vim-F}.",
    "authors": [
        "Juntao Zhang",
        "Kun Bian",
        "Peng Cheng",
        "Wenbo An",
        "Jianning Liu",
        "Jun Zhou"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel model called Vim-F is proposed, which employs pure Mamba encoders and scans in both the frequency and spatial domains and question the necessity of position embedding in ViM and remove it accordingly in Vim-F, which helps to fully utilize the efficient long-sequence modeling capability of ViM."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}