{
    "acronym": "3256eab72323fc3cbaafd55edf958c23db8dbe25",
    "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations",
    "seed_ids": [
        "classfreediffu",
        "7962cae10754ac3e9d1836f129febbcb904e4b6c",
        "cf694df964caa156ec306b45d3a3127533cb458f",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1"
    ],
    "s2id": "3256eab72323fc3cbaafd55edf958c23db8dbe25",
    "abstract": "Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE, a novel contrastive learning method that is both parameter and data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE priors consistently deliver high performance while significantly reducing resource dependency.",
    "authors": [
        "Maitreya Patel",
        "C. Kim",
        "Sheng Cheng",
        "Chitta Baral",
        "Yezhou Yang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "ECLIPSE is introduced, a novel contrastive learning method that is both parameter and data-efficient and achieves performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions."
    },
    "citationCount": 7,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}