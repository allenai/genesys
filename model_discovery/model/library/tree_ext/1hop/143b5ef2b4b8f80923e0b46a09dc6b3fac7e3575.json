{
    "acronym": "143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575",
    "title": "A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization",
    "seed_ids": [
        "bigbird",
        "longformer",
        "memcompress",
        "longt5",
        "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "274f903041b1a830b37f57929d837c1706e94ec7",
        "999deaecf0adb9defa3b233be32c6a1c3f7090a3",
        "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "addd2d86d19c1e7c8854e827fb2656a50c250440",
        "6e6a2fe517b33e1f29d761ae31fb37ddccb9a213",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7cc730da554003dda77796d2cb4f06da5dfd5592",
        "6f785623450c17f4d4089dd812bc0de8bcfbb55c",
        "6b149cd0974e353ab7a58b5a5af759b03e82665c",
        "366c40349b2813c4eb281b4747f5210f99a4d62e"
    ],
    "s2id": "143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575",
    "abstract": "Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize well across different domains, the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task. Across 10 MDS benchmarks from various domains, our method outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters. It outperforms its corresponding PLM backbone by up to 3 Rouge-L and is favored by humans.",
    "authors": [
        "Chenhui Shen",
        "Liying Cheng",
        "Yang You",
        "Lidong Bing"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work enforces hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task, and outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}