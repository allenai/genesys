{
    "acronym": "c12e232657d38c47116f9e7295ed6083f2b2c2b5",
    "title": "Tools for Verifying Neural Models' Training Data",
    "seed_ids": [
        "gpt2",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "e342165a614588878ad0f4bc9bacf3905df34d08",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c12e232657d38c47116f9e7295ed6083f2b2c2b5",
    "abstract": "It is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. We introduce the concept of a\"Proof-of-Training-Data\": any protocol that allows a model trainer to convince a Verifier of the training data that produced a set of model weights. Such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. These include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. We show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the Proof-of-Learning literature.",
    "authors": [
        "Dami Choi",
        "Yonadav Shavit",
        "D. Duvenaud"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work explores efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures and exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}