{
    "acronym": "4c966838f3361911c434574bef0dd5c3bf4e149d",
    "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
    "seed_ids": [
        "transformerxl",
        "streamingllm",
        "0595dac8260443365dfbe4821787419736baaa66",
        "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b",
        "713806165610c237f551a7b68e6b09b3ded75502",
        "cb0ac335adda4ceef9987cbcbca9129e71c37f0a",
        "a27dced654158b905c7447aae1aa294ebc8ecaf0",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
        "73290ecbec2f38d1d647ddef1ada69cee41725b3",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "e3aa232577bb427b1f3a34acbdef84bd85734042",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4c966838f3361911c434574bef0dd5c3bf4e149d",
    "abstract": "This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward\"drop-in\"integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.",
    "authors": [
        "Ruisi Cai",
        "Yuandong Tian",
        "Zhangyang Wang",
        "Beidi Chen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}