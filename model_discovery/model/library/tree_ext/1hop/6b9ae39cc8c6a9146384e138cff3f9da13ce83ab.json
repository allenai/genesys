{
    "acronym": "6b9ae39cc8c6a9146384e138cff3f9da13ce83ab",
    "title": "LLMs achieve adult human performance on higher-order theory of mind tasks",
    "seed_ids": [
        "gpt3",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c"
    ],
    "s2id": "6b9ae39cc8c6a9146384e138cff3f9da13ce83ab",
    "abstract": "This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in a recursive manner (e.g. I think that you believe that she knows). This paper builds on prior work by introducing a handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to compare the performance of five LLMs to a newly gathered adult human benchmark. We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. Our results suggest that there is an interplay between model size and finetuning for the realisation of ToM abilities, and that the best-performing LLMs have developed a generalised capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviours, these findings have significant implications for user-facing LLM applications.",
    "authors": [
        "Winnie Street",
        "John Oliver Siy",
        "Geoff Keeling",
        "Adrien Baranes",
        "Benjamin Barnett",
        "Michael McKibben",
        "Tatenda Kanyere",
        "Alison Lentz",
        "B. A. Y. Arcas",
        "Robin I. M. Dunbar"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}