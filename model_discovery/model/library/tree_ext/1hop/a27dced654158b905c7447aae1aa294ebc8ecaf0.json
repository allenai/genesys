{
    "acronym": "a27dced654158b905c7447aae1aa294ebc8ecaf0",
    "title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer",
    "seed_ids": [
        "gpt2",
        "gpt3",
        "mega",
        "ssaugtrans",
        "661e8d555c4424b5953f17434f2ba910bfcf3afe",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "f75d05e759447c2aedb7097728f29f9a520d9bc1",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a27dced654158b905c7447aae1aa294ebc8ecaf0",
    "abstract": "Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost - quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on natural language modeling and generation tasks show that a decoder-only MASFormer model of 1.3B parameters can achieve competitive performance to vanilla transformers with full attention while significantly reducing computational cost (up to 75%). Additionally, we investigate the effectiveness of continual training with long sequence data and how sequence length impacts downstream generation performance, which may be of independent interest.",
    "authors": [
        "Qingru Zhang",
        "Dhananjay Ram",
        "Cole Hawkins",
        "Sheng Zha",
        "Tuo Zhao"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans, is proposed, which is equipped with full attention to capture long-range dependencies, but only at a small number of layers."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}