{
    "acronym": "81bfc1d382df3d9b04325749d9f48aad829cb107",
    "title": "MaIL: Improving Imitation Learning with Mamba",
    "seed_ids": [
        "s4",
        "mamba",
        "02d3afd9c306ce0a0d39e211d60468732c591f8f",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "1334a47e8f4e4ffd04ff534329d76a5e5cc16f46",
        "60c8d0619481eaafdd1189af610d0e636271fed5",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51"
    ],
    "s2id": "81bfc1d382df3d9b04325749d9f48aad829cb107",
    "abstract": "This work introduces Mamba Imitation Learning (MaIL), a novel imitation learning (IL) architecture that offers a computationally efficient alternative to state-of-the-art (SoTA) Transformer policies. Transformer-based policies have achieved remarkable results due to their ability in handling human-recorded data with inherently non-Markovian behavior. However, their high performance comes with the drawback of large models that complicate effective training. While state space models (SSMs) have been known for their efficiency, they were not able to match the performance of Transformers. Mamba significantly improves the performance of SSMs and rivals against Transformers, positioning it as an appealing alternative for IL policies. MaIL leverages Mamba as a backbone and introduces a formalism that allows using Mamba in the encoder-decoder structure. This formalism makes it a versatile architecture that can be used as a standalone policy or as part of a more advanced architecture, such as a diffuser in the diffusion process. Extensive evaluations on the LIBERO IL benchmark and three real robot experiments show that MaIL: i) outperforms Transformers in all LIBERO tasks, ii) achieves good performance even with small datasets, iii) is able to effectively process multi-modal sensory inputs, iv) is more robust to input noise compared to Transformers.",
    "authors": [
        "Xiaogang Jia",
        "Qian Wang",
        "Atalay Donat",
        "Bowen Xing",
        "Ge Li",
        "Hongyi Zhou",
        "Onur Celik",
        "Denis Blessing",
        "Rudolf Lioutikov",
        "Gerhard Neumann"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces Mamba Imitation Learning (MaIL), a novel imitation learning (IL) architecture that offers a computationally efficient alternative to state-of-the-art (SoTA) Transformer policies and significantly improves the performance of SSMs and rivals against Transformers."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}