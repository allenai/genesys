{
    "acronym": "6b1da517a8699f0a62368d032fda3311871372c5",
    "title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering",
    "seed_ids": [
        "gpt3",
        "36e21ca0d05949b49ea8095d75b45f92ecaf2fd6",
        "69cc8b43f2e178102599c706efb856b4808e73eb",
        "96d104dfe727f78a35faaafe81481f3672b485ee"
    ],
    "s2id": "6b1da517a8699f0a62368d032fda3311871372c5",
    "abstract": "While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated by the research of retrieval-augmented generation in the field of natural language processing, we use Dense Passage Retrieval (DPR) to retrieve related knowledge to help the model answer questions. However, DPR conduct retrieving in natural language space, which may not ensure comprehensive acquisition of image information. Thus, the retrieved knowledge is not truly conducive to helping answer the question, affecting the performance of the overall system. To address this issue, we propose a novel framework that leverages the visual-language model to select the key knowledge retrieved by DPR and answer questions. The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83\\%.",
    "authors": [
        "Dongze Hao",
        "Qunbo Wang",
        "Longteng Guo",
        "Jie Jiang",
        "Jing Liu"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel framework that leverages the visual-language model to select the key knowledge retrieved by DPR and answer questions is proposed, significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83\\%."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}