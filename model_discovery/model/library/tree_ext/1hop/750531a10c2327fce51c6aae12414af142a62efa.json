{
    "acronym": "750531a10c2327fce51c6aae12414af142a62efa",
    "title": "Joint Linguistic Steganography With BERT Masked Language Model and Graph Attention Network",
    "seed_ids": [
        "bert",
        "333212e246fb65f7c9d43862021e78f007c48449"
    ],
    "s2id": "750531a10c2327fce51c6aae12414af142a62efa",
    "abstract": "Generation-based linguistic steganography embeds secret information by generating high-quality text sequences according to the information. However, this method may cause low statistical feature consistency between normal text and the text with embedded information, thus making the steganalysis models detect the steganographic texts easier. And conditional generation-based text steganography, which embeds secret information into the suitable position in the text sequence to improve the statistical feature consistency, has a disadvantage in embedding capacity. To solve the problems above, in this article, we propose a joint linguistic steganography method that combines conditional generation-based steganography with substitution-based steganography that uses the BERT pretrained model. The graph attention network is also used in this method to extract and analyze spatial features of the text sequence, which are taken as auxiliary information in the text generation process based on temporal features. The comparative experimental results with other models show that our steganography model improves the embedding capacity of conditional generation-based text steganography while ensuring the consistency of text features, and is superior to most state-of-the-art models in imperceptibility and anti-steganalysis ability.",
    "authors": [
        "Changhao Ding",
        "Zhangjie Fu",
        "Qi Yu",
        "Fan Wang",
        "Xianyi Chen"
    ],
    "venue": "IEEE Transactions on Cognitive and Developmental Systems",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article proposes a joint linguistic steganography method that combines conditional generation-based steganography with substitution-based steganography that uses the BERT pretrained model and is superior to most state-of-the-art models in imperceptibility and anti-steganalysis ability."
    },
    "citationCount": 4,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}