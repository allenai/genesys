{
    "acronym": "ab3cc3f039457324ec68827f1314f1ac33b75748",
    "title": "Information Guided Regularization for Fine-tuning Language Models",
    "seed_ids": [
        "bert",
        "4f451ba06c4c9effd6c4ac0bae222495501a6200",
        "b47381e04739ea3f392ba6c8faaf64105493c196",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ab3cc3f039457324ec68827f1314f1ac33b75748",
    "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more surgical approach to regularization needs to exist for smoother transfer learning. Towards this end, we investigate how the pretraining loss landscape is affected by these task-sensitive parameters through an information-theoretic lens. We then leverage the findings from our investigations to devise a novel approach to dropout for improved model regularization and better downstream generalization. This approach, named guided dropout, is both task&architecture agnostic and adds no computational overhead to the fine-tuning process. Through empirical evaluations, we showcase that our approach to regularization yields consistently better performance, even in scenarios of data paucity, compared to standardized baselines.",
    "authors": [
        "Mandar Sharma",
        "N. Muralidhar",
        "Shengzhe Xu",
        "Raquib Bin Yousuf",
        "Naren Ramakrishnan"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work investigates how the pretraining loss landscape is affected by these task-sensitive parameters through an information-theoretic lens and devise a novel approach to dropout for improved model regularization and better downstream generalization."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}