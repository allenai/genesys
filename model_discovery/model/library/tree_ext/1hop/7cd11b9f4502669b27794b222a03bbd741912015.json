{
    "acronym": "7cd11b9f4502669b27794b222a03bbd741912015",
    "title": "Demystify Mamba in Vision: A Linear Attention Perspective",
    "seed_ids": [
        "transformer",
        "mamba",
        "31fdba3a68f286894f025e734a277e2ce94dd84c",
        "cbaf689fd9ea9bc939510019d90535d6249b3367",
        "62ac3ef81e54e1d1930fb5980b236345ee2e4f32",
        "5867382590f9f0ff8caf15804d20bde10845b2d2",
        "26e6cd121c5fdb147df83cb848e4813c926737c8",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "131ba9932572c92155874db93626cf299659254e",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "a883336e5c2e9f46f5012343227a6be4671c9ca0",
        "ec139916edd6feb9b3cb3a0325ca96e21dbb0147",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "7cd11b9f4502669b27794b222a03bbd741912015",
    "abstract": "Mamba is an effective state space model with linear computation complexity. It has recently shown impressive efficiency in dealing with high-resolution inputs across various vision tasks. In this paper, we reveal that the powerful Mamba model shares surprising similarities with linear attention Transformer, which typically underperform conventional Transformer in practice. By exploring the similarities and disparities between the effective Mamba and subpar linear attention Transformer, we provide comprehensive analyses to demystify the key factors behind Mamba's success. Specifically, we reformulate the selective state space model and linear attention within a unified formulation, rephrasing Mamba as a variant of linear attention Transformer with six major distinctions: input gate, forget gate, shortcut, no attention normalization, single-head, and modified block design. For each design, we meticulously analyze its pros and cons, and empirically evaluate its impact on model performance in vision tasks. Interestingly, the results highlight the forget gate and block design as the core contributors to Mamba's success, while the other four designs are less crucial. Based on these findings, we propose a Mamba-Like Linear Attention (MLLA) model by incorporating the merits of these two key designs into linear attention. The resulting model outperforms various vision Mamba models in both image classification and high-resolution dense prediction tasks, while enjoying parallelizable computation and fast inference speed. Code is available at https://github.com/LeapLabTHU/MLLA.",
    "authors": [
        "Dongchen Han",
        "Ziyi Wang",
        "Zhuofan Xia",
        "Yizeng Han",
        "Yifan Pu",
        "Chunjiang Ge",
        "Jun Song",
        "Shiji Song",
        "Bo Zheng",
        "Gao Huang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper reformulates the selective state space model and linear attention within a unified formulation, rephrasing Mamba as a variant of linear attention Transformer with six major distinctions: input gate, forget gate, shortcut, no attention normalization, single-head, and modified block design."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}