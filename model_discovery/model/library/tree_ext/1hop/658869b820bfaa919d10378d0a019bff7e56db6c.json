{
    "acronym": "658869b820bfaa919d10378d0a019bff7e56db6c",
    "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
    "seed_ids": [
        "bigbird",
        "longformer",
        "stm",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "dbc368bc8b49347dd27679894524fa62f88492c9",
        "35a3979bc9b680023d6e2bcf29a24d67053d0d86",
        "3b39efe6c91ae432dd35bb79431edb8a6719f906",
        "732e3faec4e5be4d144256f2c379b9dc49f0b227",
        "6edccbd83a9aae204785d4821f97855677c33866",
        "4eb45f33446018175e266738be22f4d830ed697e",
        "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
        "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
        "68cf0b9021f904a765e760291d0c9a509aab0067",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "274f903041b1a830b37f57929d837c1706e94ec7",
        "c600b697700c844cbc85009be70f1cdfeef3593e",
        "0a41cb292242a82b2b09b3bf23b48349b981a640",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "63857190aaf5aab1d94b54bb257b7b03b8cb5a50",
        "6e6a2fe517b33e1f29d761ae31fb37ddccb9a213",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf"
    ],
    "s2id": "658869b820bfaa919d10378d0a019bff7e56db6c",
    "abstract": "Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating scheme inspired by reinforcement learning, which regards the decoders of transformers as environments, and the downstream performance metrics as the rewards to evaluate the hidden selection actions. Our empirical results on real-world long-text summarization and reading comprehension tasks demonstrate effective improvements compared to prior longsequence processing baselines.",
    "authors": [
        "Jiawen Xie",
        "Pengyu Cheng",
        "Xiao Liang",
        "Yong Dai",
        "Nan Du"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}