{
    "acronym": "5a11808dd8f18d177aba5637fd75982ad42dabc8",
    "title": "Mamba in Speech: Towards an Alternative to Self-Attention",
    "seed_ids": [
        "mamba",
        "cbaf689fd9ea9bc939510019d90535d6249b3367",
        "da9178eae82d1ca5492aaecd0151ba49481cb8b1",
        "319762c8841f8e1e413642bc551ed748add4777b",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "5a11808dd8f18d177aba5637fd75982ad42dabc8",
    "abstract": "Transformer and its derivatives have achieved success in diverse tasks across computer vision, natural language processing, and speech processing. To reduce the complexity of computations within the multi-head self-attention mechanism in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an alternative. Mamba exhibited its effectiveness in natural language processing and computer vision tasks, but its superiority has rarely been investigated in speech signal processing. This paper explores solutions for applying Mamba to speech processing using two typical speech processing tasks: speech recognition, which requires semantic and sequential information, and speech enhancement, which focuses primarily on sequential patterns. The experimental results exhibit the superiority of bidirectional Mamba (BiMamba) for speech processing to vanilla Mamba. Moreover, experiments demonstrate the effectiveness of BiMamba as an alternative to the self-attention module in Transformer and its derivates, particularly for the semantic-aware task. The crucial technologies for transferring Mamba to speech are then summarized in ablation studies and the discussion section to offer insights for future research.",
    "authors": [
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Hexin Liu",
        "Tianyi Xiao",
        "Xinyuan Qian",
        "Beena Ahmed",
        "E. Ambikairajah",
        "Haizhou Li",
        "Julien Epps"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experiments demonstrate the superiority of bidirectional Mamba (BiMamba) for speech processing to vanilla Mamba and the effectiveness of BiMamba as an alternative to the self-attention module in Transformer and its derivates, particularly for the semantic-aware task."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}