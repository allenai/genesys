{
    "acronym": "b5416fc248f3dbcece522ef59f571a1f56e5e9a4",
    "title": "Technical Report on Token Position Bias in Transformers",
    "seed_ids": [
        "transformerxl",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "4889ba5a8ae8b2169dd44d1d3a605bf9820bae8d",
        "84476fdf6ead3553f4493dff8e02308439d6222b",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b5416fc248f3dbcece522ef59f571a1f56e5e9a4",
    "abstract": "Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\\% and 9\\% in their performance. To mitigate this effect, we propose two methods: Random Position Shifting and Context Perturbation, that we apply on batches during the training process. The results show an improvement of $\\approx$ 2\\% in the performance of the model on CoNLL03, UD_en, and TweeBank.",
    "authors": [
        "Mehdi Ben Amor",
        "M. Granitzer",
        "Jelena Mitrovi'c"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks and proposes two methods: Random Position Shifting and Context Perturbation that are applied on batches during the training process."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}