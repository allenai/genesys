{
    "acronym": "9ede78aa6a5471b2bbf07987ed62232df7446f4e",
    "title": "Efficient Linear Attention for Fast and Accurate Keypoint Matching",
    "seed_ids": [
        "lineartransformer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "9ede78aa6a5471b2bbf07987ed62232df7446f4e",
    "abstract": "Recently Transformers have provided state-of-the-art performance in sparse matching, crucial to realize high-performance 3D vision applications. Yet, these Transformers lack efficiency due to the quadratic computational complexity of their attention mechanism. To solve this problem, we employ an efficient linear attention for the linear computational complexity. Then, we propose a new attentional aggregation that achieves high accuracy by aggregating both the global and local information from sparse keypoints. To further improve the efficiency, we propose the joint learning of feature matching and description. Our learning enables simpler and faster matching than Sinkhorn, often used in matching the learned descriptors from Transformers. Our method achieves competitive performance with only 0.84M learnable parameters against the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M parameters), on three benchmarks, HPatch, ETH, Aachen Day-Night.",
    "authors": [
        "Suwichaya Suwanwimolkul",
        "S. Komorita"
    ],
    "venue": "International Conference on Multimedia Retrieval",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work employs an efficient linear attention for the linear computational complexity of Transformers, and proposes a new attentional aggregation that achieves high accuracy by aggregating both the global and local information from sparse keypoints."
    },
    "citationCount": 11,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}