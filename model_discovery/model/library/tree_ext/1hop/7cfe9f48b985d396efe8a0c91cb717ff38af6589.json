{
    "acronym": "7cfe9f48b985d396efe8a0c91cb717ff38af6589",
    "title": "The Advance of GPTs and Language Model in Cyber Security",
    "seed_ids": [
        "gpt2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "7cfe9f48b985d396efe8a0c91cb717ff38af6589",
    "abstract": "Nature language processing (NLP), one of the most remarkable machine learning techniques currently available, is gaining traction with the public and has achieved great success in many applications. Many companies have developed language models, such as BERT, BART models from Google, and GPT (generative pre-trained transformer) series models from OpenAI. GPT is an unsupervised learning model that generates responses and uses unsupervised pre-training and supervised fine-tuning. GPT-2 is a multitask unsupervised learner that completes tasks using an unsupervised pre-trained model, including a zero-shot setting. GPT-3 extends the few-shot learning approach introduced in GPT, which does not require any gradient updates or fine-tuning for specific tasks. InstructGPT focuses on the alignment that could fit human intention by fine-tuning with human feedback. The outputs of InstructGPT significantly improved in truthfulness and were slightly less toxic than GPT-3, but bias and simple mistakes still existed. This paper aims to provide a detailed overview of the technical advancements utilized in GPT, GPT2, GPT3, and InstructGPT, explore the techniques in different models, and focus on the applications in the cybersecurity aspect. This paper compares the upgrade of GPT models and summarizes the SecureBERT model\u2019s effects on cyber security.",
    "authors": [
        "Mingze Gao"
    ],
    "venue": "Highlights in Science Engineering and Technology",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The upgrade of GPT models is compared and the SecureBERT model\u2019s effects on cyber security are summarized, which improves in truthfulness and were slightly less toxic than GPT-3, but bias and simple mistakes still existed."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}