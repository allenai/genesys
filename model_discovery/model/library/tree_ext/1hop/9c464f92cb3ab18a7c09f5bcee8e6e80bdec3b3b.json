{
    "acronym": "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b",
    "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
    "seed_ids": [
        "rfa",
        "lineartransformer",
        "routingtransformer",
        "brt",
        "transformerxl",
        "compressivetransformer",
        "flash",
        "memorizingtrans",
        "2d01b6afbc86cba1cb895dbcd9396b13952bf0e5",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "412e266cddfd87c79087a88ba1e4d11b89a45a13",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5e52d654fd31f04c1bd884cd5480e6af8c95ad50",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "12809bcb734beafeb47876f42e7b438e27fe99fe",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "231e768f0cd280faa0f725bb353262cb4fed08d1",
        "f75d05e759447c2aedb7097728f29f9a520d9bc1",
        "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
        "5d032bd2632b6f5847767f39ce247098c6bbc563",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "64a29bee2e1ad29547d590a3cc26274f4c537145",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
        "830995ef17cc291c13f42dfd9f462137de1d2179",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b",
    "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}",
    "authors": [
        "Lucas D. Lingle"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}