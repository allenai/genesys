{
    "acronym": "c7e1196b998dadff9b1a3f3e6d8af044daeed456",
    "title": "Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR",
    "seed_ids": [
        "performer",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "09e2c7adbed37440d4a339852cfa34e5b660f768",
        "6954a6bb9d6f3e365b26b694c963ae1d62a03444"
    ],
    "s2id": "c7e1196b998dadff9b1a3f3e6d8af044daeed456",
    "abstract": "Conformer models maintain a large number of internal states, the vast majority of which are associated with self-attention layers. With limited memory bandwidth, reading these from memory at each inference step can slow down inference. In this paper, we design an optimized conformer that is small enough to meet on-device restrictions and has fast inference on TPUs. We explore various ideas to improve the execution speed, including replacing lower conformer blocks with convolution-only blocks, strategically downsizing the architecture, and utilizing an RNNAttention-Performer. Our optimized conformer can be readily incorporated into a cascaded-encoder setting, allowing a second-pass decoder to operate on its output and improve the accuracy whenever more resources are available. Altogether, we find that these optimizations can reduce latency by a factor of 6.8x, and come at a reasonable trade-off in quality. With the cascaded second-pass, we show that the recognition accuracy is completely recoverable. Thus, our proposed encoder can double as a strong standalone encoder in on device, and as the first part of a high-performance ASR pipeline.",
    "authors": [
        "Rami Botros",
        "Anmol Gulati",
        "Tara N. Sainath",
        "K. Choromanski",
        "Ruoming Pang",
        "Trevor Strohman",
        "Weiran Wang",
        "Jiahui Yu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An optimized conformer that is small enough to meet on-device restrictions and has fast inference on TPUs, and can double as a strong standalone encoder in on device, and as the first part of a high-performance ASR pipeline."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}