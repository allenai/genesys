{
    "acronym": "8d42407b17cde23d8784ab7e4463c821a22dd517",
    "title": "Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "8d42407b17cde23d8784ab7e4463c821a22dd517",
    "abstract": "As human society transitions into the information age, reduction in our attention span is a contingency, and people who spend time reading lengthy news articles are decreasing rapidly and the need for succinct information is higher than ever before. Therefore, it is essential to provide a quick overview of important news by concisely summarizing the top news article and the most intuitive headline. When humans try to make summaries, they extract the essential information from the source and add useful phrases and grammatical annotations from the original extract. Humans have a unique ability to create abstractions. However, automatic summarization is a complicated problem to solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive text summarization has been ascending as far as prevalence. Numerous innovative strategies have been proposed to develop the current seq2seq models further, permitting them to handle different issues like saliency, familiarity, and human lucidness and create excellent synopses. In this article, we aimed toward enhancing the present architectures and models for abstractive text summarization. The modifications have been aimed at fine-tuning hyper-parameters, attempting specific encoder-decoder combinations. We examined many experiments on an extensively used CNN/DailyMail dataset to check the effectiveness of various models.",
    "authors": [
        "Aditya Saxena",
        "Ashutosh Ranjan"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article examined many experiments on an extensively used CNN/DailyMail dataset to check the effectiveness of various models for abstractive text summarization, and aimed toward enhancing the present architectures and models for abstractive text summarization."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}