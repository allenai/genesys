{
    "acronym": "e29abe559e5d7521afe37e9bd1b5de1b4e1af98a",
    "title": "Gated recurrent neural networks discover attention",
    "seed_ids": [
        "mamba",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "f5e9337477d7a9eb6267d0310549fdefafbb7fe2",
        "55d8837c72863e63259a506b56222d08812699b0",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3"
    ],
    "s2id": "e29abe559e5d7521afe37e9bd1b5de1b4e1af98a",
    "abstract": "Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention under the hood.",
    "authors": [
        "Nicolas Zucchet",
        "Seijin Kobayashi",
        "Yassir Akram",
        "J. Oswald",
        "Maxime Larcher",
        "Angelika Steger",
        "J. Sacramento"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown how RNNs equipped with linear recurrent layers interconnected by feedforward paths with multiplicative gating can exactly implement (linear) self-attention, the main building block of Transformers."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}