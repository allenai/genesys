{
    "acronym": "e8e294084495db4309cd73d86ccce5cce156b7e3",
    "title": "\"What's The Context?\" : Long Context NLM Adaptation for ASR Rescoring in Conversational Agents",
    "seed_ids": [
        "transformerxl",
        "6f22b0da3447eebff003d0f6d4b9ed0864b85d2a",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "a022bda79947d1f656a1164003c1b3ae9a843df9"
    ],
    "s2id": "e8e294084495db4309cd73d86ccce5cce156b7e3",
    "abstract": "Neural Language Models (NLM), when trained and evaluated with context spanning multiple utterances, have been shown to consistently outperform both conventional n-gram language models and NLMs that use limited context. In this paper, we investigate various techniques to incorporate turn based context history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent based NLMs, we explore context carry over mechanism and feature based augmentation, where we incorporate other forms of contextual information such as bot response and system dialogue acts as classi\ufb01ed by a Natural Language Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem with contextual NLM, we propose the use of attention layer over lexical metadata to improve feature based augmentation. Additionally, we adapt our contextual NLM towards user provided on-the-\ufb02y speech patterns by leveraging encodings from a large pre-trained masked language model and performing fusion with a Transformer-XL based NLM. We test our proposed models using N-best rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on downstream NLU tasks such as intent classi\ufb01cation and slot labeling. The best performing model shows a relative WER between 1.6% and 9.1% and a slot labeling F1 score improvement of 4% over non-contextual baselines.",
    "authors": [
        "Ashish Shenoy",
        "S. Bodapati",
        "Monica Sunkara",
        "S. Ronanki",
        "K. Kirchhoff"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper investigates various techniques to incorporate turn based context history into both recurrent (LSTM) and Transformer-XL based NLMs and proposes the use of attention layer over lexical metadata to improve feature based augmentation."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}