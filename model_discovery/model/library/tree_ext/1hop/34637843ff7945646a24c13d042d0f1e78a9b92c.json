{
    "acronym": "34637843ff7945646a24c13d042d0f1e78a9b92c",
    "title": "A Comparative Survey of Instance Selection Methods applied to Non-Neural and Transformer-Based Text Classification",
    "seed_ids": [
        "gpt2",
        "a554a0aae55be5597de8f6ece0a4dd0bd5a0e5f4",
        "adc61e21eafecfbf6ebecc570f9f913659a2bfb2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "34637843ff7945646a24c13d042d0f1e78a9b92c",
    "abstract": "Progress in natural language processing has been dictated by the rule of more: more data, more computing power, more complexity, best exemplified by deep learning Transformers. However, training (or fine-tuning) large dense models for specific applications usually requires significant amounts of computing resources. One way to ameliorate this problem is through data engineering instead of the algorithmic or hardware perspectives. Our focus here is an under-investigated data engineering technique, with enormous potential in the current scenario \u2013 Instance Selection (IS) (a.k.a. Selective Sampling, Prototype Selection). The IS goal is to reduce the training set size by removing noisy or redundant instances while maintaining or improving the effectiveness (accuracy) of the trained models and reducing the training process cost. We survey classical and recent state-of-the-art IS techniques and provide a scientifically sound comparison of IS methods applied to an essential natural language processing task\u2014Automatic Text Classification (ATC). IS methods have been normally applied to small tabular datasets and have not been systematically compared in ATC. We consider several neural and non-neural state-of-the-art ATC solutions and many datasets. We answer several research questions based on tradeoffs induced by a tripod (training set reduction, effectiveness, and efficiency). Our answers reveal an enormous unfulfilled potential for IS solutions. Specially, we show that in 12 out of 19 datasets, specific IS methods\u2014namely, Condensed Nearest Neighbor, Local Set-based Smoother, and Local Set Border Selector\u2014can reduce the size of the training set without effectiveness losses. Furthermore, in the case of fine-tuning the Transformer methods, the IS methods reduce the amount of data needed, without losing effectiveness and with considerable training-time gains.",
    "authors": [
        "Washington Cunha",
        "Felipe Viegas",
        "Celso Fran\u00e7a",
        "T. Rosa",
        "L. Rocha",
        "Marcos Andr\u00e9 Gon\u00e7alves"
    ],
    "venue": "ACM Computing Surveys",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that in 12 out of 19 datasets, specific IS methods\u2014namely, Condensed Nearest Neighbor, Local Set-based Smoother, and Local Set Border Selector\u2014can reduce the size of the training set without effectiveness losses."
    },
    "citationCount": 15,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}