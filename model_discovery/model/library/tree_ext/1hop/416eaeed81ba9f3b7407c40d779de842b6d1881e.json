{
    "acronym": "416eaeed81ba9f3b7407c40d779de842b6d1881e",
    "title": "Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers",
    "seed_ids": [
        "transformer",
        "retnet",
        "f4a0c4154203808f362e4678f3741b3d317fdc82",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "d5e999aae76d5270ef272076979c809817458212",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "416eaeed81ba9f3b7407c40d779de842b6d1881e",
    "abstract": "Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters. We experimented the efficacy of the method on varying sizes and alternative attention architectures and show that \\methodabbr significantly reduces the training time up to 2.5x times and converges to a better minimum with up to 2.6% stronger model on the LM benchmarks within the same compute budget.",
    "authors": [
        "Sehyun Choi"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Cross-Architecture Transfer Learning (XATL) is proposed, in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}