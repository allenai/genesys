{
    "acronym": "b7bca7cc5c1a5d044a41d469eb4a9ea439b88876",
    "title": "Generative Large Language Models Are All-purpose Text Analytics Engines: Text-to-text Learning Is All Your Need",
    "seed_ids": [
        "gpt3",
        "18bd0822f1a605ac8215337597e652ad693cbd93"
    ],
    "s2id": "b7bca7cc5c1a5d044a41d469eb4a9ea439b88876",
    "abstract": "OBJECTIVE\nTo solve major clinical natural language processing (NLP) tasks using a unified text-to-text learning architecture based on a generative large language model (LLM) via prompt tuning.\n\n\nMETHODS\nWe formulated 7 key clinical NLP tasks as text-to-text learning and solved them using one unified generative clinical LLM, GatorTronGPT, developed using GPT-3 architecture and trained with up to 20 billion parameters. We adopted soft prompts (ie, trainable vectors) with frozen LLM, where the LLM parameters were not updated (ie, frozen) and only the vectors of soft prompts were updated, known as prompt tuning. We added additional soft prompts as a prefix to the input layer, which were optimized during the prompt tuning. We evaluated the proposed method using 7 clinical NLP tasks and compared them with previous task-specific solutions based on Transformer models.\n\n\nRESULTS AND CONCLUSION\nThe proposed approach achieved state-of-the-art performance for 5 out of 7 major clinical NLP tasks using one unified generative LLM. Our approach outperformed previous task-specific transformer models by \u223c3% for concept extraction and 7% for relation extraction applied to social determinants of health, 3.4% for clinical concept normalization, 3.4%-10% for clinical abbreviation disambiguation, and 5.5%-9% for natural language inference. Our approach also outperformed a previously developed prompt-based machine reading comprehension (MRC) model, GatorTron-MRC, for clinical concept and relation extraction. The proposed approach can deliver the \"one model for all\" promise from training to deployment using a unified generative LLM.",
    "authors": [
        "C.A.I. Peng",
        "Xi Yang",
        "Aokun Chen",
        "Zehao Yu",
        "Kaleb E. Smith",
        "Anthony B Costa",
        "Mona G Flores",
        "Jiang Bian",
        "Yonghui Wu"
    ],
    "venue": "JAMIA Journal of the American Medical Informatics Association",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The proposed approach can deliver the \"one model for all\" promise from training to deployment using a unified generative LLM via prompt tuning and outperformed previous task-specific transformer models."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}