{
    "acronym": "fd4a67189e64d9ec5b1f6466b4e4bdfb6b88ab2d",
    "title": "Measuring Pointwise $\\mathcal{V}$-Usable Information In-Context-ly",
    "seed_ids": [
        "gpt2",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "8a7df164c4687e3c402b1cbf0ab404de49cfb75e",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fd4a67189e64d9ec5b1f6466b4e4bdfb6b88ab2d",
    "abstract": "In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise $\\mathcal{V}$-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of in-context PVI. Our findings indicate that in-context PVI estimates exhibit similar characteristics to the original PVI. Specific to the in-context setting, we show that in-context PVI estimates remain consistent across different exemplar selections and numbers of shots. The variance of in-context PVI estimates across different exemplar selections is insignificant, which suggests that in-context PVI are stable. Furthermore, we demonstrate how in-context PVI can be employed to identify challenging instances. Our work highlights the potential of in-context PVI and provides new insights into the capabilities of ICL.",
    "authors": [
        "Sheng Lu",
        "Shan Chen",
        "Yingya Li",
        "Danielle S. Bitterman",
        "G. Savova",
        "Iryna Gurevych"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work adapts a recently proposed hardness metric, pointwise $\\mathcal{V}$-usable information (PVI), to an in-context version (in-context PVI), which is more efficient in that it requires only a few exemplars and does not require fine-tuning."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}