{
    "acronym": "7dd25287048fbc8753f412f30d30509e87308071",
    "title": "An Investigation of Suitability of Pre-Trained Language Models for Dialogue Generation \u2013 Avoiding Discrepancies",
    "seed_ids": [
        "gpt",
        "200050c1f51e2c930e62b078c6ce20f2a6675468",
        "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "7dd25287048fbc8753f412f30d30509e87308071",
    "abstract": "Pre-trained language models have been widely used in response generation for open-domain dialogue. These approaches are built within 4 frameworks: Transformer-ED, Transformer-Dec, Transformer-MLM and Transformer-AR. In this study, we experimentally compare them using both large and small-scale data. This reveals that decoder-only architecture is better than stacked encoder-decoder, and both left-to-right and bi-directional attention have their own advantages. We further de\ufb01ne two concepts of model discrepancy, which provides a new explanation to the model performance. As discrepancies may hinder performance, we propose two solutions to reduce them, which successfully improve the model performance.",
    "authors": [
        "Yan Zeng",
        "Jian-Yun Nie"
    ],
    "venue": "Findings",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Comparison of pre-trained language models using both large and small-scale data reveals that decoder-only architecture is better than stacked encoder-decoder, and both left-to-right and bi-directional attention have their own advantages."
    },
    "citationCount": 9,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}