{
    "acronym": "2537af99905a27d9b84ba9968715f4287f1d3359",
    "title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "seed_ids": [
        "flash",
        "gmlp",
        "fnet",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "e0cbbca02b332f398c6639b3bea0613f79166220",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "07a9f47885cae97efb7b4aa109392128532433da",
        "f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
        "2ff74d426e712522030057624510c03713fa77ba"
    ],
    "s2id": "2537af99905a27d9b84ba9968715f4287f1d3359",
    "abstract": "The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones -- the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance -- an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
    "authors": [
        "Michael Hassid",
        "Hao Peng",
        "Daniel Rotem",
        "Jungo Kasai",
        "Ivan Montero",
        "Noah A. Smith",
        "Roy Schwartz"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that without any input-dependent attention, all models achieve competitive performance -- an average relative drop of only 8% from the probing baseline, and it is shown that better-performing models lose more from applying the PAPA method than weaker models, suggesting that the utilization of the input- dependent attention mechanism might be a factor in their success."
    },
    "citationCount": 19,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}