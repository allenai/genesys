{
    "acronym": "e0b1dcb631379aad89130e94f5bf4ede9d22d0f8",
    "title": "Transformer Decoder Based Reinforcement Learning Approach for Conversational Response Generation",
    "seed_ids": [
        "gpt",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e0b1dcb631379aad89130e94f5bf4ede9d22d0f8",
    "abstract": "Developing a machine that can hold an engaging conversation with a human is one of the main challenges in designing a dialogue system in the field of natural language processing. Responses generated by neural conversational models with log-likelihood training methods tend to lack informativeness and diversity. We address the limitation of log-likelihood training in dialogue generation models, and we present the Reinforce Transformer decoder model, our new approach for training the Transformer decoder based conversational model, which incorporates proximal policy optimization techniques from re-inforcement learning with the Transformer decoder architecture. We specifically examine the use of our proposed model for multi-turn dialogue response generation in a real word human to a human dataset. To verify the effectiveness of our proposed framework, we evaluate our model on the Reddit dialogues data, which is a real word human to a human dataset. Experiments show that our proposed response generating model in a dialogue achieves significant improvement over recurrent sequence-to-sequence models and also the state of the art Transformer based dialogue generation models based on diversity and relevance evaluation metrics.",
    "authors": [
        "Farshid Faal",
        "J. Yu",
        "K. Schmitt"
    ],
    "venue": "IEEE International Joint Conference on Neural Network",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work addresses the limitation of log-likelihood training in dialogue generation models, and presents the Reinforce Transformer decoder model, a new approach for training the Trans transformer decoder based conversational model, which incorporates proximal policy optimization techniques from re-inforcement learning with the TransTransform decoder architecture."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}