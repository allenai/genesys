{
    "acronym": "ef626c307729c83153bc24fe390d9f5b4a9a5ed6",
    "title": "Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems",
    "seed_ids": [
        "gpt2",
        "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
        "5d22b241836e30d5b0d852b463951ab7e3245ea4",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ef626c307729c83153bc24fe390d9f5b4a9a5ed6",
    "abstract": "Recent works have shown considerable improvements in task-oriented dialogue (TOD) systems by utilizing pretrained large language models (LLMs) in an end-to-end manner. However, the biased behavior of each component in a TOD system and the error propagation issue in the end-to-end framework can lead to seriously biased TOD responses. Existing works of fairness only focus on the total bias of a system. In this paper, we propose a diagnosis method to attribute bias to each component of a TOD system. With the proposed attribution method, we can gain a deeper understanding of the sources of bias. Additionally, researchers can mitigate biased model behavior at a more granular level. We conduct experiments to attribute the TOD system's bias toward three demographic axes: gender, age, and race. Experimental results show that the bias of a TOD system usually comes from the response generation model.",
    "authors": [
        "Hsuan Su",
        "Rebecca Qian",
        "Chinnadhurai Sankar",
        "Shahin Shayandeh",
        "Shang-Tse Chen",
        "Hung-yi Lee",
        "D. Bikel"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a diagnosis method to attribute bias to each component of a TOD system and conducts experiments to attribute the TOD system's bias toward three demographic axes: gender, age, and race."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}