{
    "acronym": "6cea705be536770d363c56d6db6da51bfb642499",
    "title": "HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization",
    "seed_ids": [
        "poolingformer",
        "bigbird",
        "4eb45f33446018175e266738be22f4d830ed697e",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "6cea705be536770d363c56d6db6da51bfb642499",
    "abstract": "Graph Neural Network (GNN)-based models have proven effective in various Natural Language Processing (NLP) tasks in recent years. Specifically, in the case of the Extractive Document Summarization (EDS) task, modeling documents under graph structure is able to analyze the complex relations between semantic units (e.g., word-to-word, word-to-sentence, sentence-to-sentence) and enrich sentence representations via valuable information from their neighbors. However, long-form document summarization using graph-based methods is still an open research issue. The main challenge is to represent long documents in a graph structure in an effective way. In this regard, this paper proposes a new heterogeneous graph neural network (HeterGNN) model to improve the performance of long document summarization (HeterGraphLongSum). Specifically, the main idea is to add the passage nodes into the heterogeneous graph structure of word and sentence nodes for enriching the final representation of sentences. In this regard, HeterGraphLongSum is designed with three types of semantic units such as word, sentence, and passage. Experiments on two benchmark datasets for long documents such as Pubmed and Arxiv indicate promising results of the proposed model for the extractive long document summarization problem. Especially, HeterGraphLongSum is able to achieve state-of-the-art performance without relying on any pre-trained language models (e.g., BERT). The source code is available for further exploitation on the Github.",
    "authors": [
        "T. Phan",
        "Ngoc Dung Nguyen",
        "Khac-Hoai Nam Bui"
    ],
    "venue": "International Conference on Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experiments on two benchmark datasets for long documents such as Pubmed and Arxiv indicate promising results of the proposed model, HeterGraphLongSum, able to achieve state-of-the-art performance without relying on any pre-trained language models (e.g., BERT)."
    },
    "citationCount": 5,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}