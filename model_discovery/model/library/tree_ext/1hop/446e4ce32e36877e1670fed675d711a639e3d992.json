{
    "acronym": "446e4ce32e36877e1670fed675d711a639e3d992",
    "title": "Multiple Knowledge Syncretic Transformer for Natural Dialogue Generation",
    "seed_ids": [
        "gpt",
        "68e686817f2c33cd09ba3805fa082348f18affd9",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "446e4ce32e36877e1670fed675d711a639e3d992",
    "abstract": "Knowledge is essential for intelligent conversation systems to generate informative responses. This knowledge comprises a wide range of diverse modalities such as knowledge graphs (KGs), grounding documents and conversation topics. However, limited abilities in understanding language and utilizing different types of knowledge still challenge existing approaches. Some researchers try to enhance models\u2019 language comprehension ability by employing the pre-trained language models, but they neglect the importance of external knowledge in specific tasks. In this paper, we propose a novel universal transformer-based architecture for dialogue system, the Multiple Knowledge Syncretic Transformer (MKST), which fuses multi-knowledge in open-domain conversation. Firstly, the model is pre-trained on a large-scale corpus to learn commonsense knowledge. Then during fine-tuning, we divide the type of knowledge into two specific categories that are handled in different ways by our model. While the encoder is responsible for encoding dialogue contexts with multifarious knowledge together, the decoder with a knowledge-aware mechanism attentively reads the fusion of multi-knowledge to promote better generation. This is the first attempt that fuses multi-knowledge in one conversation model. The experimental results have been demonstrated that our model achieves significant improvement on knowledge-driven dialogue generation tasks than state-of-the-art baselines. Meanwhile, our new benchmark could facilitate the further study in this research area.",
    "authors": [
        "Xiangyu Zhao",
        "Longbiao Wang",
        "Ruifang He",
        "Ting Yang",
        "Jinxin Chang",
        "Ruifang Wang"
    ],
    "venue": "The Web Conference",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel universal transformer-based architecture for dialogue system, the Multiple Knowledge Syncretic Transformer (MKST), which fuses multi-knowledge in open-domain conversation and achieves significant improvement on knowledge-driven dialogue generation tasks than state-of-the-art baselines."
    },
    "citationCount": 22,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}