{
    "acronym": "77fde89a0f28cae77fd488ee3b641dee716e9c77",
    "title": "Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors",
    "seed_ids": [
        "s4",
        "s4d",
        "resurrectrnn",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "59b7448f816908cfb49a2ab5e63b2fa5786387f7",
        "54155c2977a977bf129849455dcae3a2b79b3f41",
        "661e8d555c4424b5953f17434f2ba910bfcf3afe",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617",
        "ca444821352a4bd91884413d8070446e2960715a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "77fde89a0f28cae77fd488ee3b641dee716e9c77",
    "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",
    "authors": [
        "Ido Amos",
        "Jonathan Berant",
        "Ankit Gupta"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work finds vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and improves the best reported results of SSMs on the PathX-256 task by 20 absolute points."
    },
    "citationCount": 10,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}