{
    "acronym": "d60a72f5d0b1b035cf26d752bdb0390e30f1a7f5",
    "title": "Knowledge Efficient Deep Learning for Natural Language Processing",
    "seed_ids": [
        "gpt",
        "6491d8bcac8490d9ffde33612f87e15c90a44e97",
        "dec69765fc6c188897b09c8282d32db788e2c261",
        "bb104dc51121a0f64a5327526fad449cb03dd1bb",
        "6ff68b34a5f78bdd14437fe5a79aebbc42c26467",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "d60a72f5d0b1b035cf26d752bdb0390e30f1a7f5",
    "abstract": "Deep learning has become the workhorse for a wide range of natural language processing applications. But much of the success of deep learning relies on annotated examples. Annotation is time-consuming and expensive to produce at scale. Here we are interested in methods for reducing the required quantity of annotated data -- by making the learning methods more knowledge efficient so as to make them more applicable in low annotation (low resource) settings. There are various classical approaches to making the models more knowledge efficient such as multi-task learning, transfer learning, weakly supervised and unsupervised learning etc. This thesis focuses on adapting such classical methods to modern deep learning models and algorithms. \nThis thesis describes four works aimed at making machine learning models more knowledge efficient. First, we propose a knowledge rich deep learning model (KRDL) as a unifying learning framework for incorporating prior knowledge into deep models. In particular, we apply KRDL built on Markov logic networks to denoise weak supervision. Second, we apply a KRDL model to assist the machine reading models to find the correct evidence sentences that can support their decision. Third, we investigate the knowledge transfer techniques in multilingual setting, where we proposed a method that can improve pre-trained multilingual BERT based on the bilingual dictionary. Fourth, we present an episodic memory network for language modelling, in which we encode the large external knowledge for the pre-trained GPT.",
    "authors": [
        "Hai Wang"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A knowledge rich deep learning model (KRDL) is proposed as a unifying learning framework for incorporating prior knowledge into deep models and an episodic memory network for language modelling is presented, in which the large external knowledge is encoded for the pre-trained GPT."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}