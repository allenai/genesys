{
    "acronym": "7e3864e2ab94cd5bc54830392dcdb49a927f1ba6",
    "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
    "seed_ids": [
        "gpt3",
        "42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
        "914254fac74a2da051cccf6ca16afcaad416a079",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "7e3864e2ab94cd5bc54830392dcdb49a927f1ba6",
    "abstract": "In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language'' with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rigorous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2L-Tokenizer.",
    "authors": [
        "Lei Zhu",
        "Fangyun Wei",
        "Yanye Lu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Vision-to-Language Tokenizer is presented, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language'' with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}