{
    "acronym": "10da92399630f5ee4e37f51ac21d278d829e9253",
    "title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
    "seed_ids": [
        "gpt",
        "gpt2",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "10da92399630f5ee4e37f51ac21d278d829e9253",
    "abstract": "In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese language examinations. CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios. From the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques.Furthermore, we evaluate a range of existing large language models~(LLMs), spanning from open-sourced to API-based models. Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities.",
    "authors": [
        "Jiahao Zhao",
        "Jingwei Zhu",
        "Minghuan Tan",
        "Min Yang",
        "Di Yang",
        "Chenhao Zhang",
        "Guancheng Ye",
        "Chengming Li",
        "Xiping Hu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese language examinations, and evaluates a range of existing large language models~(LLMs), spanning from open-sourced to API-based models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}