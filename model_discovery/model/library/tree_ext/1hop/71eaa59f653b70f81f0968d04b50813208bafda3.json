{
    "acronym": "71eaa59f653b70f81f0968d04b50813208bafda3",
    "title": "Pretrained Hybrids with MAD Skills",
    "seed_ids": [
        "mamba",
        "mechdesignscaling",
        "fe33be95849c556ce0bdffaa1d2c7db9bb2e2c61",
        "05c1dc502ed51162580ccd320d5668d2fec94a7a",
        "d53fe76bd2795a19ddf52d012917782f6f6f2c1e",
        "f4a0c4154203808f362e4678f3741b3d317fdc82",
        "77fde89a0f28cae77fd488ee3b641dee716e9c77",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "71ba5f845bd22d42003675b7cea970ca9e590bcc",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "6f68e1bb253925d8431588555d3010419f322e04"
    ],
    "s2id": "71eaa59f653b70f81f0968d04b50813208bafda3",
    "abstract": "While Transformers underpin modern large language models (LMs), there is a growing list of alternative architectures with new capabilities, promises, and tradeoffs. This makes choosing the right LM architecture challenging. Recently-proposed $\\textit{hybrid architectures}$ seek a best-of-all-worlds approach that reaps the benefits of all architectures. Hybrid design is difficult for two reasons: it requires manual expert-driven search, and new hybrids must be trained from scratch. We propose $\\textbf{Manticore}$, a framework that addresses these challenges. Manticore $\\textit{automates the design of hybrid architectures}$ while reusing pretrained models to create $\\textit{pretrained}$ hybrids. Our approach augments ideas from differentiable Neural Architecture Search (NAS) by incorporating simple projectors that translate features between pretrained blocks from different architectures. We then fine-tune hybrids that combine pretrained models from different architecture families -- such as the GPT series and Mamba -- end-to-end. With Manticore, we enable LM selection without training multiple models, the construction of pretrained hybrids from existing pretrained models, and the ability to $\\textit{program}$ pretrained hybrids to have certain capabilities. Manticore hybrids outperform existing manually-designed hybrids, achieve strong performance on Long Range Arena (LRA) tasks, and can improve on pretrained transformers and state space models.",
    "authors": [
        "Nicholas Roberts",
        "Samuel Guo",
        "Zhiqi Gao",
        "Satya Sai Srinath Namburi",
        "Sonia Cromp",
        "Chengjun Wu",
        "Chengyu Duan",
        "Frederic Sala"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Manticore automates the design of hybrid architectures while reusing pretrained models to create hybrids, which outperform existing manually-designed hybrids, achieve strong performance on Long Range Arena (LRA) tasks, and can improve on pretrained transformers and state space models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}