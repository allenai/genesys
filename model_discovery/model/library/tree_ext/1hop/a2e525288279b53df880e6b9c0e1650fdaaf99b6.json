{
    "acronym": "a2e525288279b53df880e6b9c0e1650fdaaf99b6",
    "title": "Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care",
    "seed_ids": [
        "bert",
        "91663fbb9257b1ee5d22d7b9e796e03f374a7a6a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "7b72dab2d1a8a1d91b6b8ad5f9ddef71a0c29a0e"
    ],
    "s2id": "a2e525288279b53df880e6b9c0e1650fdaaf99b6",
    "abstract": "Pre-trained Language Models (PLMs) have the potential to transform mental health support by providing accessible and culturally sensitive resources. However, despite this potential, their effectiveness in mental health care and specifically for the Arabic language has not been extensively explored. To bridge this gap, this study evaluates the effectiveness of foundational models for classification of Questions and Answers (Q&A) in the domain of mental health care. We leverage the MentalQA dataset, an Arabic collection featuring Q&A interactions related to mental health. In this study, we conducted experiments using four different types of learning approaches: traditional feature extraction, PLMs as feature extractors, Fine-tuning PLMs and prompting large language models (GPT-3.5 and GPT-4) in zero-shot and few-shot learning settings. While traditional feature extractors combined with Support Vector Machines (SVM) showed promising performance, PLMs exhibited even better results due to their ability to capture semantic meaning. For example, MARBERT achieved the highest performance with a Jaccard Score of 0.80 for question classification and a Jaccard Score of 0.86 for answer classification. We further conducted an in-depth analysis including examining the effects of fine-tuning versus non-fine-tuning, the impact of varying data size, and conducting error analysis. Our analysis demonstrates that fine-tuning proved to be beneficial for enhancing the performance of PLMs, and the size of the training data played a crucial role in achieving high performance. We also explored prompting, where few-shot learning with GPT-3.5 yielded promising results. There was an improvement of 12% for question and classification and 45% for answer classification. Based on our findings, it can be concluded that PLMs and prompt-based approaches hold promise for mental health support in Arabic.",
    "authors": [
        "Hassan Alhuzali",
        "Ashwag Alasmari"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study evaluates the effectiveness of foundational models for classification of Questions and Answers (Q&A) in the domain of mental health care and concludes that PLMs and prompt-based approaches hold promise for mental health support in Arabic."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}