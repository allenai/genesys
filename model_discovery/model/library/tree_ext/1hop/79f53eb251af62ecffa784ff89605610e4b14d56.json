{
    "acronym": "79f53eb251af62ecffa784ff89605610e4b14d56",
    "title": "A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting",
    "seed_ids": [
        "performer",
        "30f75d952a5d1e4d4a245963da55f0b4606c2182",
        "016e99ccef0afb7011f3ea3b1b932611c8fb957b",
        "8064d3873c646dc9ff949d72c54c634a906fc092",
        "defecf3dc299214f4cb76a093c6eed2297eaa46f",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "6c761cfdb031701072582e434d8f64d436255da6",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9"
    ],
    "s2id": "79f53eb251af62ecffa784ff89605610e4b14d56",
    "abstract": "In order to enhance the performance of Transformer models for long-term multivariate forecasting while minimizing computational demands, this paper introduces the Joint Time-Frequency Domain Transformer (JTFT). JTFT combines time and frequency domain representations to make predictions. The frequency domain representation efficiently extracts multi-scale dependencies while maintaining sparsity by utilizing a small number of learnable frequencies. Simultaneously, the time domain (TD) representation is derived from a fixed number of the most recent data points, strengthening the modeling of local relationships and mitigating the effects of non-stationarity. Importantly, the length of the representation remains independent of the input sequence length, enabling JTFT to achieve linear computational complexity. Furthermore, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies, thus preventing performance degradation resulting from the entanglement of temporal and channel-wise modeling. Experimental results on eight real-world datasets demonstrate that JTFT outperforms state-of-the-art baselines in predictive performance.",
    "authors": [
        "Yushu Chen",
        "Shengzhuo Liu",
        "Jinzhe Yang",
        "Hao Jing",
        "Wenlai Zhao",
        "Guang-Wu Yang"
    ],
    "venue": "Neural Networks",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "JTFT combines time and frequency domain representations to make predictions, and a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies, thus preventing performance degradation resulting from the entanglement of temporal and channel-wise modeling."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}