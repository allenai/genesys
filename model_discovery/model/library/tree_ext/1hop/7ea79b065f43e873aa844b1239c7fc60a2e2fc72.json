{
    "acronym": "7ea79b065f43e873aa844b1239c7fc60a2e2fc72",
    "title": "Cost-Effective Tweet Classification through Transfer Learning in Low-Resource NLP Settings",
    "seed_ids": [
        "bert",
        "adc61e21eafecfbf6ebecc570f9f913659a2bfb2",
        "069e0d896da7c79faeee4cf057548d5da7ce885e",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "7ea79b065f43e873aa844b1239c7fc60a2e2fc72",
    "abstract": "Recently, pre-trained Transformer-based Language Models (TLM) for specific task has led to significant advancements in Natural Language Processing (NLP) tasks, achieving state-of-the-art results. Despite the ability of TLM and its benefits for low-resource languages where labelled data is scarce. Limited research has been conducted on investigating the applicability of TLM in low-resource settings for French tweets classification. In this paper, our focus is on exploring the effectiveness of Transfer Learning using fine-tuned Transformer language models, specifically FlauBERT and XLM-RoBERTa applied to French tweet dataset. We investigate their ability to achieve higher performance in text classification tasks while requiring less training data compared to traditional text classification methods.Experiments conducted on our labeled dataset consisting of 4000 French tweets reveal that FlauBERT outperforms its rival, achieving an average F1-score of 0.84 compared to XLM-R\u2019s 0.79. However, in scenarios with even fewer available data, XLM-R has the potential to surpass FlauBERT.Furthermore, the environmental sustainability is considered while highlighting the potential CO2 emissions reduction through manipulating the layers of FlauBERT without compromising the performance.",
    "authors": [
        "Sarkis Elio",
        "Darazi Rony",
        "Tannoury Anthony"
    ],
    "venue": "2023 IEEE 4th International Multidisciplinary Conference on Engineering Technology (IMCET)",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper investigates FlauBERT and XLM-RoBERTa ability to achieve higher performance in text classification tasks while requiring less training data compared to traditional text classification methods, and the environmental sustainability is considered while highlighting the potential CO2 emissions reduction through manipulating the layers of FlauBERT without compromising the performance."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}