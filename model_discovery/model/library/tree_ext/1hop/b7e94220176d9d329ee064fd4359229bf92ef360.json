{
    "acronym": "b7e94220176d9d329ee064fd4359229bf92ef360",
    "title": "Reconsidering the Past: Optimizing Hidden States in Language Models",
    "seed_ids": [
        "transformerxl",
        "f51497f463566581874c941353dd9d80069c5b77",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "cd63025532a62fa245a02ec05e32ac4d23089631",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b7e94220176d9d329ee064fd4359229bf92ef360",
    "abstract": "We present Hidden-State Optimization (HSO), a gradient-based method for improving the performance of transformer language models at inference time. Similar to dynamic evaluation (Krause et al., 2018), HSO computes the gradient of the log-probability the language model assigns to an evaluation text, but uses it to update the cached hidden states rather than the model parameters. We test HSO with pretrained Transformer-XL and GPT-2 language models, finding improvement on the WikiText103 and PG-19 datasets in terms of perplexity, especially when evaluating a model outside of its training distribution. We also demonstrate downstream applicability by showing gains in the recently developed prompt-based few-shot evaluation setting, again with no extra parameters or training data.",
    "authors": [
        "Davis Yoshida",
        "Kevin Gimpel"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Hidden-State Optimization is presented, a gradient-based method for improving the performance of transformer language models at inference time that computes the gradient of the log-probability the language model assigns to an evaluation text but uses it to update the cached hidden states rather than the model parameters."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}