{
    "acronym": "e866687bc3053c0a38aa2847122f50c573df29dc",
    "title": "A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries",
    "seed_ids": [
        "gpt3",
        "daaa7d4ffb9265226e4baadd2db9a01aa7b2f6fb",
        "b15469d0ab3dc3a9dec037d761817b3fe546bed6",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e866687bc3053c0a38aa2847122f50c573df29dc",
    "abstract": "Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performance of these LLMs across varying context-length inputs using conventional natural language similarity metrics. We further perform a qualitative study where five diverse clinicians blindly compare clinician-written BHCs and two LLM-generated BHCs for 30 samples across metrics of comprehensiveness, conciseness, factual correctness, and fluency. Overall, we present a new benchmark and pre-processed dataset for using LLMs in BHC synthesis from clinical notes. We observe high-quality summarization performance for both in-context proprietary and fine-tuned open-source LLMs using both quantitative metrics and a qualitative clinical reader study. We propose our work as a benchmark to motivate future works to adapt and assess the performance of LLMs in BHC synthesis.",
    "authors": [
        "Asad Aali",
        "Dave Van Veen",
        "Yamin Arefeen",
        "Jason Hom",
        "Christian Bluethgen",
        "E. Reis",
        "S. Gatidis",
        "Namuun Clifford",
        "Joseph Daws",
        "A. S. Tehrani",
        "Jangwon Kim",
        "Akshay S. Chaudhari"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs is introduced and proposed as a benchmark to motivate future works to adapt and assess the performance of LLMs in BHC synthesis."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}