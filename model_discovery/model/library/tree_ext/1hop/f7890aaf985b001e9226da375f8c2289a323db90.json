{
    "acronym": "f7890aaf985b001e9226da375f8c2289a323db90",
    "title": "Non-stationary Transformers: Rethinking the Stationarity in Time Series Forecasting",
    "seed_ids": [
        "reformer",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "f7890aaf985b001e9226da375f8c2289a323db90",
    "abstract": "Transformers have shown great power in time series forecasting due to their global-range modeling ability. However, their performance can degenerate terribly on non-stationary real-world data in which the joint distribution changes over time. Previous studies primarily adopt stationarization to reduce the non-stationarity of original series for better predictability. But the stationarized series deprived of inherent non-stationarity can be less instructive for real-world bursty events forecasting. This problem, termed over-stationarization in this paper, leads Transformers to generate indistinguishable temporal attentions for different series and impedes the predictive capability of deep models. To tackle the dilemma between series predictability and model capability, we propose Non-stationary Transformers as a generic framework with two interdependent modules: Series Stationarization and De-stationary Attention. Concretely, Series Stationarization uni\ufb01es the statistics of each input and converts the output with restored statistics for better predictability. To address over-stationarization, De-stationary Attention is devised to recover the intrinsic non-stationary information into temporal dependencies by approximating distinguishable attentions learned from unstationarized series. Our Non-stationary Transformers framework consistently boosts mainstream Transformers by a large margin, which reduces 49.43% MSE on Transformer, 47.34% on Informer, and 46.89% on Reformer, making them the state-of-the-art in time series forecasting. For the input-96-predict-336 long- term setting, Non-stationary Transformer surpasses previous best results by 4.4% ( 0 . 615 \u2192 0 . 588 ) in ETTh1, 3.5% ( 0 . 572 \u2192 0 . 552 ) in ETTh2 and 26.7% ( 0 . 675 \u2192 0 . 495 ) MSE reduction in ETTm1. These results show averaged 11.5% MSE reduction over previous state-of-the-art deep forecasting models. effectiveness of our can be for future research. And our work introduces an essential and promising direction to further improve forecasting performance: to increase the stationarity of time series towards better predictability and mitigate the over-stationarization problem for predictive capability of deep models simultaneously.",
    "authors": [
        "Yong Liu",
        "Haixu Wu",
        "Jianmin Wang",
        "Mingsheng Long"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Non-stationary Transformers framework consistently boosts mainstream Transformers by a large margin, making them the state-of-the-art in time series forecasting, and mitigate the over-stationarization problem for predictive capability of deep models simultaneously."
    },
    "citationCount": 42,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}