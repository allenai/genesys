{
    "acronym": "862479f7bd78a69c52a0691766848caa8eb4660f",
    "title": "Linearizing Large Language Models",
    "seed_ids": [
        "mamba",
        "retnet",
        "lineartransformer",
        "3fd5bc3077d04965eaa3498372c39bbdd09d55e4",
        "d53fe76bd2795a19ddf52d012917782f6f6f2c1e",
        "f4a0c4154203808f362e4678f3741b3d317fdc82",
        "85447eeb6e5276e713957835125a2273f9ac0694",
        "7294c426b8a95975ca932eaf8f700acdd3d950b2",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "d5e999aae76d5270ef272076979c809817458212",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "862479f7bd78a69c52a0691766848caa8eb4660f",
    "abstract": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.",
    "authors": [
        "Jean-Pierre Mercat",
        "Igor Vasiljevic",
        "Sedrick Scott Keh",
        "Kushal Arora",
        "Achal Dave",
        "Adrien Gaidon",
        "Thomas Kollar"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}