{
    "acronym": "04c3f1debbba3a856f26c58edc585d8688139740",
    "title": "Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform",
    "seed_ids": [
        "fnet",
        "be76b0f32e287d866bc7aefc700052f9825ed3ce",
        "7618f17179bb316002cb6cc472d61382776af6b7",
        "5ac3e887de399e6c9149b0356e8992460be51ebd",
        "fd33e77884e69f6bc099990fc2790248af2749d9",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "1e3e65e7773b7869d9bd7f5394b54199e48195e6",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "04c3f1debbba3a856f26c58edc585d8688139740",
    "abstract": "Since its introduction, the transformers architecture has seen great adoption in NLP applications, but it also has limitations. Although the self-attention mechanism allows for generating very rich representations of the input text, its effectiveness may be limited in specialized domains such as legal, where, for example, language models often have to process very long texts. In this paper, we explore alternatives to replace the attention-based layers with simpler token-mixing mechanisms: Hartley and Fourier transforms. Using these non-parametric techniques, we train models with long input documents from scratch in the legal domain setting. We also introduce a new hybrid Seq2Seq architecture, a no-attention-based encoder connected with an attention-based decoder, which performs quite well on existing summarization tasks with much less compute and memory requirements. We believe that similar, if not better performance, as in the case of long correlations of abstractive text summarization tasks, can be achieved by adopting these simpler infrastructures. This not only makes training models from scratch accessible to more people, but also contributes to the reduction of the carbon footprint during training.",
    "authors": [
        "Daniele Giofr'e",
        "Sneha Ghantasala"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper explores alternatives to replace the attention-based layers with simpler token-mixing mechanisms: Hartley and Fourier transforms, and introduces a new hybrid Seq2Seq architecture, a no-attention-based encoder connected with an attention- based decoder that performs quite well on existing summarization tasks with much less compute and memory requirements."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}