{
    "acronym": "b7a4a942502a7b34baa498cc34fe6c138779d7a8",
    "title": "Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model",
    "seed_ids": [
        "lineartransformer",
        "longformer",
        "transformerxl",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06"
    ],
    "s2id": "b7a4a942502a7b34baa498cc34fe6c138779d7a8",
    "abstract": "Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments and apply attention to the individual segments. We propose a segmented recurrent transformer (SRformer) that combines segmented (local) attention with recurrent attention. The loss caused by reducing the attention window length is compensated by aggregating information across segments with recurrent attention. SRformer leverages Recurrent Accumulate-and-Fire (RAF) neurons' inherent memory to update the cumulative product of keys and values. The segmented attention and lightweight RAF neurons ensure the efficiency of the proposed transformer. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. We apply the proposed method to T5 and BART transformers. The modified models are tested on summarization datasets including CNN-dailymail, XSUM, ArXiv, and MediaSUM. Notably, using segmented inputs of varied sizes, the proposed model achieves $6-22\\%$ higher ROUGE1 scores than a segmented transformer and outperforms other recurrent transformer approaches. Furthermore, compared to full attention, the proposed model reduces the computational complexity of cross attention by around $40\\%$.",
    "authors": [
        "Yinghan Long",
        "Sayeed Shafayet Chowdhury",
        "Kaushik Roy"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A segmented recurrent transformer that combines segmented (local) attention with recurrent attention that achieves higher ROUGE1 scores than a segmented transformer and outperforms other recurrent transformer approaches."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}