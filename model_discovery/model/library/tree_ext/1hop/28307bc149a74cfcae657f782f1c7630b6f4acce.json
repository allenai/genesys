{
    "acronym": "28307bc149a74cfcae657f782f1c7630b6f4acce",
    "title": "Contextualized Embeddings based Transformer Encoder for Sentence Similarity Modeling in Answer Selection Task",
    "seed_ids": [
        "transformerxl",
        "8659bf379ca8756755125a487c43cfe8611ce842"
    ],
    "s2id": "28307bc149a74cfcae657f782f1c7630b6f4acce",
    "abstract": "Word embeddings that consider context have attracted great attention for various natural language processing tasks in recent years. In this paper, we utilize contextualized word embeddings with the transformer encoder for sentence similarity modeling in the answer selection task. We present two different approaches (feature-based and fine-tuning-based) for answer selection. In the feature-based approach, we utilize two types of contextualized embeddings, namely the Embeddings from Language Models (ELMo) and the Bidirectional Encoder Representations from Transformers (BERT) and integrate each of them with the transformer encoder. We find that integrating these contextual embeddings with the transformer encoder is effective to improve the performance of sentence similarity modeling. In the second approach, we fine-tune two pre-trained transformer encoder models for the answer selection task. Based on our experiments on six datasets, we find that the fine-tuning approach outperforms the feature-based approach on all of them. Among our fine-tuning-based models, the Robustly Optimized BERT Pretraining Approach (RoBERTa) model results in new state-of-the-art performance across five datasets.",
    "authors": [
        "Md Tahmid Rahman Laskar",
        "Xiangji Huang",
        "Enamul Hoque"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper utilizes contextualized word embeddings with the transformer encoder for sentence similarity modeling in the answer selection task and fine-tune two pre-trained transformer encoding models for theanswer selection task."
    },
    "citationCount": 79,
    "influentialCitationCount": 14,
    "code": null,
    "description": null,
    "url": null
}