{
    "acronym": "65c21a6b8203a6f989d6a388f0f6fa1f7e984ddc",
    "title": "Compressive Performers in Language Modelling",
    "seed_ids": [
        "compressivetransformer",
        "77706ee4cbdbb23345da22af37bc1b9f5ec8f110",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "65c21a6b8203a6f989d6a388f0f6fa1f7e984ddc",
    "abstract": "This work introduces the Compressive Performer, a hybrid Transformer variant based on two existing model architectures: the Performer, which reduces the memory requirement and processing time of the Transformer to linear complexity, and the Compressive Transformer, which re-tains contextual dependencies over a long range by compressing old activations instead of discarding them. Experiments in language modelling at the character level, the word level, and the sub-word level demonstrate that the Compressive Per-former shows improved perplexity scores on the enwik-8 dataset, compared to its base models. This work also compares convolutional compression with autoen-coder compression, determining that both show similar perplexity scores.",
    "authors": [
        "Anjali Ragupathi",
        "Siddharth Shanmuganathan",
        "Manu Madhavan"
    ],
    "venue": "International Conference on Natural Language and Speech Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}