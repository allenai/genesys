{
    "acronym": "83fac78857c7e65fe10a11a798674dd3cd259c1d",
    "title": "Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs",
    "seed_ids": [
        "memcompress",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "83fac78857c7e65fe10a11a798674dd3cd259c1d",
    "abstract": "Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.",
    "authors": [
        "Angela Fan",
        "Claire Gardent",
        "Chlo\u00e9 Braud",
        "Antoine Bordes"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting, which compresses the web search information and reduces redundancy."
    },
    "citationCount": 98,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}