{
    "acronym": "33d4763ee93452505fbd5b91e52dc567354bd9cc",
    "title": "Dynamic Temporal Dependency Model for Multiple Steps Ahead Short-Term Load Forecasting of Power System",
    "seed_ids": [
        "transformer",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530"
    ],
    "s2id": "33d4763ee93452505fbd5b91e52dc567354bd9cc",
    "abstract": "Short-term load forecasting (STLF) plays a crucial role in the efficient and economical management of power systems. The primary challenge of STLF is effectively capturing the long-term dynamic temporal dependency while ensuring accurate multiple steps ahead forecasting. To address this issue, this paper proposes a novel dynamic temporal dependency model (DTDM), which is a modified version of the transformer algorithm for the nature language process to be suitable for implementing the mechanism of STLF. With an encoder-decoder structure, DTDM allows for easy modification of the historical and forecast range, enabling straightforward learning of multi-step temporal dependencies to mitigate error accumulation. Moreover, DTDM leverages the multi-head attention mechanism to recognize and aggregate diverse dynamic temporal dependencies by capturing similarities among different timestamps. Notably, DTDM expands the range of the attention mechanism to encompass both short and long temporal dependencies within the same attention head. We conducted extensive experiments comparing DTDM with several baseline models using a public dataset from Switzerland and a private dataset from Guiyang, China. The two numerical results demonstrate the effectiveness of the proposed model, which achieves a minimum average improvement of 12.22% and 4.28% in terms of mean absolute percentage error, respectively. Furthermore, the attention map visualizations further verify DTDM's ability of modeling dynamic temporal dependency.",
    "authors": [
        "Bozhen Jiang",
        "Hongyuan Yang",
        "Yidi Wang",
        "Yi Liu",
        "Hua Geng",
        "Huarong Zeng",
        "Jiangqiao Ding"
    ],
    "venue": "IEEE transactions on industry applications",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel dynamic temporal dependency model (DTDM), which is a modified version of the transformer algorithm for the nature language process to be suitable for implementing the mechanism of STLF, is proposed, enabling straightforward learning of multi-step temporal dependencies to mitigate error accumulation."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}