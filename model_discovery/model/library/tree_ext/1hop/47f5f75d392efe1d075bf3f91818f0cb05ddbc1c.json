{
    "acronym": "47f5f75d392efe1d075bf3f91818f0cb05ddbc1c",
    "title": "Transformers need glasses! Information over-squashing in language tasks",
    "seed_ids": [
        "transformer",
        "b842b83a7ff5dff8e3b83915d8c15423b6085728",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a"
    ],
    "s2id": "47f5f75d392efe1d075bf3f91818f0cb05ddbc1c",
    "abstract": "We study how information propagates in decoder-only Transformers, which are the architectural backbone of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis -- specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a representational collapse phenomenon: we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways -- leading to errors in, e.g., tasks involving counting or copying. Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory also points to simple solutions towards ameliorating these issues.",
    "authors": [
        "Federico Barbero",
        "Andrea Banino",
        "Steven Kapturowski",
        "D. Kumaran",
        "J. G. Ara'ujo",
        "Alex Vitvitskyi",
        "Razvan Pascanu",
        "Petar Velivckovi'c"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is proved that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token, which relates to the well-known phenomenon of over-squashing in graph neural networks."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}