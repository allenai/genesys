{
    "acronym": "c82d20f93eecc764a41fca89c2f78c758fe91dc4",
    "title": "An Introduction of Deep Learning Based Word Representation Applied to Natural Language Processing",
    "seed_ids": [
        "gpt",
        "67b1f8e48118bb1aa250f400d475425317bf4117",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "c82d20f93eecc764a41fca89c2f78c758fe91dc4",
    "abstract": "In the area of Natural Language Processing, high-quality word representations play key roles in neural language processing tasks. Recently, various model designs and methods have blossomed in the domain of word representation. In this paper, we will explain the theories of two major language models such as autoencoding (AE) and autoregressive (AR), and illustrate the architectures of several notable examples of AE and AR including ELMo, GPT, BERT and XLnet. By comparing the pros and cons of these models theoretically and the performances of these models in experiments, we will deepen the understanding of various learning methods and realize the trend in the development of language models.",
    "authors": [
        "Z. Fu"
    ],
    "venue": "2019 International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The theories of two major language models such as autoencoding (AE) and autoregressive (AR) are explained, and the architectures of several notable examples of AE and AR including ELMo, GPT, BERT and XLnet are illustrated."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}