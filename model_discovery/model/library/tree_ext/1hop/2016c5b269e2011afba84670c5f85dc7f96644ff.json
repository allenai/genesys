{
    "acronym": "2016c5b269e2011afba84670c5f85dc7f96644ff",
    "title": "PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models",
    "seed_ids": [
        "ring",
        "28f59093730d88719b96041f9544c73671f798bd",
        "29053ced75c38a121f2a76ffb36880614c7e188a",
        "6dc5c6190dfbe55c8b45b7b23800614c21e5b51c",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "2016c5b269e2011afba84670c5f85dc7f96644ff",
    "abstract": "This paper introduces PipeFusion, a novel approach that harnesses multi-GPU parallelism to address the high computational and latency challenges of generating high-resolution images with diffusion transformers (DiT) models. PipeFusion splits images into patches and distributes the network layers across multiple devices. It employs a pipeline parallel manner to orchestrate communication and computations. By leveraging the high similarity between the input from adjacent diffusion steps, PipeFusion eliminates the waiting time in the pipeline by reusing the one-step stale feature maps to provide context for the current step. Our experiments demonstrate that it can generate higher image resolution where existing DiT parallel approaches meet OOM. PipeFusion significantly reduces the required communication bandwidth, enabling DiT inference to be hosted on GPUs connected via PCIe rather than the more costly NVLink infrastructure, which substantially lowers the overall operational expenses for serving DiT models. Our code is publicly available at https://github.com/PipeFusion/PipeFusion.",
    "authors": [
        "Jiannan Wang",
        "Jiarui Fang",
        "Aoyu Li",
        "PengCheng Yang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "PipeFusion significantly reduces the required communication bandwidth, enabling DiT inference to be hosted on GPUs connected via PCIe rather than the more costly NVLink infrastructure, which substantially lowers the overall operational expenses for serving DiT models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}