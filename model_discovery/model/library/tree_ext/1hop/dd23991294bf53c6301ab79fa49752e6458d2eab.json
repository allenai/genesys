{
    "acronym": "dd23991294bf53c6301ab79fa49752e6458d2eab",
    "title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers",
    "seed_ids": [
        "alibi",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "5d2289dea222330dcab41b3fbec12a6de9c91365"
    ],
    "s2id": "dd23991294bf53c6301ab79fa49752e6458d2eab",
    "abstract": "Speech-driven 3D facial animation is challenging due to the complex geometry of human faces and the limited availability of 3D audio-visual data. Prior works typically focus on learning phoneme-level features of short audio windows with limited context, occasionally resulting in inaccurate lip movements. To tackle this limitation, we propose a Transformer-based autoregressive model, Face-Former, which encodes the long-term audio context and autoregressively predicts a sequence of animated 3D face meshes. To cope with the data scarcity issue, we integrate the self-supervised pre-trained speech representations. Also, we devise two biased attention mechanisms well suited to this specific task, including the biased cross-modal multi-head (MH) attention and the biased causal MH self-attention with a periodic positional encoding strategy. The former effectively aligns the audio-motion modalities, whereas the latter offers abilities to generalize to longer audio sequences. Extensive experiments and a perceptual user study show that our approach outperforms the existing state-of-the-arts. The code and the video are available at: https://evelynfan.github.io/audio2face/",
    "authors": [
        "Yingruo Fan",
        "Zhaojiang Lin",
        "Jun Saito",
        "Wenping Wang",
        "T. Komura"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Transformer-based autoregressive model, Face-Former, is proposed, which encodes the long-term audio context and autoregressively predicts a sequence of animated 3D face meshes and devise two biased attention mechanisms well suited to this specific task."
    },
    "citationCount": 125,
    "influentialCitationCount": 46,
    "code": null,
    "description": null,
    "url": null
}