{
    "acronym": "2c6a6eb161c04d1f4149b38321b23878d24f2da3",
    "title": "A survey on Transformers for long sentences and its application to medical notes",
    "seed_ids": [
        "bigbird",
        "f30e95be411456a709e7cb9a8b3a3e557bd0356a",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2a31319e73d4486716168b65cdf7559baeda18ce"
    ],
    "s2id": "2c6a6eb161c04d1f4149b38321b23878d24f2da3",
    "abstract": "The Self-attention mechanism in Transformer has been successful so far. However, it needed adequate performance in electronic health records and text summarization tasks that operate on long sentences due to computational complexity during self-attention calculation. Therefore, this paper provides a survey on the proposed method to uncover sparse attention that operates on long sentences and examine the case of improving the performance in the EHR task.",
    "authors": [
        "Jangyeong Jeon",
        "Junyeong Kim"
    ],
    "venue": "International Conference on Electronics, Information and Communications",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A survey on the proposed method to uncover sparse attention that operates on long sentences and the case of improving the performance in the EHR task is examined."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}