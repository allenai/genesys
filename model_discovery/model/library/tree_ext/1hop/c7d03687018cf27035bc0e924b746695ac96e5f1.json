{
    "acronym": "c7d03687018cf27035bc0e924b746695ac96e5f1",
    "title": "PIM-DL: Expanding the Applicability of Commodity DRAM-PIMs for Deep Learning via Algorithm-System Co-Optimization",
    "seed_ids": [
        "bert",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "6e8f35c6d54acb14109c9b792a62609eac8a7b5e",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c7d03687018cf27035bc0e924b746695ac96e5f1",
    "abstract": "DRAM-based processing-in-memory (DRAM-PIM) has gained commercial prominence in recent years. However, their integration for deep learning acceleration poses inherent challenges. Existing DRAM-PIMs are limited in computational capabilities, primarily applicable for element-wise and GEMV operators. Unfortunately, these operators contribute only a small portion of the execution time in most DNN workloads. Current systems still necessitate powerful hosts to handle a significant portion of compute-heavy operators. To expand the applicability of commodity DRAM-PIMs in accelerating deep learning, we introduce a novel PIM-DL framework. The philosophy behind PIM-DL is to replace the compute-heavy GEMM operations in linear layers with Lookup-Tables (LUTs). Such LUT-based neural networks (LUT-NNs) substantially reduce multiplications in DNN inference, rendering them suitable for efficient execution on DRAM-PIMs. To accurately convert DNNs into LUT-NNs and achieve optimal inference serving performance, we first introduce an enhanced LUT-NN (eLUT-NN) algorithm for model calibration, then we propose an Auto-Tuner capable of optimizing the mapping parameters on diverse DRAM-PIM platforms. We evaluate PIM-DL on off-the-shelf UPMEM PIM-DIMM products and simulated HBM-PIM/AiM platforms across multiple contemporary DNN workloads. Compared with GEMM-based inference on DRAM-PIMs, PIM-DL achieves 22.6\u00d7~37.1\u00d7 speedup. Compared with CPU/GPU-based inference, PIM-DL achieves up to 3.54\u00d7/1.20\u00d7 speedup.",
    "authors": [
        "Cong Li",
        "Zhe Zhou",
        "Yang Wang",
        "Fan Yang",
        "Ting Cao",
        "Mao Yang",
        "Yun Liang",
        "Guangyu Sun"
    ],
    "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel PIM-DL framework to replace the compute-heavy GEMM operations in linear layers with Lookup-Tables (LUTs), which substantially reduce multiplications in DNN inference, rendering them suitable for efficient execution on DRAM-PIMs."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}