{
    "acronym": "99afcf5b6db3535f7ab05a379a200a706aab2e73",
    "title": "Towards Variable and Coordinated Holistic Co-Speech Motion Generation",
    "seed_ids": [
        "transformer",
        "3985b81334ab90fbcbb30bbb3cc2840d00d509ba",
        "dd23991294bf53c6301ab79fa49752e6458d2eab"
    ],
    "s2id": "99afcf5b6db3535f7ab05a379a200a706aab2e73",
    "abstract": "This paper addresses the problem of generating lifelike holistic co-speech motions for 3D avatars, focusing on two key aspects: variability and coordination. Variability allows the avatar to exhibit a wide range of motions even with similar speech content, while coordination ensures a harmonious alignment among facial expressions, hand gestures, and body poses. We aim to achieve both with ProbTalk, a unified probabilistic framework designed to jointly model facial, hand, and body movements in speech. ProbTalk builds on the variational autoencoder (VAE) architecture and incorporates three core designs. First, we introduce product quantization (PQ) to the VAE, which enriches the representation of complex holistic motion. Second, we devise a novel non-autoregressive model that embeds 2D positional encoding into the product-quantized representation, thereby preserving essential structure information of the PQ codes. Last, we employ a secondary stage to refine the preliminary prediction, further sharpening the high-frequency details. Coupling these three designs enables ProbTalk to generate natural and diverse holistic co-speech motions, outperforming several state-of-the-art methods in qualitative and quantitative evaluations, particularly in terms of realism. Our code and model will be released for research purposes at https://feifeifeiliu.github.io/probtalk/.",
    "authors": [
        "Yifei Liu",
        "Qiong Cao",
        "Yandong Wen",
        "Huaiguang Jiang",
        "Changxing Ding"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper addresses the problem of generating lifelike holistic co-speech motions for 3D avatars, focusing on two key aspects: variability and coordination, with ProbTalk, a unified probabilistic framework designed to jointly model facial, hand, and body movements in speech."
    },
    "citationCount": 1,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}