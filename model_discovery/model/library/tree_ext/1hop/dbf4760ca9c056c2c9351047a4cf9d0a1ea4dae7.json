{
    "acronym": "dbf4760ca9c056c2c9351047a4cf9d0a1ea4dae7",
    "title": "When Few-Shot Learning Meets Large-Scale Knowledge-Enhanced Pre-training: Alibaba at FewCLUE",
    "seed_ids": [
        "transformerxl",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "d56c1fc337fb07ec004dc846f80582c327af717c",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "b47381e04739ea3f392ba6c8faaf64105493c196"
    ],
    "s2id": "dbf4760ca9c056c2c9351047a4cf9d0a1ea4dae7",
    "abstract": null,
    "authors": [
        "Ziyun Xu",
        "Chengyu Wang",
        "Peng Li",
        "Yang Li",
        "Ming Wang",
        "Boyu Hou",
        "Minghui Qiu",
        "Chengguang Tang",
        "Jun Huang"
    ],
    "venue": "Natural Language Processing and Chinese Computing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents the solution to FewCLUE tasks by means of large-scale knowledge-enhanced pre-training over massive texts and knowledge triples, together with a new few-shot learning algorithm for downstream tasks."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}