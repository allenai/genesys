{
    "acronym": "9f1d4054d5a5daaca68ce7b2b7b398085bc2ff49",
    "title": "Extending Input Contexts of Language Models through Training on Segmented Sequences",
    "seed_ids": [
        "gpt2",
        "alibi",
        "c9603ec967879c24973b5bd48861df2e5555932e",
        "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b",
        "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "af385c0fdd0eda2bbf429bea6fedffc327c8a180",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "2e6fa3095df1d1ed041dfb4f5a18e31d4b7bd7bb",
        "f8d44802ac8190864c61c9aaf4a8b450261873ab",
        "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
        "a2fc77f075f666b462d9350e7576f0ba9845c61b",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "7509c66a666e2e3f14bc8676b969b945ee6e136f",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "dc48bc1a4d81e0f37603013fd2a95644dc233bd0",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "9f1d4054d5a5daaca68ce7b2b7b398085bc2ff49",
    "abstract": "Effectively training language models on long inputs poses many technical challenges. As a cost consideration, languages models are pretrained on a fixed sequence length before being adapted to longer sequences. We explore various methods for adapting models to longer inputs by training on segmented sequences and an interpolation-based method for extending absolute positional embeddings. We develop a training procedure to extend the input context size of pretrained models with no architectural changes and no additional memory costs than training on the original input lengths. By sub-sampling segments from long inputs while maintaining their original position the model is able to learn new positional interactions. Our method benefits both models trained with absolute positional embeddings, by extending their input contexts, as well as popular relative positional embedding methods showing a reduced perplexity on sequences longer than they were trained on. We demonstrate our method can extend input contexts by a factor of 4x while improving perplexity.",
    "authors": [
        "Petros Karypis",
        "Julian J. McAuley",
        "G. Karypis"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops a training procedure to extend the input context size of pretrained models with no architectural changes and no additional memory costs than training on the original input lengths by sub-sampling segments from long inputs while maintaining their original position."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}