{
    "acronym": "b72a8884a7a2f93d60d44930dd77d0af85dd32b8",
    "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models",
    "seed_ids": [
        "transformer",
        "gpt2",
        "nystromformer",
        "performer",
        "lineartransformer",
        "flash",
        "transnormer",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "5ce76c60e0bd53d4a6aac8b3f4672188611a7149",
        "92a95c5d3ea87e08ac527d8ce25383ff8c1015be",
        "977351c92f156db27592e88b14dee2c22d4b312a",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "200ef1cde362aafbf598a2b5a1c5f35504ca2289",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "f51497f463566581874c941353dd9d80069c5b77",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "88695b5bb6462872ce1dd946cff00dd6ebabf2d9",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b72a8884a7a2f93d60d44930dd77d0af85dd32b8",
    "abstract": "Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.",
    "authors": [
        "Haoran You",
        "Yichao Fu",
        "Zheng Wang",
        "Amir Yazdanbakhsh",
        "Y. Lin"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}