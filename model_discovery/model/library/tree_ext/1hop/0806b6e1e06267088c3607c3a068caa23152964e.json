{
    "acronym": "0806b6e1e06267088c3607c3a068caa23152964e",
    "title": "Intriguing Properties of Positional Encoding in Time Series Forecasting",
    "seed_ids": [
        "transformer",
        "bert",
        "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
        "31957789eb16c42542b50a53e6135fa8322ced16",
        "30f75d952a5d1e4d4a245963da55f0b4606c2182",
        "624b2604cd644f895454b794e0689578dd72f410",
        "4a6efc258088dd2c2245592e5366bf475f92e674",
        "6787343183353807a2f896d8c29d082d611dba51",
        "863171ed35ca0035074f73bb202b153cc346f2f3",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676"
    ],
    "s2id": "0806b6e1e06267088c3607c3a068caa23152964e",
    "abstract": "Transformer-based methods have made significant progress in time series forecasting (TSF). They primarily handle two types of tokens, i.e., temporal tokens that contain all variables of the same timestamp, and variable tokens that contain all input time points for a specific variable. Transformer-based methods rely on positional encoding (PE) to mark tokens' positions, facilitating the model to perceive the correlation between tokens. However, in TSF, research on PE remains insufficient. To address this gap, we conduct experiments and uncover intriguing properties of existing PEs in TSF: (i) The positional information injected by PEs diminishes as the network depth increases; (ii) Enhancing positional information in deep networks is advantageous for improving the model's performance; (iii) PE based on the similarity between tokens can improve the model's performance. Motivated by these findings, we introduce two new PEs: Temporal Position Encoding (T-PE) for temporal tokens and Variable Positional Encoding (V-PE) for variable tokens. Both T-PE and V-PE incorporate geometric PE based on tokens' positions and semantic PE based on the similarity between tokens but using different calculations. To leverage both the PEs, we design a Transformer-based dual-branch framework named T2B-PE. It first calculates temporal tokens' correlation and variable tokens' correlation respectively and then fuses the dual-branch features through the gated unit. Extensive experiments demonstrate the superior robustness and effectiveness of T2B-PE. The code is available at: \\href{https://github.com/jlu-phyComputer/T2B-PE}{https://github.com/jlu-phyComputer/T2B-PE}.",
    "authors": [
        "Jianqi Zhang",
        "Jingyao Wang",
        "Wenwen Qiang",
        "Fanjiang Xu",
        "Changwen Zheng",
        "Fuchun Sun",
        "Hui Xiong"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Transformer-based dual-branch framework named T2B-PE, which first calculates temporal tokens' correlation and variable tokens' correlation respectively and then fuses the dual-branch features through the gated unit and demonstrates the superior robustness and effectiveness of T2B-PE."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}