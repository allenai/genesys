{
    "acronym": "992ec0118708752892dadd07eb37c41446b122cf",
    "title": "Quantized Distributed Training of Large Models with Convergence Guarantees",
    "seed_ids": [
        "gpt",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "992ec0118708752892dadd07eb37c41446b122cf",
    "abstract": "Communication-reduction techniques are a popular way to improve scalability in data-parallel training of deep neural networks (DNNs). The recent emergence of large language models such as GPT has created the need for new approaches to exploit data-parallelism. Among these, fully-sharded data parallel (FSDP) training is highly popular, yet it still encounters scalability bottlenecks. One reason is that applying compression techniques to FSDP is challenging: as the vast majority of the communication involves the model's weights, direct compression alters convergence and leads to accuracy loss. We present QSDP, a variant of FSDP which supports both gradient and weight quantization with theoretical guarantees, is simple to implement and has essentially no overheads. To derive QSDP we prove that a natural modification of SGD achieves convergence even when we only maintain quantized weights, and thus the domain over which we train consists of quantized points and is, therefore, highly non-convex. We validate this approach by training GPT-family models with up to 1.3 billion parameters on a multi-node cluster. Experiments show that QSDP preserves model accuracy, while completely removing the communication bottlenecks of FSDP, providing end-to-end speedups of up to 2.2x.",
    "authors": [
        "I. Markov",
        "Adrian Vladu",
        "Qi Guo",
        "Dan Alistarh"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "To derive QSDP, a variant of FSDP which supports both gradient and weight quantization with theoretical guarantees, is simple to implement and has essentially no overheads, it is proved that a natural modification of SGD achieves convergence even when it only maintains quantized weights."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}