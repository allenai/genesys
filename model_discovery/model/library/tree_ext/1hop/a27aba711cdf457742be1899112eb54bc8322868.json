{
    "acronym": "a27aba711cdf457742be1899112eb54bc8322868",
    "title": "Reformed QANet - Optimizing the Spatial Complexity of QANet",
    "seed_ids": [
        "reformer"
    ],
    "s2id": "a27aba711cdf457742be1899112eb54bc8322868",
    "abstract": "The feed-forward QANet architecture replaced the bidirectional LSTMs of traditional Q&A models\u2019 encoder components with convolution + self-attention to increase the speed of the model without sacrificing accuracy [1]. We achieved scores of 64.5 EM/67.9 F1 on the dev set and 61.64 EM/65.30 F1 on the test set. While the parallel nature of QANet\u2019s CNN architecture allows for a significant speed boost, it entails GPU memory requirements to reap those benefits. We preform an ablation study to measure changes to spatial complexity, speed, and performance on the QANet architecture, replacing the self attention and feed-forward layer with LSH attention, reversible residual networks, and an entire reformer. We found that implementing LSH attention successfully decreased memory usage while maintaining reasonable performance. While the other modifications did not quite maintain the original QANet model\u2019s EM and FI scores, they significantly improved memory complexity.",
    "authors": [
        "Alex Fuster",
        "Andrew Hojel",
        "Dina Voevodsky"
    ],
    "venue": "",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that implementing LSH attention successfully decreased memory usage while maintaining reasonable performance, and while the other modifications did not quite maintain the original QANet model\u2019s EM and FI scores, they significantly improved memory complexity."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}