{
    "acronym": "e65122fb3176efc60e00d4f1c44800af8b9f4271",
    "title": "SciDeBERTa: Learning DeBERTa for Science and Technology Documents and Fine-tuning Information Extraction Tasks",
    "seed_ids": [
        "gpt",
        "319b84be7a843250bc81d7086f79a4126d550277",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "d56c1fc337fb07ec004dc846f80582c327af717c",
        "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e65122fb3176efc60e00d4f1c44800af8b9f4271",
    "abstract": "Deep learning-based language models (LMs) have transcended the gold standard (human baseline) of SQuAD 1.1 and GLUE benchmarks in April and July 2019, respectively. As of 2022, the top five LMs on the SuperGLUE benchmark leaderboard have exceeded the gold standard. Even people with good general knowledge will struggle to solve problems in specialized fields such as medicine and artificial intelligence. Just as humans learn specialized knowledge through bachelor\u2019s, master\u2019s, and doctoral courses, LMs also require a process to develop the ability to understand domain-specific knowledge. Thus, this study proposes SciDeBERTa and SciDeBERTa (CS) as pretrained LMs (PLMs) specialized in the science and technology domain. We further pretrained DeBERTa, which was trained with a general corpus, with the science and technology domain corpus. Experiments verified that SciDeBERTa (CS) continually pretrained in the computer science domain achieved 3.53% and 2.17% higher accuracies than SciBERT and S2ORC-SciBERT, respectively, which are science and technology domain specialized PLMs, in the task of recognizing entity names in the SciERC dataset. In the JRE task of the SciERC dataset, SciDeBERTa (CS) achieved a 6.7% higher performance than the baseline SCIIE. In the GENIA dataset, SciDeBERTa achieved the best performance compared to S2ORC-SciBERT, SciBERT, BERT, DeBERTa and SciDeBERTa (CS). Furthermore, re-initialization technology and optimizers after Adam were explored during fine-tuning to verify the language understanding of PLMs.",
    "authors": [
        "Yuna Jeong",
        "Eunhui Kim"
    ],
    "venue": "IEEE Access",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SciDeberTa and SciDeBERTa (CS) are proposed as pretrained LMs (PLMs) specialized in the science and technology domain, and re-initialization technology and optimizers after Adam were explored during fine-tuning to verify the language understanding of PLMs."
    },
    "citationCount": 4,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}