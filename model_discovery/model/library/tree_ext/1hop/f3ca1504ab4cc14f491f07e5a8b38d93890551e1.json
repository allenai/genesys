{
    "acronym": "f3ca1504ab4cc14f491f07e5a8b38d93890551e1",
    "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering",
    "seed_ids": [
        "longformer",
        "longt5",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "79cb9bca730a4c5bbe73d97c1f40da1d0debe568",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "274f903041b1a830b37f57929d837c1706e94ec7",
        "5397ef2da78aac248826b66156bed824d8aa03fb",
        "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
        "161321ef451d658d66b762cba5c202b12260220e",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7cc730da554003dda77796d2cb4f06da5dfd5592"
    ],
    "s2id": "f3ca1504ab4cc14f491f07e5a8b38d93890551e1",
    "abstract": "The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective.To that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and challenge the model, during pre-training, to answer these questions while \u201cpeeking\u201d into other topically-related documents.In a similar manner, the model is also challenged to recover the sentence from which the question was generated, again while leveraging cross-document information.This novel multi-document QA formulation directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre-training data. Further, unlike prior multi-document models that focus on either classification or summarization tasks, our pre-training objective formulation enables the model to perform tasks that involve both short text generation (e.g., QA) and long text generation (e.g., summarization).Following this scheme, we pre-train our model - termed QAmden - and evaluate its performance across several multi-document tasks, including multi-document QA, summarization, and query-focused summarization, yielding improvements of up to 7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.",
    "authors": [
        "Avi Caciularu",
        "Matthew E. Peters",
        "J. Goldberger",
        "Ido Dagan",
        "Arman Cohan"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes pre- training a generic multi-document model from a novel cross-document question answering pre-training objective, and develops a novel multi- document QA formulation that directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre- Training data."
    },
    "citationCount": 17,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}