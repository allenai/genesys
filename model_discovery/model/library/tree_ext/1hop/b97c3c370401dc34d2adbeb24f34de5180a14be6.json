{
    "acronym": "b97c3c370401dc34d2adbeb24f34de5180a14be6",
    "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture",
    "seed_ids": [
        "bigbird",
        "longformer",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "f6390beca54411b06f3bde424fb983a451789733",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b97c3c370401dc34d2adbeb24f34de5180a14be6",
    "abstract": "In recent years, attention-based models have achieved impressive performance in natural language processing and computer vision applications by effectively capturing contextual knowledge from the entire sequence. However, the attention mechanism inherently contains a large number of redundant connections, imposing a heavy computational burden on model deployment. To this end, sparse attention has emerged as an attractive approach to reduce the computation and memory footprint, which involves the sampled dense-dense matrix multiplication (SDDMM) and sparse-dense matrix multiplication (SpMM) at the same time, thus requiring the hardware to eliminate zero-valued operations effectively. Existing techniques based on irregular sparse patterns or regular but coarse-grained patterns lead to low hardware efficiency or less computation saving. This paper proposes Sanger, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design. The software part prunes the attention matrix into a dynamic structured pattern, and the hardware part features a reconfigurable architecture that exploits such patterns. Specifically, we dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix. Then, the sparse mask is re-arranged into structured blocks that are more amenable to hardware implementation. The hardware design of Sanger features a score-stationary dataflow that keeps sparse scores stationary in the PE to avoid decoding overhead. Using this dataflow and a reconfigurable systolic array design, we can unify the computation of SDDMM and SpMM operations. Typically, the PEs can be configured during runtime to support different data access and partial sum accumulation schemes. Experiments on BERT show that Sanger can prune the model to 0.08 - 0.27 sparsity without accuracy loss, achieving 4.64X, 22.7X, 2.39X, and 1.47X speedup compared to V100 GPU, AMD Ryzen Threadripper 3970X CPU, as well as the state-of-the-art attention accelerators A3 and SpAtten.",
    "authors": [
        "Liqiang Lu",
        "Yicheng Jin",
        "Hangrui Bi",
        "Zizhang Luo",
        "Peng Li",
        "Tao Wang",
        "Yun Liang"
    ],
    "venue": "Micro",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Sanger is proposed, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design that dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix."
    },
    "citationCount": 79,
    "influentialCitationCount": 13,
    "code": null,
    "description": null,
    "url": null
}