{
    "acronym": "2d968cbfbf8d5bd40041dc50b9d490642ce47e98",
    "title": "On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation",
    "seed_ids": [
        "bert",
        "44279244407a64431810f982be6d0c7da4429dd7",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "4e3c7891b643acb0849983eb506ae70bf126bc00",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1"
    ],
    "s2id": "2d968cbfbf8d5bd40041dc50b9d490642ce47e98",
    "abstract": "This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although keyphrase extraction with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify a favorable parameter allocation towards model depth rather than width when employing encoder-decoder architectures initialized with encoder-only PLMs. The study sheds light on the potential of utilizing encoder-only PLMs for advancing KPG systems and provides a groundwork for future KPG methods. Our code and pre-trained checkpoints are released at https://github.com/uclanlp/DeepKPG.",
    "authors": [
        "Di Wu",
        "Wasi Uddin Ahmad",
        "Kai-Wei Chang"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}