{
    "acronym": "4ea5ca620122e6a9a2b000444d36491cebf49c7c",
    "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
    "seed_ids": [
        "gpt2",
        "pi",
        "yarn",
        "alibi",
        "flashattn",
        "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b",
        "a54761081c2b001c057fb6e1ea9a48058d5aa5e0",
        "539fadfb615ef84c240f4741061c44eeda540091",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
        "b6346f9fa093b8e85df712485a2b851b9f680dac",
        "73290ecbec2f38d1d647ddef1ada69cee41725b3",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "0b0debb710366cdff461938c80763eace1651af6",
        "2dfb9171e180dcb0af23d305e024d43d311708ab",
        "b0db25e317cf856f1ec1ca3df0e573d850ed4696",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "80980cd10d19f021c14a6b7eee871b6a5d328024",
        "d203c764fb5dec2b053be667c8b06e516ea6ef10",
        "af385c0fdd0eda2bbf429bea6fedffc327c8a180",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "d9964ab436eefd21f923a4bc833c6b66692c7f00",
        "dbc368bc8b49347dd27679894524fa62f88492c9",
        "594d8e1696619f3cebb7c6bffdad8e0a5592f006",
        "68adb03744692247fb834406798894db9fe77010",
        "5735e49e501c8e51e9be4079592e46e047747b03",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "661e8d555c4424b5953f17434f2ba910bfcf3afe",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "732e3faec4e5be4d144256f2c379b9dc49f0b227",
        "f8d44802ac8190864c61c9aaf4a8b450261873ab",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "92173d081b15824d22a9ef070e118744ceee8052",
        "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "274f903041b1a830b37f57929d837c1706e94ec7",
        "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
        "5d032bd2632b6f5847767f39ce247098c6bbc563",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "16e623059ffccab60f4c35be028a2d4f10933515",
        "64a29bee2e1ad29547d590a3cc26274f4c537145",
        "6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5",
        "f4566761fe39c4b5273d696d9bc3f4195c9325bb",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe",
        "320efa53dea3e8f836790682fbd4196132c49749",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "67ee20536c30a225b86902af2f091e28e5e19b40",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "01b15017ac59b8d6f2ce3598c4a7d6358c211426",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
        "0b5d7a79205b44952e24025ce5d46e9f3aa401a1",
        "2a31319e73d4486716168b65cdf7559baeda18ce",
        "e3aa232577bb427b1f3a34acbdef84bd85734042",
        "955f90930d48750e7239478b4eed440eb84131cd",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4ea5ca620122e6a9a2b000444d36491cebf49c7c",
    "abstract": "Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.",
    "authors": [
        "Yunpeng Huang",
        "Jingwei Xu",
        "Zixu Jiang",
        "Junyu Lai",
        "Zenan Li",
        "Yuan Yao",
        "Taolue Chen",
        "Lijuan Yang",
        "Zhou Xin",
        "Xiaoxing Ma"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference is offered."
    },
    "citationCount": 23,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}