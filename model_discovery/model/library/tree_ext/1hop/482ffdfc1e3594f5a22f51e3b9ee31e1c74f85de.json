{
    "acronym": "482ffdfc1e3594f5a22f51e3b9ee31e1c74f85de",
    "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling",
    "seed_ids": [
        "transformerxl",
        "unlimiformer",
        "d9964ab436eefd21f923a4bc833c6b66692c7f00",
        "dbc368bc8b49347dd27679894524fa62f88492c9",
        "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0",
        "e0cbbca02b332f398c6639b3bea0613f79166220",
        "64a29bee2e1ad29547d590a3cc26274f4c537145",
        "c0f709acf38eb27702b0fbce1215db0ebaa2de2b",
        "29168348f4729d418df5acc8a5fce4f1c428a7e3",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f4238bd2385a52413ccbacfd9e409a650235bd13"
    ],
    "s2id": "482ffdfc1e3594f5a22f51e3b9ee31e1c74f85de",
    "abstract": "The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.",
    "authors": [
        "Haofei Yu",
        "Cunxiang Wang",
        "Yue Zhang",
        "Wei Bi"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric, and the results indicate an improvement without having additional training or adding additional parameters."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}