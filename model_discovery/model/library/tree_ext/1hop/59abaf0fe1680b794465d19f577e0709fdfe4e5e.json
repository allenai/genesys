{
    "acronym": "59abaf0fe1680b794465d19f577e0709fdfe4e5e",
    "title": "An Autoformer-CSA Approach for Long-Term Spectrum Prediction",
    "seed_ids": [
        "reformer",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676"
    ],
    "s2id": "59abaf0fe1680b794465d19f577e0709fdfe4e5e",
    "abstract": "In this letter, we develop an Autoformer with a series channel-spatial attention module (CSAM) (Autoformer-CSA) for long-term spectrum prediction. More specifically, the CSAM ingeniously replaces 2-dimensional (2D) convolution in image attention (including channel attention and spatial attention) with 1-dimensional (1D) convolution. The CSAM replaces Autoformer\u2019s feed-forward network. It is used to assign different concentrations to features mapped to the high-dimensional space, improving the learning ability of the Autoformer. We follow the series decomposition block and auto-correlation mechanism of the Autoformer. Experiments on a real-world dataset show that the Autoformer-CSA is superior to the state-of-the-art benchmarks.",
    "authors": [
        "Guangliang Pan",
        "Qihui Wu",
        "Guoru Ding",
        "Wei Wang",
        "Jie Li",
        "Bo Zhou"
    ],
    "venue": "IEEE Wireless Communications Letters",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An Autoformer with a series channel-spatial attention module (CSAM) (Autoformer-CSA) for long-term spectrum prediction, which ingeniously replaces 2-dimensional convolution in image attention (including channel attention and spatial attention) with 1-dimensional (1D) convolution."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}