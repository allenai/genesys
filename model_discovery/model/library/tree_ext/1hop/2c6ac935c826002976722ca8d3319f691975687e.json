{
    "acronym": "2c6ac935c826002976722ca8d3319f691975687e",
    "title": "Self-conditioned Embedding Diffusion for Text Generation",
    "seed_ids": [
        "diffusionlm",
        "analogbits",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "2c6ac935c826002976722ca8d3319f691975687e",
    "abstract": "Can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion, a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models - while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion.",
    "authors": [
        "Robin Strudel",
        "Corentin Tallec",
        "Florent Altch'e",
        "Yilun Du",
        "Yaroslav Ganin",
        "A. Mensch",
        "Will Grathwohl",
        "Nikolay Savinov",
        "S. Dieleman",
        "L. Sifre",
        "R\u00e9mi Leblond"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Self-conditioned Embedding Diffusion, a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation."
    },
    "citationCount": 62,
    "influentialCitationCount": 9,
    "code": null,
    "description": null,
    "url": null
}