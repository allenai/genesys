{
    "acronym": "44a517b5d9acdcc032c7001a034ba953d9aa787e",
    "title": "Emage: Non-Autoregressive Text-to-Image Generation",
    "seed_ids": [
        "bert",
        "f39f24210d413b6cc785d029851b59b54ce47d88",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "44a517b5d9acdcc032c7001a034ba953d9aa787e",
    "abstract": "Autoregressive and diffusion models drive the recent breakthroughs on text-to-image generation. Despite their huge success of generating high-realistic images, a common shortcoming of these models is their high inference latency - autoregressive models run more than a thousand times successively to produce image tokens and diffusion models convert Gaussian noise into images with many hundreds of denoising steps. In this work, we explore non-autoregressive text-to-image models that efficiently generate hundreds of image tokens in parallel. We develop many model variations with different learning and inference strategies, initialized text encoders, etc. Compared with autoregressive baselines that needs to run one thousand times, our model only runs 16 times to generate images of competitive quality with an order of magnitude lower inference latency. Our non-autoregressive model with 346M parameters generates an image of 256$\\times$256 with about one second on one V100 GPU.",
    "authors": [
        "Zhangyin Feng",
        "Runyi Hu",
        "Liangxin Liu",
        "Fan Zhang",
        "Duyu Tang",
        "Yong Dai",
        "Xiaocheng Feng",
        "Jiwei Li",
        "Bing Qin",
        "Shuming Shi"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work explores non-autoregressive text-to-image models that efficiently generate hundreds of image tokens in parallel and runs 16 times to generate images of competitive quality with an order of magnitude lower inference latency."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}