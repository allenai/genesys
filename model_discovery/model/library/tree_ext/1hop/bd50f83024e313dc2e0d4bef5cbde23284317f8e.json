{
    "acronym": "bd50f83024e313dc2e0d4bef5cbde23284317f8e",
    "title": "L AUGHING H YENA D ISTILLERY Extracting Compact Recurrences From Convolutions",
    "seed_ids": [
        "hyena",
        "resurrectrnn",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "a7d68b1702af08ce4dbbf2cd0b083e744ae5c6be",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "54155c2977a977bf129849455dcae3a2b79b3f41",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "bd50f83024e313dc2e0d4bef5cbde23284317f8e",
    "abstract": "Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads \u2013 naively requiring a full pass (or caching of activations) over the input sequence for each generated token \u2013 similarly to attention-based models. In this paper, we seek to enable O (1) compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena : by weight-tying the filters across channels into heads , we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10 \u00d7 higher throughput than Transformers and 1 . 5 \u00d7 higher than Hyena at 1 . 3 B parameters, without any loss in quality after distillation.",
    "authors": [
        "\u2217. StefanoMassaroli",
        "\u2217. MichaelPoli",
        "\u2217. DanielY.Fu",
        "Hermann Kumbong",
        "Rom N. Parnichkun",
        "Aman Timalsina",
        "David W. Romero",
        "Quinn McIntyre",
        "Beidi Chen",
        "Atri Rudra",
        "Ce Zhang",
        "Christopher R\u00e9",
        "Stefano Ermon",
        "Y. Bengio"
    ],
    "venue": "",
    "year": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper seeks to enable O (1) compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation to enable attention-free sequence models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}