{
    "acronym": "49158f9065e68d0f41bddf4fc2d489749555c997",
    "title": "EGA-Depth: Efficient Guided Attention for Self-Supervised Multi-Camera Depth Estimation",
    "seed_ids": [
        "linformer",
        "066c143b427571fb5568f2c581ea9066478d2e55",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "49158f9065e68d0f41bddf4fc2d489749555c997",
    "abstract": "The ubiquitous multi-camera setup on modern autonomous vehicles provides an opportunity to construct surround-view depth. Existing methods, however, either perform independent monocular depth estimations on each camera or rely on computationally heavy self attention mechanisms. In this paper, we propose a novel guided attention architecture, EGA-Depth, which can improve both the efficiency and accuracy of self-supervised multi-camera depth estimation. More specifically, for each camera, we use its perspective view as the query to cross-reference its neighboring views to derive informative features for this camera view. This allows the model to perform attention only across views with considerable overlaps and avoid the costly computations of standard self-attention. Given its efficiency, EGA-Depth enables us to exploit higher-resolution visual features, leading to improved accuracy. Furthermore, EGA-Depth can incorporate more frames from previous time steps as it scales linearly w.r.t. the number of views and frames. Extensive experiments on two challenging autonomous driving benchmarks nuScenes and DDAD demonstrate the efficacy of our proposed EGA-Depth and show that it achieves the new state-of-the-art in self-supervised multi-camera depth estimation.",
    "authors": [
        "Y. Shi",
        "H. Cai",
        "Amin Ansari",
        "F. Porikli"
    ],
    "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel guided attention architecture, EGA-Depth, which can improve both the efficiency and accuracy of self-supervised multi-camera depth estimation by using its perspective view as the query to cross-reference its neighboring views to derive informative features for this camera view."
    },
    "citationCount": 11,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}