{
    "acronym": "06e40b7a703079c280f8f0886ac2bd984cd318ce",
    "title": "Relating transformers to models and neural representations of the hippocampal formation",
    "seed_ids": [
        "hopfield"
    ],
    "s2id": "06e40b7a703079c280f8f0886ac2bd984cd318ce",
    "abstract": "Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.",
    "authors": [
        "James C. R. Whittington",
        "Joseph Warren",
        "T. Behrens"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work shows that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells."
    },
    "citationCount": 62,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}