{
    "acronym": "b8e59871669c0c4f77c414e8b413b1de10078072",
    "title": "Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation",
    "seed_ids": [
        "d3pms",
        "008bf3537ffac6fb160a0638c5e3d631a7501d7b",
        "8b20edda4a0013d628b9c4e9e4b42a5f1310b12f",
        "7e243fd3fc349ff9fc7c3011543823645ac25ed6",
        "3ea31f9b80bb69537f11f2c0e7d39c97d0742e3b",
        "599bc7cfe98c2b57ddbe111412203a636da57be0"
    ],
    "s2id": "b8e59871669c0c4f77c414e8b413b1de10078072",
    "abstract": "Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules. Pard is open-sourced at https://github.com/LingxiaoShawn/Pard.",
    "authors": [
        "Lingxiao Zhao",
        "Xueying Ding",
        "L. Akoglu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity, and achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}