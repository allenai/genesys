{
    "acronym": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
    "title": "Effective Long-Context Scaling of Foundation Models",
    "seed_ids": [
        "pi",
        "yarn",
        "0b0debb710366cdff461938c80763eace1651af6",
        "b0db25e317cf856f1ec1ca3df0e573d850ed4696",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
    "abstract": "We present an effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens. Our models are built through continual pretraining from Llama 2 checkpoints with longer text sequences and on a dataset where long texts are upsampled. We perform extensive evaluation using language modeling, synthetic context probing tasks, and a wide range of downstream benchmarks. Across all evaluations, our models achieve consistent improvements on most regular-context tasks and significant improvements on long-context tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure that is free of expensive annotation, the presented models can already surpass \\texttt{gpt-3.5-turbo-16k}\u2018s overall performance on long-context benchmarks. Alongside these results, we provide an in-depth analysis on each individual component of our method. We delve into Llama\u2019s position encodings and discuss its key limitation in modeling long data. We examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths \u2013 ablation results suggest that having abundant long texts in the pretrain dataset is \\textit{not} the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.",
    "authors": [
        "Wenhan Xiong",
        "Jingyu Liu",
        "Igor Molybog",
        "Hejia Zhang",
        "Prajjwal Bhargava",
        "Rui Hou",
        "Louis Martin",
        "Rashi Rungta",
        "Karthik Abinav Sankararaman",
        "Barlas O\u011fuz",
        "Madian Khabsa",
        "Han Fang",
        "Yashar Mehdad",
        "Sharan Narang",
        "Kshitiz Malik",
        "Angela Fan",
        "Shruti Bhosale",
        "Sergey Edunov",
        "Mike Lewis",
        "Sinong Wang",
        "Hao Ma"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens is presented and ablation results suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance."
    },
    "citationCount": 97,
    "influentialCitationCount": 10,
    "code": null,
    "description": null,
    "url": null
}