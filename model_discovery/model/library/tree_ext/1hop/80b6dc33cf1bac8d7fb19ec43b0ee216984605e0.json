{
    "acronym": "80b6dc33cf1bac8d7fb19ec43b0ee216984605e0",
    "title": "Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes",
    "seed_ids": [
        "gpt3",
        "a2fc77f075f666b462d9350e7576f0ba9845c61b",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "80b6dc33cf1bac8d7fb19ec43b0ee216984605e0",
    "abstract": "In-context learning (ICL) refers to the ability of a model to condition on a few in-context demonstrations (input-output examples of the underlying task) to generate the answer for a new query input, without updating parameters. Despite the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is sensitive to input demonstrations and limited to short context lengths. To understand the limitations and principles for successful ICL, we conduct an investigation with ICL linear regression of transformers. We characterize several Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL failures and compare transformers with DeepSet, a simple yet powerful architecture for ICL. Surprisingly, DeepSet outperforms transformers across a variety of distribution shifts, implying that preserving permutation invariance symmetry to input demonstrations is crucial for OOD ICL. The phenomenon specifies a fundamental requirement by ICL, which we termed as ICL invariance. Nevertheless, the positional encodings in LLMs will break ICL invariance. To this end, we further evaluate transformers with identical positional encodings and find preserving ICL invariance in transformers achieves state-of-the-art performance across various ICL distribution shifts",
    "authors": [
        "Yongqiang Chen",
        "Binghui Xie",
        "Kaiwen Zhou",
        "Bo Han",
        "Yatao Bian",
        "James Cheng"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "To understand the limitations and principles for successful ICL, an investigation with ICL linear regression of transformers is conducted and preserving ICL invariance in transformers achieves state-of-the-art performance across various ICL distribution shifts."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}