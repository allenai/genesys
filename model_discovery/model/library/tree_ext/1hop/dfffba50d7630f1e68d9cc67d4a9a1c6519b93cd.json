{
    "acronym": "dfffba50d7630f1e68d9cc67d4a9a1c6519b93cd",
    "title": "Text Style Transfer Evaluation Using Large Language Models",
    "seed_ids": [
        "gpt2",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "dfffba50d7630f1e68d9cc67d4a9a1c6519b93cd",
    "abstract": "Evaluating Text Style Transfer (TST) is a complex task due to its multi-faceted nature. The quality of the generated text is measured based on challenging factors, such as style transfer accuracy, content preservation, and overall fluency. While human evaluation is considered to be the gold standard in TST assessment, it is costly and often hard to reproduce. Therefore, automated metrics are prevalent in these domains. Nonetheless, it is uncertain whether and to what extent these automated metrics correlate with human evaluations. Recent strides in Large Language Models (LLMs) have showcased their capacity to match and even exceed average human performance across diverse, unseen tasks. This suggests that LLMs could be a viable alternative to human evaluation and other automated metrics in TST evaluation. We compare the results of different LLMs in TST evaluation using multiple input prompts. Our findings highlight a strong correlation between (even zero-shot) prompting and human evaluation, showing that LLMs often outperform traditional automated metrics. Furthermore, we introduce the concept of prompt ensembling, demonstrating its ability to enhance the robustness of TST evaluation. This research contributes to the ongoing efforts for more robust and diverse evaluation methods by standardizing and validating TST evaluation with LLMs.",
    "authors": [
        "Phil Ostheimer",
        "M. Nagda",
        "M. Kloft",
        "Sophie Fellenz"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A strong correlation between prompting and human evaluation is highlighted, showing that LLMs often outperform traditional automated metrics in TST evaluation, and the concept of prompt ensembling is introduced, demonstrating its ability to enhance the robustness of TST evaluation."
    },
    "citationCount": 6,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}