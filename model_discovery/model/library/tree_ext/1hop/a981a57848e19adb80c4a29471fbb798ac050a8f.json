{
    "acronym": "a981a57848e19adb80c4a29471fbb798ac050a8f",
    "title": "Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts",
    "seed_ids": [
        "gpt2",
        "8c62277dada489904a63de4dd87336c27c68fb5e",
        "a3184d40d390793232c99c89b57b8f65c16320b2",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a981a57848e19adb80c4a29471fbb798ac050a8f",
    "abstract": "Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing pre-trained models (PTMs) that simply prepends a soft prompt to the input and only optimizes the prompt to adapt PTMs to downstream tasks. Although it is parameter- and deployment-efficient, its performance still lags behind other state-of-the-art PETuning methods. Besides, the training cost of prompt tuning is not significantly reduced due to the back-propagation through the entire model. Through empirical analyses, we shed some light on the lagging performance of prompt tuning and recognize a trade-off between the propagation distance from label signals to the inserted prompt and the influence of the prompt on model outputs. Further, we present Late Prompt Tuning (LPT) that inserts a late prompt into an intermediate layer of the PTM instead of the input layer or all layers. The late prompt is obtained by a neural prompt generator conditioned on the hidden states before the prompt insertion layer and therefore is instance-dependent. Through extensive experimental results across various tasks and PTMs, we show that LPT can achieve competitive performance to full model tuning and other PETuning methods under both full-data and few-shot scenarios while possessing faster training speed and lower memory cost.",
    "authors": [
        "Xiangyang Liu",
        "Tianxiang Sun",
        "Xuanjing Huang",
        "Xipeng Qiu"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Late Prompt Tuning (LPT) is presented that can achieve competitive performance to full model tuning and other PETuning methods under both full-data and few-shot scenarios while possessing faster training speed and lower memory cost."
    },
    "citationCount": 18,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}