{
    "acronym": "c9706e8421d086348845b97f53c96f34254dfd79",
    "title": "Optimizing DNN Compilation for Distributed Training With Joint OP and Tensor Fusion",
    "seed_ids": [
        "reformer",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "c9706e8421d086348845b97f53c96f34254dfd79",
    "abstract": "This article proposes DisCo, an automatic deep learning compilation module for data-parallel distributed training. Unlike most deep learning compilers that focus on training or inference on a single device, DisCo optimizes a DNN model for distributed training over multiple GPU machines. Existing single-device compilation strategies do not work well in distributed training, due mainly to communication inefficiency that they incur. DisCo generates optimized, joint computation operator and communication tensor fusion strategies to enable highly efficient distributed training. A GNN-based simulator is built to effectively estimate per-iteration training time achieved by operator/tensor fusion candidates. A backtracking search algorithm is driven by the simulator, navigating efficiently in the large strategy space to identify good operator/tensor fusion strategies that minimize distributed training time. We compare DisCo with existing DL fusion schemes and show that it achieves good training speed-up close to the ideal, full computation-communication overlap case.",
    "authors": [
        "Xiaodong Yi",
        "Shiwei Zhang",
        "Lansong Diao",
        "Chuan Wu",
        "Zhen Zheng",
        "Shiqing Fan",
        "Siyu Wang",
        "Jun Yang",
        "W. Lin"
    ],
    "venue": "IEEE Transactions on Parallel and Distributed Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article proposes DisCo, an automatic deep learning compilation module for data-parallel distributed training that optimizes a DNN model for distributed training over multiple GPU machines and compares it with existing DL fusion schemes."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}