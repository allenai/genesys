{
    "acronym": "cef7c8825b59713324b62110a626c3b0589150a5",
    "title": "Embedded Translations for Low-resource Automated Glossing",
    "seed_ids": [
        "bert"
    ],
    "s2id": "cef7c8825b59713324b62110a626c3b0589150a5",
    "abstract": "We investigate automatic interlinear glossing in low-resource settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97\\%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\\%-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the documentation and preservation of languages, with our experiments on shared task datasets indicating significant advancements over the existing state of the art.",
    "authors": [
        "Changbing Yang",
        "Garrett Nicolai",
        "Miikka Silfverberg"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A hard-attentional neural model is augmented with embedded translation information extracted from interlinear glossed text, which highlights the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}