{
    "acronym": "91b07210ec07a229e5caf2d5f009a523b39e40ae",
    "title": "Momentum Decoding: Open-ended Text Generation As Graph Exploration",
    "seed_ids": [
        "gpt2",
        "5697a0ede5425954d48daa6e1893dc87bd7d8be7",
        "05bcf9999525656cfaa59bc71f8572d771ff3776",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "492a655a67e6ec7423a968cedb70eec0cdbc8e98",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "91b07210ec07a229e5caf2d5f009a523b39e40ae",
    "abstract": "Open-ended text generation with autoregressive language models (LMs) is one of the core tasks in natural language processing. However, maximization-based decoding methods (e.g., greedy/beam search) often lead to the degeneration problem, i.e., the generated text is unnatural and contains undesirable repetitions. Existing solutions to this problem either introduce randomness prone to incoherence or require a look-ahead mechanism that demands extra computational overhead. In this study, we formulate open-ended text generation from a new perspective, i.e., we view it as an exploration process within a directed graph. Thereby, we understand the phenomenon of degeneration as circular loops within the directed graph. Based on our formulation, we propose a novel decoding method -- \\textit{momentum decoding} -- which encourages the LM to \\textit{greedily} explore new nodes outside the current graph. Meanwhile, it also allows the LM to return to the existing nodes with a momentum downgraded by a pre-defined resistance function. We extensively test our approach on three benchmarks from different domains through automatic and human evaluations. The results show that momentum decoding performs comparably with the current state of the art while enjoying notably improved inference speed and computation FLOPs. Furthermore, we conduct a detailed analysis to reveal the merits and inner workings of our approach. Our codes and other related resources are publicly available at https://github.com/gmftbyGMFTBY/MomentumDecoding.",
    "authors": [
        "Tian Lan",
        "Yixuan Su",
        "Shuhang Liu",
        "Heyan Huang",
        "Xian-Ling Mao"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study formulate open-ended text generation from a new perspective, i.e., it is viewed as an exploration process within a directed graph, and proposes a novel decoding method -- \\textit{momentum decoding} -- which encourages the LM to explore new nodes outside the current graph."
    },
    "citationCount": 3,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}