{
    "acronym": "5c326260baa5e91a6a39c4d87ebe6770ade86816",
    "title": "Effective Theory of Transformers at Initialization",
    "seed_ids": [
        "gpt2",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "f8292d4ddf7a6dfe240eeaa9685f5d18eed9a3f6",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5c326260baa5e91a6a39c4d87ebe6770ade86816",
    "abstract": "We perform an effective-theory analysis of forward-backward signal propagation in wide and deep Transformers, i.e., residual neural networks with multi-head self-attention blocks and multilayer perceptron blocks. This analysis suggests particular width scalings of initialization and training hyperparameters for these models. We then take up such suggestions, training Vision and Language Transformers in practical setups.",
    "authors": [
        "Emily Dinan",
        "Sho Yaida",
        "Susan Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 12,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}