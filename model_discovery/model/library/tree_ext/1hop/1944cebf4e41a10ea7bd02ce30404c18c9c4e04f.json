{
    "acronym": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
    "title": "Linear Complexity Randomized Self-attention Mechanism",
    "seed_ids": [
        "nystromformer",
        "scatterbrain",
        "performer",
        "linformer",
        "reformer",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "12640af46eaf4c16125557b517a2d37fca70a82d",
        "e0cbbca02b332f398c6639b3bea0613f79166220",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "5d032bd2632b6f5847767f39ce247098c6bbc563",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "78a0fb70b79116eb8d42c5951ced4f9efba513f0",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "c0f709acf38eb27702b0fbce1215db0ebaa2de2b",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "29168348f4729d418df5acc8a5fce4f1c428a7e3",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "5a9bc55f6332e38f62eb509b684147a1d4f10fd9"
    ],
    "s2id": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
    "abstract": "Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.",
    "authors": [
        "Lin Zheng",
        "Chong Wang",
        "Lingpeng Kong"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers is proposed and sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA)."
    },
    "citationCount": 23,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}