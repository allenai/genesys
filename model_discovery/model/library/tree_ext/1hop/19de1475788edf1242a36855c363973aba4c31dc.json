{
    "acronym": "19de1475788edf1242a36855c363973aba4c31dc",
    "title": "Recommender Transformers with Behavior Pathways",
    "seed_ids": [
        "sparsetransformer",
        "beec9c8657d370efdb1b259cc27fcbf5697282c9",
        "d3f51870f4da5dd9c2a08a55cfa8a380b8d49208",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8"
    ],
    "s2id": "19de1475788edf1242a36855c363973aba4c31dc",
    "abstract": "Sequential recommendation requires the recommender to capture the evolving behavior characteristics from logged user behavior data for accurate recommendations. Nevertheless, user behavior sequences are viewed as a script with multiple ongoing threads intertwined. We find that only a small set of pivotal behaviors can be evolved into the user's future action. As a result, the future behavior of the user is hard to predict. We conclude this characteristic for sequential behaviors of each user as thebehavior pathway. Different users have their unique behavior pathways. Among existing sequential models, transformers have shown great capacity in capturing global-dependent characteristics. However, these models mainly provide a dense distribution over all previous behaviors using the self-attention mechanism, making the final predictions overwhelmed by the trivial behaviors not adjusted to each user. In this paper, we build the Recommender Transformer (RETR) with a novel Pathway Attention mechanism. RETR can dynamically plan the behavior pathway specified for each user, and sparingly activate the network through this behavior pathway to effectively capture evolving patterns useful for recommendation. The key design is a learned binary route to prevent the behavior pathway from being overwhelmed by trivial behaviors. Pathway attention is model-agnostic and can be applied to a series of transformer-based models for sequential recommendation. We empirically evaluate RETR on seven intra-domain benchmarks and RETR yields state-of-the-art performance. On another five cross-domain benchmarks, RETR can capture more domain-invariant representations for sequential recommendation.",
    "authors": [
        "Zhiyu Yao",
        "Xinyang Chen",
        "Sinan Wang",
        "Qinyan Dai",
        "Yumeng Li",
        "Tanchao Zhu",
        "Mingsheng Long"
    ],
    "venue": "The Web Conference",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Recommender Transformer (RETR) is built with a novel Pathway Attention mechanism that can dynamically plan the behavior pathway specified for each user, and sparingly activate the network through this behavior pathway to effectively capture evolving patterns useful for recommendation."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}