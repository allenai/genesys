{
    "acronym": "c600b697700c844cbc85009be70f1cdfeef3593e",
    "title": "Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents",
    "seed_ids": [
        "reformer",
        "0a41cb292242a82b2b09b3bf23b48349b981a640",
        "f8c6d9ed61fdf04cd390dce017a817152cf4d580",
        "ac95a18762133d4065ac8af518c33084d83c5582",
        "fa51076458b7bcf9a60f476d525755e47199a6d8",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "01b15017ac59b8d6f2ce3598c4a7d6358c211426",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "4d244972ed2e0286363bfb054cb269574d21a72c"
    ],
    "s2id": "c600b697700c844cbc85009be70f1cdfeef3593e",
    "abstract": "Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose Summ^N, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. Summ^N first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, Summ^N is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that Summ^N outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at https://github.com/psunlpgroup/Summ-N.",
    "authors": [
        "Yusen Zhang",
        "Ansong Ni",
        "Ziming Mao",
        "Chen Henry Wu",
        "Chenguang Zhu",
        "Budhaditya Deb",
        "A. Awadallah",
        "Dragomir R. Radev",
        "Rui Zhang"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Summ^N is the first multi-stage split-then-summarize framework for long input summarization and outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum."
    },
    "citationCount": 78,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}