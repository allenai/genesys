{
    "acronym": "7a09101ac03b74db501648597fa54e992a0fc84f",
    "title": "Towards Making the Most of BERT in Neural Machine Translation",
    "seed_ids": [
        "gpt",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "7a09101ac03b74db501648597fa54e992a0fc84f",
    "abstract": "GPT-2 and BERT demonstrate the effectiveness of using pre-trained language models (LMs) on various natural language processing tasks. However, LM fine-tuning often suffers from catastrophic forgetting when applied to resource-rich tasks. In this work, we introduce a concerted training framework (CTnmt) that is the key to integrate the pre-trained LMs to neural machine translation (NMT). Our proposed CTnmt} consists of three techniques: a) asymptotic distillation to ensure that the NMT model can retain the previous pre-trained knowledge; b) a dynamic switching gate to avoid catastrophic forgetting of pre-trained knowledge; and c) a strategy to adjust the learning paces according to a scheduled policy. Our experiments in machine translation show CTnmt gains of up to 3 BLEU score on the WMT14 English-German language pair which even surpasses the previous state-of-the-art pre-training aided NMT by 1.4 BLEU score. While for the large WMT14 English-French task with 40 millions of sentence-pairs, our base model still significantly improves upon the state-of-the-art Transformer big model by more than 1 BLEU score.",
    "authors": [
        "Jiacheng Yang",
        "Mingxuan Wang",
        "Hao Zhou",
        "Chengqi Zhao",
        "Yong Yu",
        "Weinan Zhang",
        "Lei Li"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A concerted training framework (CTnmt) that is the key to integrate the pre-trained LMs to neural machine translation (NMT) and consists of three techniques: asymptotic distillation to ensure that the NMT model can retain the previous pre- trained knowledge."
    },
    "citationCount": 141,
    "influentialCitationCount": 17,
    "code": null,
    "description": null,
    "url": null
}