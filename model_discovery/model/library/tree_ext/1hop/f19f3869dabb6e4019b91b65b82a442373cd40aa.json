{
    "acronym": "f19f3869dabb6e4019b91b65b82a442373cd40aa",
    "title": "Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention",
    "seed_ids": [
        "hgrn",
        "tnn",
        "lineartransformer",
        "flashattn",
        "flash",
        "transnormer",
        "434d751d355d7a7c20efa570e785c76286245e77",
        "2f0203386f3dcbffb47c9f7fe2d19d373d9dda2f",
        "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "54155c2977a977bf129849455dcae3a2b79b3f41",
        "ac608a4a6b19b3208e560eee5daadb3cc18638a2",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "86c8d930b492a4f9cadc6c60aecdaaded49acc86",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "ca444821352a4bd91884413d8070446e2960715a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28"
    ],
    "s2id": "f19f3869dabb6e4019b91b65b82a442373cd40aa",
    "abstract": "We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.",
    "authors": [
        "Zhen Qin",
        "Weigao Sun",
        "Dong Li",
        "Xuyang Shen",
        "Weixuan Sun",
        "Yiran Zhong"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}