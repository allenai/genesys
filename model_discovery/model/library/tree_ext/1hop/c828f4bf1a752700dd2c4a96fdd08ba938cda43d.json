{
    "acronym": "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
    "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering",
    "seed_ids": [
        "sparsetransformer",
        "reformer",
        "clusteredattn",
        "routingtransformer",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
    "abstract": "Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically with respect to the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. The proposed framework is pivoted on two unique types of Transformer layer: Sliding-Window Layer and Cluster-Former Layer, which encode local sequence information and global context jointly and iteratively. This new design allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.",
    "authors": [
        "Shuohang Wang",
        "Luowei Zhou",
        "Zhe Gan",
        "Yen-Chun Chen",
        "Yuwei Fang",
        "S. Sun",
        "Yu Cheng",
        "Jingjing Liu"
    ],
    "venue": "Findings",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Cluster-Former is proposed, a novel clustering-based sparse Transformer to perform attention across chunked sequences that allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies."
    },
    "citationCount": 25,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}