{
    "acronym": "83238f3402fc4a7c4efa48bae180d81cabed23ee",
    "title": "\\infty-former: Infinite Memory Transformer",
    "seed_ids": [
        "performer",
        "rfa",
        "bigbird",
        "longformer",
        "sparsetransformer",
        "transformerxl",
        "compressivetransformer",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "46c585ee9abf76779ea4b863d2da4358efd0d1d3",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "07a9f47885cae97efb7b4aa109392128532433da",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "84898960f68fa78296a102edc8ac81739f9a9408",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "83238f3402fc4a7c4efa48bae180d81cabed23ee",
    "abstract": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \\infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \\infty-former\u2019s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, \\infty-former maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \\infty-former\u2019s ability to retain information from long sequences.",
    "authors": [
        "Pedro Henrique Martins",
        "Zita Marinho",
        "Andr\u00e9 F. T. Martins"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The \\infty-former is proposed, which extends the vanilla transformer with an unbounded long-term memory, and maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed."
    },
    "citationCount": 11,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}