{
    "acronym": "9bfd79af22a25c57becad12964034f0dd5ef53e6",
    "title": "KDLGT: A Linear Graph Transformer Framework via Kernel Decomposition Approach",
    "seed_ids": [
        "performer",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "9058d322a09bfc0c93a070f87cac8fd840e63088",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28"
    ],
    "s2id": "9bfd79af22a25c57becad12964034f0dd5ef53e6",
    "abstract": "In recent years, graph Transformers (GTs) have been demonstrated as a robust architecture for a wide range of graph learning tasks. However, the quadratic complexity of GTs limits their scalability on large-scale data, in comparison to Graph Neural Networks (GNNs). In this work, we propose the Kernel Decomposition Linear Graph Transformer (KDLGT), an accelerating framework for building scalable and powerful GTs. KDLGT employs the kernel decomposition approach to rearrange the order of matrix multiplication, thereby reducing complexity to linear. Additionally, it categorizes GTs into three distinct types and provides tailored accelerating methods for each category to encompass all types of GTs. Furthermore, we provide a theoretical analysis of the performance gap between KDLGT and self-attention to ensure its effectiveness. Under this framework, we select two representative GTs to design our models. Experiments on both real-world and synthetic datasets indicate that KDLGT not only achieves state-of-the-art performance on various datasets but also reaches an acceleration ratio of approximately 10 on graphs of certain sizes.",
    "authors": [
        "Yi Wu",
        "Yanyang Xu",
        "Wenhao Zhu",
        "Guojie Song",
        "Zhouchen Lin",
        "Liangji Wang",
        "Shaoguo Liu"
    ],
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Kernel Decomposition Linear Graph Transformer (KDLGT) is proposed, an accelerating framework for building scalable and powerful GTs that employs the kernel decomposition approach to rearrange the order of matrix multiplication, thereby reducing complexity to linear."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}