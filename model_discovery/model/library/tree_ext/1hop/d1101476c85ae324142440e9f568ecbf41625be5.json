{
    "acronym": "d1101476c85ae324142440e9f568ecbf41625be5",
    "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
    "seed_ids": [
        "gpt2",
        "c21d3ac8a235e2ee5f783c4c8a146f6fd3ae12e5",
        "4b7215ebc0457a7e171bb3007c1e11623eef6615",
        "fef7e9eeed1ec5f0ba28ff88699f2965cf4d8c40",
        "a5ed9dfc0725bffb6428a2cc297a15265377906c",
        "cfa90e184cab9701a68e9b2fdd9222a1f508a354",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "d1101476c85ae324142440e9f568ecbf41625be5",
    "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence-or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10\u00d7 more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
    "authors": [
        "Nils Lukas",
        "A. Salem",
        "Robert Sim",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B'eguelin"
    ],
    "venue": "IEEE Symposium on Security and Privacy",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Novel attacks that can extract up to 10\u00d7 more PII sequences than existing attacks are introduced, showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of P II sequences, and a subtle connection between record-level membership inference and PII reconstruction are shown."
    },
    "citationCount": 110,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}