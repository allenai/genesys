{
    "acronym": "2ff74d426e712522030057624510c03713fa77ba",
    "title": "Linear Transformers Are Secretly Fast Weight Memory Systems",
    "seed_ids": [
        "performer",
        "lineartransformer",
        "cec7872b194aadf54140578b9be52939eb1112e9",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "f51497f463566581874c941353dd9d80069c5b77",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2ff74d426e712522030057624510c03713fa77ba",
    "abstract": "We show the formal equivalence of linearised self-attention mechanisms and fast weight memories from the early \u201990s. From this observation we infer a memory capacity limitation of recent lin-earised softmax attention variants. With \ufb01nite memory, a desirable behaviour of fast weight memory models is to manipulate the contents of memory and dynamically interact with it. Inspired by previous work on fast weights, we propose to replace the update rule with an alternative rule yielding such behaviour. We also propose a new kernel function to linearise attention, balancing simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the bene\ufb01ts of our methods.",
    "authors": [
        "Imanol Schlag",
        "Kazuki Irie",
        "J. Schmidhuber"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The formal equivalence of linearised self-attention mechanisms and fast weight memories from the early \u201990s is shown and a memory capacity limitation of recent lin-earised softmax attention variants is inferred."
    },
    "citationCount": 20,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}