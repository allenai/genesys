{
    "acronym": "23070cb7b4e282f784237c2e475b518238ff7e44",
    "title": "Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation",
    "seed_ids": [
        "d3pms",
        "f9fff0d57ea37ec2a3a47bdbb8ced605e43f6b87",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "479eaafcee15a44b632b3cab41491b517e1531fe",
        "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
        "4d2a05140dd9bafaf035a846e7bda05f956304d2"
    ],
    "s2id": "23070cb7b4e282f784237c2e475b518238ff7e44",
    "abstract": "Diffusion probabilistic models (DPMs) have become a popular approach to conditional generation, due to their promising results and support for cross-modal synthesis. A key desideratum in conditional synthesis is to achieve high correspondence between the conditioning input and generated output. Most existing methods learn such relationships implicitly, by incorporating the prior into the variational lower bound. In this work, we take a different route\u2014we enhance input-output connections by maximizing their mutual information using contrastive learning. To this end, we introduce a Conditional Discrete Contrastive Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to effectively incorporate it into the denoising process. We formulate CDCD by connecting it with the conventional variational objectives. We demonstrate the ef\ufb01cacy of our approach in evaluations with three diverse, multimodal conditional synthesis tasks: dance-to-music generation, text-to-image synthesis, and class-conditioned image synthesis. On each, we achieve state-of-the-art or higher synthesis quality and improve the input-output correspondence. Furthermore, the proposed approach improves the convergence of diffusion models, reducing the number of required diffusion steps by more than 35% on two benchmarks, signi\ufb01cantly increasing the inference speed. 1",
    "authors": [
        "Ye Zhu",
        "Yuehua Wu",
        "Kyle Olszewski",
        "Jian Ren",
        "S. Tulyakov",
        "Yan Yan"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a Conditional Discrete Contrastive Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to effectively incorporate it into the denoising process, and forms CDCD by connecting it with the conventional variational objectives."
    },
    "citationCount": 28,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}