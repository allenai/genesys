{
    "acronym": "26a6ddaed67780010fb1448d13d05ce07d3e6f48",
    "title": "Attention as Robust Representation for Time Series Forecasting",
    "seed_ids": [
        "lineartransformer",
        "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "26a6ddaed67780010fb1448d13d05ce07d3e6f48",
    "abstract": "Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core neural network architecture. It serves as a versatile component that can readily replace recent patching based embedding schemes in transformer-based models, boosting their performance.",
    "authors": [
        "Peisong Niu",
        "Tian Zhou",
        "Xue Wang",
        "Liang Sun",
        "Rong Jin"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution, and outperforms state-of-the-art models."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}