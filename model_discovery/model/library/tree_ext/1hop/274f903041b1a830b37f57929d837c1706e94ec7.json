{
    "acronym": "274f903041b1a830b37f57929d837c1706e94ec7",
    "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
    "seed_ids": [
        "bigbird",
        "longformer",
        "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
        "6e6a2fe517b33e1f29d761ae31fb37ddccb9a213",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7cc730da554003dda77796d2cb4f06da5dfd5592"
    ],
    "s2id": "274f903041b1a830b37f57929d837c1706e94ec7",
    "abstract": "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.",
    "authors": [
        "Wen Xiao",
        "Iz Beltagy",
        "G. Carenini",
        "Arman Cohan"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "PRIMERA is introduced, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data."
    },
    "citationCount": 95,
    "influentialCitationCount": 29,
    "code": null,
    "description": null,
    "url": null
}