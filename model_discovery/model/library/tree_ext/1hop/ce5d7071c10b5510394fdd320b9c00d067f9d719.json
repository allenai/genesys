{
    "acronym": "ce5d7071c10b5510394fdd320b9c00d067f9d719",
    "title": "SAS-BERT: BERT for Sales and Support Conversation Classification using a Novel Multi-Objective Pre-Training Framework",
    "seed_ids": [
        "bert",
        "2a218786f4615b82389f78472e7ff22e6ce57490",
        "d56c1fc337fb07ec004dc846f80582c327af717c",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "ce5d7071c10b5510394fdd320b9c00d067f9d719",
    "abstract": "Recent emergence of large language models (LLMs), particularly GPT variants has created a lot of buzz due to their state-of-the-art performance results. However, for highly domainspecific datasets such as sales and support conversations, most LLMs do not exhibit high performance out-of-the-box. Thus, fine-tuning is neededwhich many budget-constrained businesses cannot afford. Also, these models have very slow inference times making them unsuitable for many real-time applications. Lack of interpretability and access to probabilistic inferences is another problem. For such reasons, BERT-based models are preferred. In this paper, we present SAS-BERT, a BERT-based architecture for sales and support conversations. Through novel pre-training enhancements and GPT-3.5 led data augmentation, we demonstrate improvement in BERT performance for highly domainspecific datasets which iscomparable withfine-tuned LLMs. Our architecture has 98.5% fewer parameters compared to the largest LLM considered, trains under 72 hours, and can be hosted on a single large CPU for inference.",
    "authors": [
        "Aanchal Varma",
        "Chetan Bhat"
    ],
    "venue": "Artificial Intelligence and Big Data",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SAS-BERT is presented, a BERT-based architecture for sales and support conversations which has 98.5% fewer parameters compared to the largest LLM considered, trains under 72 hours, and can be hosted on a single large CPU for inference."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}