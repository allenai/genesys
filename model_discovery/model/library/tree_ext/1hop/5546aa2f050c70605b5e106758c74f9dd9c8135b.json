{
    "acronym": "5546aa2f050c70605b5e106758c74f9dd9c8135b",
    "title": "Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget",
    "seed_ids": [
        "bert",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e"
    ],
    "s2id": "5546aa2f050c70605b5e106758c74f9dd9c8135b",
    "abstract": "Compared to standard language model (LM) pretraining (i.e., from scratch), Knowledge Distillation (KD) entails an additional forward pass through a teacher model that is typically substantially larger than the target student model. As such, KD in LM pretraining materially slows down throughput of pretraining instances vis-a-vis pretraining from scratch. Scaling laws of LM pretraining suggest that smaller models can close the gap to larger counterparts if trained on more data (i.e., processing more tokens)\u2014and under a fixed computation budget, smaller models are able to process more data than larger models. We thus hypothesize that KD might, in fact, be suboptimal to pretraining from scratch for obtaining smaller LMs, when appropriately accounting for the compute budget. To test this, we compare pretraining from scratch against several KD strategies for masked language modeling (MLM) in a fair experimental setup, with respect to amount of computation as well as pretraining data. Downstream results on GLUE, however, do not confirm our hypothesis: while pretraining from scratch performs comparably to ordinary KD under a fixed computation budget, more sophisticated KD strategies, namely TinyBERT and MiniLM, outperform it by a notable margin. We further find that KD yields larger gains over pretraining from scratch when the data can be repeated under the fixed computation budget.",
    "authors": [
        "Minh Duc Bui",
        "Fabian David Schmidt",
        "Goran Glava\u0161",
        "K. Wense"
    ],
    "venue": "First Workshop on Insights from Negative Results in NLP",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is hypothesize that KD might, in fact, be suboptimal to pretraining from scratch for obtaining smaller LMs, when appropriately accounting for the compute budget, and finds that KD yields larger gains over pretraining from scratch when the data can be repeated under the fixed computation budget."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}