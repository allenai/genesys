{
    "acronym": "4e33dcb6f578a4429dd0e427a68aadd1a4fac140",
    "title": "MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition",
    "seed_ids": [
        "gpt3",
        "34103f1294844c447fe8872bf5c3ab1c7ce32103"
    ],
    "s2id": "4e33dcb6f578a4429dd0e427a68aadd1a4fac140",
    "abstract": "Large-scale pre-trained models have demonstrated impressive performance in vision and language tasks within open-world scenarios. Due to the lack of comparable pre-trained models for 3D shapes, recent methods utilize language-image pre-training to realize zero-shot 3D shape recognition. However, due to the modality gap, pretrained language-image models are not confident enough in the generalization to 3D shape recognition. Consequently, this paper aims to improve the confidence with view selection and hierarchical prompts. Leveraging the CLIP model as an example, we employ view selection on the vision side by identifying views with high prediction confidence from multiple rendered views of a 3D shape. On the textual side, the strategy of hierarchical prompts is proposed for the first time. The first layer prompts several classification candidates with traditional class-level descriptions, while the second layer refines the prediction based on function-level descriptions or further distinctions between the candidates. Remarkably, without the need for additional training, our proposed method achieves impressive zero-shot 3D classification accuracies of 84.44%, 91.51%, and 66.17% on ModelNet40, ModelNet10, and ShapeNet Core55, respectively. Furthermore, we will make the code publicly available to facilitate reproducibility and further research in this area.",
    "authors": [
        "Dan Song",
        "Xinwei Fu",
        "Weizhi Nie",
        "Wenhui Li",
        "Anan Liu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper aims to improve the confidence with view selection and hierarchical prompts with impressive zero-shot 3D classification accuracies on ModelNet40, ModelNet10, and ShapeNet Core55, respectively."
    },
    "citationCount": 2,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}