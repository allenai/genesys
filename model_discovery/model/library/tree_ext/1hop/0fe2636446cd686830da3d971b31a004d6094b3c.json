{
    "acronym": "0fe2636446cd686830da3d971b31a004d6094b3c",
    "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
    "seed_ids": [
        "gpt",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "0fe2636446cd686830da3d971b31a004d6094b3c",
    "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both \u201cbimodal\u201d data of NL-PL pairs and \u201cunimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
    "authors": [
        "Zhangyin Feng",
        "Daya Guo",
        "Duyu Tang",
        "Nan Duan",
        "Xiaocheng Feng",
        "Ming Gong",
        "Linjun Shou",
        "Bing Qin",
        "Ting Liu",
        "Daxin Jiang",
        "Ming Zhou"
    ],
    "venue": "Findings",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops CodeBERT with Transformer-based neural architecture, and trains it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators."
    },
    "citationCount": 1864,
    "influentialCitationCount": 445,
    "code": null,
    "description": null,
    "url": null
}