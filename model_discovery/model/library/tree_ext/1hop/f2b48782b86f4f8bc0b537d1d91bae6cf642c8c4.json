{
    "acronym": "f2b48782b86f4f8bc0b537d1d91bae6cf642c8c4",
    "title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation",
    "seed_ids": [
        "transformer",
        "classfreediffu",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "f2b48782b86f4f8bc0b537d1d91bae6cf642c8c4",
    "abstract": "Non-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data. Although NATs generate high-quality outputs and offer faster inference than autoregressive models, they tend to produce incoherent and repetitive results due to complex data distribution (e.g., acoustic and linguistic variations in speech). In this work, we introduce DiffNorm, a diffusion-based normalization strategy that simplifies data distributions for training NAT models. After training with a self-supervised noise estimation objective, DiffNorm constructs normalized target data by denoising synthetically corrupted speech features. Additionally, we propose to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training. Our strategies result in a notable improvement of about +7 ASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr) translations on the CVSS benchmark, while attaining over 14x speedup for En-Es and 5x speedup for En-Fr translations compared to autoregressive baselines.",
    "authors": [
        "Weiting Tan",
        "Jingyu Zhang",
        "Lingfeng Shen",
        "Daniel Khashabi",
        "Philipp Koehn"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "DiffNorm is introduced, a diffusion-based normalization strategy that simplifies data distributions for training NAT models, and is proposed to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}