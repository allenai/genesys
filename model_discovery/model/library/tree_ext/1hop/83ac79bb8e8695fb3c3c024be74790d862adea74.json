{
    "acronym": "83ac79bb8e8695fb3c3c024be74790d862adea74",
    "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting",
    "seed_ids": [
        "gpt2",
        "f45f85fa1beaa795c24c4ff86f1f2deece72252f",
        "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
        "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "d84cf745c534c010b8e55e5a4a04878906848dc3",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "863171ed35ca0035074f73bb202b153cc346f2f3",
        "8064d3873c646dc9ff949d72c54c634a906fc092",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "83ac79bb8e8695fb3c3c024be74790d862adea74",
    "abstract": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.",
    "authors": [
        "Defu Cao",
        "Furong Jia",
        "Sercan \u00d6. Arik",
        "Tomas Pfister",
        "Yixiang Zheng",
        "Wen Ye",
        "Yan Liu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series."
    },
    "citationCount": 34,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}