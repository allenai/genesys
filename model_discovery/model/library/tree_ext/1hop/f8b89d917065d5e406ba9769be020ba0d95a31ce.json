{
    "acronym": "f8b89d917065d5e406ba9769be020ba0d95a31ce",
    "title": "UER: An Open-Source Toolkit for Pre-training Models",
    "seed_ids": [
        "gpt",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "f8b89d917065d5e406ba9769be020ba0d95a31ce",
    "abstract": "Existing works, including ELMO and BERT, have revealed the importance of pre-training for NLP tasks. While there does not exist a single pre-training model that works best in all cases, it is of necessity to develop a framework that is able to deploy various pre-training models efficiently. For this purpose, we propose an assemble-on-demand pre-training toolkit, namely Universal Encoder Representations (UER). UER is loosely coupled, and encapsulated with rich modules. By assembling modules on demand, users can either reproduce a state-of-the-art pre-training model or develop a pre-training model that remains unexplored. With UER, we have built a model zoo, which contains pre-trained models based on different corpora, encoders, and targets (objectives). With proper pre-trained models, we could achieve new state-of-the-art results on a range of downstream datasets.",
    "authors": [
        "Zhe Zhao",
        "Hui Chen",
        "Jinbin Zhang",
        "Xin Zhao",
        "Tao Liu",
        "Wei Lu",
        "Xi Chen",
        "Haotang Deng",
        "Qi Ju",
        "Xiaoyong Du"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes an assemble-on-demand pre-training toolkit, namely Universal Encoder Representations (UER), which is loosely coupled, and encapsulated with rich modules, and built a model zoo, which contains pre-trained models based on different corpora, encoders, and targets."
    },
    "citationCount": 92,
    "influentialCitationCount": 17,
    "code": null,
    "description": null,
    "url": null
}