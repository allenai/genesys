{
    "acronym": "7571307f33a29f3b2431ec7e4a7a78ec1ce11dc1",
    "title": "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions",
    "seed_ids": [
        "gpt3",
        "de27e0add04612f85b96180dc6fac9c713397d9f",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "d0086b86103a620a86bc918746df0aa642e2a8a3"
    ],
    "s2id": "7571307f33a29f3b2431ec7e4a7a78ec1ce11dc1",
    "abstract": "Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.",
    "authors": [
        "Reza Esfandiarpoor",
        "Cristina Menghini",
        "Stephen H. Bach"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations and that different VLMs prioritize different attributes in their representations."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}