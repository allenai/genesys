{
    "acronym": "eefa2776da90490b266ea017b62edf5541906996",
    "title": "On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers",
    "seed_ids": [
        "lineartransformer",
        "f2f68ed280d27bd25d61782224f8a465db8f43bd",
        "59b7448f816908cfb49a2ab5e63b2fa5786387f7",
        "3ea31f9b80bb69537f11f2c0e7d39c97d0742e3b",
        "5eda60d4940d4185df45c5703e103458171d465d",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "a585828fab3ec46ad27e14adfd299953df107a47",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f6390beca54411b06f3bde424fb983a451789733"
    ],
    "s2id": "eefa2776da90490b266ea017b62edf5541906996",
    "abstract": "Graph transformers have recently received significant attention in graph learning, partly due to their ability to capture more global interaction via self-attention. Nevertheless, while higher-order graph neural networks have been reasonably well studied, the exploration of extending graph transformers to higher-order variants is just starting. Both theoretical understanding and empirical results are limited. In this paper, we provide a systematic study of the theoretical expressive power of order-$k$ graph transformers and sparse variants. We first show that, an order-$k$ graph transformer without additional structural information is less expressive than the $k$-Weisfeiler Lehman ($k$-WL) test despite its high computational cost. We then explore strategies to both sparsify and enhance the higher-order graph transformers, aiming to improve both their efficiency and expressiveness. Indeed, sparsification based on neighborhood information can enhance the expressive power, as it provides additional information about input graph structures. In particular, we show that a natural neighborhood-based sparse order-$k$ transformer model is not only computationally efficient, but also expressive -- as expressive as $k$-WL test. We further study several other sparse graph attention models that are computationally efficient and provide their expressiveness analysis. Finally, we provide experimental results to show the effectiveness of the different sparsification strategies.",
    "authors": [
        "Cai Zhou",
        "Rose Yu",
        "Yusu Wang"
    ],
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that a natural neighborhood-based sparse order-$k$ transformer model is not only computationally efficient, but also expressive -- as expressive as the Weisfeiler Lehman test despite its high computational cost."
    },
    "citationCount": 2,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}