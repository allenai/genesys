{
    "acronym": "fbfe920579cc1c13358521d403cfce31f2afbead",
    "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches",
    "seed_ids": [
        "mamba",
        "streamingllm",
        "landmarkattn",
        "fe33be95849c556ce0bdffaa1d2c7db9bb2e2c61",
        "46732358e98ce6be0c564ae11f71d556a64b4c35",
        "d8b51d518f2dd62943762ceaa8961d3b1bfbcc1a",
        "cbaf689fd9ea9bc939510019d90535d6249b3367",
        "d53fe76bd2795a19ddf52d012917782f6f6f2c1e",
        "b085968c4362fb286ad6c5ef71a5db9630da0498",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "0b0debb710366cdff461938c80763eace1651af6",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4"
    ],
    "s2id": "fbfe920579cc1c13358521d403cfce31f2afbead",
    "abstract": "Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive. However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches -- such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures -- have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights -- as well as a friendly workbench -- for the future development of long context-capable LLMs. The source code will be available at https://github.com/henryzhongsc/longctx_bench",
    "authors": [
        "Jiayi Yuan",
        "Hongyi Liu",
        "Shaochen Zhong",
        "Yu-Neng Chuang",
        "Songchen Li",
        "Guanchu Wang",
        "Duy Le",
        "Hongye Jin",
        "V. Chaudhary",
        "Zhaozhuo Xu",
        "Zirui Liu",
        "Xia Hu"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks is provided, which reveals numerous previously unknown phenomena and offers insights for the future development of long context-capable LLMs."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}