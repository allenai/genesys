{
    "acronym": "a1850badd796ade462819600920c39f3edc909d5",
    "title": "Cached Transformers: Improving Transformers with Differentiable Memory Cache",
    "seed_ids": [
        "linformer",
        "bigbird",
        "reformer",
        "transformerxl",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "64a29bee2e1ad29547d590a3cc26274f4c537145",
        "8514e90a83b2609d188c07be1cde90307b2c6afe",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "168fc3525f7b97695a97b04e257ee9bd1e832acb",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a1850badd796ade462819600920c39f3edc909d5",
    "abstract": "This work introduces a new Transformer model called Cached Transformer, which uses Gated Recurrent Cached (GRC) attention to extend the self-attention mechanism with a differentiable memory cache of tokens. GRC attention enables attending to both past and current tokens, increasing the receptive field of attention and allowing for exploring long-range dependencies. By utilizing a recurrent gating unit to continuously update the cache, our model achieves significant advancements in \\textbf{six} language and vision tasks, including language modeling, machine translation, ListOPs, image classification, object detection, and instance segmentation. Furthermore, our approach surpasses previous memory-based techniques in tasks such as language modeling and displays the ability to be applied to a broader range of situations.",
    "authors": [
        "Zhaoyang Zhang",
        "Wenqi Shao",
        "Yixiao Ge",
        "Xiaogang Wang",
        "Jinwei Gu",
        "Ping Luo"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a new Transformer model that uses Gated Recurrent Cached (GRC) attention to extend the self-attention mechanism with a differentiable memory cache of tokens, and surpasses previous memory-based techniques in tasks such as language modeling and displays the ability to be applied to a broader range of situations."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}