{
    "acronym": "572c7592f64b27dc5ac639504444d1da34ddffa7",
    "title": "A Review of the Challenges with Massive Web-mined Corpora Used in Large Language Models Pre-Training",
    "seed_ids": [
        "gpt3",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "572c7592f64b27dc5ac639504444d1da34ddffa7",
    "abstract": "This article presents a comprehensive review of the challenges associated with using massive web-mined corpora for the pre-training of large language models (LLMs). This review identifies key challenges in this domain, including challenges such as noise (irrelevant or misleading information), duplication of content, the presence of low-quality or incorrect information, biases, and the inclusion of sensitive or personal information in web-mined corpora. Addressing these issues is crucial for the development of accurate, reliable, and ethically responsible language models. Through an examination of current methodologies for data cleaning, pre-processing, bias detection and mitigation, we highlight the gaps in existing approaches and suggest directions for future research. Our discussion aims to catalyze advancements in developing more sophisticated and ethically responsible LLMs.",
    "authors": [
        "Michal Perelkiewicz",
        "Rafal Po'swiata"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article presents a comprehensive review of the challenges associated with using massive web-mined corpora for the pre-training of large language models (LLMs), including challenges such as noise, duplication of content, the presence of low-quality information, biases, and the inclusion of sensitive or personal information in web-mined corpora."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}