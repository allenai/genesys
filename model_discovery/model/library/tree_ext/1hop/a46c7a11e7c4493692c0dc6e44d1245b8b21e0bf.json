{
    "acronym": "a46c7a11e7c4493692c0dc6e44d1245b8b21e0bf",
    "title": "Comprehensive Survey of Model Compression and Speed up for Vision Transformers",
    "seed_ids": [
        "nystromformer",
        "scatterbrain",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87"
    ],
    "s2id": "a46c7a11e7c4493692c0dc6e44d1245b8b21e0bf",
    "abstract": "Vision Transformers (ViT) have marked a paradigm shift in computer vision, outperforming state-of-the-art models across diverse tasks. However, their practical deployment is hampered by high computational and memory demands. This study addresses the challenge by evaluating four primary model compression techniques: quantization, low-rank approximation, knowledge distillation, and pruning. We methodically analyze and compare the efficacy of these techniques and their combinations in optimizing ViTs for resource-constrained environments. Our comprehensive experimental evaluation demonstrates that these methods facilitate a balanced compromise between model accuracy and computational efficiency, paving the way for wider application in edge computing devices.",
    "authors": [
        "Feiyang Chen",
        "Ziqian Luo",
        "Lisang Zhou",
        "Xueting Pan",
        "Ying Jiang"
    ],
    "venue": "Journal of Information, Technology and Policy",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study evaluating four primary model compression techniques: quantization, low-rank approximation, knowledge distillation, and pruning demonstrates that these methods facilitate a balanced compromise between model accuracy and computational efficiency, paving the way for wider application in edge computing devices."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}