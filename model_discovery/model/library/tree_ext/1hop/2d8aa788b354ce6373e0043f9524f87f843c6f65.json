{
    "acronym": "2d8aa788b354ce6373e0043f9524f87f843c6f65",
    "title": "Neural Netspeak \u2013 Exploring the Performance of Transformer Models as Idiomatic Writing Assistants Bachelor\u2019s Thesis",
    "seed_ids": [
        "gpt",
        "98332ae5475bfa286d9ab74e20d86c72ceb3cde6"
    ],
    "s2id": "2d8aa788b354ce6373e0043f9524f87f843c6f65",
    "abstract": "Since writing is a difficult task, authors often resort to sophisticated tools writing assistants to help them with spelling, grammar, style checks, or word choice. Query-based writing assistants offer the experienced writer an easy way to work with complex resources, like dictionaries, phrase-books, and idioms from a corpus, by providing a query language for context-sensitive search and a transparent ranking. Netspeak uses an index over the Google N-gram dataset to retrieve matching phrases ordered by frequency of occurrence in the dataset. However, the use of n-grams imposes a limit on Netspeak, requiring the queries not to exceed five words, limiting the context that can be captured in a single query. Also, the longer the queries are, the lower is the probability of finding matching n-grams. Here we show that using the Transformer-based BERT language model, trained on masked word prediction, we can circumvent these two limitations and, depending on the type of query, increase the number of answered queries dramatically. On 140,000 queries generated from four corpora of different domains, our language-model based result retrieveal strategy increases the number of answered queries by up to 81 % compared to Netspeak on the same queries. Given additional context through longer queries, we are able to answer up to 100 % queries more than Netspeak. However, we are not able to rank the expected result as high as Netspeak most cases, even with additional context. Our results demonstrate how certain capabilities of language models can independently be used even without further fine-tuning to improve certain aspects of a query answer retrieval process.",
    "authors": [
        "F. Thies"
    ],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Transformer-based BERT language model, trained on masked word prediction, is used to circumvent two limitations on Netspeak and, depending on the type of query, increase the number of answered queries dramatically."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}