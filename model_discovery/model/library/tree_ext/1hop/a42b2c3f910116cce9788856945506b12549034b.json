{
    "acronym": "a42b2c3f910116cce9788856945506b12549034b",
    "title": "ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training",
    "seed_ids": [
        "bert",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "5e00596fa946670d894b1bdaeff5a98e3867ef13"
    ],
    "s2id": "a42b2c3f910116cce9788856945506b12549034b",
    "abstract": "Despite significant advancements in medical vision-language pre-training, existing methods have largely overlooked the inherent entity-specific context within radiology reports and the complex cross-modality contextual relationships between text and images. To close this gap, we propose a novel Entity-centered Context-aware Medical Vision-language Pre-training (ECAMP) framework, which is designed to enable a more entity-centered and context-sensitive interpretation of medical data. Utilizing the recent powerful large language model, we distill entity-centered context from medical reports, which enables ECAMP to gain more effective supervision from the text modality. By further pre-training our model with carefully designed entity-aware, context-enhanced masked language modeling and context-guided super-resolution tasks, ECAMP significantly refines the interplay between text and image modalities, leading to an enhanced ability to extract entity-centered contextual features. Besides, our proposed multi-scale context fusion design also improves the semantic integration of both coarse and fine-level image representations, prompting better performance for multi-scale downstream applications. Combining these components leads to significant performance leaps over current state-of-the-art methods and establishes a new standard for cross-modality learning in medical imaging, whose effectiveness is demonstrated by our extensive experiments on various tasks including classification, segmentation, and detection across several public datasets. Code and models are available at https://github.com/ToniChopp/ECAMP.",
    "authors": [
        "Rongsheng Wang",
        "Qingsong Yao",
        "Haoran Lai",
        "Zhiyang He",
        "Xiaodong Tao",
        "Zihang Jiang",
        "S.Kevin Zhou"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel Entity-centered Context-aware Medical Vision-language Pre-training (ECAMP) framework, designed to enable a more entity-centered and context-sensitive interpretation of medical data, and establishes a new standard for cross-modality learning in medical imaging."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}