{
    "acronym": "e6b87ce03c7291ba4df7ce942626502607d0d743",
    "title": "ERNIE-DOC: The Retrospective Long-Document Modeling Transformer",
    "seed_ids": [
        "bigbird",
        "longformer",
        "sparsetransformer",
        "transformerxl",
        "2c953a3c378b40dadf2e3fb486713c8608b8e282",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e6b87ce03c7291ba4df7ce942626502607d0d743",
    "abstract": "Transformers are not suited for processing long document input due to its quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or inferior modeling capability with comparable model size. In this paper, we propose ERNIE-D OC , a document-level language pretraining model based on Recurrence Transform-ers (Dai et al., 2019). Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism enable ERNIE-D OC with much longer effective context length to capture the contextual information of a whole document. We pretrain ERNIE-D OC to explicitly learn the relationship among segments with an additional document-aware segment reordering ob-jective. Various experiments on both English and Chinese document-level tasks are conducted. ERNIE-D OC achieves SOTA language modeling result of 16.8 ppl on WikiText-103 and outperforms competitive pretraining models on most language understanding tasks such as text classi\ufb01cation, question answering by a large margin.",
    "authors": [
        "Siyu Ding",
        "Junyuan Shang",
        "Shuohuan Wang",
        "Yu Sun",
        "Hao Tian",
        "Hua Wu",
        "Haifeng Wang"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism enable ERNIE-D OC with much longer effective context length to capture the contextual information of a whole document."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}