{
    "acronym": "71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f",
    "title": "A Survey on Generative Diffusion Model",
    "seed_ids": [
        "d3pms",
        "c5349e515efe3513682ba62da122be5756650af1",
        "a979742220a88b1d32e1fbe72c41e8ba3007053c",
        "81621c53c6aa421deb81bb1359138ded0fb4e258",
        "c69361b20ad3d88ad4c5e9e1a3a66d0932f2bc43",
        "15736f7c205d961c00378a938daffaacb5a0718d",
        "ef7993ab30d0a8afabb4ebab080e471c0d5c743c",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "37232ccce1cfafbe9b9918557f0b6cdf80e5b83a",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "e9b9a47cd81c66603c827f0f2bc4fba0d9ae77c4",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "82482585e94192b4e9913727e461f89cd08e9725",
        "599bc7cfe98c2b57ddbe111412203a636da57be0",
        "6c761cfdb031701072582e434d8f64d436255da6",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "bc519f58ae61afbf6318d6e4239d2d565c7ba467",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "4d2a05140dd9bafaf035a846e7bda05f956304d2"
    ],
    "s2id": "71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f",
    "abstract": "\u2014Deep learning shows excellent potential in generation tasks thanks to deep latent representation. Generative models are classes of models that can generate observations randomly with respect to certain implied parameters. Recently, the diffusion Model has become a raising class of generative models by virtue of its power-generating ability. Nowadays, great achievements have been reached. More applications except for computer vision, speech generation, bioinformatics, and natural language processing are to be explored in this \ufb01eld. However, the diffusion model has its genuine drawback of a slow generation process, leading to many enhanced works. This survey makes a summary of the \ufb01eld of the diffusion model. We \ufb01rst state the main problem with two landmark works \u2013 DDPM and DSM. Then, we present a diverse range of advanced techniques to speed up the diffusion models \u2013 training schedule, training-free sampling, mixed-modeling, and score & diffusion uni\ufb01cation. Regarding existing models, we also provide a benchmark of FID score, IS, and NLL according to speci\ufb01c NFE. Moreover, applications with diffusion models are introduced including computer vision, sequence modeling, audio, and AI for science. Finally, there is a summarization of this \ufb01eld together with limitations & further directions.",
    "authors": [
        "Hanqun Cao",
        "Cheng Tan",
        "Zhangyang Gao",
        "Yilun Xu",
        "Guangyong Chen",
        "P. Heng",
        "Stan Z. Li"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A diverse range of advanced techniques to speed up the diffusion models \u2013 training schedule, training-free sampling, mixed-modeling, and score & diffusion uni\ufb01cation are presented."
    },
    "citationCount": 132,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}