{
    "acronym": "dc0005e3fb9ad04e43dc60b12df8e27f29dd04c3",
    "title": "Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective",
    "seed_ids": [
        "hgrn",
        "tnn",
        "lineartransformer",
        "0664d52b1040e048fff7e7d1d13a310964207768",
        "5c104f905fcacf390270f619f232a2ba4eb873f2",
        "434d751d355d7a7c20efa570e785c76286245e77",
        "d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797",
        "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "54155c2977a977bf129849455dcae3a2b79b3f41",
        "f6d8beb02771791d628f7e0773d8906261ce707c",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "88695b5bb6462872ce1dd946cff00dd6ebabf2d9"
    ],
    "s2id": "dc0005e3fb9ad04e43dc60b12df8e27f29dd04c3",
    "abstract": "We present the Linear Complexity Sequence Model (LCSM), a comprehensive solution that unites various sequence modeling techniques with linear complexity, including linear attention, state space model, long convolution, and linear RNN, within a single framework. The goal is to enhance comprehension of these models by analyzing the impact of each component from a cohesive and streamlined viewpoint. Specifically, we segment the modeling processes of these models into three distinct stages: Expand, Oscillation, and Shrink (EOS), with each model having its own specific settings. The Expand stage involves projecting the input signal onto a high-dimensional memory state. This is followed by recursive operations performed on the memory state in the Oscillation stage. Finally, the memory state is projected back to a low-dimensional space in the Shrink stage. We perform comprehensive experiments to analyze the impact of different stage settings on language modeling and retrieval tasks. Our results show that data-driven methods are crucial for the effectiveness of the three stages in language modeling, whereas hand-crafted methods yield better performance in retrieval tasks.",
    "authors": [
        "Zhen Qin",
        "Xuyang Shen",
        "Weigao Sun",
        "Dong Li",
        "Stan Birchfield",
        "Richard Hartley",
        "Yiran Zhong"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}