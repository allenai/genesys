{
    "acronym": "ce9c3a335e26f45de633fc43164d18946427038a",
    "title": "PHAED: A Speaker-Aware Parallel Hierarchical Attentive Encoder-Decoder Model for Multi-Turn Dialogue Generation",
    "seed_ids": [
        "transformerxl",
        "200050c1f51e2c930e62b078c6ce20f2a6675468",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ce9c3a335e26f45de633fc43164d18946427038a",
    "abstract": "This article presents a novel open-domain dialogue generation model emphasizing the differentiation of speakers in multi-turn conversations. Differing from prior work that treats the conversation history as a long text, we argue that capturing relative social relations among utterances (i.e., generated by either the same speaker or different persons) benefits the machine capturing fine-grained context information from a conversation history to improve context coherence in the generated response. Given that, we propose a Parallel Hierarchical Attentive Encoder-Decoder (PHAED) model that can effectively leverage conversation history by modeling each utterance with the awareness of its speaker and contextual associations with the same speaker's previous messages. Specifically, to distinguish the speaker roles over a multi-turn conversation (involving two speakers), we regard the utterances from one speaker as responses and those from the other as queries. After understanding queries via hierarchical encoder with inner-query and inter-query encodings, transformer-xl style decoder reuses the hidden states of previously generated responses to generate a new response. Our empirical results with three large-scale benchmarks show that PHAED significantly outperforms baseline models on both automatic and human evaluations. Furthermore, our ablation study shows that dialogue models with speaker tokens can generally decrease the possibility of generating non-coherent responses.",
    "authors": [
        "Zihao Wang",
        "Ming Jiang",
        "Junli Wang"
    ],
    "venue": "IEEE Transactions on Big Data",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Parallel Hierarchical Attentive Encoder-Decoder (PHAED) model is proposed that can effectively leverage conversation history by modeling each utterance with the awareness of its speaker and contextual associations with the same speaker's previous messages."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}