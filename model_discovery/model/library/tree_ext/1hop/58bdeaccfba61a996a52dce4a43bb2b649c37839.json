{
    "acronym": "58bdeaccfba61a996a52dce4a43bb2b649c37839",
    "title": "A Novel Method for Reducing Overhead of Training Sentiment Analysis Network",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "58bdeaccfba61a996a52dce4a43bb2b649c37839",
    "abstract": "\n Sentiment analysis based on statistics has rapidly developed in deep-learning. Bilateral attention neural network (BANN), especially Bidirectional Encoder Representations from Transformers (BERT), has reached high accuracy. However, with the increase of network depth and large-scale corpus, the computational overhead of BANN increases geometrically. How to reduce training corpus scale has correspondingly become an important research focus. This paper proposes a reduced corpus scale method called Concept-BERT, which consists of the following steps: firstly, using Formal Concept Analysis (FCA), Concept-BERT mines the association rules among corpus and reduces corpus attributes, and hence reducing corpus scale; secondly, reduced-corpus is inputed to BERT and the result is obtained; finally, the attention of Concept-BERT is analyzed. Concept-BERT is experimented for sentiment analysis on CoLA, SST-2, Dianping and Blogsenti, and its accuracy reaches 81.1, 92.9, 77.9 and 86.7 respectively. Our experimental results show that the proposed method has the same accuracy as BERT, using low-scale corpus and low overhead, and low-scale corpus doesn't affect model attention.",
    "authors": [
        "Yuxia Lei",
        "Linkun Zhang",
        "Zhengyan Wang",
        "Zhiqiang Kong",
        "Haiyan Ma",
        "X. Liu"
    ],
    "venue": "",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a reduced corpus scale method called Concept-BERT, which has the same accuracy as BERT, using low-scale corpus and low overhead, and low- Scale corpus doesn't affect model attention."
    },
    "citationCount": 1,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}