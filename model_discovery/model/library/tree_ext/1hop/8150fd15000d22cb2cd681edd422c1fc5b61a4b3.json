{
    "acronym": "8150fd15000d22cb2cd681edd422c1fc5b61a4b3",
    "title": "Transformers are Expressive, But Are They Expressive Enough for Regression?",
    "seed_ids": [
        "bigbird",
        "746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8150fd15000d22cb2cd681edd422c1fc5b61a4b3",
    "abstract": "Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate smooth functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as:\"Are Transformers truly Universal Function Approximators?\"To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Theoretically, we prove that Transformer Encoders cannot approximate smooth functions. Experimentally, we complement our theory and show that the full Transformer architecture cannot approximate smooth functions. By shedding light on these challenges, we advocate a refined understanding of Transformers' capabilities.",
    "authors": [
        "Swaroop Nath",
        "H. Khadilkar",
        "Pushpak Bhattacharyya"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A thorough investigation of Transformers' expressivity, providing theoretical insights and supporting evidence through experiments, proves that Transformer Encoders cannot approximate smooth functions and advocates a refined understanding of Transformers' capabilities."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}