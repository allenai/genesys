{
    "acronym": "fb4ebe2e158296cef577a373f30117882e00d221",
    "title": "Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling",
    "seed_ids": [
        "d3pms",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "37232ccce1cfafbe9b9918557f0b6cdf80e5b83a",
        "e9b9a47cd81c66603c827f0f2bc4fba0d9ae77c4",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "fb4ebe2e158296cef577a373f30117882e00d221",
    "abstract": "Learning to denoise has emerged as a prominent paradigm to design state-of-the-art deep generative models for natural images. How to use it to model the distributions of both continuous real-valued data and categorical data has been well studied in recently proposed diffusion models. However, it is found in this paper to have limited ability in modeling some other types of data, such as count and non-negative continuous data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose learning to jump as a general recipe for generative modeling of various types of data. Using a forward count thinning process to construct learning objectives to train a deep neural network, it employs a reverse count thickening process to iteratively refine its generation through that network. We demonstrate when learning to jump is expected to perform comparably to learning to denoise, and when it is expected to perform better. For example, learning to jump is recommended when the training data is non-negative and exhibits strong sparsity, skewness, heavy-tailedness, and/or heterogeneity.",
    "authors": [
        "Tianqi Chen",
        "Mingyuan Zhou"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes learning to jump as a general recipe for generative modeling of various types of data, and demonstrates when learning toJump is expected to perform comparably to learning to denoise, and when it isexpected to perform better."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}