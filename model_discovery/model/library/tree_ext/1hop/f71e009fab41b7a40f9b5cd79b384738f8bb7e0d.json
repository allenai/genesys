{
    "acronym": "f71e009fab41b7a40f9b5cd79b384738f8bb7e0d",
    "title": "XNOR-FORMER: Learning Accurate Approximations in Long Speech Transformers",
    "seed_ids": [
        "cosformer",
        "lineartransformer",
        "transformerxl",
        "roformer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "f71e009fab41b7a40f9b5cd79b384738f8bb7e0d",
    "abstract": "Transformers are among the state of the art for many tasks in speech, vision, and natural language processing, among others. Self-attentions, which are crucial contributors to this performance have quadratic computational complexity, which makes training on longer input sequences challenging. Prior work has produced state-of-the-art transformer variants with linear attention, however, current models sacrifice performance to achieve efficient implementations. In this work, we develop a novel linear transformer by examining the properties of the key-query product within self-attentions. Our model outperforms state of the art approaches on speech recognition and speech summarization, resulting in 1 % absolute WER improvement on the Librispeech-100 speech recognition benchmark and a new INTERVIEW speech recognition benchmark, and 5 points on ROUGE for summarization with How2.",
    "authors": [
        "Roshan Sharma",
        "B. Raj"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops a novel linear transformer by examining the properties of the key-query product within self-attentions within speech recognition and speech summarization, resulting in 1 % absolute WER improvement on the Librispeech-100 speech recognition benchmark and a new INTERVIEW speech recognition benchmarks."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}