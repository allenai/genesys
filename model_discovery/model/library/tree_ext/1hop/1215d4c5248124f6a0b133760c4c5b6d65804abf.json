{
    "acronym": "1215d4c5248124f6a0b133760c4c5b6d65804abf",
    "title": "When large language models meet evolutionary algorithms",
    "seed_ids": [
        "roformer",
        "1c259361caa85c2d95a7d04e5e42fa98693da85b",
        "7a29fb7a37869126840ed71ac7671db2e985f443",
        "822f41fdb57c57db614a27936474644daf12b715",
        "bbe197158adb4b6e85a6eeab4619ea0fc6857941",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "d6b414487787d0b6efd735a3236a690ad13aae70"
    ],
    "s2id": "1215d4c5248124f6a0b133760c4c5b6d65804abf",
    "abstract": "Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text generation and evolution, this paper illustrates the parallels between LLMs and EAs, which includes multiple one-to-one key characteristics: token representation and individual representation, position encoding and fitness shaping, position embedding and selection, Transformers block and reproduction, and model training and parameter adaptation. By examining these parallels, we analyze existing interdisciplinary research, with a specific focus on evolutionary fine-tuning and LLM-enhanced EAs. Drawing from these insights, valuable future directions are presented for advancing the integration of LLMs and EAs, while highlighting key challenges along the way. These parallels not only reveal the evolution mechanism behind LLMs but also facilitate the development of evolved artificial agents that approach or surpass biological organisms.",
    "authors": [
        "Wang Chao",
        "Jiaxuan Zhao",
        "Licheng Jiao",
        "Lingling Li",
        "Fang Liu",
        "Shuyuan Yang"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper illustrates the parallels between LLMs and EAs, which includes multiple one-to-one key characteristics: token representation and individual representation, position encoding and fitness shaping, position embedding and selection, Transformers block and reproduction, and model training and parameter adaptation."
    },
    "citationCount": 6,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}