{
    "acronym": "d828ec273669d7c3769d1f66ec37edfbc5de15f3",
    "title": "T RANSFORMER -QL: A S TEP T OWARDS M AKING T RANSFORMER N ETWORK Q UADRATICALLY L ARGE",
    "seed_ids": [
        "lineartransformer",
        "transformerxl",
        "compressivetransformer",
        "0cd82dfae930ac4b57c0e959f744f2d10bf87649",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "168fc3525f7b97695a97b04e257ee9bd1e832acb",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "63857190aaf5aab1d94b54bb257b7b03b8cb5a50",
        "f2fc9ef411846dd577c26225ce93f50bb1fa760b",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0"
    ],
    "s2id": "d828ec273669d7c3769d1f66ec37edfbc5de15f3",
    "abstract": "Transformer networks have shown outstanding performance on many natural language processing tasks. However the context length (the number of previous tokens on which the output states depend) of a Transformer network grows at best linearly with the memory and computational power used. This limitation prevents a transformer network to have very long context in a resource limited application. In this work, we propose a class of transformer networks, namely Transformer-QL (Quadratically Large), in which, the context length can grow at best quadratically with the memory and computational power used. We have empirically evaluated a Transformer-QL model in three long range language modeling datasets. The results show that Transformer-QL can provide significant improvements over other state of the art networks.",
    "authors": [],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a class of transformer networks, namely Transformer-QL (Quadratically Large), in which, the context length can grow at best quadratically with the memory and computational power used."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}