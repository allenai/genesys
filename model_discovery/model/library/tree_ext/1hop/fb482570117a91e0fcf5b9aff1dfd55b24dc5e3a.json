{
    "acronym": "fb482570117a91e0fcf5b9aff1dfd55b24dc5e3a",
    "title": "Exploring the Performance and Efficiency of Transformer Models for NLP on Mobile Devices",
    "seed_ids": [
        "gpt2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fb482570117a91e0fcf5b9aff1dfd55b24dc5e3a",
    "abstract": "Deep learning (DL) is characterised by its dynamic nature, with new deep neural network (DNN) architectures and approaches emerging every few years, driving the field's advancement. At the same time, the ever-increasing use of mobile devices (MDs) has resulted in a surge of DNN-based mobile applications. Although traditional architectures, like CNNs and RNNs, have been successfully integrated into MDs, this is not the case for Transformers, a relatively new model family that has achieved new levels of accuracy across AI tasks, but poses significant computational challenges. In this work, we aim to make steps towards bridging this gap by examining the current state of Transformers' on-device execution. To this end, we construct a benchmark of representative models and thoroughly evaluate their performance across MDs with different computational capabilities. Our experimental results show that Transformers are not accelerator-friendly and indicate the need for software and hardware optimisations to achieve efficient deployment.",
    "authors": [
        "I. Panopoulos",
        "Sokratis Nikolaidis",
        "Stylianos I. Venieris",
        "I. Venieris"
    ],
    "venue": "International Symposium on Computers and Communications",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A benchmark of representative models is constructed and their performance across MDs with different computational capabilities is evaluated, showing that Transformers are not accelerator-friendly and indicate the need for software and hardware optimisations to achieve efficient deployment."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}