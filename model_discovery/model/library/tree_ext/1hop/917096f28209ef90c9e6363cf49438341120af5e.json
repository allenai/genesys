{
    "acronym": "917096f28209ef90c9e6363cf49438341120af5e",
    "title": "Theoretical Foundations of Deep Selective State-Space Models",
    "seed_ids": [
        "s4",
        "mamba",
        "gla",
        "d53fe76bd2795a19ddf52d012917782f6f6f2c1e",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797",
        "b3caabbae4b7c3b842086b21940ce9d5b25d476f",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "d98b5c1d0f9a4e39dc79ea7a3f74e54789df5e13",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "7e9ff94476f41041c75e253e84f487db00e9c861"
    ],
    "s2id": "917096f28209ef90c9e6363cf49438341120af5e",
    "abstract": "Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.",
    "authors": [
        "Nicola Muca Cirone",
        "Antonio Orvieto",
        "Benjamin Walker",
        "C. Salvi",
        "Terry Lyons"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Theoretical grounding is given to this recent finding that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales."
    },
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}