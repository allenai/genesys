{
    "acronym": "687f0af2b6b6eb9dde326d7c6759dedf4b9e917a",
    "title": "Efficient Multi-order Gated Aggregation Network",
    "seed_ids": [
        "metaformer",
        "flash",
        "d1869155960e4b1b882b39171dbecd25a7eda3cd",
        "dd1139cfc609c2f3263d02e97176d5275caebc0a",
        "bf6ce546c589fa8054b3972b266532664914bd21",
        "fa717a2e31f0cef4e26921f3b147a98644d2e64c",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "ba637c4f1a170f1e2dadeadb71a63cf2b9a46de2",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "b52844a746dafd8a5051cef49abbbda64a312605",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "70a10d95e968158c2a862af217186c74c44b5e25",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "a9c214e846188adb645021cd7b1964b8ea1fef6f",
        "6e8f35c6d54acb14109c9b792a62609eac8a7b5e",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "687f0af2b6b6eb9dde326d7c6759dedf4b9e917a",
    "abstract": "Since the recent success of Vision Transformers (ViTs), explorations toward transformer-style architectures have triggered the resurgence of modern ConvNets. In this work, we explore the representation ability of DNNs through the lens of interaction complexities. We empirically show that interaction complexity is an overlooked but essential indi-cator for visual recognition. Accordingly, a new family of ef\ufb01cient ConvNets, named MogaNet, is presented to pursue informative context mining in pure ConvNet-based models, with preferable complexity-performance trade-offs. In MogaNet, interactions across multiple complexities are facil-itated and contextualized by leveraging two specially designed aggregation blocks in both spatial and channel interaction spaces. Extensive studies are conducted on ImageNet classi\ufb01cation, COCO object detection, and ADE20K semantic segmentation tasks. The results demonstrate that our MogaNet establishes new state-of-the-art over other popular methods in mainstream scenarios and all model scales. Typically, the lightweight MogaNet-T achieves 80.0% top-1 accuracy with only 1.44G FLOPs using re\ufb01ned training setup on ImageNet-1K, surpassing ParC-Net-S by 1.4% accuracy but saving 59% (2.04G) FLOPs.",
    "authors": [
        "Siyuan Li",
        "Zedong Wang",
        "Zicheng Liu",
        "Cheng Tan",
        "Haitao Lin",
        "Di Wu",
        "Zhiyuan Chen",
        "Jiangbin Zheng",
        "Stan Z. Li"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work empirically show that interaction complexity is an overlooked but essential indi-cator for visual recognition, and presents a new family of pure ConvNet-based models, named MogaNet, to pursue informative context mining in pure ConvNet-based models, with preferable complexity-performance trade-offs."
    },
    "citationCount": 14,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}