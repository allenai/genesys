{
    "acronym": "1c3e36d450ac150b921c2860074b2b90298ef7c3",
    "title": "Generating Extended and Multilingual Summaries with Pre-trained Transformers",
    "seed_ids": [
        "bigbird",
        "memcompress",
        "24b951275a7a42ef36aca8352caaf6f4cd6238d2",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7cc730da554003dda77796d2cb4f06da5dfd5592"
    ],
    "s2id": "1c3e36d450ac150b921c2860074b2b90298ef7c3",
    "abstract": "Almost all summarisation methods and datasets focus on a single language and short summaries. We introduce a new dataset called WikinewsSum for English, German, French, Spanish, Portuguese, Polish, and Italian summarisation tailored for extended summaries of approx. 11 sentences. The dataset comprises 39,626 summaries which are news articles from Wikinews and their sources. We compare three multilingual transformer models on the extractive summarisation task and three training scenarios on which we fine-tune mT5 to perform abstractive summarisation. This results in strong baselines for both extractive and abstractive summarisation on WikinewsSum. We also show how the combination of an extractive model with an abstractive one can be used to create extended abstractive summaries from long input documents. Finally, our results show that fine-tuning mT5 on all the languages combined significantly improves the summarisation performance on low-resource languages.",
    "authors": [
        "R\u00e9mi Calizzano",
        "Malte Ostendorff",
        "Qianqian Ruan",
        "Georg Rehm"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The results show that fine-tuning mT5 on all the languages combined significantly improves the summarisation performance on low-resource languages and the combination of an extractive model with an abstractive one can be used to create extended abstractive summaries from long input documents."
    },
    "citationCount": 1,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}