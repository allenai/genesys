{
    "acronym": "15c19249814219de76ba92e8aa40a05f181c7648",
    "title": "KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
    "seed_ids": [
        "longformer",
        "3d6b094f439ceae770ad1ca5cb322421debf3ba8",
        "9fa9d5dd481400b2f3904b33d542d70a6affccb9",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5"
    ],
    "s2id": "15c19249814219de76ba92e8aa40a05f181c7648",
    "abstract": "With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts \u2014 from local (e.g., sentence), document-level, to global knowledge, to enable knowledge-rich and interpretable exchange across contexts. In addition, incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length. In light of these challenges, we propose KALM, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM firstly encodes long documents and knowledge graphs into the three knowledge-aware context representations. KALM then processes each context with context-specific layers. These context-specific layers are followed by a ContextFusion layer that facilitates knowledge exchange to derive an overarching document representation. Extensive experiments demonstrate that KALM achieves state-of-the-art performance on three long document understanding tasks across 6 datasets/settings. Further analyses reveal that the three knowledge-aware contexts are complementary and they all contribute to model performance, while the importance and information exchange patterns of different contexts vary on different tasks and datasets.",
    "authors": [
        "Shangbin Feng",
        "Zhaoxuan Tan",
        "Wenqian Zhang",
        "Zhenyu Lei",
        "Yulia Tsvetkov"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "KALM is proposed, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding and achieves state-of-the-art performance on three long documentUnderstanding tasks across 6 datasets/settings."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}