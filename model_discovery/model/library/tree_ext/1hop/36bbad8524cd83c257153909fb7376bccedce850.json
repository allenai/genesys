{
    "acronym": "36bbad8524cd83c257153909fb7376bccedce850",
    "title": "Exploring the Capability of Mamba in Speech Applications",
    "seed_ids": [
        "s4",
        "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "ad80530bad63e2f5939898bffbefa4b2956ee4f8",
        "a66bf047e0830c24e6e91c583a8c27632d7995ab",
        "2567a34501c1b258c102a07e737b87e556af0809",
        "154d5cf9a40d313cdf62372621f554a809fbb103",
        "067aaf0d1cde4ee21063be137559f2fe50125570",
        "e25f6a60211aa74ecfde8001a5939ff206102de4",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f"
    ],
    "s2id": "36bbad8524cd83c257153909fb7376bccedce850",
    "abstract": "This paper explores the capability of Mamba, a recently proposed architecture based on state space models (SSMs), as a competitive alternative to Transformer-based models. In the speech domain, well-designed Transformer-based models, such as the Conformer and E-Branchformer, have become the de facto standards. Extensive evaluations have demonstrated the effectiveness of these Transformer-based models across a wide range of speech tasks. In contrast, the evaluation of SSMs has been limited to a few tasks, such as automatic speech recognition (ASR) and speech synthesis. In this paper, we compared Mamba with state-of-the-art Transformer variants for various speech applications, including ASR, text-to-speech, spoken language understanding, and speech summarization. Experimental evaluations revealed that Mamba achieves comparable or better performance than Transformer-based models, and demonstrated its efficiency in long-form speech processing.",
    "authors": [
        "Koichi Miyazaki",
        "Yoshiki Masuyama",
        "Masato Murata"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Comparisons with state-of-the-art Transformer variants for various speech applications revealed that Mamba achieves comparable or better performance than Transformer-based models, and demonstrated its efficiency in long-form speech processing."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}