{
    "acronym": "9ccb5de1e22238b93a6af01c1dc341dc9bc3f28d",
    "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision",
    "seed_ids": [
        "gpt",
        "288e64e8adb23d81e291a2cb51e3a56b315023b7",
        "0b0debb710366cdff461938c80763eace1651af6",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b",
        "7a15950dc71079285a4eaf195de5aadd87c41b40"
    ],
    "s2id": "9ccb5de1e22238b93a6af01c1dc341dc9bc3f28d",
    "abstract": "Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \\textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such \\textit{easy-to-hard generalization from evaluators} can enable \\textit{easy-to-hard generalizations in generators} either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.",
    "authors": [
        "Zhiqing Sun",
        "Longhui Yu",
        "Yikang Shen",
        "Weiyang Liu",
        "Yiming Yang",
        "Sean Welleck",
        "Chuang Gan"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel approach to scalable alignment is proposed, which firstly trains the process-supervised reward models on easy problems, and then uses them to evaluate the performance of policy models on hard problems, and suggests a promising path toward AI systems that advance beyond the frontier of human supervision."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}