{
    "acronym": "c4f4e5e86cb9468235fd5d7c9ccce09a68084463",
    "title": "WaveMix: Resource-efficient Token Mixing for Images",
    "seed_ids": [
        "nystromformer",
        "4cf65e4f358c45476790df1f0ced0fc90c2a405b",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd"
    ],
    "s2id": "c4f4e5e86cb9468235fd5d7c9ccce09a68084463",
    "abstract": "Although certain vision transformer (ViT) and CNN architectures generalize well on vision tasks, it is often impractical to use them on green, edge, or desktop computing due to their computational requirements for training and even testing. We present WaveMix as an alternative neural architecture that uses a multi-scale 2D discrete wavelet transform (DWT) for spatial token mixing. Unlike ViTs, WaveMix neither unrolls the image nor requires self-attention of quadratic complexity. Additionally, DWT introduces another inductive bias -- besides convolutional filtering -- to utilize the 2D structure of an image to improve generalization. The multi-scale nature of the DWT also reduces the requirement for a deeper architecture compared to the CNNs, as the latter relies on pooling for partial spatial mixing. WaveMix models show generalization that is competitive with ViTs, CNNs, and token mixers on several datasets while requiring lower GPU RAM (training and testing), number of computations, and storage. WaveMix have achieved State-of-the-art (SOTA) results in EMNIST Byclass and EMNIST Balanced datasets.",
    "authors": [
        "Pranav Jeevan",
        "A. Sethi"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "WaveMix is presented as an alternative neural architecture that uses a multi-scale 2D discrete wavelet transform (DWT) for spatial token mixing that shows generalization that is competitive with ViTs, CNNs, and token mixers on several datasets while requiring lower GPU RAM, number of computations, and storage."
    },
    "citationCount": 9,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}