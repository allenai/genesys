{
    "acronym": "13581a46d32822e44cbeb1acdba4a59cef2b2ec1",
    "title": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review",
    "seed_ids": [
        "gpt",
        "bb15f3727f827a3cb88b5d3ca48415c09b40a88f",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "09312f70402847d4c2b5b5348d902ad0b2d4a0d5",
        "f5241abdad7e43606f1c79fc71273d43de3f31c5",
        "13270b9759cf0296b5a346fbb58b706e8ad0a982",
        "2475b38a76a9c2dc67f74446e2e686815764b0f2",
        "99934ef57a75f49afe68736cbc7dbf480687b552",
        "81521a80f7aa939281863c15b42e446ff5a0e65a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "bf6ce546c589fa8054b3972b266532664914bd21",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "98850975e574e08695a9f32b4c8747dc7f8bcc17",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "48af9b314181b04edcc0b7224ffe4689036b755f",
        "5d032bd2632b6f5847767f39ce247098c6bbc563",
        "e2f2662f0734e2edc2b4b36a734de111c7f8d54d",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "7d2a78a1f713b71c3a337247d042c5c2f0b2da84",
        "f4fda917eb5c170de67d361b9fdc355599666dbe",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "6954a6bb9d6f3e365b26b694c963ae1d62a03444",
        "d931f84abfc4550c10ceb113b142c8eb3e07571e",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "13581a46d32822e44cbeb1acdba4a59cef2b2ec1",
    "abstract": "The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules, compression training, model initialization and model-centric curriculum learning techniques, which focus on accelerating the training via reducing the calculations on parameters; (3) optimization-centric, including the selection of learning rate, the employment of large batchsize, the designs of efficient objectives, and model average techniques, which pay attention to the training policy and improving the generality for the large-scale models; (4) budgeted training, including some distinctive acceleration methods on source-constrained situations; (5) system-centric, including some efficient open-source distributed libraries/systems which provide adequate hardware support for the implementation of acceleration algorithms. By presenting this comprehensive taxonomy, our survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction.",
    "authors": [
        "Li Shen",
        "Yan Sun",
        "Zhiyuan Yu",
        "Liang Ding",
        "Xinmei Tian",
        "Dacheng Tao"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction of training deep learning models, and presents this comprehensive taxonomy."
    },
    "citationCount": 24,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}