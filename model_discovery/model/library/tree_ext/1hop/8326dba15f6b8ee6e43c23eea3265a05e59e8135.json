{
    "acronym": "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
    "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
    "seed_ids": [
        "butterfly",
        "fbaa944e73644ce12ea4a0ac8ffb64c3280f3aff",
        "90b21dbad8969b74d704eed15a3d98722a88e464",
        "232734a65f0d903cdd77b9dfdddb1eded79eff18",
        "3451010e8fa6a3032c8dd3be1daadb4a08375c64",
        "a68c3412e60560290400d2707596f82a914b7c00",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "b1ac64438608aac1a8dfd0adf8fec8c6220f6bfd",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
    "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called\"reverse sparsification,\"Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
    "authors": [
        "Tri Dao",
        "Beidi Chen",
        "N. Sohoni",
        "Arjun D Desai",
        "Michael Poli",
        "Jessica Grogan",
        "Alexander Liu",
        "Aniruddh Rao",
        "A. Rudra",
        "Christopher R\u00e9"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution and can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications."
    },
    "citationCount": 59,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}