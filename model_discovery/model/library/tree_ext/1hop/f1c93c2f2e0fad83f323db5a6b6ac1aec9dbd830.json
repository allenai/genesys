{
    "acronym": "f1c93c2f2e0fad83f323db5a6b6ac1aec9dbd830",
    "title": "Can LLMs Patch Security Issues?",
    "seed_ids": [
        "gpt3"
    ],
    "s2id": "f1c93c2f2e0fad83f323db5a6b6ac1aec9dbd830",
    "abstract": "Large Language Models (LLMs) have shown impressive proficiency in code generation. Unfortunately, these models share a weakness with their human counterparts: producing code that inadvertently has security vulnerabilities. These vulnerabilities could allow unauthorized attackers to access sensitive data or systems, which is unacceptable for safety-critical applications. In this work, we propose Feedback-Driven Security Patching (FDSP), where LLMs automatically refine generated, vulnerable code. Our approach leverages automatic static code analysis to empower the LLM to generate and implement potential solutions to address vulnerabilities. We address the research communitys needs for safe code generation by introducing a large-scale dataset, PythonSecurityEval, covering the diversity of real-world applications, including databases, websites and operating systems. We empirically validate that FDSP outperforms prior work that uses self-feedback from LLMs by up to 17.6% through our procedure that injects targeted, external feedback. Code and data are available at \\url{https://github.com/Kamel773/LLM-code-refine}",
    "authors": [
        "Kamel Alrashedy",
        "Abdullah Aljasser"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Feedback-Driven Security Patching (FDSP), where LLMs automatically refine generated, vulnerable code, and leverages automatic static code analysis to empower the LLM to generate and implement potential solutions to address vulnerabilities."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}