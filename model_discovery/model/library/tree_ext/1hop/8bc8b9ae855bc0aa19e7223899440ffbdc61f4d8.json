{
    "acronym": "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8",
    "title": "Linearized Relative Positional Encoding",
    "seed_ids": [
        "roformer",
        "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "ee1e1afe75eb0e20d57bf316f5ab1ca2c369d100",
        "86c8d930b492a4f9cadc6c60aecdaaded49acc86",
        "6be32b4321f95b79bb2e37feeab0c3c7f902195e",
        "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "7509c66a666e2e3f14bc8676b969b945ee6e136f",
        "08ffdec40291a2ccb5f8a6cc048b01247fb34b96",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "868d4d9c9d8afee78f21a0113cff762ff5eb4961",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "84476fdf6ead3553f4493dff8e02308439d6222b",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "09e2c7adbed37440d4a339852cfa34e5b660f768",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8",
    "abstract": "Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications. Experiments show that compared with existing methods, LRPE achieves state-of-the-art performance in language modeling, text classification, and image classification. Meanwhile, it emphasizes a general paradigm for designing broadly more relative positional encoding methods that are applicable to linear transformers. The code is available at https://github.com/OpenNLPLab/Lrpe.",
    "authors": [
        "Zhen Qin",
        "Weixuan Sun",
        "Kaiyue Lu",
        "Huizhong Deng",
        "Dong Li",
        "Xiaodong Han",
        "Yuchao Dai",
        "Lingpeng Kong",
        "Yiran Zhong"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work puts together a variety of existing linear relative positional encoding approaches under a canonical form and further proposes a family of linearrelative positional encoding algorithms via unitary transformation, leading to a principled framework that can be used to develop new relative positional encode methods that preserve linear space-time complexity."
    },
    "citationCount": 10,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}