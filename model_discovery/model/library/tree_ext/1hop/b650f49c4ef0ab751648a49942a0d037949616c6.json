{
    "acronym": "b650f49c4ef0ab751648a49942a0d037949616c6",
    "title": "Rethinking Attention Mechanisms in Vision Transformers with Graph Structures",
    "seed_ids": [
        "gmlp",
        "7c7f7f191aa7ceb083e7f7295f89ce400192220f",
        "2d98048c2d2fcd3f6b989d2a54003808906ab4b7",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "b650f49c4ef0ab751648a49942a0d037949616c6",
    "abstract": "In this paper, we propose a new type of vision transformer (ViT) based on graph head attention (GHA). Because the multi-head attention (MHA) of a pure ViT requires multiple parameters and tends to lose the locality of an image, we replaced MHA with GHA by applying a graph to the attention head of the transformer. Consequently, the proposed GHA maintains both the locality and globality of the input patches and guarantees the diversity of the attention. The proposed GHA-ViT commonly outperforms pure ViT-based models using small-sized CIFAR-10/100, MNIST, and MNIST-F datasets and a medium-sized ImageNet-1K dataset in scratch training. A Top-1 accuracy of 81.7% was achieved for ImageNet-1K using GHA-B, which is a base model with approximately 29 M parameters. In addition, with CIFAR-10/100, the existing ViT and parameters are reduced 17-fold and the performance increased by 0.4/4.3%, respectively. The proposed GHA-ViT shows promising results in terms of the number of parameters and operations and the level of accuracy in comparison with other state-of-the-art ViT-lightweight models.",
    "authors": [
        "H. Kim",
        "ByoungChul Ko"
    ],
    "venue": "Italian National Conference on Sensors",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The proposed GHA-ViT shows promising results in terms of the number of parameters and operations and the level of accuracy in comparison with other state-of-the-art ViT-lightweight models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}