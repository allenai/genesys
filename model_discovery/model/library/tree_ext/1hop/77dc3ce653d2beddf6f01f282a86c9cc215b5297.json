{
    "acronym": "77dc3ce653d2beddf6f01f282a86c9cc215b5297",
    "title": "Finnish ASR with Deep Transformer Models",
    "seed_ids": [
        "transformerxl",
        "d348735a8e25716ca5a3b25175b604c3acbe7b7a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "77dc3ce653d2beddf6f01f282a86c9cc215b5297",
    "abstract": "Recently, BERT and Transformer-XL based architectures have achieved strong results in a range of NLP applications. In this paper, we explore Transformer architectures\u2014BERT and Transformer-XL\u2014as a language model for a Finnish ASR task with different rescoring schemes. We achieve strong results in both an intrinsic and an extrinsic task with Transformer-XL. Achieving 29% better perplexity and 3% better WER than our previous best LSTM-based approach. We also introduce a novel three-pass decoding scheme which improves the ASR performance by 8%. To the best of our knowledge, this is also the \ufb01rst work (i) to formulate an alpha smoothing framework to use the non-autoregressive BERT language model for an ASR task, and (ii) to explore sub-word units with Transformer-XL for an agglutinative language like Finnish.",
    "authors": [
        "Abhilash Jain",
        "Aku Rouhe",
        "Stig-Arne Gr\u00f6nroos",
        "M. Kurimo"
    ],
    "venue": "Interspeech",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper explores Transformer architectures\u2014BERT and Transformer-XL\u2014as a language model for a Finnish ASR task with different rescoring schemes and introduces a novel three-pass decoding scheme which improves the ASR performance by 8%."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}