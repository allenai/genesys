{
    "acronym": "6ffd19930ab0170f8cfdbf926e8f3c11f85e289a",
    "title": "Decoupled Model Schedule for Deep Learning Training",
    "seed_ids": [
        "flashattn",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6ffd19930ab0170f8cfdbf926e8f3c11f85e289a",
    "abstract": "Recent years have seen an increase in the development of large deep learning (DL) models, which makes training ef\ufb01-ciency crucial. Common practice is struggling with the trade-off between usability and performance. On one hand, DL frameworks such as PyTorch use dynamic graphs to facilitate model developers at a price of sub-optimal model training performance. On the other hand, practitioners propose various approaches to improving the training ef\ufb01ciency by sacri\ufb01cing some of the \ufb02exibility, ranging from making the graph static for more thorough optimization (e.g., XLA) to customizing optimization towards large-scale distributed training (e.g., DeepSpeed and Megatron-LM). In this paper, we aim to address the tension between usability and training ef\ufb01ciency through separation of concerns. Inspired by DL compilers that decouple the platform-speci\ufb01c optimizations of a tensor-level operator from its arithmetic de\ufb01nition, this paper proposes a schedule language, Slapo, to decouple model execution from de\ufb01nition. Speci\ufb01cally, Slapo works on a PyTorch model and uses a set of schedule primitives to convert the model for common model training optimizations such as high-performance kernels, effective 3D parallelism, and ef\ufb01cient activation checkpointing. Compared to existing optimization solutions, Slapo progressively optimizes the model \u201cas-needed\u201d through high-level primitives, and thus preserving programmability and debuggability for users to a large extent. Our evaluation results show that by scheduling the existing hand-crafted optimizations in a systematic way using Slapo, we are able",
    "authors": [
        "Hongzheng Chen",
        "Cody Hao Yu",
        "Shuai Zheng",
        "Zhen Zhang",
        "Zhiru Zhang",
        "Yida Wang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Inspired by DL compilers that decouple the platform-speci\ufb01c optimizations of a tensor-level operator from its arithmetic de\ufb01nition, this paper proposes a schedule language, Slapo, to decouple model execution from de\ufb01nition, and preserves programmability and debuggability for users to a large extent."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}