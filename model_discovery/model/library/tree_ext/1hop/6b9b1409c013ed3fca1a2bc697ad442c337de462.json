{
    "acronym": "6b9b1409c013ed3fca1a2bc697ad442c337de462",
    "title": "Abstractive Financial News Summarization via Transformer-BiLSTM Encoder and Graph Attention-Based Decoder",
    "seed_ids": [
        "bigbird",
        "01b15017ac59b8d6f2ce3598c4a7d6358c211426",
        "d6b414487787d0b6efd735a3236a690ad13aae70",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "6b9b1409c013ed3fca1a2bc697ad442c337de462",
    "abstract": "Financial news summarization (FNS) has been an attractive research problem in recent years, which aims to generate a shorter highlight of the news article while preserving key factual aspects, emotions, and opinions, providing significant assistance in stock trading and investment decision-making. However, FNS faces two challenges compared to the common domain. Firstly, financial news involves professional qualitative and quantitative information and salient content always scatters across long-range interactions. Secondly, financial news contains latent causal relationships, where historical information in the early generated sequence can significantly affect the subsequent decoding process. To address these difficulties, we propose an enhanced Seq2Seq model named TLGA, where the hierarchical Transformer-BiLSTM encoder can capture long-range interactions and sequential semantics while the Graph Attention-based decoder can fully utilize the historical information of decoded tokens and capture key causal relations. Moreover, we propose history-enhanced attention to concentrate on salient input content based on history semantics, guiding our decoder to generate the summary around the corresponding contents. It is also the first attempt to reuse history information of previously generated summary sequences in FNS using the idea of the Graph Attention Mechanism. Additionally, we construct the LCFNS dataset with 430,820 news-summary pairs for the lack of large-scale high-quality datasets in FNS. Experimental results on two financial datasets and two benchmark datasets indicate that our model outperforms other baselines.",
    "authors": [
        "Haozhou Li",
        "Qinke Peng",
        "Xu Mou",
        "Ying Wang",
        "Zeyuan Zeng",
        "Muhammad Fiaz Bashir"
    ],
    "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An enhanced Seq2Seq model named TLGA is proposed, where the hierarchical Transformer-BiLSTM encoder can capture long-range interactions and sequential semantics while the Graph Attention-based decoder can fully utilize the historical information of decoded tokens and capture key causal relations."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}