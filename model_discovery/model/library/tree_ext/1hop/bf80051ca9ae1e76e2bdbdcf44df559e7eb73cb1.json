{
    "acronym": "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1",
    "title": "A Practical Survey on Faster and Lighter Transformers",
    "seed_ids": [
        "nystromformer",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8",
        "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
        "cec7872b194aadf54140578b9be52939eb1112e9",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "4ee1754f89b250753242c3baf58773dda5d9bf39",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "09e2c7adbed37440d4a339852cfa34e5b660f768",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "f6390beca54411b06f3bde424fb983a451789733",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "2a31319e73d4486716168b65cdf7559baeda18ce",
        "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1",
    "abstract": "Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models\u2019 efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer\u2019s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for researchers and practitioners to determine which methods to apply in practice to meet the desired tradeoff between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make Transformers faster and lighter and by providing a comprehensive explanation of the methods\u2019 strengths, limitations, and underlying assumptions.",
    "authors": [
        "Quentin Fournier",
        "G. Caron",
        "Daniel Aloise"
    ],
    "venue": "ACM Computing Surveys",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This survey investigates popular approaches to make Transformers faster and lighter and provides a comprehensive explanation of the methods\u2019 strengths, limitations, and underlying assumptions to meet the desired tradeoff between capacity, computation, and memory."
    },
    "citationCount": 58,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}