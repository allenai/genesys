{
    "acronym": "7939b4d7fac1d3525f92dfb50dad9a5d56ad7cfb",
    "title": "Compressing Lengthy Context With UltraGist",
    "seed_ids": [
        "compresscontext",
        "4e2a3ddeaf5defb9318b43c364b9a9efe3847dcf",
        "b6346f9fa093b8e85df712485a2b851b9f680dac",
        "73290ecbec2f38d1d647ddef1ada69cee41725b3",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "2a09ebbfcca1a6994eeb472cd4159f5f3858dbf9",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "594d8e1696619f3cebb7c6bffdad8e0a5592f006",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9dc624d7258d1a56117ca720aea953ce46b66b21"
    ],
    "s2id": "7939b4d7fac1d3525f92dfb50dad9a5d56ad7cfb",
    "abstract": "Compressing lengthy context is a critical but technically challenging problem. In this paper, we propose a new method called UltraGist, which is distinguished for its high-quality compression of lengthy context due to the innovative design of the compression and learning algorithm. UltraGist brings forth the following important benefits. Firstly, it notably contributes to the flexibility of compression, as it can be effectively learned to support a broad range of context lengths and compression ratios. Secondly, it helps to produce fine-grained compression for the lengthy context, where each small segment of the context is progressively processed on top of a tailored cross-attention mechanism. Thirdly, it makes the training process sample-efficient and thus maximizes the use of training data. Finally, it facilitates the efficient running of compression for dynamic context, as the compression result can be progressively generated and hence incrementally updated. UltraGist is evaluated on a wide variety of tasks associated with lengthy context, such as document QA and summarization, few-shot learning, multi-session conversation, et al. Whilst the existing methods fail to handle these challenging scenarios, our approach is able to preserve a near-lossless compression performance throughout all the evaluations. Our data, model, and code have been released at \\url{https://github.com/namespace-Pt/UltraGist}.",
    "authors": [
        "Peitian Zhang",
        "Zheng Liu",
        "Shitao Xiao",
        "Ninglu Shao",
        "Qiwei Ye",
        "Zhicheng Dou"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a new method called UltraGist, which is distinguished for its high-quality compression of lengthy context due to the innovative design of the compression and learning algorithm."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}