{
    "acronym": "7dd88d4f372b22cb9ed0e4e8db42e27b1d53da97",
    "title": "Efficiency-oriented approaches for self-supervised speech representation learning",
    "seed_ids": [
        "linformer",
        "a66bf047e0830c24e6e91c583a8c27632d7995ab",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "d5e999aae76d5270ef272076979c809817458212",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "657329c633709dd1ac34a30d57341b186b1a47c2"
    ],
    "s2id": "7dd88d4f372b22cb9ed0e4e8db42e27b1d53da97",
    "abstract": "Self-supervised learning enables the training of large neural models without the need for large, labeled datasets. It has been generating breakthroughs in several fields, including computer vision, natural language processing, biology, and speech. In particular, the state-of-the-art in several speech processing applications, such as automatic speech recognition or speaker identification, are models where the latent representation is learned using self-supervised approaches. Several configurations exist in self-supervised learning for speech, including contrastive, predictive, and multilingual approaches. There is, however, a crucial limitation in most existing approaches: their high computational costs. These costs limit the deployment of models, the size of the training dataset, and the number of research groups that can afford research with large self-supervised models. Likewise, we should consider the environmental costs that high energy consumption implies. Efforts in this direction comprise optimization of existing models, neural architecture efficiency, improvements in finetuning for speech processing tasks, and data efficiency. But despite current efforts, more work could be done to address high computational costs in self-supervised representation learning.",
    "authors": [
        "Luis Lugo",
        "Valentin Vielzeuf"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "More work could be done to address high computational costs in self-supervised representation learning, which limit the deployment of models, the size of the training dataset, and the number of research groups that can afford research with large self-supervised models."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}