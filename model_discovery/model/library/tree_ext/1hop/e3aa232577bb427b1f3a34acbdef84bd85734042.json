{
    "acronym": "e3aa232577bb427b1f3a34acbdef84bd85734042",
    "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models",
    "seed_ids": [
        "lex",
        "alibi",
        "roformer",
        "receptivefieldana",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "d9964ab436eefd21f923a4bc833c6b66692c7f00",
        "5735e49e501c8e51e9be4079592e46e047747b03",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "7509c66a666e2e3f14bc8676b969b945ee6e136f",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "46c585ee9abf76779ea4b863d2da4358efd0d1d3",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "e3aa232577bb427b1f3a34acbdef84bd85734042",
    "abstract": "In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex domains, they often face the need to follow longer user prompts or generate longer texts. In these situations, the length generalization failure of LLMs on long sequences becomes more prominent. Most pre-training schemes truncate training sequences to a fixed length. LLMs often struggle to generate fluent and coherent texts after longer contexts, even with relative positional encoding specifically designed to cope with this problem. Common solutions such as finetuning on longer corpora often involve daunting hardware and time costs and require careful training process design. To more efficiently extrapolate existing LLMs\u2019 generation quality to longer texts, we theoretically and empirically investigate the main out-of-distribution (OOD) factors contributing to this problem. Inspired by this diagnosis, we propose a simple yet effective solution for on-the-fly length generalization, LM-Infinite. It involves only a \u039b-shaped attention mask (to avoid excessive attended tokens) and a distance limit (to avoid unseen distances) while requiring no parameter updates or learning. We find it applicable to a variety of LLMs using relative-position encoding methods. LM-Infinite is computationally efficient with O(n) time and space, and demonstrates consistent text generation fluency and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. We will make the codes publicly available following publication.",
    "authors": [
        "Chi Han",
        "Qifan Wang",
        "Wenhan Xiong",
        "Yu Chen",
        "Heng Ji",
        "Sinong Wang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A simple yet effective solution for on-the-fly length generalization, LM-Infinite, which is applicable to a variety of LLMs using relative-position encoding methods and demonstrates consistent text generation fluency and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets."
    },
    "citationCount": 52,
    "influentialCitationCount": 11,
    "code": null,
    "description": null,
    "url": null
}