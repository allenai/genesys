{
    "acronym": "745594bd0dc3e9dc86f74e100cd2c98ed36256c0",
    "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
    "seed_ids": [
        "mamba",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "d5e999aae76d5270ef272076979c809817458212",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "745594bd0dc3e9dc86f74e100cd2c98ed36256c0",
    "abstract": "State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.",
    "authors": [
        "Maciej Pi'oro",
        "Kamil Ciebiera",
        "Krystian Kr'ol",
        "Jan Ludziejewski",
        "Sebastian Jaszczur"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE and reaches the same performance as Mamba in $2.35\\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer."
    },
    "citationCount": 22,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}