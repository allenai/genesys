{
    "acronym": "77706ee4cbdbb23345da22af37bc1b9f5ec8f110",
    "title": "Sub-Linear Memory: How to Make Performers SLiM",
    "seed_ids": [
        "performer",
        "lineartransformer",
        "reformer",
        "transformerxl",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "bed7c155b843fda8a1c994ce71e9176e43b20f77",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "77706ee4cbdbb23345da22af37bc1b9f5ec8f110",
    "abstract": "The Transformer architecture has revolutionized deep learning on sequential data, becoming ubiquitous in state-of-the-art solutions for a wide variety of applications. Yet vanilla Transformers are notoriously resource-expensive, requiring $O(L^2)$ in serial time and memory as functions of input length $L$. Recent works proposed various linear self-attention mechanisms, scaling only as $O(L)$ for serial computation. We perform a thorough analysis of recent Transformer mechanisms with linear self-attention, Performers, in terms of overall computational complexity. We observe a remarkable computational flexibility: forward and backward propagation can be performed with no approximations using sublinear memory as a function of $L$ (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting. In the extreme case, a Performer consumes only $O(1)$ memory during training, and still requires $O(L)$ time. This discovered time-memory tradeoff can be used for training or, due to complete backward-compatibility, for fine-tuning on a low-memory device, e.g. a smartphone or an earlier-generation GPU, thus contributing towards decentralized and democratized deep learning.",
    "authors": [
        "Valerii Likhosherstov",
        "K. Choromanski",
        "Jared Davis",
        "Xingyou Song",
        "Adrian Weller"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A thorough analysis of recent Transformer mechanisms with linear self-attention, Performers, results in a remarkable computational flexibility: forward and backward propagation can be performed with no approximations using sublinear memory as a function of $L$ (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting."
    },
    "citationCount": 16,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}