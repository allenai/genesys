{
    "acronym": "28a840d36aeabb165804c293f9b91a47d118f3b9",
    "title": "GFNet: Global Filter Networks for Visual Recognition",
    "seed_ids": [
        "flashattn",
        "gmlp",
        "fnet",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "f75cddf2d42ed01b34686704eb3504becef67442",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "6b6ffb94626e672caffafc77097491d9ee7a8682",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "28a840d36aeabb165804c293f9b91a47d118f3b9",
    "abstract": "Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision Transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. Based on this basic design, we develop a series of isotropic models with a Transformer-style simple architecture and CNN-style hierarchical models with better performance. Isotropic GFNet models exhibit favorable accuracy/complexity trade-offs compared to recent vision Transformers and pure MLP models. Hierarchical GFNet models can inherit successful designs in CNNs and be easily scaled up with larger model sizes and more training data, showing strong performance on both image classification (e.g., 85.0% top-1 accuracy on ImageNet-1 k without any extra data or supervision, and 87.4% accuracy with ImageNet-21 k pre-training) and dense prediction tasks (e.g., 54.3 mIoU on ADE20 k val). Our results demonstrate that GFNet can be a very competitive alternative to Transformer-based models and CNNs in terms of efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet.",
    "authors": [
        "Yongming Rao",
        "Wenliang Zhao",
        "Zhengbiao Zhu",
        "Jie Zhou",
        "Jiwen Lu"
    ],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture that learns long-term spatial dependencies in the frequency domain with log-linear complexity and can be a very competitive alternative to Transformer-based models and CNNs in terms of efficiency, generalization ability and robustness."
    },
    "citationCount": 8,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}