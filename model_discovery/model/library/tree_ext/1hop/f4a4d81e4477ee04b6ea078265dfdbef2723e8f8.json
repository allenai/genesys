{
    "acronym": "f4a4d81e4477ee04b6ea078265dfdbef2723e8f8",
    "title": "GRAM: Global Reasoning for Multi-Page VQA",
    "seed_ids": [
        "colt5",
        "bigbird",
        "longformer",
        "pi",
        "alibi",
        "transformerxl",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "3d07f41628a25e2e9b895b8a3659ca3ebfd1a73f",
        "594d8e1696619f3cebb7c6bffdad8e0a5592f006",
        "1436ee75f8dded44de157cf778a96bdaf6b20a76",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "f4a4d81e4477ee04b6ea078265dfdbef2723e8f8",
    "abstract": "The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our compression-transformer (C-Former),reducing the encoded sequence length, thereby allowing a tradeoff between quality and latency. Extensive experiments showcase GRAM's state-of-the-art performance on the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our approach.",
    "authors": [
        "Tsachi Blau",
        "Sharon Fogel",
        "Roi Ronen",
        "Alona Golts",
        "Roy Ganz",
        "Elad Ben Avraham",
        "Aviad Aberdam",
        "Shahar Tsiper",
        "Ron Litman"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining, and proposes a tailored bias adaptation method to enforce the newly introduced document tokens."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}