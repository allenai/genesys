{
    "acronym": "c38ba200201655f8cb9594cddd52daa8b6f4f335",
    "title": "What Makes for Hierarchical Vision Transformer?",
    "seed_ids": [
        "gmlp",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "6b6ffb94626e672caffafc77097491d9ee7a8682",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "c38ba200201655f8cb9594cddd52daa8b6f4f335",
    "abstract": "Recent studies indicate that hierarchical Vision Transformer (ViT) with a macro architecture of interleaved non-overlapped window-based self-attention & shifted-window operation can achieve state-of-the-art performance in various visual recognition tasks, and challenges the ubiquitous convolutional neural networks (CNNs) using densely slid kernels. In most recently proposed hierarchical ViTs, self-attention is the de-facto standard for spatial information aggregation. In this paper, we question whether self-attention is the only choice for hierarchical ViT to attain strong performance, and study the effects of different kinds of cross-window communication methods. To this end, we replace self-attention layers with embarrassingly simple linear mapping layers, and the resulting proof-of-concept architecture termed <sc>TransLinear</sc> can achieve very strong performance in ImageNet-<inline-formula><tex-math notation=\"LaTeX\">$\\text{1}\\,k$</tex-math><alternatives><mml:math><mml:mrow><mml:mtext>1</mml:mtext><mml:mspace width=\"0.166667em\"/><mml:mi>k</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"wang-ieq1-3282019.gif\"/></alternatives></inline-formula> image recognition. Moreover, we find that <sc>TransLinear</sc> is able to leverage the ImageNet pre-trained weights and demonstrates competitive transfer learning properties on downstream dense prediction tasks such as object detection and instance segmentation. We also experiment with other alternatives to self-attention for content aggregation inside each non-overlapped window under different cross-window communication approaches. Our results reveal that the <italic>macro architecture</italic>, other than specific aggregation layers or cross-window communication mechanisms, is more responsible for hierarchical ViT's strong performance and is the real challenger to the ubiquitous CNN's dense sliding window paradigm.",
    "authors": [
        "Yuxin Fang",
        "Xinggang Wang",
        "Rui Wu",
        "Wenyu Liu"
    ],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The results reveal that the macro architecture, other than specific aggregation layers or cross-window communication mechanisms, is more responsible for hierarchical ViT's strong performance and is the real challenger to the ubiquitous CNN's dense sliding window paradigm."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}