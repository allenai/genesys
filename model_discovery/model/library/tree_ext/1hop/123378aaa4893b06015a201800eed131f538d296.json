{
    "acronym": "123378aaa4893b06015a201800eed131f538d296",
    "title": "Study Neural Architecture Search",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "123378aaa4893b06015a201800eed131f538d296",
    "abstract": "Due to the limitation of manually designing neural network architecture, Neural Architecture Search arises to algorithmically learn the suitable network architecture for machine learning tasks. This report will emphasize on two elements of this project, i.e. Neural Architecture Search and its application on BERT, an attention-based neural network for natural language understanding. After experiments, we realized prediction distillation is the most effective objective for sub-architecture searching over the multiheads and the feed-forward layer connection. The latest experiment result shows that the result of our architecture searching algorithm can surpass the performance of the existing BERT models of similar architecture computational complexity. Abbreviation AutoML \u2013 Automatic Machine Learning BERT \u2013 Bidirectional Encoder Representations from Transformers FLOPS \u2013 Floating Point Operations Per Second LSTM \u2013 Long Short-Term Memory NAS \u2013 Neural Architecture Search NLP \u2013 Natural Language Processing RNN \u2013 Recurrent Neural Network TinyBERT 4L \u2013 A variant of BERT with 4 hidden layers from [28]",
    "authors": [],
    "venue": "",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The latest experiment result shows that the result of the architecture searching algorithm can surpass the performance of the existing BERT models of similar architecture computational complexity."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}