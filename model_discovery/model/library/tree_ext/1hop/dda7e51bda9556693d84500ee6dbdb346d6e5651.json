{
    "acronym": "dda7e51bda9556693d84500ee6dbdb346d6e5651",
    "title": "ZipZap: Efficient Training of Language Models for Large-Scale Fraud Detection on Blockchain",
    "seed_ids": [
        "performer",
        "434f4ecbfdea4496bbcd763427fc605bf11abddc",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88"
    ],
    "s2id": "dda7e51bda9556693d84500ee6dbdb346d6e5651",
    "abstract": "Language models (LMs) have demonstrated superior performance in detecting fraudulent activities on Blockchains. Nonetheless, the sheer volume of Blockchain data results in excessive memory and computational costs when training LMs from scratch, limiting their capabilities to large-scale applications. In this paper, we present ZipZap, a framework tailored to achieve both parameter and computational efficiency when training LMs on large-scale transaction data. First, with the frequency-aware compression, an LM can be compressed down to a mere 7.5% of its initial size with an imperceptible performance dip. This technique correlates the embedding dimension of an address with its occurrence frequency in the dataset, motivated by the observation that embeddings of low-frequency addresses are insufficiently trained and thus negating the need for a uniformly large dimension for knowledge representation. Second, ZipZap accelerates the speed through the asymmetric training paradigm: It performs transaction dropping and cross-layer parameter-sharing to expedite the pre-training process, while revert to the standard training paradigm for fine-tuning to strike a balance between efficiency and efficacy, motivated by the observation that the optimization goals of pre-training and fine-tuning are inconsistent. Evaluations on real-world, large-scale datasets demonstrate that ZipZap delivers notable parameter and computational efficiency improvements for training LMs. Our implementation is available at: https://github.com/git-disl/ZipZap.",
    "authors": [
        "Sihao Hu",
        "Tiansheng Huang",
        "Ka-Ho Chow",
        "Wenqi Wei",
        "Yanzhao Wu",
        "Ling Liu"
    ],
    "venue": "The Web Conference",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents ZipZap, a framework tailored to achieve both parameter and computational efficiency when training LMs on large-scale transaction data, and demonstrates that ZipZap delivers notable parameter and computational efficiency improvements for training LMs."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}