{
    "acronym": "412e266cddfd87c79087a88ba1e4d11b89a45a13",
    "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
    "seed_ids": [
        "gpt2",
        "sparsetransformer",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "d6a0dfd5f39222d8924b7727a0a49f81fa247d71",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "12809bcb734beafeb47876f42e7b438e27fe99fe",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "f75d05e759447c2aedb7097728f29f9a520d9bc1",
        "5d032bd2632b6f5847767f39ce247098c6bbc563",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "412e266cddfd87c79087a88ba1e4d11b89a45a13",
    "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
    "authors": [
        "L. Yu",
        "Daniel Simig",
        "Colin Flaherty",
        "Armen Aghajanyan",
        "Luke Zettlemoyer",
        "M. Lewis"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes, is proposed, establishing the viability of tokenization-free autoregressive sequence modeling at scale."
    },
    "citationCount": 58,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}