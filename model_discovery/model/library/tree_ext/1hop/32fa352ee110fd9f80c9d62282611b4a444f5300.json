{
    "acronym": "32fa352ee110fd9f80c9d62282611b4a444f5300",
    "title": "On the Expressive Flexibility of Self-Attention Matrices",
    "seed_ids": [
        "performer",
        "bigbird",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "b1c39d042fdf8f00a407b0df734764beb6c3b062",
        "3694381e74445a8b9f8cb8d373e39626e47191b5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "32fa352ee110fd9f80c9d62282611b4a444f5300",
    "abstract": "Transformer networks are able to capture patterns in data coming from many domains (text, images, videos, proteins, etc.) with little or no change to architecture components. We perform a theoretical analysis of the core component responsible for signal propagation between elements, i.e. the self-attention matrix. We ask the following question: Can self-attention matrix approximate arbitrary patterns? How small is the query dimension d required for such approximation? Our first result shows that the task of deciding whether approximation of a given pattern is possible or not is NP-hard for a fixed d greater than one. In practice, self-attention matrix typically exhibits two properties: it is sparse, and it changes dynamically depending on the input to the module. Motivated by this observation, we show that the self-attention matrix can provably approximate sparse matrices. While the parameters of self-attention are fixed, various sparse matrices can be approximated by only modifying the inputs. Our proof is based on the random projection technique and uses the seminal Johnson-Lindenstrauss lemma. In particular, we show that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, d grows only logarithmically with the sequence length n.",
    "authors": [
        "Valerii Likhosherstov",
        "K. Choromanski",
        "Adrian Weller"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, d grows only logarithmically with the sequence length n."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}