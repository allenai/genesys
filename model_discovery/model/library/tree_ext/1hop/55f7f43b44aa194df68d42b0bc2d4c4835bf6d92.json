{
    "acronym": "55f7f43b44aa194df68d42b0bc2d4c4835bf6d92",
    "title": "TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models",
    "seed_ids": [
        "bert",
        "15c19249814219de76ba92e8aa40a05f181c7648",
        "1dbb523a6555d6e0c5727620e2b57daaa5b79dc0",
        "75f1c7dadb4ed733fb4d3a4cc47b9cbde9ad98cc",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3"
    ],
    "s2id": "55f7f43b44aa194df68d42b0bc2d4c4835bf6d92",
    "abstract": "KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from relation triples in knowledge graphs. However, these models do not prioritize learning embeddings for entity-related tokens. Updating all parameters in KEPLM is computationally demanding. This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models. We observe that text corpora contain entities that follow a long-tail distribution, where some are suboptimally optimized and hinder the pre-training process. To tackle this, we employ a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information. Moreover, updating a small subset of neurons in the feed-forward networks (FFNs) that store factual knowledge is both sufficient and efficient. Specifically, we utilize dynamic knowledge routing to identify knowledge paths in FFNs and selectively update parameters during pre-training. Experimental results show that TRELM achieves at least a 50% reduction in pre-training time and outperforms other KEPLMs in knowledge probing tasks and multiple knowledge-aware language understanding tasks.",
    "authors": [
        "Junbing Yan",
        "Chengyu Wang",
        "Taolin Zhang",
        "Xiao-Mei He",
        "Junyuan Huang",
        "Longtao Huang",
        "Hui Xue",
        "Wei Zhang"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models that employs a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}