{
    "acronym": "bc9bfd4035ef4fcf128accb48b8ebe6a363f7e2f",
    "title": "An Embarrassingly Simple Approach to Enhance Transformer Performance in Genomic Selection for Crop Breeding",
    "seed_ids": [
        "transformer",
        "0f4780f3f42dbe9755d54495ae17244cc88a7483",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "35a9749df07a2ab97c51af4d260b095b00da7676"
    ],
    "s2id": "bc9bfd4035ef4fcf128accb48b8ebe6a363f7e2f",
    "abstract": "Genomic selection (GS), as a critical crop breeding strategy, plays a key role in enhancing food production and addressing the global hunger crisis. The predominant approaches in GS currently revolve around employing statistical methods for prediction. However, statistical methods often come with two main limitations: strong statistical priors and linear assumptions. A recent trend is to capture the non-linear relationships between markers by deep learning. However, as crop datasets are commonly long sequences with limited samples, the robustness of deep learning models, especially Transformers, remains a challenge. In this work, to unleash the unexplored potential of attention mechanism for the task of interest, we propose a simple yet effective Transformer-based framework that enables end-to-end training of the whole sequence. Via experiments on rice3k and wheat3k datasets, we show that, with simple tricks such as k-mer tokenization and random masking, Transformer can achieve overall superior performance against seminal methods on GS tasks of interest.",
    "authors": [
        "Renqi Chen",
        "Wenwei Han",
        "Haohao Zhang",
        "Haoyang Su",
        "Zhefan Wang",
        "Xiaolei Liu",
        "Hao Jiang",
        "Wanli Ouyang",
        "Nanqing Dong"
    ],
    "venue": "Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "To unleash the unexplored potential of attention mechanism for the task of interest, a simple yet effective Transformer-based framework that enables end-to-end training of the whole sequence is proposed that achieves overall superior performance against seminal methods on GS tasks of interest."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}