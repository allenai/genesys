{
    "acronym": "3fa956ed52c7038e099223429900e9ca5baeab21",
    "title": "Impact of Large Language Models on Generating Software Specifications",
    "seed_ids": [
        "gpt2",
        "d3de0bac5703825796c240bfab8dc3c8e0a90222",
        "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a",
        "c2329c685f11efa25c562f97be71ff03103423fd",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "3fa956ed52c7038e099223429900e9ca5baeab21",
    "abstract": "Software specifications are essential for ensuring the reliability of software systems. Existing specification extraction approaches, however, suffer from limited generalizability and require manual efforts. The recent emergence of Large Language Models (LLMs), which have been successfully applied to numerous software engineering tasks, offers a promising avenue for automating this process. In this paper, we conduct the first empirical study to evaluate the capabilities of LLMs for generating software specifications from software comments or documentation. We evaluate LLMs' performance with Few Shot Learning (FSL), enabling LLMs to generalize from a small number of examples, as well as different prompt construction strategies, and compare the performance of LLMs with traditional approaches. Additionally, we conduct a comparative diagnosis of the failure cases from both LLMs and traditional methods, identifying their unique strengths and weaknesses. Lastly, we conduct extensive experiments on 15 state of the art LLMs, evaluating their performance and cost effectiveness for generating software specifications. Our results show that with FSL, LLMs outperform traditional methods (by 5.6%), and more sophisticated prompt construction strategies can further enlarge this performance gap (up to 5.1 to 10.0%). Yet, LLMs suffer from their unique challenges, such as ineffective prompts and the lack of domain knowledge, which together account for 53 to 60% of LLM unique failures. The strong performance of open source models (e.g., StarCoder) makes closed source models (e.g., GPT 3 Davinci) less desirable due to size and cost. Our study offers valuable insights for future research to improve specification generation.",
    "authors": [
        "Danning Xie",
        "B. Yoo",
        "Nan Jiang",
        "Mijung Kim",
        "Lin Tan",
        "X. Zhang",
        "Judy S. Lee"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper conducts the first empirical study to evaluate the capabilities of Large Language Models for generating software specifications from software comments or documentation with Few Shot Learning (FSL), and shows that with FSL, LLMs outperform traditional methods, and more sophisticated prompt construction strategies can further enlarge this performance gap."
    },
    "citationCount": 12,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}