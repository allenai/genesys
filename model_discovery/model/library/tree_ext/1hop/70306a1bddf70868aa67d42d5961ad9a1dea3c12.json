{
    "acronym": "70306a1bddf70868aa67d42d5961ad9a1dea3c12",
    "title": "Better Datastore, Better Translation: Generating Datastores from Pre-Trained Models for Nearest Neural Machine Translation",
    "seed_ids": [
        "gpt2",
        "46c585ee9abf76779ea4b863d2da4358efd0d1d3",
        "75352cc69a29bd5fc411e0e79737cb96b6309161",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7a09101ac03b74db501648597fa54e992a0fc84f",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "70306a1bddf70868aa67d42d5961ad9a1dea3c12",
    "abstract": "Nearest Neighbor Machine Translation (kNNMT) is a simple and effective method of augmenting neural machine translation (NMT) with a token-level nearest neighbor retrieval mechanism. The effectiveness of kNNMT directly depends on the quality of retrieved neighbors. However, original kNNMT builds datastores based on representations from NMT models, which would result in poor retrieval accuracy when NMT models are not good enough, leading to sub-optimal translation performance. In this paper, we propose PRED, a framework that leverages Pre-trained models for Datastores in kNN-MT. Better representations from pre-trained models allow us to build datastores of better quality. We also design a novel contrastive alignment objective to mitigate the representation gap between the NMT model and pre-trained models, enabling the NMT model to retrieve from better datastores. We conduct extensive experiments on both bilingual and multilingual translation benchmarks, including WMT17 English $\\leftrightarrow$ Chinese, WMT14 English $\\leftrightarrow$ German, IWSLT14 German $\\leftrightarrow$ English, and IWSLT14 multilingual datasets. Empirical results demonstrate the effectiveness of PRED.",
    "authors": [
        "Jiahuan Li",
        "Shanbo Cheng",
        "Zewei Sun",
        "Mingxuan Wang",
        "Shujian Huang"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes PRED, a framework that leverages Pre-trained models for Datastores in kNN-MT to build datastores of better quality, and designs a novel contrastive alignment objective to mitigate the representation gap between the NMT model and pre- trained models, enabling the N MT model to retrieve from better datastore."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}