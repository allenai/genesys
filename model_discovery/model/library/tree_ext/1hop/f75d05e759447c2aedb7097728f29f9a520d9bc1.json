{
    "acronym": "f75d05e759447c2aedb7097728f29f9a520d9bc1",
    "title": "Do Long-Range Language Models Actually Use Long-Range Context?",
    "seed_ids": [
        "routingtransformer",
        "compressivetransformer",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "3df83a60f55c64b40e6dbcd99cf9f67894a0736e",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "f6390beca54411b06f3bde424fb983a451789733",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "f75d05e759447c2aedb7097728f29f9a520d9bc1",
    "abstract": "Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).",
    "authors": [
        "Simeng Sun",
        "Kalpesh Krishna",
        "Andrew Mattarella-Micke",
        "Mohit Iyyer"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper performs a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens and discovers that long-ranging context helps most for literary novels."
    },
    "citationCount": 64,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}