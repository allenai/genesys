{
    "acronym": "28f59093730d88719b96041f9544c73671f798bd",
    "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
    "seed_ids": [
        "ring",
        "6dc5c6190dfbe55c8b45b7b23800614c21e5b51c",
        "ade22704be8a0fc3730d320cc7934b2ccbcd97e4",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "28f59093730d88719b96041f9544c73671f798bd",
    "abstract": "Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 47% MFU on two 8xA800 nodes using SP for the LLAMA3-8B model training using sequence length 208K. Our code is publicly available at https://github.com/feifeibear/long-context-attention.",
    "authors": [
        "Jiarui Fang",
        "Shangchun Zhao"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}