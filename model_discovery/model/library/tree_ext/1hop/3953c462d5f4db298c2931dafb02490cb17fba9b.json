{
    "acronym": "3953c462d5f4db298c2931dafb02490cb17fba9b",
    "title": "Fine-Tuning Transformer-Based Representations in Active Learning for Labelling Crisis Dataset of Tweets",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "3953c462d5f4db298c2931dafb02490cb17fba9b",
    "abstract": null,
    "authors": [
        "Nayan Ranjan Paul",
        "R. Balabantaray",
        "Deepak Sahoo"
    ],
    "venue": "SN Computer Science",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Transformer-based models, particularly BERT-like models, which have yet to be widely used in active learning, outperform more regularly used vector representations such as Bag-of-Words or other traditional word-embeddings such as GloVe."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}