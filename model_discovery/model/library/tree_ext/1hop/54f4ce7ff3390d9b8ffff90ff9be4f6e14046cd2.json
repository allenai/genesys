{
    "acronym": "54f4ce7ff3390d9b8ffff90ff9be4f6e14046cd2",
    "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks",
    "seed_ids": [
        "roformer",
        "f84f0f957f6c393065003de03c1e877e4890d396",
        "1c7db9fb18246787fbe3de6e0eaa370ae749e795",
        "ca1952a54cf85f0f449cb337f70b083100069c9c",
        "24354722e36c358b69893ab05220d6f2291989d1",
        "b085968c4362fb286ad6c5ef71a5db9630da0498",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4"
    ],
    "s2id": "54f4ce7ff3390d9b8ffff90ff9be4f6e14046cd2",
    "abstract": "How to efficiently serve Large Language Models (LLMs) has become a pressing issue because of their huge computational cost in their autoregressive generation process. To mitigate computational costs, LLMs often employ the KV Cache technique to improve the generation speed. While improving the computational efficiency, the storage requirements of the KV cache are substantial, particularly in long-context scenarios, leading to significant memory consumption. Existing KV cache eviction methods often degrade the performance of LLMs in long-context scenarios due to the information loss introduced by eviction. In this paper, we propose a novel KV cache merging approach, called KVMerger, to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets. Our approach is inspired by the intriguing observation that key states exhibit high similarity at the token level within a single sequence. To facilitate merging, we develop an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging. Our merging set identification algorithm stimulates the second observation that KV cache sparsity, from similarity perspective, is independent of the dataset and remains persistent at the model level. Subsequently, we propose a Gaussian kernel weighted merging algorithm to selectively merge all states within each merging set. We conduct extensive experiments to demonstrate the effectiveness of KVMerger for long-context tasks under constrained memory budgets, applying it to models including Llama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll benchmarks, we compare our method with other KV cache compression techniques, including H2O and CaM, showing that our method achieves superior performance across tasks with both 50% and 35% KV cache budgets.",
    "authors": [
        "Zheng Wang",
        "Boxiao Jin",
        "Zhongzhi Yu",
        "Minjia Zhang"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a novel KV cache merging approach, called KVMerger, to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets, and develops an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}