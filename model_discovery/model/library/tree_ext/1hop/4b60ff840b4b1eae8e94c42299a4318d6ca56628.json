{
    "acronym": "4b60ff840b4b1eae8e94c42299a4318d6ca56628",
    "title": "Hybrid Attention-based Transformer for Long-range Document Classification",
    "seed_ids": [
        "nystromformer",
        "performer",
        "bigbird",
        "longformer",
        "reformer",
        "routingtransformer",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "1e3e65e7773b7869d9bd7f5394b54199e48195e6",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "0cd82dfae930ac4b57c0e959f744f2d10bf87649",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "6954a6bb9d6f3e365b26b694c963ae1d62a03444"
    ],
    "s2id": "4b60ff840b4b1eae8e94c42299a4318d6ca56628",
    "abstract": "Transformer with the self-attention mechanism, which allows fully-connected contextual encoding over input tokens, has achieved outstanding performances in various NLP tasks, but it suffers from quadratic complexity with the input sequence length. Long-range contexts are often tackled by Transformer in chunks using a sliding window to avoid GPU memory overflow. However, how to achieve considerable performance on downstream tasks under the premise of modeling sequences as long as possible with limited GPU resources is still a problem to be explored. To address this issue, we propose a new framework using hybrid attention-based Transformer to capture long-range contextual features. More specifically, we make a combination of three types of attention, i.e. sliding window local attention, clustering-based long-range attention and specific global attention. Experiments comparing the performance of our model with mainstream efficient improved Transformers are conducted on document classification task in public datasets IMDB and CAIL-Long. Experimental results demonstrate that the proposed approach can outperform the state-of-the-art models on the adopted datasets, which verifies the effectiveness of our model on long-range document classification task.",
    "authors": [
        "Ruyu Qin",
        "Min Huang",
        "Jiawei Liu",
        "Q. Miao"
    ],
    "venue": "IEEE International Joint Conference on Neural Network",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a new framework using hybrid attention-based Transformer to capture long-range contextual features and demonstrates that the proposed approach can outperform the state-of-the-art models on the adopted datasets, which verifies the effectiveness of the model on long- range document classification task."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}