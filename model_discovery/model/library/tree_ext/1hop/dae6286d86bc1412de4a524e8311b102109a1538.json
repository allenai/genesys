{
    "acronym": "dae6286d86bc1412de4a524e8311b102109a1538",
    "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
    "seed_ids": [
        "infinibench",
        "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
        "cde48b24264e44355abb0e548a2cf7c70bb072b4",
        "d8b51d518f2dd62943762ceaa8961d3b1bfbcc1a",
        "e22ae34ea102a781d0494e115639e8d081bf6920",
        "539fadfb615ef84c240f4741061c44eeda540091",
        "2b35b946a8ad64e018c24b283bc1c6c65d36fb67",
        "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
        "b6346f9fa093b8e85df712485a2b851b9f680dac",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4"
    ],
    "s2id": "dae6286d86bc1412de4a524e8311b102109a1538",
    "abstract": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge the gap between open-access LLMs and leading proprietary models (e.g., GPT-4-Turbo) in long-context understanding and retrieval-augmented generation (RAG) capabilities. These two capabilities are essential for LLMs to process large volumes of information that cannot fit into a single prompt and are complementary to each other, depending on the downstream tasks and computational budgets. We present a detailed continued training recipe to extend the context window of Llama3-70B-base from 8K to 128K tokens, along with a three-stage instruction tuning process to enhance the model's instruction-following, RAG performance, and long-context understanding capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model achieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context understanding tasks and surpasses it on the RAG benchmark. Interestingly, we find that the state-of-the-art long-context retriever can alleviate the top-k context fragmentation issue in RAG, further improving RAG-based results for long-context understanding tasks. We also provide extensive comparisons between RAG and long-context solutions using state-of-the-art long-context LLMs.",
    "authors": [
        "Peng Xu",
        "Wei Ping",
        "Xianchao Wu",
        "Zihan Liu",
        "M. Shoeybi",
        "Bryan Catanzaro"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "ChatQA 2, a Llama3-based model designed to bridge the gap between open-access LLMs and leading proprietary models in long-context understanding and retrieval-augmented generation (RAG) capabilities, is introduced."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}