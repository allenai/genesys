{
    "acronym": "c699624c3cdd5e2ac0d1f67de733bbad1b1c0e37",
    "title": "Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning",
    "seed_ids": [
        "gpt2",
        "83ac79bb8e8695fb3c3c024be74790d862adea74",
        "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "d84cf745c534c010b8e55e5a4a04878906848dc3",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "863171ed35ca0035074f73bb202b153cc346f2f3",
        "3aa2c10dd6c72267ea8a622c8f30b3c9240d5fab",
        "8064d3873c646dc9ff949d72c54c634a906fc092",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c699624c3cdd5e2ac0d1f67de733bbad1b1c0e37",
    "abstract": "In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional contrastive learning or mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations. aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis.",
    "authors": [
        "Yuxuan Bian",
        "Xu Ju",
        "Jiangtong Li",
        "Zhijian Xu",
        "Dawei Cheng",
        "Qiang Xu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An innovative framework that adapts Large Language Models for time-series representation learning with a distinctive element of the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}