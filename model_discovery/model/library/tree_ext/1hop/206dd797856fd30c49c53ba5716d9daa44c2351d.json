{
    "acronym": "206dd797856fd30c49c53ba5716d9daa44c2351d",
    "title": "EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm",
    "seed_ids": [
        "metaformer",
        "f35016b3180808fa97d59acbdecf47d6e2ed2819",
        "5eda60d4940d4185df45c5703e103458171d465d",
        "6c22336873706b1cf5205ac6bd2432aa69d97821",
        "ba637c4f1a170f1e2dadeadb71a63cf2b9a46de2",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "cec7872b194aadf54140578b9be52939eb1112e9",
        "6e8f35c6d54acb14109c9b792a62609eac8a7b5e",
        "94b69cf199fa0b6c842e17fe5d6174a9d161c3df",
        "6914a7997ff4be207fa7b3472a9c5879abaec646",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "206dd797856fd30c49c53ba5716d9daa44c2351d",
    "abstract": "Motivated by biological evolution, this paper explains the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derives that both have consistent mathematical formulation. Then inspired by effective EA variants, we propose a novel pyramid EATFormer backbone that only contains the proposed \\emph{EA-based Transformer} (EAT) block, which consists of three residual parts, i.e., \\emph{Multi-Scale Region Aggregation} (MSRA), \\emph{Global and Local Interaction} (GLI), and \\emph{Feed-Forward Network} (FFN) modules, to model multi-scale, interactive, and individual information separately. Moreover, we design a \\emph{Task-Related Head} (TRH) docked with transformer backbone to complete final information fusion more flexibly and \\emph{improve} a \\emph{Modulated Deformable MSA} (MD-MSA) to dynamically model irregular locations. Massive quantitative and quantitative experiments on image classification, downstream tasks, and explanatory experiments demonstrate the effectiveness and superiority of our approach over State-Of-The-Art (SOTA) methods. \\Eg, our Mobile (1.8M), Tiny (6.1M), Small (24.3M), and Base (49.0M) models achieve 69.4, 78.4, 83.1, and 83.9 Top-1 only trained on ImageNet-1K with naive training recipe; EATFormer-Tiny/Small/Base armed Mask-R-CNN obtain 45.4/47.4/49.0 box AP and 41.4/42.9/44.2 mask AP on COCO detection, surpassing contemporary MPViT-T, Swin-T, and Swin-S by 0.6/1.4/0.5 box AP and 0.4/1.3/0.9 mask AP separately with less FLOPs; Our EATFormer-Small/Base achieve 47.3/49.3 mIoU on ADE20K by Upernet that exceeds Swin-T/S by 2.8/1.7. Code is available at \\url{https://github.com/zhangzjn/EATFormer}.",
    "authors": [
        "Jiangning Zhang",
        "Xiangtai Li",
        "Yabiao Wang",
        "Chengjie Wang",
        "Yibo Yang",
        "Yong Liu",
        "Dacheng Tao"
    ],
    "venue": "International Journal of Computer Vision",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel pyramid EATFormer backbone that only contains the proposed EA-based Transformer (EAT) block is proposed, which consists of three residual parts, i.e., multi-Scale Region Aggregation, Global and Local Interaction, and Feed-Forward Network modules, to model multi-scale, interactive, and individual information separately."
    },
    "citationCount": 24,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}