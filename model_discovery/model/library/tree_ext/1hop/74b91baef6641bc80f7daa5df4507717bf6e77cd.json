{
    "acronym": "74b91baef6641bc80f7daa5df4507717bf6e77cd",
    "title": "Long-Range Transformer Architectures for Document Understanding",
    "seed_ids": [
        "cosformer",
        "linformer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "74b91baef6641bc80f7daa5df4507717bf6e77cd",
    "abstract": null,
    "authors": [
        "Thibault Douzon",
        "S. Duffner",
        "Christophe Garcia",
        "J\u00e9r\u00e9my Espinas"
    ],
    "venue": "ICDAR Workshops",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces 2 new multi-modal (text + layout) long-range models for DU based on efficient implementations of Transformers for long sequences, and proposes 2D relative attention bias to guide self-attention towards relevant tokens without harming model efficiency."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}