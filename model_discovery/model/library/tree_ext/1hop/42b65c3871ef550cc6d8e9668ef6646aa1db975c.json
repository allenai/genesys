{
    "acronym": "42b65c3871ef550cc6d8e9668ef6646aa1db975c",
    "title": "An Optimized Data\ufb02ow for Mitigating Attention Performance Bottlenecks",
    "seed_ids": [
        "performer",
        "reformer",
        "transformerxl",
        "compressivetransformer",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "eb0931c39904a40c6cb4aa35c9b21d5e3b7dc856",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "069e0d896da7c79faeee4cf057548d5da7ce885e",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "83b8108014e3db4f46354a28ae68193f143c4e7e",
        "540f074cb6f16563a357741837e41c44c0a38234",
        "f6390beca54411b06f3bde424fb983a451789733",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "42b65c3871ef550cc6d8e9668ef6646aa1db975c",
    "abstract": "Attention mechanisms form the backbone of state-of-the-art machine learning models for a variety of tasks. Deploying them on deep neural network (DNN) accelerators, however, is prohibitively challenging especially under long sequences, as this work identi\ufb01es. This is due to operators in attention layers exhibiting limited reuse opportunities and quadratic growth in memory footprint, leading to severe memory-boundedness. To address this, we introduce a new attention-tailored data\ufb02ow, termed FLAT , which identi\ufb01es fusion opportunities within the attention layer, and implements an on-chip memory-aware interleaved execution and tiling mechanism. FLAT increases the effective memory bandwidth by ef\ufb01ciently utilizing the high-bandwidth, low-capacity on-chip buffer and thus achieves better run time and compute resource utilization. In our evaluation, FLAT achieves 1.94 x and 1.76 x speedup and 49% and 42% of energy reduction comparing to baseline execution over state-of-the-art edge and cloud accelerators.",
    "authors": [
        "Sheng-Chun Kao",
        "Suvinay Subramanian",
        "Gaurav Agrawal",
        "T. Krishna"
    ],
    "venue": "",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new attention-tailored data architecture, termed FLAT, is introduced, which identifies fusion opportunities within the attention layer, and implements an on-chip memory-aware interleaved execution and tiling mechanism, and increases the effective memory bandwidth by utilizing the high-bandwidth, low-capacity on-chip buffer."
    },
    "citationCount": 11,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}