{
    "acronym": "b04493c0865c166d00f65bb78f6476b466bbe512",
    "title": "American == White in Multimodal Language-and-Image AI",
    "seed_ids": [
        "gpt2",
        "ecb1d0a65e3f7467dc90a43f32b17f4940a98613",
        "3cfea2a4291ea4e5e0061d8a626e414e27ec5ac5",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "5dd7bc394e032eb0e982699a5f0c781fab9e3111",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "7ea0e91c5d5dc73f2133bc46d7ebb6cb83034dae",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b04493c0865c166d00f65bb78f6476b466bbe512",
    "abstract": "Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals, with effect sizes >.4 for White vs. Asian comparisons across all models. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Additional tests reveal that the number of images of Black individuals returned by an image ranking task is more strongly correlated with state-level implicit bias scores for White individuals (Pearson's \u03c1=.63 in CLIP, \u03c1=.69 in BLIP) than are state demographics (\u03c1=.60), suggesting a relationship between regional prototypicality and implicit bias. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, and the race of Black individuals as much as 18% of the time, but never remarks upon race for White individuals. Finally, when provided with an initialization image of individuals from the CFD and the text \"an American person,\" a synthetic image generator (VQGAN) using the text-based guidance of CLIP consistently lightens the skin tone of individuals of all races (by 35% for Black individuals, based on mean pixel brightness), and generates output images of White individuals with blonde hair. The results indicate that societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models.",
    "authors": [
        "R. Wolfe",
        "Aylin Caliskan"
    ],
    "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models."
    },
    "citationCount": 31,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}