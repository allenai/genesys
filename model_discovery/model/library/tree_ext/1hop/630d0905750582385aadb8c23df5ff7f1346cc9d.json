{
    "acronym": "630d0905750582385aadb8c23df5ff7f1346cc9d",
    "title": "LLM4SGG: Large Language Models for Weakly Supervised Scene Graph Generation",
    "seed_ids": [
        "gpt3",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "630d0905750582385aadb8c23df5ff7f1346cc9d",
    "abstract": "Weakly-Supervised Scene Graph Generation (WSSGG) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. In this regard, studies on WSSGG have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. However, they have overlooked the two issues involved in the triplet formation process from the captions: 1) Semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) Low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. To tackle the two issues, we propose a new approach, i.e., Large Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two issues by leveraging the LLM's in-depth understanding of language and reasoning ability during the extraction of triplets from captions and alignment of entity/predicate classes with target data. To further engage the LLM in these processes, we adopt the idea of Chain-of-Thought and the in-context few-shot learning strategy. To validate the effectiveness of LLM4SGG, we conduct extensive experiments on Visual Genome and GQA datasets, showing significant improvements in both Recall@K and mean Recall@K compared to the state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is data-efficient, enabling effective model training with a small amount of training images.",
    "authors": [
        "Kibum Kim",
        "Kanghoon Yoon",
        "Jaeyeong Jeon",
        "Yeonjun In",
        "Jinyoung Moon",
        "Donghyun Kim",
        "Chanyoung Park"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a new approach, i.e., Large Language Model for weakly-supervised SGG (LLM4SGG), where the LLM's in-depth understanding of language and reasoning ability is leveraged during the extraction of triplets from captions and alignment of entity/predicate classes with target data."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}