{
    "acronym": "21d2f55c247f83f3290ffa5311081005458cf8fe",
    "title": "Towards Lightweight Transformer Via Group-Wise Transformation for Vision-and-Language Tasks",
    "seed_ids": [
        "performer",
        "linformer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "62dc8ddb4907db4b889c5e93673d9b3c189d1f25",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "21d2f55c247f83f3290ffa5311081005458cf8fe",
    "abstract": "Despite the exciting performance, Transformer is criticized for its excessive parameters and computation cost. However, compressing Transformer remains as an open problem due to its internal complexity of the layer designs, i.e., Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this issue, we introduce Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both the parameters and computations of Transformer, while also preserving its two main properties, i.e., the efficient attention modeling on diverse subspaces of MHA, and the expanding-scaling feature transformation of FFN. We apply LW-Transformer to a set of Transformer-based networks, and quantitatively measure them on three vision-and-language tasks and six benchmark datasets. Experimental results show that while saving a large number of parameters and computations, LW-Transformer achieves very competitive performance against the original Transformer networks for vision-and-language tasks. To examine the generalization ability, we apply LW-Transformer to the task of image classification, and build its network based on a recently proposed image Transformer called Swin-Transformer, where the effectiveness can be also confirmed.",
    "authors": [
        "Gen Luo",
        "Yiyi Zhou",
        "Xiaoshuai Sun",
        "Yan Wang",
        "Liujuan Cao",
        "Yongjian Wu",
        "Feiyue Huang",
        "Rongrong Ji"
    ],
    "venue": "IEEE Transactions on Image Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "WL-Transformer applies Group-wise Transformation to reduce both the parameters and computations of Transformer, while also preserving its two main properties, i.e., the efficient attention modeling on diverse subspaces of MHA, and the expanding-scaling feature transformation of FFN."
    },
    "citationCount": 33,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}