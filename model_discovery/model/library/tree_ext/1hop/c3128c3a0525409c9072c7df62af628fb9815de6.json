{
    "acronym": "c3128c3a0525409c9072c7df62af628fb9815de6",
    "title": "Self-supervised learning based on Transformer for flow reconstruction and prediction",
    "seed_ids": [
        "roformer",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "72f207c777e4a17180cc54ccc6a743d5f43227af",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4"
    ],
    "s2id": "c3128c3a0525409c9072c7df62af628fb9815de6",
    "abstract": "Machine learning has great potential for efficient reconstruction and prediction of flow fields. However, existing datasets may have highly diversified labels for different flow scenarios, which are not applicable for training a model. To this end, we make a first attempt to apply the self-supervised learning (SSL) technique to fluid dynamics, which disregards data labels for pre-training the model. The SSL technique embraces a large amount of data (8000 snapshots) at Reynolds numbers of Re\u2009=\u2009200, 300, 400, and 500 without discriminating between them, which improves the generalization of the model. The Transformer model is pre-trained via a specially designed pretext task, where it reconstructs the complete flow fields after randomly masking 20% data points in each snapshot. For the downstream task of flow reconstruction, the pre-trained model is fine-tuned separately with 256 snapshots for each Reynolds number. The fine-tuned models accurately reconstruct the complete flow fields based on less than 5% random data points within a limited window even for Re\u2009=\u2009250 and 600, whose data were not seen in the pre-trained phase. For the other downstream task of flow prediction, the pre-training model is fine-tuned separately with 128 consecutive snapshot pairs for each corresponding Reynolds number. The fine-tuned models then correctly predict the evolution of the flow fields over many periods of cycles. We compare all results generated by models trained via SSL and models trained via supervised learning, where the former has unequivocally superior performance. We expect that the methodology presented here will have wider applications in fluid mechanics.",
    "authors": [
        "Bonan Xu",
        "Yuanye Zhou",
        "Xin Bian"
    ],
    "venue": "The Physics of Fluids",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This is a first attempt to apply the self-supervised learning (SSL) technique to fluid dynamics, which disregards data labels for pre-training the model, and compares all results generated by models trained via SSL and models trained via supervised learning, where the former has unequivocally superior performance."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}