{
    "acronym": "d1b3e2d9b8101b729e825edb8fc9210933303eb3",
    "title": "Masked Generative Story Transformer with Character Guidance and Caption Augmentation",
    "seed_ids": [
        "transformer",
        "e49cb2ab3a7990e3d05042197ae8b3fd934453de",
        "40e4ddee06e38093b4e2b86ea0c886bf3bb393d0",
        "ee1fdc0e4391f2ef7d3222d47d8677c574fcd95c",
        "a6238191cc43c41a544caadcf8b00947293cf099",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1"
    ],
    "s2id": "d1b3e2d9b8101b729e825edb8fc9210933303eb3",
    "abstract": "Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our approach. The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV benchmark (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts. The validity of our quantitative results is supported by a human survey.",
    "authors": [
        "Christos Papadimitriou",
        "Giorgos Filandrianos",
        "Maria Lymperaiou",
        "G. Stamou"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work embraces a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency, and proposes a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}