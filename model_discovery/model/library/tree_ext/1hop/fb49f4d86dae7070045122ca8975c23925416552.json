{
    "acronym": "fb49f4d86dae7070045122ca8975c23925416552",
    "title": "TraveLER: A Multi-LMM Agent Framework for Video Question-Answering",
    "seed_ids": [
        "gpt3",
        "0938d0ccc1c633fa0f8c067d914358b1ef53a44b"
    ],
    "s2id": "fb49f4d86dae7070045122ca8975c23925416552",
    "abstract": "Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to\"Traverse\"through the video, ask questions about individual frames to\"Locate\"and store key information, and then\"Evaluate\"if there is enough information to answer the question. Finally, if there is not enough information, our method is able to\"Replan\"based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several video question-answering benchmarks, such as NExT-QA, STAR, and Perception Test, without the need to fine-tune on specific datasets.",
    "authors": [
        "Chuyi Shang",
        "Amos You",
        "Sanjay Subramanian",
        "Trevor Darrell",
        "Roei Herzig"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The proposed TraveLER approach improves performance on several video question-answering benchmarks, such as NExT-QA, STAR, and Perception Test, without the need to fine-tune on specific datasets."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}