{
    "acronym": "eea5d85160b256c20d314c2bb34a831c6953facc",
    "title": "From System Models to Class Models: An In-Context Learning Paradigm",
    "seed_ids": [
        "gpt2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "eea5d85160b256c20d314c2bb34a831c6953facc",
    "abstract": "Is it possible to understand the intricacies of a dynamical system not solely from its input/output pattern, but also by observing the behavior of other systems within the same class? This central question drives the study presented in this letter. In response to this query, we introduce a novel paradigm for system identification, addressing two primary tasks: one-step-ahead prediction and multi-step simulation. Unlike conventional methods, we do not directly estimate a model for the specific system. Instead, we learn a meta model that represents a class of dynamical systems. This meta model is trained on a potentially infinite stream of synthetic data, generated by simulators whose settings are randomly extracted from a probability distribution. When provided with a context from a new system\u2013specifically, an input/output sequence\u2013the meta model implicitly discerns its dynamics, enabling predictions of its behavior. The proposed approach harnesses the power of Transformers, renowned for their in-context learning capabilities. For one-step prediction, a GPT-like decoder-only architecture is utilized, whereas the simulation problem employs an encoder-decoder structure. Initial experimental results affirmatively answer our foundational question, opening doors to fresh research avenues in system identification.",
    "authors": [
        "Marco Forgione",
        "F. Pura",
        "Dario Piga"
    ],
    "venue": "IEEE Control Systems Letters",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel paradigm for system identification, addressing two primary tasks: one-step-ahead prediction and multi-step simulation, and harnesses the power of Transformers, renowned for their in-context learning capabilities."
    },
    "citationCount": 3,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}