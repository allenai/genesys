{
    "acronym": "9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73",
    "title": "GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures",
    "seed_ids": [
        "transformerxl",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "2a218786f4615b82389f78472e7ff22e6ce57490",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "841d43cf4015042a4ee45745c5b6f2c59c184da5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73",
    "abstract": "Attention based language models have become a critical component in state-of-the-art natural language processing systems. However, these models have significant computational requirements, due to long training times, dense operations and large parameter count. In this work we demonstrate a set of modifications to the structure of a Transformer layer, producing a more efficient architecture. First, we add a convolutional module to complement the self-attention module, decoupling the learning of local and global interactions. Secondly, we rely on grouped transformations to reduce the computational cost of dense feed-forward layers and convolutions, while preserving the expressivity of the model. We apply the resulting architecture to language representation learning and demonstrate its superior performance compared to BERT models of different scales. We further highlight its improved efficiency, both in terms of floating-point operations (FLOPs) and time-to-train.",
    "authors": [
        "Ivan Chelombiev",
        "Daniel Justus",
        "Douglas Orr",
        "A. Dietrich",
        "Frithjof Gressmann",
        "A. Koliousis",
        "C. Luschi"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work demonstrates a set of modifications to the structure of a Transformer layer, producing a more efficient architecture, and applies the resulting architecture to language representation learning and demonstrates its superior performance compared to BERT models of different scales."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}