{
    "acronym": "93055c24067326fa439a2e3ec8dd8ec0e49c0f99",
    "title": "Transformer-Based Language Models for Software Vulnerability Detection: Performance, Model's Security and Platforms",
    "seed_ids": [
        "gpt",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "93055c24067326fa439a2e3ec8dd8ec0e49c0f99",
    "abstract": "The large transformer-based language models demonstrate excellent performance in natural language processing. By considering the closeness of natural languages to the high-level programming language such as C/C++, this work studies how good are the large transformer-based language models detecting software vulnerabilities. Our results demonstrate the well performance of these models on software vulnerability detection. The answer enables extending transformer-based language models to vulnerability detection and leveraging superior performance beyond the natural language processing domain. Besides, we perform the model\u2019s security check using Microsoft\u2019s Counterfit, a command-line tool to assess the model\u2019s security. Our results find that these models are vulnerable to adversarial examples. In this regard, we present a simple countermeasure and its result. Experimenting with large models is always a challenge due to the requirement of computing resources and platforms/libraries & dependencies. Based on the experiences and difficulties we faced during this work, we present our recommendation while choosing the platforms to run these large models. Moreover, the popular platforms are surveyed thoroughly in this paper.",
    "authors": [
        "Chandra Thapa",
        "Seung Ick Jang",
        "Muhammad Ejaz Ahmed",
        "S. \u00c7amtepe",
        "Josef Pieprzyk",
        "Surya Nepal"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The results demonstrate the well performance of these models on software vulnerability detection, which enables extending transformer-based language models to vulnerability detection and leveraging superior performance beyond the natural language processing domain."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}