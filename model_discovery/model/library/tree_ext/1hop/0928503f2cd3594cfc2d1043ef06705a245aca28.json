{
    "acronym": "0928503f2cd3594cfc2d1043ef06705a245aca28",
    "title": "Difformer: Empowering Diffusion Models on the Embedding Space for Text Generation",
    "seed_ids": [
        "diffuseq",
        "a1186d7d9a9ef258c76afef1177e4f348061a537",
        "bb7e779c9360a94dd2779c2468fe06b82de7af59",
        "22775e58932cdfbd273a2a835a22c5d86800a458",
        "2c6ac935c826002976722ca8d3319f691975687e",
        "0b9770a377b3f96cef9f268cee1791d39a0d4893",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "0928503f2cd3594cfc2d1043ef06705a245aca28",
    "abstract": "Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies and analyze the challenges between the continuous data space and the embedding space which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the loss function. Secondly, as the norm of embeddings varies between popular and rare words, adding the same noise scale will lead to sub-optimal results. In addition, we \ufb01nd the normal level of noise causes insuf\ufb01cient training of the model. To address the above challenges, we propose Difformer, an embedding diffusion model based on Transformer, which consists of three essential modules including an additional anchor loss function, a layer normalization module for embeddings, and a noise factor to the Gaussian noise. Experiments on two seminal text generation tasks including machine translation and text summarization show the superiority of Difformer over compared embedding diffusion baselines.",
    "authors": [
        "Zhujin Gao",
        "Junliang Guo",
        "Xu Tan",
        "Yongxin Zhu",
        "Fang Zhang",
        "Jiang Bian",
        "Linli Xu"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Difformer is proposed, an embedding diffusion model based on Transformer, which consists of three essential modules including an additional anchor loss function, a layer normalization module for embeddings, and a noise factor to the Gaussian noise."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}