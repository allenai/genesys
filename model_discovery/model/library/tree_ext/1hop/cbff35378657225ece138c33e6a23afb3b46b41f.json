{
    "acronym": "cbff35378657225ece138c33e6a23afb3b46b41f",
    "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
    "seed_ids": [
        "longformer",
        "b97c3c370401dc34d2adbeb24f34de5180a14be6",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "cbff35378657225ece138c33e6a23afb3b46b41f",
    "abstract": "The attention mechanisms of transformers effectively extract pertinent information from the input sequence. However, the quadratic complexity of self-attention w.r.t the sequence length incurs heavy computational and memory burdens, especially for tasks with long sequences. Existing accelerators face performance degradation in these tasks. To this end, we propose SALO to enable hybrid sparse attention mechanisms for long sequences. SALO contains a data scheduler to map hybrid sparse attention patterns onto hardware and a spatial accelerator to perform the efficient attention computation. We show that SALO achieves 17.66x and 89.33x speedup on average compared to GPU and CPU implementations, respectively, on typical workloads, i.e., Longformer and ViL.",
    "authors": [
        "Guan Shen",
        "Jieru Zhao",
        "Quan Chen",
        "Jingwen Leng",
        "C. Li",
        "Minyi Guo"
    ],
    "venue": "Design Automation Conference",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SALO contains a data scheduler to map hybrid sparse attention patterns onto hardware and a spatial accelerator to perform the efficient attention computation and shows that SALO achieves 17.66x and 89.33x speedup on average compared to GPU and CPU implementations, respectively, on typical workloads."
    },
    "citationCount": 14,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}