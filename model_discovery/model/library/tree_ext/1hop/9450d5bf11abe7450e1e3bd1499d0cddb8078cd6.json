{
    "acronym": "9450d5bf11abe7450e1e3bd1499d0cddb8078cd6",
    "title": "Efficient Two-stage Approach for Long Document Summarization",
    "seed_ids": [
        "longformer",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "9450d5bf11abe7450e1e3bd1499d0cddb8078cd6",
    "abstract": "Long document summarization is essential in NLP, but state-of-the-art models like BART and BERT face limitations summarizing long documents. Current methods like Longformer Encoder Decoder (LED) can handle longer document summarization but suffers from slower processing times due to their complex attention mechanisms. To address this challenge, we propose a two-stage approach that combines sentence extraction algorithms with BART for generating abstractive summaries. Our approach leverages the efficiency of extraction algorithms to identify key sentences from the input document. BART then generates more coherent and informative abstractive summaries from these extracted sentences. Our experimental results show that our approach is four times more time-efficient than the LED baseline while processing the same amount of data, with approximately the same performance in terms of Rouge 1,2, and L F-measure. With the hope of further improving the generated summaries, we also use Generative Adversarial Network to train the model. Our proposed approach has important implications for NLP applications that require summarization of long text, such as legal documents or scientific papers.",
    "authors": [
        "Stanford CS224N Custom",
        "Benson Zu",
        "Jialuo Yuan",
        "Fengming Tang",
        "Elaine Yi",
        "Ling Sui"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a two-stage approach that combines sentence extraction algorithms with BART for generating abstractive summaries and shows that this approach is four times more time-efficient than the LED baseline while processing the same amount of data."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}