{
    "acronym": "7861d04b41c9848905cb73268040d10e23409c77",
    "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
    "seed_ids": [
        "bert",
        "perceiverio",
        "412e266cddfd87c79087a88ba1e4d11b89a45a13",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "7861d04b41c9848905cb73268040d10e23409c77",
    "abstract": "Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.",
    "authors": [
        "Johnathan Xie",
        "Yoonho Lee",
        "Annie S. Chen",
        "Chelsea Finn"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method, is presented and is found to be capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}