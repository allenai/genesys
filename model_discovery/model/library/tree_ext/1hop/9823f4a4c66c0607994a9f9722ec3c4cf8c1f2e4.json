{
    "acronym": "9823f4a4c66c0607994a9f9722ec3c4cf8c1f2e4",
    "title": "MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting",
    "seed_ids": [
        "mamba",
        "02d3afd9c306ce0a0d39e211d60468732c591f8f",
        "5867382590f9f0ff8caf15804d20bde10845b2d2",
        "21ddc4fc3551619b8a64db6ae124acc72aaae2c2",
        "2dda6da7375bf5e8bcf60f87b17ba10757f3bc57",
        "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "95b2cb3f4a765014ce025afe5679660982554e6c",
        "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "a7d68b1702af08ce4dbbf2cd0b083e744ae5c6be",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9"
    ],
    "s2id": "9823f4a4c66c0607994a9f9722ec3c4cf8c1f2e4",
    "abstract": "In recent years, Transformers have become the de-facto architecture for long-term sequence forecasting (LTSF), but faces challenges such as quadratic complexity and permutation invariant bias. A recent model, Mamba, based on selective state space models (SSMs), has emerged as a competitive alternative to Transformer, offering comparable performance with higher throughput and linear complexity related to sequence length. In this study, we analyze the limitations of current Mamba in LTSF and propose four targeted improvements, leading to MambaTS. We first introduce variable scan along time to arrange the historical information of all the variables together. We suggest that causal convolution in Mamba is not necessary for LTSF and propose the Temporal Mamba Block (TMB). We further incorporate a dropout mechanism for selective parameters of TMB to mitigate model overfitting. Moreover, we tackle the issue of variable scan order sensitivity by introducing variable permutation training. We further propose variable-aware scan along time to dynamically discover variable relationships during training and decode the optimal variable scan order by solving the shortest path visiting all nodes problem during inference. Extensive experiments conducted on eight public datasets demonstrate that MambaTS achieves new state-of-the-art performance.",
    "authors": [
        "Xiuding Cai",
        "Yaoyao Zhu",
        "Xueyao Wang",
        "Yu Yao"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study analyzes the limitations of current Mamba in LTSF and proposes four targeted improvements, leading to MambaTS, and suggests that causal convolution in Mamba is not necessary for LTSF and proposes the Temporal Mamba Block (TMB)."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}