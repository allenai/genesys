{
    "acronym": "3dd5ad34012164c4ec9c571a12cc6a7561683dea",
    "title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
    "seed_ids": [
        "bert",
        "cd2277d7d5776972287b586b67e3d290af60d5d6",
        "7bcdfc0759561118cd79667379c5d174e2a747f2",
        "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51",
        "288e64e8adb23d81e291a2cb51e3a56b315023b7",
        "c74e1671cee1e19c49a5aafbedfe403af471b8ec",
        "c3382fd533b9dd7f8ed7ba7766159079bc1d3935",
        "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "0f4780f3f42dbe9755d54495ae17244cc88a7483",
        "a42fc49a300136d60aaebb668369010ee7746150",
        "2db3eb03b0a9fedde37066673532804d3e224a4a",
        "d55b69a533dea69c8b2673cde8de90c6626ee789",
        "44279244407a64431810f982be6d0c7da4429dd7",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "f30e95be411456a709e7cb9a8b3a3e557bd0356a",
        "b15469d0ab3dc3a9dec037d761817b3fe546bed6",
        "42f0bae2dacba44e9b5d8f050da3cbe41b9fc437",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "3dd5ad34012164c4ec9c571a12cc6a7561683dea",
    "abstract": "In many scientific fields, large language models (LLMs) have revolutionized the way with which text and other modalities of data (e.g., molecules and proteins) are dealt, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one to two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 250 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",
    "authors": [
        "Yu Zhang",
        "Xiusi Chen",
        "Bowen Jin",
        "Sheng Wang",
        "Shuiwang Ji",
        "Wei Wang",
        "Jiawei Han"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper comprehensively survey over 250 scientific LLMs, discusses their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality, and investigates how LLMs have been deployed to benefit scientific discovery."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}