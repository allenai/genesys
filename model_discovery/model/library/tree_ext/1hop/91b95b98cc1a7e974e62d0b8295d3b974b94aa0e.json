{
    "acronym": "91b95b98cc1a7e974e62d0b8295d3b974b94aa0e",
    "title": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training",
    "seed_ids": [
        "gpt2",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
        "0e3d6a7c9c04cf3ba9c902724548846a5ade04b4",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "91b95b98cc1a7e974e62d0b8295d3b974b94aa0e",
    "abstract": "Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies word-level KD to multiple PTs generated by both the teacher and the student.Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher. Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG.",
    "authors": [
        "Nitay Calderon",
        "Subhabrata Mukherjee",
        "Roi Reichart",
        "Amir Kantor"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work focuses on Knowledge Distillation techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student, and derives a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}