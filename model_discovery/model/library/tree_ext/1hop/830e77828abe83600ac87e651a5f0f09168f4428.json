{
    "acronym": "830e77828abe83600ac87e651a5f0f09168f4428",
    "title": "BERTer: The Efficient One",
    "seed_ids": [
        "bert"
    ],
    "s2id": "830e77828abe83600ac87e651a5f0f09168f4428",
    "abstract": "We explore advanced fine-tuning techniques to boost BERT's performance in sentiment analysis, paraphrase detection, and semantic textual similarity. Our approach leverages SMART regularization to combat overfitting, improves hyperparameter choices, employs a cross-embedding Siamese architecture for improved sentence embeddings, and introduces innovative early exiting methods. Our fine-tuning findings currently reveal substantial improvements in model efficiency and effectiveness when combining multiple fine-tuning architectures, achieving a state-of-the-art performance score of on the test set, surpassing current benchmarks and highlighting BERT's adaptability in multifaceted linguistic tasks.",
    "authors": [
        "Pradyumna Saligram",
        "Andrew Lanpouthakoun"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work explores advanced fine-tuning techniques to boost BERT's performance in sentiment analysis, paraphrase detection, and semantic textual similarity, and leverages SMART regularization to combat overfitting, and employs a cross-embedding Siamese architecture for improved sentence embeddings."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}