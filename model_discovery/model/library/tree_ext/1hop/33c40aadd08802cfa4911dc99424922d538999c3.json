{
    "acronym": "33c40aadd08802cfa4911dc99424922d538999c3",
    "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
    "seed_ids": [
        "gpt",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "914254fac74a2da051cccf6ca16afcaad416a079",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "319b84be7a843250bc81d7086f79a4126d550277",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "33c40aadd08802cfa4911dc99424922d538999c3",
    "abstract": "Natural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.",
    "authors": [
        "Rajvardhan Patil",
        "V. Gudivada"
    ],
    "venue": "Applied Sciences",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs, and explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}