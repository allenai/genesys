{
    "acronym": "f4f4ad1aae9be428323b46ba5511e66d4c66c668",
    "title": "MVP: Optimizing Multi-view Prompts for Medical Dialogue Summarization",
    "seed_ids": [
        "gpt2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "86dac3ff83a0022ed10350690b970b7415e654d6",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "f4f4ad1aae9be428323b46ba5511e66d4c66c668",
    "abstract": "Medical dialogue summarization (MDS) is commonly known as summarizing patients\u2019 electronic health records (EHRs) from doctor-patient dialogues, and the automating of this task can significantly liberate doctors from trivial recordings. Recently, state-of-the-art abstractive summarization systems in open domains are typically adapted from pre-trained language models (PLMs), providing a promising paradigm for MDS to follow. However, such large models with millions of parameters tend to overfit the limited number of training samples available from the medical community. This makes the generated EHRs always contain hallucinatory facts that never appeared in the input dialogue. To address these problems, we propose MVP, a prompt learning method with multi-view prompts to adapt large PLMs for medical dialogue summarization. It learns from the input dialogue: a single input-specific prompt (InP) captures the global dialogue features to ensure faithful results, and several distinct item-specific prompts (ItPs) extract local dialogue information for prompting the generation of each EHR item. We freeze all PLM parameters and only tune the two types of prompts under a multi-level contrastive learning framework, thus effectively avoiding the overfitting problem. Experimental results on two public datasets demonstrate the superior performances of our method.",
    "authors": [
        "Jiaxin Duan",
        "Fengyu Lu",
        "Junfei Liu"
    ],
    "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "MVP, a prompt learning method with multi-view prompts to adapt large PLMs for medical dialogue summarization, which freezes all PLM parameters and only tune the two types of prompts under a multi-level contrastive learning framework, thus effectively avoiding the overfitting problem."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}