{
    "acronym": "4bd1bdc264adb346ed16ebdaa2f92a1adfab5502",
    "title": "Transformers for End-to-End InfoSec Tasks: A Feasibility Study",
    "seed_ids": [
        "transformerxl",
        "2365410a710b421b2295cdca0074946cb50bb1d4",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f4238bd2385a52413ccbacfd9e409a650235bd13"
    ],
    "s2id": "4bd1bdc264adb346ed16ebdaa2f92a1adfab5502",
    "abstract": "Training a machine learning (ML) model from raw information security (InfoSec) data involves utilizing distinct data types and input formats that require unique considerations compared to more conventional applications of ML like natural language processing (NLP) and computer vision (CV). In this paper, we assess the viability of transformer models in end-to-end InfoSec settings, in which no intermediate feature representations or processing steps occur outside the model. We implement transformer models for two distinct InfoSec data formats - specifically URLs and PE files - in a novel end-to-end approach, and explore a variety of architectural designs, training regimes, and experimental settings to determine the ingredients necessary for performant detection models. We show that in contrast to conventional transformers trained on more standard NLP-related tasks, our URL transformer model requires a different training approach to reach high performance levels. Specifically, we show that 1) pre-training on a massive corpus of unlabeled URL data for an auto-regressive task does not readily transfer to binary classification of malicious or benign URLs, but 2) that using an auxiliary auto-regressive loss improves performance when training from scratch. We introduce a method for mixed objective optimization, which dynamically balances contributions from both loss terms so that neither one of them dominates. We show that this method yields quantitative evaluation metrics comparable to that of several top-performing benchmark classifiers. Unlike URLs, binary executables contain longer and more distributed sequences of information-rich bytes. To accommodate such lengthy byte sequences, we introduce additional context length into the transformer by providing its self-attention layers with an adaptive span similar to Sukhbaatar et al. We demonstrate that this approach performs comparably to well-established malware detection models on benchmark PE file datasets, but also point out the need for further exploration into model improvements in scalability and compute efficiency.",
    "authors": [
        "Ethan M. Rudd",
        "Mohammad Saidur Rahman",
        "Philip Tully"
    ],
    "venue": "WoRMA@AsiaCCS",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper implements transformer models for two distinct InfoSec data formats in a novel end-to-end approach, and introduces a method for mixed objective optimization, which dynamically balances contributions from both loss terms so that neither one of them dominates."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}