{
    "acronym": "1d8019f49b319aea98acd572f4ebb7fdeeddedb4",
    "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing",
    "seed_ids": [
        "classfreediffu",
        "6827e87642874d9bf69f0f1548d79a164aaa5e1e",
        "498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "82482585e94192b4e9913727e461f89cd08e9725",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "1d8019f49b319aea98acd572f4ebb7fdeeddedb4",
    "abstract": "We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set natural, art, and AI-generated images for evaluation. A user study on DragBench indicates that SDE-Drag significantly outperforms our ODE baseline, existing diffusion-based methods, and the renowned DragGAN. Our results demonstrate the superiority and versatility of SDE in image editing and push the boundary of diffusion-based editing methods.",
    "authors": [
        "Shen Nie",
        "Hanzhong Guo",
        "Cheng Lu",
        "Yuhao Zhou",
        "Chenyu Zheng",
        "Chongxuan Li"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "In the formulation, it is proved that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing."
    },
    "citationCount": 18,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}