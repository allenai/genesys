{
    "acronym": "95f0cb99549cb59fbfa14ce4eb3c5279a8c0786b",
    "title": "Not Enough Data? Deep Learning to the Rescue!",
    "seed_ids": [
        "gpt",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "95f0cb99549cb59fbfa14ce4eb3c5279a8c0786b",
    "abstract": "Based on recent advances in natural language modeling and those in text generation capabilities, we propose a novel data augmentation method for text classification tasks. We use a powerful pre-trained neural network model to artificially synthesize new labeled data for supervised learning. We mainly focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAMBADA), involves fine-tuning a state-of-the-art language generator to a specific task through an initial training phase on the existing (usually small) labeled data. Using the fine-tuned model and given a class label, new sentences for the class are generated. Our process then filters these new sentences by using a classifier trained on the original data. In a series of experiments, we show that LAMBADA improves classifiers' performance on a variety of datasets. Moreover, LAMBADA significantly improves upon the state-of-the-art techniques for data augmentation, specifically those applicable to text classification tasks with little data.",
    "authors": [
        "Ateret Anaby-Tavor",
        "Boaz Carmeli",
        "Esther Goldbraich",
        "Amir Kantor",
        "George Kour",
        "Segev Shlomov",
        "N. Tepper",
        "Naama Zwerdling"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work uses a powerful pre-trained neural network model to artificially synthesize new labeled data for supervised learning and shows that LAMBADA improves classifiers' performance on a variety of datasets."
    },
    "citationCount": 47,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}