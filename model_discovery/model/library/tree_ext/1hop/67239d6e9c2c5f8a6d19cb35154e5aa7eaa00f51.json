{
    "acronym": "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51",
    "title": "Large Language Models on Graphs: A Comprehensive Survey",
    "seed_ids": [
        "bert",
        "6246ce88c98e4d7ec6421782f7cfd3c910595000",
        "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
        "9429398c52f712e3cd9a92c83bcc7f37c1bf1378",
        "9ada8fa11b1cdece31f253acae50b62df8d5f823",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "23c265ba884b92ecbd9d18641078d964697e4590",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51",
    "abstract": "Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",
    "authors": [
        "Bowen Jin",
        "Gang Liu",
        "Chi Han",
        "Meng Jiang",
        "Heng Ji",
        "Jiawei Han"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A systematic review of scenarios and techniques related to large language models on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models is provided."
    },
    "citationCount": 54,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}