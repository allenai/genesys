{
    "acronym": "848139f931c39a59d59203eea73f941b5767f419",
    "title": "The Conversation is the Command: Interacting with Real-World Autonomous Robots Through Natural Language",
    "seed_ids": [
        "gpt2",
        "bert",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "848139f931c39a59d59203eea73f941b5767f419",
    "abstract": "In recent years, autonomous agents have surged in real-world environments such as our homes, offices, and public spaces. However, natural human-robot interaction remains a key challenge. In this paper, we introduce an approach that synergistically exploits the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable humans to interact naturally with autonomous robots through conversational dialogue. We leveraged the LLMs to decode the high-level natural language instructions from humans and abstract them into precise robot actionable commands or queries. Further, we utilised the VLMs to provide a visual and semantic understanding of the robot's task environment. Our results with 99.13% command recognition accuracy and 97.96% commands execution success show that our approach can enhance human-robot interaction in real-world applications. The video demonstrations of this paper can be found at https://osf.io/wzyf6 and the code is available at our GitHub repository.",
    "authors": [
        "Linus Nwankwo",
        "Elmar Rueckert"
    ],
    "venue": "IEEE/ACM International Conference on Human-Robot Interaction",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces an approach that synergistically exploits the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable humans to interact naturally with autonomous robots through conversational dialogue."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}