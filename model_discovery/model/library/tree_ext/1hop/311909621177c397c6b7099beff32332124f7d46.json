{
    "acronym": "311909621177c397c6b7099beff32332124f7d46",
    "title": "On Learning Universal Representations Across Languages",
    "seed_ids": [
        "gpt",
        "64f94eb97891ca17273464fcb9c507c5a5c7dba7",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "65f788fb964901e3f1149a0a53317535ca85ed7d",
        "7a09101ac03b74db501648597fa54e992a0fc84f",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "311909621177c397c6b7099beff32332124f7d46",
    "abstract": "Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations, and show the effectiveness on cross-lingual understanding and generation. We propose Hierarchical Contrastive Learning (HiCTL) to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on three benchmarks: language understanding tasks (QQP, QNLI, SST-2, MRPC, STS-B and MNLI) in the GLUE benchmark, cross-lingual natural language inference (XNLI) and machine translation. Experimental results show that the HiCTL obtains an absolute gain of 1.0%/2.2% accuracy on GLUE/XNLI as well as achieves substantial improvements of +1.7-+3.6 BLEU on both the high-resource and low-resource English-to-X translation tasks over strong baselines. We will release the source codes as soon as possible.",
    "authors": [
        "Xiangpeng Wei",
        "Yue Hu",
        "Rongxiang Weng",
        "Luxi Xing",
        "Heng Yu",
        "Weihua Luo"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Hierarchical Contrastive Learning (HiCTL) is proposed to learn universal representations for parallel sentences distributed in one or multiple languages and distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence."
    },
    "citationCount": 76,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}