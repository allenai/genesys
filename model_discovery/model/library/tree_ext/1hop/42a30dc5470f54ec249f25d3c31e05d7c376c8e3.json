{
    "acronym": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
    "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
    "seed_ids": [
        "gpt",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "22312f763328cf540791de8c2449ea1e7436f476",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
    "abstract": "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM.",
    "authors": [
        "Wen Wang",
        "Zhe Chen",
        "Xiaokang Chen",
        "Jiannan Wu",
        "Xizhou Zhu",
        "Gang Zeng",
        "Ping Luo",
        "Tong Lu",
        "Jie Zhou",
        "Y. Qiao",
        "Jifeng Dai"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions."
    },
    "citationCount": 262,
    "influentialCitationCount": 14,
    "code": null,
    "description": null,
    "url": null
}