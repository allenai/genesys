{
    "acronym": "2245e0c88f8b992d81bb34636813dce562ad4722",
    "title": "ArabicTransformer: Efficient Large Arabic Language Model with Funnel Transformer and ELECTRA Objective",
    "seed_ids": [
        "funneltransformer"
    ],
    "s2id": "2245e0c88f8b992d81bb34636813dce562ad4722",
    "abstract": "Pre-training Transformer-based models such as BERT and ELECTRA on a collection of Arabic corpora, demonstrated by both AraBERT and AraELECTRA, shows an impressive result on downstream tasks. However, pre-training Transformer-based language models is computationally expensive, especially for large-scale models. Recently, Funnel Transformer has addressed the sequential redundancy inside Transformer architecture by compressing the sequence of hidden states, leading to a significant reduction in the pretraining cost. This paper empirically studies the performance and efficiency of building an Arabic language model with Funnel Transformer and ELECTRA objective. We find that our model achieves state-of-the-art results on several Arabic downstream tasks despite using less computational resources compared to other BERT-based models.",
    "authors": [
        "Sultan Alrowili",
        "V. Shanker"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper empirically studies the performance and efficiency of building an Arabic language model with Funnel Transformer and ELECTRA objective and finds that the model achieves state-of-the-art results on several Arabic downstream tasks despite using less computational resources compared to other BERT-based models."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}