{
    "acronym": "62a76111a7c9b036e3093f5e1653c4c1f795ba12",
    "title": "Improving Neural Machine Translation with Pre-trained Representation",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "62a76111a7c9b036e3093f5e1653c4c1f795ba12",
    "abstract": "Monolingual data has been demonstrated to be helpful in improving the translation quality of neural machine translation (NMT). The current methods stay at the usage of word-level knowledge, such as generating synthetic parallel data or extracting information from word embedding. In contrast, the power of sentence-level contextual knowledge which is more complex and diverse, playing an important role in natural language generation, has not been fully exploited. In this paper, we propose a novel structure which could leverage monolingual data to acquire sentence-level contextual representations. Then, we design a framework for integrating both source and target sentence-level representations into NMT model to improve the translation quality. Experimental results on Chinese-English, German-English machine translation tasks show that our proposed model achieves improvement over strong Transformer baselines, while experiments on English-Turkish further demonstrate the effectiveness of our approach in the low-resource scenario.",
    "authors": [
        "Rongxiang Weng",
        "Heng Yu",
        "Shujian Huang",
        "Weihua Luo",
        "Jiajun Chen"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel structure which could leverage monolingual data to acquire sentence-level contextual representations is proposed and a framework for integrating both source and target sentence- level representations into NMT model to improve the translation quality is designed."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}