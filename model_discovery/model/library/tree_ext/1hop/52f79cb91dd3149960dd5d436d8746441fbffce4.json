{
    "acronym": "52f79cb91dd3149960dd5d436d8746441fbffce4",
    "title": "Optimizing small BERTs trained for German NER",
    "seed_ids": [
        "bigbird",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "52f79cb91dd3149960dd5d436d8746441fbffce4",
    "abstract": "Currently, the most widespread neural network architecture for training language models is the so-called BERT, which led to improvements in various NLP tasks. In general, the larger the number of parameters in a BERT model, the better the results obtained in these NLP tasks. Unfortunately, the memory consumption and the training duration drastically increases with the size of these models. In this article, we investigate various training techniques of smaller BERT models: We combine different methods from other BERT variants, such as ALBERT, RoBERTa, and relative positional encoding. In addition, we propose two new fine-tuning modifications leading to better performance: CSE tagging and a modified form of LCRF. Furthermore, we introduce WWA, which reduces BERT memory usage and leads to a small increase in performance compared to classical Multi-Head-Attention. We evaluate these techniques on five public German NER tasks, of which two are introduced by this article.",
    "authors": [
        "Jochen Z\u00f6llner",
        "Konrad Sperfeld",
        "C. Wick",
        "R. Labahn"
    ],
    "venue": "Inf.",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article investigates various training techniques of smaller BERT models, and introduces WWA, which reduces BERT memory usage and leads to a small increase in performance compared to classical Multi-Head-Attention."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}