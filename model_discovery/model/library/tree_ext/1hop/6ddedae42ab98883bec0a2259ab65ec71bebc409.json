{
    "acronym": "6ddedae42ab98883bec0a2259ab65ec71bebc409",
    "title": "UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation",
    "seed_ids": [
        "mamba",
        "62ac3ef81e54e1d1930fb5980b236345ee2e4f32",
        "0a32e6ff6eaac83ff325bae4557a8362222979aa",
        "3af7273d7ca20c0c63cbaa47e60b058840835052",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "aa2d103e58acb17908799954fb50058125ef17ca",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51"
    ],
    "s2id": "6ddedae42ab98883bec0a2259ab65ec71bebc409",
    "abstract": "Recent diffusion-based human image animation techniques have demonstrated impressive success in synthesizing videos that faithfully follow a given reference identity and a sequence of desired movement poses. Despite this, there are still two limitations: i) an extra reference model is required to align the identity image with the main video branch, which significantly increases the optimization burden and model parameters; ii) the generated video is usually short in time (e.g., 24 frames), hampering practical applications. To address these shortcomings, we present a UniAnimate framework to enable efficient and long-term human video generation. First, to reduce the optimization difficulty and ensure temporal coherence, we map the reference image along with the posture guidance and noise video into a common feature space by incorporating a unified video diffusion model. Second, we propose a unified noise input that supports random noised input as well as first frame conditioned input, which enhances the ability to generate long-term video. Finally, to further efficiently handle long sequences, we explore an alternative temporal modeling architecture based on state space model to replace the original computation-consuming temporal Transformer. Extensive experimental results indicate that UniAnimate achieves superior synthesis results over existing state-of-the-art counterparts in both quantitative and qualitative evaluations. Notably, UniAnimate can even generate highly consistent one-minute videos by iteratively employing the first frame conditioning strategy. Code and models will be publicly available. Project page: https://unianimate.github.io/.",
    "authors": [
        "Xiang Wang",
        "Shiwei Zhang",
        "Changxin Gao",
        "Jiayu Wang",
        "Xiaoqiang Zhou",
        "Yingya Zhang",
        "Luxin Yan",
        "Nong Sang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents a UniAnimate framework to enable efficient and long-term human video generation, and proposes a unified noise input that supports random noised input as well as first frame conditioned input, which enhances the ability to generate long-term video."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}