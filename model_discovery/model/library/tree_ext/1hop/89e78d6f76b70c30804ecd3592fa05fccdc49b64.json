{
    "acronym": "89e78d6f76b70c30804ecd3592fa05fccdc49b64",
    "title": "Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar",
    "seed_ids": [
        "gpt",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "0fe2636446cd686830da3d971b31a004d6094b3c"
    ],
    "s2id": "89e78d6f76b70c30804ecd3592fa05fccdc49b64",
    "abstract": "We introduce NSEdit (neural-symbolic edit), a novel Transformer-based code repair method. Given only the source code that contains bugs, NSEdit predicts an editing sequence that can fix the bugs. The edit grammar is formulated as a regular language, and the Transformer uses it as a neural-symbolic scripting interface to generate editing programs. We modify the Transformer and add a pointer network to select the edit locations. An ensemble of rerankers are trained to re-rank the editing sequences generated by beam search. We fine-tune the rerankers on the validation set to reduce over-fitting. NSEdit is evaluated on various code repair datasets and achieved a new state-of-the-art accuracy ($24.04\\%$) on the Tufano small dataset of the CodeXGLUE benchmark. NSEdit performs robustly when programs vary from packages to packages and when buggy programs are concrete. We conduct detailed analysis on our methods and demonstrate the effectiveness of each component.",
    "authors": [
        "Yaojie Hu",
        "Xingjian Shi",
        "Qiang Zhou",
        "Lee Pike"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces NSEdit (neural-symbolic edit), a novel Transformer-based code repair method that predicts an editing sequence that can fix the bugs in source code given only the source code that contains bugs."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}