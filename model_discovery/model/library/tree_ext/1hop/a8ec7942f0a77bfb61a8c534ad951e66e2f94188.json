{
    "acronym": "a8ec7942f0a77bfb61a8c534ad951e66e2f94188",
    "title": "Data Augmentation for Spoken Language Understanding via Pretrained Language Models",
    "seed_ids": [
        "gpt",
        "7eba731a7fd8de712b7b79b5af41a6e2d4dbd191"
    ],
    "s2id": "a8ec7942f0a77bfb61a8c534ad951e66e2f94188",
    "abstract": "The training of spoken language understanding (SLU) models often faces the problem of data scarcity. In this paper, we put forward a data augmentation method using pretrained language models to boost the variability and accuracy of generated utterances. Furthermore, we investigate and propose solutions to two previously overlooked semi-supervised learning scenarios of data scarcity in SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue acts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances are available. Empirical results show that our method can produce synthetic training data that boosts the performance of language understanding models in various scenarios.",
    "authors": [
        "Baolin Peng",
        "Chenguang Zhu",
        "Michael Zeng",
        "Jianfeng Gao"
    ],
    "venue": "Interspeech",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A data augmentation method using pretrained language models to boost the variability and accuracy of generated utterances and investigate and propose solutions to two previously overlooked semi-supervised learning scenarios of data scarcity in SLU."
    },
    "citationCount": 12,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}