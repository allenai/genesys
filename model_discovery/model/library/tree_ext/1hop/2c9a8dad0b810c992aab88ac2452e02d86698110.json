{
    "acronym": "2c9a8dad0b810c992aab88ac2452e02d86698110",
    "title": "Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood",
    "seed_ids": [
        "classfreediffu",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "82482585e94192b4e9913727e461f89cd08e9725",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "2c9a8dad0b810c992aab88ac2452e02d86698110",
    "abstract": "Training energy-based models (EBMs) on high-dimensional data can be both challenging and time-consuming, and there exists a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versions of a dataset, paired with an initializer model for each EBM. At each noise level, the two models are jointly estimated within a cooperative training framework: samples from the initializer serve as starting points that are refined by a few MCMC sampling steps from the EBM. The EBM is then optimized by maximizing recovery likelihood, while the initializer model is optimized by learning from the difference between the refined samples and the initial samples. In addition, we made several practical designs for EBM training to further improve the sample quality. Combining these advances, our approach significantly boost the generation performance compared to existing EBM methods on CIFAR-10 and ImageNet datasets. We also demonstrate the effectiveness of our models for several downstream tasks, including classifier-free guided generation, compositional generation, image inpainting and out-of-distribution detection.",
    "authors": [
        "Y. Zhu",
        "Jianwen Xie",
        "Yingnian Wu",
        "Ruiqi Gao"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versions of a dataset, paired with an initializer model for each EBM."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}