{
    "acronym": "85447eeb6e5276e713957835125a2273f9ac0694",
    "title": "In-Context Language Learning: Architectures and Algorithms",
    "seed_ids": [
        "hyena",
        "mamba",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "ca444821352a4bd91884413d8070446e2960715a",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "d5e999aae76d5270ef272076979c809817458212",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "6f68e1bb253925d8431588555d3010419f322e04"
    ],
    "s2id": "85447eeb6e5276e713957835125a2273f9ac0694",
    "abstract": "Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the\"real\"ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including several RNNs, Transformers, and state-space model variants) on regular ICLL tasks, aiming to answer three questions: (1) Which model classes are empirically capable of ICLL? (2) What algorithmic solutions do successful models implement to perform ICLL? (3) What architectural changes can improve ICLL in less performant models? We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks. Next, we provide evidence that their ability to do so relies on specialized\"n-gram heads\"(higher-order variants of induction heads) that compute input-conditional next-token distributions. Finally, we show that hard-wiring these heads into neural models improves performance not just on ICLL, but natural language modeling -- improving the perplexity of 340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.",
    "authors": [
        "Ekin Aky\u00fcrek",
        "Bailin Wang",
        "Yoon Kim",
        "Jacob Andreas"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks, and there is evidence that their ability to do so relies on specialized\"n-gram heads\"(higher-order variants of induction heads) that compute input-conditional next-token distributions."
    },
    "citationCount": 13,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}