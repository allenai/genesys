{
    "acronym": "c2b13c2a04a9aeee1b77371ff1708f494989fca5",
    "title": "Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models",
    "seed_ids": [
        "gpt2",
        "72c7cb545f7da68efd1014afe3a4f01b590e435b",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
        "5d22b241836e30d5b0d852b463951ab7e3245ea4",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c2b13c2a04a9aeee1b77371ff1708f494989fca5",
    "abstract": "Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.",
    "authors": [
        "Hsuan Su",
        "Cheng-Chu Cheng",
        "Hua Farn",
        "Shachi H. Kumar",
        "Saurav Sahay",
        "Shang-Tse Chen",
        "Hung-yi Lee"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias and proposes a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning."
    },
    "citationCount": 1,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}