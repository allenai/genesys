{
    "acronym": "dc5d2fc432993c295294db41dc61214f4d50555f",
    "title": "Evaluation of large language models using an Indian language LGBTI+ lexicon",
    "seed_ids": [
        "gpt3"
    ],
    "s2id": "dc5d2fc432993c295294db41dc61214f4d50555f",
    "abstract": "Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine the behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM\u2019s behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English. The methodology presented in this paper can be useful for LGBTI+ lexicons in other languages as well as other domain-specific lexicons. The work done in this paper opens avenues for responsible behaviour of LLMs in the Indian context, especially with prevalent social perception of the LGBTI+ community.",
    "authors": [
        "Aditya Joshi",
        "S. Rawat",
        "A. Dange"
    ],
    "venue": "AI Ethics Journal",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages and shows that the three LLMs the authors experiment on are unable to detect underlying hateful content."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}