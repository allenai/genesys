{
    "acronym": "a37daf2fb463fe2e9c895f2e94a055c3d3a655a6",
    "title": "SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization",
    "seed_ids": [
        "longformer",
        "f78fe02f681a0a9a6867b007bd39e3884de64a91",
        "3bcea238b0c323d8f891829714bbe6e8a3de894c",
        "3b39efe6c91ae432dd35bb79431edb8a6719f906",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "c600b697700c844cbc85009be70f1cdfeef3593e",
        "ac95a18762133d4065ac8af518c33084d83c5582",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a37daf2fb463fe2e9c895f2e94a055c3d3a655a6",
    "abstract": "Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. We then perform a pre-training, which encompasses the detection of speaker changes, and masked utterance generation. Experimental results of fine-tuned models demonstrate that our model achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of our approach. Our findings highlight the importance of curating pre-training datasets that exhibit diversity and variations in length distribution to ensure effective alignment with downstream datasets.",
    "authors": [
        "Sangwoo Cho",
        "Kaiqiang Song",
        "Chao Zhao",
        "Xiaoyang Wang",
        "Dong Yu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues and achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of the approach."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}