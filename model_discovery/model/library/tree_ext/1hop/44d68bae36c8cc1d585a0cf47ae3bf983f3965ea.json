{
    "acronym": "44d68bae36c8cc1d585a0cf47ae3bf983f3965ea",
    "title": "SKIP: Skill-Localized Prompt Tuning for Inference Speed Boost-Up",
    "seed_ids": [
        "bert"
    ],
    "s2id": "44d68bae36c8cc1d585a0cf47ae3bf983f3965ea",
    "abstract": "Prompt-tuning methods have shown comparable performance as parameter-efficient fine-tuning (PEFT) methods in various natural language understanding tasks. However, existing prompt tuning methods still utilize the entire model architecture; thus, they fail to accelerate inference speed in the application. In this paper, we propose a novel approach called SKIll-localized Prompt tuning (SKIP), which is extremely efficient in inference time. Our method significantly enhances inference efficiency by investigating and utilizing a skill-localized subnetwork in a language model. Surprisingly, our method improves the inference speed up to 160% while pruning 52% of the parameters. Furthermore, we demonstrate that our method is applicable across various transformer-based architectures, thereby confirming its practicality and scalability.",
    "authors": [
        "Nakyeong Yang",
        "Junseok Kim",
        "Jiwon Moon",
        "Yunah Jang",
        "Kyomin Jung"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a novel approach called SKIll-localized Prompt tuning (SKIP), which is extremely efficient in inference time and significantly enhances inference efficiency by investigating and utilizing a skill-localized subnetwork in a language model."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}