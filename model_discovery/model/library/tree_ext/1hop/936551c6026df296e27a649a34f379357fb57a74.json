{
    "acronym": "936551c6026df296e27a649a34f379357fb57a74",
    "title": "Mnemosyne: Learning to Train Transformers with Transformers",
    "seed_ids": [
        "performer",
        "988e21e7aa6ee5ee7889f785337231cbbcebbed7",
        "dc1b905c0af4dc318b63cd52fbc867c788df4b8c",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "a25370452533bf47549243e97852b9cdf7a0ee0e",
        "9058d322a09bfc0c93a070f87cac8fd840e63088",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "77706ee4cbdbb23345da22af37bc1b9f5ec8f110",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "936551c6026df296e27a649a34f379357fb57a74",
    "abstract": "In this work, we propose a new class of learnable optimizers, called \\textit{Mnemosyne}. It is based on the novel spatio-temporal low-rank implicit attention Transformers that can learn to train entire neural network architectures, including other Transformers, without any task-specific optimizer tuning. We show that Mnemosyne: (a) outperforms popular LSTM optimizers (also with new feature engineering to mitigate catastrophic forgetting of LSTMs), (b) can successfully train Transformers while using simple meta-training strategies that require minimal computational resources, (c) matches accuracy-wise SOTA hand-designed optimizers with carefully tuned hyper-parameters (often producing top performing models). Furthermore, Mnemosyne provides space complexity comparable to that of its hand-designed first-order counterparts, which allows it to scale to training larger sets of parameters. We conduct an extensive empirical evaluation of Mnemosyne on: (a) fine-tuning a wide range of Vision Transformers (ViTs) from medium-size architectures to massive ViT-Hs (36 layers, 16 heads), (b) pre-training BERT models and (c) soft prompt-tuning large 11B+ T5XXL models. We complement our results with a comprehensive theoretical analysis of the compact associative memory used by Mnemosyne which we believe was never done before.",
    "authors": [
        "Deepali Jain",
        "K. Choromanski",
        "Sumeet Singh",
        "Vikas Sindhwani",
        "Tingnan Zhang",
        "Jie Tan",
        "Kumar Avinava Dubey"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Mnemosyne is a new class of learnable optimizers based on the novel spatio-temporal low-rank implicit attention Transformers that can learn to train entire neural network architectures, including other Transformers, without any task-specific optimizer tuning."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}