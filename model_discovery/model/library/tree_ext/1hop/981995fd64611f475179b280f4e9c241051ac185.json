{
    "acronym": "981995fd64611f475179b280f4e9c241051ac185",
    "title": "Knowledge Inheritance for Pre-trained Language Models",
    "seed_ids": [
        "gpt",
        "bc7984bfcfae537dbe633eeeb8d69c42a994c724",
        "7a49beff86a855f237f96ae3f0aefc9780cb31be",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "2365410a710b421b2295cdca0074946cb50bb1d4",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "75352cc69a29bd5fc411e0e79737cb96b6309161",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "981995fd64611f475179b280f4e9c241051ac185",
    "abstract": "Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named \u201cknowledge inheritance\u201d (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs\u2019 pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.",
    "authors": [
        "Yujia Qin",
        "Yankai Lin",
        "Jing Yi",
        "Jiajie Zhang",
        "Xu Han",
        "Zhengyan Zhang",
        "Yusheng Su",
        "Zhiyuan Liu",
        "Peng Li",
        "Maosong Sun",
        "Jie Zhou"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A pre-training framework named \u201cknowledge inheritance\u201d (KI) is introduced and how could knowledge distillation serve as auxiliary supervision during pre- training to efficiently learn larger PLMs is explored, demonstrating the superiority of KI in training efficiency."
    },
    "citationCount": 40,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}