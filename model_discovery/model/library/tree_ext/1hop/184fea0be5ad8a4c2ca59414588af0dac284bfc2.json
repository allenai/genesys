{
    "acronym": "184fea0be5ad8a4c2ca59414588af0dac284bfc2",
    "title": "Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "184fea0be5ad8a4c2ca59414588af0dac284bfc2",
    "abstract": "In this work, we propose an aggregation method to combine the Bidirectional Encoder Representations from Transformer (BERT) with a MatchLSTM layer for Sequence Matching. Given a sentence pair, we extract the output representations of it from BERT. Then we extend BERT with a MatchLSTM layer to get further interaction of the sentence pair for sequence matching tasks. Taking natural language inference as an example, we split BERT output into two parts, which is from premise sentence and hypothesis sentence. At each position of the hypothesis sentence, both the weighted representation of the premise sentence and the representation of the current token are fed into LSTM. We jointly train the aggregation layer and pre-trained layer for sequence matching. We conduct an experiment on two publicly available datasets, WikiQA and SNLI. Experiments show that our model achieves significantly improvement compared with state-of-the-art methods on both datasets.",
    "authors": [
        "Bo Shao",
        "Yeyun Gong",
        "Weizhen Qi",
        "Nan Duan",
        "X. Lin"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An aggregation method to combine the Bidirectional Encoder Representations from Transformer (BERT) with a MatchLSTM layer for Sequence Matching with significantly improvement compared with state-of-the-art methods on both datasets."
    },
    "citationCount": 2,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}