{
    "acronym": "4a78f23be51f314240d17fa0a340785ee88bf032",
    "title": "Cross-lingual Language Model Pretraining for Retrieval",
    "seed_ids": [
        "longformer",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "65f788fb964901e3f1149a0a53317535ca85ed7d",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4a78f23be51f314240d17fa0a340785ee88bf032",
    "abstract": "Existing research on cross-lingual retrieval cannot take good advantage of large-scale pretrained language models such as multilingual BERT and XLM. We hypothesize that the absence of cross-lingual passage-level relevance data for finetuning and the lack of query-document style pretraining are key factors of this issue. In this paper, we introduce two novel retrieval-oriented pretraining tasks to further pretrain cross-lingual language models for downstream retrieval tasks such as cross-lingual ad-hoc retrieval (CLIR) and cross-lingual question answering (CLQA). We construct distant supervision data from multilingual Wikipedia using section alignment to support retrieval-oriented language model pretraining. We also propose to directly finetune language models on part of the evaluation collection by making Transformers capable of accepting longer sequences. Experiments on multiple benchmark datasets show that our proposed model can significantly improve upon general multilingual language models in both the cross-lingual retrieval setting and the cross-lingual transfer setting.",
    "authors": [
        "Puxuan Yu",
        "Hongliang Fei",
        "P. Li"
    ],
    "venue": "The Web Conference",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces two novel retrieval-oriented pretraining tasks to further pretrain cross-lingual language models for downstream retrieval tasks such as cross-lingsual ad-hoc retrieval (CLIR), and proposes to directly finetune language models on part of the evaluation collection by making Transformers capable of accepting longer sequences."
    },
    "citationCount": 35,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}