{
    "acronym": "01f2b214962997260020279bd1fd1f8f372249d4",
    "title": "Evaluating Commonsense in Pre-trained Language Models",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "e2587eddd57bc4ba286d91b27c185083f16f40ee",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "01f2b214962997260020279bd1fd1f8f372249d4",
    "abstract": "Contextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models' commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research.",
    "authors": [
        "Xuhui Zhou",
        "Yue Zhang",
        "Leyang Cui",
        "Dandan Huang"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work studies the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models' commonsens ability while bi-directional context and larger training set are bonuses."
    },
    "citationCount": 163,
    "influentialCitationCount": 10,
    "code": null,
    "description": null,
    "url": null
}