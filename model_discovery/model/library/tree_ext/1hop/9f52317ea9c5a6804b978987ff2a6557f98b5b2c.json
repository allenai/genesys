{
    "acronym": "9f52317ea9c5a6804b978987ff2a6557f98b5b2c",
    "title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks",
    "seed_ids": [
        "synthesizer",
        "reformer",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "2ace8667f2b331001136391cae237d50c0db6383",
        "d5e999aae76d5270ef272076979c809817458212",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "9f52317ea9c5a6804b978987ff2a6557f98b5b2c",
    "abstract": "As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.",
    "authors": [
        "Rui Zhu",
        "Qihang Zhao",
        "J. Eshraghian"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language."
    },
    "citationCount": 54,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}