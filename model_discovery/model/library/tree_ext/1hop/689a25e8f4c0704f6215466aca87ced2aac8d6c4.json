{
    "acronym": "689a25e8f4c0704f6215466aca87ced2aac8d6c4",
    "title": "WISE: Wavelet Transformation for Boosting Transformers\u2019 Long Sequence Learning Ability",
    "seed_ids": [
        "performer",
        "lineartransformer",
        "bigbird",
        "sinkhorn",
        "fnet",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "c4f4e5e86cb9468235fd5d7c9ccce09a68084463",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "07a9f47885cae97efb7b4aa109392128532433da",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1"
    ],
    "s2id": "689a25e8f4c0704f6215466aca87ced2aac8d6c4",
    "abstract": "Transformer and its variants are fundamental neural architectures in deep learning. Recent works show that learning attention in the Fourier space can improve the long sequence learning capability of Transformers. We argue that wavelet transform shall be a better choice because it captures both position and frequency information with a linear time complexity. Therefore, in this paper, we systematically study the synergy between wavelet transform and Transformers. Speci\ufb01cally, we focus on a new paradigm WISE, which replaces the attention in Transformers by (1) applying forward wavelet transform to project the input sequences to multi-resolution bases, (2) conducting non-linear transformations in the wavelet coef\ufb01-cient space, and (3) reconstructing the representation in input space via backward wavelet transform. Extensive experiments on the Long Range Arena benchmark demonstrate that learning attention in the wavelet space using either \ufb01xed or adaptive wavelets can consistently improve Trans-former\u2019s performance and also signi\ufb01cantly out-perform Fourier-based methods.",
    "authors": [
        "Yufan Zhuang",
        "Zihan Wang",
        "Fangbo Tao",
        "Jingbo Shang"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper systematically study the synergy between wavelet transform and Transformers, and focuses on a new paradigm WISE, which replaces the attention in Transformers by applying forward wavelet transform to project the input sequences to multi-resolution bases."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}