{
    "acronym": "9c5609baff6175b0a2e436bb69e89737c4be3cf4",
    "title": "Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers",
    "seed_ids": [
        "gpt2",
        "3bef44ef3aacddaae56d6efc9f6767e64bc65b31",
        "ea4fa9e7155cf064de19862e41a6e63738770012",
        "d3a95b0727ef67ef9264cd20da10a545977ace51",
        "58b8da3821affc426895a85dbac5556322e6e2a9",
        "9aacdbc8b04fa63e6fe93f62a737a11c613f08fb",
        "d25121da56c9050137800c69520111b30201d1ed",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "9c5609baff6175b0a2e436bb69e89737c4be3cf4",
    "abstract": "Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation.",
    "authors": [
        "Chen Tang",
        "Shunyu Wang",
        "Tomas Goldsack",
        "Chenghua Lin"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel attention-based citation aggregation model is proposed that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from Citation papers."
    },
    "citationCount": 9,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}