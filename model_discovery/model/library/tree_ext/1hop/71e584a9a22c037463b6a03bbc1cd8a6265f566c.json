{
    "acronym": "71e584a9a22c037463b6a03bbc1cd8a6265f566c",
    "title": "Document Structure in Long Document Transformers",
    "seed_ids": [
        "longformer",
        "longt5",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "247ac77a559d00d69f4353b02fb9e6529e3849cd",
        "732e3faec4e5be4d144256f2c379b9dc49f0b227",
        "3d318019788418b21478e8736d03afadc1607690",
        "24b951275a7a42ef36aca8352caaf6f4cd6238d2",
        "79cb9bca730a4c5bbe73d97c1f40da1d0debe568",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "34042e2680e475510a1030b54165a81534ad88d3",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "320efa53dea3e8f836790682fbd4196132c49749",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "0311101ef8ec57b90b67cefa50e32c44145530da"
    ],
    "s2id": "71e584a9a22c037463b6a03bbc1cd8a6265f566c",
    "abstract": "Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task performance. To foster research on the role of document structure in NLP modeling, we make our data and code publicly available.",
    "authors": [
        "Jan Buchmann",
        "Max Eichler",
        "Jan-Micha Bodensohn",
        "Ilia Kuznetsov",
        "Iryna Gurevych"
    ],
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task performance."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}