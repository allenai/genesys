{
    "acronym": "fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a",
    "title": "Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt",
    "seed_ids": [
        "gpt2",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a",
    "abstract": "Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pre-trained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.",
    "authors": [
        "Xinyin Ma",
        "Xinchao Wang",
        "Gongfan Fang",
        "Yongliang Shen",
        "Weiming Lu"
    ],
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks."
    },
    "citationCount": 9,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}