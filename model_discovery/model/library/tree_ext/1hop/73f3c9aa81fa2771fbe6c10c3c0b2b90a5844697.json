{
    "acronym": "73f3c9aa81fa2771fbe6c10c3c0b2b90a5844697",
    "title": "CSP: Code-Switching Pre-training for Neural Machine Translation",
    "seed_ids": [
        "gpt",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "62a76111a7c9b036e3093f5e1653c4c1f795ba12",
        "7a09101ac03b74db501648597fa54e992a0fc84f",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "73f3c9aa81fa2771fbe6c10c3c0b2b90a5844697",
    "abstract": "This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the cross-lingual alignment information extracted from the source and target monolingual corpus. Additionally, we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask]. To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.",
    "authors": [
        "Zhen Yang",
        "Bojie Hu",
        "Ambyera Han",
        "Shen Huang",
        "Qi Ju"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experimental results show that CSP achieves significant improvements over baselines without pre- training or with other pre-training methods, and relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask]."
    },
    "citationCount": 58,
    "influentialCitationCount": 13,
    "code": null,
    "description": null,
    "url": null
}