{
    "acronym": "92a95c5d3ea87e08ac527d8ce25383ff8c1015be",
    "title": "ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer",
    "seed_ids": [
        "nystromformer",
        "977351c92f156db27592e88b14dee2c22d4b312a",
        "200ef1cde362aafbf598a2b5a1c5f35504ca2289",
        "13270b9759cf0296b5a346fbb58b706e8ad0a982",
        "2475b38a76a9c2dc67f74446e2e686815764b0f2",
        "ec139916edd6feb9b3cb3a0325ca96e21dbb0147",
        "e574fbeee3e163c67693c60db0a68547029f234e",
        "b52844a746dafd8a5051cef49abbbda64a312605",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "7d2a78a1f713b71c3a337247d042c5c2f0b2da84",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "92a95c5d3ea87e08ac527d8ce25383ff8c1015be",
    "abstract": "Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. However, both the attention mechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently efficient due to dense multiplications, leading to costly training and inference. To this end, we propose to reparameterize pre-trained ViTs with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on GPUs without requiring training from scratch. Specifically, all $\\texttt{MatMuls}$ among queries, keys, and values are reparameterized using additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized with shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameterization on attention maintains model accuracy, while inevitably leading to accuracy drops when being applied to MLPs. To marry the best of both worlds, we further propose a new mixture of experts (MoE) framework to reparameterize MLPs by taking multiplication or its primitives as experts, e.g., multiplication and shift, and designing a new latency-aware load-balancing loss. Such a loss helps to train a generic router for assigning a dynamic amount of input tokens to different experts according to their latency. Extensive experiments on various 2D/3D Transformer-based vision tasks consistently validate the effectiveness of our proposed ShiftAddViT, achieving up to $\\textbf{5.18$\\times$}$ latency reductions on GPUs and $\\textbf{42.9}$% energy savings, while maintaining a comparable accuracy as original or efficient ViTs.",
    "authors": [
        "Haoran You",
        "Huihong Shi",
        "Yipin Guo",
        "Yingyan Lin"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes to reparameterize pre-trained ViTs with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed ShiftAddViT, which aims to achieve end-to-end inference speedups on GPUs without requiring training from scratch."
    },
    "citationCount": 10,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}