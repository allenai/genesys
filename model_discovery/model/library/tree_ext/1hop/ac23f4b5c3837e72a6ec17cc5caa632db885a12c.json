{
    "acronym": "ac23f4b5c3837e72a6ec17cc5caa632db885a12c",
    "title": "Efficient Time Series Processing for Transformers and State-Space Models through Token Merging",
    "seed_ids": [
        "hyena",
        "f45f85fa1beaa795c24c4ff86f1f2deece72252f",
        "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "8064d3873c646dc9ff949d72c54c634a906fc092",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9"
    ],
    "s2id": "ac23f4b5c3837e72a6ec17cc5caa632db885a12c",
    "abstract": "Transformer architectures have shown promising results in time series processing. However, despite recent advances in subquadratic attention mechanisms or state-space models, processing very long sequences still imposes significant computational requirements. Token merging, which involves replacing multiple tokens with a single one calculated as their linear combination, has shown to considerably improve the throughput of vision transformer architectures while maintaining accuracy. In this work, we go beyond computer vision and perform the first investigations of token merging in time series analysis on both time series transformers and state-space models. To effectively scale token merging to long sequences, we introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, adjusting the computational complexity from linear to quadratic based on the neighborhood size. Our comprehensive empirical evaluation demonstrates that token merging offers substantial computational benefits with minimal impact on accuracy across various models and datasets. On the recently proposed Chronos foundation model, we achieve accelerations up to 5400% with only minor accuracy degradations.",
    "authors": [
        "Leon G\u00f6tz",
        "Marcel Kollovieh",
        "Stephan G\u00fcnnemann",
        "Leo Schwinn"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work performs the first investigations of token merging in time series analysis on both time series transformers and state-space models, and introduces local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, adjusting the computational complexity from linear to quadratic based on the neighborhood size."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}