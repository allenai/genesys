{
    "acronym": "adc19c97fd19d67948db30614898ae7a8be3739c",
    "title": "Auto-Learning: An Adversarial Process of Two Pre-trained Models for Natural Language Generation",
    "seed_ids": [
        "memcompress",
        "78d70680a91024b0485ddfa55753e9a0676df7be",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "b76e98a0a023d37c6534aa2ead09c8ff595f0bae",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "adc19c97fd19d67948db30614898ae7a8be3739c",
    "abstract": "Pre-trained models have been used in many \ufb01elds in recent years, ranging from natural language understanding to computer vision and natural language generation. However, the performance of these natural language generation models is overly dependent on the scale of the model and the size of the dataset. While the larger language model is excellent in some re-spects, it cannot learn up-to-date knowledge and is relatively dif\ufb01cult to relearn. In this paper, a new adversarial process learning method called Auto-Learning . This can improve the performance of any natural language generation model without the help of additional datasets. Auto-Learning includes two models: G is a text generation model and D can test whether the data generated by G is legitimate. Firstly, the \ufb01ne-tuned D model is used as the brain\u2019s knowledge base before the process. Then the text generated by the G model is used as the input of D to determine whether the text is legitimate or not. Finally, G is \ufb01ne-tuned according to the output of D . This adversarial process is like a self-escalation of the brain through some a priori knowledge. When this adversarial system wants to learn some-thing new, simply \ufb01ne-tune the D model. Our approach applies to Autoregressive Language Modeling for all Transformer classes. The results are good in existing experimental tasks, including more grammatical text generation and better performance on",
    "authors": [
        "Zheng Yuan",
        "Yuelin Lu",
        "C. Zhang",
        "HU Xue"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new adversarial process learning method called Auto-Learning is introduced, which can improve the performance of any natural language generation model without the help of additional datasets."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}