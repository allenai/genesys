{
    "acronym": "bcd84a2b8f9ae40a908f375425f113c82f8dd739",
    "title": "Sparse Universal Transformer",
    "seed_ids": [
        "universaltrans",
        "e82e3f4347674b75c432cb80604d38ee630d4bf6",
        "6edccbd83a9aae204785d4821f97855677c33866",
        "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617",
        "ed535e93d5b5a8b689e861e9c6083a806d1535c2",
        "3456c1e95d8d2f985a0701232dd55171b3cbd5e0",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "0bd2602df71e89c8961562175c8759e625e99389"
    ],
    "s2id": "bcd84a2b8f9ae40a908f375425f113c82f8dd739",
    "abstract": "The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\\% reduction in computation during inference with very little performance decrease on formal language tasks.",
    "authors": [
        "Shawn Tan",
        "Yikang Shen",
        "Zhenfang Chen",
        "Aaron Courville",
        "Chuang Gan"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}