{
    "acronym": "0b74ad893b8b5f6054d9e05ae706edd53b26d6a5",
    "title": "Masked Graph Transformer for Large-Scale Recommendation",
    "seed_ids": [
        "performer",
        "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b",
        "131ba9932572c92155874db93626cf299659254e",
        "5eda60d4940d4185df45c5703e103458171d465d",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87"
    ],
    "s2id": "0b74ad893b8b5f6054d9e05ae706edd53b26d6a5",
    "abstract": "Graph Transformers have garnered significant attention for learning graph-structured data, thanks to their superb ability to capture long-range dependencies among nodes. However, the quadratic space and time complexity hinders the scalability of Graph Transformers, particularly for large-scale recommendation. Here we propose an efficient Masked Graph Transformer, named MGFormer, capable of capturing all-pair interactions among nodes with a linear complexity. To achieve this, we treat all user/item nodes as independent tokens, enhance them with positional embeddings, and feed them into a kernelized attention module. Additionally, we incorporate learnable relative degree information to appropriately reweigh the attentions. Experimental results show the superior performance of our MGFormer, even with a single attention layer.",
    "authors": [
        "Huiyuan Chen",
        "Zhe Xu",
        "Chin-Chia Michael Yeh",
        "Vivian Lai",
        "Yan Zheng",
        "Minghua Xu",
        "Hanghang Tong"
    ],
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An efficient Masked Graph Transformer, named MGFormer, capable of capturing all-pair interactions among nodes with a linear complexity, is proposed, which treats all user/item nodes as independent tokens, enhance them with positional embeddings, and feed them into a kernelized attention module."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}