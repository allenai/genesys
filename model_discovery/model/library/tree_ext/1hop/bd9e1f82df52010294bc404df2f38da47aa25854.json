{
    "acronym": "bd9e1f82df52010294bc404df2f38da47aa25854",
    "title": "Lita: Accelerating Distributed Training of Sparsely Activated Models",
    "seed_ids": [
        "transformerxl",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "bd9e1f82df52010294bc404df2f38da47aa25854",
    "abstract": "Scaling model parameters usually improves model quality, but at the price of high computation overhead. Sparsely activated models, usually in the form of Mixture of Experts (MoE) architecture, have constant computation cost over their dense counterparts, thus providing opportunities to train and serve a large model at a reasonable cost. However, the distributed training of an MoE model is prone to low ef\ufb01ciency, mainly due to the interleaved all-to-all communication during model computation. This paper makes three main contributions. First, we systematically analyze the all-to-all overhead in distributed training of MoE. Second, we propose a new communication scheduling scheme based on tensor partitioning that prioritizes the all-to-all operations over other communication, due to its blocking nature. Third, we introduce expert packing that reduces the all-to-all transfer size and incorporates optimizations to mitigate its overheads. Both techniques effectively tackle the all-to-all bottleneck, and we integrate them into a new system called Lita. Experiments on an A100 GPU testbed show that Lita improves the training step time of popular NLP models by up to 1.73x over the state-of-the-art.",
    "authors": [
        "Jiamin Li",
        "Yimin Jiang",
        "Yibo Zhu",
        "Cong Wang",
        "Hong-Yu Xu"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new communication scheduling scheme based on tensor partitioning that prioritizes the all-to-all operations over other communication, due to its blocking nature, and expert packing that reduces the all-to-all transfer size and incorporates optimizations to mitigate its overheads are introduced."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}