{
    "acronym": "919fa663be69d09718aaabdb7608cc43dd26cab1",
    "title": "Auditing an Automatic Grading Model with deep Reinforcement Learning",
    "seed_ids": [
        "bert"
    ],
    "s2id": "919fa663be69d09718aaabdb7608cc43dd26cab1",
    "abstract": "We explore the use of deep reinforcement learning to audit an automatic short answer grading (ASAG) model. Automatic grading may decrease the time burden of rating open-ended items for educators, but a lack of robust evaluation methods for these models can result in uncertainty of their quality. Current state-of-the-art ASAG models are configured to match human ratings from a training set, and researchers typically assess their quality with accuracy metrics that signify agreement between model and human scores. In this paper, we show that a high level of agreement to human ratings does not give sufficient evidence that an ASAG model is infallible. We train a reinforcement learning agent to revise student responses with the objective of achieving a high rating from an automatic grading model in the least number of revisions. By analyzing the agent's revised responses that achieve a high grade from the ASAG model but would not be considered a high scoring responses according to a scoring rubric, we discover ways in which the automated grader can be exploited, exposing shortcomings in the grading model.",
    "authors": [
        "Aubrey Condor",
        "Z. Pardos"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that a high level of agreement to human ratings does not give sufficient evidence that an ASAG model is infallible, and ways in which the automated grader can be exploited are discovered."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}