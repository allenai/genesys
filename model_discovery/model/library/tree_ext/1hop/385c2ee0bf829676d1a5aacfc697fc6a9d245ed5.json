{
    "acronym": "385c2ee0bf829676d1a5aacfc697fc6a9d245ed5",
    "title": "DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass",
    "seed_ids": [
        "gpt",
        "gpt2",
        "b3c73de96640ee858f83c3f0eda2a3d15d59b847",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "385c2ee0bf829676d1a5aacfc697fc6a9d245ed5",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) adds noise to gradients in back-propagation, safeguarding training data from privacy leakage, particularly membership inference. It fails to cover (inference-time) threats like embedding inversion and sensitive attribute inference. It is also costly in storage and computation when used to fine-tune large pre-trained language models (LMs). We propose DP-Forward, which directly perturbs embedding matrices in the forward pass of LMs. It satisfies stringent local DP requirements for training and inference data. To instantiate it using the smallest matrix-valued noise, we devise an analytic matrix Gaussian mechanism (aMGM) by drawing possibly non-i.i.d. noise from a matrix Gaussian distribution. We then investigate perturbing outputs from different hidden (sub-)layers of LMs with aMGM noises. Its utility on three typical tasks almost hits the non-private baseline and outperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves 3x time and memory costs compared to DP-SGD with the latest high-speed library. It also reduces the average success rates of embedding inversion and sensitive attribute inference by up to 88pp and 41pp, respectively, whereas DP-SGD fails.",
    "authors": [
        "Minxin Du",
        "Xiang Yue",
        "Sherman S. M. Chow",
        "Tianhao Wang",
        "Chenyu Huang",
        "Huan Sun"
    ],
    "venue": "Conference on Computer and Communications Security",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "DP-Forward is proposed, which directly perturbs embedding matrices in the forward pass of LMs and satisfies stringent local DP requirements for training and inference data, and saves 3x time and memory costs compared to DP-SGD with the latest high-speed library."
    },
    "citationCount": 32,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}