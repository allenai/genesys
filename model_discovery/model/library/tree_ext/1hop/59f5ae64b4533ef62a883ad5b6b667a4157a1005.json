{
    "acronym": "59f5ae64b4533ef62a883ad5b6b667a4157a1005",
    "title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models",
    "seed_ids": [
        "streamingllm",
        "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6",
        "c43e75cbbf06a51a128ed41e5558db19f4ebbb45",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "d1870f667cbd309df45a244c170d1d4ba36bac03",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "f51497f463566581874c941353dd9d80069c5b77",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28"
    ],
    "s2id": "59f5ae64b4533ef62a883ad5b6b667a4157a1005",
    "abstract": "Huge memory consumption has been a major bottleneck for deploying high-throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the transformer architecture consumes a significant amount of memory, especially when the number of layers is large for deep language models. In this paper, we propose a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput. Our experiments on large language models show that our method achieves up to 26$\\times$ higher throughput than standard transformers and competitive performance in language modeling and downstream tasks. In addition, our method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency. Our code is available at https://github.com/whyNLP/LCKV.",
    "authors": [
        "Haoyi Wu",
        "Kewei Tu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel method is proposed that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput, and is orthogonal to existing transformer memory-saving techniques."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}