{
    "acronym": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8",
    "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "seed_ids": [
        "bigbird",
        "sparsetransformer",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
        "2a31319e73d4486716168b65cdf7559baeda18ce"
    ],
    "s2id": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8",
    "abstract": "Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.",
    "authors": [
        "Han Shi",
        "Jiahui Gao",
        "Xiaozhe Ren",
        "Hang Xu",
        "Xiaodan Liang",
        "Zhenguo Li",
        "J. Kwok"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions, and a proof is provided showing that these diagonal elements can indeed be removed without deteriorating model performance."
    },
    "citationCount": 44,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}