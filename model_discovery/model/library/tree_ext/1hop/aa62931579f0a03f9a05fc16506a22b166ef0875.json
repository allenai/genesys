{
    "acronym": "aa62931579f0a03f9a05fc16506a22b166ef0875",
    "title": "Trainable Transformer in Transformer",
    "seed_ids": [
        "gpt2",
        "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b",
        "f5e9337477d7a9eb6267d0310549fdefafbb7fe2",
        "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
        "37ba9c33025fb31f25436010e12c65a0bafc0e1f",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "e82e3f4347674b75c432cb80604d38ee630d4bf6",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
        "cd63025532a62fa245a02ec05e32ac4d23089631",
        "3694381e74445a8b9f8cb8d373e39626e47191b5",
        "2ff74d426e712522030057624510c03713fa77ba",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "aa62931579f0a03f9a05fc16506a22b166ef0875",
    "abstract": "Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.",
    "authors": [
        "A. Panigrahi",
        "Sadhika Malladi",
        "Mengzhou Xia",
        "Sanjeev Arora"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models), and introduces innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulateand fine-Tune a 125 million parameter transformer model within a single forward pass."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}