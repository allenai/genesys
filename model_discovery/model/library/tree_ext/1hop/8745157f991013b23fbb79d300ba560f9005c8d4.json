{
    "acronym": "8745157f991013b23fbb79d300ba560f9005c8d4",
    "title": "Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models",
    "seed_ids": [
        "hopfield",
        "3ff7153fd6bd47d08084c7f50f8fd70026c126e7",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "82482585e94192b4e9913727e461f89cd08e9725",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "94bcd712aed610b8eaeccc57136d65ec988356f2"
    ],
    "s2id": "8745157f991013b23fbb79d300ba560f9005c8d4",
    "abstract": "Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.",
    "authors": [
        "Geon Yeong Park",
        "Jeongsol Kim",
        "Beomsu Kim",
        "Sang Wan Lee",
        "Jong-Chul Ye"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions."
    },
    "citationCount": 11,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}