{
    "acronym": "b76e98a0a023d37c6534aa2ead09c8ff595f0bae",
    "title": "A Robustly Optimized BERT Pre-training Approach with Post-training",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
        "256623ff025f36d343588bcd0b966c1fd26afcf8"
    ],
    "s2id": "b76e98a0a023d37c6534aa2ead09c8ff595f0bae",
    "abstract": null,
    "authors": [
        "Zhuang Liu",
        "Wayne Lin",
        "Yafei Shi",
        "Jun Zhao"
    ],
    "venue": "China National Conference on Chinese Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Proposed three-stage BERT paradigm is a more flexible and pluggable model where post-training approach is able to be plugged into other PLMs that are based on BERT."
    },
    "citationCount": 319,
    "influentialCitationCount": 55,
    "code": null,
    "description": null,
    "url": null
}