{
    "acronym": "f35e9db1bc60b4f504fb0e76087f077c796d9a5d",
    "title": "Sentiment analysis of student feedback using attention-based RNN and transformer embedding",
    "seed_ids": [
        "bert"
    ],
    "s2id": "f35e9db1bc60b4f504fb0e76087f077c796d9a5d",
    "abstract": "Sentiment analysis systems aim to assess people\u2019s opinions across various domains by collecting and categorizing feedback and reviews. In our study, researchers put forward a sentiment analysis system that leverages three distinct embedding techniques: automatic, global vectors (GloVe) for word representation, and bidirectional encoder representations from transformers (BERT). This system features an attention layer, with the best model chosen through rigorous comparisons. In developing the sentiment analysis model, we employed a hybrid dataset comprising students\u2019 feedback and comments. This dataset comprises 3,820 comments, including 2,773 from formal evaluations and 1,047 generated by ChatGPT and prompting engineering. Our main motivation for integrating generative AI was to balance both positive and negative comments. We also explored recurrent neural network (RNN), gated recurrent unit (GRU), long short-term memory (LSTM), and bidirectional long short-term memory (Bi-LSTM), with and without pre-trained GloVe embedding. These techniques produced F-scores ranging from 67% to 69%. On the other hand, the sentiment model based on BERT, particularly its KERAS implementation, achieved higher F-scores ranging from 83% to 87%. The Bi-LSTM architecture outperformed other models and the inclusion of an attention layer further enhanced the performance, resulting in F-scores of 89% and 88% from the Bi-LSTM-BERT sentiment models, respectively.",
    "authors": [
        "Imad Zyout",
        "Mo\u2019ath Zyout"
    ],
    "venue": "IAES International Journal of Artificial Intelligence (IJ-AI)",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A sentiment analysis system that leverages three distinct embedding techniques: automatic, global vectors (GloVe) for word representation, and bidirectional encoder representations from transformers (BERT) to balance both positive and negative comments is put forward."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}