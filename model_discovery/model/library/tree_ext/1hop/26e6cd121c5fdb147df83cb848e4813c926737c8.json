{
    "acronym": "26e6cd121c5fdb147df83cb848e4813c926737c8",
    "title": "The Hidden Attention of Mamba Models",
    "seed_ids": [
        "hyena",
        "2dda6da7375bf5e8bcf60f87b17ba10757f3bc57",
        "9da427202cc48370fd66359f5d72ff5ff3bc8b57",
        "57a6c75ebb987ea29a1f904de23f72451e095032",
        "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
        "3169a2478154e26fd7f63fdf43cf3a24f1007962",
        "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "95b2cb3f4a765014ce025afe5679660982554e6c",
        "745594bd0dc3e9dc86f74e100cd2c98ed36256c0",
        "8420fddf489bd7c5b822bd904aa11ff3742bfb78",
        "e6917b14918f90e8fb89ad4debebd3937e57a123",
        "59708496c88f173276a40d779a1f83bcfe2e7842",
        "7368c3cdf7cbed194e96dc4da53ed61f185e3d82",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "debbb47abc9fb757857f7c06aa86ca558d37c2d7",
        "2d01b6afbc86cba1cb895dbcd9396b13952bf0e5",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "d98b5c1d0f9a4e39dc79ea7a3f74e54789df5e13",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a"
    ],
    "s2id": "26e6cd121c5fdb147df83cb848e4813c926737c8",
    "abstract": "The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains, including NLP, long-range sequence processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.",
    "authors": [
        "Ameen Ali",
        "Itamar Zimerman",
        "Lior Wolf"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A third view is added and it is shown that selective SSMs can be viewed as attention-driven models and empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers."
    },
    "citationCount": 19,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}