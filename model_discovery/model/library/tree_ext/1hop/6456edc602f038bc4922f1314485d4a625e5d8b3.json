{
    "acronym": "6456edc602f038bc4922f1314485d4a625e5d8b3",
    "title": "CyBERT: Contextualized Embeddings for the Cybersecurity Domain",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "6456edc602f038bc4922f1314485d4a625e5d8b3",
    "abstract": "We present CyBERT, a domain-specific Bidirectional Encoder Representations from Transformers (BERT) model, fine-tuned with a large corpus of textual cybersecurity data. State-of-the-art natural language models that can process dense, fine-grained textual threat, attack, and vulnerability information can provide numerous benefits to the cybersecurity community. The primary contribution of this paper is providing the security community with an initial fine-tuned BERT model that can perform a variety of cybersecurity-specific downstream tasks with high accuracy and efficient use of resources. We create a cybersecurity corpus from open-source unstructured and semi-unstructured Cyber Threat Intelligence (CTI) data and use it to fine-tune a base BERT model with Masked Language Modeling (MLM) to recognize specialized cybersecurity entities. We evaluate the model using various downstream tasks that can benefit modern Security Operations Centers (SOCs). The fine-tuned CyBERT model outperforms the base BERT model in the domain-specific MLM evaluation. We also provide use-cases of CyBERT application in cybersecurity based downstream tasks.",
    "authors": [
        "P. Ranade",
        "Aritran Piplai",
        "A. Joshi",
        "Timothy W. Finin"
    ],
    "venue": "2021 IEEE International Conference on Big Data (Big Data)",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The primary contribution of this paper is providing the security community with an initial fine-tuned BERT model that can perform a variety of cybersecurity-specific downstream tasks with high accuracy and efficient use of resources."
    },
    "citationCount": 28,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}