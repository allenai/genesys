{
    "acronym": "6c761cfdb031701072582e434d8f64d436255da6",
    "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing",
    "seed_ids": [
        "gpt",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "7509c66a666e2e3f14bc8676b969b945ee6e136f",
        "981995fd64611f475179b280f4e9c241051ac185",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "2a218786f4615b82389f78472e7ff22e6ce57490",
        "09bfe057c9285577242636950c6835b8731a07fb",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "1c8aea2bfb61f4661b6907018a5a8bca390900dd",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "069e0d896da7c79faeee4cf057548d5da7ce885e",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "65f788fb964901e3f1149a0a53317535ca85ed7d",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "d56c1fc337fb07ec004dc846f80582c327af717c",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "b47381e04739ea3f392ba6c8faaf64105493c196",
        "2af4e8d14d03cd9031ae4a6b1ef39fce2ab3f504",
        "9405cc0d6169988371b2755e573cc28650d14dfe",
        "2a0870bc2ecfd17dfb1b96cc34613bb73bb4506a"
    ],
    "s2id": "6c761cfdb031701072582e434d8f64d436255da6",
    "abstract": "Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.",
    "authors": [
        "Katikapalli Subramanyam Kalyan",
        "A. Rajasekharan",
        "S. Sangeetha"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic."
    },
    "citationCount": 195,
    "influentialCitationCount": 12,
    "code": null,
    "description": null,
    "url": null
}