{
    "acronym": "8d41082767eafaa3eee9380aa1adad4c6452fe9e",
    "title": "Vim4Path: Self-Supervised Vision Mamba for Histopathology Images",
    "seed_ids": [
        "mamba",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "ca444821352a4bd91884413d8070446e2960715a",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51"
    ],
    "s2id": "8d41082767eafaa3eee9380aa1adad4c6452fe9e",
    "abstract": "Representation learning from Gigapixel Whole Slide Images (WSI) poses a significant challenge in computational pathology due to the complicated nature of tissue structures and the scarcity of labeled data. Multi-instance learning methods have addressed this challenge, leveraging image patches to classify slides utilizing pretrained models using Self-Supervised Learning (SSL) approaches. The performance of both SSL and MIL methods relies on the architecture of the feature encoder. This paper proposes leveraging the Vision Mamba (Vim) architecture, inspired by state space models, within the DINO framework for representation learning in computational pathology. We evaluate the performance of Vim against Vision Transformers (ViT) on the Camelyon16 dataset for both patch-level and slide-level classification. Our findings highlight Vim's enhanced performance compared to ViT, particularly at smaller scales, where Vim achieves an 8.21 increase in ROC AUC for models of similar size. An explainability analysis further highlights Vim's capabilities, which reveals that Vim uniquely emulates the pathologist workflow-unlike ViT. This alignment with human expert analysis highlights Vim's potential in practical diagnostic settings and contributes significantly to developing effective representation-learning algorithms in computational pathology. We release the codes and pretrained weights at \\url{https://github.com/AtlasAnalyticsLab/Vim4Path}.",
    "authors": [
        "Ali Nasiri-Sarvi",
        "Vincent Quoc-Huy Trinh",
        "Hassan Rivaz",
        "Mahdi S. Hosseini"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes leveraging the Vision Mamba (Vim) architecture, inspired by state space models, within the DINO framework for representation learning in computational pathology, and highlights Vim's enhanced performance compared to ViT, particularly at smaller scales."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}