{
    "acronym": "6151ee4af6a3fe78f2df7c605598cd9e02b23c5b",
    "title": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
    "seed_ids": [
        "gpt2",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "492a655a67e6ec7423a968cedb70eec0cdbc8e98",
        "7ade458d52d2dfe997b8a617a6b524bda12a619d",
        "1c8aea2bfb61f4661b6907018a5a8bca390900dd",
        "25db56fc85fe15625c3375064a35e908ba6dfd2a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6151ee4af6a3fe78f2df7c605598cd9e02b23c5b",
    "abstract": "While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \\textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \\textbf{DITTO} (Pseu\\underline{D}o-Repet\\underline{IT}ion Penaliza\\underline{T}i\\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method.",
    "authors": [
        "Jin Xu",
        "Xiaojiang Liu",
        "Jianhao Yan",
        "Deng Cai",
        "Huayang Li",
        "Jian Li"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A simple and effective training method, DITTO, where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data, which mitigates the repetition issue without sacrificing perplexity, and achieves better generation quality."
    },
    "citationCount": 50,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}