{
    "acronym": "a25370452533bf47549243e97852b9cdf7a0ee0e",
    "title": "Learning the Transformer Kernel",
    "seed_ids": [
        "synthesizer",
        "performer",
        "94e46e18d2628343a926acf6c3d0817e11d35d58",
        "e2ee883fca5f8f32a1dfa2dc06c742d57f2c38b9",
        "1900de2b966ca55ee5ca24ec94d5debe66e80c5b",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "a68ab49816d5729435c3d994b434c75c6f162da0",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0",
        "605c69f22a2be97e18478987c69be29d596a3dd2",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "c1a4278f969acfc6682a924e31b95e1ade9703ee",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "3694381e74445a8b9f8cb8d373e39626e47191b5",
        "c49c292e1fb1d215c88828a52134b7ccfa52be44"
    ],
    "s2id": "a25370452533bf47549243e97852b9cdf7a0ee0e",
    "abstract": "In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data driven framework for learning the kernel function in Transformers. Our framework approximates the Transformer kernel as a dot product between spectral feature maps and learns the kernel by learning the spectral distribution. This not only helps in learning a generic kernel end-to-end, but also reduces the time and space complexity of Transformers from quadratic to linear. We show that KERNELIZED TRANSFORMERS achieve performance comparable to existing efficient Transformer architectures, both in terms of accuracy as well as computational efficiency. Our study also demonstrates that the choice of the kernel has a substantial impact on performance, and kernel learning variants are competitive alternatives to fixed kernel Transformers, both in long as well as short sequence tasks.",
    "authors": [
        "Sankalan Pal Chowdhury",
        "Adamos Solomou",
        "Kumar Avinava Dubey",
        "Mrinmaya Sachan"
    ],
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "KERNELIZED TRANSFORMER is introduced, a generic, scalable, data driven framework for learning the kernel function in Transformers that approximates the Transformer kernel as a dot product between spectral feature maps and learns the kernel by learning the spectral distribution."
    },
    "citationCount": 12,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}