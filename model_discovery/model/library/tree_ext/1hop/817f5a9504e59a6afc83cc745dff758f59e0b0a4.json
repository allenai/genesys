{
    "acronym": "817f5a9504e59a6afc83cc745dff758f59e0b0a4",
    "title": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss",
    "seed_ids": [
        "gpt2",
        "brt",
        "2b8439f319dfa73df62ca8957ff6d0c1f3c7a73c",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "b0db25e317cf856f1ec1ca3df0e573d850ed4696",
        "cbbc2cc774c50b0b19922185b80e9ce90b7cd2f6",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "594d8e1696619f3cebb7c6bffdad8e0a5592f006",
        "d6a0dfd5f39222d8924b7727a0a49f81fa247d71",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "67ee20536c30a225b86902af2f091e28e5e19b40",
        "70557ea6b65846fc30729ceed224acd4ac64ca5d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "817f5a9504e59a6afc83cc745dff758f59e0b0a4",
    "abstract": "This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $11\\times 10^6$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.",
    "authors": [
        "Yuri Kuratov",
        "Aydar Bulatov",
        "Petr Anokhin",
        "Dmitry Sorokin",
        "Artyom Sorokin",
        "Mikhail Burtsev"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "BABILong is introduced, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts within extensive texts, demonstrating a significant improvement in the processing capabilities for long sequences."
    },
    "citationCount": 15,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}