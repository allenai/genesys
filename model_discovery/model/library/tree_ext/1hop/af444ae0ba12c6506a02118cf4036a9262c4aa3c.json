{
    "acronym": "af444ae0ba12c6506a02118cf4036a9262c4aa3c",
    "title": "Explanation Regeneration via Information Bottleneck",
    "seed_ids": [
        "gpt2",
        "538288d24bdad73d831dfed44b706958287ed318",
        "78d3955e30d99650d078ba3ce1a523745da31040",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "b304ef413e0c81d6139321e575e9eff10b6d9cb2",
        "882a7756d91e98a498af294c6dd7d54cbe4b490b",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "af444ae0ba12c6506a02118cf4036a9262c4aa3c",
    "abstract": "Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Due to the superior generative capacity of large pretrained language models, recent work built on prompt engineering enables explanation generation without specific training. However, explanation generated through single-pass prompting often lacks sufficiency and conciseness. To address this problem, we develop an information bottleneck method EIB to produce refined explanations that are sufficient and concise. Our approach regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained. Experiments on two out-of-domain tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.",
    "authors": [
        "Qintong Li",
        "Zhiyong Wu",
        "Lingpeng Kong",
        "Wei Bi"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops an information bottleneck method EIB that regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}