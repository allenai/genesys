{
    "acronym": "a979742220a88b1d32e1fbe72c41e8ba3007053c",
    "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
    "seed_ids": [
        "d3pms",
        "diffusionlm",
        "diffuseq",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "599bc7cfe98c2b57ddbe111412203a636da57be0",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "4d2a05140dd9bafaf035a846e7bda05f956304d2"
    ],
    "s2id": "a979742220a88b1d32e1fbe72c41e8ba3007053c",
    "abstract": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines.",
    "authors": [
        "Zhengfu He",
        "Tianxiang Sun",
        "Kuan Wang",
        "Xuanjing Huang",
        "Xipeng Qiu"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines and achieves significant improve- ment over existing diffusion models for text and previous generative masked language models in terms of perplexity and BLEU score."
    },
    "citationCount": 69,
    "influentialCitationCount": 8,
    "code": null,
    "description": null,
    "url": null
}