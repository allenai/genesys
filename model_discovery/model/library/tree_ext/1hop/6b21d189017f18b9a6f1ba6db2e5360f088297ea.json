{
    "acronym": "6b21d189017f18b9a6f1ba6db2e5360f088297ea",
    "title": "Taiwan Legal Longformer: A Longformer-LSTM Model for Effective Legal Case Retrieval",
    "seed_ids": [
        "longformer",
        "156a66da569b68ea776c5db38e033aadb4755971",
        "4e885ce070664e8eb3278659d0b20c7deed34029",
        "1e3e65e7773b7869d9bd7f5394b54199e48195e6",
        "f6245b3e6270e4dc2e279c4b728030523dffcff4",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6b21d189017f18b9a6f1ba6db2e5360f088297ea",
    "abstract": "Legal artificial intelligence (LegalAI) has been increasing applications within legal systems, propelled by advancements in natural language processing (NLP). Compared with general documents, legal case documents are typically long text sequences with intrinsic logical structures. However, most existing pre-trained language models (PLMs) struggle with understanding the long-distance dependencies between different sections within the complex structure of legal case documents. Another unique challenge in Taiwan Legal field is the lack of labeled datasets. This deficiency makes it difficult to accurately evaluate model performance. To date, models in Taiwan have yet to be specifically trained on legal case data. Given these challenges, this research aims to develop a model that can effectively understand and process long legal case documents and accurately retrieve similar legal cases. This research proposes a Longformer-LSTM model, Taiwan Legal Longformer, combining Longformer and LSTM in a sequenced manner. The Longformer, pre-trained on a Taiwanese legal case corpus, serves as the base model, processing input documents and generating context embeddings that capture local and global dependencies. These embeddings are then processed by the LSTM layer, adept at capturing long-distance dependencies in sequences. The model in this research is evaluated using our dataset and the Average Entropy of the Reason Clustering (AERC) metric, which is based on the assumption that case scenarios under the same reason of legal cases are similar. Compared with baseline models such as CKIP BERT Base Chinese and Longformer, our experimental results illustrate our model's advancements in handling similarity comparisons within extensive legal judgments.",
    "authors": [
        "Hsin Lee",
        "Hsuan Lee"
    ],
    "venue": "2023 5th International Workshop on Artificial Intelligence and Education (WAIE)",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This research proposes a Longformer-LSTM model, Taiwan Legal Longformer, combining Longformer and LSTM in a sequenced manner, and experimental results illustrate the model's advancements in handling similarity comparisons within extensive legal judgments."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}