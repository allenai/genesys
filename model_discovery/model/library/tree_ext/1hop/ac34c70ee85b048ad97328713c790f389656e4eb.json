{
    "acronym": "ac34c70ee85b048ad97328713c790f389656e4eb",
    "title": "Ecco: An Open Source Library for the Explainability of Transformer Language Models",
    "seed_ids": [
        "gpt",
        "ca4ecf116a9b97ce525a01f3f51117877688ddf5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ac34c70ee85b048ad97328713c790f389656e4eb",
    "abstract": "Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the transparency of Transformer-based language models, we present Ecco \u2013 an open-source library for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and interactively explore the inner mechanics of these models. This includes (1) gradient-based feature attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and examination tools for neuron activations in the under-explored Feed-Forward Neural Network sublayer of Transformer layers. (4) convenient examination of activation vectors via canonical correlation analysis (CCA), non-negative matrix factorization (NMF), and probing classifiers. We find that syntactic information can be retrieved from BERT\u2019s FFNN representations in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntactic information. Ecco is available at https://www.eccox.io/",
    "authors": [
        "J. Alammar"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Eco \u2013 an open-source library for the explainability of Transformer-based NLP models, which finds that syntactic information can be retrieved from BERT\u2019s FFNN representations in levels comparable to those in hidden state representations and that the model builds up syntactic Information in its hidden states even when intermediate FFNNs indicate diminished levels of syntacticInformation."
    },
    "citationCount": 43,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}