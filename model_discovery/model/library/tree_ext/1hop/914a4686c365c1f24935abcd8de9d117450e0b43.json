{
    "acronym": "914a4686c365c1f24935abcd8de9d117450e0b43",
    "title": "Shot Contrastive Self-Supervised Learning for Scene Boundary Detection",
    "seed_ids": [
        "linformer",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87"
    ],
    "s2id": "914a4686c365c1f24935abcd8de9d117450e0b43",
    "abstract": "Scenes play a crucial role in breaking the storyline of movies and TV episodes into semantically cohesive parts. However, given their complex temporal structure, finding scene boundaries can be a challenging task requiring large amounts of labeled training data. To address this challenge, we present a self-supervised shot contrastive learning approach (ShotCoL) to learn a shot representation that maximizes the similarity between nearby shots compared to randomly selected shots. We show how to apply our learned shot representation for the task of scene boundary detection to offer state-of-the-art performance on the MovieNet [33] dataset while requiring only ~25% of the training labels, using 9\u00d7 fewer model parameters and offering 7\u00d7 faster runtime. To assess the effectiveness of ShotCoL on novel applications of scene boundary detection, we take on the problem of finding timestamps in movies and TV episodes where video-ads can be inserted while offering a minimally disruptive viewing experience. To this end, we collected a new dataset called AdCuepoints with 3, 975 movies and TV episodes, 2.2 million shots and 19, 119 minimally disruptive ad cue-point labels. We present a thorough empirical analysis on this dataset demonstrating the effectiveness of ShotCoL for ad cue-points detection.",
    "authors": [
        "Shixing Chen",
        "Xiaohan Nie",
        "David D. Fan",
        "Dongqing Zhang",
        "Vimal Bhat",
        "Raffay Hamid"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown how to apply the learned shot representation for the task of scene boundary detection to offer state-of-the-art performance on the MovieNet dataset while requiring only ~25% of the training labels, using 9\u00d7 fewer model parameters and offering 7\u00d7 faster runtime."
    },
    "citationCount": 52,
    "influentialCitationCount": 13,
    "code": null,
    "description": null,
    "url": null
}