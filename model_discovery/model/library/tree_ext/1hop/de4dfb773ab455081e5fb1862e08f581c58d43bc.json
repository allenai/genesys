{
    "acronym": "de4dfb773ab455081e5fb1862e08f581c58d43bc",
    "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
    "seed_ids": [
        "gpt3",
        "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
        "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
        "2f130a320798dba1b13fa2e2822d42273e453038",
        "dab05c972fe4537e362b262b33dffc00de5f5311",
        "8ea24b1dbb3e690ebc64543c03f0552a6c1fb49d",
        "cb754310302086dfbbcd098263200e2a03f65874",
        "6327740b98005b5c9d090e5f1d474ff656d4174b",
        "9ada8fa11b1cdece31f253acae50b62df8d5f823",
        "94fec3a214e91e3a395c3f202cd8de06fe7231ec",
        "17606dbe67df42d973015fdd35f2807b0cafc15b",
        "16d83e930a4dab2d49f5d276838ddce79df3f787",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "d1101476c85ae324142440e9f568ecbf41625be5",
        "465471bb5bf1a945549d6291c2d23367966b4957",
        "9ab5c6644082b6fdbd6a2b0e6ae3527668244424",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "2811559b89a32fdf9facf24f4847f1d37393d158",
        "7ef43bacd43393ff116e6fcda6a52a6902e016d7",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
        "31ced335258047c2b6703165887d87048b8acc98",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1"
    ],
    "s2id": "de4dfb773ab455081e5fb1862e08f581c58d43bc",
    "abstract": "Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.",
    "authors": [
        "Tianyu Cui",
        "Yanling Wang",
        "Chuanpu Fu",
        "Yong Xiao",
        "Sijia Li",
        "Xinhao Deng",
        "Yunpeng Liu",
        "Qinglin Zhang",
        "Ziyi Qiu",
        "Peiyang Li",
        "Zhixing Tan",
        "Junwu Xiong",
        "Xinyu Kong",
        "Zujie Wen",
        "Ke Xu",
        "Qi Li"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A comprehensive taxonomy is proposed, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies, aiming to facilitate the risk assessment of LLM systems."
    },
    "citationCount": 22,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}