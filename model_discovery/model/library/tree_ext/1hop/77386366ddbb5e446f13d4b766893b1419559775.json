{
    "acronym": "77386366ddbb5e446f13d4b766893b1419559775",
    "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
    "seed_ids": [
        "bert",
        "bigbird",
        "78a0fb70b79116eb8d42c5951ced4f9efba513f0",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "34a4e6818d680875ff0bef9a76de0376118446d1"
    ],
    "s2id": "77386366ddbb5e446f13d4b766893b1419559775",
    "abstract": "Transformer-based models have demonstrated remarkable performance in various domains, including natural language processing, image processing and generative modeling. The most significant contributor to the successful performance of Transformer models is the self-attention mechanism, which allows for a comprehensive understanding of the interactions between tokens in the input sequence. However, there is a well-known scalability issue, the quadratic dependency (i.e. O(n^2)) of self-attention operations on the input sequence length n, making the handling of lengthy sequences challenging. To address this limitation, there has been a surge of research on efficient transformers, aiming to alleviate the quadratic dependency on the input sequence length. Among these, the Nystr\u00f6mformer, which utilizes the Nystr\u00f6m method to decompose the attention matrix, achieves superior performance in both accuracy and throughput. However, its landmark selection exhibits redundancy, and the model incurs computational overhead when calculating the pseudo-inverse matrix. We propose a novel Nystr\u00f6m method-based transformer, called Proxyformer. Unlike the traditional approach of selecting landmarks from input tokens, the Proxyformer utilizes trainable neural memory, called proxy tokens, for landmarks. By integrating contrastive learning, input injection, and a specialized dropout for the decomposed matrix, Proxyformer achieves top-tier performance for long sequence tasks in the Long Range Arena benchmark.",
    "authors": [
        "Sangho Lee",
        "Hayun Lee",
        "Dongkun Shin"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a novel Nystr\u00f6m method-based transformer, called Proxyformer, which achieves top-tier performance for long sequence tasks in the Long Range Arena benchmark by integrating contrastive learning, input injection, and a specialized dropout for the decomposed matrix."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}