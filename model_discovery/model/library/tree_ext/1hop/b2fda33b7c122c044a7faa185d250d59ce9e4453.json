{
    "acronym": "b2fda33b7c122c044a7faa185d250d59ce9e4453",
    "title": "Investigating Data Contamination for Pre-training Language Models",
    "seed_ids": [
        "gpt2",
        "cb754310302086dfbbcd098263200e2a03f65874",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b2fda33b7c122c044a7faa185d250d59ce9e4453",
    "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.",
    "authors": [
        "Minhao Jiang",
        "Ken Ziyu Liu",
        "Ming Zhong",
        "Rylan Schaeffer",
        "Siru Ouyang",
        "Jiawei Han",
        "Sanmi Koyejo"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models from scratch is explored, offering new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies."
    },
    "citationCount": 26,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}