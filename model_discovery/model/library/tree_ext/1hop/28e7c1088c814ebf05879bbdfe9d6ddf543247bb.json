{
    "acronym": "28e7c1088c814ebf05879bbdfe9d6ddf543247bb",
    "title": "Neural architecture impact on identifying temporally extended Reinforcement Learning tasks",
    "seed_ids": [
        "transformerxl",
        "59a916cdc943f0282908e6f3fa0360f4c5fb78d0"
    ],
    "s2id": "28e7c1088c814ebf05879bbdfe9d6ddf543247bb",
    "abstract": "Inspired by recent developments in attention models for image classification and natural language processing, we present various Attention based architectures in reinforcement learning (RL) domain, capable of performing well on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep Reinforcement learning techniques in various fields like robotics, gaming and healthcare, they suffer from a major drawback that neural networks are difficult to interpret. We try to get around this problem with the help of Attention based models. In Attention based models, extracting and overlaying of attention map onto images allows for direct observation of information used by agent to select actions and easier interpretation of logic behind the chosen actions. Our models in addition to playing well on gym-Atari environments, also provide insights on how agent perceives its environment. In addition, motivated by recent developments in attention based video-classification models using Vision Transformer, we come up with an architecture based on Vision Transformer, for image-based RL domain too. Compared to previous works in Vision Transformer, our model is faster to train and requires fewer computational resources. 3",
    "authors": [
        "Victor Vadakechirayath George"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents various Attention based architectures in reinforcement learning (RL) domain, capable of performing well on OpenAI Gym Atari-2600 game suite, and comes up with an architecture based on Vision Transformer, for image-based RL domain too."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}