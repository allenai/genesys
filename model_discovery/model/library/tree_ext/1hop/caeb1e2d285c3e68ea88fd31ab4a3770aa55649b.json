{
    "acronym": "caeb1e2d285c3e68ea88fd31ab4a3770aa55649b",
    "title": "Attention as an RNN",
    "seed_ids": [
        "transformer",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "99b0c3a18050889c591e1db6d51ca01298638437",
        "24bed5e91f2ac7d97584e04c25c860f3ec2b1299",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "8064d3873c646dc9ff949d72c54c634a906fc092",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "6f68e1bb253925d8431588555d3010419f322e04"
    ],
    "s2id": "caeb1e2d285c3e68ea88fd31ab4a3770aa55649b",
    "abstract": "The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \\textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \\textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \\textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.",
    "authors": [
        "Leo Feng",
        "Frederick Tung",
        "Hossein Hajimirsadeghi",
        "Mohamed Osama Ahmed",
        "Y. Bengio",
        "Greg Mori"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Aaren is introduced, an attention-based module that can not only be trained in parallel but also be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs)."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}