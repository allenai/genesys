{
    "acronym": "838446de88c800bf2186d148514fff3a43ccbe13",
    "title": "A Survey on Transformers in NLP with Focus on Efficiency",
    "seed_ids": [
        "transformer",
        "gpt",
        "gpt2",
        "gpt3",
        "bert",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "838446de88c800bf2186d148514fff3a43ccbe13",
    "abstract": "The advent of transformers with attention mechanisms and associated pre-trained models have revolutionized the field of Natural Language Processing (NLP). However, such models are resource-intensive due to highly complex architecture. This limits their application to resource-constrained environments. While choosing an appropriate NLP model, a major trade-off exists over choosing accuracy over efficiency and vice versa. This paper presents a commentary on the evolution of NLP and its applications with emphasis on their accuracy as-well-as efficiency. Following this, a survey of research contributions towards enhancing the efficiency of transformer-based models at various stages of model development along with hardware considerations has been conducted. The goal of this survey is to determine how current NLP techniques contribute towards a sustainable society and to establish a foundation for future research.",
    "authors": [
        "Wazib Ansar",
        "Saptarsi Goswami",
        "Amlan Chakrabarti"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A survey of research contributions towards enhancing the efficiency of transformer-based models at various stages of model development along with hardware considerations has been conducted to determine how current NLP techniques contribute towards a sustainable society and to establish a foundation for future research."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}