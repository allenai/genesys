{
    "acronym": "e10b7d87bdbeb935a04e94cc8670239faccbacdc",
    "title": "LLM Generative AI and Students\u2019 Exam Code Evaluation: Qualitative and Quantitative Analysis",
    "seed_ids": [
        "gpt3",
        "d3de0bac5703825796c240bfab8dc3c8e0a90222"
    ],
    "s2id": "e10b7d87bdbeb935a04e94cc8670239faccbacdc",
    "abstract": "Since the introduction of generative artificial intelligence (GAI) technology in the context of large language models (LLMs), it has been widely used for information extraction and/or extrapolation from different sources. In computer science education, a potential application of such technology is for automatic code review, i. e. shifting the burden of debugging non-compilable code, detecting overlooked optimization concerns such as poor memory management in code that otherwise passes automated tests, and other advanced tasks from a human grader to LLMs. However, LLMs are currently not capable of evaluating code or mathematical expressions with 100% reliability, i. e. beyond token pattern recognition and subsequent probabilistic answer generation. With that in mind, in this paper, we explore the risk of incorrect LLM code evaluation, both descriptive and numerical, as well as begin research on its mitigation and propose further work directions.",
    "authors": [
        "E. Smoli\u0107",
        "M. Paveli\u0107",
        "B. Boras",
        "Igor Mekterovic",
        "Tomislav Jagust"
    ],
    "venue": "International Convention on Information and Communication Technology, Electronics and Microelectronics",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper explores the risk of incorrect LLM code evaluation, both descriptive and numerical, as well as begin research on its mitigation and propose further work directions."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}