{
    "acronym": "43112afbeba63e6eb6b70cb9de36506af75804ba",
    "title": "Grape: Practical and Efficient Graph-based Executions for Dynamic Deep Neural Networks on GPUs",
    "seed_ids": [
        "gpt2",
        "ea9bdd6de80ea298f0a4ec9aaa7be44dc3ebc2ef",
        "c142bfce3b6849ee9d6aa0d5637ab28b170fd507",
        "ca46ca04b554fb9a7b1e2ab8345064e603898333",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "43112afbeba63e6eb6b70cb9de36506af75804ba",
    "abstract": "Achieving high performance in machine learning workloads is a crucial yet difficult task. To achieve high runtime performance on hardware platforms such as GPUs, graph-based executions such as CUDA graphs are often used to eliminate CPU runtime overheads by submitting jobs in the granularity of multiple kernels. However, many machine learning workloads, especially dynamic deep neural networks (DNNs) with varying-sized inputs or data-dependent control flows, face challenges when directly using CUDA graphs to achieve optimal performance. We observe that the use of graph-based executions poses three key challenges in terms of efficiency and even practicability: (1) Extra data movements when copying input values to graphs\u2019 placeholders. (2) High GPU memory consumption due to the numerous CUDA graphs created to efficiently support dynamic-shape workloads. (3) Inability to handle data-dependent control flows.To address those challenges, we propose Grape, a new graph compiler that enables practical and efficient graph-based executions for dynamic DNNs on GPUs. Grape comprises three key components: (1) an alias predictor that automatically removes extra data movements by leveraging code positions at the Python frontend, (2) a metadata compressor that efficiently utilizes the data redundancy in CUDA graphs\u2019 memory regions by compressing them, and (3) a predication rewriter that safely replaces control flows with predication contexts while preserving programs\u2019 semantics. The three components improve the efficiency and broaden the optimization scope of graph-based executions while allowing machine learning practitioners to program dynamic DNNs at the Python level with minimal source code changes.We evaluate Grape on state-of-the-art text generation (GPT-2, GPT-J) and speech recognition (Wav2Vec2) workloads, which include both training and inference, using real systems with modern GPUs. Our evaluation shows that Grape achieves up to 36.43\u00d7 less GPU memory consumption and up to 1.26\u00d7 better performance than prior works on graph-based executions that directly use CUDA graphs. Furthermore, Grape can optimize workloads that are impractical for prior works due to the three key challenges, achieving 1.78\u00d7 and 1.82\u00d7 better performance on GPT-J and Wav2Vec2 respectively than the original implementations that do not use graph-based executions.CCS CONCEPTS\u2022 Computing methodologies \u2192 Parallel computing methodologies; Machine learning; Artificial intelligence.",
    "authors": [
        "Bojian Zheng",
        "Cody Hao Yu",
        "Jie Wang",
        "Yaoyao Ding",
        "Yizhi Liu",
        "Yida Wang",
        "Gennady Pekhimenko"
    ],
    "venue": "Micro",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Grape, a new graph compiler that enables practical and efficient graph-based executions for dynamic DNNs on GPUs and allows machine learning practitioners to program dynamic DNNs at the Python level with minimal source code changes is proposed."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}