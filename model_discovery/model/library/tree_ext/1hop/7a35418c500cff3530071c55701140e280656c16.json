{
    "acronym": "7a35418c500cff3530071c55701140e280656c16",
    "title": "Paragraph-level Attention-Aware Inference for Multi-Document Neural Abstractive Summarization.",
    "seed_ids": [
        "memcompress",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7cc730da554003dda77796d2cb4f06da5dfd5592",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0"
    ],
    "s2id": "7a35418c500cff3530071c55701140e280656c16",
    "abstract": "Inspired by Google's Neural Machine Translation (NMT) that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage. Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model to learn the dependency between the optimal attention distribution and the source. By refining the vanilla beam search with the attention-aware mechanism, significant improvements on the quality of summaries could be observed. Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance.",
    "authors": [
        "Ye Ma",
        "Lu Zong"
    ],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study proposes an attention-aware inference algorithm for Neural Abstractive Summarization to regulate generated summaries to attend to source contents with the optimal coverage."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}