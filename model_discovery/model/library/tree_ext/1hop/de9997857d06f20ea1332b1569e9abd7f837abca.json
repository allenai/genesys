{
    "acronym": "de9997857d06f20ea1332b1569e9abd7f837abca",
    "title": "Delay Embedding Theory of Neural Sequence Models",
    "seed_ids": [
        "resurrectrnn",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "ca444821352a4bd91884413d8070446e2960715a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "35a9749df07a2ab97c51af4d260b095b00da7676"
    ],
    "s2id": "de9997857d06f20ea1332b1569e9abd7f837abca",
    "abstract": "To generate coherent responses, language models infer unobserved meaning from their input text sequence. One potential explanation for this capability arises from theories of delay embeddings in dynamical systems, which prove that unobserved variables can be recovered from the history of only a handful of observed variables. To test whether language models are effectively constructing delay embeddings, we measure the capacities of sequence models to reconstruct unobserved dynamics. We trained 1-layer transformer decoders and state-space sequence models on next-step prediction from noisy, partially-observed time series data. We found that each sequence layer can learn a viable embedding of the underlying system. However, state-space models have a stronger inductive bias than transformers-in particular, they more effectively reconstruct unobserved information at initialization, leading to more parameter-efficient models and lower error on dynamics tasks. Our work thus forges a novel connection between dynamical systems and deep learning sequence models via delay embedding theory.",
    "authors": [
        "Mitchell Ostrow",
        "Adam J. Eisen",
        "I. Fiete"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work trained 1-layer transformer decoders and state-space sequence models on next-step prediction from noisy, partially-observed time series data and found that each sequence layer can learn a viable embedding of the underlying system."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}