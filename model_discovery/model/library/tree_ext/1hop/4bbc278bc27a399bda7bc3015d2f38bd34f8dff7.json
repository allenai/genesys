{
    "acronym": "4bbc278bc27a399bda7bc3015d2f38bd34f8dff7",
    "title": "Large Language Model Adaptation for Financial Sentiment Analysis",
    "seed_ids": [
        "bert",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4bbc278bc27a399bda7bc3015d2f38bd34f8dff7",
    "abstract": "Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.",
    "authors": [
        "Pau Rodriguez Inserte",
        "Mariam Nakhl'e",
        "Raheel Qader",
        "Ga\u00ebtan Caillaut",
        "Jingshu Liu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis, and observes that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}