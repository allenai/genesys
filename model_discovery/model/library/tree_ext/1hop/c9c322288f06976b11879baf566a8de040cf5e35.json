{
    "acronym": "c9c322288f06976b11879baf566a8de040cf5e35",
    "title": "German Text Embedding Clustering Benchmark",
    "seed_ids": [
        "bert",
        "2c953a3c378b40dadf2e3fb486713c8608b8e282",
        "65f788fb964901e3f1149a0a53317535ca85ed7d"
    ],
    "s2id": "c9c322288f06976b11879baf566a8de040cf5e35",
    "abstract": "This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains. This benchmark is driven by the increasing use of clustering neural text embeddings in tasks that require the grouping of texts (such as topic modeling) and the need for German resources in existing benchmarks. We provide an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms. Results include strong performing mono- and multilingual models. Reducing the dimensions of embeddings can further improve clustering. Additionally, we conduct experiments with continued pre-training for German BERT models to estimate the benefits of this additional training. Our experiments suggest that significant performance improvements are possible for short text. All code and datasets are publicly available.",
    "authors": [
        "Silvan Wehrli",
        "Bert Arnrich",
        "Christopher Irrgang"
    ],
    "venue": "Conference on Natural Language Processing",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains and provides an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}