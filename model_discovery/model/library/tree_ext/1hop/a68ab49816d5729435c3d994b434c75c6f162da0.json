{
    "acronym": "a68ab49816d5729435c3d994b434c75c6f162da0",
    "title": "Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences",
    "seed_ids": [
        "nystromformer",
        "linformer",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "b0de1d5fe394226cec0a59d783ab739eb52da76f",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "0cd82dfae930ac4b57c0e959f744f2d10bf87649",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28"
    ],
    "s2id": "a68ab49816d5729435c3d994b434c75c6f162da0",
    "abstract": "Transformer-based models are not efficient in processing long sequences due to the quadratic space and time complexity of the self-attention modules. To address this limitation, Linformer and Informer reduce the quadratic complexity to linear (modulo logarithmic factors) via low-dimensional projection and row selection, respectively. These two models are intrinsically connected, and to understand their connection we introduce a theoretical framework of matrix sketching. Based on the theoretical analysis, we propose Skeinformer to accelerate self-attention and further improve the accuracy of matrix approximation to self-attention with column sampling, adaptive row normalization and pilot sampling reutilization. Experiments on the Long Range Arena benchmark demonstrate that our methods outperform alternatives with a consistently smaller time/space footprint.",
    "authors": [
        "Yifan Chen",
        "Qi Zeng",
        "Dilek Z. Hakkani-T\u00fcr",
        "Di Jin",
        "Heng Ji",
        "Yun Yang"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Skeinformer to accelerate self-attention and further improve the accuracy of matrix approximation to self-Attention with column sampling, adaptive row normalization and pilot sampling reutilization."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}