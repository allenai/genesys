{
    "acronym": "904b37ae6466cc90826adce53ed23b1eee3dbde0",
    "title": "TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages",
    "seed_ids": [
        "bert",
        "e25f6a60211aa74ecfde8001a5939ff206102de4",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "904b37ae6466cc90826adce53ed23b1eee3dbde0",
    "abstract": "The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.",
    "authors": [
        "Minsu Kim",
        "Jee-weon Jung",
        "Hyeongseop Rha",
        "Soumi Maiti",
        "Siddhant Arora",
        "Xuankai Chang",
        "Shinji Watanabe",
        "Y. Ro"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel Tri-Modal Translation model that translates between arbitrary modalities spanning speech, image, and text is proposed, and it outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}