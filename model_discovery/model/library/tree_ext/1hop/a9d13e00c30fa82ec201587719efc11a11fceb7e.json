{
    "acronym": "a9d13e00c30fa82ec201587719efc11a11fceb7e",
    "title": "Efficiency 360: Efficient Vision Transformers",
    "seed_ids": [
        "metaformer",
        "gmlp",
        "bf6ce546c589fa8054b3972b266532664914bd21",
        "6c22336873706b1cf5205ac6bd2432aa69d97821",
        "605c69f22a2be97e18478987c69be29d596a3dd2",
        "f75cddf2d42ed01b34686704eb3504becef67442",
        "71363797140647ebb3f540584de0a8758d2f7aa2",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "a9d13e00c30fa82ec201587719efc11a11fceb7e",
    "abstract": "Transformers are widely used for solving tasks in natural language processing, computer vision, speech, and music domains. In this paper, we talk about the efficiency of transformers in terms of memory (the number of parameters), computation cost (number of floating points operations), and performance of models, including accuracy, the robustness of the model, and fair \\&bias-free features. We mainly discuss the vision transformer for the image classification task. Our contribution is to introduce an efficient 360 framework, which includes various aspects of the vision transformer, to make it more efficient for industrial applications. By considering those applications, we categorize them into multiple dimensions such as privacy, robustness, transparency, fairness, inclusiveness, continual learning, probabilistic models, approximation, computational complexity, and spectral complexity. We compare various vision transformer models based on their performance, the number of parameters, and the number of floating point operations (FLOPs) on multiple datasets.",
    "authors": [
        "B. N. Patro",
        "Vijay Srinivas Agneeswaran"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper discusses the efficiency of transformers in terms of memory, computation cost, and performance of models, including accuracy, the robustness of the model, and fair \\&bias-free features, and introduces an efficient 360 framework, which includes various aspects of the vision transformer to make it more efficient for industrial applications."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}