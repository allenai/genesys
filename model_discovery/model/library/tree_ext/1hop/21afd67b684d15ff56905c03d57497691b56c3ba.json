{
    "acronym": "21afd67b684d15ff56905c03d57497691b56c3ba",
    "title": "Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model",
    "seed_ids": [
        "gpt",
        "31ced335258047c2b6703165887d87048b8acc98",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "21afd67b684d15ff56905c03d57497691b56c3ba",
    "abstract": "Style control, content preservation, and fluency determine the quality of text style transfer models. To train on a nonparallel corpus, several existing approaches aim to deceive the style discriminator with an adversarial loss. However, adversarial training significantly degrades fluency compared to the other two metrics. In this work, we explain this phenomenon using energy-based interpretation, and leverage a pretrained language model to improve fluency. Specifically, we propose a novel approach which applies the pretrained language model to the text style transfer framework by restructuring the discriminator and the model itself, allowing the generator and the discriminator to also take advantage of the power of the pretrained model. We evaluated our model on three public benchmarks GYAFC, Amazon, and Yelp and achieved state-of-the-art performance on the overall metrics.",
    "authors": [
        "Hojun Cho",
        "Dohee Kim",
        "Seung-kook Ryu",
        "chaeHun Park",
        "Hyungjong Noh",
        "J. Hwang",
        "Minseok Choi",
        "E. Choi",
        "J. Choo"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a novel approach which applies the pretrained language model to the text style transfer framework by restructuring the discrimator and the model itself, allowing the generator and the discriminator to also take advantage of the power of the Pretrained model."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}