{
    "acronym": "030690341bd25d05255f6ff69f8cf72a545a6a44",
    "title": "GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling",
    "seed_ids": [
        "gpt2",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "030690341bd25d05255f6ff69f8cf72a545a6a44",
    "abstract": "Multimodal event argument role labeling (EARL), a task that assigns a role for each event participant (object) in an image is a complex challenge. It requires reasoning over the entire image, the depicted event, and the interactions between various objects participating in the event. Existing models heavily rely on high-quality event-annotated training data to understand the event semantics and structures, and they fail to generalize to new event types and domains. In this paper, we propose GenEARL, a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task. Specifically, GenEARL comprises two stages of generative prompting with a frozen vision-language model (VLM) and a frozen large language model (LLM). First, a generative VLM learns the semantics of the event argument roles and generates event-centric object descriptions based on the image. Subsequently, a LLM is prompted with the generated object descriptions with a predefined template for EARL (i.e., assign an object with an event argument role). We show that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4% and 14.2% accuracy for zero-shot EARL on the M2E2 and SwiG datasets, respectively. In addition, we outperform CLIP-Event by 22% precision on M2E2 dataset. The framework also allows flexible adaptation and generalization to unseen domains.",
    "authors": [
        "Hritik Bansal",
        "Po-Nien Kung",
        "P. Brantingham",
        "Kai-Wei Chang",
        "Nanyun Peng",
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "A. Mensch",
        "Katie Millican",
        "Malcolm Reynolds",
        "Roman Ring",
        "Eliza Rutherford",
        "Serkan Cabi",
        "Tengda Han",
        "Zhitao Gong",
        "Sina Samangooei",
        "Marianne Monteiro",
        "Jacob Menick",
        "Sebastian Borgeaud",
        "Andy Brock",
        "Aida Nematzadeh",
        "Sahand Sharifzadeh",
        "M. Binkowski",
        "Ricardo Barreira",
        "O. Vinyals",
        "Andrew Zisserman",
        "Karen Simonyan. 2022",
        "Anas Awadalla",
        "Irena Gao",
        "Josh Gardner",
        "Jack Hes-sel",
        "Yusuf Hanafy",
        "Wanrong Zhu",
        "K. Marathe",
        "Yonatan Bitton",
        "S. Gadre",
        "J. Jitsev",
        "Pang Simon Kornblith",
        "Wei Koh",
        "Gabriel Ilharco",
        "Mitchell Wortsman",
        "Ludwig Schmidt. 2023",
        "Open-flamingo Hritik",
        "Da Bansal",
        "Masoud Yin",
        "Monajatipoor Kai-Wei",
        "Chang",
        "Philipp Blandfort",
        "Desmond U Patton",
        "William R. Frey",
        "Svebor Karaman",
        "Surabhi Bhargava",
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Junqi Zhao",
        "Weisheng Wang",
        "Boyang Li",
        "Pascale Fung",
        "Steven Hoi. 2023",
        "George Doddington",
        "Alexis Mitchell",
        "Mark A Przy-bocki",
        "lan A. Ramshaw",
        "Stephanie Strassel",
        "Ralph M Weischedel. 2004",
        "Salvatore Giorgi",
        "Sharath Chandra Guntuku",
        "McKenzie Himelein-Wachowiak",
        "Amy Kwarteng",
        "Sy Hwang",
        "Muhammad Rahman",
        "Brenda L. Curtis",
        "Tao Gong",
        "Chengqi Lyu",
        "Shilong Zhang",
        "Yudong Wang",
        "Miao Zheng",
        "Qian Zhao",
        "Kuikun Liu",
        "Wenwei Zhang",
        "Ping Luo",
        "Kai Chen",
        "Multimodal-gpt",
        "Jordan Hoffmann",
        "Arthur Men-sch",
        "Elena Buchatskaya",
        "Trevor Cai",
        "Eliza Ruther-ford",
        "Diego de",
        "Las Casas",
        "Lisa Anne Hendricks",
        "Johannes Welbl",
        "Aidan Clark",
        "I-Hung Hsu",
        "Kuan-Hao Huang",
        "Elizabeth Boschee",
        "Scott Miller",
        "Premkumar Natarajan",
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Ves Stoyanov",
        "Luke Zettlemoyer. 2019",
        "Bart"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "GenEARL is a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task and outperforms the contrastive pretraining baseline."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}