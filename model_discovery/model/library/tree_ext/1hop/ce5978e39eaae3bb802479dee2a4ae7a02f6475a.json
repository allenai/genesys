{
    "acronym": "ce5978e39eaae3bb802479dee2a4ae7a02f6475a",
    "title": "DF-Conformer: Integrated Architecture of Conv-Tasnet and Conformer Using Linear Complexity Self-Attention for Speech Enhancement",
    "seed_ids": [
        "performer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04"
    ],
    "s2id": "ce5978e39eaae3bb802479dee2a4ae7a02f6475a",
    "abstract": "Single-channel speech enhancement (SE) is an important task in speech processing. A widely used framework combines an anal-ysis/synthesis filterbank with a mask prediction network, such as the Conv-TasNet architecture. In such systems, the denoising performance and computational efficiency are mainly affected by the structure of the mask prediction network. In this study, we aim to improve the sequential modeling ability of Conv-TasNet architectures by integrating Conformer layers into a new mask prediction network. To make the model computationally feasible, we extend the Conformer using linear complexity attention and stacked 1-D dilated depthwise convolution layers. We trained the model on 3,396 hours of noisy speech data, and show that (i) the use of linear complexity attention avoids high computational complexity, and (ii) our model achieves higher scale-invariant signal-to-noise ratio than the improved time-dilated convolution network (TDCN++), an extended version of Conv-TasNet.",
    "authors": [
        "Yuma Koizumi",
        "Shigeki Karita",
        "Scott Wisdom",
        "Hakan Erdogan",
        "J. Hershey",
        "Llion Jones",
        "M. Bacchiani"
    ],
    "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study aims to improve the sequential modeling ability of Conv-TasNet architectures by integrating Conformer layers into a new mask prediction network, and extends the Conformer using linear complexity attention and stacked 1-D dilated depthwise convolution layers."
    },
    "citationCount": 33,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}