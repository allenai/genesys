{
    "acronym": "d602cab8086e2524d5cf2b4c2cfbe815c92dbd78",
    "title": "Recurrently Controlled Recurrent Networks",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "d602cab8086e2524d5cf2b4c2cfbe815c92dbd78",
    "abstract": "Recurrent neural networks (RNNs) such as long short-term memory and gated recurrent units are pivotal building blocks across a broad spectrum of sequence modeling problems. This paper proposes a recurrently controlled recurrent network (RCRN) for expressive and powerful sequence encoding. More concretely, the key idea behind our approach is to learn the recurrent gating functions using recurrent networks. Our architecture is split into two components - a controller cell and a listener cell whereby the recurrent controller actively influences the compositionality of the listener cell. We conduct extensive experiments on a myriad of tasks in the NLP domain such as sentiment analysis (SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment classification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading comprehension (NarrativeQA). Across all 26 datasets, our results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture. Additionally, RCRN achieves state-of-the-art results on several well-established datasets.",
    "authors": [
        "Yi Tay",
        "Anh Tuan Luu",
        "S. Hui"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2018,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Across all 26 datasets, the results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiL STMs, suggesting that the controller architecture might be a suitable replacement for the widely adopted stacked architecture."
    },
    "citationCount": 16,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}