{
    "acronym": "037f360a564d81015ad2d404a6bb0cd3a1468088",
    "title": "Balanced Meta Learning and Diverse Sampling for Lifelong Task-Oriented Dialogue Systems",
    "seed_ids": [
        "gpt2",
        "553703db1b3e54e957ed91bad952ff3ba4f59bd5",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "037f360a564d81015ad2d404a6bb0cd3a1468088",
    "abstract": "In real-world scenarios, it is crucial to build a lifelong taskoriented dialogue system (TDS) that continually adapts to new knowledge without forgetting previously acquired experiences. Existing approaches mainly focus on mitigating the catastrophic forgetting in lifelong TDS. However, the transfer ability to generalize the accumulated old knowledge to new tasks is underexplored. In this paper, we propose a two-stage lifelong task-oriented dialogue generation method to mitigate catastrophic forgetting and encourage knowledge transfer simultaneously, inspired by the learning process. In the first stage, we learn task-specific masks which adaptively preserve the knowledge of each visited task so as to mitigate catastrophic forgetting. In this stage, we are expected to learn the task-specific knowledge which is tailored for each task. In the second stage, we bring the knowledge from the encountered tasks together and understand thoroughly. To this end, we devise a balanced meta learning strategy for both forward and backward knowledge transfer in the lifelong learning process. In particular, we perform meta-update with a meta-test set sampled from the current training data for forward knowledge transfer. In addition, we employ an uncertainty-based sampling strategy to select and store representative dialogue samples into episodic memory and perform meta-update with a meta-test set sampled from the memory for backward knowledge transfer. With extensive experiments on 29 tasks, we show that MetaLTDS outperforms the strong baselines in terms of both effectiveness and efficiency. For reproducibility, we submit our code at: https: //github.com/travis-xu/MetaLTDS.",
    "authors": [
        "Qiancheng Xu",
        "Min Yang",
        "Ruifeng Xu"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a two-stage lifelong task-oriented dialogue generation method to mitigate catastrophic forgetting and encourage knowledge transfer simultaneously, inspired by the learning process and devise a balanced meta learning strategy for both forward and backward knowledge transfer in the lifelong learning process."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}