{
    "acronym": "f78fe02f681a0a9a6867b007bd39e3884de64a91",
    "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
    "seed_ids": [
        "gpt2",
        "8add69e155596bc128df70e1ddd5a41c68698399",
        "36c50e6638dddc8324eef9bfa064bfcab80cbef4",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "f78fe02f681a0a9a6867b007bd39e3884de64a91",
    "abstract": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.",
    "authors": [
        "Hyunwoo Kim",
        "Jack Hessel",
        "Liwei Jiang",
        "Peter West",
        "Ximing Lu",
        "Youngjae Yu",
        "Pei Zhou",
        "Ronan Le Bras",
        "Malihe Alikhani",
        "Gunhee Kim",
        "Maarten Sap",
        "Yejin Choi"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents SODA: the first publicly available, million-scale high-quality social dialogue dataset, and trains COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models."
    },
    "citationCount": 108,
    "influentialCitationCount": 16,
    "code": null,
    "description": null,
    "url": null
}