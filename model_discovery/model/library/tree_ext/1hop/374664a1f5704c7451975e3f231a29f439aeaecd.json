{
    "acronym": "374664a1f5704c7451975e3f231a29f439aeaecd",
    "title": "To Transformers and Beyond: Large Language Models for the Genome",
    "seed_ids": [
        "gpt2",
        "gpt3",
        "hyena",
        "flashattn",
        "f0f0b680ac74960922634e3da5cf1e6f6883c840",
        "717102ed1022827869fc003052fefa0c7c8f2f57",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "0f4780f3f42dbe9755d54495ae17244cc88a7483",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "a883336e5c2e9f46f5012343227a6be4671c9ca0",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "ec139916edd6feb9b3cb3a0325ca96e21dbb0147",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "374664a1f5704c7451975e3f231a29f439aeaecd",
    "abstract": "In the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational challenges. This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the transformer architecture, in genomics. Building on the foundation of traditional convolutional neural networks and recurrent neural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics. Additionally, we contemplate the future of genomic modeling beyond the transformer architecture based on current trends in research. The paper aims to serve as a guide for computational biologists and computer scientists interested in LLMs for genomic data. We hope the paper can also serve as an educational introduction and discussion for biologists to a fundamental shift in how we will be analyzing genomic data in the future.",
    "authors": [
        "Micaela Elisa Consens",
        "Cameron Dufault",
        "Michael Wainberg",
        "Duncan Forster",
        "Mehran Karimzadeh",
        "Hani Goodarzi",
        "Fabian J. Theis",
        "Alan Moses",
        "Bo Wang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the transformer architecture, in genomics, and explores both the strengths and limitations of transformers and other LLMs for genomics."
    },
    "citationCount": 11,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}