{
    "acronym": "6c9a8bad0f225601e0b76101bdcb93a00e2f7f23",
    "title": "A study on surprisal and semantic relatedness for eye-tracking data prediction",
    "seed_ids": [
        "gpt2",
        "db6d3dfebaf1ee8b9aeb97b83d8697e5bd62a9b3",
        "97a531f96a8d9a4c1d379637daf7dcc714db1b35",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6c9a8bad0f225601e0b76101bdcb93a00e2f7f23",
    "abstract": "Previous research in computational linguistics dedicated a lot of effort to using language modeling and/or distributional semantic models to predict metrics extracted from eye-tracking data. However, it is not clear whether the two components have a distinct contribution, with recent studies claiming that surprisal scores estimated with large-scale, deep learning-based language models subsume the semantic relatedness component. In our study, we propose a regression experiment for estimating different eye-tracking metrics on two English corpora, contrasting the quality of the predictions with and without the surprisal and the relatedness components. Different types of relatedness scores derived from both static and contextual models have also been tested. Our results suggest that both components play a role in the prediction, with semantic relatedness surprisingly contributing also to the prediction of function words. Moreover, they show that when the metric is computed with the contextual embeddings of the BERT model, it is able to explain a higher amount of variance.",
    "authors": [
        "Lavinia Salicchi",
        "Emmanuele Chersoni",
        "Alessandro Lenci"
    ],
    "venue": "Frontiers in Psychology",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The results suggest that both components play a role in the prediction, with semantic relatedness surprisingly contributing also to the prediction of function words and when the metric is computed with the contextual embeddings of the BERT model, it is able to explain a higher amount of variance."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}