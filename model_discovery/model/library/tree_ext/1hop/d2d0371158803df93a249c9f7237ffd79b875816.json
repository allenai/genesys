{
    "acronym": "d2d0371158803df93a249c9f7237ffd79b875816",
    "title": "Sparse Modular Activation for Efficient Sequence Modeling",
    "seed_ids": [
        "s4",
        "mega",
        "flash",
        "s5",
        "01e721df7fcf8b664deb2cdc97ff58d65553af6b",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "690a37a2ba67b44b012bf9aa92e6a7f7670f487f",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "ca444821352a4bd91884413d8070446e2960715a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "64a29bee2e1ad29547d590a3cc26274f4c537145",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f4238bd2385a52413ccbacfd9e409a650235bd13"
    ],
    "s2id": "d2d0371158803df93a249c9f7237ffd79b875816",
    "abstract": "Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.",
    "authors": [
        "Liliang Ren",
        "Yang Liu",
        "Shuo Wang",
        "Yichong Xu",
        "Chenguang Zhu",
        "Chengxiang Zhai"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models."
    },
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}