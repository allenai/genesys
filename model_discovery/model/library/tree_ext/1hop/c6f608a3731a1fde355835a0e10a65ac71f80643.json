{
    "acronym": "c6f608a3731a1fde355835a0e10a65ac71f80643",
    "title": "Towards Full-line Code Completion with Neural Language Models",
    "seed_ids": [
        "gpt",
        "40df572b0fbeae0f3db9b364be838c6467d189f2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c6f608a3731a1fde355835a0e10a65ac71f80643",
    "abstract": "A code completion system suggests future code elements to developers given a partially-complete code snippet. Code completion is one of the most useful features in Integrated Development Environments (IDEs). Currently, most code completion techniques predict a single token at a time. In this paper, we take a further step and discuss the probability of directly completing a whole line of code instead of a single token. We believe suggesting longer code sequences can further improve the efficiency of developers. Recently neural language models have been adopted as a preferred approach for code completion, and we believe these models can still be applied to full-line code completion with a few improvements. We conduct our experiments on two real-world python corpora and evaluate existing neural models based on source code tokens or syntactical actions. The results show that neural language models can achieve acceptable results on our tasks, with significant room for improvements.",
    "authors": [
        "Wenhan Wang",
        "Sijie Shen",
        "Ge Li",
        "Zhi Jin"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper conducts experiments on two real-world python corpora and evaluates existing neural models based on source code tokens or syntactical actions and shows that neural language models can achieve acceptable results on the authors' tasks, with significant room for improvements."
    },
    "citationCount": 13,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}