{
    "acronym": "b0e3daac5e94a7442d5e624714a05681b628594e",
    "title": "How well can Hippos learn? A Novel Foray into the In-Context Learning Capabilities of H3",
    "seed_ids": [
        "s4",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "a30ac45ac5b7bd2148d3fb80ee7f3c29724e3170",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b0e3daac5e94a7442d5e624714a05681b628594e",
    "abstract": "Hungry Hungry Hippos (H3) is a new class of state-space model (SSM) developed by Dao & Fu et al for language modeling tasks, which has outperformed trans-formers on many major benchmarks including SuperGLUE [1]. We investigate the in-context learning (ICL) capacity of H3 as an emergent scaling behavior. In particular, given the theoretically unbounded context window of this novel architecture, we analyze performance in the many-shot domain which cannot be tested for transformer architectures. We compare performance on the GLUE Quora Question Pairs (QQP) task between H3 and GPT-2[2]. Our results demonstrate novel emergent phenomena in ICL for H3 relative to transformers. We find that, contrary to scaling with transformers in the few-shot domain, increases in shot size do not translate consistently to increases in ICL performance in the many-shot domain and tend to lead to poorer performance potentially caused by model overfitting. Furthermore, our results suggest that H3 models are significantly more resistant to class imbalances in ICL. In imbalanced classification tasks, H3 is able to achieve robust accuracy and F1 scores where transformers fail, and generally produce results similarly to or better than transformers of the same size for ICL. In noisy and non-noisy label settings, we uncover fundamental differences in the way H3 models predict class probabilities relative to transformer models, which increases as shot size scales up. Finally, we find that H3 models are less permutation invariant relative to transformers and that the ordering of examples in an ICL shot can affect the robustness of performance. This study is a first-of-its-kind analyzing emergent capabilities in state space models for language applications, especially relative to state-of-the-art transformer models of comparable size. We find that H3 models yield performance advantages in many regards, warranting further investigation as an alternative to transformers in the ICL setting",
    "authors": [
        "Dhruv Pai",
        "Andres Carranza",
        "Shreyas Kar",
        "Dan Fu",
        "Isabel Papadimitriou",
        "Christopher Manning"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study investigates the in-context learning (ICL) capacity of H3 as an emergent scaling behavior and finds that H3 models yield performance advantages in many regards, warranting further investigation as an alternative to transformers in the ICL setting."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}