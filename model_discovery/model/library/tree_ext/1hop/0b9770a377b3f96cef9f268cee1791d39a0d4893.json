{
    "acronym": "0b9770a377b3f96cef9f268cee1791d39a0d4893",
    "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
    "seed_ids": [
        "gpt2",
        "d3pms",
        "diffusionlm",
        "analogbits",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "4a6a65968a8eb8c09ffb57a7774ddabb596565b1",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
        "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "0b9770a377b3f96cef9f268cee1791d39a0d4893",
    "abstract": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM\u2014a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
    "authors": [
        "Xiaochuang Han",
        "Sachin Kumar",
        "Yulia Tsvetkov"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates, and is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation."
    },
    "citationCount": 52,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}