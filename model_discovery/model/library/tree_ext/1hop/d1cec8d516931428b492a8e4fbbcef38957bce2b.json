{
    "acronym": "d1cec8d516931428b492a8e4fbbcef38957bce2b",
    "title": "Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning",
    "seed_ids": [
        "longformer",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "d1cec8d516931428b492a8e4fbbcef38957bce2b",
    "abstract": "In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, layout, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and state-of-the-art baselines. We discuss the current limitations and next steps for our work.",
    "authors": [
        "Subhojeet Pramanik",
        "Shashank Mujumdar",
        "Hima Patel"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre- training tasks to learn a generic document representation and designs the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, layout, and image dimensions."
    },
    "citationCount": 25,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}