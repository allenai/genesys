{
    "acronym": "559415fa81069d8d859c0d7f479601f1fc8aca8f",
    "title": "Do chemical language models provide a better compound representation?",
    "seed_ids": [
        "bert"
    ],
    "s2id": "559415fa81069d8d859c0d7f479601f1fc8aca8f",
    "abstract": "In recent years, several chemical language models have been developed, inspired by the success of protein language models and advancements in natural language processing. In this study, we explore whether pre-training a chemical language model on billion-scale compound datasets, such as Enamine and ZINC20, can lead to improved compound representation in the drug space. We compare the learned representations of these models with the de facto standard compound representation, and evaluate their potential application in drug discovery and development by benchmarking them on biophysics, physiology, and physical chemistry datasets. Our findings suggest that the conventional masked language modeling approach on these extensive pre-training datasets is insufficient in enhancing compound representations. This highlights the need for additional physicochemical inductive bias in the modeling beyond scaling the dataset size.",
    "authors": [
        "Mirko Torrisi",
        "Saeid Asadollahi",
        "Antonio de la Vega de Le\u00f3n",
        "Kai Wang",
        "Wilbert Copeland"
    ],
    "venue": "bioRxiv",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study explores whether pre-training a chemical language model on billion-scale compound datasets, such as Enamine and ZINC20, can lead to improved compound representation in the drug space, and suggests that the conventional masked language modeling approach is insufficient in enhancing compound representations."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}