{
    "acronym": "19b7562dcc0d8715f032162e27bab02df199ab54",
    "title": "Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders",
    "seed_ids": [
        "bert",
        "06cd65936fdb2d2a5d51ca7fd612c48fbffc228e",
        "e2587eddd57bc4ba286d91b27c185083f16f40ee"
    ],
    "s2id": "19b7562dcc0d8715f032162e27bab02df199ab54",
    "abstract": "The injection of syntactic information in Variational AutoEncoders (VAEs) has been shown to result in an overall improvement of performances and generalisation. An effective strategy to achieve such a goal is to separate the encoding of distributional semantic features and syntactic structures into heterogeneous latent spaces via multi-task learning or dual encoder architectures. However, existing works employing such techniques are limited to LSTM-based VAEs. In this paper, we investigate latent space separation methods for structural syntactic injection in Transformer-based VAE architectures (i.e., Optimus). Specifically, we explore how syntactic structures can be leveraged in the encoding stage through the integration of graph-based and sequential models, and how multiple, specialised latent representations can be injected into the decoder's attention mechanism via low-rank operators. Our empirical evaluation, carried out on natural language sentences and mathematical expressions, reveals that the proposed end-to-end VAE architecture can result in a better overall organisation of the latent space, alleviating the information loss occurring in standard VAE setups, resulting in enhanced performances on language modelling and downstream generation tasks.",
    "authors": [
        "Yingji Zhang",
        "Marco Valentino",
        "Danilo S. Carvalho",
        "Ian Pratt-Hartmann",
        "Andr\u00e9 Freitas"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The empirical evaluation reveals that the proposed end-to-end VAE architecture can result in a better overall organisation of the latent space, alleviating the information loss occurring in standard VAE setups, resulting in enhanced performances on language modelling and downstream generation tasks."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}