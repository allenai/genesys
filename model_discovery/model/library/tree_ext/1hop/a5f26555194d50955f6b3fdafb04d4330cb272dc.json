{
    "acronym": "a5f26555194d50955f6b3fdafb04d4330cb272dc",
    "title": "A Survey on Human Preference Learning for Large Language Models",
    "seed_ids": [
        "gpt3",
        "66e7edf09589527ebb58418632418758cee668cd",
        "0f7308fbcae43d22813f70c334c2425df0b1cce1",
        "2be910eb19f2f8f2e8038d2a835bc48f868ccbf1",
        "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "a3184d40d390793232c99c89b57b8f65c16320b2",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a5f26555194d50955f6b3fdafb04d4330cb272dc",
    "abstract": "The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.",
    "authors": [
        "Ruili Jiang",
        "Kehai Chen",
        "Xuefeng Bai",
        "Zhixuan He",
        "Juntao Li",
        "Muyun Yang",
        "Tiejun Zhao",
        "Liqiang Nie",
        "Min Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This survey reviews the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}