{
    "acronym": "fe5170b641e68bada2db64fe6177d57aa529343b",
    "title": "Retaining Semantics in Image to Music Conversion",
    "seed_ids": [
        "gpt2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fe5170b641e68bada2db64fe6177d57aa529343b",
    "abstract": "We propose a method for generating music from a given image through three stages of translation, from image to caption, caption to lyrics, and lyrics to instrumental music, which forms the content to be combined with a given style. We train our proposed model, which we call BGT (BLIP-GPT2-TeleMelody), on two open-source datasets, one containing over 200,000 labeled images, and another containing more than 175,000 MIDI music files. In contrast with pixel level translation, the BGT model retains the semantics of the input image. We verify our claim through a user study in which participants were asked to match input images with generated music without access to the intermediate caption and lyrics. The results show that, while the matching rate among participants with low music expertise is essentially random, the rate among those with composition experience is significantly high, which strongly indicates that some semantic content of the input image is retained in the generated music.",
    "authors": [
        "Zeyu Xiong",
        "Pei-Chun Lin",
        "Amin Farjudian"
    ],
    "venue": "IEEE International Symposium on Multimedia",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The results show that, while the matching rate among participants with low music expertise is essentially random, the rate among those with composition experience is significantly high, which strongly indicates that some semantic content of the input image is retained in the generated music."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}