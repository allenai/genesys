{
    "acronym": "6e33594cb0af7def21d805046be3957ab9461cf4",
    "title": "Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation",
    "seed_ids": [
        "transformer",
        "bert",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf"
    ],
    "s2id": "6e33594cb0af7def21d805046be3957ab9461cf4",
    "abstract": "The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation (NMT). Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the \u201cAlign-to-Distill\u201d (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module (AAM) in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De\u2192Dsb and WMT-2014 En\u2192De, respectively, compared to Transformer baselines.The code and data are available at https://github.com/ncsoft/Align-to-Distill.",
    "authors": [
        "Heegon Jin",
        "Seonil Son",
        "Jemin Park",
        "Youngseok Kim",
        "Hyungjong Noh",
        "Yeonsoo Lee"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The \u201cAlign-to-Distill\u201d (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training, is introduced."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}