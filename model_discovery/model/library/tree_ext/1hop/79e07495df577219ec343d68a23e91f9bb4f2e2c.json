{
    "acronym": "79e07495df577219ec343d68a23e91f9bb4f2e2c",
    "title": "Dynamic Grained Encoder for Vision Transformers",
    "seed_ids": [
        "performer",
        "837ac4ed6825502f0460caec45e12e734c85b113",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "79e07495df577219ec343d68a23e91f9bb4f2e2c",
    "abstract": "Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Specifically, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a fine-grained representation in discriminative regions while keeping high efficiency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40%-60% while maintaining comparable performance on image classification. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack.",
    "authors": [
        "Lin Song",
        "Songyang Zhang",
        "Songtao Liu",
        "Zeming Li",
        "Xuming He",
        "Hongbin Sun",
        "Jian Sun",
        "Nanning Zheng"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Dynamic Grained Encoder for vision transformers is proposed, which can adaptively assign a suitable number of queries to each spatial region and achieves a fine-grained representation in discriminative regions while keeping high efficiency."
    },
    "citationCount": 25,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}