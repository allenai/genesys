{
    "acronym": "1ec16018e9152c50bbaed1d49c3077b9fb6d2838",
    "title": "Interpreting Language Models with Contrastive Explanations",
    "seed_ids": [
        "gpt2",
        "5a2263092f49540fd0e049050a96882ff29b00c3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "1ec16018e9152c50bbaed1d49c3077b9fb6d2838",
    "abstract": "Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics.Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding.To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.",
    "authors": [
        "Kayo Yin",
        "Graham Neubig"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is demonstrated that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers."
    },
    "citationCount": 51,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}