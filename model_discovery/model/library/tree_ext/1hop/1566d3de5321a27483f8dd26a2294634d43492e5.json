{
    "acronym": "1566d3de5321a27483f8dd26a2294634d43492e5",
    "title": "Linearizing Transformer with Key-Value Memory Bank",
    "seed_ids": [
        "synthesizer",
        "rfa",
        "linformer",
        "lineartransformer",
        "sinkhorn",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "168fc3525f7b97695a97b04e257ee9bd1e832acb",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "f4238bd2385a52413ccbacfd9e409a650235bd13"
    ],
    "s2id": "1566d3de5321a27483f8dd26a2294634d43492e5",
    "abstract": "Transformer has brought great success to a wide range of natural language processing tasks. Nevertheless, the computational overhead of the vanilla transformer scales quadratically with sequence length. Many efforts have been made to develop more ef\ufb01cient transformer variants. A line of work (e.g., Linformer) projects the input sequence into a low-rank space, achieving linear time complexity. However, Linformer does not suit well for text generation tasks as the sequence length must be pre-speci\ufb01ed. We propose MemSizer, an approach also projects the source sequence into lower dimension representation but can take input with dynamic length, with a different perspective of the attention mechanism. MemSizer not only achieves the same linear time complexity but also enjoys ef\ufb01cient recurrent-style autoregressive generation, which yields constant memory complexity and reduced computation at inference. We demonstrate that MemSizer provides an improved tradeoff between ef\ufb01ciency and accuracy over the vanilla transformer and other linear variants in language modeling and machine translation tasks, revealing a viable direction to-wards further inference ef\ufb01ciency improve-ment.",
    "authors": [
        "Yizhe Zhang",
        "Deng Cai"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is demonstrated that MemSizer provides an improved tradeoff between ef\ufb01ciency and accuracy over the vanilla transformer and other linear variants in language modeling and machine translation tasks, revealing a viable direction to-wards further inference ef-ciency improve-ment."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}