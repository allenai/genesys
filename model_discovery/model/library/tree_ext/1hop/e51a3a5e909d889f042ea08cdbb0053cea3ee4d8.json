{
    "acronym": "e51a3a5e909d889f042ea08cdbb0053cea3ee4d8",
    "title": "MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation",
    "seed_ids": [
        "transformer",
        "yarn",
        "alibi",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "5735e49e501c8e51e9be4079592e46e047747b03",
        "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "7072db6eddb85ecd2c117365d91bd694760f726e",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "dc48bc1a4d81e0f37603013fd2a95644dc233bd0",
        "3689b7ca7b07924b6135b8a71b9f1b7937b0a3d5"
    ],
    "s2id": "e51a3a5e909d889f042ea08cdbb0053cea3ee4d8",
    "abstract": "When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kernel function adheres to a consistent mean weight coefficient, harnessing the synergistic advantages of different kernels to formulate an innovative bias function. Subsequently, specific slopes are tailored for each kernel function, applying penalties at varying rates, to enhance the model's extrapolation capabilities. Finally, this bias is seamlessly incorporated as a penalty to the post-softmax scores. We present two distinct versions of our method: a parameter-free variant that requires no new learnable parameters, which enhances length extrapolation capabilities without compromising training efficiency, and a parameterized variant capable of integrating state-of-the-art techniques. Empirical evaluations across diverse datasets have demonstrated that both variants of our method achieve state-of-the-art performance, outperforming traditional parameter-free and parameterized approaches.",
    "authors": [
        "Weiguo Gao"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions to generate a bias that is applied to post-softmax attention scores, and is seamlessly incorporated as a penalty to the post-softmax scores."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}