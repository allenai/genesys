{
    "acronym": "979810ca765695a481c37126103b8ba256ee2192",
    "title": "Real-World Robot Learning with Masked Visual Pre-training",
    "seed_ids": [
        "gpt",
        "4990f7542f0600e0501a7e7a931b32eb7cb804d5",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7"
    ],
    "s2id": "979810ca765695a481c37126103b8ba256ee2192",
    "abstract": "In this work, we explore self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks. Like prior work, our visual representations are pre-trained via a masked autoencoder (MAE), frozen, and then passed into a learnable control module. Unlike prior work, we show that the pre-trained representations are effective across a range of real-world robotic tasks and embodiments. We find that our encoder consistently outperforms CLIP (up to 75%), supervised ImageNet pre-training (up to 81%), and training from scratch (up to 81%). Finally, we train a 307M parameter vision transformer on a massive collection of 4.5M images from the Internet and egocentric videos, and demonstrate clearly the benefits of scaling visual pre-training for robot learning.",
    "authors": [
        "Ilija Radosavovic",
        "Tete Xiao",
        "Stephen James",
        "P. Abbeel",
        "J. Malik",
        "Trevor Darrell"
    ],
    "venue": "Conference on Robot Learning",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work explores self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks via a masked autoencoder, frozen, and then passed into a learnable control module to show that the pre-trained representations are effective across a range of real- world robotic tasks and embodiments."
    },
    "citationCount": 169,
    "influentialCitationCount": 15,
    "code": null,
    "description": null,
    "url": null
}