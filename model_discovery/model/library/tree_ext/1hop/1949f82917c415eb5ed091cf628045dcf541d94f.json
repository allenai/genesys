{
    "acronym": "1949f82917c415eb5ed091cf628045dcf541d94f",
    "title": "PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization",
    "seed_ids": [
        "longformer",
        "f3ca1504ab4cc14f491f07e5a8b38d93890551e1",
        "3b39efe6c91ae432dd35bb79431edb8a6719f906",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
        "6aaec722a90eee0185d4bbfebbcd4f228ed1577f",
        "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c"
    ],
    "s2id": "1949f82917c415eb5ed091cf628045dcf541d94f",
    "abstract": "We investigate pre-training techniques for abstractive multi-document summarization (MDS), which is much less studied than summarizing single documents. Though recent work has demonstrated the effectiveness of highlighting information salience for pre-training strategy design, they struggle to generate abstractive and reflective summaries, which are critical properties for MDS. To this end, we present **PELMS**, a pre-trained model that uses pre-training objectives based on semantic coherence heuristics and faithfulness constraints together with unlabeled multi-document inputs, to promote the generation of concise, fluent, and faithful summaries. To support the training of PELMS, we compile **MultiPT**, a multi-document pre-training corpus containing over 93 million documents to form more than 3million unlabeled topic-centric document clusters, covering diverse genres such as product reviews, news, and general knowledge. We perform extensive evaluation of PELMS in low-shot settings on a wide range of MDS datasets. Our approach consistently outperforms competitive comparisons with respect to overall informativeness, abstractiveness, coherence, and faithfulness, and with minimal fine-tuning can match performance of language models at a much larger scale (e.g., GPT-4).",
    "authors": [
        "Joseph Peper",
        "Wenzhao Qiu",
        "Lu Wang"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents PELMS, a pre-trained model that uses pre-training objectives based on semantic coherence heuristics and faithfulness constraints together with unlabeled multi-document inputs, to promote the generation of concise, fluent, and faithful summaries for MDS."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}