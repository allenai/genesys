{
    "acronym": "9058d322a09bfc0c93a070f87cac8fd840e63088",
    "title": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers",
    "seed_ids": [
        "performer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "08ffdec40291a2ccb5f8a6cc048b01247fb34b96",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "0a82b81fbc0bc25bf4d60f9a18c8ee3571e80d7d",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "9058d322a09bfc0c93a070f87cac8fd840e63088",
    "abstract": "In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.",
    "authors": [
        "K. Choromanski",
        "Han Lin",
        "Haoxian Chen",
        "Tianyi Zhang",
        "Arijit Sehanobish",
        "Valerii Likhosherstov",
        "Jack Parker-Holder",
        "Tam\u00e1s Sarl\u00f3s",
        "Adrian Weller",
        "Thomas Weingarten"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper provides the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way and obtains several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking."
    },
    "citationCount": 23,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}