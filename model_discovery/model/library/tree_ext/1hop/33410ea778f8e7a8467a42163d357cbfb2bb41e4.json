{
    "acronym": "33410ea778f8e7a8467a42163d357cbfb2bb41e4",
    "title": "STEP: Generating Semantic Text Embeddings with Prompt",
    "seed_ids": [
        "gpt3",
        "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "33410ea778f8e7a8467a42163d357cbfb2bb41e4",
    "abstract": "In recent years, semantic embeddings for text has played a bigger role in the field of natural language processing (NLP), additionally, it has shown great potential in real-life applications like search and recommendation systems. Therefore, models for generating semantic text embeddings have received extensive study. State-of-the-art solutions for text embeddings have evolved from traditional methods (like Word2Vec, Glove, etc.) to deep neural network based solutions (such as LSTM, Transformer, and pre-trained models like BERT and RoBERTa, etc), besides, frameworks like Sentence Transformer have already lowered the bar of training models for semantic text representation using customized models and datasets. In this paper, we investigated several well trained models according to Massive Text Embedding Benchmark (MTEB) in Huggingface website. Enlighted by the extensive use of prompt engineering in large language models like Llama or GPT3, we proposed STEP: a novel method using prompt to improve performance of text embeddings on downstream tasks, making it applicable to almost any pre-trained language models for text embeddings. Besides, STEP does not need to modify base model structure. In the experiment, we applied STEP to five pre-trained models chosen from MTEB, trained and evaluated our approach on two separated datasets, final results indicated that our approach could improve performance of tasks related to semantic text similarity.",
    "authors": [
        "Wenqiang Cao",
        "Qing Li",
        "Siying Zhang",
        "Rixin Xu",
        "Youqi Li"
    ],
    "venue": "International Conference on Advanced Cloud and Big Data",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}