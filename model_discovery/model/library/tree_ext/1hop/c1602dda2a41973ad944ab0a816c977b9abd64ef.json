{
    "acronym": "c1602dda2a41973ad944ab0a816c977b9abd64ef",
    "title": "Training With\"Paraphrasing the Original Text'' Improves Long-Context Performance",
    "seed_ids": [
        "roformer",
        "b6346f9fa093b8e85df712485a2b851b9f680dac",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4"
    ],
    "s2id": "c1602dda2a41973ad944ab0a816c977b9abd64ef",
    "abstract": "As Large Language Models (LLMs) continue to evolve, more are being designed to handle long-context inputs. Despite this advancement, many models face challenges in achieving high precision on long-context tasks, often showing a ``lost in the middle'' issue. This paper identifies the root of these issues as a deficiency in retrieval capabilities, exacerbated by the sparsity of key information in long contexts. To tackle this challenge, we introduce a novel approach called ``Paraphrasing the Original Text'', aimed at augmenting LLMs' proficiency in extracting information from long context. This enhancement is achieved through a specialized supervised fine-tuning stage that incorporates paraphrasing information into training samples, thereby improving the model's retrieval capabilities for long-context scenarios. Testing on datasets like LongBench and NaturalQuestions Multi-document QA dataset, our method demonstrated significant improvements in managing long-context tasks, effectively addressing the ``lost in the middle'' dilemma. Specifically, we observed an average performance increase of 6.4\\% and 5.9\\% across these datasets, respectively. Moreover, our approach is efficient, requiring minimal overhead with fine-tuning needed on just 19k samples. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k).",
    "authors": [
        "Yijiong Yu"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces a novel approach called ``Paraphrasing the Original Text'', aimed at augmenting LLMs' proficiency in extracting information from long context by incorporating paraphrasing information into training samples, thereby improving the model's retrieval capabilities for long-context scenarios."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}