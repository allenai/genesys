{
    "acronym": "7acc71fad70c4c65203739f156bcb440587df901",
    "title": "Scalable Adaptive Computation for Iterative Generation",
    "seed_ids": [
        "perceiverio",
        "analogbits",
        "22775e58932cdfbd273a2a835a22c5d86800a458",
        "2c6ac935c826002976722ca8d3319f691975687e",
        "8f48171bf05474449777d9bbf6766d480332e09f",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "168fc3525f7b97695a97b04e257ee9bd1e832acb",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "7acc71fad70c4c65203739f156bcb440587df901",
    "abstract": "Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024X1024 images without cascades or guidance, while being domain-agnostic and up to 10X more efficient than 2D and 3D U-Nets.",
    "authors": [
        "A. Jabri",
        "David J. Fleet",
        "Ting Chen"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Recurrent Interface Networks (RINs) are proposed, an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data."
    },
    "citationCount": 67,
    "influentialCitationCount": 10,
    "code": null,
    "description": null,
    "url": null
}