{
    "acronym": "a82c93e7d5e1a0e36e96d11bef11f0da41bd7e9d",
    "title": "Returning to the Start: Generating Narratives with Related Endpoints",
    "seed_ids": [
        "gpt2",
        "684e0925aa11628a165a6faf2095e45447258769",
        "df7bf316e7bad359e87b10544155be09336720ca",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a82c93e7d5e1a0e36e96d11bef11f0da41bd7e9d",
    "abstract": "Human writers often *bookend* their writing with ending sentences that relate back to the beginning sentences in order to compose a satisfying narrative that \u201ccloses the loop.\u201d Motivated by this observation, we propose RENarGen, a controllable story-generation paradigm that generates narratives by ensuring the first and last sentences are related and then infilling the middle sentences. Our contributions include an initial exploration of how various methods of bookending from Narratology affect language modeling for stories. Automatic and human evaluations indicate RENarGen produces better stories with more narrative closure than current autoregressive models.",
    "authors": [
        "Anneliese Brei",
        "Chao Zhao",
        "Snigdha Chaturvedi"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes RENarGen, a controllable story-generation paradigm that generates narratives by ensuring the first and last sentences are related and then infilling the middle sentences."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}