{
    "acronym": "e28351d2f04444db1e1b2ce96c5b347b291c9a8f",
    "title": "PERCEIVER-VL: Efficient Vision-and-Language Modeling with Iterative Latent Attention",
    "seed_ids": [
        "perceiverio",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "145b8b5d99a2beba6029418ca043585b90138d12"
    ],
    "s2id": "e28351d2f04444db1e1b2ce96c5b347b291c9a8f",
    "abstract": "We present PERCEIVER-VL, a vision-and-language framework that efficiently handles high-dimensional multi- modal inputs such as long videos and text. Powered by the iterative latent-cross-attention of Perceiver, our framework scales with linear complexity, in contrast to the quadratic complexity of self-attention used in many state-of-the-art transformer-based models. To further improve the efficiency of our framework, we also study applying LayerDrop on cross-attention layers and introduce a mixed- stream architecture for cross-modal retrieval. We evaluate PERCEIVER-VL on diverse video-text and image-text benchmarks, where PERCEIVER-VL achieves the lowest GFLOPs and latency, while maintaining competitive performance. In addition, we also provide comprehensive analyses over various aspects of our framework, including pretraining data, scalability of latent size and input size, dropping cross-attention layers at inference to reduce latency, modality aggregation strategy, positional encoding, and weight initialization strategy.1",
    "authors": [
        "Zineng Tang",
        "Jaemin Cho",
        "Jie Lei",
        "Mohit Bansal"
    ],
    "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents PERCEIVER-VL, a vision-and-language framework that efficiently handles high-dimensional multi- modal inputs such as long videos and text and provides comprehensive analyses over various aspects of the framework, including pretraining data, scalability of latent size and input size, and dropping cross-attention layers at inference to reduce latency."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}