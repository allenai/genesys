{
    "acronym": "6483a39d7fcd853bc6301c848c1401545d0eb795",
    "title": "Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning",
    "seed_ids": [
        "bert",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6483a39d7fcd853bc6301c848c1401545d0eb795",
    "abstract": "In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.",
    "authors": [
        "Shang-Hsuan Chiang",
        "Ming-Chih Lo",
        "Lin-Wei Chao",
        "Wen-Chih Peng"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification, is presented, leveraging In-Context Learning, Fine-tuned Large Language Models, and the FakeNet model to address the challenges of fact verification."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}