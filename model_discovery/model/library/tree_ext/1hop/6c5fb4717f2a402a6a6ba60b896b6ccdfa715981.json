{
    "acronym": "6c5fb4717f2a402a6a6ba60b896b6ccdfa715981",
    "title": "I-BERT: Inductive Generalization of Transformer to Arbitrary Context Lengths",
    "seed_ids": [
        "transformerxl",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6c5fb4717f2a402a6a6ba60b896b6ccdfa715981",
    "abstract": "Self-attention has emerged as a vital component of state-of-the-art sequence-to-sequence models for natural language processing in recent years, brought to the forefront by pre-trained bi-directional Transformer models. Its effectiveness is partly due to its non-sequential architecture, which promotes scalability and parallelism but limits the model to inputs of a bounded length. In particular, such architectures perform poorly on algorithmic tasks, where the model must learn a procedure which generalizes to input lengths unseen in training, a capability we refer to as inductive generalization. Identifying the computational limits of existing self-attention mechanisms, we propose I-BERT, a bi-directional Transformer that replaces positional encodings with a recurrent layer. The model inductively generalizes on a variety of algorithmic tasks where state-of-the-art Transformer models fail to do so. We also test our method on masked language modeling tasks where training and validation sets are partitioned to verify inductive generalization. Out of three algorithmic and two natural language inductive generalization tasks, I-BERT achieves state-of-the-art results on four tasks.",
    "authors": [
        "Hyoungwook Nam",
        "S. Seo",
        "Vikram Sharma Malithody",
        "Noor Michael",
        "Lang Li"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "I-BERT is proposed, a bi-directional Transformer that replaces positional encodings with a recurrent layer that inductively generalizes on a variety of algorithmic tasks where state-of-the-art Transformer models fail to do so."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}