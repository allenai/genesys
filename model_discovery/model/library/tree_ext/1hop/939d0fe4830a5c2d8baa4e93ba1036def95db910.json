{
    "acronym": "939d0fe4830a5c2d8baa4e93ba1036def95db910",
    "title": "Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG",
    "seed_ids": [
        "infinigram",
        "1a3f7e23ef8f0bf06d0efa0dc174e4e361226ead",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "939d0fe4830a5c2d8baa4e93ba1036def95db910",
    "abstract": "How novel are texts generated by language models (LMs) relative to their training corpora? In this work, we investigate the extent to which modern LMs generate $n$-grams from their training data, evaluating both (i) the probability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the proportion of $n$-grams generated by an LM that did not appear in the training data (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search over a corpus in constant time, we develop Rusty-DAWG, a novel search tool inspired by indexing of genomic data. We compare the novelty of LM-generated text to human-written text and explore factors that affect generation novelty, focusing on the Pythia models. We find that, for $n>4$, LM-generated text is less novel than human-written text, though it is more novel for smaller $n$. Larger LMs and more constrained decoding strategies both decrease novelty. Finally, we show that LMs complete $n$-grams with lower loss if they are more frequent in the training data. Overall, our results reveal factors influencing the novelty of LM-generated text, and we release Rusty-DAWG to facilitate further pretraining data research.",
    "authors": [
        "William Merrill",
        "Noah A. Smith",
        "Yanai Elazar"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}