{
    "acronym": "4ab9de9b2f1ccdc3d0461bbe096872aea1ab635d",
    "title": "Grounded Intuition of GPT-Vision's Abilities with Scientific Images",
    "seed_ids": [
        "gpt3"
    ],
    "s2id": "4ab9de9b2f1ccdc3d0461bbe096872aea1ab635d",
    "abstract": "GPT-Vision has impressed us on a range of vision-language tasks, but it comes with the familiar new challenge: we have little idea of its capabilities and limitations. In our study, we formalize a process that many have instinctively been trying already to develop\"grounded intuition\"of this new model. Inspired by the recent movement away from benchmarking in favor of example-driven qualitative evaluation, we draw upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing. We use our technique to examine alt text generation for scientific figures, finding that GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships. Our method and analysis aim to help researchers ramp up their own grounded intuitions of new models while exposing how GPT-Vision can be applied to make information more accessible.",
    "authors": [
        "Alyssa Hwang",
        "Andrew Head",
        "Chris Callison-Burch"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study formalizes a process that many have instinctively been trying already to develop\"grounded intuition\" of this new model GPT-Vision, and draws upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}