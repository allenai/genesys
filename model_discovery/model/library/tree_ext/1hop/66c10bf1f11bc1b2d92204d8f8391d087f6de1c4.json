{
    "acronym": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
    "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "seed_ids": [
        "lineartransformer",
        "08ffdec40291a2ccb5f8a6cc048b01247fb34b96",
        "868d4d9c9d8afee78f21a0113cff762ff5eb4961",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "84476fdf6ead3553f4493dff8e02308439d6222b",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
    "abstract": null,
    "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Bo Wen",
        "Yunfeng Liu"
    ],
    "venue": "Neurocomputing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel method named Rotary Position Embedding(RoPE) is proposed to effectively leverage the positional information in transformer-based language models and enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding."
    },
    "citationCount": 991,
    "influentialCitationCount": 100,
    "code": null,
    "description": null,
    "url": null
}