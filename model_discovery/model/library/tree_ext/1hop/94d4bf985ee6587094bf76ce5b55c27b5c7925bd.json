{
    "acronym": "94d4bf985ee6587094bf76ce5b55c27b5c7925bd",
    "title": "Wiki-based Prompts for Enhancing Relation Extraction using Language Models",
    "seed_ids": [
        "gpt3",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "da454295392cf4caaa39cc465734237ffe55392f",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a"
    ],
    "s2id": "94d4bf985ee6587094bf76ce5b55c27b5c7925bd",
    "abstract": "Prompt-tuning and instruction-tuning of language models have exhibited significant results in few-shot Natural Language Processing (NLP) tasks, such as Relation Extraction (RE), which involves identifying relationships between entities within a sentence. However, the effectiveness of these methods relies heavily on the design of the prompts. A compelling question is whether incorporating external knowledge can enhance the language model's understanding of NLP tasks. In this paper, we introduce wiki-based prompt construction that leverages Wikidata as a source of information to craft more informative prompts for both prompt-tuning and instruction-tuning of language models in RE. Our experiments show that using wiki-based prompts enhances cutting-edge language models in RE, emphasizing their potential for improving RE tasks. Our code and datasets are available at GitHub 1.",
    "authors": [
        "Amirhossein Layegh",
        "A. H. Payberah",
        "A. Soylu",
        "Dumitru Roman",
        "M. Matskin"
    ],
    "venue": "ACM Symposium on Applied Computing",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces wiki-based prompt construction that leverages Wikidata as a source of information to craft more informative prompts for both prompt-tuning and instruction-tuning of language models in RE."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}