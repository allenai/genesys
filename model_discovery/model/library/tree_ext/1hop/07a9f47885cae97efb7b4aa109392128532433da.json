{
    "acronym": "07a9f47885cae97efb7b4aa109392128532433da",
    "title": "Hard-Coded Gaussian Attention for Neural Machine Translation",
    "seed_ids": [
        "lighdynconv",
        "59e3f0fcd07a44fd1293681528209d0b0d78e75e",
        "f6390beca54411b06f3bde424fb983a451789733"
    ],
    "s2id": "07a9f47885cae97efb7b4aa109392128532433da",
    "abstract": "Recent work has questioned the importance of the Transformer\u2019s multi-headed attention for achieving high translation quality. We push further in this direction by developing a \u201chard-coded\u201d attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.",
    "authors": [
        "Weiqiu You",
        "Simeng Sun",
        "Mohit Iyyer"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A \u201chard-coded\u201d attention variant without any learned parameters is developed, which offers insight into which components of the Transformer are actually important, which it is hoped will guide future work into the development of simpler and more efficient attention-based models."
    },
    "citationCount": 63,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}