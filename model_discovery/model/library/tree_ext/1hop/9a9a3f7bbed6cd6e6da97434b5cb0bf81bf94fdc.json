{
    "acronym": "9a9a3f7bbed6cd6e6da97434b5cb0bf81bf94fdc",
    "title": "A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation",
    "seed_ids": [
        "gpt3",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "49a77a36a0a60d29aa7838ac49a055a69658b195",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f"
    ],
    "s2id": "9a9a3f7bbed6cd6e6da97434b5cb0bf81bf94fdc",
    "abstract": "In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing the automatic generation of product descriptions in a wide range of applications. Data and code are at https://github.com/HITsz-TMG/Multimodal-In-Context-Tuning",
    "authors": [
        "Yunxin Li",
        "Baotian Hu",
        "Wenhan Luo",
        "Lin Ma",
        "Yuxin Ding",
        "Min Zhang"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}