{
    "acronym": "7f8e4e1e8a264ce8909e4482136835396dbbb662",
    "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
    "seed_ids": [
        "gpt3",
        "d4793bcb3fd92b9da9a62b15e0dc93f64e452578",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "7f8e4e1e8a264ce8909e4482136835396dbbb662",
    "abstract": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
    "authors": [
        "Ya-Zhen Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level, and outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies."
    },
    "citationCount": 10,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}