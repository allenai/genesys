{
    "acronym": "14ada95b263a8a7b4ca3cbc6ce08a15f16b1bae9",
    "title": "\u03b5-ViLM : Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer",
    "seed_ids": [
        "bert",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "4990f7542f0600e0501a7e7a931b32eb7cb804d5",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "14ada95b263a8a7b4ca3cbc6ce08a15f16b1bae9",
    "abstract": "To build scalable models for the challenging real-world tasks, it is important to learn from diverse, multi-modal data in various forms (e.g., videos, text, images). Amongst the existing works, a plethora of them have been focusing on leveraging large but cumbersome cross-modal architectures. Regardless of their effectiveness, larger architectures unavoidably prevent the models from being extended to real-world applications, so building a lightweight VL architecture and an efficient learning schema is of great practical value. In this paper, we propose an Efficient Video-Language Model (dubbed as E-ViLM) and a masked video modeling (MVM) schema, assisted with a semantic vector-quantized tokenizer: In particular, our E-ViLM learns to reconstruct the semantic labels of masked video regions, produced by the pre-trained vector-quantized tokenizer which discretizes the continuous visual signals into labels. We show that with our simple MVM task and regular VL pre-training modelings, our E-ViLM, despite its compactness, is able to learn expressive representations from Video-Language corpus and generalize well to extensive Video-Language tasks including video question answering, text-to-video retrieval, etc. In particular, our E-ViLM obtains obvious efficiency improvements by reaching competing performances with faster inference speed: i.e., our model reaches 39.3% Top-1 accuracy on the MSRVTT benchmark, retaining 91.4% of the accuracy of state-of-the-art larger VL architecture with only 15% parameters and 94.8% fewer GFLOPs. We also provide extensive ablative studies that validate the effectiveness of our proposed learning schema for E-ViLM.",
    "authors": [
        "Jacob Zhiyuan Fang",
        "Skyler Zheng",
        "Vasu Sharma",
        "Robinson Piramuthu"
    ],
    "venue": "2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes an Efficient Video-Language Model (dubbed as E-ViLM) and a masked video modeling (MVM) schema, assisted with a semantic vector-quantized tokenizer: in particular, the E-ViLM learns to reconstruct the semantic labels of masked video regions, produced by the pre-trained vector-quantized tokenizer which discretizes the continuous visual signals into labels."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}