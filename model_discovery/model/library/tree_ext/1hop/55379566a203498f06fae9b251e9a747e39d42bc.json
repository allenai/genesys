{
    "acronym": "55379566a203498f06fae9b251e9a747e39d42bc",
    "title": "Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering",
    "seed_ids": [
        "longformer",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "55379566a203498f06fae9b251e9a747e39d42bc",
    "abstract": "Long-range transformer models have achieved encouraging results on long-context question answering (QA) tasks. Such tasks often require reasoning over a long document, and they bene\ufb01t from identifying a set of evidence spans (e.g., sentences) that provide supporting evidence for addressing the question. In this work, we propose a novel method for equip-ping long-range transformers with an additional sequence-level objective for better iden-ti\ufb01cation of supporting evidence spans. We achieve this by proposing an additional contrastive supervision signal in \ufb01netuning, where the model is encouraged to explicitly discriminate supporting evidence sentences from negative ones by maximizing the question-evidence similarity. The proposed additional loss exhibits consistent improvements on three different strong long-context transformer models, across two challenging question answering benchmarks \u2013 HotpotQA and QAsper. 1",
    "authors": [
        "Avi Caciularu",
        "Ido Dagan",
        "J. Goldberger",
        "Arman Cohan"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a novel method for equip-ping long-range transformers with an additional sequence-level objective for better iden-ti\ufb01cation of supporting evidence spans by proposing an additional contrastive supervision signal in netuning."
    },
    "citationCount": 1,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}