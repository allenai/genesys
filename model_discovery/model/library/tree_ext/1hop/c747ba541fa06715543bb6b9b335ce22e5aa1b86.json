{
    "acronym": "c747ba541fa06715543bb6b9b335ce22e5aa1b86",
    "title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "seed_ids": [
        "transformer",
        "lineartransformer",
        "d163cca5cfea5d967873d34023554e3d1771716b",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "48af9b314181b04edcc0b7224ffe4689036b755f",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "72f207c777e4a17180cc54ccc6a743d5f43227af",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
        "830995ef17cc291c13f42dfd9f462137de1d2179",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c747ba541fa06715543bb6b9b335ce22e5aa1b86",
    "abstract": "Self-attention is key to the remarkable success of transformers in sequence modeling tasks including many applications in natural language processing and computer vision. Like neural network layers, these attention mechanisms are often developed by heuristics and experience. To provide a principled framework for constructing attention layers in transformers, we show that the self-attention corresponds to the support vector expansion derived from a support vector regression problem, whose primal formulation has the form of a neural network layer. Using our framework, we derive popular attention layers used in practice and propose two new attentions: 1) the Batch Normalized Attention (Attention-BN) derived from the batch normalization layer and 2) the Attention with Scaled Head (Attention-SH) derived from using less training data to fit the SVR model. We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification.",
    "authors": [
        "T. Nguyen",
        "Tam Nguyen",
        "Nhat Ho",
        "A. Bertozzi",
        "Richard Baraniuk",
        "S. Osher"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work demonstrates the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}