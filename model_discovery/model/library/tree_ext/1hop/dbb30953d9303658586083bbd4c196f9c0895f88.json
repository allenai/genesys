{
    "acronym": "dbb30953d9303658586083bbd4c196f9c0895f88",
    "title": "Investigating Recurrent Transformers with Dynamic Halt",
    "seed_ids": [
        "templatent",
        "staircaseattn",
        "universaltrans",
        "neuraldatarouter",
        "contrnn",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "8420fddf489bd7c5b822bd904aa11ff3742bfb78",
        "2fc074288f66711e4ee37350d364e74c1c401163",
        "434d751d355d7a7c20efa570e785c76286245e77",
        "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b",
        "bcd84a2b8f9ae40a908f375425f113c82f8dd739",
        "9e3e56957e249cdebdd8673fd1174980ed694560",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "d2d0371158803df93a249c9f7237ffd79b875816",
        "0a067fab18c67d4a386efa846c080f8afff5e8f3",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "465ec2212d865e875e64638b3dd1ecaac21c5ddd",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "1f346f74e8eabececa4896d734ab9b261f30830d",
        "24576dcca716c82f66b8cc3c85ecfae18be41edd",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "e82e3f4347674b75c432cb80604d38ee630d4bf6",
        "f024706d6e60aa87312f790f579e0e4e4d59d7a3",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "ca444821352a4bd91884413d8070446e2960715a",
        "d6a0dfd5f39222d8924b7727a0a49f81fa247d71",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "e528466e2aff981511d4ca6e063211297c0b4175",
        "b50815251c948f00baedccaf5f56c281ffa7650f",
        "837ac4ed6825502f0460caec45e12e734c85b113",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "67ee20536c30a225b86902af2f091e28e5e19b40",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "70557ea6b65846fc30729ceed224acd4ac64ca5d",
        "f51497f463566581874c941353dd9d80069c5b77",
        "9a618cca0d2fc78db1be1aed70517401cb3f3859",
        "0bd2602df71e89c8961562175c8759e625e99389",
        "88695b5bb6462872ce1dd946cff00dd6ebabf2d9"
    ],
    "s2id": "dbb30953d9303658586083bbd4c196f9c0895f88",
    "abstract": "In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.",
    "authors": [
        "Jishnu Ray Chowdhury",
        "Cornelia Caragea"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - the approach of incorporating a depth-wise recurrence similar to Universal Transformers and a chunk-wise temporal recurrence like Temporal Latent Bottleneck are studied."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}