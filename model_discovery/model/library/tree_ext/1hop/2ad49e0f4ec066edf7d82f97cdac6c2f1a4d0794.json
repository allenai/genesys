{
    "acronym": "2ad49e0f4ec066edf7d82f97cdac6c2f1a4d0794",
    "title": "NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning",
    "seed_ids": [
        "gpt2",
        "05d70085d1b580b2369942410ae77c48d1eeacca",
        "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
        "df602516e28a9ef0ef665ed0aef551984d8d770d",
        "a68ab49816d5729435c3d994b434c75c6f162da0",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "83b8108014e3db4f46354a28ae68193f143c4e7e",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2ad49e0f4ec066edf7d82f97cdac6c2f1a4d0794",
    "abstract": "Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural language understanding (NLU) and generation (NLG) tasks are provided to verify the effectiveness of the proposed method MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.",
    "authors": [
        "Tianxin Wei",
        "Zeming Guo",
        "Yifan Chen",
        "Jingrui He"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM is investigated and proposed to coin a lightweight PLM through NTK-approximating MLP fusion, which is surprisingly shown to well approximate the NTK of the original PLM."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}