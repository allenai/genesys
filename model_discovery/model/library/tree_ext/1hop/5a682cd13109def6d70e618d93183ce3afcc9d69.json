{
    "acronym": "5a682cd13109def6d70e618d93183ce3afcc9d69",
    "title": "STRec: Sparse Transformer for Sequential Recommendations",
    "seed_ids": [
        "linformer",
        "bigbird",
        "flash",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8"
    ],
    "s2id": "5a682cd13109def6d70e618d93183ce3afcc9d69",
    "abstract": "With the rapid evolution of transformer architectures, researchers are exploring their application in sequential recommender systems (SRSs) and presenting promising performance on SRS tasks compared with former SRS models. However, most existing transformer-based SRS frameworks retain the vanilla attention mechanism, which calculates the attention scores between all item-item pairs. With this setting, redundant item interactions can harm the model performance and consume much computation time and memory. In this paper, we identify the sparse attention phenomenon in transformer-based SRS models and propose Sparse Transformer for sequential Recommendation tasks (STRec) to achieve the efficient computation and improved performance. Specifically, we replace self-attention with cross-attention, making the model concentrate on the most relevant item interactions. To determine these necessary interactions, we design a novel sampling strategy to detect relevant items based on temporal information. Extensive experimental results validate the effectiveness of STRec, which achieves the state-of-the-art accuracy while reducing 54% inference time and 70% memory cost. We also provide massive extended experiments to further investigate the property of our framework.",
    "authors": [
        "Chengxi Li",
        "Yejing Wang",
        "Qidong Liu",
        "Xiangyu Zhao",
        "Wanyu Wang",
        "Yiqi Wang",
        "Lixin Zou",
        "Wenqi Fan",
        "Qing Li"
    ],
    "venue": "ACM Conference on Recommender Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper identifies the sparse attention phenomenon in transformer-based SRS models and proposes Sparse Transformer for sequential Recommendation tasks (STRec), which achieves the state-of-the-art accuracy while reducing 54% inference time and 70% memory cost."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}