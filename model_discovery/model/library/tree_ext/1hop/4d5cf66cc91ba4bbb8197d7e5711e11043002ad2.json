{
    "acronym": "4d5cf66cc91ba4bbb8197d7e5711e11043002ad2",
    "title": "DBA: Efficient Transformer with Dynamic Bilinear Low-Rank Attention",
    "seed_ids": [
        "s4",
        "luna",
        "nystromformer",
        "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "90b21dbad8969b74d704eed15a3d98722a88e464",
        "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "48af9b314181b04edcc0b7224ffe4689036b755f",
        "c88e2d70e44493d5508bfe517be978a9040be6a5",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "01d08fa6c229bf3070600e49f8ab05449361817e",
        "c49c292e1fb1d215c88828a52134b7ccfa52be44",
        "5a9bc55f6332e38f62eb509b684147a1d4f10fd9",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "4d5cf66cc91ba4bbb8197d7e5711e11043002ad2",
    "abstract": "Many studies have been conducted to improve the efficiency of Transformer from quadric to linear. Among them, the low-rank-based methods aim to learn the projection matrices to compress the sequence length. However, the projection matrices are fixed once they have been learned, which compress sequence length with dedicated coefficients for tokens in the same position. Adopting such input-invariant projections ignores the fact that the most informative part of a sequence varies from sequence to sequence, thus failing to preserve the most useful information that lies in varied positions. In addition, previous efficient Transformers only focus on the influence of sequence length while neglecting the effect of hidden state dimension. To address the aforementioned problems, we present an efficient yet effective attention mechanism, namely the Dynamic Bilinear Low-Rank Attention (DBA), which compresses the sequence length by input-sensitive dynamic projection matrices and achieves linear time and space complexity by jointly optimizing the sequence length and hidden state dimension while maintaining state-of-the-art performance. Specifically, we first theoretically demonstrate that the sequence length can be compressed non-destructively from a novel perspective of information theory, with compression matrices dynamically determined by the input sequence. Furthermore, we show that the hidden state dimension can be approximated by extending the Johnson-Lindenstrauss lemma, optimizing the attention in bilinear form. Theoretical analysis shows that DBA is proficient in capturing high-order relations in cross-attention problems. Experiments over tasks with diverse sequence length conditions show that DBA achieves state-of-the-art performance compared with various strong baselines while maintaining less memory consumption with higher speed.",
    "authors": [
        "Bosheng Qin",
        "Juncheng Li",
        "Siliang Tang",
        "Yueting Zhuang"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Dynamic Bilinear Low-Rank Attention (DBA), which compresses the sequence length by input-sensitive dynamic projection matrices and achieves linear time and space complexity by jointly optimizing the sequencelength and hidden state dimension while maintaining state-of-the-art performance."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}