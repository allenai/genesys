{
    "acronym": "0221d3f45899e226ba7840ca9b19117de3a394bc",
    "title": "Semi-Parametric Neural Image Synthesis",
    "seed_ids": [
        "classfreediffu",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "a225d5d846ba5110232ed5bb32d54ea742b1c2d4",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8"
    ],
    "s2id": "0221d3f45899e226ba7840ca9b19117de3a394bc",
    "abstract": "Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Much of this success is due to the scalability of these architectures and hence caused by a dramatic increase in model complexity and in the computational resources invested in training these models. Our work questions the underlying paradigm of compressing large training data into ever growing parametric representations. We rather present an orthogonal, semi-parametric approach. We complement comparably small diffusion or autoregressive models with a separate image database and a retrieval strategy. During training we retrieve a set of nearest neighbors from this external database for each training instance and condition the generative model on these informative samples. While the retrieval approach is providing the (local) content, the model is focusing on learning the composition of scenes based on this content. As demonstrated by our experiments, simply swapping the database for one with different contents transfers a trained model post-hoc to a novel domain. The evaluation shows competitive performance on tasks which the generative model has not been trained on, such as class-conditional synthesis, zero-shot stylization or text-to-image synthesis without requiring paired text-image data. With negligible memory and computational overhead for the external database and retrieval we can significantly reduce the parameter count of the generative model and still outperform the state-of-the-art.",
    "authors": [
        "A. Blattmann",
        "Robin Rombach",
        "Kaan Oktay",
        "Jonas Muller",
        "B. Ommer"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work questions the underlying paradigm of compressing large training data into ever growing parametric representations and presents an orthogonal, semi-parametric approach that can significantly reduce the parameter count of the generative model and still outperform the state-of-the-art."
    },
    "citationCount": 23,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}