{
    "acronym": "beec9c8657d370efdb1b259cc27fcbf5697282c9",
    "title": "Denoising Self-Attentive Sequential Recommendation",
    "seed_ids": [
        "bigbird",
        "longformer",
        "sparsetransformer",
        "dca4d9abbc82e57dfa52f932e893d467a63e0682",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f6390beca54411b06f3bde424fb983a451789733",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
        "2a31319e73d4486716168b65cdf7559baeda18ce"
    ],
    "s2id": "beec9c8657d370efdb1b259cc27fcbf5697282c9",
    "abstract": "Transformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies. This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence. However, real-world item sequences are often noisy, which is particularly true for implicit feedback. For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. As such, the current user action only depends on a subset of items, not on the entire sequences. Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items. This may lead to sub-optimal performance if Transformers are not regularized properly. Here we propose the Rec-denoiser model for better training of self-attentive recommender systems. In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction. To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions. This largely purifies item-item dependencies and provides better model interpretability. In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations. Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences. Our Rec-denoiser is a general plugin that is compatible to many Transformers. Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines.",
    "authors": [
        "Huiyuan Chen",
        "Yusan Lin",
        "Menghai Pan",
        "Lan Wang",
        "Chin-Chia Michael Yeh",
        "Xiaoting Li",
        "Yan Zheng",
        "Fei Wang",
        "Hao Yang"
    ],
    "venue": "ACM Conference on Recommender Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "In Rec-denoiser, each self-attention layer is attached with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions that largely purifies item-item dependencies and provides better model interpretability."
    },
    "citationCount": 32,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}