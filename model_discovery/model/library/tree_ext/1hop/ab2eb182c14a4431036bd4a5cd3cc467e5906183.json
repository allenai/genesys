{
    "acronym": "ab2eb182c14a4431036bd4a5cd3cc467e5906183",
    "title": "Recurrent Hierarchical Topic-Guided RNN for Language Generation",
    "seed_ids": [
        "gpt",
        "transformerxl",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ab2eb182c14a4431036bd4a5cd3cc467e5906183",
    "abstract": "To simultaneously capture syntax and global semantics from a text corpus, we propose a new larger-context recurrent neural network (RNN) based language model, which extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation. Moving beyond a conventional RNN-based language model that ignores long-range word dependencies and sentence order, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence topic dependencies. For inference, we develop a hybrid of stochastic-gradient Markov chain Monte Carlo and recurrent autoencoding variational Bayes. Experimental results on a variety of real-world text corpora demonstrate that the proposed model not only outperforms larger-context RNN-based language models, but also learns interpretable recurrent multilayer topics and generates diverse sentences and paragraphs that are syntactically correct and semantically coherent.",
    "authors": [
        "D. Guo",
        "Bo Chen",
        "Ruiying Lu",
        "Mingyuan Zhou"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new larger-context recurrent neural network (RNN) based language model, which extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation and develops a hybrid of stochastic-gradient Markov chain Monte Carlo and recurrent autoencoding variational Bayes."
    },
    "citationCount": 18,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}