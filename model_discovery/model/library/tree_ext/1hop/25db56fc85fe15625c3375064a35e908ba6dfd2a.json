{
    "acronym": "25db56fc85fe15625c3375064a35e908ba6dfd2a",
    "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training",
    "seed_ids": [
        "gpt",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "25db56fc85fe15625c3375064a35e908ba6dfd2a",
    "abstract": "This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.",
    "authors": [
        "Yu Yan",
        "Weizhen Qi",
        "Yeyun Gong",
        "Dayiheng Liu",
        "Nan Duan",
        "Jiusheng Chen",
        "Ruofei Zhang",
        "Ming Zhou"
    ],
    "venue": "Findings",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new sequence-to-sequence pre-training model called ProphetNet is presented, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism that predicts the next n tokens simultaneously based on previous context tokens at each time step."
    },
    "citationCount": 394,
    "influentialCitationCount": 60,
    "code": null,
    "description": null,
    "url": null
}