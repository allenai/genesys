{
    "acronym": "366827d876b51c62fe5640b10c806e2d85673369",
    "title": "BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records",
    "seed_ids": [
        "bert"
    ],
    "s2id": "366827d876b51c62fe5640b10c806e2d85673369",
    "abstract": "The advent of clinical language models integrated into electronic health records (EHR) for clinical decision support has marked a significant advancement, leveraging the depth of clinical notes for improved decision-making. Despite their success, the potential vulnerabilities of these models remain largely unexplored. This paper delves into the realm of backdoor attacks on clinical language models, introducing an innovative attention-based backdoor attack method, BadCLM (Bad Clinical Language Models). This technique clandestinely embeds a backdoor within the models, causing them to produce incorrect predictions when a pre-defined trigger is present in inputs, while functioning accurately otherwise. We demonstrate the efficacy of BadCLM through an in-hospital mortality prediction task with MIMIC III dataset, showcasing its potential to compromise model integrity. Our findings illuminate a significant security risk in clinical decision support systems and pave the way for future endeavors in fortifying clinical language models against such vulnerabilities.",
    "authors": [
        "Weimin Lyu",
        "Zexin Bi",
        "Fusheng Wang",
        "Chao Chen"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces an innovative attention-based backdoor attack method, BadCLM (Bad Clinical Language Models), which clandestinely embeds a backdoor within the models, causing them to produce incorrect predictions when a pre-defined trigger is present in inputs, while functioning accurately otherwise."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}