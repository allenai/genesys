{
    "acronym": "07bfb0193b34801b864748d1a9d5c21b67144af2",
    "title": "Breadth-First Pipeline Parallelism",
    "seed_ids": [
        "gpt2",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "07bfb0193b34801b864748d1a9d5c21b67144af2",
    "abstract": "We introduce Breadth-First Pipeline Parallelism, a novel training schedule which optimizes the combination of pipeline and data parallelism. Breadth-First Pipeline Parallelism lowers training time, cost and memory usage by combining a high GPU utilization with a small batch size per GPU, and by making use of fully sharded data parallelism. Experimentally, we observed an increase of up to 43% in training throughput for a 52 billion-parameter model using a small batch size per GPU compared to Megatron-LM, which would reduce the training time and cost by the same amount on a large GPU cluster.",
    "authors": [
        "J. Lamy-Poirier"
    ],
    "venue": "",
    "year": 2022,
    "tldr": null,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}