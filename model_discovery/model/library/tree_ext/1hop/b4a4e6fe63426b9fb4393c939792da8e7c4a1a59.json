{
    "acronym": "b4a4e6fe63426b9fb4393c939792da8e7c4a1a59",
    "title": "GDP: Generalized Device Placement for Dataflow Graphs",
    "seed_ids": [
        "transformerxl",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b4a4e6fe63426b9fb4393c939792da8e7c4a1a59",
    "abstract": "Runtime and scalability of large neural networks can be significantly affected by the placement of operations in their dataflow graphs on suitable devices. With increasingly complex neural network architectures and heterogeneous device characteristics, finding a reasonable placement is extremely challenging even for domain experts. Most existing automated device placement approaches are impractical due to the significant amount of compute required and their inability to generalize to new, previously held-out graphs. To address both limitations, we propose an efficient end-to-end method based on a scalable sequential attention mechanism over a graph neural network that is transferable to new graphs. On a diverse set of representative deep learning models, including Inception-v3, AmoebaNet, Transformer-XL, and WaveNet, our method on average achieves 16% improvement over human experts and 9.2% improvement over the prior art with 15 times faster convergence. To further reduce the computation cost, we pre-train the policy network on a set of dataflow graphs and use a superposition network to fine-tune it on each individual graph, achieving state-of-the-art performance on large hold-out graphs with over 50k nodes, such as an 8-layer GNMT.",
    "authors": [
        "Yanqi Zhou",
        "Sudip Roy",
        "AmirAli Abdolrashidi",
        "Daniel Wong",
        "Peter C. Ma",
        "Qiumin Xu",
        "Ming Zhong",
        "Hanxiao Liu",
        "Anna Goldie",
        "Azalia Mirhoseini",
        "J. Laudon"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes an efficient end-to-end method based on a scalable sequential attention mechanism over a graph neural network that is transferable to new graphs that achieves state-of-the-art performance on large hold-out graphs with over 50k nodes, such as an 8-layer GNMT."
    },
    "citationCount": 36,
    "influentialCitationCount": 13,
    "code": null,
    "description": null,
    "url": null
}