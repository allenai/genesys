{
    "acronym": "53ff81883dded3b41501f7ae2c1cefedb5539ba0",
    "title": "Can Peanuts Fall in Love with Distributional Semantics?",
    "seed_ids": [
        "gpt2",
        "4061a9941fa0ff106e884272d9ed753650417ec4",
        "ea0ea0da2e774bc0b73c4470de6cbd1fc979b265",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "53ff81883dded3b41501f7ae2c1cefedb5539ba0",
    "abstract": "Context changes expectations about upcoming words - following a story involving an anthropomorphic peanut, comprehenders expect the sentence the peanut was in love more than the peanut was salted, as indexed by N400 amplitude (Nieuwland&van Berkum, 2006). This updating of expectations has been explained using Situation Models - mental representations of a described event. However, recent work showing that N400 amplitude is predictable from distributional information alone raises the question whether situation models are necessary for these contextual effects. We model the results of Nieuwland and van Berkum (2006) using six computational language models and three sets of word vectors, none of which have explicit situation models or semantic grounding. We find that a subset of these can fully model the effect found by Nieuwland and van Berkum (2006). Thus, at least some processing effects normally explained through situation models may not in fact require explicit situation models.",
    "authors": [
        "J. Michaelov",
        "S. Coulson",
        "B. Bergen"
    ],
    "venue": "Annual Meeting of the Cognitive Science Society",
    "year": 2023,
    "tldr": null,
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}