{
    "acronym": "d1f8f4dfdf6aae9262129efb507064c4e99f6ee7",
    "title": "Language Model Using Neural Turing Machine Based on Localized Content-Based Addressing",
    "seed_ids": [
        "transformerxl",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "d1f8f4dfdf6aae9262129efb507064c4e99f6ee7",
    "abstract": "The performance of a long short-term memory (LSTM) recurrent neural network (RNN)-based language model has been improved on language model benchmarks. Although a recurrent layer has been widely used, previous studies showed that an LSTM RNN-based language model (LM) cannot overcome the limitation of the context length. To train LMs on longer sequences, attention mechanism-based models have recently been used. In this paper, we propose a LM using a neural Turing machine (NTM) architecture based on localized content-based addressing (LCA). The NTM architecture is one of the attention-based model. However, the NTM encounters a problem with content-based addressing because all memory addresses need to be accessed for calculating cosine similarities. To address this problem, we propose an LCA method. The LCA method searches for the maximum of all cosine similarities generated from all memory addresses. Next, a specific memory area including the selected memory address is normalized with the softmax function. The LCA method is applied to pre-trained NTM-based LM during the test stage. The proposed architecture is evaluated on Penn Treebank and enwik8 LM tasks. The experimental results indicate that the proposed approach outperforms the previous NTM architecture.",
    "authors": [
        "Donghyun Lee",
        "Jeong-Sik Park",
        "M. Koo",
        "Ji-Hwan Kim"
    ],
    "venue": "Applied Sciences",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a LM using a neural Turing machine (NTM) architecture based on localized content-based addressing (LCA) and indicates that the proposed approach outperforms the previous NTM architecture."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}