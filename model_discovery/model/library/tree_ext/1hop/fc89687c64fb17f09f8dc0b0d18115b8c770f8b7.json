{
    "acronym": "fc89687c64fb17f09f8dc0b0d18115b8c770f8b7",
    "title": "Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering",
    "seed_ids": [
        "gpt",
        "83fac78857c7e65fe10a11a798674dd3cd259c1d",
        "710d183174844da5b7f392667f3cc25d2b098dde",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fc89687c64fb17f09f8dc0b0d18115b8c770f8b7",
    "abstract": "Understanding narratives requires dynamically reasoning about the implicit causes, effects, and states of the situations described in text, which in turn requires understanding rich background knowledge about how the social and physical world works. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. \nIn this paper, we present initial studies toward zero-shot commonsense QA by formulating the task as probabilistic inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually relevant knowledge on demand using generative neural commonsense knowledge models. \nEmpirical results on the SocialIQa and StoryCommonsense datasets in a zero-shot setting demonstrate that using commonsense knowledge models to dynamically construct and reason over knowledge graphs achieves performance boosts over pre-trained language models and using knowledge models to directly evaluate answers.",
    "authors": [
        "Antoine Bosselut",
        "Yejin Choi"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Empirical results on the SocialIQa and StoryCommonsense datasets in a zero-shot setting demonstrate that using commonsense knowledge models to dynamically construct and reason over knowledge graphs achieves performance boosts over pre-trained language models and usingknowledge models to directly evaluate answers."
    },
    "citationCount": 40,
    "influentialCitationCount": 8,
    "code": null,
    "description": null,
    "url": null
}