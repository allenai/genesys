{
    "acronym": "a1186d7d9a9ef258c76afef1177e4f348061a537",
    "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers",
    "seed_ids": [
        "diffusionlm",
        "diffuseq",
        "contdiffu",
        "selfcondembdiffu",
        "analogbits",
        "bb7e779c9360a94dd2779c2468fe06b82de7af59",
        "a979742220a88b1d32e1fbe72c41e8ba3007053c",
        "22775e58932cdfbd273a2a835a22c5d86800a458",
        "2c6ac935c826002976722ca8d3319f691975687e",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "a1186d7d9a9ef258c76afef1177e4f348061a537",
    "abstract": "Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studied. Sequence-to-sequence text generation is one of the essential natural language processing topics. In this work, we apply diffusion models to approach sequence-to-sequence text generation, and explore whether the superiority generation performance of diffusion model can transfer to natural language domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to model denoising function. In order to improve generation quality, SeqDiffuSeq combines the self-conditioning technique and a newly proposed adaptive noise schedule technique. The adaptive noise schedule has the difficulty of denoising evenly distributed across time steps, and considers exclusive noise schedules for tokens at different positional order. Experiment results illustrate the good performance on sequence-to-sequence generation in terms of text quality and inference time.",
    "authors": [
        "Hongyi Yuan",
        "Zheng Yuan",
        "Chuanqi Tan",
        "Fei Huang",
        "Songfang Huang"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation that uses an encoder-decoder Transformers architecture to model denoising function and combines the self-conditioning technique and a newly proposed adaptive noise schedule technique."
    },
    "citationCount": 54,
    "influentialCitationCount": 9,
    "code": null,
    "description": null,
    "url": null
}