{
    "acronym": "9166caa474031b62bacad8a920db8308e6a15120",
    "title": "An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification",
    "seed_ids": [
        "lineartransformer",
        "bigbird",
        "longformer",
        "hitrans",
        "94e46e18d2628343a926acf6c3d0817e11d35d58",
        "e96493b4181de6c60b761dc66492db8e66fd784f",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "fd33e77884e69f6bc099990fc2790248af2749d9",
        "84daddd294fa3cc12596b5785f81c2a153d2fb1d",
        "29584ed6d68a06fdf91440a018f6bc83a44fd177",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "cc2d1a66c701e9be9124eb95431b6c3dd79a61d3"
    ],
    "s2id": "9166caa474031b62bacad8a920db8308e6a15120",
    "abstract": "Non-hierarchical sparse attention Transformer-based models, such as Longformer and Big Bird, are popular approaches to working with long documents. There are clear benefits to these approaches compared to the original Transformer in terms of efficiency, but Hierarchical Attention Transformer (HAT) models are a vastly understudied alternative. We develop and release fully pre-trained HAT models that use segment-wise followed by cross-segment encoders and compare them with Longformer models and partially pre-trained HATs. In several long document downstream classification tasks, our best HAT model outperforms equally-sized Longformer models while using 10-20% less GPU memory and processing documents 40-45% faster. In a series of ablation studies, we find that HATs perform best with cross-segment contextualization throughout the model than alternative configurations that implement either early or late cross-segment contextualization. Our code is on GitHub: https://github.com/coastalcph/hierarchical-transformers.",
    "authors": [
        "Ilias Chalkidis",
        "Xiang Dai",
        "Manos Fergadiotis",
        "Prodromos Malakasiotis",
        "Desmond Elliott"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops and releases fully pre-trained HAT models that use segment-wise followed by cross-segment encoders and compares them with Longformer models and partially pre- trained HATs to find the best HAT model outperforms equally-sized Longform models while using 10-20% less GPU memory and processing documents 40-45% faster."
    },
    "citationCount": 25,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}