{
    "acronym": "72e3a1b4745d6f25b8cd186bd56fbad6e3a8464b",
    "title": "Judgment aggregation, discursive dilemma and reflective equilibrium: Neural language models as self-improving doxastic agents",
    "seed_ids": [
        "gpt2",
        "7096304d19457833972daec4d3f5107befe30b1c",
        "9fa9d5dd481400b2f3904b33d542d70a6affccb9",
        "89cb62dc83c1b1895267bd28639fbf5bb7ed21a4",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "72e3a1b4745d6f25b8cd186bd56fbad6e3a8464b",
    "abstract": "Neural language models (NLMs) are susceptible to producing inconsistent output. This paper proposes a new diagnosis as well as a novel remedy for NLMs' incoherence. We train NLMs on synthetic text corpora that are created by simulating text production in a society. For diagnostic purposes, we explicitly model the individual belief systems of artificial agents (authors) who produce corpus texts. NLMs, trained on those texts, can be shown to aggregate the judgments of individual authors during pre-training according to sentence-wise vote ratios (roughly, reporting frequencies), which inevitably leads to so-called discursive dilemmas: aggregate judgments are inconsistent even though all individual belief states are consistent. As a remedy for such inconsistencies, we develop a self-training procedure\u2014inspired by the concept of reflective equilibrium\u2014that effectively reduces the extent of logical incoherence in a model's belief system, corrects global mis-confidence, and eventually allows the model to settle on a new, epistemically superior belief state. Thus, social choice theory helps to understand why NLMs are prone to produce inconsistencies; epistemology suggests how to get rid of them.",
    "authors": [
        "Gregor Betz",
        "Kyle Richardson"
    ],
    "venue": "Frontiers in Artificial Intelligence",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A self-training procedure is developed that effectively reduces the extent of logical incoherence in a model's belief system, corrects global mis-confidence, and eventually allows the model to settle on a new, epistemically superior belief state."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}