{
    "acronym": "d7b3e795d657ccf1562053f47ce1ff81044fc2a0",
    "title": "C O CA: F USING POSITION EMBEDDING WITH C OLLINEAR C ONSTRAINED A TTENTION FOR FINE - TUNING FREE CONTEXT WINDOW EXTENDING",
    "seed_ids": [
        "pi",
        "alibi",
        "roformer",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "d7b3e795d657ccf1562053f47ce1ff81044fc2a0",
    "abstract": "Self-attention and position embedding are two key modules in Transformer based LLMs. The potential relationship among them are far from well studied, especially for context window extending. In this paper, we introduce collinear constrained relationship to fuse RoPE and self-attention, and name it as Collinear Constrained Attention (CoCA). We\u2019ve analyzed the computational and spatial complexity of CoCA and have determined that it adds only minimal additional overhead compared to the original Transformer-based models. We provide an efficient implementation of CoCA, and make it drop-in replacement for any existing position embedding and attention modules in Transformer based models. Experiments show that CoCA performs extraordinary well on context window extending. For instance, a CoCA based GPT model trained with 512 context length can extend the context window up to 8K without perplexity diverging. This indicates more than 16x context window extending without any fine-tuning. Our code is released here: https://github.com/codefuse-ai/Collinear-Constrained-Attention",
    "authors": [
        "Shiyi Zhu",
        "Jingting Ye",
        "Wei Jiang",
        "Qi Zhang",
        "Yifan Wu",
        "Jianguo Li"
    ],
    "venue": "",
    "year": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces collinear constrained relationship to fuse RoPE and self-attention, and name it as Collinear Constrained Attention (CoCA), and analyzes the computational and spatial complexity of CoCA to determine that it adds only minimal additional overhead compared to the original Transformer-based models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}