{
    "acronym": "bdff3ed23673787f8d54125456ca45ccbdd1960a",
    "title": "Prompting Disentangled Embeddings for Knowledge Graph Completion with Pre-trained Language Model",
    "seed_ids": [
        "bert",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "933cb8bf1cd50d6d5833a627683327b15db28836",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "bdff3ed23673787f8d54125456ca45ccbdd1960a",
    "abstract": "Both graph structures and textual information play a critical role in Knowledge Graph Completion (KGC). With the success of Pre-trained Language Models (PLMs) such as BERT, they have been applied for text encoding for KGC. However, the current methods mostly prefer to fine-tune PLMs, leading to huge training costs and limited scalability to larger PLMs. In contrast, we propose to utilize prompts and perform KGC on a frozen PLM with only the prompts trained. Accordingly, we propose a new KGC method named PDKGC with two prompts -- a hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction, and a disentangled structure prompt which learns disentangled graph representation so as to enable the PLM to combine more relevant structure knowledge with the text information. With the two prompts, PDKGC builds a textual predictor and a structural predictor, respectively, and their combination leads to more comprehensive entity prediction. Solid evaluation on two widely used KGC datasets has shown that PDKGC often outperforms the baselines including the state-of-the-art, and its components are all effective. Our codes and data are available at https://github.com/genggengcss/PDKGC.",
    "authors": [
        "Yuxia Geng",
        "Jiaoyan Chen",
        "Yuhang Zeng",
        "Zhuo Chen",
        "Wen Zhang",
        "Jeff Z. Pan",
        "Yuxiang Wang",
        "Xiaoliang Xu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a new KGC method named PDKGC with two prompts -- a hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction, and a disentangled structure prompt which learns disentangling graph representation so as to enable thePLM to combine more relevant structure knowledge with the text information."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}