{
    "acronym": "e7b10b4a989dec11887215a64dfb2671370d5200",
    "title": "Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models",
    "seed_ids": [
        "gpt2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e7b10b4a989dec11887215a64dfb2671370d5200",
    "abstract": "Recent studies in natural language processing (NLP) have focused on modern languages and achieved state-of-the-art results in many tasks. Meanwhile, little attention has been paid to ancient texts and related tasks. Classical Chinese first came to Japan approximately 2,000 years ago. It was gradually adapted to a Japanese form called Kanbun-Kundoku (Kanbun) in Japanese reading and translating methods, which has significantly impacted Japanese literature. However, compared to the rich resources for ancient texts in mainland China, Kanbun resources remain scarce in Japan. To solve this problem, we construct the first Classical-Chinese-to-Kanbun dataset in the world. Furthermore, we introduce two tasks, character reordering and machine translation, both of which play a significant role in Kanbun comprehension. We also test the current language models on these tasks and discuss the best evaluation method by comparing the results with human scores. We release our code and dataset on GitHub.",
    "authors": [
        "Hao Wang",
        "Hirofumi Shimizu",
        "Daisuke Kawahara"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work constructs the first Classical-Chinese-to-Kanbun dataset in the world and introduces two tasks, character reordering and machine translation, both of which play a significant role in Kanbun comprehension."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}