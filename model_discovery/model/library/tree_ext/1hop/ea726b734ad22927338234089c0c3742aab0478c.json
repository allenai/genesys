{
    "acronym": "ea726b734ad22927338234089c0c3742aab0478c",
    "title": "Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics",
    "seed_ids": [
        "rwkv4",
        "bc9dbbc23e39c93edead2ed707d2c62189f56f20",
        "ac70fb2c74aa87420878c441c3d24969947e0294",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "eaee0b647d336c6fc8b844812675ec35cddf14a1",
        "4061a9941fa0ff106e884272d9ed753650417ec4",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "ebc0acee471e61fd7d51c8720c404a53b7a9e29b",
        "ea0ea0da2e774bc0b73c4470de6cbd1fc979b265",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "97a531f96a8d9a4c1d379637daf7dcc714db1b35"
    ],
    "s2id": "ea726b734ad22927338234089c0c3742aab0478c",
    "abstract": "Transformers have supplanted Recurrent Neural Networks as the dominant architecture for both natural language processing tasks and, despite criticisms of cognitive implausibility, for modelling the effect of predictability on online human language comprehension. However, two recently developed recurrent neural network architectures, RWKV and Mamba, appear to perform natural language tasks comparably to or better than transformers of equivalent scale. In this paper, we show that contemporary recurrent models are now also able to match - and in some cases, exceed - performance of comparably sized transformers at modeling online human language comprehension. This suggests that transformer language models are not uniquely suited to this task, and opens up new directions for debates about the extent to which architectural features of language models make them better or worse models of human language comprehension.",
    "authors": [
        "J. Michaelov",
        "Catherine Arnett",
        "Benjamin Bergen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that contemporary recurrent models are now also able to match - and in some cases, exceed - performance of comparably sized transformers at modeling online human language comprehension."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}