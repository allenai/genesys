{
    "acronym": "6aded7d6f93fd93c5632b627b6520830a3999ad4",
    "title": "Molformer: Large Scale Chemical Language Representations Capture Molecular Structure and Properties",
    "seed_ids": [
        "lineartransformer",
        "roformer",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "6aded7d6f93fd93c5632b627b6520830a3999ad4",
    "abstract": "Predicting the properties of a chemical molecule is of great importance in many applications, including drug discovery and material design. Machine learning-based models promise to enable more accurate and faster molecular property predictions than the current state-of-the-art techniques, such as Density Functional Theory calculations or wet-lab experiments. Various supervised machine learning models, including graph neural nets, have demonstrated promising performance in molecular property prediction tasks. However, the vast chemical space and the limited availability of property labels make supervised learning challenging, calling for learning a general-purpose molecular representation. Recently, unsupervised transformer-based language models pre-trained on large unlabeled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, we present molecular embeddings obtained by training an ef\ufb01cient transformer encoder model, MoLFormer, which uses rotary positional embeddings. This model employs a linear attention mechanism, coupled with highly distributed training, on SMILES sequences of 1.1 billion unlabeled molecules from the PubChem and ZINC datasets.Experiments show that utilizing the learned molecular representation outperforms existing baselines on downstream tasks, including supervised and self-supervised graph neural net baselines and language models, on several classi\ufb01cation and regression tasks from ten benchmark datasets while performing competitively on two others. Further analyses, speci\ufb01cally through the lens of attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns the spatial relationships between atoms within a molecule. These results provide encouraging evidence that the large-scale molecular language models can capture suf\ufb01cient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties.",
    "authors": [
        "Jerret Ross",
        "Brian M. Belgodere",
        "V. Chenthamarakshan",
        "Inkit Padhi",
        "Youssef Mroueh",
        "Payel Das"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experiments show that utilizing the learned molecular representation outperforms existing baselines on downstream tasks, including supervised and self-supervised graph neural net baselines and language models, on several classi\ufb01cation and regression tasks from ten benchmark datasets while performing competitively on two others."
    },
    "citationCount": 16,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}