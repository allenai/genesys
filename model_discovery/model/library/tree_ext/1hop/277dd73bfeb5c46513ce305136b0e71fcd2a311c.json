{
    "acronym": "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
    "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
    "seed_ids": [
        "performer",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "6c761cfdb031701072582e434d8f64d436255da6",
        "9058d322a09bfc0c93a070f87cac8fd840e63088",
        "9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "30dcc0e191a376fea0e7a46f94c53872c029efc9"
    ],
    "s2id": "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
    "abstract": "We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being $\\textit{local}$, $\\textit{global}$ or $\\textit{relative}$. The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the first architecture with a complexity linear in the number of nodes and edges $O(N+E)$ by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework $\\textit{GraphGPS}$ that supports multiple types of encodings and that provides efficiency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical benefits gained by the modularity and the combination of different strategies.",
    "authors": [
        "Ladislav Ramp\u00e1\u0161ek",
        "Mikhail Galkin",
        "Vijay Prakash Dwivedi",
        "A. Luu",
        "Guy Wolf",
        "D. Beaini"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes the first architecture with a complexity linear in the number of nodes and edges $O(N+E)$ by decoupled the local real-edge aggregation from the fully-connected Transformer, and argues that this decoupling does not negatively affect the expressivity, with the architecture being a universal function approximator on graphs."
    },
    "citationCount": 327,
    "influentialCitationCount": 80,
    "code": null,
    "description": null,
    "url": null
}