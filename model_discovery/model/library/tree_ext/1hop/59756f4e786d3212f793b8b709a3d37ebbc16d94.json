{
    "acronym": "59756f4e786d3212f793b8b709a3d37ebbc16d94",
    "title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models",
    "seed_ids": [
        "gpt2",
        "e2f8864c3e40298513ca320de0012818ce092bea",
        "a74a20be53e5767648b5970e30b2d81a9ba8293f",
        "d58c87575a4e00da93be0d63af568ac10532aab4",
        "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c",
        "61a4023aad982d435dcde9f40bb9c2a735e88a9c",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "e4f82c0a13cae6739239ae0c25a554b6daff35af",
        "9405cc0d6169988371b2755e573cc28650d14dfe",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28"
    ],
    "s2id": "59756f4e786d3212f793b8b709a3d37ebbc16d94",
    "abstract": "Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL. Metric-based and GPT-4-based evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses.",
    "authors": [
        "Taiqiang Wu",
        "Chaofan Tao",
        "Jiahao Wang",
        "Zhe Zhao",
        "Ngai Wong"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL, is proposed, which outperforms the baselines across various tasks and improves the diversity and quality of generated responses."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}