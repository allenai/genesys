{
    "acronym": "ef4eb30579533e33395472b3cebd3f75432da38d",
    "title": "Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias",
    "seed_ids": [
        "gpt2",
        "b04493c0865c166d00f65bb78f6476b466bbe512",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "7ea0e91c5d5dc73f2133bc46d7ebb6cb83034dae",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ef4eb30579533e33395472b3cebd3f75432da38d",
    "abstract": "Warning: The content of this paper may be upsetting or triggering. Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person\u2019s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model\u2019s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d > 0.80) and sadness (d > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of \"a [age] year old girl\" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications.",
    "authors": [
        "R. Wolfe",
        "Yiwei Yang",
        "Billy Howe",
        "Aylin Caliskan"
    ],
    "venue": "Conference on Fairness, Accountability and Transparency",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications."
    },
    "citationCount": 34,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}