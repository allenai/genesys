{
    "acronym": "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
    "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting",
    "seed_ids": [
        "nystromformer",
        "blockbert",
        "reformer",
        "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28"
    ],
    "s2id": "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
    "abstract": "Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer ({\\bf FEDformer}), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by $14.8\\%$ and $22.6\\%$ for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.",
    "authors": [
        "Tian Zhou",
        "Ziqing Ma",
        "Qingsong Wen",
        "Xue Wang",
        "Liang Sun",
        "Rong Jin"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "FEDformer, a frequency enhanced Transformer that is more efficient than standard Transformer with a linear complexity to the sequence length, and compared with state-of-the-art methods, FEDformer can reduce prediction error by $14.8\\% and $22.6\\% for multivariate and univariate time series, respectively."
    },
    "citationCount": 668,
    "influentialCitationCount": 110,
    "code": null,
    "description": null,
    "url": null
}