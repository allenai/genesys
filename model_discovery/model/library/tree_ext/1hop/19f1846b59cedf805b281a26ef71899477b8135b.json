{
    "acronym": "19f1846b59cedf805b281a26ef71899477b8135b",
    "title": "Robust Multimodal Emotion Recognition from Conversation with Transformer-Based Crossmodality Fusion",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "19f1846b59cedf805b281a26ef71899477b8135b",
    "abstract": "Decades of scientific research have been conducted on developing and evaluating methods for automated emotion recognition. With exponentially growing technology, there is a wide range of emerging applications that require emotional state recognition of the user. This paper investigates a robust approach for multimodal emotion recognition during a conversation. Three separate models for audio, video and text modalities are structured and fine-tuned on the MELD. In this paper, a transformer-based crossmodality fusion with the EmbraceNet architecture is employed to estimate the emotion. The proposed multimodal network architecture can achieve up to 65% accuracy, which significantly surpasses any of the unimodal models. We provide multiple evaluation techniques applied to our work to show that our model is robust and can even outperform the state-of-the-art models on the MELD.",
    "authors": [
        "Baijun Xie",
        "Mariia Sidulova",
        "C. Park"
    ],
    "venue": "Italian National Conference on Sensors",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A transformer-based crossmodality fusion with the EmbraceNet architecture is employed to estimate the emotion and the proposed multimodal network architecture can achieve up to 65% accuracy, which significantly surpasses any of the unimodal models."
    },
    "citationCount": 56,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}