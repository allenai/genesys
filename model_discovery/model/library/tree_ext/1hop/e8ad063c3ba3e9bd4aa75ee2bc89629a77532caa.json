{
    "acronym": "e8ad063c3ba3e9bd4aa75ee2bc89629a77532caa",
    "title": "An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "e8ad063c3ba3e9bd4aa75ee2bc89629a77532caa",
    "abstract": "Deep neural network (DNN)-based transformer models have demonstrated remarkable performance in natural language processing (NLP) applications. Unfortunately, the unique scaled dot-product attention mechanism and intensive memory access pose a significant challenge during inference on power-constrained edge devices. One emerging solution to this challenge is computing-in-memory (CIM), which uses memory cells for logic computation to reduce data movement and overcome the memory wall. However, existing CIM designs do not support high-precision computations, such as floating-point operations, which are essential for NLP applications. Furthermore, CIM architectures require complex control modules and costly peripheral circuits to harness the full potential of in-memory computation. Hence, this article proposes a scalable RRAM-based in-memory floating-point computation architecture (RIME) that uses single-cycle NOR, NAND, and minority logic to implement in-memory floating-point operations. RIME features efficient parallel and pipeline capabilities with a centralized control module and a simplified peripheral circuit to eliminate data movement during computation. Furthermore, the article proposes pipelined implementations of matrix\u2013matrix multiplication (MatMul) and softmax functions, enabling the construction of a transformer accelerator based on RIME. Extensive experimental results show that compared with GPU-based implementation, the RIME-based transformer accelerator improves timing efficiency by $2.3\\times $ and energy efficiency by $1.7\\times $ without compromising inference accuracy.",
    "authors": [
        "Zhaojun Lu",
        "Xueyan Wang",
        "Md Tanvir Arafin",
        "Haoxiang Yang",
        "Zhenglin Liu",
        "Jiliang Zhang",
        "Gang Qu"
    ],
    "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A scalable RRAM-based in-memory floating-point computation architecture (RIME) that uses single-cycle NOR, NAND, and minority logic to implement in-memory floating-point operations and pipelined implementations of matrix\u2013matrix multiplication and softmax functions are proposed, enabling the construction of a transformer accelerator based on RIME."
    },
    "citationCount": 2,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}