{
    "acronym": "35a3979bc9b680023d6e2bcf29a24d67053d0d86",
    "title": "Replacing Language Model for Style Transfer",
    "seed_ids": [
        "gpt2",
        "0e3d6a7c9c04cf3ba9c902724548846a5ade04b4",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "35a3979bc9b680023d6e2bcf29a24d67053d0d86",
    "abstract": "We introduce replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer (TST). Our method autoregressively replaces each token of the source sentence with a text span that has a similar meaning but in the target style. The new span is generated via a non-autoregressive masked language model, which can better preserve the local-contextual meaning of the replaced token. This RLM generation scheme gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods. To control the generation style more precisely, we conduct a token-level style-content disentanglement on the hidden representations of RLM. Empirical results on real-world text datasets demonstrate the effectiveness of RLM compared with other TST baselines. The code is at https://github.com/Linear95/RLM.",
    "authors": [
        "Peng Cheng",
        "Rui Li"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer (TST), which gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}