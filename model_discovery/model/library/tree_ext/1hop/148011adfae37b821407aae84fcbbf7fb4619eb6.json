{
    "acronym": "148011adfae37b821407aae84fcbbf7fb4619eb6",
    "title": "On the Expressive Power of Self-Attention Matrices",
    "seed_ids": [
        "performer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "b1c39d042fdf8f00a407b0df734764beb6c3b062",
        "3694381e74445a8b9f8cb8d373e39626e47191b5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "148011adfae37b821407aae84fcbbf7fb4619eb6",
    "abstract": "Transformer networks are able to capture patterns in data coming from many domains (text, images, videos, proteins, etc.) with little or no change to architecture components. We perform a theoretical analysis of the core component responsible for signal propagation between elements, i.e. the self-attention matrix. In practice, this matrix typically exhibits two properties: (1) it is sparse, meaning that each token only attends to a small subset of other tokens; and (2) it changes dynamically depending on the input to the module. With these considerations in mind, we ask the following question: Can a fixed self-attention module approximate arbitrary sparse patterns depending on the input? How small is the hidden size $d$ required for such approximation? We make progress in answering this question and show that the self-attention matrix can provably approximate sparse matrices, where sparsity is in terms of a bounded number of nonzero elements in each row and column. While the parameters of self-attention are fixed, various sparse matrices can be approximated by only modifying the inputs. Our proof is based on the random projection technique and uses the seminal Johnson-Lindenstrauss lemma. Our proof is constructive, enabling us to propose an algorithm for finding adaptive inputs and fixed self-attention parameters in order to approximate a given matrix. In particular, we show that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, $d$ grows only logarithmically with the sequence length $L$ (i.e. $d = O(\\log L)$).",
    "authors": [
        "Valerii Likhosherstov",
        "K. Choromanski",
        "Adrian Weller"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, d grows only logarithmically with the sequence length $L$ (i.e. $d = O(\\log L)$)."
    },
    "citationCount": 27,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}