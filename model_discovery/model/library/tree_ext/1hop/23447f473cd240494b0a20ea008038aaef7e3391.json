{
    "acronym": "23447f473cd240494b0a20ea008038aaef7e3391",
    "title": "RankGen: Improving Text Generation with Large Ranking Models",
    "seed_ids": [
        "gpt2",
        "compressivetransformer",
        "5697a0ede5425954d48daa6e1893dc87bd7d8be7",
        "b9a701c90f3d3df27366f5b29a97f798eb940ac7",
        "6960918666a8c9d50343bbe9e94baf6415edd5fb",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "4a6a65968a8eb8c09ffb57a7774ddabb596565b1",
        "492a655a67e6ec7423a968cedb70eec0cdbc8e98",
        "f75d05e759447c2aedb7097728f29f9a520d9bc1",
        "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
        "25db56fc85fe15625c3375064a35e908ba6dfd2a",
        "f51497f463566581874c941353dd9d80069c5b77",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "23447f473cd240494b0a20ea008038aaef7e3391",
    "abstract": "Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and (2) sequences generated from a large language model conditioned on the prefix. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We release our model checkpoints, code, and human preference data with explanations to facilitate future research.",
    "authors": [
        "Kalpesh Krishna",
        "Yapei Chang",
        "J. Wieting",
        "Mohit Iyyer"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix that significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics and human evaluations with English writers."
    },
    "citationCount": 57,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}