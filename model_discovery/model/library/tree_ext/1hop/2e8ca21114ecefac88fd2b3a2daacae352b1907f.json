{
    "acronym": "2e8ca21114ecefac88fd2b3a2daacae352b1907f",
    "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
    "seed_ids": [
        "mamba",
        "4ea5ca620122e6a9a2b000444d36491cebf49c7c",
        "4d76206515d6b33903937474273885476fc2771e",
        "a54761081c2b001c057fb6e1ea9a48058d5aa5e0",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "0456cd227edb95e596e3915ebcfd1133bcc8d725",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "ca31b8584b6c022ef15ddfe994fe361e002b7729",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "52045d4d4ae305aebb9e92fbbcf23104242c4d31",
        "980e55d9226cac302d0fae7732da4e67b8bc952c",
        "9575afb5702bc33d7df14c48feeee5901ea00369",
        "a7ca1bce0af7fe4703f5c3296db2dcc8dc112f20",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "d5e999aae76d5270ef272076979c809817458212",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8"
    ],
    "s2id": "2e8ca21114ecefac88fd2b3a2daacae352b1907f",
    "abstract": "Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational cost. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.",
    "authors": [
        "Xindi Wang",
        "Mahsa Salmani",
        "Parsa Omidi",
        "Xiangyu Ren",
        "Mehdi Rezagholizadeh",
        "A. Eshaghi"
    ],
    "venue": "Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, are reviewed, designed to enhance the processing of longer sequences while avoiding a proportional increase in computational cost."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}