{
    "acronym": "21032d1669b7e6fc58ef3a272e6a2f1f4ba15f10",
    "title": "Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models",
    "seed_ids": [
        "gpt2",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "21032d1669b7e6fc58ef3a272e6a2f1f4ba15f10",
    "abstract": "Fine-tuning language models (LMs) has demonstrated success in a wide array of downstream tasks. However, as LMs are scaled up, the memory requirements for backpropagation become prohibitively high. Zeroth-order (ZO) optimization methods can leverage memory-efficient forward passes to estimate gradients. More recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently outperform zero-shot and in-context learning when combined with suitable task prompts. In this work, we couple ZO methods with variance reduction techniques to enhance stability and convergence for inference-based LM fine-tuning. We introduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) and demonstrate its efficacy across multiple LM fine-tuning tasks, eliminating the reliance on task-specific prompts. Evaluated across a range of both masked and autoregressive LMs on benchmark GLUE tasks, MeZO-SVRG outperforms MeZO with up to 20% increase in test accuracies in both full- and partial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced computation time as it often surpasses MeZO's peak test accuracy with a $2\\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required memory footprint compared to first-order SGD, i.e. by $2\\times$ for autoregressive models. Our experiments highlight that MeZO-SVRG's memory savings progressively improve compared to SGD with larger batch sizes.",
    "authors": [
        "Tanmay Gautam",
        "Youngsuk Park",
        "Hao Zhou",
        "Parameswaran Raman",
        "Wooseok Ha"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) is introduced and its efficacy across multiple LM fine-tuning tasks is demonstrated, eliminating the reliance on task-specific prompts."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}