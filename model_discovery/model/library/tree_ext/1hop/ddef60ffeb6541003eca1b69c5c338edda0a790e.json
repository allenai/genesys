{
    "acronym": "ddef60ffeb6541003eca1b69c5c338edda0a790e",
    "title": "ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution",
    "seed_ids": [
        "gpt3",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ddef60ffeb6541003eca1b69c5c338edda0a790e",
    "abstract": "Lexical Substitution discovers appropriate substitutes for a given target word in a context sentence. However, the task fails to consider substitutes that are of equal or higher proficiency than the target, an aspect that could be beneficial for language learners looking to improve their writing. To bridge this gap, we propose a new task, language proficiency-oriented lexical substitution. We also introduce ProLex, a novel benchmark designed to assess systems' ability to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency. Besides the benchmark, we propose models that can automatically perform the new task. We show that our best model, a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex.",
    "authors": [
        "Xuanming Zhang",
        "Zixun Chen",
        "Zhou Yu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The best model is shown that a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}