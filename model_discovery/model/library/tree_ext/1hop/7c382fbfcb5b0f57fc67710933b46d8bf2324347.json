{
    "acronym": "7c382fbfcb5b0f57fc67710933b46d8bf2324347",
    "title": "Optimized Tokenization Process for Open-Vocabulary Code Completion: An Empirical Study",
    "seed_ids": [
        "gpt",
        "gpt2",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "49cf6a22a5dac5bc98b653534af65ffa0bc0e76d",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "7c382fbfcb5b0f57fc67710933b46d8bf2324347",
    "abstract": "Studies have substantiated the efficacy of deep learning-based models in various source code modeling tasks. These models are usually trained on large datasets that are divided into smaller units, known as tokens, utilizing either an open or closed vocabulary system. The selection of a tokenization method can have a profound impact on the number of tokens generated, which in turn can significantly influence the performance of the model. This study investigates the effect of different tokenization methods on source code modeling and proposes an optimized tokenizer to enhance the tokenization performance. The proposed tokenizer employs a hybrid approach that initializes with a global vocabulary based on the most frequent unigrams and incrementally builds an open-vocabulary system. The proposed tokenizer is evaluated against popular tokenization methods such as Closed, Unigram, WordPiece, and BPE tokenizers, as well as tokenizers provided by large pre-trained models such as PolyCoder and CodeGen. The results indicate that the choice of tokenization method can significantly impact the number of sub-tokens generated, which can ultimately influence the modeling performance of a model. Furthermore, our empirical evaluation demonstrates that the proposed tokenizer outperforms other baselines, achieving improved tokenization performance both in terms of a reduced number of sub-tokens and time cost. In conclusion, this study highlights the significance of the choice of tokenization method in source code modeling and the potential for improvement through optimized tokenization techniques.",
    "authors": [
        "Yasir Hussain",
        "Zhiqiu Huang",
        "Yu Zhou",
        "I. A. Khan",
        "Nasrullah Khan",
        "Muhammad Zahid Abbas"
    ],
    "venue": "International Conference on Evaluation & Assessment in Software Engineering",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The proposed tokenizer employs a hybrid approach that initializes with a global vocabulary based on the most frequent unigrams and incrementally builds an open-vocabulary system, achieving improved tokenization performance both in terms of a reduced number of sub-tokens and time cost."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}