{
    "acronym": "7e2cf6f2f2b0df1c4e4bc8c9dd5d1dfd2e38caae",
    "title": "ULSeq-TA: Ultra-Long Sequence Attention Fusion Transformer Accelerator Supporting Grouped Sparse Softmax and Dual-Path Sparse LayerNorm",
    "seed_ids": [
        "bert",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "7e2cf6f2f2b0df1c4e4bc8c9dd5d1dfd2e38caae",
    "abstract": "Transformer networks have been increasingly successful in various fields. The input sequence lengths have become much larger as the algorithm and task complexity develops, which is challenging due to high computational and storage cost. Softmax and LayerNorm are bottleneck nonlinear operators in ultra-long sequence Transformer networks. To improve the efficiency of Softmax, assumption-based and quantization-based Softmax approaches are introduced. However, the sparsity potential to accelerate Softmax itself is not fully discovered. To improve the efficiency of LayerNorm, some works reduce the input size, and some works explore the pipeline. However, the sparsity potential is also not yet explored. To address these challenges, this article presents the ULSeq-TA software\u2013hardware co-design framework. The software includes 1) the grouped sparse Softmax method to leverage the data magnifying characteristic to explore the middle and post-Softmax sparse processing and 2) the dual-path sparse LayerNorm method which explores the dimensional significance for sparse calculation. The hardware includes 1) an attention fusion architecture which reduces the on-chip memory with fused operators; 2) the grouped sparse Softmax core; and 3) the dual-path sparse LayerNorm core. Experiments show that the software achieves <inline-formula> <tex-math notation=\"LaTeX\">$4.45\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$7.59\\times $ </tex-math></inline-formula> computation reduction with little output difference for Softmax and LayerNorm, respectively. The hardware architecture supports at most 32768 sequence length with only 186-kB on-chip memory and achieves <inline-formula> <tex-math notation=\"LaTeX\">$1.75\\times -1.98\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$3.22\\times -4.32\\times $ </tex-math></inline-formula> speedups for sparse Softmax core and sparse LayerNorm core with little accuracy loss, respectively.",
    "authors": [
        "Jingyu Wang",
        "Lu Zhang",
        "Xueqing Li",
        "Huazhong Yang",
        "Yongpan Liu"
    ],
    "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The ULSeq-TA software\u2013hardware co-design framework is presented, which includes the grouped sparse Softmax method to leverage the data magnifying characteristic to explore the middle and post-Softmax sparse processing and the dual-path sparse LayerNorm method which explores the dimensional significance for sparse calculation."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}