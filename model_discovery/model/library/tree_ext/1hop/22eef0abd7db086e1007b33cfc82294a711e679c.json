{
    "acronym": "22eef0abd7db086e1007b33cfc82294a711e679c",
    "title": "SEHY: A Simple yet Effective Hybrid Model for Summarization of Long Scientific Documents",
    "seed_ids": [
        "bigbird",
        "longformer",
        "longt5",
        "e96493b4181de6c60b761dc66492db8e66fd784f",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "a14ef90b8fa01b3ddc231eb491c76a6f7458976e",
        "29168348f4729d418df5acc8a5fce4f1c428a7e3",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "01b15017ac59b8d6f2ce3598c4a7d6358c211426",
        "bb0ce614ab4c0251e6a4fc16f695b5facbfc1d70",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "22eef0abd7db086e1007b33cfc82294a711e679c",
    "abstract": "Long-document summarization has been recently recognized as one of the most important natural language processing (NLP) tasks, yet one of the least solved ones. Extractive approaches attempt to choose salient sentences via understanding the whole document, but long documents cover numerous subjects with varying details and will not ease content understanding. Instead, abstractive approaches elaborate to generate related tokens while suffering from truncating the source document due to their input sizes. To this end, we propose a S imple yet E ffective HY brid approach, which we call SEHY , that exploits the discourse information of a document to select salient sections instead sentences for summary generation. On the one hand, SEHY avoids the full-text understanding; on the other hand, it retains salient information given the length limit. In particular, we design two simple strategies for training the extractor: extracting sections incrementally and based on salience-analysis. Then, we use strong abstractive models to generate the final summary. We evaluate our approach on a large-scale scientific paper dataset: arXiv. Further, we discuss how the disciplinary class (e.g., computer science, math or physics) of a scientific paper affects the performance of SEHY as its writing style indicates, which is unexplored yet in existing works. Experimental results show the effectiveness of our approach and interesting findings on arXiv and its subsets generated in this paper.",
    "authors": [
        "Zhihua Jiang",
        "Junzhan Yang",
        "Dongning Rao"
    ],
    "venue": "AACL/IJCNLP",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a approach, which is called SEHY, that exploits the discourse information of a document to select salient sections instead sentences for summary generation and uses strong abstractive models to generate the final summary."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}