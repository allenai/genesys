{
    "acronym": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
    "title": "Efficient Transformers: A Survey",
    "seed_ids": [
        "synthesizer",
        "funneltransformer",
        "performer",
        "linformer",
        "lineartransformer",
        "bigbird",
        "longformer",
        "blockbert",
        "sparsetransformer",
        "sinkhorn",
        "routingtransformer",
        "transformerxl",
        "compressivetransformer",
        "productkeymem",
        "memcompress",
        "etc",
        "perceiverio",
        "axialattn",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "0cd82dfae930ac4b57c0e959f744f2d10bf87649",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "f6390beca54411b06f3bde424fb983a451789733",
        "830995ef17cc291c13f42dfd9f462137de1d2179",
        "e763fdc9ae56826ff799163ea035b29bffd8ea6f",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "2a31319e73d4486716168b65cdf7559baeda18ce",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
    "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
    "authors": [
        "Yi Tay",
        "Mostafa Dehghani",
        "Dara Bahri",
        "Donald Metzler"
    ],
    "venue": "ACM Computing Surveys",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains."
    },
    "citationCount": 898,
    "influentialCitationCount": 76,
    "code": null,
    "description": null,
    "url": null
}