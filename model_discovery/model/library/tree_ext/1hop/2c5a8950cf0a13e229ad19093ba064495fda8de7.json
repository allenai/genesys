{
    "acronym": "2c5a8950cf0a13e229ad19093ba064495fda8de7",
    "title": "A Neural Scaling Law from the Dimension of the Data Manifold",
    "seed_ids": [
        "gpt",
        "d28c18a3c2a0afdc0a8634d18345af8d36e1f948",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2c5a8950cf0a13e229ad19093ba064495fda8de7",
    "abstract": "When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \\propto N^{-\\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\\alpha \\approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.",
    "authors": [
        "Utkarsh Sharma",
        "J. Kaplan"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work confirms the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, and test the theory with CNN image classifiers on several datasets and with GPT-type language models."
    },
    "citationCount": 41,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}