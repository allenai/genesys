{
    "acronym": "53a803388e83ae89261624099d7be4287ace67cb",
    "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
    "seed_ids": [
        "transformer",
        "roformer",
        "b085968c4362fb286ad6c5ef71a5db9630da0498",
        "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28"
    ],
    "s2id": "53a803388e83ae89261624099d7be4287ace67cb",
    "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
    "authors": [
        "Zhihong Shao",
        "Damai Dai",
        "Daya Guo",
        "Bo Liu",
        "Zihan Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models."
    },
    "citationCount": 46,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}