{
    "acronym": "468baf87af6971ba4bdd47ee1b3f05bbae8ecdc7",
    "title": "Proxy-based Metric Learning for Emotion Recognition",
    "seed_ids": [
        "gpt2",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "468baf87af6971ba4bdd47ee1b3f05bbae8ecdc7",
    "abstract": "Emotion Recognition (ER) is an essential research area of natural language processing that can be applied to various fields. Texts in the fields of health care, marketing, and psychological counseling take various forms, and it is very important from a business point of view to find the emotions inherent in these texts. Recently, ER using text embeddings generated through a pre-trained language model with a large corpus was performed. However, since the embeddings are generalized to various domains, there is a limitation to directly using them for ER. In this study, to overcome the limitation, we propose a method that modifies generalized embeddings to emotional embeddings by performing proxy-based metric learning. In the proposed method, we fine-tuned the pre-trained language model by using proxy-anchor loss so that embeddings represent emotion appropriately. Previous studies only added linear classifiers. But, it is possible to capture emotional relationships between data by using proxy-based metric learning. In this study, we conducted ER experiments with benchmark datasets. The experimental result shows that the proposed method achieves better performance than the baseline and creates emotion-specific embeddings.",
    "authors": [
        "Junhyeong Park",
        "Geonsik Youn",
        "Bohan Yoon",
        "Byeonghun Kim",
        "J. Rhee"
    ],
    "venue": "International Conference on Advances in Artificial Intelligence",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A method is proposed that modifies generalized embeddings to emotional embeddeddings by performing proxy-based metric learning and fine-tuned the pre-trained language model by using proxy-anchor loss so thatembeddings represent emotion appropriately."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}