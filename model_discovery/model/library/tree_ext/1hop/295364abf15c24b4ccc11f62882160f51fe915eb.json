{
    "acronym": "295364abf15c24b4ccc11f62882160f51fe915eb",
    "title": "EdgeTran: Device-Aware Co-Search of Transformers for Efficient Inference on Mobile Edge Platforms",
    "seed_ids": [
        "bert",
        "1bcd42583a7b4475d3b456678e7f3752acd9edd1",
        "2a218786f4615b82389f78472e7ff22e6ce57490",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "b76e98a0a023d37c6534aa2ead09c8ff595f0bae"
    ],
    "s2id": "295364abf15c24b4ccc11f62882160f51fe915eb",
    "abstract": "Automated design of efficient transformer models has recently attracted significant attention from industry and academia. However, most works only focus on certain metrics while searching for the best-performing transformer architecture. Furthermore, running traditional, complex, and large transformer models on low-compute edge platforms is a challenging problem. In this work, we propose a framework, called ProTran, to profile the hardware performance measures for a design space of transformer architectures and a diverse set of edge devices. We use this profiler in conjunction with the proposed co-search technique to obtain the best-performing models that have high accuracy on the given task and minimize latency, energy consumption, and peak power draw to enable edge deployment. We refer to our framework for co-optimizing accuracy and hardware performance measures as EdgeTran. It searches for the best transformer model and edge device pair. Finally, we propose GPTran, a multi-stage block-level grow-and-prune post-processing step that further improves accuracy in a hardware-aware manner. The obtained transformer model is 2.8\u00d7 smaller and has a 0.8% higher GLUE score than the baseline (BERT-Base). Inference with it on the selected edge device enables 15.0% lower latency, 10.0\u00d7 lower energy, and 10.8\u00d7 lower peak power draw compared to an off-the-shelf GPU.",
    "authors": [
        "Shikhar Tuli",
        "N. Jha"
    ],
    "venue": "IEEE Transactions on Mobile Computing",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a framework, called ProTran, to profile the hardware performance measures for a design space of transformer architectures and a diverse set of edge devices and proposes GPTran, a multi-stage block-level grow-and-prune post-processing step that further improves accuracy in a hardware-aware manner."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}