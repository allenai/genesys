{
    "acronym": "a5edb776606696127a27223315af48c01f62797c",
    "title": "Unsupervised Multi-View Post-OCR Error Correction With Language Models",
    "seed_ids": [
        "gpt",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a5edb776606696127a27223315af48c01f62797c",
    "abstract": "We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for error correction is too risky. We evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view. We also show the importance of domain adaptation for post-OCR correction on out-of-domain documents.",
    "authors": [
        "Harsh Gupta",
        "Luciano Del Corro",
        "Samuel Broscheit",
        "Johannes Hoffart",
        "Eliot Brenner"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study investigates post-OCR correction in a setting where the authors have access to different OCR views of the same document and evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view."
    },
    "citationCount": 8,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}