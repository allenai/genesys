{
    "acronym": "f0a4ea1386ba62561019ef7d33bbb2aec23239c5",
    "title": "GraphGPT: Graph Learning with Generative Pre-trained Transformers",
    "seed_ids": [
        "gpt3",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "5eda60d4940d4185df45c5703e103458171d465d",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "f0a4ea1386ba62561019ef7d33bbb2aec23239c5",
    "abstract": "We introduce \\textit{GraphGPT}, a novel model for Graph learning by self-supervised Generative Pre-training Transformers. Our model transforms each graph or sampled subgraph into a sequence of tokens representing the node, edge and attributes reversibly using the Eulerian path first. Then we feed the tokens into a standard transformer decoder and pre-train it with the next-token-prediction (NTP) task. Lastly, we fine-tune the GraphGPT model with the supervised tasks. This intuitive, yet effective model achieves superior or close results to the state-of-the-art methods for the graph-, edge- and node-level tasks on the large scale molecular dataset PCQM4Mv2, the protein-protein association dataset ogbl-ppa and the ogbn-proteins dataset from the Open Graph Benchmark (OGB). Furthermore, the generative pre-training enables us to train GraphGPT up to 400M+ parameters with consistently increasing performance, which is beyond the capability of GNNs and previous graph transformers. The source code and pre-trained checkpoints will be released soon\\footnote{\\url{https://github.com/alibaba/graph-gpt}} to pave the way for the graph foundation model research, and also to assist the scientific discovery in pharmaceutical, chemistry, material and bio-informatics domains, etc.",
    "authors": [
        "Qifang Zhao",
        "Weidong Ren",
        "Tianyu Li",
        "Xiaoxiao Xu",
        "Hong Liu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The generative pre-training enables the GraphGPT model to train up to 400M+ parameters with consistently increasing performance, which is beyond the capability of GNNs and previous graph transformers."
    },
    "citationCount": 5,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}