{
    "acronym": "cab81372c4886ba5af952807cc01c970fc3a1df7",
    "title": "UNSUPERVISED GENERATIVE PRE-TRAINING",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "cab81372c4886ba5af952807cc01c970fc3a1df7",
    "abstract": "The French TARPON project aims to build a national injury surveillance system based on emergency room (ER) visit reports. To this end, it is necessary to develop a coding system capable of classifying the causes of these visits based on the automatic reading of clinical notes written by emergency room clinicians. While supervised learning techniques have shown good results in this area, they require manual coding of a large number of texts in order to build a sufficiently large learning annotated sample. New levels of performance have been recently achieved in neural language models (NLM) over the past two years with the use of models based on the Transformer architecture with an unsupervised generative pre-training step. Our hypothesis is that methods involving a generative self-supervised pre-training step significantly reduce the number of annotated samples required for the supervised fine-tuning phase. To measure the potential gain in terms of manual annotation work obtained by adopting this pre-training step, we exploited the fact that we could derive from available diagnostic codes the traumatic/non-traumatic nature of the cause of the ER visit. We then designed a case study to predict from free text clinical notes whether a given ER visit was the consequence of a traumatic or a non-traumatic event. We compared two strategies: Strategy A consisted in training the GPT-2 NLM on the training dataset (with a maximum of 161 930 samples) with all labels (trauma/non-trauma) in a single fully-supervised phase. In Strategy B, we split the training dataset in two parts, a large one of 151 930 samples without any label for the self-supervised pre-training phase and a much smaller one (up to 10 000 samples) for the supervised fine-tuning with labels. \u2217corresponding author, emmanuel.lagarde@u-bordeaux.fr ar X iv :1 90 9. 01 13 6v 2 [ cs .C L ] 4 S ep 2 01 9 Neural Language Model for Automated Classification of Electronic Medical Records at the Emergency Room. The Significant Benefit of Unsupervised Generative Pre-training A PREPRINT The results showed that in Strategy A, AUC and F1 score reach 0.97 and 0.89 respectively after the processing of 40 000 samples. The use of generative pre-training (Strategy B) achieved an AUC of 0.93 and an F1-score of 0.80 after the processing of only 120 samples. The same performance was achieved with only 30 labeled samples processed 3 times (3 epochs of learning). To conclude, it is possible to easily adapt a multi-purpose NLM model such as the GPT-2 to create a powerful tool for classification of free-text notes with the need of a very small number of labeled samples.",
    "authors": [
        "A. Preprint",
        "Binbin Xu",
        "\u00c9ric Tellier",
        "E. Lagarde"
    ],
    "venue": "",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is possible to easily adapt a multi-purpose NLM model such as the GPT-2 to create a powerful tool for classification of free-text notes with the need of a very small number of labeled samples, and to measure the potential gain in terms of manual annotation work obtained by adopting this pre-training step."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}