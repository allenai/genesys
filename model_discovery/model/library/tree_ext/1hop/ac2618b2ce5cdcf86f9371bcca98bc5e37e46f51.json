{
    "acronym": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "seed_ids": [
        "hippo",
        "ca444821352a4bd91884413d8070446e2960715a",
        "11df9ac34655f4ad746e4db39c49f928f0cbd201",
        "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "af34ea4242ca8725ea739ec1bef674bec10c1fa9"
    ],
    "s2id": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
    "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",
    "authors": [
        "Albert Gu",
        "Karan Goel",
        "Christopher R'e"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths."
    },
    "citationCount": 757,
    "influentialCitationCount": 119,
    "code": null,
    "description": null,
    "url": null
}