{
    "acronym": "bafb2ede2e3483650a8f71541da4f1a112c39f12",
    "title": "The CUR Decomposition of Self-attention",
    "seed_ids": [
        "nystromformer",
        "soft",
        "7368c3cdf7cbed194e96dc4da53ed61f185e3d82",
        "131ba9932572c92155874db93626cf299659254e",
        "ec139916edd6feb9b3cb3a0325ca96e21dbb0147",
        "0e76c22414af949a03edf7b77e2df4a117aa1f5e",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "2d98048c2d2fcd3f6b989d2a54003808906ab4b7",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "bafb2ede2e3483650a8f71541da4f1a112c39f12",
    "abstract": "Transformers have achieved great success in natural language processing and computer vision. The core and basic technique of transformers is the self-attention mechanism. The vanilla self-attention mechanism has quadratic complexity, which limits its applications to vision tasks. Most of the existing linear self-attention mechanisms will sacri\ufb01ce performance to some extent for reducing complexity. In this paper, we propose a novel linear approximation of the vanilla self-attention mechanism named CURSA to achieve both high performance and low complexity at the same time. CURSA is based on the CUR decomposition to decompose the multiplication of large matrices into the multiplication of several small matrices to achieve linear complexity. Experiment results of CURSA in image classi\ufb01cation tasks show that it outperforms state-of-the-art self-attention mechanisms with better data e\ufb03ciency, faster speed",
    "authors": [
        "Chong Wu",
        "Maolin Che",
        "Hong Yan"
    ],
    "venue": "",
    "year": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experimental results of CURSA in image classi\ufb01cation tasks show that it outperforms state-of-the-art self-attention mechanisms with better data e\ufb03ciency, faster speed."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}