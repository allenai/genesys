{
    "acronym": "ca09c5d201b938fd7a14bd4e469a5313f1966827",
    "title": "ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys",
    "seed_ids": [
        "linformer",
        "93e58491830abe1eb965ab37ec64fa97263f6048",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "c49c292e1fb1d215c88828a52134b7ccfa52be44"
    ],
    "s2id": "ca09c5d201b938fd7a14bd4e469a5313f1966827",
    "abstract": "We propose a new attention mechanism with linear complexity, ATP, that fixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than on each individual token. Particularly, ATP is driven by an important observation that input sequences are typically low-rank, i.e., input sequences can be represented by a few principal bases. Therefore, instead of directly iterating over all the input tokens, ATP transforms inputs into an orthogonal space and computes attention only on the top principal bases (keys). Owing to the observed low-rank structure in input sequences, ATP is able to capture semantic relationships in input sequences with a few principal keys. Furthermore, the attention complexity is reduced from \\emph{quadratic} to \\emph{linear} without incurring a noticeable performance drop. ATP further reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely target the attention module. Our evaluations on various models (e.g., BERT and Llama) demonstrate that ATP achieves comparable accuracy with much lower computation and memory complexity than the standard attention mechanism. In particular, ATP barely loses accuracy with only $1/2$ principal keys, and only incurs around $2\\%$ accuracy drops with $1/4$ principal keys.",
    "authors": [
        "Yue Niu",
        "Saurav Prakash",
        "S. Avestimehr"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "ATP achieves comparable accuracy with much lower computation and memory complexity than the standard attention mechanism, and reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely target the attention module."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}