{
    "acronym": "c572f53f66896845430aa7a43f2e99a15ad1919b",
    "title": "Randomness Regularization With Simple Consistency Training for Neural Networks",
    "seed_ids": [
        "bert",
        "ef8854a62e05c8e741894166689a9cd8352a1df0",
        "25db56fc85fe15625c3375064a35e908ba6dfd2a",
        "3bc53c49ae68adacf2d5be2fa795bcb879e2717a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c572f53f66896845430aa7a43f2e99a15ad1919b",
    "abstract": "Randomness is widely introduced in neural network training to simplify model optimization or avoid the over-fitting problem. Among them, dropout and its variations in different aspects (e.g., data, model structure) are prevalent in regularizing the training of deep neural networks. Though effective and performing well, the randomness introduced by these dropout-based methods causes nonnegligible inconsistency between training and inference. In this paper, we introduce a simple consistency training strategy to regularize such randomness, namely R-Drop, which forces two output distributions sampled by each type of randomness to be consistent. Specifically, R-Drop minimizes the bidirectional KL-divergence between two output distributions produced by dropout-based randomness for each training sample. Theoretical analysis reveals that R-Drop can reduce the above inconsistency by reducing the inconsistency among the sampled sub structures and bridging the gap between the loss calculated by the full model and sub structures. Experiments on <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{7}$</tex-math><alternatives><mml:math><mml:mn mathvariant=\"bold\">7</mml:mn></mml:math><inline-graphic xlink:href=\"li-ieq1-3370716.gif\"/></alternatives></inline-formula> widely-used deep learning tasks (<inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{23}$</tex-math><alternatives><mml:math><mml:mn mathvariant=\"bold\">23</mml:mn></mml:math><inline-graphic xlink:href=\"li-ieq2-3370716.gif\"/></alternatives></inline-formula> datasets in total) demonstrate that R-Drop is universally effective for different types of neural networks (i.e., feed-forward, recurrent, and graph neural networks) and different learning paradigms (supervised, parameter-efficient, and semi-supervised). In particular, it achieves state-of-the-art performances with the vanilla Transformer model on WMT14 English <inline-formula><tex-math notation=\"LaTeX\">$\\to$</tex-math><alternatives><mml:math><mml:mo>\u2192</mml:mo></mml:math><inline-graphic xlink:href=\"li-ieq3-3370716.gif\"/></alternatives></inline-formula> German translation (<inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{30.91}$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=\"bold\">30</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant=\"bold\">91</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq4-3370716.gif\"/></alternatives></inline-formula> BLEU) and WMT14 English <inline-formula><tex-math notation=\"LaTeX\">$\\to$</tex-math><alternatives><mml:math><mml:mo>\u2192</mml:mo></mml:math><inline-graphic xlink:href=\"li-ieq5-3370716.gif\"/></alternatives></inline-formula> French translation (<inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{43.95}$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=\"bold\">43</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant=\"bold\">95</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq6-3370716.gif\"/></alternatives></inline-formula> BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models.",
    "authors": [
        "Juntao Li",
        "Xiaobo Liang",
        "Lijun Wu",
        "Yue Wang",
        "Qi Meng",
        "Tao Qin",
        "Min Zhang",
        "Tie-Yan Liu"
    ],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "R-Drop is universally effective for different types of neural networks and different learning paradigms (supervised, parameter-efficient, and semi-supervised) and achieves state-of-the-art performances with the vanilla Transformer model."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}