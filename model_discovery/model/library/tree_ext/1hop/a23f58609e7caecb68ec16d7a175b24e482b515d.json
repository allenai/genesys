{
    "acronym": "a23f58609e7caecb68ec16d7a175b24e482b515d",
    "title": "Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation",
    "seed_ids": [
        "longformer",
        "e7c698bdace380f7183dedbe657686f1885f615c",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a23f58609e7caecb68ec16d7a175b24e482b515d",
    "abstract": "With the recent advances of open-domain story generation, the lack of reliable automatic evaluation metrics becomes an increasingly imperative issue that hinders the fast development of story generation. According to conducted researches in this regard, learnable evaluation metrics have promised more accurate assessments by having higher correlations with human judgments. A critical bottleneck of obtaining a reliable learnable evaluation metric is the lack of high-quality training data for classifiers to efficiently distinguish plausible and implausible machine-generated stories. Previous works relied on heuristically manipulated plausible examples to mimic possible system drawbacks such as repetition, contradiction, or irrelevant content in the text level, which can be unnatural and oversimplify the characteristics of implausible machine-generated stories. We propose to tackle these issues by generating a more comprehensive set of implausible stories using plots, which are structured representations of controllable factors used to generate stories. Since these plots are compact and structured, it is easier to manipulate them to generate text with targeted undesirable properties, while at the same time maintain the grammatical correctness and naturalness of the generated sentences. To improve the quality of generated implausible stories, we further apply the adversarial filtering procedure presented by (CITATION) to select a more nuanced set of implausible texts. Experiments show that the evaluation metrics trained on our generated data result in more reliable automatic assessments that correlate remarkably better with human judgments compared to the baselines.",
    "authors": [
        "Sarik Ghazarian",
        "Zixi Liu",
        "S. Akash",
        "R. Weischedel",
        "A. Galstyan",
        "Nanyun Peng"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experiments show that the evaluation metrics trained on the generated data result in more reliable automatic assessments that correlate remarkably better with human judgments compared to the baselines."
    },
    "citationCount": 13,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}