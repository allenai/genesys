{
    "acronym": "f54aa5a594d054e9564413ed4c30d18f2e747bc7",
    "title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models",
    "seed_ids": [
        "gpt3",
        "diffusionbert",
        "diffusionlm",
        "likelihooddifflm",
        "diffuseq",
        "1bfe2a9a40a5f34c5c6b99c182d37a6e93f95aa9",
        "017e4a43b69340aecf7d002aa1bb5991f9e91ffb",
        "21bc2ba103891049075d70bc3a857b16ca1a71aa",
        "9cbbb250a565228ba328038ee7944b89cff53e84",
        "4c26a1c6e573c772e1634b1747d7e9a3b33dce52",
        "d9ffb44ee3c8ec0b6692df8a90451384c1edd89b",
        "4ca824e792f3a2c777ffa5896a2e7cdf11b9518d",
        "3a22aad6c18a9559be3bbb197494b434b872a05a",
        "020a50f6a7154850ac81e3cde69ad8198ded6751",
        "1f898d66acabff511a3871b82799aa73c0055402",
        "48abfc41a0abf023d2037ebb2f274835e0d322d0",
        "a1186d7d9a9ef258c76afef1177e4f348061a537",
        "a979742220a88b1d32e1fbe72c41e8ba3007053c",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "92173d081b15824d22a9ef070e118744ceee8052",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "24425954960ce968e5f14360fbdd0605abcadfcf"
    ],
    "s2id": "f54aa5a594d054e9564413ed4c30d18f2e747bc7",
    "abstract": "Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy. In addition to that, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models.",
    "authors": [
        "Jiacheng Ye",
        "Shansan Gong",
        "Liheng Chen",
        "Lin Zheng",
        "Jiahui Gao",
        "Han Shi",
        "Chuan Wu",
        "Zhenguo Li",
        "Wei Bi",
        "Lingpeng Kong"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Diffusion-of-Thought (DoT), a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}