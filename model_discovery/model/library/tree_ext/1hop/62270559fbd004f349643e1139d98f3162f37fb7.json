{
    "acronym": "62270559fbd004f349643e1139d98f3162f37fb7",
    "title": "ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO",
    "seed_ids": [
        "gpt2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "62270559fbd004f349643e1139d98f3162f37fb7",
    "abstract": "Image-Text matching (ITM) is a common task for evaluating the quality of Vision and Language (VL) models. However, existing ITM benchmarks have a significant limitation. They have many missing correspondences, originating from the data construction process itself. For example, a caption is only matched with one image although the caption can be matched with other similar images and vice versa. To correct the massive false negatives, we construct the Extended COCO Validation (ECCV) Caption dataset by supplying the missing associations with machine and human annotators. We employ five state-of-the-art ITM models with diverse properties for our annotation process. Our dataset provides x3.6 positive image-to-caption associations and x8.5 caption-to-image associations compared to the original MS-COCO. We also propose to use an informative ranking-based metric mAP@R, rather than the popular Recall@K (R@K). We re-evaluate the existing 25 VL models on existing and proposed benchmarks. Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K, CxC R@1 are highly correlated with each other, while the rankings change when we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias introduced by the choice of machine annotator. Source code and dataset are available at https://github.com/naver-ai/eccv-caption",
    "authors": [
        "Sanghyuk Chun",
        "Wonjae Kim",
        "Song Park",
        "Minsuk Chang",
        "Seong Joon Oh"
    ],
    "venue": "European Conference on Computer Vision",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work constructs the Extended COCO Validation (ECCV) Caption dataset by supplying the missing associations with machine and human annotators, and re-evaluate the existing 25 VL models on existing and proposed benchmarks."
    },
    "citationCount": 31,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}