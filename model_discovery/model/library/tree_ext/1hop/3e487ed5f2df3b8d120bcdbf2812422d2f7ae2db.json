{
    "acronym": "3e487ed5f2df3b8d120bcdbf2812422d2f7ae2db",
    "title": "Searching Effective Transformer for Seq2Seq Keyphrase Generation",
    "seed_ids": [
        "transformerxl",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "d6b414487787d0b6efd735a3236a690ad13aae70"
    ],
    "s2id": "3e487ed5f2df3b8d120bcdbf2812422d2f7ae2db",
    "abstract": null,
    "authors": [
        "Yige Xu",
        "Yichao Luo",
        "Yicheng Zhou",
        "Zhengyan Li",
        "Qi Zhang",
        "Xipeng Qiu",
        "Xuanjing Huang"
    ],
    "venue": "Natural Language Processing and Chinese Computing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Comprehensive experiments on multiple KG benchmarks showed that in KG tasks, uninformative content abounds in documents while salient information is diluted globally, and the vanilla Transformer equipped with a fully-connected self-attention mechanism may overlook the local context, leading to performance degradation."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}