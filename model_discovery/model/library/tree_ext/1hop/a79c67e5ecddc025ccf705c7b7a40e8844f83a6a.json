{
    "acronym": "a79c67e5ecddc025ccf705c7b7a40e8844f83a6a",
    "title": "LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism",
    "seed_ids": [
        "transformer",
        "gpt3",
        "ring",
        "flashattn",
        "28f59093730d88719b96041f9544c73671f798bd",
        "ade22704be8a0fc3730d320cc7934b2ccbcd97e4",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "dd4dd3ed1a95beb6e6712ea356a49d1ab818f616",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8"
    ],
    "s2id": "a79c67e5ecddc025ccf705c7b7a40e8844f83a6a",
    "abstract": "Efficiently training LLMs with long sequences is important yet challenged by the massive computation and memory requirements. Sequence parallelism has been proposed to tackle these problems, but existing methods suffer from scalability or efficiency issues. We propose LoongTrain, a novel system to efficiently train LLMs with long sequences at scale. The core of LoongTrain is the 2D-Attention mechanism, which combines both head-parallel and context-parallel techniques to break the scalability constraints while maintaining efficiency. We introduce Double-Ring-Attention and analyze the performance of device placement strategies to further speed up training. We implement LoongTrain with the hybrid ZeRO and Selective Checkpoint++ techniques. Experiment results show that LoongTrain outperforms state-of-the-art baselines, i.e., DeepSpeed-Ulysses and Megatron Context Parallelism, in both end-to-end training speed and scalability, and improves Model FLOPs Utilization (MFU) by up to 2.88x.",
    "authors": [
        "Diandian Gu",
        "Peng Sun",
        "Qi Hu",
        "Ting Huang",
        "Xun Chen",
        "Yingtong Xiong",
        "Guoteng Wang",
        "Qiaoling Chen",
        "Shangchun Zhao",
        "Jiarui Fang",
        "Yonggang Wen",
        "Tianwei Zhang",
        "Xin Jin",
        "Xuanzhe Liu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The core of LoongTrain is the 2D-Attention mechanism, which combines both head-parallel and context-parallel techniques to break the scalability constraints while maintaining efficiency, and improves Model FLOPs Utilization by up to 2.88x."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}