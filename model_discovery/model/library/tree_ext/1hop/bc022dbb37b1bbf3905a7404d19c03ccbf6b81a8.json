{
    "acronym": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
    "title": "Generative Pretraining From Pixels",
    "seed_ids": [
        "sparsetransformer",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
    "abstract": "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we \ufb01nd that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full \ufb01ne-tuning, matching the top supervised pre-trained models. An even larger model trained on a mix-ture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",
    "authors": [
        "Mark Chen",
        "Alec Radford",
        "Jeff Wu",
        "Heewoo Jun",
        "Prafulla Dhariwal",
        "D. Luan",
        "I. Sutskever"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation, despite training on low-resolution ImageNet without labels."
    },
    "citationCount": 1264,
    "influentialCitationCount": 107,
    "code": null,
    "description": null,
    "url": null
}