{
    "acronym": "4ee1754f89b250753242c3baf58773dda5d9bf39",
    "title": "Transformer on a Diet",
    "seed_ids": [
        "gpt",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "8cd4054b41936ba0889edc26be8969c3dc8491d8",
        "2a31319e73d4486716168b65cdf7559baeda18ce",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4ee1754f89b250753242c3baf58773dda5d9bf39",
    "abstract": "Transformer has been widely used thanks to its ability to capture sequence information in an efficient way. However, recent developments, such as BERT and GPT-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefully-designed light Transformer architectures to figure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available.",
    "authors": [
        "Chenguang Wang",
        "Zihao Ye",
        "Aston Zhang",
        "Zheng Zhang",
        "Alex Smola"
    ],
    "venue": "arXiv.org",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Three carefully-designed light Transformer architectures are explored to figure out whether the Transformer with less computations could produce competitive results, and experimental results on language model benchmark datasets hint that such trade-off is promising."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}