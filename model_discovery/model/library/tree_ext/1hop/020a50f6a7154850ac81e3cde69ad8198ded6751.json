{
    "acronym": "020a50f6a7154850ac81e3cde69ad8198ded6751",
    "title": "DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises",
    "seed_ids": [
        "diffusionlm",
        "diffuseq",
        "contdiffu",
        "9cbbb250a565228ba328038ee7944b89cff53e84",
        "6827e87642874d9bf69f0f1548d79a164aaa5e1e",
        "a1186d7d9a9ef258c76afef1177e4f348061a537",
        "23b7cde603b5ec8d5d13d46e1c453dc52d7c3f6c",
        "a979742220a88b1d32e1fbe72c41e8ba3007053c",
        "22775e58932cdfbd273a2a835a22c5d86800a458",
        "2c6ac935c826002976722ca8d3319f691975687e",
        "0b9770a377b3f96cef9f268cee1791d39a0d4893",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "b64537bdf7a103aa01972ba06ea24a9c08f7cd74",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "24425954960ce968e5f14360fbdd0605abcadfcf",
        "e512964293671abbdc409f313d127cbe85ffe5cd"
    ],
    "s2id": "020a50f6a7154850ac81e3cde69ad8198ded6751",
    "abstract": "While diffusion models have achieved great success in generating continuous signals such as images and audio, it remains elusive for diffusion models in learning discrete sequence data like natural languages. Although recent advances circumvent this challenge of discreteness by embedding discrete tokens as continuous surrogates, they still fall short of satisfactory generation quality. To understand this, we first dive deep into the denoised training protocol of diffusion-based sequence generative models and determine their three severe problems, i.e., 1) failing to learn, 2) lack of scalability, and 3) neglecting source conditions. We argue that these problems can be boiled down to the pitfall of the not completely eliminated discreteness in the embedding space, and the scale of noises is decisive herein. In this paper, we introduce DINOISER to facilitate diffusion models for sequence generation by manipulating noises. We propose to adaptively determine the range of sampled noise scales for counter-discreteness training; and encourage the proposed diffused sequence learner to leverage source conditions with amplified noise scales during inference. Experiments show that DINOISER enables consistent improvement over the baselines of previous diffusion-based sequence generative models on several conditional sequence modeling benchmarks thanks to both effective training and inference strategies. Analyses further verify that DINOISER can make better use of source conditions to govern its generative process.",
    "authors": [
        "Jiasheng Ye",
        "Zaixiang Zheng",
        "Yu Bao",
        "Lihua Qian",
        "Mingxuan Wang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "DINOISER is introduced to facilitate diffusion models for sequence generation by manipulating noises to adaptively determine the range of sampled noise scales for counter-discreteness training; and encourage the proposed diffused sequence learner to leverage source conditions with amplified noise scales during inference."
    },
    "citationCount": 27,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}