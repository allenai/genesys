{
    "acronym": "a66bf047e0830c24e6e91c583a8c27632d7995ab",
    "title": "SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding",
    "seed_ids": [
        "hypermixer",
        "fb7e324729be2931dc463a572566283802d8aff2",
        "1ec58a9f1958f1c2540baadfa8f9809388788ab4",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "6954a6bb9d6f3e365b26b694c963ae1d62a03444",
        "ce920a970dcbb6ebfe19fe986ae7083ad73c7774"
    ],
    "s2id": "a66bf047e0830c24e6e91c583a8c27632d7995ab",
    "abstract": "Modern speech processing systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference and training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but they fail to consistently reach the same level of accuracy. This paper, therefore, proposes a novel linear-time alternative to self-attention. It summarises an utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method\"SummaryMixing\". Introducing SummaryMixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while making training and inference up to 28% faster and reducing memory use by half.",
    "authors": [
        "Titouan Parcollet",
        "R. V. Dalen",
        "Shucong Zhang",
        "S. Bhattacharya"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SummaryMixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while making training and inference up to 28% faster and reducing memory use by half."
    },
    "citationCount": 3,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}