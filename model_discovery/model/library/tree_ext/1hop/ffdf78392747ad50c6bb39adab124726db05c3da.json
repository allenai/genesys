{
    "acronym": "ffdf78392747ad50c6bb39adab124726db05c3da",
    "title": "Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens",
    "seed_ids": [
        "bigbird",
        "longformer",
        "5e52d654fd31f04c1bd884cd5480e6af8c95ad50",
        "ebd1619e5856084cfe60b40cc141a7f69d75b523",
        "79bee63bc21a99dc12dd2725a0c0dcfffa4611ef",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "ffdf78392747ad50c6bb39adab124726db05c3da",
    "abstract": "Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models (as a function of the sequence length), dealing with ultra long sequences (e.g., with more than 16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens (we call VIP-tokens) are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only efficient (achieving more than $3\\times$ efficiency gain compared to baselines on 4K and 16K lengths), but also offers competitive/better performance on a large number of tasks. Further, we show that our algorithm scales to 128K tokens (or more) while consistently offering accuracy improvement.",
    "authors": [
        "Zhanpeng Zeng",
        "Cole Hawkins",
        "Min-Fong Hong",
        "Aston Zhang",
        "Nikolaos Pappas",
        "Vikas Singh",
        "Shuai Zheng"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A VIP-token centric compression scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens, which is not only efficient (achieving more than $3\\times$ efficiency gain compared to baselines on 4K and 16K lengths), but also offers competitive/better performance on a large number of tasks."
    },
    "citationCount": 3,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}