{
    "acronym": "e2bdcdc4b3a4acecd31aa589d690f9aa5c68ca95",
    "title": "Informer, an Information Organization Transformer Architecture",
    "seed_ids": [
        "linformer",
        "sparsetransformer",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06"
    ],
    "s2id": "e2bdcdc4b3a4acecd31aa589d690f9aa5c68ca95",
    "abstract": "The use of architectures based on transformers presents a state of the art revolution in natural language processing (NLP). The employment of these architectures with high computational costs has increased in the last few months, despite the existing use of parallelization techniques. This is due to the high performance that is obtained by increasing the size of the learnable parameters for these kinds of architectures, while maintaining the models\u2019 predictability. This relates to the fact that it is difficult to do research with limited computational resources. A restrictive element is the memory usage, which seriously affects the replication of experiments. We are presenting a new architecture called Informer, which seeks to exploit the concept of information organization. For the sake of evaluation, we use a neural machine translation (NMT) dataset, the English-Vietnamese IWSLT15 dataset (Luong and Manning, 2015). In this paper, we also compare this proposal with architectures that reduce the computational cost to O(n \u00b7 r), such as Linformer (Wang et al., 2020). In addition, we have managed to improve the SOTA of the BLEU score from 33.27 to 35.11.",
    "authors": [
        "Cristian David Estupi\u00f1\u00e1n Ojeda",
        "Cayetano Guerra",
        "M. Hern\u00e1ndez-Tejera"
    ],
    "venue": "International Conference on Agents and Artificial Intelligence",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new architecture called Informer is presented, which seeks to exploit the concept of information organization and is compared with architectures that reduce the computational cost to O(n \u00b7 r), such as Linformer (Wang et al., 2020)."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}