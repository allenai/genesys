{
    "acronym": "d0e2e6c98a7b2c8360aa091c0dc0a270edec8468",
    "title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations",
    "seed_ids": [
        "bert",
        "2fa3f7ce620a1c7155daef6620dd6bb0e01934f3",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "d0e2e6c98a7b2c8360aa091c0dc0a270edec8468",
    "abstract": "Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the subspace is found, we can directly project the original embeddings into the null space to boost language agnosticism without finetuning. We systematically evaluate our method on various tasks including the challenging language-agnostic QA retrieval task. Empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs.",
    "authors": [
        "Zhihui Xie",
        "Handong Zhao",
        "Tong Yu",
        "Shuai Li"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel view of projecting away language-specific factors from a multilingual embedding space is provided and it is found that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information)."
    },
    "citationCount": 5,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}