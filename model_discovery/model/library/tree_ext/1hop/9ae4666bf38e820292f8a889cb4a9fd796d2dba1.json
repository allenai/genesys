{
    "acronym": "9ae4666bf38e820292f8a889cb4a9fd796d2dba1",
    "title": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation",
    "seed_ids": [
        "gpt2",
        "25db56fc85fe15625c3375064a35e908ba6dfd2a",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
        "791c3c30f2af10ac06f4fbc5b1e8960064aacbc7",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "9ae4666bf38e820292f8a889cb4a9fd796d2dba1",
    "abstract": "Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses. With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental results show that our model achieves the new state-of-the-art results on all these datasets.",
    "authors": [
        "Wei Chen",
        "Yeyun Gong",
        "Song Wang",
        "Bolun Yao",
        "Weizhen Qi",
        "Zhongyu Wei",
        "Xiao-Mei Hu",
        "Bartuer Zhou",
        "Yi Mao",
        "Weizhu Chen",
        "Biao Cheng",
        "Nan Duan"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new dialog pre-training framework called DialogVED is proposed, which introduces continuous latent variables into the enhanced encoder-decoder pre- training framework to increase the relevance and diversity of responses."
    },
    "citationCount": 41,
    "influentialCitationCount": 6,
    "code": null,
    "description": null,
    "url": null
}