{
    "acronym": "6b204642e9a231932b9e72b2e401d7a5bc17a51e",
    "title": "Structural Information Guided Multimodal Pre-training for Vehicle-centric Perception",
    "seed_ids": [
        "bert",
        "4990f7542f0600e0501a7e7a931b32eb7cb804d5",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6b204642e9a231932b9e72b2e401d7a5bc17a51e",
    "abstract": "Understanding vehicles in images is important for various applications such as intelligent transportation and self-driving system. Existing vehicle-centric works typically pre-train models on large-scale classification datasets and then fine-tune them for specific downstream tasks. However, they neglect the specific characteristics of vehicle perception in different tasks and might thus lead to sub-optimal performance. To address this issue, we propose a novel vehicle-centric pre-training framework called VehicleMAE, which incorporates the structural information including the spatial structure from vehicle profile information and the semantic structure from informative high-level natural language descriptions for effective masked vehicle appearance reconstruction. To be specific, we explicitly extract the sketch lines of vehicles as a form of the spatial structure to guide vehicle reconstruction. The more comprehensive knowledge distilled from the CLIP big model based on the similarity between the paired/unpaired vehicle image-text sample is further taken into consideration to help achieve a better understanding of vehicles. A large-scale dataset is built to pre-train our model, termed Autobot1M, which contains about 1M vehicle images and 12693 text information. Extensive experiments on four vehicle-based downstream tasks fully validated the effectiveness of our VehicleMAE. The source code and pre-trained models will be released at https://github.com/Event-AHU/VehicleMAE.",
    "authors": [
        "Xiao Wang",
        "Wentao Wu",
        "Chenglong Li",
        "Zhicheng Zhao",
        "Zhe Chen",
        "Yukai Shi",
        "Jin Tang"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel vehicle-centric pre-training framework called VehicleMAE, which incorporates the structural information including the spatial structure from vehicle profile information and the semantic structure from informative high-level natural language descriptions for effective masked vehicle appearance reconstruction."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}