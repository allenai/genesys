{
    "acronym": "e282d80788b934a440959c587b0327b0e3de05f4",
    "title": "Text2Grasp: Grasp synthesis by text prompts of object grasping parts",
    "seed_ids": [
        "gpt3",
        "15736f7c205d961c00378a938daffaacb5a0718d"
    ],
    "s2id": "e282d80788b934a440959c587b0327b0e3de05f4",
    "abstract": "The hand plays a pivotal role in human ability to grasp and manipulate objects and controllable grasp synthesis is the key for successfully performing downstream tasks. Existing methods that use human intention or task-level language as control signals for grasping inherently face ambiguity. To address this challenge, we propose a grasp synthesis method guided by text prompts of object grasping parts, Text2Grasp, which provides more precise control. Specifically, we present a two-stage method that includes a text-guided diffusion model TextGraspDiff to first generate a coarse grasp pose, then apply a hand-object contact optimization process to ensure both plausibility and diversity. Furthermore, by leveraging Large Language Model, our method facilitates grasp synthesis guided by task-level and personalized text descriptions without additional manual annotations. Extensive experiments demonstrate that our method achieves not only accurate part-level grasp control but also comparable performance in grasp quality.",
    "authors": [
        "Xiaoyun Chang",
        "Yi Sun"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a grasp synthesis method guided by text prompts of object grasping parts, Text2Grasp, which provides more precise control and achieves not only accurate part-level grasp control but also comparable performance in grasp quality."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}