{
    "acronym": "b6cfb544d37bc54cdb4250c9ccc5ff2a968961c6",
    "title": "Horizontal and Vertical Attention in Transformers",
    "seed_ids": [
        "linformer",
        "2d98048c2d2fcd3f6b989d2a54003808906ab4b7",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "a8427ce5aee6d62800c725588e89940ed4910e0d",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "b6cfb544d37bc54cdb4250c9ccc5ff2a968961c6",
    "abstract": "Transformers are built upon multi-head scaled dot-product attention and positional encoding, which aim to learn the feature representations and token dependencies. In this work, we focus on enhancing the distinctive representation by learning to augment the feature maps with the self-attention mechanism in Transformers. Specifically, we propose the horizontal attention to re-weight the multi-head output of the scaled dot-product attention before dimensionality reduction, and propose the vertical attention to adaptively re-calibrate channel-wise feature responses by explicitly modelling inter-dependencies among different channels. We demonstrate the Transformer models equipped with the two attentions have a high generalization capability across different supervised learning tasks, with a very minor additional computational cost overhead. The proposed horizontal and vertical attentions are highly modular, which can be inserted into various Transformer models to further improve the performance. Our code is available in the supplementary material.",
    "authors": [
        "Litao Yu",
        "Jing Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes the horizontal attention to re-weight the multi-head output of the scaled dot-product attention before dimensionality reduction, and the vertical attention to adaptively re-calibrate channel-wise feature responses by explicitly modelling inter-dependencies among different channels."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}