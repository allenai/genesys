{
    "acronym": "c9ab65bb8241a0a11e05ddb008d96948dcb15786",
    "title": "A Mixture of Experts Approach to 3D Human Motion Prediction",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "c9ab65bb8241a0a11e05ddb008d96948dcb15786",
    "abstract": "This project addresses the challenge of human motion prediction, a critical area for applications such as au- tonomous vehicle movement detection. Previous works have emphasized the need for low inference times to provide real time performance for applications like these. Our primary objective is to critically evaluate existing model ar- chitectures, identifying their advantages and opportunities for improvement by replicating the state-of-the-art (SOTA) Spatio-Temporal Transformer model as best as possible given computational con- straints. These models have surpassed the limitations of RNN-based models and have demonstrated the ability to generate plausible motion sequences over both short and long term horizons through the use of spatio-temporal rep- resentations. We also propose a novel architecture to ad- dress challenges of real time inference speed by incorpo- rating a Mixture of Experts (MoE) block within the Spatial- Temporal (ST) attention layer. The particular variation that is used is Soft MoE, a fully-differentiable sparse Transformer that has shown promising ability to enable larger model capacity at lower inference cost. We make out code publicly available at https://github.com/edshieh/motionprediction",
    "authors": [
        "Edmund Shieh",
        "Joshua Lee Franco",
        "Kang Min Bae",
        "Tej Lalvani"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This project addresses the challenge of human motion prediction by critically evaluating existing modelitectures, identifying their advantages and opportunities for improvement by replicating the state-of-the-art SOTA Spatio-Temporal Transformer model as best as possible given computational constraints."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}