{
    "acronym": "2d77b7203824e617206634277bce7eec2b71a2bd",
    "title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
    "seed_ids": [
        "bert",
        "299a77bf4050c9686d35b905b96d8902734845c5",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2d77b7203824e617206634277bce7eec2b71a2bd",
    "abstract": "Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.",
    "authors": [
        "Pengwei Zhan",
        "Zhen Xu",
        "Qian Tan",
        "Jie Song",
        "Ru Xie"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is revealed that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans, and a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE) is proposed."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}