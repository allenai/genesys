{
    "acronym": "e673d258703cc317b3714bc7da0d9d9b02ebc146",
    "title": "PASS: Pruning Attention Heads with Almost-sure Sparsity Targets",
    "seed_ids": [
        "linformer",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "d291149a75d7ac194382bd61e515eb40ed0aa106",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "83b8108014e3db4f46354a28ae68193f143c4e7e",
        "f6390beca54411b06f3bde424fb983a451789733",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "e673d258703cc317b3714bc7da0d9d9b02ebc146",
    "abstract": "Transformer models have been widely used to obtain high accuracy values in multiple fields including natural language processing (NLP), computer vision, and more. This superior performance typically comes at the expense of substantial computational overhead. Multi-head attention is the key factor in the success of Transformer models that has been found to be computationally expensive. Significant research effort has been devoted to improving attention compute efficiency by pruning redundant attention heads. A widely adopted paradigm is to jointly learn a set of gate variables and apply thresholds on gate values to prune heads. Previous work shows a high level of sensitivity to threshold tuning which can limit subnetwork performance and prevent them from wider adoption in practice. We propose the notion of almost-sure sparsity to overcome this limitation and develop a generic framework for P runing with A lmost-S ure S parsity (PASS) targets over attention heads. To further boost efficiency, we design a novel technique, concentrator , based on which we develop PASSCONC ( PASS with CONC entrator). We also present a simple-yet-effective strategy to further improve subnetwork performance by clipping and selectively reopening learned gates. We investigate PASS and PASSCONC on two widely studied architectures: encoder-decoder (ED) Transformer and encoder-only Transformer (e.g., BERT). Experiments on IWSLT14 German-to-English translation and GLUE benchmark tasks demonstrate that our approaches outperform the SOTA by achieving up to 1 . 33 higher BLEU scores, 1 . 44% higher accuracy, and 60% higher attention speedups.",
    "authors": [],
    "venue": "",
    "year": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The notion of almost-sure sparsity is proposed to overcome this limitation and develop a generic framework for P runing with A lmost-S ure S parsity (PASS) targets over attention heads and a simple-yet-effective strategy to further improve subnetwork performance by clipping and selectively reopening learned gates."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}