{
    "acronym": "99db217d802f8bd0a318f9a4d2c7496aec2a1acf",
    "title": "Attention Mechanism in Natural Language Processing",
    "seed_ids": [
        "reformer"
    ],
    "s2id": "99db217d802f8bd0a318f9a4d2c7496aec2a1acf",
    "abstract": "Natural language processing (NLP) technologies have always been actively developed to solve tasks in the most widespread languages like English or German. However, there are a few papers and solutions dedicated to smaller Slavic languages like Czech. This paper describes various NLP algorithms of sequence processing. It is primarily focused on modern techniques like Attention mechanism and such architectures like Transformer or Reformer, which are built on it. This thesis examines what advantages this mechanism has compared to conventional recurrent neural networks. It is afterwards tested on two Czech NLP tasks of various degrees of complexity diacritics correction and abstractive text summarization.",
    "authors": [
        "A. Kretov"
    ],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This thesis examines what advantages this mechanism has compared to conventional recurrent neural networks and is afterwards tested on two Czech NLP tasks of various degrees of complexity diacritics correction and abstractive text summarization."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}