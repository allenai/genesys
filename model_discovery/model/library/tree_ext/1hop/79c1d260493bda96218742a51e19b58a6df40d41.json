{
    "acronym": "79c1d260493bda96218742a51e19b58a6df40d41",
    "title": "Stacking Diverse Architectures to Improve Machine Translation",
    "seed_ids": [
        "gmlp",
        "lighdynconv",
        "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "16c844fd4d97f3c6eb38b0d6527c87d184efedc3"
    ],
    "s2id": "79c1d260493bda96218742a51e19b58a6df40d41",
    "abstract": "Repeated applications of the same neural block primarily based on self-attention characterize the current state-of-the-art in neural architectures for machine translation. In such architectures the decoder adopts a masked version of the same encoding block. Although simple this strategy doesn\u2019t encode the various inductive biases such as locality that arise from alternative architectures and that are central to the modelling of translation. We propose Lasagna, an encoder-decoder model that aims to combine the inductive bene\ufb01ts of di\ufb00erent architectures by layering multiple instances of di\ufb00erent blocks. Lasagna\u2019s encoder \ufb01rst grows the representation from local to mid-sized using convolutional blocks and only then applies a pair of \ufb01nal self-attention blocks. Lasagna\u2019s decoder uses only convolutional blocks that attend to the encoder representation. On a large suit of machine translation tasks, we \ufb01nd that Lasagna not only matches or outperforms the Transformer baseline, but it does so more e\ufb03ciently thanks to widespread use of the e\ufb03cient convolutional blocks. These \ufb01ndings suggest that the widespread use of uniform architectures may be suboptimal in certain scenarios and exploiting the diversity of inductive architectural biases can lead to substantial gains.",
    "authors": [
        "Andrea Schioppa",
        "Nal Kalchbrenner"
    ],
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Lasagna, an encoder-decoder model that aims to combine the inductive benevolence of di\ufb01ts of di\ufb01ts of di-erent architectures by layering multiple instances of di-erent blocks, is proposed, suggesting that the widespread use of uniform architectures may be suboptimal in certain scenarios and exploiting the diversity of inductive architectural biases can lead to substantial gains."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}