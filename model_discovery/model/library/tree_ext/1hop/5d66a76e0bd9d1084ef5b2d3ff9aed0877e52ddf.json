{
    "acronym": "5d66a76e0bd9d1084ef5b2d3ff9aed0877e52ddf",
    "title": "Transformer-XL on QAnet",
    "seed_ids": [
        "transformerxl"
    ],
    "s2id": "5d66a76e0bd9d1084ef5b2d3ff9aed0877e52ddf",
    "abstract": "When it was published in 2018, QAnet [1] revolutionized question answering models by replacing traditional LSTM-based models with transformers. Transformers don\u2019t suffer from long-term dependancy information loss like RNNs do, but since they process entire sentences at a time, they can only learn from context within their fixed input window. My aim is to fix this issue by reintroducing an element of recursion into the transformer-based model as described in Transformer-XL [2] and update QAnet to be competitive on SQuAD v2.0. To accomplish this, I 1) redesigned and re-implemented the QAnet structure to include word-character embedding convolutional layers to increase contextual information learned and 2) applied transformer-XL to each encoder block by storing previous hidden states in memory, reusing them in calculations of the next hidden state, and using relative positional embeddings. I found that the word-character convolution worked exceptionally well, and when applied to the baseline BiDAF model were able to achieve EM/F1/AvNA = 59.5/63/69 . The base QAnet was able to perform at similar levels to the baseline, at EM/F1/AvNA = 53.44/56.15/64.31 , but the transformer-XL improvements did not help and in fact made the model worse, clocking in at EM/F1/AvNA = 59.5/63/69 .",
    "authors": [
        "Stanford CS224N",
        "James Chao",
        "\u2022. Mentor",
        "Kaili Huang"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The QAnet structure was redesigned and re-implemented to include word-character embedding convolutional layers to increase contextual information learned and transformer-XL was applied to each encoder block by storing previous hidden states in memory, reusing them in calculations of the next hidden state, and using relative positional embeddings."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}