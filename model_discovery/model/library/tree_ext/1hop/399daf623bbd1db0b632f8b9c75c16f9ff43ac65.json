{
    "acronym": "399daf623bbd1db0b632f8b9c75c16f9ff43ac65",
    "title": "QANet-XL: Applying Transformer-XL to QANet",
    "seed_ids": [
        "transformerxl"
    ],
    "s2id": "399daf623bbd1db0b632f8b9c75c16f9ff43ac65",
    "abstract": "Question Answering has become a classic NLP task, especially over the past few years with the rise of transformers and pre-trained contextual embeddings. On the SQuAD 1.1 dataset, the QANet [10] architecture performed extremely well and produced the majority of the top scores by incorporating aspects of transformers [9]. However, PCE models have overwhelmingly dominated the SQuAD 2.0 leaderboard. My goal in this project is to try to apply new ideas about non-\ufb01xed-length context from Transformer-XL [3] to the QANet model in order to improve upon scores. Despite running into some problems in implementing models, I did end up seeing an improvement in performance",
    "authors": [
        "Ryan Cole"
    ],
    "venue": "",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The goal in this project is to try to apply new ideas about non-\ufb01xed-length context from Transformer-XL to the QANet model in order to improve upon scores."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}