{
    "acronym": "8d1f2e1beaf6905641740c6fee995f0b3f3e0938",
    "title": "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models",
    "seed_ids": [
        "gpt2",
        "71ba5f845bd22d42003675b7cea970ca9e590bcc",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "5e00596fa946670d894b1bdaeff5a98e3867ef13",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8d1f2e1beaf6905641740c6fee995f0b3f3e0938",
    "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model. Code is available at https://github.com/ExplainableML/ProbVLM",
    "authors": [
        "Uddeshya Upadhyay",
        "Shyamgopal Karthik",
        "Massimiliano Mancini",
        "Zeynep Akata"
    ],
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing is proposed."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}