{
    "acronym": "2330035c7586a0dc0b1f09e9c00106b295acf543",
    "title": "Long-Context Language Modeling with Parallel Context Encoding",
    "seed_ids": [
        "streamingllm",
        "pcw",
        "1d4c48335d841014d0145256c3c4e7f6c426b8fb",
        "189fde3f4dfa105bb51472a8945618f395919560",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "cbbc2cc774c50b0b19922185b80e9ce90b7cd2f6",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "dbc368bc8b49347dd27679894524fa62f88492c9",
        "980e55d9226cac302d0fae7732da4e67b8bc952c",
        "732e3faec4e5be4d144256f2c379b9dc49f0b227",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "2330035c7586a0dc0b1f09e9c00106b295acf543",
    "abstract": "Extending large language models (LLMs) to process longer inputs is crucial for a wide range of applications. However, the substantial computational cost of transformers and limited generalization of positional encoding restrict the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE employs a small encoder to process long inputs chunk by chunk, enabling the frozen decoder to utilize additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, it extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CEPE variant that can extend the context window of instruction-tuned models using only unlabeled data, and showcase its effectiveness on LLAMA-2-CHAT, leading to a strong instruction-following model that can leverage very long contexts on downstream tasks.",
    "authors": [
        "Howard Yen",
        "Tianyu Gao",
        "Danqi Chen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window, and introduces a CEPE variant that can extend the context window of instruction-tuned models using only unlabeled data."
    },
    "citationCount": 10,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}