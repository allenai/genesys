{
    "acronym": "f8c6cb00ab9775f90ded5025b49cc260cede9350",
    "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
    "seed_ids": [
        "gpt2",
        "d972c9bccc5ccef16d71b450dd3ab4c77f67224a",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "f8c6cb00ab9775f90ded5025b49cc260cede9350",
    "abstract": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ``unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt and https://github.com/YuchuanTian/AIGC_text_detector.",
    "authors": [
        "Yuchuan Tian",
        "Hanting Chen",
        "Xutao Wang",
        "Zheyuan Bai",
        "Qinghua Zhang",
        "Ruifeng Li",
        "Chaoxi Xu",
        "Yunhe Wang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts, and experiments show that the MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors."
    },
    "citationCount": 22,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}