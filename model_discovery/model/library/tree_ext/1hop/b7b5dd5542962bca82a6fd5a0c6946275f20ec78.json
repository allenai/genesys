{
    "acronym": "b7b5dd5542962bca82a6fd5a0c6946275f20ec78",
    "title": "PoolImagen: Text-to-Image Diffusion Models With an Efficient Transformer Without Attention",
    "seed_ids": [
        "metaformer",
        "dd1139cfc609c2f3263d02e97176d5275caebc0a",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b7b5dd5542962bca82a6fd5a0c6946275f20ec78",
    "abstract": "Recent advancements in the field of image generation models have been particularly notable for diffusion models. Imagen has the most remarkable image generation capabilities among these models, particularly at high resolutions. However, Imagen comes with limitations, as creating high-quality results requires considerable computational resources and lengthy training times. To address these limitations, we propose PoolImagen, a novel and improved variant of Imagen that combines high performance with low computational costs. PoolImagen introduces various improvements to overcome the constraints of Imagen. Notably, we adopted the idea, first propose in MetaFormer, which suggests replacing the attention module with a pooling structure in the transformer architecture of Imagen to mitigate the issues related to increased training costs and computational complexity. Additionally, considering the influence of text encoder size on text-to-image transformation quality, we incorporate the large language models (e.g. flan-t5-xxl), an extension of the t5 model that offers more parameters and refined text processing capabilities. With a well-trained transformer, PoolImagen achieves image generation with consistent performance and significantly accelerated training velocities. In experiments based on bird image datasets, PoolImagen demonstrates improved performance in terms of Fr\u00e9chet Inception Distance (FID) and training time. In the case of the bird datasets, PoolImagen exhibits an approximately 11.29% improvement in FID compared to Imagen, while training time is reduced by 2.25 times. In addition, we conducted additional experiments to evaluate ability of PoolImagen to represent domain-specific features in generated images. These findings emphasize the potential of PoolImagen as a powerful tool for rapidly generating text-to-image outputs and suggest promising directions for enhancing the future performance of diffusion models.",
    "authors": [
        "Hyeeun Ku",
        "Minhyeok Lee",
        "Kanghyeok Ko",
        "Sun Jae Baek"
    ],
    "venue": "Digital Signal Processing and Signal Processing Education Workshop",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "PoolImagen is proposed, a novel and improved variant of Imagen that combines high performance with low computational costs and incorporates the large language models of t5, an extension of the t5 model that offers more parameters and refined text processing capabilities."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}