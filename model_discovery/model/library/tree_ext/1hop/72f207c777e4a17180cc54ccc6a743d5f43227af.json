{
    "acronym": "72f207c777e4a17180cc54ccc6a743d5f43227af",
    "title": "Choose a Transformer: Fourier or Galerkin",
    "seed_ids": [
        "deltanet",
        "performer",
        "linformer",
        "lineartransformer",
        "ed535e93d5b5a8b689e861e9c6083a806d1535c2",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "2ff74d426e712522030057624510c03713fa77ba"
    ],
    "s2id": "72f207c777e4a17180cc54ccc6a743d5f43227af",
    "abstract": "In this paper, we apply the self-attention from the state-of-the-art Transformer in Attention Is All You Need for the first time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efficacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the first time that the softmax normalization in the scaled dot-product attention is sufficient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projection is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data. Finally, we present three operator learning experiments, including the viscid Burgers' equation, an interface Darcy flow, and an inverse interface coefficient identification problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows significant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts.",
    "authors": [
        "Shuhao Cao"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new layer normalization scheme mimicking the Petrov-Galerkin projection is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data."
    },
    "citationCount": 137,
    "influentialCitationCount": 27,
    "code": null,
    "description": null,
    "url": null
}