{
    "acronym": "980d1c3bf9d1a3c0ce68567e0efc1a72f203f12c",
    "title": "Global memory transformer for processing long documents",
    "seed_ids": [
        "longt5",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "73d64ecbe3e846394444dab6c5e89ba33e5daa49",
        "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed",
        "63857190aaf5aab1d94b54bb257b7b03b8cb5a50",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "980d1c3bf9d1a3c0ce68567e0efc1a72f203f12c",
    "abstract": null,
    "authors": [
        "Arij Al Adel"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study aims to verify the ability of the proposed model to handle chunks as if they were one chunk comparing with the base model to overcome the baseline on Masked language modeling task with specific training parameters."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}