{
    "acronym": "efaa0074c2e367768e74a5e8b059ac3a8ab405b7",
    "title": "ETTrack: Enhanced Temporal Motion Predictor for Multi-Object Tracking",
    "seed_ids": [
        "transformer"
    ],
    "s2id": "efaa0074c2e367768e74a5e8b059ac3a8ab405b7",
    "abstract": "Many Multi-Object Tracking (MOT) approaches exploit motion information to associate all the detected objects across frames. However, many methods that rely on filtering-based algorithms, such as the Kalman Filter, often work well in linear motion scenarios but struggle to accurately predict the locations of objects undergoing complex and non-linear movements. To tackle these scenarios, we propose a motion-based MOT approach with an enhanced temporal motion predictor, ETTrack. Specifically, the motion predictor integrates a transformer model and a Temporal Convolutional Network (TCN) to capture short-term and long-term motion patterns, and it predicts the future motion of individual objects based on the historical motion information. Additionally, we propose a novel Momentum Correction Loss function that provides additional information regarding the motion direction of objects during training. This allows the motion predictor rapidly adapt to motion variations and more accurately predict future motion. Our experimental results demonstrate that ETTrack achieves a competitive performance compared with state-of-the-art trackers on DanceTrack and SportsMOT, scoring 56.4% and 74.4% in HOTA metrics, respectively.",
    "authors": [
        "Xudong Han",
        "Nobuyuki Oishi",
        "Yueying Tian",
        "Elif Ucurum",
        "R. Young",
        "C. Chatwin",
        "Philip Birch"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a motion-based MOT approach with an enhanced temporal motion predictor, ETTrack, that integrates a transformer model and a Temporal Convolutional Network to capture short-term and long-term motion patterns, and it predicts the future motion of individual objects based on the historical motion information."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}