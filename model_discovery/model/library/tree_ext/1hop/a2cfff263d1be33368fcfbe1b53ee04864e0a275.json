{
    "acronym": "a2cfff263d1be33368fcfbe1b53ee04864e0a275",
    "title": "DiffAnt: Diffusion Models for Action Anticipation",
    "seed_ids": [
        "diffusionlm",
        "diffuseq",
        "81621c53c6aa421deb81bb1359138ded0fb4e258",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "17d068e78e6f25e65cb08319b19b58279bb8b214",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "a2cfff263d1be33368fcfbe1b53ee04864e0a275",
    "abstract": "Anticipating future actions is inherently uncertain. Given an observed video segment containing ongoing actions, multiple subsequent actions can plausibly follow. This uncertainty becomes even larger when predicting far into the future. However, the majority of existing action anticipation models adhere to a deterministic approach, neglecting to account for future uncertainties. In this work, we rethink action anticipation from a generative view, employing diffusion models to capture different possible future actions. In this framework, future actions are iteratively generated from standard Gaussian noise in the latent space, conditioned on the observed video, and subsequently transitioned into the action space. Extensive experiments on four benchmark datasets, i.e., Breakfast, 50Salads, EpicKitchens, and EGTEA Gaze+, are performed and the proposed method achieves superior or comparable results to state-of-the-art methods, showing the effectiveness of a generative approach for action anticipation. Our code and trained models will be published on GitHub.",
    "authors": [
        "Zeyun Zhong",
        "Chengzhi Wu",
        "Manuel Martin",
        "Michael Voit",
        "Juergen Gall",
        "J. Beyerer"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work rethink action anticipation from a generative view, employing diffusion models to capture different possible future actions, iteratively generated from standard Gaussian noise in the latent space, conditioned on the observed video, and subsequently transitioned into the action space."
    },
    "citationCount": 2,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}