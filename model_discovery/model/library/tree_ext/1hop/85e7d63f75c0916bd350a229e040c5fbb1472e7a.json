{
    "acronym": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
    "title": "Making Pre-trained Language Models Better Few-shot Learners",
    "seed_ids": [
        "gpt",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "b47381e04739ea3f392ba6c8faaf64105493c196",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
    "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF\u2014better few-shot fine-tuning of language models\u2014a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
    "authors": [
        "Tianyu Gao",
        "Adam Fisch",
        "Danqi Chen"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The LM-BFF approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning."
    },
    "citationCount": 1576,
    "influentialCitationCount": 213,
    "code": null,
    "description": null,
    "url": null
}