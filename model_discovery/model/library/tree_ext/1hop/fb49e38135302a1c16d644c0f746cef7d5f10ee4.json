{
    "acronym": "fb49e38135302a1c16d644c0f746cef7d5f10ee4",
    "title": "Understanding BLOOM: An empirical study on diverse NLP tasks",
    "seed_ids": [
        "gpt2",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "a2d534fda2eafabf5ac19934ce500cd975e33030",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fb49e38135302a1c16d644c0f746cef7d5f10ee4",
    "abstract": "We view the landscape of large language models (LLMs) through the lens of the recently released BLOOM model to understand the performance of BLOOM and other decoder-only LLMs compared to BERT-style encoder-only models. We achieve this by evaluating the smaller BLOOM model variants (\\textit{350m/560m} and \\textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards. We make the following observations: (1) BLOOM performance does not scale with parameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning BLOOM models show that the 560m variant performs similarly to or better than the 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning experiments show that BLOOM is at par or worse than monolingual GPT-2 models, and (3) Toxicity analysis of prompt-based text generation using the RealToxicityPrompts dataset shows that the text generated by BLOOM is at least 17\\% less toxic than GPT-2 and GPT-3 models.",
    "authors": [
        "Parag Dakle",
        "Sai Krishna Rallabandi",
        "Preethi Raghavan"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Evaluating the smaller BLOOM model variants on several NLP benchmark datasets and popular leaderboards shows that the 560m variant performs similarly to or better than the 1b7 variant, and toxicity analysis of prompt-based text generation using the RealToxicityPrompts dataset shows that it is at least 17\\% less toxic than GPT-2 and G PT-3 models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}