{
    "acronym": "68a65925ac6d270ca27a4fe99e58cf2ed0795821",
    "title": "Time-FFM: Towards LM-Empowered Federated Foundation Model for Time Series Forecasting",
    "seed_ids": [
        "gpt2",
        "b8e57155bbcc1ce8a112482c85b3a3bb25f3fe52",
        "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "68a65925ac6d270ca27a4fe99e58cf2ed0795821",
    "abstract": "Unlike natural language processing and computer vision, the development of Foundation Models (FMs) for time series forecasting is blocked due to data scarcity. While recent efforts are focused on building such FMs by unlocking the potential of language models (LMs) for time series analysis, dedicated parameters for various downstream forecasting tasks need training, which hinders the common knowledge sharing across domains. Moreover, data owners may hesitate to share the access to local data due to privacy concerns and copyright protection, which makes it impossible to simply construct a FM on cross-domain training instances. To address these issues, we propose Time-FFM, a Federated Foundation Model for Time series forecasting by leveraging pretrained LMs. Specifically, we begin by transforming time series into the modality of text tokens. To bootstrap LMs for time series reasoning, we propose a prompt adaption module to determine domain-customized prompts dynamically instead of artificially. Given the data heterogeneity across domains, we design a personalized federated training strategy by learning global encoders and local prediction heads. Our comprehensive experiments indicate that Time-FFM outperforms state-of-the-arts and promises effective few-shot and zero-shot forecaster.",
    "authors": [
        "Qingxiang Liu",
        "Xu Liu",
        "Chenghao Liu",
        "Qingsong Wen",
        "Yuxuan Liang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Time-FFM, a Federated Foundation Model for Time series forecasting by leveraging pretrained LMs is proposed, which begins by transforming time series into the modality of text tokens and proposes a prompt adaption module to bootstrap LMs for time series reasoning."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}