{
    "acronym": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
    "title": "Rethinking Attention with Performers",
    "seed_ids": [
        "linformer",
        "lineartransformer",
        "longformer",
        "sparsetransformer",
        "reformer",
        "routingtransformer",
        "compressivetransformer",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "765866ecb5fe6a225d4e791498caf6a8351c16c7",
        "f51497f463566581874c941353dd9d80069c5b77",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "bed7c155b843fda8a1c994ce71e9176e43b20f77",
        "fb507ada871d1e8c29e376dbf7b7879689aa89f9"
    ],
    "s2id": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
    "abstract": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.",
    "authors": [
        "K. Choromanski",
        "Valerii Likhosherstov",
        "David Dohan",
        "Xingyou Song",
        "Andreea Gane",
        "Tam\u00e1s Sarl\u00f3s",
        "Peter Hawkins",
        "Jared Davis",
        "Afroz Mohiuddin",
        "Lukasz Kaiser",
        "David Belanger",
        "Lucy J. Colwell",
        "Adrian Weller"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity, without relying on any priors such as sparsity or low-rankness are introduced."
    },
    "citationCount": 1217,
    "influentialCitationCount": 176,
    "code": null,
    "description": null,
    "url": null
}