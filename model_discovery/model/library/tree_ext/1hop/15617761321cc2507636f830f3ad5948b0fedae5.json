{
    "acronym": "15617761321cc2507636f830f3ad5948b0fedae5",
    "title": "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning",
    "seed_ids": [
        "gpt",
        "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "690df0820f35a47e1ce44f90e6ddb4132aa09267"
    ],
    "s2id": "15617761321cc2507636f830f3ad5948b0fedae5",
    "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have significantly improve performance in image comprehension tasks, such as formatted charts and rich-content images. Yet, Graphical User Interface (GUI) pose a greater challenge due to their structured format and detailed textual information. Existing LVLMs often overly depend on internal knowledge and neglect image content, resulting in hallucinations and incorrect responses in GUI comprehension. To address these issues, we introduce VGA, a fine-tuned model designed for comprehensive GUI understanding. Our model aims to enhance the interpretation of visual data of GUI and reduce hallucinations. We first construct a Vision Question Answering (VQA) dataset of 63.8k high-quality examples with our propose Referent Method, which ensures the model's responses are highly depend on visual content within the image. We then design a two-stage fine-tuning method called Foundation and Advanced Comprehension (FAC) to enhance both the model's ability to extract information from image content and alignment with human intent. Experiments show that our approach enhances the model's ability to extract information from images and achieves state-of-the-art results in GUI understanding tasks. Our dataset and fine-tuning script will be released soon.",
    "authors": [
        "Ziyang Meng",
        "Yu Dai",
        "Zezheng Gong",
        "Shaoxiong Guo",
        "Minglong Tang",
        "Tongquan Wei"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "VGA is introduced, a fine-tuned model designed for comprehensive GUI understanding that enhances the model's ability to extract information from images and achieves state-of-the-art results in GUI understanding tasks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}