{
    "acronym": "54983a66dc8bc15af89890bc9e1a63e056594179",
    "title": "Composite Slice Transformer: An Efficient Transformer with Composition of Multi-Scale Multi-Range Attentions",
    "seed_ids": [
        "luna",
        "nystromformer",
        "scatterbrain",
        "performer",
        "linformer",
        "lineartransformer",
        "reformer",
        "sinkhorn",
        "htransformer1d",
        "lstransformer",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
        "5d032bd2632b6f5847767f39ce247098c6bbc563",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "f51497f463566581874c941353dd9d80069c5b77",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "54983a66dc8bc15af89890bc9e1a63e056594179",
    "abstract": "the family of efficient attention approaches in which the lengths of the attention operands are reduced to M ( < N ) by applying an abstraction function, resulting in reduced complexity of the attention while retaining the form of basic attention computation in Eq. 3. Abstrac-tive attentions can be further categorized to either resolution preserving or non-preserving , according to which operands are chosen to be abstracted. Resolution non-preserving attention is the",
    "authors": [
        "Mingu Lee",
        "Saurabh Pitre",
        "Tianyu Jiang",
        "Pierre-David L\u00e9tourneau",
        "Matthew J. Morse",
        "Kanghwan Jang",
        "Joseph B. Soriaga",
        "Parham Noorzad",
        "Hsin-Pai Cheng",
        "Chris Lott"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}