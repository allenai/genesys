{
    "acronym": "7691c40975cf25fae4b7a55278a65d2dcb361646",
    "title": "Transformers discover an elementary calculation system exploiting local attention and grid-like problem representation",
    "seed_ids": [
        "alibi",
        "universaltrans",
        "2c0a266f9cb88bb914c138ece0deaab8cf528f78"
    ],
    "s2id": "7691c40975cf25fae4b7a55278a65d2dcb361646",
    "abstract": "Mathematical reasoning is one of the most impressive achievements of human intellect but remains a formidable challenge for artificial intelligence systems. In this work we explore whether modern deep learning architectures can learn to solve a symbolic addition task by discovering effective arithmetic procedures. Although the problem might seem trivial at first glance, generalizing arithmetic knowledge to operations involving a higher number of terms, possibly composed by longer sequences of digits, has proven extremely challenging for neural networks. Here we show that universal transformers equipped with local attention and adaptive halting mechanisms can learn to exploit an external, grid-like memory to carry out multi-digit addition. The proposed model achieves remarkable accuracy even when tested with problems requiring extrapolation outside the training distribution; most notably, it does so by discovering human-like calculation strategies such as place value alignment.",
    "authors": [
        "Samuel Cognolato",
        "Alberto Testolin"
    ],
    "venue": "IEEE International Joint Conference on Neural Network",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that universal transformers equipped with local attention and adaptive halting mechanisms can learn to exploit an external, grid-like memory to carry out multi-digit addition."
    },
    "citationCount": 6,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}