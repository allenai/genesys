{
    "acronym": "b99c61f6957c1b04ec1376b74f82dd1e83559695",
    "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
    "seed_ids": [
        "gpt",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b99c61f6957c1b04ec1376b74f82dd1e83559695",
    "abstract": "Existing pre-trained models for knowledge-graph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new state-of-the-art performance on various KG-to-text datasets.",
    "authors": [
        "Pei Ke",
        "Haozhe Ji",
        "Yuanyuan Ran",
        "Xin Cui",
        "Liwei Wang",
        "Linfeng Song",
        "Xiaoyan Zhu",
        "Minlie Huang"
    ],
    "venue": "Findings",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A graph-text joint representation learning model called JointGT is proposed which devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure during encoding and shows new state-of-the-art performance on various KG-to-text datasets."
    },
    "citationCount": 79,
    "influentialCitationCount": 22,
    "code": null,
    "description": null,
    "url": null
}