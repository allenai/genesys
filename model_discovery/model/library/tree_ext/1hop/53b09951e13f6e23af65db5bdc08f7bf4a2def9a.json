{
    "acronym": "53b09951e13f6e23af65db5bdc08f7bf4a2def9a",
    "title": "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection",
    "seed_ids": [
        "gpt2",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "53b09951e13f6e23af65db5bdc08f7bf4a2def9a",
    "abstract": "Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: https://cfeng16.github.io/audio-visual-forensics.",
    "authors": [
        "Chao Feng",
        "Ziyang Chen",
        "Andrew Owens"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An autoregressive model is trained to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound, and obtains strong performance on the task of detecting manipulated speech videos."
    },
    "citationCount": 26,
    "influentialCitationCount": 8,
    "code": null,
    "description": null,
    "url": null
}