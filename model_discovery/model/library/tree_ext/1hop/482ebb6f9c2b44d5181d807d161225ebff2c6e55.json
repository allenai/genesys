{
    "acronym": "482ebb6f9c2b44d5181d807d161225ebff2c6e55",
    "title": "Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models",
    "seed_ids": [
        "classfreediffu",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "82482585e94192b4e9913727e461f89cd08e9725",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "482ebb6f9c2b44d5181d807d161225ebff2c6e55",
    "abstract": "Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\\mathbf{\\ge 2\\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\\times$64, 1.93 on AFHQv2-Wild-64$\\times$64, and 2.72 on ImageNet-256$\\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.",
    "authors": [
        "Zhendong Wang",
        "Yifan Jiang",
        "Huangjie Zheng",
        "Peihao Wang",
        "Pengcheng He",
        "Zhangyang Wang",
        "Weizhu Chen",
        "Mingyuan Zhou"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Patch Diffusion is proposed, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users."
    },
    "citationCount": 45,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}