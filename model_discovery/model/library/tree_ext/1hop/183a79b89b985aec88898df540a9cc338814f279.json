{
    "acronym": "183a79b89b985aec88898df540a9cc338814f279",
    "title": "Communication-Efficient Model Parallelism for Distributed In-Situ Transformer Inference",
    "seed_ids": [
        "bert",
        "16e623059ffccab60f4c35be028a2d4f10933515",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "183a79b89b985aec88898df540a9cc338814f279",
    "abstract": "Transformer models have shown significant success in a wide range of tasks. Meanwhile, massive resources required by its inference prevent scenarios with resource-constrained devices from in-situ deployment, leaving a high threshold of integrating its advances. Observing that these scenarios, e.g. smart home of edge computing, are usually comprise a rich set of trusted devices with untapped resources, it is promising to distribute Transformer inference onto multiple devices. However, due to the tightly-coupled feature of Transformer model, existing model parallelism approaches necessitate frequent communication to resolve data dependencies, making them unacceptable for distributed inference, especially under weak interconnect of edge scenarios. In this paper, we propose DeTransformer, a communication-efficient distributed in-situ Transformer inference system for edge scenarios. DeTransformer is based on a novel block parallelism approach, with the key idea of restructuring the original Trans-former layer with a single block to the decoupled layer with multi-ple sub-blocks and exploit model parallelism between sub-blocks. Next, DeTransformer contains an adaptive placement approach to automatically select the optimal placement strategy by striking a trade-off among communication capability, computing power and memory budget. Experimental results show that DeTransformer can reduce distributed inference latency by up to 2.81 x compared to the SOTA approach on 4 devices, while effectively maintaining task accuracy and a consistent model size.",
    "authors": [
        "Yuanxin Wei",
        "Shengyuan Ye",
        "Jiazhi Jiang",
        "Xu Chen",
        "Dan Huang",
        "Jiangsu Du",
        "Yutong Lu"
    ],
    "venue": "Design, Automation and Test in Europe",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "DeTransformer is a communication-efficient distributed in-situ Transformer inference system for edge scenarios based on a novel block parallelism approach, with the key idea of restructuring the original Trans-former layer with a single block to the decoupled layer with multi-ple sub-blocks and exploit model parallelism between sub-blocks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}