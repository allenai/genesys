{
    "acronym": "cf188f980d987d70358e414f44505e8427496d08",
    "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs",
    "seed_ids": [
        "lara",
        "nystromformer",
        "performer",
        "linformer",
        "longformer",
        "reformer",
        "htransformer1d",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
        "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
        "c1a4278f969acfc6682a924e31b95e1ade9703ee",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "cf188f980d987d70358e414f44505e8427496d08",
    "abstract": "One limitation of existing Transformer-based models is that they cannot handle very long sequences as input since their self-attention operations exhibit quadratic time and space complexity. This problem becomes especially acute when Transformers are deployed on hardware platforms equipped only with CPUs. To address this issue, we propose a novel method for accelerating self-attention at inference time that works with pretrained Transformer models out-of-the-box without requiring retraining. We experiment using our method to accelerate various long-sequence Transformers, including a leading LLaMA 2-based LLM, on various benchmarks and demonstrate a greater speedup of 2.73x - 7.63x while retaining 98.6% - 99.6% of the accuracy of the original pretrained models. The code is available on our project website at https://yuzhenmao.github.io/IceFormer/.",
    "authors": [
        "Yuzhen Mao",
        "Martin Ester",
        "Ke Li"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a novel method for accelerating self-attention at inference time that works with pretrained Transformer models out-of-the-box without requiring retraining and experiments using this method to accelerate various long-sequence Transformers."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}