{
    "acronym": "af9adfeb6e7b1d7250ec186a9fc88dbb18eee34c",
    "title": "On the Provable Advantage of Unsupervised Pretraining",
    "seed_ids": [
        "gpt",
        "145b8b5d99a2beba6029418ca043585b90138d12"
    ],
    "s2id": "af9adfeb6e7b1d7250ec186a9fc88dbb18eee34c",
    "abstract": "Unsupervised pretraining, which learns a useful representation using a large amount of unlabeled data to facilitate the learning of downstream tasks, is a critical component of modern large-scale machine learning systems. Despite its tremendous empirical success, the rigorous theoretical understanding of why unsupervised pretraining generally helps remains rather limited -- most existing results are restricted to particular methods or approaches for unsupervised pretraining with specialized structural assumptions. This paper studies a generic framework, where the unsupervised representation learning task is specified by an abstract class of latent variable models $\\Phi$ and the downstream task is specified by a class of prediction functions $\\Psi$. We consider a natural approach of using Maximum Likelihood Estimation (MLE) for unsupervised pretraining and Empirical Risk Minimization (ERM) for learning downstream tasks. We prove that, under a mild ''informative'' condition, our algorithm achieves an excess risk of $\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{C}_\\Phi/m} + \\sqrt{\\mathcal{C}_\\Psi/n})$ for downstream tasks, where $\\mathcal{C}_\\Phi, \\mathcal{C}_\\Psi$ are complexity measures of function classes $\\Phi, \\Psi$, and $m, n$ are the number of unlabeled and labeled data respectively. Comparing to the baseline of $\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{C}_{\\Phi \\circ \\Psi}/n})$ achieved by performing supervised learning using only the labeled data, our result rigorously shows the benefit of unsupervised pretraining when $m \\gg n$ and $\\mathcal{C}_{\\Phi\\circ \\Psi}>\\mathcal{C}_\\Psi$. This paper further shows that our generic framework covers a wide range of approaches for unsupervised pretraining, including factor models, Gaussian mixture models, and contrastive learning.",
    "authors": [
        "Jiawei Ge",
        "Shange Tang",
        "Jianqing Fan",
        "Chi Jin"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is proved that, under a mild ''informative'' condition, the result rigorously shows the benefit of unsupervised pretraining when $m \\gg n$ and $\\mathcal{C}_{\\Phi\\circ \\Psi}>\\mathcal {C}_\\Psi$."
    },
    "citationCount": 10,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}