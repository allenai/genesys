{
    "acronym": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
    "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
    "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
    "authors": [
        "Ian Tenney",
        "Patrick Xia",
        "Berlin Chen",
        "Alex Wang",
        "Adam Poliak",
        "R. Thomas McCoy",
        "Najoung Kim",
        "Benjamin Van Durme",
        "Samuel R. Bowman",
        "Dipanjan Das",
        "Ellie Pavlick"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel edge probing task design is introduced and a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline are constructed to investigate how sentence structure is encoded across a range of syntactic, semantic, local, and long-range phenomena."
    },
    "citationCount": 777,
    "influentialCitationCount": 76,
    "code": null,
    "description": null,
    "url": null
}