{
    "acronym": "2fc229bfe561f42aae6f3bb84598cfa5737a8b6a",
    "title": "Approximating Two-Layer Feedforward Networks for Efficient Transformers",
    "seed_ids": [
        "transformerxl",
        "productkeymem",
        "52045d4d4ae305aebb9e92fbbcf23104242c4d31",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "e528466e2aff981511d4ca6e063211297c0b4175",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2fc229bfe561f42aae6f3bb84598cfa5737a8b6a",
    "abstract": "How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.",
    "authors": [
        "R'obert Csord'as",
        "Kazuki Irie",
        "J. Schmidhuber"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs, including product-key memories (PKMs), and proposes methods to improve both MoEs and PKMs."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}