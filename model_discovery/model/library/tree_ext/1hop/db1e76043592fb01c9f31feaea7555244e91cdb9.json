{
    "acronym": "db1e76043592fb01c9f31feaea7555244e91cdb9",
    "title": "Interpreting Pretrained Language Models via Concept Bottlenecks",
    "seed_ids": [
        "bert",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "1ec16018e9152c50bbaed1d49c3077b9fb6d2838",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "db1e76043592fb01c9f31feaea7555244e91cdb9",
    "abstract": "Pretrained language models (PLMs) have made significant strides in various natural language processing tasks. However, the lack of interpretability due to their ``black-box'' nature poses challenges for responsible implementation. Although previous studies have attempted to improve interpretability by using, e.g., attention weights in self-attention layers, these weights often lack clarity, readability, and intuitiveness. In this research, we propose a novel approach to interpreting PLMs by employing high-level, meaningful concepts that are easily understandable for humans. For example, we learn the concept of ``Food'' and investigate how it influences the prediction of a model's sentiment towards a restaurant review. We introduce C$^3$M, which combines human-annotated and machine-generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task-specific concepts. Through empirical evaluations on real-world datasets, we manifest that our approach offers valuable insights to interpret PLM behavior, helps diagnose model failures, and enhances model robustness amidst noisy concept labels.",
    "authors": [
        "Zhen Tan",
        "Lu Cheng",
        "Song Wang",
        "Yuan Bo",
        "Jundong Li",
        "Huan Liu"
    ],
    "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel approach to interpreting PLMs by employing high-level, meaningful concepts that are easily understandable for humans is proposed, which combines human-annotated and machine-generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task-specific concepts."
    },
    "citationCount": 7,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}