{
    "acronym": "cfc24ad313d67aedf5a3c90b057ef43e155d4466",
    "title": "Nested Diffusion Processes for Anytime Image Generation",
    "seed_ids": [
        "classfreediffu",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "82482585e94192b4e9913727e461f89cd08e9725",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "cfc24ad313d67aedf5a3c90b057ef43e155d4466",
    "abstract": "Diffusion models are the current state-of-the-art in image generation, synthesizing high-quality images by breaking down the generation process into many fine-grained denoising steps. Despite their good performance, diffusion models are computationally expensive, requiring many neural function evaluations (NFEs). In this work, we propose an anytime diffusion-based method that can generate viable images when stopped at arbitrary times before completion. Using existing pretrained diffusion models, we show that the generation scheme can be recomposed as two nested diffusion processes, enabling fast iterative refinement of a generated image. In experiments on ImageNet and Stable Diffusion-based text-to-image generation, we show, both qualitatively and quantitatively, that our method\u2019s intermediate generation quality greatly exceeds that of the original diffusion model, while the final generation result remains comparable. We illustrate the applicability of Nested Diffusion in several settings, including for solving inverse problems, and for rapid text-based content creation by allowing user intervention throughout the sampling process. 1",
    "authors": [
        "Noam Elata",
        "Bahjat Kawar",
        "T. Michaeli",
        "Michael Elad"
    ],
    "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes an anytime diffusion-based method that can generate viable images when stopped at arbitrary times before completion, using existing pretrained diffusion models, and shows that the generation scheme can be recomposed as two nested diffusion processes, enabling fast iterative refinement of a generated image."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}