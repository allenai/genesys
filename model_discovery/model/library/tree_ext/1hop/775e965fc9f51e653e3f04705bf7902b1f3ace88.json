{
    "acronym": "775e965fc9f51e653e3f04705bf7902b1f3ace88",
    "title": "Hyperbolic Cosine Transformer for LiDAR 3D Object Detection",
    "seed_ids": [
        "cosformer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21"
    ],
    "s2id": "775e965fc9f51e653e3f04705bf7902b1f3ace88",
    "abstract": "Recently, Transformer has achieved great success in computer vision. However, it is constrained because the spatial and temporal complexity grows quadratically with the number of large points in 3D object detection applications. Previous point-wise methods are suffering from time consumption and limited receptive fields to capture information among points. In this paper, we propose a two-stage hyperbolic cosine transformer (ChTR3D) for 3D object detection from LiDAR point clouds. The proposed ChTR3D refines proposals by applying cosh-attention in linear computation complexity to encode rich contextual relationships among points. The cosh-attention module reduces the space and time complexity of the attention operation. The traditional softmax operation is replaced by non-negative ReLU activation and hyperbolic-cosine-based operator with re-weighting mechanism. Extensive experiments on the widely used KITTI dataset demonstrate that, compared with vanilla attention, the cosh-attention significantly improves the inference speed with competitive performance. Experiment results show that, among two-stage state-of-the-art methods using point-level features, the proposed ChTR3D is the fastest one.",
    "authors": [
        "Jigang Tong",
        "Fan Yang",
        "Sen Yang",
        "Enzeng Dong",
        "Shengzhi Du",
        "Xing-jun Wang",
        "Xianlin Yi"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Extensive experiments on the widely used KITTI dataset demonstrate that, compared with vanilla attention, the cosh-attention significantly improves the inference speed with competitive performance, and experiments show that the proposed ChTR3D is the fastest one."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}