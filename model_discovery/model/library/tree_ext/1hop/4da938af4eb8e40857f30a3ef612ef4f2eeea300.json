{
    "acronym": "4da938af4eb8e40857f30a3ef612ef4f2eeea300",
    "title": "ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models",
    "seed_ids": [
        "gpt2",
        "bert",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "5a2263092f49540fd0e049050a96882ff29b00c3",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4da938af4eb8e40857f30a3ef612ef4f2eeea300",
    "abstract": "With the ever-increasing popularity of pretrained Video-Language Models (VidLMs), there is a pressing need to develop robust evaluation methodologies that delve deeper into their visio-linguistic capabilities. To address this challenge, we present ViLMA (Video Language Model Assessment), a task-agnostic benchmark that places the assessment of fine-grained capabilities of these models on a firm footing. Task-based evaluations, while valuable, fail to capture the complexities and specific temporal aspects of moving images that VidLMs need to process. Through carefully curated counterfactuals, ViLMA offers a controlled evaluation suite that sheds light on the true potential of these models, as well as their performance gaps compared to human-level understanding. ViLMA also includes proficiency tests, which assess basic capabilities deemed essential to solving the main counterfactual tests. We show that current VidLMs' grounding abilities are no better than those of vision-language models which use static images. This is especially striking once the performance on proficiency tests is factored in. Our benchmark serves as a catalyst for future research on VidLMs, helping to highlight areas that still need to be explored.",
    "authors": [
        "Ilker Kesen",
        "Andrea Pedrotti",
        "Mustafa Dogan",
        "Michele Cafagna",
        "Emre Can Acikgoz",
        "Letitia Parcalabescu",
        "Iacer Calixto",
        "Anette Frank",
        "Albert Gatt",
        "Aykut Erdem",
        "Erkut Erdem"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that current VidLMs' grounding abilities are no better than those of vision-language models which use static images, and this benchmark serves as a catalyst for future research on VidL Ms, helping to highlight areas that still need to be explored."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}