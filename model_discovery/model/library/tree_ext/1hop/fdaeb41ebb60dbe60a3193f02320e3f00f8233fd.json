{
    "acronym": "fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
    "title": "Stealing the Decoding Algorithms of Language Models",
    "seed_ids": [
        "gpt2",
        "5697a0ede5425954d48daa6e1893dc87bd7d8be7",
        "23447f473cd240494b0a20ea008038aaef7e3391",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
    "abstract": "A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., 0.8, 1, 4, and 40 for the four versions of GPT-3.",
    "authors": [
        "A. Naseh",
        "Kalpesh Krishna",
        "Mohit Iyyer",
        "Amir Houmansadr"
    ],
    "venue": "Conference on Computer and Communications Security",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work shows, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs."
    },
    "citationCount": 13,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}