{
    "acronym": "85a70c0a048cba4f53dcf332ee73f6032a2e53bc",
    "title": "Noise-Free Score Distillation",
    "seed_ids": [
        "classfreediffu",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d"
    ],
    "s2id": "85a70c0a048cba4f53dcf332ee73f6032a2e53bc",
    "abstract": "Score Distillation Sampling (SDS) has emerged as the de facto approach for text-to-content generation in non-image domains. In this paper, we reexamine the SDS process and introduce a straightforward interpretation that demystifies the necessity for large Classifier-Free Guidance (CFG) scales, rooted in the distillation of an undesired noise term. Building upon our interpretation, we propose a novel Noise-Free Score Distillation (NFSD) process, which requires minimal modifications to the original SDS framework. Through this streamlined design, we achieve more effective distillation of pre-trained text-to-image diffusion models while using a nominal CFG scale. This strategic choice allows us to prevent the over-smoothing of results, ensuring that the generated data is both realistic and complies with the desired prompt. To demonstrate the efficacy of NFSD, we provide qualitative examples that compare NFSD and SDS, as well as several other methods.",
    "authors": [
        "Oren Katzir",
        "Or Patashnik",
        "D. Cohen-Or",
        "D. Lischinski"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A straightforward interpretation is introduced that demystifies the necessity for large Classifier-Free Guidance scales, rooted in the distillation of an undesired noise term, and a novel Noise-Free Score Distillation (NFSD) process is proposed, which requires minimal modifications to the original SDS framework."
    },
    "citationCount": 37,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}