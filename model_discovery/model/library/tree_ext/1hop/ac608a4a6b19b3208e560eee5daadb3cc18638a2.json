{
    "acronym": "ac608a4a6b19b3208e560eee5daadb3cc18638a2",
    "title": "Efficient Attention via Control Variates",
    "seed_ids": [
        "deltanet",
        "lara",
        "nystromformer",
        "performer",
        "rfa",
        "linformer",
        "sparsetransformer",
        "reformer",
        "flashattn",
        "lstransformer",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "29587ba6dca7e713c3abf6f2d1045e4a5bf10586",
        "ebd1619e5856084cfe60b40cc141a7f69d75b523",
        "dc1b905c0af4dc318b63cd52fbc867c788df4b8c",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13f10195d2a24fea28c0ea57cbd393aa76256c26",
        "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "90b21dbad8969b74d704eed15a3d98722a88e464",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "a25370452533bf47549243e97852b9cdf7a0ee0e",
        "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0",
        "12640af46eaf4c16125557b517a2d37fca70a82d",
        "e0cbbca02b332f398c6639b3bea0613f79166220",
        "37abe53ed31caa23ae833b2e67bb4aa1892e8d25",
        "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
        "5d032bd2632b6f5847767f39ce247098c6bbc563",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "08ffdec40291a2ccb5f8a6cc048b01247fb34b96",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "5a9bc55f6332e38f62eb509b684147a1d4f10fd9",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "ac608a4a6b19b3208e560eee5daadb3cc18638a2",
    "abstract": "Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.",
    "authors": [
        "Lin Zheng",
        "Jianbo Yuan",
        "Chong Wang",
        "Lingpeng Kong"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity."
    },
    "citationCount": 15,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}