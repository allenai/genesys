{
    "acronym": "aedfd41fdb6cf8bf363fcb52cb5352fd9d086156",
    "title": "Analysis of GraphSum\u2019s Attention Weights to Improve the Explainability of Multi-Document Summarization",
    "seed_ids": [
        "memcompress",
        "7cc730da554003dda77796d2cb4f06da5dfd5592"
    ],
    "s2id": "aedfd41fdb6cf8bf363fcb52cb5352fd9d086156",
    "abstract": "Modern multi-document summarization (MDS) methods are based on transformer architectures. They generate state of the art summaries, but lack explainability. To overcome this, we analyze the attention weights of a graph-based MDS such as GraphSum. We compare GraphSum\u2019s performance utilizing different textual units, i. e., sentences versus paragraphs, on two MDS benchmark datasets, namely WikiSum and MultiNews. Our experiments show that paragraph-level representations provide the best summarization performance. Subsequently, we focus on analyzing the paragraph-level attention weights of GraphSum\u2019s multi-heads and decoding layers. Furthermore, we examine source origin information via text similarity over a ROUGE-based reference metric. We observe a high correlation between the attention weights and our reference metric, especially on the later decoding layers. Finally, we investigate if the generated summaries follow a pattern of positional bias. Our results show that there is a high correlation between the position of paragraphs in the input documents and the content provided for the generated summary.",
    "authors": [
        "M. Hickmann",
        "Fabian Wurzberger",
        "Megi Hoxhalli",
        "Arne Lochner",
        "Jessica T\u00f6llich",
        "A. Scherp"
    ],
    "venue": "International Conference on Information Integration and Web-based Applications & Services",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work analyzes the attention weights of a graph-based MDS such as GraphSum to show that paragraph-level representations provide the best summarization performance and investigates if the generated summaries follow a pattern of positional bias."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}