{
    "acronym": "aef05160d4dc63e8a24de32388848ef8ef650da7",
    "title": "On the Anatomy of Attention",
    "seed_ids": [
        "transformer",
        "lineartransformer",
        "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "07a9f47885cae97efb7b4aa109392128532433da",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "84898960f68fa78296a102edc8ac81739f9a9408",
        "2a31319e73d4486716168b65cdf7559baeda18ce",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "aef05160d4dc63e8a24de32388848ef8ef650da7",
    "abstract": "We introduce a category-theoretic diagrammatic formalism in order to systematically relate and reason about machine learning models. Our diagrams present architectures intuitively but without loss of essential detail, where natural relationships between models are captured by graphical transformations, and important differences and similarities can be identified at a glance. In this paper, we focus on attention mechanisms: translating folklore into mathematical derivations, and constructing a taxonomy of attention variants in the literature. As a first example of an empirical investigation underpinned by our formalism, we identify recurring anatomical components of attention, which we exhaustively recombine to explore a space of variations on the attention mechanism.",
    "authors": [
        "Nikhil Khatri",
        "Tuomas Laakkonen",
        "Jonathon Liu",
        "Vincent Wang-Ma'scianica"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper identifies recurring anatomical components of attention, which it exhaustively recombine to explore a space of variations on the attention mechanism, and constructs a taxonomy of attention variants in the literature."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}