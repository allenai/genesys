{
    "acronym": "720882609a4727f97652095f1aa33f93ba192603",
    "title": "Towards Hierarchical Discrete Variational Autoencoders",
    "seed_ids": [
        "productkeymem",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "720882609a4727f97652095f1aa33f93ba192603",
    "abstract": "Variational Autoencoders (VAEs) have proven to be powerful latent variable models. However, the form of the approximate posterior can limit the expressiveness of the model. Cat-egorical distributions are \ufb02exible and useful building blocks for example in neural memory layers. We introduce the Hierarchical Discrete Variational Autoencoder (HD-VAE): a hierarchy of variational memory layers. The Concrete/Gumbel-Softmax relaxation allows maximizing a surrogate of the Evidence Lower Bound by stochastic gradient ascent. We show that, when using a limited number of latent variables, HD-VAE outperforms the Gaussian baseline on modelling multiple binary image datasets. Training very deep HD-VAE remains a challenge due to the relaxation bias that is induced by the use of a surrogate objective. We introduce a formal de\ufb01nition and conduct a preliminary theoretical and empirical study of the bias.",
    "authors": [
        "Valentin Li\u00e9vin",
        "Andrea Dittadi",
        "Lars Maal\u00f8e",
        "O. Winther"
    ],
    "venue": "",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Hierarchical Discrete Variational Autoencoder (HD-VAE) is introduced: a hierarchy of variational memory layers and it is shown that, when using a limited number of latent variables, HD-VAE outperforms the Gaussian baseline on modelling multiple binary image datasets."
    },
    "citationCount": 8,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}