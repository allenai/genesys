{
    "acronym": "bee79110f7b89292955984c7110ed0de8ae719a1",
    "title": "Fusing Pre-Trained Language Models with Multimodal Prompts through Reinforcement Learning",
    "seed_ids": [
        "gpt2",
        "05bcf9999525656cfaa59bc71f8572d771ff3776",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "5e00596fa946670d894b1bdaeff5a98e3867ef13",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "4d16457cded23bce6eaa91cd17aefd22af2279f0",
        "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "bee79110f7b89292955984c7110ed0de8ae719a1",
    "abstract": "Language models are capable of commonsense reasoning: while domain-specific models can learn from explicit knowledge (e.g. commonsense graphs [6] ethical norms [25]), and larger models like GPT-3 [7] mani-fest broad commonsense reasoning capacity. Can their knowledge be extended to multimodal inputs such as images and audio without paired domain data? In this work, we propose \u2021ESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pretrained models to address multimodal tasks such as visual commonsense reasoning. Our key novelty is to use rein-forcement learning to align multimodal inputs to language model generations without direct supervision: for example, our reward optimization relies only on cosine similarity derived from CLIP [52] and requires no additional paired (image, text) data. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of multimodal text generation tasks ranging from captioning to commonsense reasoning; these include a new benchmark we collect and release, the ESP dataset, which tasks models with generating the text of several different domains for each image. Our code and data are publicly released at https://github.com/JiwanChung/esper.",
    "authors": [
        "Youngjae Yu",
        "Jiwan Chung",
        "Heeseung Yun",
        "Jack Hessel",
        "J. Park",
        "Ximing Lu",
        "Rowan Zellers",
        "Prithviraj Ammanabrolu",
        "Ronan Le Bras",
        "Gunhee Kim",
        "Yejin Choi"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes \u2021ESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pretrained models to address multimodal tasks such as visual commonsense reasoning."
    },
    "citationCount": 9,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}