{
    "acronym": "6de9c3deff16b61f7c450e2ea02d39538b96abb8",
    "title": "Efficient Ensembles Improve Training Data Attribution",
    "seed_ids": [
        "transformer",
        "d6a546b23c3b67aa2dc2d03ef3e7692f031ebd2d",
        "fb507ada871d1e8c29e376dbf7b7879689aa89f9"
    ],
    "s2id": "6de9c3deff16b61f7c450e2ea02d39538b96abb8",
    "abstract": "Training data attribution (TDA) methods aim to quantify the influence of individual training data points on the model predictions, with broad applications in data-centric AI, such as mislabel detection, data selection, and copyright compensation. However, existing methods in this field, which can be categorized as retraining-based and gradient-based, have struggled with the trade-off between computational efficiency and attribution efficacy. Retraining-based methods can accurately attribute complex non-convex models but are computationally prohibitive, while gradient-based methods are efficient but often fail for non-convex models. Recent research has shown that augmenting gradient-based methods with ensembles of multiple independently trained models can achieve significantly better attribution efficacy. However, this approach remains impractical for very large-scale applications. In this work, we discover that expensive, fully independent training is unnecessary for ensembling the gradient-based methods, and we propose two efficient ensemble strategies, DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative to naive independent ensemble. These strategies significantly reduce training time (up to 80%), serving time (up to 60%), and space cost (up to 80%) while maintaining similar attribution efficacy to the naive independent ensemble. Our extensive experimental results demonstrate that the proposed strategies are effective across multiple TDA methods on diverse datasets and models, including generative settings, significantly advancing the Pareto frontier of TDA methods with better computational efficiency and attribution efficacy.",
    "authors": [
        "Junwei Deng",
        "Ting-Wei Li",
        "Shichang Zhang",
        "Jiaqi Ma"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work discovers that expensive, fully independent training is unnecessary for ensembling the gradient-based methods, and proposes two efficient ensemble strategies, DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative to naive independent ensemble, significantly advancing the Pareto frontier of TDA methods with better computational efficiency and attribution efficacy."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}