{
    "acronym": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5",
    "title": "QuadTree Attention for Vision Transformers",
    "seed_ids": [
        "lineartransformer",
        "12975ab22e9bee532404337a944b5c509a35cde5",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5",
    "abstract": "Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks, e.g. with 4.0% improvement in feature matching on ScanNet, about 50% flops reduction in stereo matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at https://github.com/Tangshitao/QuadtreeAttention.",
    "authors": [
        "Shitao Tang",
        "Jiahui Zhang",
        "Siyu Zhu",
        "Ping Tan"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces QuadTree Attention, which reduces the computational complexity from quadratic to linear, builds token pyramids and computes attention in a coarse-to-fine manner, and achieves state-of-the-art performance in various vision tasks."
    },
    "citationCount": 116,
    "influentialCitationCount": 22,
    "code": null,
    "description": null,
    "url": null
}