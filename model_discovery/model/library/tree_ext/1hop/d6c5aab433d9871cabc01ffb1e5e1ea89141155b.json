{
    "acronym": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
    "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation",
    "seed_ids": [
        "alibi",
        "roformer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "a9c214e846188adb645021cd7b1964b8ea1fef6f",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "7509c66a666e2e3f14bc8676b969b945ee6e136f",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "7072db6eddb85ecd2c117365d91bd694760f726e",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "84476fdf6ead3553f4493dff8e02308439d6222b",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
    "abstract": "Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\\url{https://github.com/chijames/KERPLE.git}.",
    "authors": [
        "Ta-Chung Chi",
        "Ting-Han Fan",
        "P. Ramadge",
        "Alexander I. Rudnicky"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "KERPLE is proposed, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences using conditionally positive definite (CPD) kernels, and it is shown that a CPD kernel can be transformed into a PD kernel by adding a constant offset."
    },
    "citationCount": 39,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}