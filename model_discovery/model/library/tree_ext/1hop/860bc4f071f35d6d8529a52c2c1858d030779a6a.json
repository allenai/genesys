{
    "acronym": "860bc4f071f35d6d8529a52c2c1858d030779a6a",
    "title": "In-context Reinforcement Learning with Algorithm Distillation",
    "seed_ids": [
        "gpt",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8"
    ],
    "s2id": "860bc4f071f35d6d8529a52c2c1858d030779a6a",
    "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.",
    "authors": [
        "M. Laskin",
        "Luyu Wang",
        "Junhyuk Oh",
        "Emilio Parisotto",
        "Stephen Spencer",
        "Richie Steigerwald",
        "D. Strouse",
        "S. Hansen",
        "Angelos Filos",
        "Ethan A. Brooks",
        "M. Gazeau",
        "Himanshu Sahni",
        "Satinder Singh",
        "Volodymyr Mnih"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is demonstrated that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and it is found that AD learns a more data-efficient RL algorithm than the one that generated the source data."
    },
    "citationCount": 77,
    "influentialCitationCount": 17,
    "code": null,
    "description": null,
    "url": null
}