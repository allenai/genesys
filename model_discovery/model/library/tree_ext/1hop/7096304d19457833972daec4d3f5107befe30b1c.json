{
    "acronym": "7096304d19457833972daec4d3f5107befe30b1c",
    "title": "Do Neural Language Models Overcome Reporting Bias?",
    "seed_ids": [
        "gpt",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "7096304d19457833972daec4d3f5107befe30b1c",
    "abstract": "Mining commonsense knowledge from corpora suffers from reporting bias, over-representing the rare at the expense of the trivial (Gordon and Van Durme, 2013). We study to what extent pre-trained language models overcome this issue. We find that while their generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their training corpus.",
    "authors": [
        "Vered Shwartz",
        "Yejin Choi"
    ],
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that while pre-trained language models' generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their training corpus."
    },
    "citationCount": 55,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}