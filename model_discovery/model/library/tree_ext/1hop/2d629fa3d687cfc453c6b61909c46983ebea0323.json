{
    "acronym": "2d629fa3d687cfc453c6b61909c46983ebea0323",
    "title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
    "seed_ids": [
        "gpt2",
        "s4d",
        "abc",
        "cosformer",
        "lara",
        "nystromformer",
        "performer",
        "rfa",
        "732e3faec4e5be4d144256f2c379b9dc49f0b227",
        "ca444821352a4bd91884413d8070446e2960715a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "e0cbbca02b332f398c6639b3bea0613f79166220",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c828f4bf1a752700dd2c4a96fdd08ba938cda43d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "016a3ba7adcae71f5a23ed2663d8062ae1da63e6",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "2a31319e73d4486716168b65cdf7559baeda18ce",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2d629fa3d687cfc453c6b61909c46983ebea0323",
    "abstract": "Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.",
    "authors": [
        "Jinchao Zhang",
        "Shuyang Jiang",
        "Jiangtao Feng",
        "Lin Zheng",
        "Lingpeng Kong"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self,Noncausal cross, and causal cross attentions, and sheds light on the fundamental problems of efficient attentions."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}