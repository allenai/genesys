{
    "acronym": "addd2d86d19c1e7c8854e827fb2656a50c250440",
    "title": "WikiAsp: A Dataset for Multi-domain Aspect-based Summarization",
    "seed_ids": [
        "memcompress",
        "83fac78857c7e65fe10a11a798674dd3cd259c1d",
        "0bd5beed98675a63f9207851674d478009549900",
        "7cc730da554003dda77796d2cb4f06da5dfd5592"
    ],
    "s2id": "addd2d86d19c1e7c8854e827fb2656a50c250440",
    "abstract": "Abstract Aspect-based summarization is the task of generating focused summaries based on specific points of interest. Such summaries aid efficient analysis of text, such as quickly understanding reviews or opinions from different angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the development of previous models has tended to be domain-specific. In this paper, we propose WikiAsp,1 a large-scale dataset for multi-domain aspect- based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and boundaries of each article as a proxy for aspect annotation. We propose several straightforward baseline models for this task and conduct experiments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and consistent explanation of time-sensitive events.",
    "authors": [
        "Hiroaki Hayashi",
        "Prashant Budania",
        "Peng Wang",
        "Chris Ackerson",
        "Raj Neervannan",
        "Graham Neubig"
    ],
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 49,
    "influentialCitationCount": 11,
    "code": null,
    "description": null,
    "url": null
}