{
    "acronym": "48498bb5d9c4479165b5f9e383b300c6c367fa52",
    "title": "eRock at Qur\u2019an QA 2022: Contemporary Deep Neural Networks for Qur\u2019an based Reading Comprehension Question Answers",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "48498bb5d9c4479165b5f9e383b300c6c367fa52",
    "abstract": "Question Answering (QA) has enticed the interest of NLP community in recent years. NLP enthusiasts are engineering new Models and fine-tuning the existing ones that can give out answers for the posed questions. The deep neural network models are found to perform exceptionally on QA tasks, but these models are also data intensive. For instance, BERT has outperformed many of its contemporary contenders on SQuAD dataset. In this work, we attempt at solving the closed domain reading comprehension Question Answering task on QRCD (Qur\u2019anic Reading Comprehension Dataset) to extract an answer span from the provided passage, using BERT as a baseline model. We improved the model\u2019s output by applying regularization techniques like weight-decay and data augmentation. Using different strategies we had 0.59% and 0.31% partial Reciprocal Ranking (pRR) on development and testing data splits respectively.",
    "authors": [
        "Esha Aftab",
        "M. K. Malik"
    ],
    "venue": "OSACT",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work attempts at solving the closed domain reading comprehension Question Answering task on QRCD (Qur\u2019anic Reading Comprehension Dataset) to extract an answer span from the provided passage, using BERT as a baseline model and improved the model\u2019s output by applying regularization techniques like weight-decay and data augmentation."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}