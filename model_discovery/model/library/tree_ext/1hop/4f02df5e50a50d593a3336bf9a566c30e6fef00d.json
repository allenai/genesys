{
    "acronym": "4f02df5e50a50d593a3336bf9a566c30e6fef00d",
    "title": "A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models",
    "seed_ids": [
        "griffin",
        "retnet",
        "rwkv4",
        "hgrn",
        "hiddenattnmamba",
        "47741d9e57f7a986a350f5cb20de287b1b1b8ff8",
        "ba4c5a116d07b37dea1046b6d16a60cb2d01cd47",
        "46732358e98ce6be0c564ae11f71d556a64b4c35",
        "157ed5647da39a7f5d33a84a90414b2a9e97e301",
        "cbaf689fd9ea9bc939510019d90535d6249b3367",
        "05c1dc502ed51162580ccd320d5668d2fec94a7a",
        "51f38bd957fa863022feb5878fa1ba3bea6657cf",
        "26e6cd121c5fdb147df83cb848e4813c926737c8",
        "d53fe76bd2795a19ddf52d012917782f6f6f2c1e",
        "40438187c1037ebce7b477a400853ba1b47ef772",
        "2dda6da7375bf5e8bcf60f87b17ba10757f3bc57",
        "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f",
        "3169a2478154e26fd7f63fdf43cf3a24f1007962",
        "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "745594bd0dc3e9dc86f74e100cd2c98ed36256c0",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "8420fddf489bd7c5b822bd904aa11ff3742bfb78",
        "434d751d355d7a7c20efa570e785c76286245e77",
        "d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797",
        "e6917b14918f90e8fb89ad4debebd3937e57a123",
        "59708496c88f173276a40d779a1f83bcfe2e7842",
        "22a0bfac8cc0cb9c01123d8a898e3235ddcab269",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "debbb47abc9fb757857f7c06aa86ca558d37c2d7",
        "2d01b6afbc86cba1cb895dbcd9396b13952bf0e5",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "a128b1c47e6842605fb95bceae930d2135fc38fc",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "d5e999aae76d5270ef272076979c809817458212"
    ],
    "s2id": "4f02df5e50a50d593a3336bf9a566c30e6fef00d",
    "abstract": "Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.",
    "authors": [
        "Itamar Zimerman",
        "Ameen Ali",
        "Lior Wolf"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}