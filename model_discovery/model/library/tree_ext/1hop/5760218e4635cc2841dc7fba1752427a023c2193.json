{
    "acronym": "5760218e4635cc2841dc7fba1752427a023c2193",
    "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
    "seed_ids": [
        "gpt3",
        "bert",
        "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b",
        "ca31b8584b6c022ef15ddfe994fe361e002b7729",
        "16d83e930a4dab2d49f5d276838ddce79df3f787",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "e8e035f9768a4d4e7fe9a2e167cd93d170407b1b",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "dec42306af017bc778bbf1496776f3cd4d5bd42e",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "7ea0e91c5d5dc73f2133bc46d7ebb6cb83034dae",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "069e0d896da7c79faeee4cf057548d5da7ce885e",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5760218e4635cc2841dc7fba1752427a023c2193",
    "abstract": "Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs including its category and evaluation metrics, and summarize the existing debiasing techniques. Finally, we discuss existing challenges and point out promising research directions. By demonstrating these aspects, this paper aims to facilitate a deeper understanding of MLLMs and their potentiality in various domains.",
    "authors": [
        "Yuemei Xu",
        "Ling Hu",
        "Jiayi Zhao",
        "Zihan Qiu",
        "Yuqi Ye",
        "Hanwen Gu"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities, and surveys the existing studies on multilingual representations to investigate whether the current MLLMs can learn a universal language representation."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}