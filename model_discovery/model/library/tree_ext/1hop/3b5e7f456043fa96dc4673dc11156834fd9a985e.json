{
    "acronym": "3b5e7f456043fa96dc4673dc11156834fd9a985e",
    "title": "Learning diverse attacks on large language models for robust red-teaming and safety tuning",
    "seed_ids": [
        "gpt2",
        "29083f7d40c7c9f4e490a8871d80e4f0fe18706f",
        "fe13cc3650fd7df8f98d0596bfc13178af82c799",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "9405cc0d6169988371b2755e573cc28650d14dfe",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28"
    ],
    "s2id": "3b5e7f456043fa96dc4673dc11156834fd9a985e",
    "abstract": "Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming typically uses reinforcement learning to fine-tune an attacker language model to generate prompts that elicit undesirable responses from a target LLM, as measured, for example, by an auxiliary toxicity classifier. We show that even with explicit regularization to favor novelty and diversity, existing approaches suffer from mode collapse or fail to generate effective attacks. As a flexible and probabilistically principled alternative, we propose to use GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts. We find that the attacks generated by our method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs. Finally, we demonstrate that models safety-tuned using a dataset of red-teaming prompts generated by our method are robust to attacks from other RL-based red-teaming approaches.",
    "authors": [
        "Seanie Lee",
        "Minsu Kim",
        "Lynn Cherif",
        "David Dobre",
        "Juho Lee",
        "Sung Ju Hwang",
        "Kenji Kawaguchi",
        "Gauthier Gidel",
        "Y. Bengio",
        "Nikolay Malkin",
        "Moksh Jain"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes to use GFlowNet fine-tuning followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts, and finds that the attacks generated by the method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}