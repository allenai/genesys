{
    "acronym": "ae9a2bcd460354c706aaea8797b1c2c15841a6b6",
    "title": "A Survey on Vision-Language-Action Models for Embodied AI",
    "seed_ids": [
        "transformer",
        "gpt",
        "gpt3",
        "bert",
        "13d12b26db345f62e8e512db181b96a7f8763b47",
        "690df0820f35a47e1ce44f90e6ddb4132aa09267",
        "25425e299101b13ec2872417a14f961f4f8aa18e",
        "979810ca765695a481c37126103b8ba256ee2192",
        "498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "60c8d0619481eaafdd1189af610d0e636271fed5",
        "235303a8bc1e4892efd525a38ead657422d8a519",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "ada81a4de88a6ce474df2e2446ad11fea480616e",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "5e00596fa946670d894b1bdaeff5a98e3867ef13",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ae9a2bcd460354c706aaea8797b1c2c15841a6b6",
    "abstract": "Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components through pretraining. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.",
    "authors": [
        "Yueen Ma",
        "Zixing Song",
        "Yuzheng Zhuang",
        "Jianye Hao",
        "Irwin King"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is imperative to capture the evolving landscape of embodied AI through a comprehensive survey of vision-language-action models, reflecting the rapid advancement of embodied AI."
    },
    "citationCount": 3,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}