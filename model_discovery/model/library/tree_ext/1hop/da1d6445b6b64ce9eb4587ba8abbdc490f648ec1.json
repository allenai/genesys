{
    "acronym": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
    "title": "Training Language Models with Memory Augmentation",
    "seed_ids": [
        "transformerxl",
        "compressivetransformer",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "a99cbe3d7b2e201491a2fa1f4ba57b1e80cca3a3",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "64a29bee2e1ad29547d590a3cc26274f4c537145",
        "6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
        "46c585ee9abf76779ea4b863d2da4358efd0d1d3",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280"
    ],
    "s2id": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
    "abstract": "Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories\u2014local, long-term, and external memory\u2014at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",
    "authors": [
        "Zexuan Zhong",
        "Tao Lei",
        "Danqi Chen"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents TRIME, a novel yet simple training approach designed for training LMs with memory augmentation that adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs."
    },
    "citationCount": 107,
    "influentialCitationCount": 9,
    "code": null,
    "description": null,
    "url": null
}