{
    "acronym": "8af925f4edf45131b5b6fed8aa655089d58692fa",
    "title": "Lite Transformer with Long-Short Range Attention",
    "seed_ids": [
        "lighdynconv",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "830995ef17cc291c13f42dfd9f462137de1d2179",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "16c844fd4d97f3c6eb38b0d6527c87d184efedc3"
    ],
    "s2id": "8af925f4edf45131b5b6fed8aa655089d58692fa",
    "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications since mobile phones are tightly constrained by the hardware resources and battery. In this paper, we investigate the mobile setting (under 500M Mult-Adds) for NLP tasks to facilitate the deployment on the edge devices. We present Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group captures the long-distance relationship (by attention). Based on this primitive, we design Lite Transformer that is tailored for the mobile NLP application. Our Lite Transformer demonstrates consistent improvement over the transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. It outperforms the transformer on WMT\u201914 English-French by 1.2 BLEU under 500M Mult-Adds and 1.7 BLEU under 100M Mult-Adds, and reduces the computation of transformer base model by 2.5x. Further, with general techniques, our Lite Transformer achieves 18.2x model size compression. For language modeling, our Lite Transformer also achieves 3.8 lower perplexity than the transformer around 500M Mult-Adds. Without the costly architecture search that requires more than 250 GPU years, our Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU under the mobile setting.",
    "authors": [
        "Zhanghao Wu",
        "Zhijian Liu",
        "Ji Lin",
        "Yujun Lin",
        "Song Han"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper investigates the mobile setting for NLP tasks to facilitate the deployment on the edge devices and designs Lite Transformer, which demonstrates consistent improvement over the transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling."
    },
    "citationCount": 255,
    "influentialCitationCount": 24,
    "code": null,
    "description": null,
    "url": null
}