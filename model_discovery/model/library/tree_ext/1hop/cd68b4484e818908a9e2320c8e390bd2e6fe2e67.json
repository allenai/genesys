{
    "acronym": "cd68b4484e818908a9e2320c8e390bd2e6fe2e67",
    "title": "Hierarchical Bayesian nonparametric models for power-law sequences",
    "seed_ids": [
        "transformerxl",
        "cd63025532a62fa245a02ec05e32ac4d23089631",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "cd68b4484e818908a9e2320c8e390bd2e6fe2e67",
    "abstract": "Sequence data that exhibits power-law behavior in its marginal and conditional distributions arises frequently from natural processes, with natural language text being a prominent example. We study probabilistic models for such sequences based on a hierarchical non-parametric Bayesian prior, develop inference and learning procedures for making these models useful in practice and applicable to large, real-world data sets, and empirically demonstrate their excellent predictive performance. In particular, we consider models based on the infinite-depth variant of the hierarchical Pitman-Yor process (HPYP) language model [Teh, 2006b] known as the Sequence Memoizer, as well as Sequence Memoizer-based cache language models and hybrid models combining the HPYP with neural language models. We empirically demonstrate that these models performwell on languagemodelling and data compression tasks.",
    "authors": [
        "Jan Gasthaus"
    ],
    "venue": "",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}