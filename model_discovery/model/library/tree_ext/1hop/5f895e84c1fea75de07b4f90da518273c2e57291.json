{
    "acronym": "5f895e84c1fea75de07b4f90da518273c2e57291",
    "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation",
    "seed_ids": [
        "performer",
        "lineartransformer",
        "bigbird",
        "longformer",
        "reformer",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "77706ee4cbdbb23345da22af37bc1b9f5ec8f110",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "c0f709acf38eb27702b0fbce1215db0ebaa2de2b",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "a68c3412e60560290400d2707596f82a914b7c00",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "b1ac64438608aac1a8dfd0adf8fec8c6220f6bfd",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d"
    ],
    "s2id": "5f895e84c1fea75de07b4f90da518273c2e57291",
    "abstract": "Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.",
    "authors": [
        "Beidi Chen",
        "Tri Dao",
        "Eric Winsor",
        "Zhao Song",
        "A. Rudra",
        "C. R\u00e9"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, Scatterbrain is proposed, a novel way to unify sparse and low-rank attention for accurate and efficient approximation and is unbiased with provably low error."
    },
    "citationCount": 94,
    "influentialCitationCount": 11,
    "code": null,
    "description": null,
    "url": null
}