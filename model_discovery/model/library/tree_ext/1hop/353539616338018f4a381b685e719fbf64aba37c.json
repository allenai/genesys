{
    "acronym": "353539616338018f4a381b685e719fbf64aba37c",
    "title": "WallFacer: Guiding Transformer Model Training Out of the Long-Context Dark Forest with N-body Problem",
    "seed_ids": [
        "ring",
        "8511ea96d61593de57cbc2e996910e5cb3dbfe84",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9",
        "f8d44802ac8190864c61c9aaf4a8b450261873ab",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "16e623059ffccab60f4c35be028a2d4f10933515",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "353539616338018f4a381b685e719fbf64aba37c",
    "abstract": "In recent years, Transformer-based Large Language Models (LLMs) have garnered significant attention due to their exceptional performance across a variety of tasks. However, training these models on long sequences presents a substantial challenge in terms of efficiency and scalability. Current methods are constrained either by the number of attention heads, limiting scalability, or by excessive communication overheads. In this paper, we propose an insight that Attention Computation can be considered as a special case of n-body problem with direct interactions. Based on this concept, this paper introduces WallFacer, an efficient long-sequence training system with a novel multi-dimensional ring sequence parallelism, fostering an efficient communication paradigm and extra tuning space for communication arrangement. Through comprehensive experiments under diverse environments and model settings, we demonstrate that WallFacer significantly surpasses state-of-the-art method that supports near-infinite sequence length, achieving performance improvements of up to 77.12%.",
    "authors": [
        "Ziming Liu",
        "Shaoyu Wang",
        "Shenggan Cheng",
        "Zhongkai Zhao",
        "Xuanlei Zhao",
        "Jim Demmel",
        "Yang You"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "WallFacer is introduced, an efficient long-sequence training system with a novel multi-dimensional ring sequence parallelism, fostering an efficient communication paradigm and extra tuning space for communication arrangement that significantly surpasses state-of-the-art method that supports near-infinite sequence length."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}