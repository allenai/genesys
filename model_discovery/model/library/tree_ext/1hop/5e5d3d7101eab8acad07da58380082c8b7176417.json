{
    "acronym": "5e5d3d7101eab8acad07da58380082c8b7176417",
    "title": "Enhancing Medical Language Understanding: Adapting LLMs to the Medical Domain through Hybrid Granularity Mask Learning",
    "seed_ids": [
        "bert",
        "1d26c947406173145a4665dd7ab255e03494ea28",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f"
    ],
    "s2id": "5e5d3d7101eab8acad07da58380082c8b7176417",
    "abstract": "Large Language models have made remarkable strides in natural language understanding and generation. However, their performance in specialized fields like medicine often falls short due to the lack of domain-specific knowledge during pre-training. While fine-tuning on labeled medical data is a common approach for task adaptation, it may not capture the comprehensive medical knowledge required. In this paper, we proposed a Hybrid Granularity Mask Learning (HGM) method for domain adaptation in the medical field. Our method incorporates multi-level linguistic characteries including token, entity, and subsentence to enable the model to acquire medical knowledge comprehensively. We fine-tune a medical-specific language model derived from ChatGLM-6B and Bloom-7B on downstream medical tasks and evaluate its performance. The results demonstrate a significant improvement compared to the baseline, thus affirming the effectiveness of our proposed method.",
    "authors": [
        "Longjun Fan",
        "Xiaohong Liu",
        "Yuhao Wang",
        "Guoxing Yang",
        "Zongxin Du",
        "Guangyu Wang"
    ],
    "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper fine-tune a medical-specific language model derived from ChatGLM-6B and Bloom-7B on downstream medical tasks and evaluates its performance, demonstrating a significant improvement compared to the baseline, thus affirming the effectiveness of the proposed HGM method."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}