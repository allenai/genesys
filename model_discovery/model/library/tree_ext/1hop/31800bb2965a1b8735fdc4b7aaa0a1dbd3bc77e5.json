{
    "acronym": "31800bb2965a1b8735fdc4b7aaa0a1dbd3bc77e5",
    "title": "Learning to Estimate System Specifications in Linear Temporal Logic using Transformers and Mamba",
    "seed_ids": [
        "transformer",
        "mamba",
        "0a32e6ff6eaac83ff325bae4557a8362222979aa",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2"
    ],
    "s2id": "31800bb2965a1b8735fdc4b7aaa0a1dbd3bc77e5",
    "abstract": "Temporal logic is a framework for representing and reasoning about propositions that evolve over time. It is commonly used for specifying requirements in various domains, including hardware and software systems, as well as robotics. Specification mining or formula generation involves extracting temporal logic formulae from system traces and has numerous applications, such as detecting bugs and improving interpretability. Although there has been a surge of deep learning-based methods for temporal logic satisfiability checking in recent years, the specification mining literature has been lagging behind in adopting deep learning methods despite their many advantages, such as scalability. In this paper, we introduce autoregressive models that can generate linear temporal logic formulae from traces, towards addressing the specification mining problem. We propose multiple architectures for this task: transformer encoder-decoder, decoder-only transformer, and Mamba, which is an emerging alternative to transformer models. Additionally, we devise a metric for quantifying the distinctiveness of the generated formulae and a straightforward algorithm to enforce the syntax constraints. Our experiments show that the proposed architectures yield promising results, generating correct and distinct formulae at a fraction of the compute cost needed for the combinatorial baseline.",
    "authors": [
        "I. Isik",
        "Ebru Aydin Gol",
        "R. G. Cinbis"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces autoregressive models that can generate linear temporal logic formulae from traces, towards addressing the specification mining problem, and proposes multiple architectures for this task: transformer encoder-decoder, decoder-only transformer, and Mamba, which is an emerging alternative to transformer models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}