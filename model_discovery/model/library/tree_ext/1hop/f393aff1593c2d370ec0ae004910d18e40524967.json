{
    "acronym": "f393aff1593c2d370ec0ae004910d18e40524967",
    "title": "Resurrecting Recurrent Neural Networks for Long Sequences",
    "seed_ids": [
        "s4",
        "hippo",
        "s4d",
        "s5",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "a30ac45ac5b7bd2148d3fb80ee7f3c29724e3170",
        "ca444821352a4bd91884413d8070446e2960715a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "defecf3dc299214f4cb76a093c6eed2297eaa46f",
        "9226ae23b95b3f6891461e086d910ffeb7ac448a",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "7e9ff94476f41041c75e253e84f487db00e9c861"
    ],
    "s2id": "f393aff1593c2d370ec0ae004910d18e40524967",
    "abstract": "Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.",
    "authors": [
        "Antonio Orvieto",
        "Samuel L. Smith",
        "Albert Gu",
        "Anushan Fernando",
        "Caglar Gulcehre",
        "Razvan Pascanu",
        "Soham De"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency."
    },
    "citationCount": 149,
    "influentialCitationCount": 26,
    "code": null,
    "description": null,
    "url": null
}