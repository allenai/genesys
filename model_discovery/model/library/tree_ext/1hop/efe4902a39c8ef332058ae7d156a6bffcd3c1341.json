{
    "acronym": "efe4902a39c8ef332058ae7d156a6bffcd3c1341",
    "title": "Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension",
    "seed_ids": [
        "universaltrans"
    ],
    "s2id": "efe4902a39c8ef332058ae7d156a6bffcd3c1341",
    "abstract": "Multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer. In this paper, we introduce the Dynamic Self-attention Network (DynSAN) for multi-passage reading comprehension task, which processes cross-passage information at token-level and meanwhile avoids substantial computational costs. The core module of the dynamic self-attention is a proposed gated token selection mechanism, which dynamically selects important tokens from a sequence. These chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies. Besides, convolutional layers are combined with the dynamic self-attention to enhance the model\u2019s capacity of extracting local semantic. The experimental results show that the proposed DynSAN achieves new state-of-the-art performance on the SearchQA, Quasar-T and WikiHop datasets. Further ablation study also validates the effectiveness of our model components.",
    "authors": [
        "Yimeng Zhuang",
        "H. Wang"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The experimental results show that the proposed DynSAN achieves new state-of-the-art performance on the SearchQA, Quasar-T and WikiHop datasets, and ablation study also validates the effectiveness of the model components."
    },
    "citationCount": 27,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}