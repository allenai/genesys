{
    "acronym": "e2a9b9f13182ba604a8dcf560fbdf21086b0c520",
    "title": "Vector Quantized Time Series Generation with a Bidirectional Prior Model",
    "seed_ids": [
        "classfreediffu",
        "e875667d1ae8fd8f3b760eee6feb6c8a79497e8c",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1"
    ],
    "s2id": "e2a9b9f13182ba604a8dcf560fbdf21086b0c520",
    "abstract": "Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, than its competing TSG methods. Our experimental evaluation is conducted on all datasets from the UCR archive, using well-established metrics in the IMG literature, such as Fr\\'echet inception distance and inception scores. Our implementation on GitHub: \\url{https://github.com/ML4ITS/TimeVQVAE}.",
    "authors": [
        "Daesoo Lee",
        "Sara Malacarne",
        "Erlend Aune"
    ],
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "TimeVQVAE is proposed, the first work, to the knowledge, that uses vector quantization (VQ) techniques to address the TSG problem, and the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency."
    },
    "citationCount": 11,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}