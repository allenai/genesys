{
    "acronym": "ef4bd399a66f6e2b4a0b6bfe2aa2063eaef10375",
    "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
    "seed_ids": [
        "gpt2",
        "c4fb74b5f69e5c0f3822b82e1bd082d5d95fc8d0",
        "12daedd80a4f860960c5b50314d09d6827f4fd4a",
        "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "ef4bd399a66f6e2b4a0b6bfe2aa2063eaef10375",
    "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form optimal solution with reasonable modeling capacity. Despite the success, several challenges arise when apply NADO to a wide range of scenarios. Vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on a regularization to satisfy the stochastic version of Bellman equation. In addition, the vanilla implementation of NADO introduces a few additional transformer layers, suffering from a limited capacity especially compared to other finetune-based model adaptation methods like LoRA. In this paper, we propose a improved version of the NADO algorithm, namely DiNADO (norm-Disentangled NeurAlly-Decomposed Oracles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA. We discuss in depth how DiNADO achieves better capacity, stability and flexibility with both empirical and theoretical results. Experiments on formality control in machine translation and the lexically constrained generation task CommonGen demonstrates the significance of the improvements.",
    "authors": [
        "Sidi Lu",
        "Wenbo Zhao",
        "Chenyang Tao",
        "Arpit Gupta",
        "Shan Wu",
        "Tagyoung Chung",
        "Nanyun Peng"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Improved version of the NADO algorithm, namely DiNADO (norm-Disentangled NeurAlly-Decomposed Oracles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}