{
    "acronym": "bb3faec0406af580a5c1dfe05bba59295a8272f7",
    "title": "Unifying Layout Generation with a Decoupled Diffusion Model",
    "seed_ids": [
        "d3pms",
        "69144d537f90f214d5b07a7c79121d16afd7da16",
        "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "23070cb7b4e282f784237c2e475b518238ff7e44"
    ],
    "s2id": "bb3faec0406af580a5c1dfe05bba59295a8272f7",
    "abstract": "Layout generation aims to synthesize realistic graphic scenes consisting of elements with different attributes in-cluding category, size, position, and between-element relation. It is a crucial task for reducing the burden on heavyduty graphic design works for formatted scenes, e.g., publications, documents, and user interfaces (UIs). Diverse application scenarios impose a big challenge in unifying various layout generation subtasks, including conditional and unconditional generation. In this paper, we propose a Layout Diffusion Generative Model (LDGM) to achieve such unification with a single decoupled diffusion model. LDGM views a layout of arbitrary missing or coarse element attributes as an intermediate diffusion status from a completed layout. Since different attributes have their individual semantics and characteristics, we propose to decouple the diffusion processes for them to improve the diversity of training samples and learn the reverse process jointly to exploit global-scope contexts for facilitating generation. As a result, our LDGM can generate layouts either from scratch or conditional on arbitrary available attributes. Extensive qualitative and quantitative experiments demonstrate our proposed LDGM outperforms existing layout generation models in both functionality and performance.",
    "authors": [
        "Mude Hui",
        "Zhizheng Zhang",
        "Xiaoyi Zhang",
        "Wenxuan Xie",
        "Yuwang Wang",
        "Yan Lu"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Layout Diffusion Generative Model (LDGM) is proposed to decouple the diffusion processes for them to improve the diversity of training samples and learn the reverse process jointly to exploit global-scope contexts for facilitating generation."
    },
    "citationCount": 18,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}