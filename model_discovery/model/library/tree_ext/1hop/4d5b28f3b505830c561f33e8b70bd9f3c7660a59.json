{
    "acronym": "4d5b28f3b505830c561f33e8b70bd9f3c7660a59",
    "title": "Recurrent Early Exits for Federated Learning with Heterogeneous Clients",
    "seed_ids": [
        "transformer",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c"
    ],
    "s2id": "4d5b28f3b505830c561f33e8b70bd9f3c7660a59",
    "abstract": "Federated learning (FL) has enabled distributed learning of a model across multiple clients in a privacy-preserving manner. One of the main challenges of FL is to accommodate clients with varying hardware capacities; clients have differing compute and memory requirements. To tackle this challenge, recent state-of-the-art approaches leverage the use of early exits. Nonetheless, these approaches fall short of mitigating the challenges of joint learning multiple exit classifiers, often relying on hand-picked heuristic solutions for knowledge distillation among classifiers and/or utilizing additional layers for weaker classifiers. In this work, instead of utilizing multiple classifiers, we propose a recurrent early exit approach named ReeFL that fuses features from different sub-models into a single shared classifier. Specifically, we use a transformer-based early-exit module shared among sub-models to i) better exploit multi-layer feature representations for task-specific prediction and ii) modulate the feature representation of the backbone model for subsequent predictions. We additionally present a per-client self-distillation approach where the best sub-model is automatically selected as the teacher of the other sub-models at each client. Our experiments on standard image and speech classification benchmarks across various emerging federated fine-tuning baselines demonstrate ReeFL's effectiveness over previous works.",
    "authors": [
        "Royson Lee",
        "J. Fern\u00e1ndez-Marqu\u00e9s",
        "S. Hu",
        "Da Li",
        "Stefanos Laskaridis",
        "L. Dudziak",
        "Timothy M. Hospedales",
        "Ferenc Husz'ar",
        "N. Lane"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a recurrent early exit approach named ReeFL that fuses features from different sub-models into a single shared classifier and uses a transformer-based early-exit module shared among sub-models to better exploit multi-layer feature representations for task-specific prediction."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}