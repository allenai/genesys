{
    "acronym": "199dbed0f3104b5d7367d62ee89561351a26d2ce",
    "title": "Online Compressive Transformer for End-to-End Speech Recognition",
    "seed_ids": [
        "compressivetransformer",
        "09e2c7adbed37440d4a339852cfa34e5b660f768",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "199dbed0f3104b5d7367d62ee89561351a26d2ce",
    "abstract": "Traditionally, transformer with connectionist temporal classi-\ufb01cation (CTC) was developed for of\ufb02ine speech recognition where the transcription was generated after the whole utterance has been spoken. However, it is crucial to carry out online transcription of speech signal for many applications including live broadcasting and meeting. This paper presents an online trans-former for real-time speech recognition where online transcription is generated chunk by chuck. In particular, an online compressive transformer (OCT) is proposed for end-to-end speech recognition. This OCT aims to generate immediate transcription for each audio chunk while the comparable performance with of\ufb02ine speech recognition can be still achieved. In the implementation, OCT tightly combines with both CTC and recurrent neural network transducer by minimizing their losses for training. In addition, this OCT systematically merges with compressive memory to reduce potential performance degradation due to online processing. This degradation is caused by online transcription which is generated by the chunks without history information. The experiments on speech recognition show that OCT does not only obtain comparable performance with of\ufb02ine transformer, but also work faster than the baseline model.",
    "authors": [
        "Chi-Hang Leong",
        "Yu-Han Huang",
        "Jen-Tzung Chien"
    ],
    "venue": "Interspeech",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An online trans-former for real-time speech recognition where online transcription is generated chunk by chuck and this OCT does not only obtain comparable performance with of\ufb02ine transformer, but also work faster than the baseline model."
    },
    "citationCount": 11,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}