{
    "acronym": "d7271e947a3f8c797d6ae7ee7579730167e6d154",
    "title": "Unsupervised Pre-training with Structured Knowledge for Improving Natural Language Inference",
    "seed_ids": [
        "gpt",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "81d89880586ff87b3ac8a14588b4e3f55c110ff8",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "d7271e947a3f8c797d6ae7ee7579730167e6d154",
    "abstract": "While recent research on natural language inference has considerably benefited from large annotated datasets, the amount of inference-related knowledge (including commonsense) provided in the annotated data is still rather limited. There have been two lines of approaches that can be used to further address the limitation: (1) unsupervised pretraining can leverage knowledge in much larger unstructured text data; (2) structured (often human-curated) knowledge has started to be considered in neural-network-based models for NLI. An immediate question is whether these two approaches complement each other, or how to develop models that can bring together their advantages. In this paper, we propose models that leverage structured knowledge in different components of pre-trained models. Our results show that the proposed models perform better than previous BERT-based state-of-the-art models. Although our models are proposed for NLI, they can be easily extended to other sentence or sentence-pair classification problems.",
    "authors": [
        "Xiaoyu Yang",
        "Xiao-Dan Zhu",
        "Zhan Shi",
        "Tianda Li"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes models that leverage structured knowledge in different components of pre-trained models for NLI and shows that the proposed models perform better than previous BERT-based state-of-the-art models."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}