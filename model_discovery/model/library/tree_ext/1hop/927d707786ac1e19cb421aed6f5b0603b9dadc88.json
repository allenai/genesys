{
    "acronym": "927d707786ac1e19cb421aed6f5b0603b9dadc88",
    "title": "Robustify Transformers with Robust Kernel Density Estimation",
    "seed_ids": [
        "deltanet",
        "48af9b314181b04edcc0b7224ffe4689036b755f",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "72f207c777e4a17180cc54ccc6a743d5f43227af",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "955f90930d48750e7239478b4eed440eb84131cd",
        "05b22d6ec2cff81bcfbac2a6cf67bc1e9ef0f60a",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "927d707786ac1e19cb421aed6f5b0603b9dadc88",
    "abstract": "Recent advances in Transformer architecture have empowered its empirical success in various tasks across di\ufb00erent domains. However, existing works mainly focus on improving the standard accuracy and computational cost, without considering the robustness of contaminated samples. Existing work [40] has shown that the self-attention mechanism, which is the center of the Transformer architecture, can be viewed as a non-parametric estimator based on the well-known kernel density estimation (KDE). This motivates us to leverage the robust kernel density estimation (RKDE) in the self-attention mechanism, to alleviate the issue of the contamination of data by down-weighting the weight of bad samples in the estimation process. The modi\ufb01ed self-attention mechanism can be incorporated into di\ufb00erent Transformer variants. Empirical results on language modeling and image classi\ufb01cation tasks demonstrate the e\ufb00ectiveness of this approach.",
    "authors": [
        "Xing Han",
        "Tongzheng Ren",
        "T. Nguyen",
        "Khai Nguyen",
        "J. Ghosh",
        "Nhat Ho"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work leverages the robust kernel density estimation (RKDE) in the self-attention mechanism, to alleviate the issue of the contamination of data by down-weighting the weight of bad samples in the estimation process."
    },
    "citationCount": 5,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}