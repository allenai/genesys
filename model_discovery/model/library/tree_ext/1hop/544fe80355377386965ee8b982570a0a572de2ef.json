{
    "acronym": "544fe80355377386965ee8b982570a0a572de2ef",
    "title": "VisionLLaMA: A Unified LLaMA Interface for Vision Tasks",
    "seed_ids": [
        "pi",
        "roformer",
        "5351614e845a70c7df0582fc306336b56dd51f25",
        "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
        "0b0debb710366cdff461938c80763eace1651af6",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "544fe80355377386965ee8b982570a0a572de2ef",
    "abstract": "Large language models are built on top of a transformer-based architecture to process textual inputs. For example, the LLaMA family of models stands out among many open-source implementations. Can the same transformer be used to process 2D images? In this paper, we answer this question by unveiling a LLaMA-like vision transformer in plain and pyramid forms, termed VisionLLaMA , which is tailored for this purpose. VisionLLaMA is a unified and generic modeling framework for solving most vision tasks. We extensively evaluate its effectiveness using typical pre-training paradigms in a good portion of downstream tasks of image perception and especially image generation. In many cases, VisionLLaMA have exhibited substantial gains over the previous state-of-the-art vision transformers. We believe that VisionLLaMA can serve as a strong new base-line model for vision generation and understanding. Our code will be released at https://github.com/Mei tuan-AutoML/VisionLLaMA .",
    "authors": [
        "Xiangxiang Chu",
        "Jianlin Su",
        "Bo Zhang",
        "Chunhua Shen"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper unveils a LLaMA-like vision transformer in plain and pyramid forms, termed VisionLLaMA, which is tailored for this purpose and believes that VisionLLaMA can serve as a strong new base-line model for vision generation and understanding."
    },
    "citationCount": 5,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}