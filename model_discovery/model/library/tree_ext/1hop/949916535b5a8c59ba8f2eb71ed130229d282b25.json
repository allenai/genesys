{
    "acronym": "949916535b5a8c59ba8f2eb71ed130229d282b25",
    "title": "Text Summarization from Judicial Records using Deep Neural Machines",
    "seed_ids": [
        "longformer",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "949916535b5a8c59ba8f2eb71ed130229d282b25",
    "abstract": "Courts are generating a large amount of data as legal proceedings. In Pakistan, the ratio of cases that are registered every year and the judgments made is very high mainly due to the time it takes to prepare for a trial. Text Summarization is one of the applications of Natural Language Processing (NLP) that can be used to provide a brief overview of the judgment to both the lawyers and the judges which will help save a lot of their precious time, and hence speedy justice can be provided to the people. Transformer-based models in NLP are a benchmark in solving sequence-to-sequence modeling problems. A downside of these powerful machines is that training a model demands high computation power. We have shown that fine-tuning a pre-trained legal Longformer Encoder-Decoder (LED) transformer model on a downstream task provides better accuracy scores on Australian judgments and our prepared datasets from the Supreme Court of Pakistan (SCP) and Islamabad High Court of Pakistan (IHCP). ROUGE, a commonly used metric in sequence modeling, is used to evaluate the trained model. For the Australian judgments, our approach exhibited a significant improvement for ROUGE-1 and ROUGE-2 scores of 37.97% and 20.04%. For our prepared dataset, our approach produced a ROUGE-1 score of 53.11%, a ROUGE-2 score of 32.12%, and a ROUGE-L score of 34.09%.",
    "authors": [
        "A. Sarwar",
        "Seemab Latif",
        "Rabia Irfan",
        "A. Ul-Hasan",
        "F. Shafait"
    ],
    "venue": "2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that fine-tuning a pre-trained legal Longformer Encoder-Decoder transformer model on a downstream task provides better accuracy scores on Australian judgments and the authors' prepared datasets from the Supreme Court of Pakistan (SCP) and Islamabad High Court ofPakistan (IHCP)."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}