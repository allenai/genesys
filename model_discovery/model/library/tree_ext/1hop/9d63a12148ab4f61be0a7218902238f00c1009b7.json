{
    "acronym": "9d63a12148ab4f61be0a7218902238f00c1009b7",
    "title": "Automating Continual Learning",
    "seed_ids": [
        "deltanet",
        "90f17a693e82e5046979b7b2300a4876f93cde64",
        "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
        "8f30c30f92fbe1c307ee3c7a68c80a2c0dc8d619",
        "ac3c0bfa0e38cbd26aca06cf0fcf7ad6d7deaa4d",
        "4fd61f6b860acc9c5da8766b7c9064f0ec896301",
        "ed535e93d5b5a8b689e861e9c6083a806d1535c2",
        "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "9d63a12148ab4f61be0a7218902238f00c1009b7",
    "abstract": "General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF) -- previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives. Our experiments demonstrate that ACL effectively solves\"in-context catastrophic forgetting\"; our ACL-learned algorithms outperform hand-crafted ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification datasets.",
    "authors": [
        "Kazuki Irie",
        "R'obert Csord'as",
        "J\u00fcrgen Schmidhuber"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms, and encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}