{
    "acronym": "45314de9beef18dcce99f0bc5e067446a0196505",
    "title": "OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
    "seed_ids": [
        "gpt3",
        "94ce1d5924e05e8d75e43ce70044293ddcef850a",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad"
    ],
    "s2id": "45314de9beef18dcce99f0bc5e067446a0196505",
    "abstract": null,
    "authors": [
        "Jenish Maharjan",
        "A. Garikipati",
        "N. Singh",
        "Leo Cyrus",
        "Mayank Sharma",
        "M. Ciobanu",
        "G. Barnes",
        "R. Thapa",
        "Q. Mao",
        "R. Das"
    ],
    "venue": "Scientific Reports",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "OpenMedLM, a prompting platform delivering state-of-the-art SOTA performance for OS LLMs on medical benchmarks, shows the first results to date demonstrating the ability of OS foundation models to optimize performance, absent specialized fine-tuning, and highlights medical-specific emergent properties in OS LLMs not documented elsewhere to date."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}