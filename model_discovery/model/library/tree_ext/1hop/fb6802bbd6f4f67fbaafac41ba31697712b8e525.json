{
    "acronym": "fb6802bbd6f4f67fbaafac41ba31697712b8e525",
    "title": "Revealing Secrets From Pre-trained Models",
    "seed_ids": [
        "gpt2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fb6802bbd6f4f67fbaafac41ba31697712b8e525",
    "abstract": "\u2014With the growing burden of training deep learning models with large data sets, transfer-learning has been widely adopted in many emerging deep learning algorithms. Trans- former models such as BERT are the main player in natural language processing and use transfer-learning as a de facto standard training method. A few big data companies release pre-trained models that are trained with a few popular datasets with which end users and researchers \ufb01ne-tune the model with their own datasets. Transfer-learning signi\ufb01cantly reduces the time and effort of training models. However, it comes at the cost of security concerns. In this paper, we show a new observation that pre-trained models and \ufb01ne-tuned models have signi\ufb01cantly high similarities in weight values. Also, we demonstrate that there exist vendor-speci\ufb01c computing patterns even for the same models. With these new \ufb01ndings, we propose a new model extraction attack that reveals the model architecture and the pre-trained model used by the black-box victim model with vendor-speci\ufb01c computing patterns and then estimates the entire model weights based on the weight value similarities between the \ufb01ne-tuned model and pre-trained model. We also show that the weight similarity can be leveraged for increasing the model extraction feasibility through a novel weight extraction pruning. ,",
    "authors": [
        "Mujahid Al Rafi",
        "Yuan Feng",
        "Hyeran Jeon"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new observation that pre-trained models and \ufb01ne-tuned models have signi\ufb01cantly high similarities in weight values is shown and the weight similarity can be leveraged for increasing the model extraction feasibility through a novel weight extraction pruning."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}