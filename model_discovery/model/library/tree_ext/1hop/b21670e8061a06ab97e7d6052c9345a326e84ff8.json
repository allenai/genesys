{
    "acronym": "b21670e8061a06ab97e7d6052c9345a326e84ff8",
    "title": "UL2: Unifying Language Learning Paradigms",
    "seed_ids": [
        "gpt2",
        "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "8db711adf1beb3e0c2ec492f3936841d827404e9",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "395aae6e7a79e5760457ca38e868acc970016230",
        "42f8a3da7021bc725fa14fdb63fa9c7c9fc934f6",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "2365410a710b421b2295cdca0074946cb50bb1d4",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "2a7023e7d1dbd6ea0d98efd09a1f18d8599fe78f",
        "9405cc0d6169988371b2755e573cc28650d14dfe",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "b21670e8061a06ab97e7d6052c9345a326e84ff8",
    "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.",
    "authors": [
        "Yi Tay",
        "Mostafa Dehghani",
        "Vinh Q. Tran",
        "Xavier Garc\u00eda",
        "Jason Wei",
        "Xuezhi Wang",
        "Hyung Won Chung",
        "Dara Bahri",
        "Tal Schuster",
        "H. Zheng",
        "Denny Zhou",
        "N. Houlsby",
        "Donald Metzler"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A unified framework for pre-training models that are universally effective across datasets and setups is presented and Mixture-of-Denoisers (MoD) is proposed, a pre- Training objective that combines diverse pre- training paradigms together."
    },
    "citationCount": 222,
    "influentialCitationCount": 31,
    "code": null,
    "description": null,
    "url": null
}