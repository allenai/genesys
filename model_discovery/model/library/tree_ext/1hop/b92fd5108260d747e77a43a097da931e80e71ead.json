{
    "acronym": "b92fd5108260d747e77a43a097da931e80e71ead",
    "title": "BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge",
    "seed_ids": [
        "transformerxl",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3"
    ],
    "s2id": "b92fd5108260d747e77a43a097da931e80e71ead",
    "abstract": "We introduce a simple yet effective method of integrating contextual embeddings with commonsense graph embeddings, dubbed BERT Infused Graphs: Matching Over Other embeDdings. First, we introduce a preprocessing method to improve the speed of querying knowledge bases. Then, we develop a method of creating knowledge embeddings from each knowledge base. We introduce a method of aligning tokens between two misaligned tokenization methods. Finally, we contribute a method of contextualizing BERT after combining with knowledge base embeddings. We also show BERTs tendency to correct lower accuracy question types. Our model achieves a higher accuracy than BERT, and we score fifth on the official leaderboard of the shared task and score the highest without any additional language model pretraining.",
    "authors": [
        "Jeff Da"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The model achieves a higher accuracy than BERT, and it score fifth on the official leaderboard of the shared task and score the highest without any additional language model pretraining."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}