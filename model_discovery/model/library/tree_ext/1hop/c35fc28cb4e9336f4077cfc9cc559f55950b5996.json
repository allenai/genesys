{
    "acronym": "c35fc28cb4e9336f4077cfc9cc559f55950b5996",
    "title": "VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning",
    "seed_ids": [
        "transformerxl",
        "4df12d0e186adcae91c22d8a7a7466c344725ed1",
        "70557ea6b65846fc30729ceed224acd4ac64ca5d"
    ],
    "s2id": "c35fc28cb4e9336f4077cfc9cc559f55950b5996",
    "abstract": "Video Paragraph Captioning aims to generate a multi-sentence description of an untrimmed video with multiple temporal event locations in a coherent storytelling. \nFollowing the human perception process, where the scene is effectively understood by decomposing it into visual (e.g. human, animal) and non-visual components (e.g. action, relations) under the mutual influence of vision and language, we first propose a visual-linguistic (VL) feature. In the proposed VL feature, the scene is modeled by three modalities including (i) a global visual environment; (ii) local visual main agents; (iii) linguistic scene elements. We then introduce an autoregressive Transformer-in-Transformer (TinT) to simultaneously capture the semantic coherence of intra- and inter-event contents within a video. Finally, we present a new VL contrastive loss function to guarantee the learnt embedding features are consistent with the captions semantics. Comprehensive experiments and extensive ablation studies on the ActivityNet Captions and YouCookII datasets show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms previous state-of-the-art methods in terms of accuracy and diversity. The source code is made publicly available at: https://github.com/UARK-AICV/VLTinT.",
    "authors": [
        "Kashu Yamazaki",
        "Khoa T. Vo",
        "Sang Truong",
        "B. Raj",
        "Ngan T. H. Le"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Comprehensive experiments and extensive ablation studies show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms previous state-of-the-art methods in terms of accuracy and diversity."
    },
    "citationCount": 21,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}