{
    "acronym": "09299aa885d4cef2c391c41b3f3ee444f3c00414",
    "title": "SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition",
    "seed_ids": [
        "gpt",
        "gpt2",
        "546e76210d6b5679d88a649566c45ee2b850ac45",
        "f98525843f7e03d247a2dc94951799a0d7101437",
        "f537eb37522aa762f904c003e2af00f14113b44c",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "09299aa885d4cef2c391c41b3f3ee444f3c00414",
    "abstract": "Error correction in automatic speech recognition (ASR) aims to correct those incorrect words in sentences generated by ASR models. Since recent ASR models usually have low word error rate (WER), to avoid affecting originally correct tokens, error correction models should only modify incorrect words, and therefore detecting incorrect words is important for error correction. Previous works on error correction either implicitly detect error words through target-source attention or CTC (connectionist temporal classification) loss, or explicitly locate specific deletion/substitution/insertion errors. However, implicit error detection does not provide clear signal about which tokens are incorrect and explicit error detection suffers from low detection accuracy. In this paper, we propose SoftCorrect with a soft error detection mechanism to avoid the limitations of both explicit and implicit error detection. Specifically, we first detect whether a token is correct or not through a probability produced by a dedicatedly designed language model, and then design a constrained CTC loss that only duplicates the detected incorrect tokens to let the decoder focus on the correction of error tokens. Compared with implicit error detection with CTC loss, SoftCorrect provides explicit signal about which words are incorrect and thus does not need to duplicate every token but only incorrect tokens; compared with explicit error detection, SoftCorrect does not detect specific deletion/substitution/insertion errors but just leaves it to CTC loss. Experiments on AISHELL-1 and Aidatatang datasets show that SoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperforming previous works by a large margin, while still enjoying fast speed of parallel generation.",
    "authors": [
        "Yichong Leng",
        "Xu Tan",
        "Wenjie Liu",
        "Kaitao Song",
        "Rui Wang",
        "Xiang-Yang Li",
        "Tao Qin",
        "Ed Lin",
        "Tie-Yan Liu"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SoftCorrect with a soft error detection mechanism to avoid the limitations of both explicit and implicit error detection, which first detects whether a token is correct or not through a probability produced by a dedicatedly designed language model, and then design a constrained CTC loss that only duplicates the detected incorrect tokens to let the decoder focus on the correction of error tokens."
    },
    "citationCount": 10,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}