{
    "acronym": "78b9f3b29d16704fb9fac31013ff8d3e714b44b6",
    "title": "Exploring the Limitations of Large Language Models in Compositional Relation Reasoning",
    "seed_ids": [
        "gpt3",
        "db4ab91d5675c37795e719e997a2827d3d83cd45",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "78b9f3b29d16704fb9fac31013ff8d3e714b44b6",
    "abstract": "We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts.",
    "authors": [
        "Jinman Zhao",
        "Xueyan Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}