{
    "acronym": "a53c8ba374d430d6c3786d13c04edb200d547750",
    "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
    "seed_ids": [
        "etc",
        "longt5",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "ada81a4de88a6ce474df2e2446ad11fea480616e",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "a53c8ba374d430d6c3786d13c04edb200d547750",
    "abstract": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.",
    "authors": [
        "Izzeddin Gur",
        "Hiroki Furuta",
        "Austin Huang",
        "Mustafa Safdari",
        "Yutaka Matsuo",
        "D. Eck",
        "Aleksandra Faust"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "WebAgent is introduced, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization."
    },
    "citationCount": 84,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}