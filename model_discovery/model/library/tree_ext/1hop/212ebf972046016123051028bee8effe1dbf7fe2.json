{
    "acronym": "212ebf972046016123051028bee8effe1dbf7fe2",
    "title": "P ARALLEL A TTENTION AND F EED -F ORWARD N ET D ESIGN FOR P RE - TRAINING AND I NFERENCE ON T RANSFORMERS",
    "seed_ids": [
        "linformer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "a022bda79947d1f656a1164003c1b3ae9a843df9"
    ],
    "s2id": "212ebf972046016123051028bee8effe1dbf7fe2",
    "abstract": "In this paper, we introduce Parallel Attention and Feed-Forward Net Design (PAF) for transformer models. Transformer models are indisputably the backbone of all Natural Language Processing applications. Therefore, any efforts aimed at improving their ef\ufb01ciency are guaranteed to have an enormous impact. Transformer models consist of many layers and each layer has an attention block followed by a feed-forward network (FFN) that processes the input based on the attention block\u2019s output. We refer to this standard design as Series Attention and Feed-Forward Net Design (SAF). For each layer in our proposed PAF design for transformer models, we make FFN block\u2019s computations independent of the output of the attention block. This decoupling allows FFN block of each layer to run in parallel to the attention block of that layer. We evaluate PAF design by training two large language models (RoBERTa-large and bert-large-uncased) and comparing them to their SAF counterparts on six tasks of the General Language Understanding (GLUE) benchmark which test a multitude of semantic attributes. PAF models achieves nearly identical performance as their SAF counterparts on all the six tasks. We also compare time complexities of attention blocks with FFN blocks and \ufb01nd that running both blocks in parallel can theoretically and in practice achieve upto 1.5x to 2x gains in speed. We leave the development of fast and ef\ufb01cient libraries for implementation of PAF design for future work.",
    "authors": [
        "Shashank Sonkar",
        "Richard Baraniuk"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces Parallel Attention and Feed-Forward Net Design (PAF) for transformer models and demonstrates that running both blocks in parallel can theoretically and in practice achieve upto 1.5x to 2x gains in speed."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}