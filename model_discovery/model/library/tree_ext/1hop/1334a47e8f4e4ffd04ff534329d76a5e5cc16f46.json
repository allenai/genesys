{
    "acronym": "1334a47e8f4e4ffd04ff534329d76a5e5cc16f46",
    "title": "Goal-Conditioned Imitation Learning using Score-based Diffusion Policies",
    "seed_ids": [
        "classfreediffu",
        "15736f7c205d961c00378a938daffaacb5a0718d",
        "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "1334a47e8f4e4ffd04ff534329d76a5e5cc16f46",
    "abstract": "We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture\"$\\textbf{BE}$havior generation with $\\textbf{S}$c$\\textbf{O}$re-based Diffusion Policies\"(BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 denoising steps, compared to 30+ steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clustering for effective goal-conditioned behavior learning. Finally, we show how BESO can even be used to learn a goal-independent policy from play-data using classifier-free guidance. To the best of our knowledge this is the first work that a) represents a behavior policy based on such a decoupled SDM b) learns an SDM based policy in the domain of GCIL and c) provides a way to simultaneously learn a goal-dependent and a goal-independent policy from play-data. We evaluate BESO through detailed simulation and show that it consistently outperforms several state-of-the-art goal-conditioned imitation learning methods on challenging benchmarks. We additionally provide extensive ablation studies and experiments to demonstrate the effectiveness of our method for goal-conditioned behavior generation. Demonstrations and Code are available at https://intuitive-robots.github.io/beso-website/",
    "authors": [
        "Moritz Reuss",
        "M. Li",
        "Xiaogang Jia",
        "Rudolf Lioutikov"
    ],
    "venue": "Robotics: Science and Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a new policy representation based on score-based diffusion models (SDMs) in the domain of Goal-Conditioned Imitation Learning (GCIL) and demonstrates how BESO can be used to learn a goal-independent policy from play-data using classifier-free guidance."
    },
    "citationCount": 60,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}