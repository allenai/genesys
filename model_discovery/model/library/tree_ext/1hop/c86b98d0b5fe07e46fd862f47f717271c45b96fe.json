{
    "acronym": "c86b98d0b5fe07e46fd862f47f717271c45b96fe",
    "title": "UniMem: Towards a Unified View of Long-Context Large Language Models",
    "seed_ids": [
        "poolingformer",
        "longformer",
        "rmt",
        "transformerxl",
        "longt5",
        "compresscontext",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "dbc368bc8b49347dd27679894524fa62f88492c9",
        "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "f75d05e759447c2aedb7097728f29f9a520d9bc1",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "c86b98d0b5fe07e46fd862f47f717271c45b96fe",
    "abstract": "Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms. Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower perplexity than baselines.",
    "authors": [
        "Junjie Fang",
        "Likai Tang",
        "Hongzhe Bi",
        "Yujia Qin",
        "Si Sun",
        "Zhenyu Li",
        "Haolun Li",
        "Yongjian Li",
        "Xin Cong",
        "Yukun Yan",
        "Xiaodong Shi",
        "Sen Song",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper reformulates 16 existing methods based on UniMem and analyzes four representative methods into equivalent UniMem forms to reveal their design principles and strengths, and proposes UniMix, an innovative approach that integrates the strengths of these algorithms."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}