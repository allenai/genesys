{
    "acronym": "2a7023e7d1dbd6ea0d98efd09a1f18d8599fe78f",
    "title": "PRIMER: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
    "seed_ids": [
        "longformer",
        "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
        "6aaec722a90eee0185d4bbfebbcd4f228ed1577f",
        "6e6a2fe517b33e1f29d761ae31fb37ddccb9a213",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "7cc730da554003dda77796d2cb4f06da5dfd5592",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2a7023e7d1dbd6ea0d98efd09a1f18d8599fe78f",
    "abstract": "Recently proposed pre-trained generation models achieve strong performance on single-document summarization benchmarks. However, most of them are pre-trained with general-purpose objectives and mainly aim to process single document inputs. In this paper, we propose PRIMER, a pre-trained model for multi-document representation with focus on summarization that reduces the need for dataset-speci\ufb01c architectures and large amounts of \ufb01ne-tuning labeled data. Specifically, we adopt the Longformer architecture with proper input transformation and global attention to \ufb01t for multi-document inputs, and we use Gap Sentence Generation objective with a new strategy to select salient sentences for the whole cluster, called Entity Pyramid, to teach the model to select and aggregate information across a cluster of related documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on the zero-shot, few-shot and full-supervised settings, our model, PRIMER, outperforms current state-of-the-art models on most of these settings with large margins. 1",
    "authors": [
        "Wen Xiao",
        "Iz Beltagy",
        "G. Carenini",
        "Arman Cohan"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A pre-trained model for multi-document representation with focus on summarization that reduces the need for dataset-speci\ufb01c architectures and large amounts of labeled data and outperforms current state-of-the-art models on most of these settings with large margins."
    },
    "citationCount": 29,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}