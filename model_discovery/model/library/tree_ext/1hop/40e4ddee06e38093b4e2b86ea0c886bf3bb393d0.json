{
    "acronym": "40e4ddee06e38093b4e2b86ea0c886bf3bb393d0",
    "title": "Story Visualization by Online Text Augmentation with Context Memory",
    "seed_ids": [
        "transformerxl",
        "a6238191cc43c41a544caadcf8b00947293cf099",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "dc29d6a74d9e583dfe9d613a5b885b45ca6cc4f0",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "70557ea6b65846fc30729ceed224acd4ac64ca5d"
    ],
    "s2id": "40e4ddee06e38093b4e2b86ea0c886bf3bb393d0",
    "abstract": "Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformer framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.",
    "authors": [
        "Daechul Ahn",
        "Daneul Kim",
        "Gwangmo Song",
        "Seung Wook Kim",
        "Honglak Lee",
        "Dongyeop Kang",
        "Jonghyun Choi"
    ],
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel memory architecture for the Bi-directional Transformer framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference is proposed."
    },
    "citationCount": 5,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}