{
    "acronym": "d931f84abfc4550c10ceb113b142c8eb3e07571e",
    "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training",
    "seed_ids": [
        "gpt",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "d931f84abfc4550c10ceb113b142c8eb3e07571e",
    "abstract": "Recent works have demonstrated great success in training large autoregressive language models (e.g., GPT-3) on unlabeled text corpus for text generation. To reduce their expensive training cost, practitioners attempt to increase the batch sizes and learning rates. However, increasing them often cause training instabilities and poor generalization. On the other side, using smaller batch sizes or learning rates would reduce the training ef\ufb01ciency, signi\ufb01cantly increasing training time and cost. We investigate this stability-ef\ufb01ciency dilemma and identify that long sequence length is one of the main causes of training instability in large-scale GPT model pre-training. Based on our analysis, we present a novel sequence length warmup method that simultaneously improves training stability and ef\ufb01ciency. As a kind of curriculum learning approach, our method improves the training convergence speed of autoregressive models. More importantly, our in-depth analysis shows that our method exerts a gradient variance reduction effect and regular-izes early stages of training where the amount of training data is much smaller than the model capacity. This enables stable training with much larger batch sizes and learning rates, further improving the training speed. Evaluations show that our approach enables stable GPT-2 (117M and 1.5B) pre-training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot WikiText-103/LAMBADA evaluation results, our approach reduces the required number of pre-training to-kens and wall clock time by up to 55% and 73%, respectively.",
    "authors": [
        "Conglong Li",
        "Minjia Zhang",
        "Yuxiong He"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents a novel sequence length warmup method that simultaneously improves training stability and ef\ufb01ciency and improves the training convergence speed of autoregressive models."
    },
    "citationCount": 25,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}