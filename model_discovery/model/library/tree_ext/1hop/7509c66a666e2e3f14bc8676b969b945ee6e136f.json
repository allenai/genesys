{
    "acronym": "7509c66a666e2e3f14bc8676b969b945ee6e136f",
    "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings",
    "seed_ids": [
        "transformerxl",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "7072db6eddb85ecd2c117365d91bd694760f726e",
        "94b69cf199fa0b6c842e17fe5d6174a9d161c3df",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "84476fdf6ead3553f4493dff8e02308439d6222b",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
        "dc35daba3fb34b2e6a5b12530badb7b799262bbf"
    ],
    "s2id": "7509c66a666e2e3f14bc8676b969b945ee6e136f",
    "abstract": "Without positional information, attention-based Transformer neural networks are permutation-invariant. Absolute or relative positional embeddings are the most popular ways to feed Transformer models with positional information. Absolute positional embeddings are simple to implement, but suffer from generalization issues when evaluating on sequences longer than seen at training time. Relative positions are more robust to input length change, but are more complex to implement and yield inferior model throughput due to extra computational and memory costs. In this paper, we propose an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute (simplicity and speed) and relative positional embeddings (better generalization). In addition, our empirical evaluation on state-of-the-art models in machine translation, image and speech recognition demonstrates that CAPE leads to better generalization performance as well as increased stability with respect to training hyper-parameters.",
    "authors": [
        "Tatiana Likhomanenko",
        "Qiantong Xu",
        "R. Collobert",
        "Gabriel Synnaeve",
        "A. Rogozhnikov"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute and relative positions and leads to better generalization performance as well as increased stability with respect to training hyper-parameters."
    },
    "citationCount": 42,
    "influentialCitationCount": 9,
    "code": null,
    "description": null,
    "url": null
}