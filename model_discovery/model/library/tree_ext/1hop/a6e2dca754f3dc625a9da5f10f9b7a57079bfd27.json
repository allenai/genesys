{
    "acronym": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
    "title": "MambaByte: Token-free Selective State Space Model",
    "seed_ids": [
        "mamba",
        "compressivetransformer",
        "roformer",
        "megabyte",
        "d53fe76bd2795a19ddf52d012917782f6f6f2c1e",
        "f1a9e0830bc36c048fa4659beaa62609869895b5",
        "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
        "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
        "412e266cddfd87c79087a88ba1e4d11b89a45a13",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "7cdebb73662387d9040da4f27a7dc04dbffa3c3e",
        "a128b1c47e6842605fb95bceae930d2135fc38fc",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "a30ac45ac5b7bd2148d3fb80ee7f3c29724e3170",
        "ca444821352a4bd91884413d8070446e2960715a",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "736eb449526fe7128917954ec5532b59e318ec78",
        "12809bcb734beafeb47876f42e7b438e27fe99fe",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "231e768f0cd280faa0f725bb353262cb4fed08d1",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
    "abstract": "Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.",
    "authors": [
        "Junxiong Wang",
        "Tushaar Gangavarapu",
        "Jing Nathan Yan",
        "Alexander M. Rush"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences, and develops an adaptation of speculative decoding with tokenized drafting and byte-level verification, establishing the viability of SSMs in enabling token-free language modeling."
    },
    "citationCount": 18,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}