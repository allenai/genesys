{
    "acronym": "29e3ff317a1a053939af736abd158cdd91577e4e",
    "title": "Reward modeling for mitigating toxicity in transformer-based language models",
    "seed_ids": [
        "gpt2",
        "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "29e3ff317a1a053939af736abd158cdd91577e4e",
    "abstract": null,
    "authors": [
        "Farshid Faal",
        "K. Schmitt",
        "Jia Yuan Yu"
    ],
    "venue": "Applied intelligence (Boston)",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The experiments demonstrate that the Reinforce-Detoxify method for language model detoxification outperforms existing detoxification approaches in automatic evaluation metrics, indicating that the approach is less prone to unintended bias toward social identities in generated content."
    },
    "citationCount": 21,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}