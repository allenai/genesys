{
    "acronym": "36697944858ab17ca23b23ae2043aa6c0b2e3d5d",
    "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
    "seed_ids": [
        "gpt",
        "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "0b0debb710366cdff461938c80763eace1651af6",
        "b0db25e317cf856f1ec1ca3df0e573d850ed4696",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "5735e49e501c8e51e9be4079592e46e047747b03",
        "d2fe7536b347a7039a241bb60d507880ada686e8",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "1487b51b327028576bc480120be96ef84efa6723",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "3df83a60f55c64b40e6dbcd99cf9f67894a0736e",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "36697944858ab17ca23b23ae2043aa6c0b2e3d5d",
    "abstract": "This paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency.",
    "authors": [
        "Kaiqiang Song",
        "Xiaoyang Wang",
        "Sangwoo Cho",
        "Xiaoman Pan",
        "Dong Yu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes a new model architecture, referred to as Zebra, that efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}