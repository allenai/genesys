{
    "acronym": "efe38b80b55c3ed3f2ecd81161cad7e918cec7c5",
    "title": "General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level",
    "seed_ids": [
        "bert",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "5e9c85235210b59a16bdd84b444a904ae271f7e7"
    ],
    "s2id": "efe38b80b55c3ed3f2ecd81161cad7e918cec7c5",
    "abstract": "The social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. Compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. In this paper, we propose an automatic multi-token debiasing pipeline called \\textbf{General Phrase Debiaser}, which is capable of mitigating phrase-level biases in masked language models. Specifically, our method consists of a \\textit{phrase filter stage} that generates stereotypical phrases from Wikipedia pages as well as a \\textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. The latter searches for prompts that trigger model's bias, and then uses them for debiasing. State-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and multiple disciplines, across models with varying parameter sizes.",
    "authors": [
        "Bingkang Shi",
        "Xiaodan Zhang",
        "Dehan Kong",
        "Yulei Wu",
        "Zongzhen Liu",
        "Honglei Lyu",
        "Longtao Huang"
    ],
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An automatic multi-token debiasing pipeline capable of mitigating phrase-level biases in masked language models is proposed, which can significantly reduce gender biases on both career and multiple disciplines, across models with varying parameter sizes."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}