{
    "acronym": "663dc434fabc1cf1d3e85fff3f7ddcd313035d18",
    "title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
    "seed_ids": [
        "gpt2",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "663dc434fabc1cf1d3e85fff3f7ddcd313035d18",
    "abstract": "Multivariate time-series forecasting is vital in various domains, e.g., economic planning and weather prediction. Deep train-from-scratch models have exhibited effective performance yet require large amounts of data, which limits real-world applicability. Recently, researchers have leveraged the representation learning transferability of pre-trained Large Language Models (LLMs) to handle limited non-linguistic datasets effectively. However, incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the \\textit{time-series alignment} stage to align LLMs with the nuances of time-series data, and the \\textit{forecasting fine-tuning} stage for downstream time-series forecasting tasks. Furthermore, our framework features a novel two-level aggregation method that integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to interpret time-specific information. In experiments across 7 time-series forecasting datasets, LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios, and also achieves an average improvement of 6.84% in MSE in few-shot scenarios. In addition, evaluations compared with different self-supervised learning approaches highlight LLM4TS's effectiveness with representation learning in forecasting tasks.",
    "authors": [
        "Ching Chang",
        "Wenjie Peng",
        "Tien-Fu Chen"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios, and also achieves an average improvement of 6.84% in MSE in few-shot scenarios."
    },
    "citationCount": 15,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}