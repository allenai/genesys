{
    "acronym": "8fc1dc137a19f9d79c504423b71cdfd9bf3eb832",
    "title": "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking",
    "seed_ids": [
        "gpt2",
        "77ced33cba86b4d01fbfe6622c8f564c89d6a1b3",
        "0e3d6a7c9c04cf3ba9c902724548846a5ade04b4",
        "7ade458d52d2dfe997b8a617a6b524bda12a619d",
        "e763fdc9ae56826ff799163ea035b29bffd8ea6f",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "8fc1dc137a19f9d79c504423b71cdfd9bf3eb832",
    "abstract": "In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented without adversarial training or architectural changes. We identify the SequenceMatch-$\\chi^2$ divergence as a more suitable training objective for autoregressive models which are used for generation. We show that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models and arithmetic.",
    "authors": [
        "Chris Cundy",
        "Stefano Ermon"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The SequenceMatch-$\\chi^2$ divergence is identified as a more suitable training objective for autoregressive models which are used for generation and it is shown that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models and arithmetic."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}