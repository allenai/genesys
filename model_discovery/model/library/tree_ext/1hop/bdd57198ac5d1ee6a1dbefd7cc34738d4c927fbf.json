{
    "acronym": "bdd57198ac5d1ee6a1dbefd7cc34738d4c927fbf",
    "title": "Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs",
    "seed_ids": [
        "gpt",
        "bb58f2f63888456a3e04a56a18996ab8dacdb257",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "bdd57198ac5d1ee6a1dbefd7cc34738d4c927fbf",
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.CCS CONCEPTS\u2022 Software and its engineering \u2192 Software testing and debug-ging; Empirical software validation.",
    "authors": [
        "Ziyu Li",
        "Donghwan Shin"
    ],
    "venue": "2024 IEEE/ACM 3rd International Conference on AI Engineering \u2013 Software Engineering for AI (CAIN)",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel method, called Mutation-based Consistency Testing (MCT), is proposed to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets."
    },
    "citationCount": 4,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}