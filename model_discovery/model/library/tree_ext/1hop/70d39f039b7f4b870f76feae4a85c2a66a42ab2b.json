{
    "acronym": "70d39f039b7f4b870f76feae4a85c2a66a42ab2b",
    "title": "Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting",
    "seed_ids": [
        "s4",
        "alibi",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "97833e2aa0da5240e62436373b58af988a4ab6ab",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "70d39f039b7f4b870f76feae4a85c2a66a42ab2b",
    "abstract": "Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electronic health records (EHRs). In ultra-long-term forecasting experiment, TimelyGPT achieves accurate extrapolation up to 6,000 timesteps of body temperature during the sleep stage transition given a short look-up window (i.e., prompt) containing only 2,000 timesteps. We further demonstrated TimelyGPT's forecasting capabilities on a preprocessed longitudinal healthcare administrative database called PopHR consisting of 489,000 patients randomly sampled from Montreal population. Together, we envision TimelyGPT to be useful in a broad spectrum of health domains including long-term patient health state forecasting and patient risk trajectory prediction.",
    "authors": [
        "Ziyang Song",
        "Qincheng Lu",
        "Hao Xu",
        "David L. Buckeridge",
        "Yue Li"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Timely Generative Pre-trained Transformer employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations and integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}