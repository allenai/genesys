{
    "acronym": "d488445bb2bf6719bc48a4d39bd906116274abda",
    "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models",
    "seed_ids": [
        "gpt2",
        "b8e57155bbcc1ce8a112482c85b3a3bb25f3fe52",
        "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
        "83ac79bb8e8695fb3c3c024be74790d862adea74",
        "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "d84cf745c534c010b8e55e5a4a04878906848dc3",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "863171ed35ca0035074f73bb202b153cc346f2f3",
        "8064d3873c646dc9ff949d72c54c634a906fc092",
        "defecf3dc299214f4cb76a093c6eed2297eaa46f",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
        "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "d488445bb2bf6719bc48a4d39bd906116274abda",
    "abstract": "Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To further exploit the general-purpose token transition and multi-step generation ability of large language models, we propose AutoTimes to repurpose LLMs as autoregressive time series forecasters, which independently projects time series segments into the embedding space and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability of the LLM size. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By adopting textual timestamps as position embeddings, AutoTimes integrates multimodality for multivariate scenarios. Empirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and over 5 times training/inference speedup compared to advanced LLM-based forecasters.",
    "authors": [
        "Yong Liu",
        "Guo Qin",
        "Xiangdong Huang",
        "Jianmin Wang",
        "Mingsheng Long"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes AutoTimes to repurpose LLMs as autoregressive time series forecasters, which independently projects time series segments into the embedding space and autoregressively generates future predictions with arbitrary lengths, and integrates multimodality for multivariate scenarios."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}