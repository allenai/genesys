{
    "acronym": "a7a62a32989afe90d514e744a436229724cc04a7",
    "title": "A Slice and Dice Approach to Accelerate Compound Sparse Attention on GPU",
    "seed_ids": [
        "bigbird",
        "longformer",
        "ec34748a74c84f67edde7cc763922fa6d4486022",
        "dca4d9abbc82e57dfa52f932e893d467a63e0682",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "9eb7860ae9777997ed17f6de623b0ab08cfc2df5",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "a7a62a32989afe90d514e744a436229724cc04a7",
    "abstract": "Transformer-based models are vital to various research domains, including NLP, computer vision, and recommendation systems. However, because the attention mechanism with quadratic complexity limits computation and memory footprint in long sequences, numerous sparse attention-based transformer models are proposed to alleviate these problems. To efficiently infer these models on GPUs, prior solutions such as Triton and Sputnik have accelerated sparse attention in the sparse transformers, including optimizations for sampled dense-dense matrix multiplication (SDDMM), sparse softmax (SpSoftmax), and sparse matrix-matrix multiplication (SpMM). Although the existing optimizations have achieved impressive improvement, they have shortcomings, including low data reuse, unnecessary computation, and unnecessary memory accesses in the latest sparse transformer models based on compound sparse attention.We propose Multigrain to accelerate sparse attention by applying compound sparse GPU kernels with multi-stream for compound sparse patterns. First, we take sparse patterns with high spatial locality and execute them on our customized coarse-grained kernels, which exploit data reuse and high-performance tensor core units. Second, we execute the sparse patterns with low spatial locality on the fine-grained kernels to reduce unnecessary computation and memory accesses. Last, we process the coarse-grained and fine-grained kernels in parallel using multi-stream for the SDDMM and SpMM. In the latest sparse transformer models such as Longformer and QDS-transformer, Multigrain achieved 2.07\u00d7 and 1.55\u00d7 end-to-end speedup, respectively, over Triton, which only uses the coarse-grained method. We also achieved 2.08\u00d7 and 1.08\u00d7 end-to-end speedup, respectively, compared to Sputnik, which only uses the fine-grained kernel.",
    "authors": [
        "Hailong Li",
        "Jaewan Choi",
        "Jung Ho Ahn"
    ],
    "venue": "IEEE International Symposium on Workload Characterization",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Multigrain is proposed to accelerate sparse attention by applying compound sparse GPU kernels with multi-stream for compound sparse patterns to reduce unnecessary computation and memory accesses in the latest sparse transformer models based on compound sparse attention."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}