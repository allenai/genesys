{
    "acronym": "d3f51870f4da5dd9c2a08a55cfa8a380b8d49208",
    "title": "Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation",
    "seed_ids": [
        "linformer",
        "lineartransformer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8"
    ],
    "s2id": "d3f51870f4da5dd9c2a08a55cfa8a380b8d49208",
    "abstract": "Self-attention networks (SANs) have been intensively applied for sequential recommenders, but they are limited due to: (1) the quadratic complexity and vulnerability to over-parameterization in self-attention; (2) inaccurate modeling of sequential relations between items due to the implicit position encoding. In this work, we propose the low-rank decomposed self-attention networks (LightSANs) to overcome these problems. Particularly, we introduce the low-rank decomposed self-attention, which projects user's historical items into a small constant number of latent interests and leverages item-to-interest interaction to generate the context-aware representation. It scales linearly w.r.t. the user's historical sequence length in terms of time and space, and is more resilient to over-parameterization. Besides, we design the decoupled position encoding, which models the sequential relations between items more precisely. Extensive experimental studies are carried out on three real-world datasets, where LightSANs outperform the existing SANs-based recommenders in terms of both effectiveness and efficiency.",
    "authors": [
        "Xinyan Fan",
        "Zheng Liu",
        "Jianxun Lian",
        "Wayne Xin Zhao",
        "Xing Xie",
        "Ji-Rong Wen"
    ],
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces the low-rank decomposed self-attention, which projects user's historical items into a small constant number of latent interests and leverages item-to-interest interaction to generate the context-aware representation and designs the decoupled position encoding, which models the sequential relations between items more precisely."
    },
    "citationCount": 82,
    "influentialCitationCount": 11,
    "code": null,
    "description": null,
    "url": null
}