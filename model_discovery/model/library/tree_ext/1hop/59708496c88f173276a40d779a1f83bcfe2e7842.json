{
    "acronym": "59708496c88f173276a40d779a1f83bcfe2e7842",
    "title": "RMT: Retentive Networks Meet Vision Transformers",
    "seed_ids": [
        "retnet",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "182e6c877155d7730378846380a3d1dc294fb54a",
        "bf6ce546c589fa8054b3972b266532664914bd21",
        "fa717a2e31f0cef4e26921f3b147a98644d2e64c",
        "ba637c4f1a170f1e2dadeadb71a63cf2b9a46de2",
        "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "59708496c88f173276a40d779a1f83bcfe2e7842",
    "abstract": "Retentive Network first emerged in the domain of NLP and immediately gained widespread attention due to its remarkable performance. A significant portion of its impressive capabilities stems from its explicit decay mechanism, which incorporates valuable prior knowledge. However, this explicit decay is unidirectional and one-dimensional, making it unsuitable for the bidirectional, two-dimensional modeling required in image-based tasks. To solve this, we propose a bidirectional, two-dimensional form of explicit decay specifically designed for vision models to introduce distance-related prior knowledge. Besides, unlike language models, the vision backbones use the same parallel form during training and inference. If this parallel form is replaced with recurrent or chunk-wise recurrent form, the parallelism of the model will be significantly disrupted, resulting in extremely slow inference speed. So we discard the two additional inference modes present in the original RetNet, retaining only the parallel form. Specifically, we incorporate bidirectional, two-dimensional explicit decay into the Self-Attention to form \\textbf{Re}tentive \\textbf{S}elf-\\textbf{A}ttention (ReSA). Furthermore, to reduce the complexity of global modeling, we decompose ReSA along the two axes of the image. Building upon ReSA, we construct RMT, a strong vision backbone. Abundant experiments have demonstrated that our RMT exhibits exceptional performance across various computer vision tasks. For example, RMT achieves \\textbf{84.1\\%} Top1-acc on ImageNet-1k using merely \\textbf{4.5G} FLOPs. To the best of our knowledge, among all models, RMT achieves the highest Top1-acc when models are of similar size and trained with the same strategy. Moreover, RMT significantly outperforms existing vision backbones in downstream tasks. Code will be released at https://github.com/qhfan/RMT.",
    "authors": [
        "Qihang Fan",
        "Huaibo Huang",
        "Mingrui Chen",
        "Hongmin Liu",
        "Ran He"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A bidirectional, two-dimensional form of explicit decay specifically designed for vision models to introduce distance-related prior knowledge is proposed, and is incorporated into the Self-Attention to form ReSA, a strong vision backbone."
    },
    "citationCount": 20,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}