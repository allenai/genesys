{
    "acronym": "5be7e6b04c5a240cff340034aae2b57c677e211f",
    "title": "A Survey on Efficient Inference for Large Language Models",
    "seed_ids": [
        "hyena",
        "hippo",
        "rwkv4",
        "streamingllm",
        "canmambaicl",
        "48c1bf8bab85f4d6a2490a4d3efc8b1fb4a2b261",
        "d2421cffac277e230cb97fc2355b32e03dd8bb1f",
        "7351898febca53d01453283c9b1a541b662e1ed3",
        "9da427202cc48370fd66359f5d72ff5ff3bc8b57",
        "a74a20be53e5767648b5970e30b2d81a9ba8293f",
        "f1a9e0830bc36c048fa4659beaa62609869895b5",
        "b085968c4362fb286ad6c5ef71a5db9630da0498",
        "a7e7fa7ca4a32a92f578adf07c613088fa89f5b0",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "3695739b3a8b0e92b8ae90081124d098ae33b15c",
        "745594bd0dc3e9dc86f74e100cd2c98ed36256c0",
        "47beae741f6a4470dbd286d4f3f05ba2031c52d2",
        "51fb6598a3ebe36b371b096b4824d718e6e527fb",
        "4d76206515d6b33903937474273885476fc2771e",
        "a8b995f0da78a79447dfb18c2337972b044f4239",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
        "5df422fc18974d687febd171adcac35b3012c50a",
        "1dede9d21db0be1c58208e1f970e57aac4fc45f8",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "e28f4687b9ddf562807d12d9799add07aa191d51",
        "e586a4591ba0303b769f2c07cbddaf1899cb72e4",
        "d2d0371158803df93a249c9f7237ffd79b875816",
        "0a067fab18c67d4a386efa846c080f8afff5e8f3",
        "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df",
        "d203c764fb5dec2b053be667c8b06e516ea6ef10",
        "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "e92a5332390f0ba94615935541da4da9bed56512",
        "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "a128b1c47e6842605fb95bceae930d2135fc38fc",
        "43014fc85c4860487336579ec98f509fec1803f7",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "b0c5c673c690c644a7d4af73adb783bd98486181",
        "240300b1da360f22bf0b82c6817eacebba6deed4",
        "22b58dce1a13382418b8372bbd50ed3b2533f899",
        "33be243ac9dd8723e6267dea45fd6a6172d4f6a5",
        "13270b9759cf0296b5a346fbb58b706e8ad0a982",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "ca444821352a4bd91884413d8070446e2960715a",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "436278ce3b85265dc5cface29e63c714fe979d23",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "d5e999aae76d5270ef272076979c809817458212",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "3d473cbb7a377cf960abff31748a1a39bb6c7d7c",
        "4b56eef2862f7f553686f1dd190c56017122a6a0",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5be7e6b04c5a240cff340034aae2b57c677e211f",
    "abstract": "Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.",
    "authors": [
        "Zixuan Zhou",
        "Xuefei Ning",
        "Ke Hong",
        "Tianyu Fu",
        "Jiaming Xu",
        "Shiyao Li",
        "Yuming Lou",
        "Luning Wang",
        "Zhihang Yuan",
        "Xiuhong Li",
        "Shengen Yan",
        "Guohao Dai",
        "Xiao-Ping Zhang",
        "Yuhan Dong",
        "Yu Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A comprehensive survey of the existing literature on efficient LLM inference is presented, analyzing the primary causes of the inefficient LLM inference and introducing a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization."
    },
    "citationCount": 12,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}