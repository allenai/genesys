{
    "acronym": "37b1ce339678f63315c82841c6824dd739269636",
    "title": "Length Generalization of Causal Transformers without Position Encoding",
    "seed_ids": [
        "compressivetransformer",
        "2b8439f319dfa73df62ca8957ff6d0c1f3c7a73c",
        "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b",
        "4ea5ca620122e6a9a2b000444d36491cebf49c7c",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "b31a5884a8ebe96b6300839b28608b97f8f8ef76",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "161bf3f0705ef8e088f53b383363338daac9af44",
        "68adb03744692247fb834406798894db9fe77010",
        "a2fc77f075f666b462d9350e7576f0ba9845c61b",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "f51497f463566581874c941353dd9d80069c5b77"
    ],
    "s2id": "37b1ce339678f63315c82841c6824dd739269636",
    "abstract": "Generalizing to longer sentences is important for recent Transformer-based language models. Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge. In this paper, we study the length generalization property of NoPE. We find that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length. We identify a connection between the failure of NoPE's generalization and the distraction of attention distributions. We propose a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE's context size. Experiments on long sequence language modeling, the synthetic passkey retrieval task and real-world long context tasks show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms. The source code is publicly accessible",
    "authors": [
        "Jie Wang",
        "Tao Ji",
        "Yuanbin Wu",
        "Hang Yan",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Xiaoling Wang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper finds that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length, and proposes a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE's context size."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}