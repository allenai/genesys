{
    "acronym": "c55ec6a805949c311983922cb1ec0f06cac0b9f3",
    "title": "Toward a deeper understanding: RetNet viewed through Convolution",
    "seed_ids": [
        "retnet",
        "59708496c88f173276a40d779a1f83bcfe2e7842",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "36b9d0f8610a82fd25854889d9327a04da4ff8fd",
        "2d98048c2d2fcd3f6b989d2a54003808906ab4b7",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "c55ec6a805949c311983922cb1ec0f06cac0b9f3",
    "abstract": null,
    "authors": [
        "Chenghao Li",
        "Chaoning Zhang"
    ],
    "venue": "Pattern Recognition",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work improves ViT's local modeling by applying a weight mask on the original self-attention matrix and proposes a novel Gaussian mixture mask (GMM) in which one mask only has two learnable parameters and it can be conveniently used in any ViT variants whose attention mechanism allows the use of masks."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}