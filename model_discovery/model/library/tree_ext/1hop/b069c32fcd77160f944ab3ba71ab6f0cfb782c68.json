{
    "acronym": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
    "title": "Focused Transformer: Contrastive Training for Context Scaling",
    "seed_ids": [
        "memorizingtrans",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1",
        "a2fc77f075f666b462d9350e7576f0ba9845c61b",
        "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "231e768f0cd280faa0f725bb353262cb4fed08d1",
        "f75d05e759447c2aedb7097728f29f9a520d9bc1",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68",
    "abstract": "Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-scale models to lengthen their effective context. This is demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The resulting models, which we name LongLLaMA, exhibit advancements in tasks requiring a long context. We further illustrate that our LongLLaMA models adeptly manage a $256 k$ context length for passkey retrieval.",
    "authors": [
        "Szymon Tworkowski",
        "Konrad Staniszewski",
        "Mikolaj Pacek",
        "Yuhuai Wu",
        "H. Michalewski",
        "Piotr Milo's"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Focused Transformer (FoT) is introduced, a technique that employs a training process inspired by contrastive learning that enhances the structure of the (key, value) space, enabling an extension of the context length."
    },
    "citationCount": 84,
    "influentialCitationCount": 7,
    "code": null,
    "description": null,
    "url": null
}