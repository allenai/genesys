{
    "acronym": "37f31bb5d9a8fd31085f9d69f528b0be4e8d724c",
    "title": "Javanese part-of-speech tagging using cross-lingual transfer learning",
    "seed_ids": [
        "bert",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "37f31bb5d9a8fd31085f9d69f528b0be4e8d724c",
    "abstract": "<span dir=\"ltr\">Large datasets that are publicly available for POS tagging do not always exist\u00a0</span><span dir=\"ltr\">for some languages.\u00a0</span><span dir=\"ltr\">One of those languages is Javanese, a local language in\u00a0</span><span dir=\"ltr\">Indonesia, which is considered as a low-resource language. This research aims\u00a0</span><span dir=\"ltr\">to examine the effectiveness of cross-lingual transfer learning for Javanese POS\u00a0</span><span dir=\"ltr\">tagging by fine-tuning the state-of-the-art Transformer-based models (such as\u00a0</span><span dir=\"ltr\">IndoBERT, mBERT, and XLM-RoBERTa) using different kinds of source lan</span><span dir=\"ltr\">guages that have a higher resource (such as Indonesian, English, Uyghur, Latin,\u00a0</span><span dir=\"ltr\">and Hungarian languages), and then fine-tuning it again using the Javanese lan</span><span dir=\"ltr\">guage as the target language. We found that the models using cross-lingual trans</span><span dir=\"ltr\">fer learning can increase the accuracy of the models without using cross-lingual\u00a0</span><span dir=\"ltr\">transfer learning by 14.3%\u201315.3% over LSTM-based models, and by 0.21%\u2013</span><span dir=\"ltr\">3.95% over Transformer-based models. Our results show that the most accurate\u00a0</span><span dir=\"ltr\">Javanese POS tagger model is XLM-RoBERTa that is fine-tuned in two stages\u00a0</span><span dir=\"ltr\">(the first one using Indonesian language as the source language, and the second\u00a0</span><span dir=\"ltr\">one using Javanese language as the target language), capable of achieving an\u00a0</span><span dir=\"ltr\">accuracy of 87.65%</span>",
    "authors": [
        "Gabriel Enrique",
        "Ika Alfina",
        "Evi Yulianti"
    ],
    "venue": "IAES International Journal of Artificial Intelligence (IJ-AI)",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The models using cross-lingual transfer learning can increase the accuracy of the models without using cross-lingual transfer learning by 14.3%\u201315.3% over LSTM-based models, and by 0.21%\u20133.95% over Transformer-based models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}