{
    "acronym": "e263e08a20080a2543d0ca29d3d63c4717a8beb6",
    "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning",
    "seed_ids": [
        "gpt3",
        "1f86bf1e334200ec0481349255559fbfe7a33caa",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "22312f763328cf540791de8c2449ea1e7436f476"
    ],
    "s2id": "e263e08a20080a2543d0ca29d3d63c4717a8beb6",
    "abstract": "Vision-language pre-training and instruction tuning have demonstrated general-purpose capabilities in 2D visual reasoning tasks by aligning visual encoders with state-of-the-art large language models (LLMs). In this paper, we introduce a simple, yet effective, cross-modality framework built atop frozen LLMs that allows the integration of various modalities without extensive modality-specific customization. To facilitate instruction-modality fine-tuning, we collect high-quality instruction tuning data in an automatic and scalable manner, composed of 24K QA samples for audio and 250K QA samples for 3D. Leveraging instruction-aware representations, our model performs comparably with leading-edge counterparts without the need of extensive modality-specific pre-training or customization. Furthermore, our approach demonstrates cross-modal reasoning abilities across two or more input modalities, despite each modality projection being trained individually. To study the model's cross-modal abilities, we contribute a novel Discriminative Cross-modal Reasoning (DisCRn) evaluation task, comprising 9K audio-video QA samples and 28K image-3D QA samples that require the model to reason discriminatively across disparate input modalities.",
    "authors": [
        "Artemis Panagopoulou",
        "Le Xue",
        "Ning Yu",
        "Junnan Li",
        "Dongxu Li",
        "Shafiq R. Joty",
        "Ran Xu",
        "Silvio Savarese",
        "Caiming Xiong",
        "Juan Carlos Niebles"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A simple, yet effective, cross-modality framework built atop frozen LLMs that allows the integration of various modalities without extensive modality-specific customization is introduced."
    },
    "citationCount": 17,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}