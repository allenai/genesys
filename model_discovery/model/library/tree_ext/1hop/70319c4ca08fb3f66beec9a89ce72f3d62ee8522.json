{
    "acronym": "70319c4ca08fb3f66beec9a89ce72f3d62ee8522",
    "title": "WeatherFormer: A Pretrained Encoder Model for Learning Robust Weather Representations from Small Datasets",
    "seed_ids": [
        "bert"
    ],
    "s2id": "70319c4ca08fb3f66beec9a89ce72f3d62ee8522",
    "abstract": "This paper introduces WeatherFormer, a transformer encoder-based model designed to learn robust weather features from minimal observations. It addresses the challenge of modeling complex weather dynamics from small datasets, a bottleneck for many prediction tasks in agriculture, epidemiology, and climate science. WeatherFormer was pretrained on a large pretraining dataset comprised of 39 years of satellite measurements across the Americas. With a novel pretraining task and fine-tuning, WeatherFormer achieves state-of-the-art performance in county-level soybean yield prediction and influenza forecasting. Technical innovations include a unique spatiotemporal encoding that captures geographical, annual, and seasonal variations, adapting the transformer architecture to continuous weather data, and a pretraining strategy to learn representations that are robust to missing weather features. This paper for the first time demonstrates the effectiveness of pretraining large transformer encoder models for weather-dependent applications across multiple domains.",
    "authors": [
        "Adib Hasan",
        "Mardavij Roozbehani",
        "M. Dahleh"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "WeatherFormer, a transformer encoder-based model designed to learn robust weather features from minimal observations, achieves state-of-the-art performance in county-level soybean yield prediction and influenza forecasting."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}