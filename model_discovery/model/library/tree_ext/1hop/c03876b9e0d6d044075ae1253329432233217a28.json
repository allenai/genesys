{
    "acronym": "c03876b9e0d6d044075ae1253329432233217a28",
    "title": "CageViT: Convolutional Activation Guided Efficient Vision Transformer",
    "seed_ids": [
        "transformerxl",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "5a9bc55f6332e38f62eb509b684147a1d4f10fd9",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "c03876b9e0d6d044075ae1253329432233217a28",
    "abstract": "Recently, Transformers have emerged as the go-to architecture for both vision and language modeling tasks, but their computational efficiency is limited by the length of the input sequence. To address this, several efficient variants of Transformers have been proposed to accelerate computation or reduce memory consumption while preserving performance. This paper presents an efficient vision Transformer, called CageViT, that is guided by convolutional activation to reduce computation. Our CageViT, unlike current Transformers, utilizes a new encoder to handle the rearranged tokens, bringing several technical contributions: 1) Convolutional activation is used to pre-process the token after patchifying the image to select and rearrange the major tokens and minor tokens, which substantially reduces the computation cost through an additional fusion layer. 2) Instead of using the class activation map of the convolutional model directly, we design a new weighted class activation to lower the model requirements. 3) To facilitate communication between major tokens and fusion tokens, Gated Linear SRA is proposed to further integrate fusion tokens into the attention mechanism. We perform a comprehensive validation of CageViT on the image classification challenge. Experimental results demonstrate that the proposed CageViT outperforms the most recent state-of-the-art backbones by a large margin in terms of efficiency, while maintaining a comparable level of accuracy (e.g. a moderate-sized 43.35M model trained solely on 224 x 224 ImageNet-1K can achieve Top-1 accuracy of 83.4% accuracy).",
    "authors": [
        "Hao Zheng",
        "Jinbao Wang",
        "Xiantong Zhen",
        "H. Chen",
        "Jingkuan Song",
        "Feng Zheng"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experimental results demonstrate that the proposed CageViT outperforms the most recent state-of-the-art backbones by a large margin in terms of efficiency, while maintaining a comparable level of accuracy."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}