{
    "acronym": "9a618cca0d2fc78db1be1aed70517401cb3f3859",
    "title": "Deep Equilibrium Models",
    "seed_ids": [
        "transformerxl"
    ],
    "s2id": "9a618cca0d2fc78db1be1aed70517401cb3f3859",
    "abstract": "We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective \u201cdepth\u201d of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq.",
    "authors": [
        "Shaojie Bai",
        "J. Z. Kolter",
        "V. Koltun"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that DEQs often improve performance over these state-of-the-art models (for similar parameter counts); have similar computational requirements to existing models; and vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in the authors' experiments."
    },
    "citationCount": 549,
    "influentialCitationCount": 93,
    "code": null,
    "description": null,
    "url": null
}