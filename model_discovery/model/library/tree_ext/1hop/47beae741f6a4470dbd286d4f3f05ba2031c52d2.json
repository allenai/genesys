{
    "acronym": "47beae741f6a4470dbd286d4f3f05ba2031c52d2",
    "title": "Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference",
    "seed_ids": [
        "gpt2",
        "bert",
        "0a6906bd6f026d3da3031c641ed03081bd0b574e",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "47beae741f6a4470dbd286d4f3f05ba2031c52d2",
    "abstract": "Recent advancements in large language models (LLMs) boasting billions of parameters have generated a significant demand for efficient deployment in inference workloads. While hardware accelerators for Transformer-based models have been extensively studied, the majority of existing approaches rely on temporal architectures that reuse hardware units for different network layers and operators. However, these methods often encounter challenges in achieving low latency due to considerable memory access overhead. This paper investigates the feasibility and potential of model-specific spatial acceleration for LLM inference on FPGAs. Our approach involves the specialization of distinct hardware units for specific operators or layers, facilitating direct communication between them through a dataflow architecture while minimizing off-chip memory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA. This model can be extended to multi-FPGA settings for distributed inference. Through our analysis, we can identify the most effective parallelization and buffering schemes for the accelerator and, crucially, determine the scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart. To enable more productive implementations of an LLM model on FPGAs, we further provide a library of high-level synthesis (HLS) kernels that are composable and reusable. This library will be made available as open-source. To validate the effectiveness of both our analytical model and HLS library, we have implemented BERT and GPT2 on an AMD Xilinx Alveo U280 FPGA device. Experimental results demonstrate our approach can achieve up to 13.4 \u00d7 speedup when compared to previous FPGA-based accelerators for the BERT model. For GPT generative inference, we attain a 2.2 \u00d7 speedup compared to DFX, an FPGA overlay, in the prefill stage, while achieving a 1.9 \u00d7 speedup and a 5.7 \u00d7 improvement in energy efficiency compared to the NVIDIA A100 GPU in the decode stage.",
    "authors": [
        "Hongzheng Chen",
        "Jiahao Zhang",
        "Yixiao Du",
        "Shaojie Xiang",
        "Zichao Yue",
        "Niansong Zhang",
        "Yaohui Cai",
        "Zhiru Zhang"
    ],
    "venue": "ACM Transactions on Reconfigurable Technology and Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA, and provides a library of high-level synthesis kernels that are composable and reusable."
    },
    "citationCount": 7,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}