{
    "acronym": "bc67a1d69d53bb6b5a4efa86721122f09532c97e",
    "title": "Progressive Self-Attention Network with Unsymmetrical Positional Encoding for Sequential Recommendation",
    "seed_ids": [
        "performer",
        "linformer",
        "d3f51870f4da5dd9c2a08a55cfa8a380b8d49208",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8"
    ],
    "s2id": "bc67a1d69d53bb6b5a4efa86721122f09532c97e",
    "abstract": "In real-world recommendation systems, the preferences of users are often affected by long-term constant interests and short-term temporal needs. The recently proposed Transformer-based models have proved superior in the sequential recommendation, modeling temporal dynamics globally via the remarkable self-attention mechanism. However, all equivalent item-item interactions in original self-attention are cumbersome, failing to capture the drifting of users' local preferences, which contain abundant short-term patterns. In this paper, we propose a novel interpretable convolutional self-attention, which efficiently captures both short- and long-term patterns with a progressive attention distribution. Specifically, a down-sampling convolution module is proposed to segment the overall long behavior sequence into a series of local subsequences. Accordingly, the segments are interacted with each item in the self-attention layer to produce locality-aware contextual representations, during which the quadratic complexity in original self-attention is reduced to nearly linear complexity. Moreover, to further enhance the robust feature learning in the context of Transformers, an unsymmetrical positional encoding strategy is carefully designed. Extensive experiments are carried out on real-world datasets, \\eg ML-1M, Amazon Books, and Yelp, indicating that the proposed method outperforms the state-of-the-art methods w.r.t. both effectiveness and efficiency.",
    "authors": [
        "Yuehua Zhu",
        "Bo Huang",
        "Shaohua Jiang",
        "Muli Yang",
        "Yanhua Yang",
        "Leon Wenliang Zhong"
    ],
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel interpretable convolutional self-attention is proposed, which efficiently captures both short- and long-term patterns with a progressive attention distribution, and outperforms the state-of-the-art methods w.r.t. both effectiveness and efficiency."
    },
    "citationCount": 6,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}