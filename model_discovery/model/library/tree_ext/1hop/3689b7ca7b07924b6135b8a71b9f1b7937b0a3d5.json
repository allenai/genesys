{
    "acronym": "3689b7ca7b07924b6135b8a71b9f1b7937b0a3d5",
    "title": "Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding",
    "seed_ids": [
        "transformerxl",
        "kerple",
        "receptivefieldana",
        "51fb6598a3ebe36b371b096b4824d718e6e527fb",
        "4ea5ca620122e6a9a2b000444d36491cebf49c7c",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "73290ecbec2f38d1d647ddef1ada69cee41725b3",
        "0b0debb710366cdff461938c80763eace1651af6",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
        "af385c0fdd0eda2bbf429bea6fedffc327c8a180",
        "5735e49e501c8e51e9be4079592e46e047747b03",
        "97833e2aa0da5240e62436373b58af988a4ab6ab",
        "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
        "7509c66a666e2e3f14bc8676b969b945ee6e136f",
        "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1",
        "84476fdf6ead3553f4493dff8e02308439d6222b",
        "dc48bc1a4d81e0f37603013fd2a95644dc233bd0",
        "dc35daba3fb34b2e6a5b12530badb7b799262bbf"
    ],
    "s2id": "3689b7ca7b07924b6135b8a71b9f1b7937b0a3d5",
    "abstract": "Transformer has taken the natural language processing (NLP) field by storm since birth, owing to its superior ability to model complex dependencies in sequences. Despite the great success of pretrained language models (PLMs) based on Transformer across almost all NLP tasks, they all suffer from a preset length limit and thus can hardly extend this success to longer sequences beyond seen data, namely the length extrapolation problem. Length extrapolation has aroused great interest among researchers, as it is the core feature of human language capacity. To enhance length extrapolation of Transformers, a plethora of methods have been proposed, mostly focusing on extrap-olatable position encodings. In this article, we provide an organized and systematical review of these research efforts in a unified notation from a position encoding perspective, aiming to enable the reader to gain a deep understanding of existing methods and provide stimuli for future research.",
    "authors": [
        "Liang Zhao",
        "Xiaocheng Feng",
        "Xiachong Feng",
        "Bing Qin",
        "Ting Liu"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An organized and systematical review of research efforts in a unified notation from a position encoding perspective is provided, aiming to enable the reader to gain a deep understanding of existing methods and provide stimuli for future research."
    },
    "citationCount": 10,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}