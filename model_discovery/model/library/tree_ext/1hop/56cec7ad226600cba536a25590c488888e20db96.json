{
    "acronym": "56cec7ad226600cba536a25590c488888e20db96",
    "title": "Sparse Modern Hopfield Networks",
    "seed_ids": [
        "hopfield",
        "abb79cc72fab35bfeb50585a121375b9bebafbb0",
        "06e40b7a703079c280f8f0886ac2bd984cd318ce",
        "f6390beca54411b06f3bde424fb983a451789733"
    ],
    "s2id": "56cec7ad226600cba536a25590c488888e20db96",
    "abstract": "Ramsauer et al. (2021) recently pointed out a connection between modern Hopfield networks and attention heads in transformers. In this paper, we extend their framework to a broader family of energy functions which can be written as a difference of a quadratic regularizer and a Fenchel-Young loss (Blondel et al., 2020), parametrized by a generalized negentropy function \u2126 . By working with Tsallis negentropies, the resulting update rules become end-to-end differentiable sparse transformations, establishing a new link to adaptively sparse transformers (Correia et al., 2019) and allowing for exact convergence to single memory patterns. Experiments on simulated data show a higher tendency to avoid metastable states.",
    "authors": [
        "Andr\u00e9 F. T. Martins",
        "Vlad Niculae",
        "Daniel McNamee"
    ],
    "venue": "",
    "year": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "By working with Tsallis negentropies, the resulting update rules become end-to-end differentiable sparse transformations, establishing a new link to adaptively sparse transformers and allowing for exact convergence to single memory patterns."
    },
    "citationCount": 3,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}