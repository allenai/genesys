{
    "acronym": "44aa16ed0f3e22c82b10fcc9d23459ecfb7fcdb3",
    "title": "Sequencer: Deep LSTM for Image Classification",
    "seed_ids": [
        "metaformer",
        "730a34374384f8abb886e464758b1a145edef938",
        "87e6f235c7a1fdeceb41605db64419fa11f7b98b",
        "485c08025157973bb52a935c6aa3bee74f990c01",
        "f75cddf2d42ed01b34686704eb3504becef67442",
        "71363797140647ebb3f540584de0a8758d2f7aa2",
        "9b6af0e358e76d22f209c75b1702c3e6ea7815b1",
        "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "44aa16ed0f3e22c82b10fcc9d23459ecfb7fcdb3",
    "abstract": "In recent computer vision research, the advent of the Vision Transformer (ViT) has rapidly revolutionized various architectural design efforts: ViT achieved state-of-the-art image classification performance using self-attention found in natural language processing, and MLP-Mixer achieved competitive performance using simple multi-layer perceptrons. In contrast, several studies have also suggested that carefully redesigned convolutional neural networks (CNNs) can achieve advanced performance comparable to ViT without resorting to these new ideas. Against this background, there is growing interest in what inductive bias is suitable for computer vision. Here we propose Sequencer, a novel and competitive architecture alternative to ViT that provides a new perspective on these issues. Unlike ViTs, Sequencer models long-range dependencies using LSTMs rather than self-attention layers. We also propose a two-dimensional version of Sequencer module, where an LSTM is decomposed into vertical and horizontal LSTMs to enhance performance. Despite its simplicity, several experiments demonstrate that Sequencer performs impressively well: Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only ImageNet-1K. Not only that, we show that it has good transferability and the robust resolution adaptability on double resolution-band.",
    "authors": [
        "Yuki Tatsunami",
        "M. Taki"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Sequencer, a novel and competitive architecture alternative to ViT that provides a new perspective on what inductive bias is suitable for computer vision, and proposes a two-dimensional version of Sequencer module, where an LSTM is decomposed into vertical and horizontal LSTMs to enhance performance."
    },
    "citationCount": 46,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}