{
    "acronym": "d8785264bbce47ca1ea7f97e7f3bc4ca6cbe824c",
    "title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation",
    "seed_ids": [
        "bert",
        "f1e56def812bc398d1b2b8c9a7ea6a623abd38e5",
        "4a6a65968a8eb8c09ffb57a7774ddabb596565b1",
        "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "75acc731bdd2b626edc74672a30da3bc51010ae8"
    ],
    "s2id": "d8785264bbce47ca1ea7f97e7f3bc4ca6cbe824c",
    "abstract": "Recent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.",
    "authors": [
        "Jarad Forristal",
        "Fatemehsadat Mireshghallah",
        "Greg Durrett",
        "Taylor Berg-Kirkpatrick"
    ],
    "venue": "Conference on Computational Natural Language Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel MH sampler is developed that proposes re-writes of the entire sequence in each step via iterative prompting of a large language model and allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}