{
    "acronym": "4c64c35198453662882afd63d618a832c7d8c1b2",
    "title": "PopMAG: Pop Music Accompaniment Generation",
    "seed_ids": [
        "transformerxl",
        "7a79099447bef9a3ea13b1dc409d04b3dff57320",
        "ce510c6cfeac703706460680e977c54554840830",
        "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "24a70db0bbb5f486126477e32a6a44ab917a4b11",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "fb507ada871d1e8c29e376dbf7b7879689aa89f9"
    ],
    "s2id": "4c64c35198453662882afd63d618a832c7d8c1b2",
    "abstract": "In pop music, accompaniments are usually played by multiple instruments (tracks) such as drum, bass, string and guitar, and can make a song more expressive and contagious by arranging together with its melody. Previous works usually generate multiple tracks separately and the music notes from different tracks not explicitly depend on each other, which hurts the harmony modeling. To improve harmony, in this paper, we propose a novel MUlti-track MIDI representation (MuMIDI), which enables simultaneous multi-track generation in a single sequence and explicitly models the dependency of the notes from different tracks. While this greatly improves harmony, unfortunately, it enlarges the sequence length and brings the new challenge of long-term music modeling. We further introduce two new techniques to address this challenge: 1) We model multiple note attributes (e.g., pitch, duration, velocity) of a musical note in one step instead of multiple steps, which can shorten the length of a MuMIDI sequence. 2) We introduce extra long-context as memory to capture long-term dependency in music. We call our system for pop music accompaniment generation as PopMAG. We evaluate PopMAG on multiple datasets (LMD, FreeMidi and CPMD, a private dataset of Chinese pop songs) with both subjective and objective metrics. The results demonstrate the effectiveness of PopMAG for multi-track harmony modeling and long-term context modeling. Specifically, PopMAG wins 42%/38%/40% votes when comparing with ground truth musical pieces on LMD, FreeMidi and CPMD datasets respectively and largely outperforms other state-of-the-art music accompaniment generation models and multi-track MIDI representations in terms of subjective and objective metrics.",
    "authors": [
        "Yi Ren",
        "Jinzheng He",
        "Xu Tan",
        "Tao Qin",
        "Zhou Zhao",
        "Tie-Yan Liu"
    ],
    "venue": "ACM Multimedia",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel MUlti-track MIDI representation (MuMIDI), which enables simultaneous multi-track generation in a single sequence and explicitly models the dependency of the notes from different tracks and is called PopMAG, which largely outperforms other state-of-the-art music accompaniment generation models and multi- track MIDI representations in terms of subjective and objective metrics."
    },
    "citationCount": 88,
    "influentialCitationCount": 8,
    "code": null,
    "description": null,
    "url": null
}