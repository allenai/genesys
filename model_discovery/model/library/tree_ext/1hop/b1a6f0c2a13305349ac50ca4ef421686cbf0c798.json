{
    "acronym": "b1a6f0c2a13305349ac50ca4ef421686cbf0c798",
    "title": "Gaitcotr: Improved Spatial-Temporal Representation for Gait Recognition with a Hybrid Convolution-Transformer Framework",
    "seed_ids": [
        "settransformer"
    ],
    "s2id": "b1a6f0c2a13305349ac50ca4ef421686cbf0c798",
    "abstract": "This work presents a novel hybrid convolution-transformer framework for gait recognition, termed GaitCoTr. The developed framework captures the appearance and short-term temporal features by convolution and extracts the long-term temporal features by transformer architecture, achieving a comprehensive spatial-temporal representation of gait. To unleash the potential of this hybrid framework for extracting richness and generalized temporal features, we propose a new variant of transformer tailored for gait, including temporally shifted tokenization, length-flexible position embedding, and inter-frame encoder. In addition, we introduce an auxiliary task\u2014view label prediction\u2014aiming to disentangle view from ID information. Extensive experimental results on two well-known gait benchmark datasets, CASIA-B and GREW, demonstrate the superior performance of the proposed Gait-CoTr.",
    "authors": [
        "Jingqi Li",
        "Yuzhen Zhang",
        "Hongming Shan",
        "Junping Zhang"
    ],
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces an auxiliary task\u2014view label prediction\u2014aiming to disentangle view from ID information, and proposes a new variant of transformer tailored for gait, including temporally shifted tokenization, length-flexible position embedding, and inter-frame encoder."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}