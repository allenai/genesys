{
    "acronym": "977351c92f156db27592e88b14dee2c22d4b312a",
    "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference",
    "seed_ids": [
        "linformer",
        "lineartransformer",
        "9b069ba5259d229bfd4fe3ac3768148e2d1092f8",
        "200ef1cde362aafbf598a2b5a1c5f35504ca2289",
        "2475b38a76a9c2dc67f74446e2e686815764b0f2",
        "ec139916edd6feb9b3cb3a0325ca96e21dbb0147",
        "dd1139cfc609c2f3263d02e97176d5275caebc0a",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "7d2a78a1f713b71c3a337247d042c5c2f0b2da84",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "977351c92f156db27592e88b14dee2c22d4b312a",
    "abstract": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.",
    "authors": [
        "Haoran You",
        "Yunyang Xiong",
        "Xiaoliang Dai",
        "Bichen Wu",
        "Peizhao Zhang",
        "Haoqi Fan",
        "P\u00e9ter Vajda",
        "Yingyan Lin"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear- angular attention during inference."
    },
    "citationCount": 14,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}