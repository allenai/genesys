{
    "acronym": "b8ca67cb22dd0fcd9c03ecfcd0c9e1f1695b886b",
    "title": "Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach",
    "seed_ids": [
        "gpt3",
        "68850153b0210615c86f9a72624f34e2913bcddf",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "b8ca67cb22dd0fcd9c03ecfcd0c9e1f1695b886b",
    "abstract": "The emergence of pre-trained models has significantly impacted Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta-features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta-features as a metric for evaluating pre-trained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and image models.",
    "authors": [
        "P. Aboagye",
        "Yan Zheng",
        "Junpeng Wang",
        "Uday Singh Saini",
        "Xin Dai",
        "Michael Yeh",
        "Yujie Fan",
        "Zhongfang Zhuang",
        "Shubham Jain",
        "Liang Wang",
        "Wei Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel approach is explored where the meta-features associated with each entity are used as a source of worldly knowledge and employ entity representations from the models and the consistency between these representations and the meta-features are used as a metric for evaluating pre-trained models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}