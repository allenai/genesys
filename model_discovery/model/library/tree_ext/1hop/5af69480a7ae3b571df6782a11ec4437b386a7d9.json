{
    "acronym": "5af69480a7ae3b571df6782a11ec4437b386a7d9",
    "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks",
    "seed_ids": [
        "reformer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "830995ef17cc291c13f42dfd9f462137de1d2179",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5af69480a7ae3b571df6782a11ec4437b386a7d9",
    "abstract": "The self-attention mechanism is rapidly emerging as one of the most important key primitives in neural networks (NNs) for its ability to identify the relations within input entities. The self-attention-oriented NN models such as Google Transformer and its variants have established the state-of-the-art on a very wide range of natural language processing tasks, and many other self-attention-oriented models are achieving competitive results in computer vision and recommender systems as well. Unfortunately, despite its great benefits, the self-attention mechanism is an expensive operation whose cost increases quadratically with the number of input entities that it processes, and thus accounts for a significant portion of the inference runtime. Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism. Specifically, based on the intuition that not all relations are equal, we devise a novel approximation scheme that significantly reduces the amount of computation by efficiently filtering out relations that are unlikely to affect the final output. With the specialized hardware for this approximate self-attention mechanism, ELSA achieves a geomean speedup of 58.1\u00d7 as well as over three orders of magnitude improvements in energy efficiency compared to GPU on self-attention computation in modern NN models while maintaining less than 1% loss in the accuracy metric.",
    "authors": [
        "Tae Jun Ham",
        "Yejin Lee",
        "Seong Hoon Seo",
        "Soo-Uck Kim",
        "Hyunji Choi",
        "Sungjun Jung",
        "Jae W. Lee"
    ],
    "venue": "International Symposium on Computer Architecture",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism."
    },
    "citationCount": 89,
    "influentialCitationCount": 15,
    "code": null,
    "description": null,
    "url": null
}