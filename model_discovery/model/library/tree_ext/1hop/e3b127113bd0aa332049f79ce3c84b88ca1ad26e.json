{
    "acronym": "e3b127113bd0aa332049f79ce3c84b88ca1ad26e",
    "title": "IFA: Interaction Fidelity Attention for Entire Lifelong Behaviour Sequence Modeling",
    "seed_ids": [
        "linformer",
        "lineartransformer",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2"
    ],
    "s2id": "e3b127113bd0aa332049f79ce3c84b88ca1ad26e",
    "abstract": "The lifelong user behavior sequence provides abundant information of user preference and gains impressive improvement in the recommendation task, however increases computational consumption significantly. To meet the severe latency requirement in online service, a short sub-sequence is sampled based on similarity to the target item. Unfortunately, items not in the sub-sequence are abandoned, leading to serious information loss. In this paper, we propose a new efficient paradigm to model the full lifelong sequence, which is named as \\textbf{I}nteraction \\textbf{F}idelity \\textbf{A}ttention (\\textbf{IFA}). In IFA, we input all target items in the candidate set into the model at once, and leverage linear transformer to reduce the time complexity of the cross attention between the candidate set and the sequence without any interaction information loss. We also additionally model the relationship of all target items for optimal set generation, and design loss function for better consistency of training and inference. We demonstrate the effectiveness and efficiency of our model by off-line and online experiments in the recommender system of Kuaishou.",
    "authors": [
        "Wenhui Yu",
        "Chao Feng",
        "Yanze Zhang",
        "Lantao Hu",
        "Peng Jiang",
        "Han Li"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new efficient paradigm to model the full lifelong sequence, which is named as IFA, where all target items in the candidate set are input into the model at once at once, and linear transformer is used to reduce the time complexity of the cross attention between the candidate set and the sequence without any interaction information loss."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}