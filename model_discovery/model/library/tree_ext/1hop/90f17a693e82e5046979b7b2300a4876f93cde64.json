{
    "acronym": "90f17a693e82e5046979b7b2300a4876f93cde64",
    "title": "Recasting Continual Learning as Sequence Modeling",
    "seed_ids": [
        "performer",
        "lineartransformer",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "90f17a693e82e5046979b7b2300a4876f93cde64",
    "abstract": "In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.",
    "authors": [
        "Soochan Lee",
        "Jaehyeon Son",
        "Gunhee Kim"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning, under the meta-continual learning (MCL) framework."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}