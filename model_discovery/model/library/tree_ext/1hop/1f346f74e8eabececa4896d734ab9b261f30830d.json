{
    "acronym": "1f346f74e8eabececa4896d734ab9b261f30830d",
    "title": "Modular Deep Learning",
    "seed_ids": [
        "lrt",
        "71ba5f845bd22d42003675b7cea970ca9e590bcc",
        "9953b1c969399026fa5050522752ff7d70710ccd",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c"
    ],
    "s2id": "1f346f74e8eabececa4896d734ab9b261f30830d",
    "abstract": "Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.",
    "authors": [
        "Jonas Pfeiffer",
        "Sebastian Ruder",
        "Ivan Vulic",
        "E. Ponti"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A survey of modular architectures is offered, providing a unified view over several threads of research that evolved independently in the scientific literature, and various additional purposes of modularity are explored, including scaling language models, causal inference, programme induction, and planning in reinforcement learning."
    },
    "citationCount": 46,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}