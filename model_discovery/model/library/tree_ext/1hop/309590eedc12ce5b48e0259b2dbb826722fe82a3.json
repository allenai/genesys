{
    "acronym": "309590eedc12ce5b48e0259b2dbb826722fe82a3",
    "title": "Backpack Language Models",
    "seed_ids": [
        "gpt2",
        "307d522c7bd7eafc21e67027b207ad9690243715",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "5a2263092f49540fd0e049050a96882ff29b00c3",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "309590eedc12ce5b48e0259b2dbb826722fe82a3",
    "abstract": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model\u2019s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM\u2019s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.",
    "authors": [
        "John Hewitt",
        "John Thickstun",
        "Christopher D. Manning",
        "Percy Liang"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents Backpacks, a new neural architecture that marries strong modeling performance with an interface for interpretability and control, and trains a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer."
    },
    "citationCount": 10,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}