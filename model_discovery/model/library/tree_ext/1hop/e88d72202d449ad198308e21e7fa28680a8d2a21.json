{
    "acronym": "e88d72202d449ad198308e21e7fa28680a8d2a21",
    "title": "EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data",
    "seed_ids": [
        "bert",
        "83edcfbb206ddad38a971d605da09390604248ea",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "f670f8e0ba95a0a39e7e7e1d08f6e839fc4b1093",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e88d72202d449ad198308e21e7fa28680a8d2a21",
    "abstract": "Large Language Models (LLMs) pre-trained on massive corpora have exhibited remarkable performance on various NLP tasks. However, applying these models to specific domains still poses significant challenges, such as lack of domain knowledge, limited capacity to leverage domain knowledge and inadequate adaptation to domain-specific data formats. Considering the exorbitant cost of training LLMs from scratch and the scarcity of annotated data within particular domains, in this work, we focus on domain-specific continual pre-training of LLMs using E-commerce domain as an exemplar. Specifically, we explore the impact of continual pre-training on LLMs employing unlabeled general and E-commercial corpora. Furthermore, we design a mixing strategy among different data sources to better leverage E-commercial semi-structured data. We construct multiple tasks to assess LLMs' few-shot In-context Learning ability and their zero-shot performance after instruction tuning in E-commerce domain. Experimental results demonstrate the effectiveness of continual pre-training of E-commerce LLMs and the efficacy of our devised data mixing strategy.",
    "authors": [
        "Shirong Ma",
        "Shen Huang",
        "Shulin Huang",
        "Xiaobin Wang",
        "Yangning Li",
        "Hai-Tao Zheng",
        "Pengjun Xie",
        "Fei Huang",
        "Yong Jiang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work explores the impact of continual pre-training on LLMs employing unlabeled general and E-commercial corpora, and designs a mixing strategy among different data sources to better leverage E-commercial semi-structured data."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}