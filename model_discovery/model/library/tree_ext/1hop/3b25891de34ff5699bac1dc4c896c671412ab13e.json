{
    "acronym": "3b25891de34ff5699bac1dc4c896c671412ab13e",
    "title": "A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation",
    "seed_ids": [
        "gpt2",
        "9c5609baff6175b0a2e436bb69e89737c4be3cf4",
        "ea4fa9e7155cf064de19862e41a6e63738770012",
        "d3a95b0727ef67ef9264cd20da10a545977ace51",
        "58b8da3821affc426895a85dbac5556322e6e2a9",
        "9aacdbc8b04fa63e6fe93f62a737a11c613f08fb",
        "a23f58609e7caecb68ec16d7a175b24e482b515d",
        "07a9f47885cae97efb7b4aa109392128532433da",
        "e7c698bdace380f7183dedbe657686f1885f615c",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "3b25891de34ff5699bac1dc4c896c671412ab13e",
    "abstract": "Despite recent advancements, existing story generation systems continue to encounter difficulties in effectively incorporating contextual and event features, which greatly influence the quality of generated narratives. To tackle these challenges, we introduce a novel neural generation model, EtriCA, that enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping. This feature capturing mechanism enables our model to exploit logical relationships between events more effectively during the story generation process. To further enhance our proposed model, we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This allows EtriCA to adapt to a wider range of data samples. This results in approximately 5\\% improvement in automatic metrics and over 10\\% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives.",
    "authors": [
        "Chen Tang",
        "Tyler Loakman",
        "Chenghua Lin"
    ],
    "venue": "Computer Speech and Language",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel neural generation model is introduced, EtriCA, that enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping, which enables the model to exploit logical relationships between events more effectively during the story generation process."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}