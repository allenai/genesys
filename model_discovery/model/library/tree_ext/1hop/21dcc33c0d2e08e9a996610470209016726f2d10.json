{
    "acronym": "21dcc33c0d2e08e9a996610470209016726f2d10",
    "title": "Using BERT Encoding and Sentence-Level Language Model for Sentence Ordering",
    "seed_ids": [
        "universaltrans"
    ],
    "s2id": "21dcc33c0d2e08e9a996610470209016726f2d10",
    "abstract": null,
    "authors": [
        "Melika Golestani",
        "S. Z. Razavi",
        "Zeinab Borhanifard",
        "Farnaz Tahmasebian",
        "H. Faili"
    ],
    "venue": "Workshop on Time-Delay Systems",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes an algorithm for sentence ordering in a corpus of short stories that uses a language model based on Universal Transformers (UT) that captures sentences' dependencies by employing an attention mechanism and improves the previous state-of-the-art in terms of Perfect Match Ratio (PMR) score."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}