{
    "acronym": "4375abf311b0ef51644551276153efaf11426916",
    "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
    "seed_ids": [
        "transformerxl",
        "streamingllm",
        "compresscontext",
        "5be7e6b04c5a240cff340034aae2b57c677e211f",
        "3fd5bc3077d04965eaa3498372c39bbdd09d55e4",
        "cf93c04b73d50ba097151ee3e2a9f47fda4a8525",
        "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6",
        "275b005c33a315ad603f236cd5766efe07ef6a54",
        "c9603ec967879c24973b5bd48861df2e5555932e",
        "ef1b02dc1b82f9955fc4760fcefd92c0fff9f227",
        "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d",
        "2e8ca21114ecefac88fd2b3a2daacae352b1907f",
        "23b09ed66024fdd04d6713b9ba621b866f033d20",
        "36697944858ab17ca23b23ae2043aa6c0b2e3d5d",
        "ade22704be8a0fc3730d320cc7934b2ccbcd97e4",
        "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
        "fdc53c2c10742464087c0525f77e32604827a21d",
        "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9",
        "73290ecbec2f38d1d647ddef1ada69cee41725b3",
        "a7fc585cc4c2b6822646b2c410e0c427a20798f2",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
        "68adb03744692247fb834406798894db9fe77010",
        "9b069ba5259d229bfd4fe3ac3768148e2d1092f8",
        "cbff35378657225ece138c33e6a23afb3b46b41f",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "d84b292c2e90d0b0edfedc33141d305d8e9de5df",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "b97c3c370401dc34d2adbeb24f34de5180a14be6",
        "16e623059ffccab60f4c35be028a2d4f10933515",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "67ee20536c30a225b86902af2f091e28e5e19b40",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "168fc3525f7b97695a97b04e257ee9bd1e832acb",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "f6390beca54411b06f3bde424fb983a451789733",
        "dc48bc1a4d81e0f37603013fd2a95644dc233bd0",
        "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8"
    ],
    "s2id": "4375abf311b0ef51644551276153efaf11426916",
    "abstract": "We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.",
    "authors": [
        "Pai Zeng",
        "Zhenyu Ning",
        "Jieru Zhao",
        "Weihao Cui",
        "Mengwei Xu",
        "Liwei Guo",
        "Xusheng Chen",
        "Yizhou Shan"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}