{
    "acronym": "dc8f507fc2fda1dad396f5fb11876e93fa35637f",
    "title": "EdgeMesh: A hybrid distributed training mechanism for heterogeneous edge devices",
    "seed_ids": [
        "gpt2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "dc8f507fc2fda1dad396f5fb11876e93fa35637f",
    "abstract": "The proliferation of large\u2010scale distributed Internet of Things (IoT) applications has resulted in a surge in demand for network models such as deep neural networks (DNNs) to be trained and inferred at the edge. Due to the central data transmission mechanism, heterogeneity of edge devices, and resource constraints, the existing single data\u2010parallel, and model\u2010parallel distributed training mechanisms frequently fail to fully utilize the computing power of edge devices, network topology and bandwidth resources. In light of the shortcomings mentioned earlier, this article proposes EdgeMesh, a hybrid parallel training mode based on the Mesh\u2010Tensorflow framework, consisting of an adaptive meshing strategy and a dynamic model convolutional partitioning strategy. The computing power of IoT edge devices significantly speeds up the DNN training process. In a resource\u2010constrained environment, each node only supports a subset of the model's parallel computing tasks, reducing communication and memory overhead while retaining high scalability. Experiments show that when compared to single\u2010machine training and data parallel mode, EdgeMesh distributed training mechanism can reduce the average delay by 3.2 times and average memory overhead by 43% while maintaining model accuracy. The computing power of IoT edge devices effectively accelerates the DNN training process.",
    "authors": [
        "Fei Xu",
        "Jianqiang Feng",
        "Zhuoya Zhang",
        "Linpeng Ning",
        "Yuxi Jia"
    ],
    "venue": "Transactions on Emerging Telecommunications Technologies",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This article proposes EdgeMesh, a hybrid parallel training mode based on the Mesh\u2010Tensorflow framework, consisting of an adaptive meshing strategy and a dynamic model convolutional partitioning strategy that can reduce the average delay by 3.2 times and average memory overhead by 43% while maintaining model accuracy."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}