{
    "acronym": "9afea202f08ed53db3b62bc9e37845bcf25203ae",
    "title": "SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces",
    "seed_ids": [
        "s4",
        "mamba",
        "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27",
        "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "d98b5c1d0f9a4e39dc79ea7a3f74e54789df5e13",
        "edd23cff90be3f27e50e74d7b24cd0ca92370bbd",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "31b27d90c2d5732386dd3a46a7d8336c0555c6fe",
        "498ac9b2e494601d20a3d0211c16acf2b7954a54",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "ca444821352a4bd91884413d8070446e2960715a",
        "9226ae23b95b3f6891461e086d910ffeb7ac448a",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "94bcd712aed610b8eaeccc57136d65ec988356f2",
        "2d9ae4c167510ed78803735fc57ea67c3cc55a35",
        "de18baa4964804cf471d85a5a090498242d2e79f",
        "7e9ff94476f41041c75e253e84f487db00e9c861"
    ],
    "s2id": "9afea202f08ed53db3b62bc9e37845bcf25203ae",
    "abstract": "Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64, 200, and 400. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.",
    "authors": [
        "Yuta Oshima",
        "Shohei Taniguchi",
        "Masahiro Suzuki",
        "Yutaka Matsuo"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a SSM-based model that can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}