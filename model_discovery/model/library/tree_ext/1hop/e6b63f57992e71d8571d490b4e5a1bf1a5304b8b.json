{
    "acronym": "e6b63f57992e71d8571d490b4e5a1bf1a5304b8b",
    "title": "Nebula: Self-Attention for Dynamic Malware Analysis",
    "seed_ids": [
        "gpt",
        "4bd1bdc264adb346ed16ebdaa2f92a1adfab5502"
    ],
    "s2id": "e6b63f57992e71d8571d490b4e5a1bf1a5304b8b",
    "abstract": "Dynamic analysis enables detecting Windows malware by executing programs in a controlled environment and logging their actions. Previous work has proposed training machine learning models, i.e., convolutional and long short-term memory networks, on homogeneous input features like runtime APIs to either detect or classify malware, neglecting other relevant information coming from heterogeneous data like network and file operations. To overcome these issues, we introduce Nebula, a versatile, self-attention Transformer-based neural architecture that generalizes across different behavioral representations and formats, combining diverse information from dynamic log reports. Nebula is composed by several components needed to tokenize, filter, normalize and encode data to feed the transformer architecture. We firstly perform a comprehensive ablation study to evaluate their impact on the performance of the whole system, highlighting which components can be used as-is, and which must be enriched with specific domain knowledge. We perform extensive experiments on both malware detection and classification tasks, using three datasets acquired from different dynamic analyses platforms, show that, on average, Nebula outperforms state-of-the-art models at low false positive rates, with a peak of 12% improvement. Moreover, we showcase how self-supervised learning pre-training matches the performance of fully-supervised models with only 20% of training data, and we inspect the output of Nebula through explainable AI techniques, pinpointing how attention is focusing on specific tokens correlated to malicious activities of malware families. To foster reproducibility, we open-source our findings and models at https://github.com/dtrizna/nebula.",
    "authors": [
        "Dmitrijs Trizna",
        "Luca Demetrio",
        "B. Biggio",
        "Fabio Roli"
    ],
    "venue": "IEEE Transactions on Information Forensics and Security",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Nebula is introduced, a versatile, self-attention Transformer-based neural architecture that generalizes across different behavioral representations and formats, combining diverse information from dynamic log reports and demonstrates how self-supervised learning pre-training matches the performance of fully-supervised models with only 20% of training data."
    },
    "citationCount": 3,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}