{
    "acronym": "c87b56ede2479fa809627a0ec89c97f5a1533ff9",
    "title": "Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics",
    "seed_ids": [
        "rfa",
        "hyperattention",
        "reformer",
        "93e58491830abe1eb965ab37ec64fa97263f6048",
        "240103933ffe3dac2179cc160a2bd91299357a53",
        "59b7448f816908cfb49a2ab5e63b2fa5786387f7",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "a9c214e846188adb645021cd7b1964b8ea1fef6f",
        "0d508600d77d8a7e6a655cdb6d139779732f649f",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "c0f709acf38eb27702b0fbce1215db0ebaa2de2b",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "34a4e6818d680875ff0bef9a76de0376118446d1"
    ],
    "s2id": "c87b56ede2479fa809627a0ec89c97f5a1533ff9",
    "abstract": "This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR&AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (HEPT), which combines E$^2$LSH with OR&AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance on two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at https://github.com/Graph-COM/HEPT.",
    "authors": [
        "Siqi Miao",
        "Zhiyuan Lu",
        "Mia Liu",
        "Javier Duarte",
        "Pan Li"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics, and proposes LSH-based Efficient Point Transformer (HEPT), which combines E$^2$LSH with OR&AND constructions and is built upon regular computations."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}