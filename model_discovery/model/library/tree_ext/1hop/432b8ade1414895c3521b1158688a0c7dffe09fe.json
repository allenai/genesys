{
    "acronym": "432b8ade1414895c3521b1158688a0c7dffe09fe",
    "title": "Sparse Attention-Based Neural Networks for Code Classification",
    "seed_ids": [
        "sparsetransformer",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8"
    ],
    "s2id": "432b8ade1414895c3521b1158688a0c7dffe09fe",
    "abstract": "Categorizing source codes accurately and efficiently is a challenging problem in real-world programming education platform management. In recent years, model-based approaches utilizing abstract syntax trees (ASTs) have been widely applied to code classification tasks. Many of these methods start by parsing source code into abstract syntax trees, followed by encoding them as sequences for input into temporal models or Transformer models for subsequent processing. However, existing methods suffer from issues such as loss of structural information and slow training speeds. In response to these challenges, we introduce an approach named the Sparse Attention-based neural network for Code Classification (SACC) in this paper. The approach involves two main steps: In the first step, source code undergoes syntax parsing and preprocessing. The generated abstract syntax tree is split into sequences of subtrees and then encoded using a recursive neural network to obtain a high-dimensional representation. This step simultaneously considers both the logical structure and lexical level information contained within the code. In the second step, the encoded sequences of subtrees are fed into a Transformer model that incorporates sparse attention mechanisms for the purpose of classification. This method efficiently reduces the computational cost of the self-attention mechanisms, thus improving the training speed while preserving effectiveness. Our work introduces a carefully designed sparse attention pattern that is specifically designed to meet the unique needs of code classification tasks. This design helps reduce the influence of redundant information and enhances the overall performance of the model. Finally, we also deal with problems in previous related research, which include issues like incomplete classification labels and a small dataset size. We annotated the CodeNet dataset with algorithm-related labeling categories, which contains a significantly large amount of data. Extensive comparative experimental results demonstrate the effectiveness and efficiency of SACC for the code classification tasks.",
    "authors": [
        "Ziyang Xiang",
        "Zaixin Zhang",
        "Qi Liu"
    ],
    "venue": "2023 3rd International Conference on Digital Society and Intelligent Systems (DSInS)",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a carefully designed sparse attention pattern that is specifically designed to meet the unique needs of code classification tasks, and helps reduce the influence of redundant information and enhances the overall performance of the model."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}