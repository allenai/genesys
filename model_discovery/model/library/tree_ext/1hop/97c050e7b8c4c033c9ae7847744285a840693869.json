{
    "acronym": "97c050e7b8c4c033c9ae7847744285a840693869",
    "title": "Transformer-XL Architecture For Question Answering",
    "seed_ids": [
        "transformerxl"
    ],
    "s2id": "97c050e7b8c4c033c9ae7847744285a840693869",
    "abstract": "In this project, we aim to incorporate the Transformer-XL model[1] into the SQuAD QA system and determine if the model is better performing than vanilla transformer in QA tasks. To approach this final goal of our project, we completed two intermediate goals: the first goal was to add character embedding to the BiDAF model using character-level convnets, as described in[2]. The second goal we completed was to adapt the transformer model and apply to the QA model. This method was previously implemented in QANet[3], which we used as a reference in developing our QANet. Because the Transformer-XL model uses segment-level recursion and is mainly developed for text generation tasks, we point out some possible limitations to apply such model to reading comprehension tasks, and that it might not be an improvement over QANet. The results for the Transformer-XL model were F1: 66.13, EM: 63.06 whereas the QANet scored F1: 67.92, EM: 64.31. Because of different model configurations and less training time for the Transformer-XL model, it can only be concluded that Transformer-XL had similar performance to vanilla transformer QANet for the RC task. Lastly, some error modes in the model prediction are discussed.",
    "authors": [
        "Chenkai Mao",
        "Qinghong Zheng",
        "\u2022. Mentor",
        "Kendrick Shen"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It can only be concluded that Transformer-XL had similar performance to vanilla transformer QANet for the RC task, and some error modes in the model prediction are discussed."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}