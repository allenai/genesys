{
    "acronym": "54ce4ab5dcb7099bda3a1ea2f004f54331479a31",
    "title": "Iterative Evidence Searching over Long Structured Documents for Question Answering",
    "seed_ids": [
        "etc",
        "af38829cdb55ee7b71d49399f71397d975e40a95",
        "395aae6e7a79e5760457ca38e868acc970016230",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "93d3e45395117e21214d404c8753b578c29266d1",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "01b15017ac59b8d6f2ce3598c4a7d6358c211426"
    ],
    "s2id": "54ce4ab5dcb7099bda3a1ea2f004f54331479a31",
    "abstract": "We propose a simple yet effective model, D OC - 001 H OPPER , for selecting evidence from long 002 structured documents to answer complex ques-003 tions. Similar to multi-hop question-answering 004 (QA) systems, at each step, D OC H OPPER it-005 eratively uses a query q to extract information 006 from a document, and combines this informa-007 tion with q to produce the next query. How-008 ever, in contrast to most previous multi-hop 009 QA systems, D OC H OPPER is able to extract 010 either short or long sections of the document, 011 thus emulating a multi-step process of \u201cnavi-012 gating\u201d through a long document to answer a 013 question. To enable this novel behavior, D OC - 014 H OPPER does not combine document informa-015 tion with q by concatenating text to the text of 016 q , but by combining a compact neural represen-017 tation of q with a compact neural representation 018 of a (potentially large) hierarchical part of the 019 document. We evaluate D OC H OPPER on three 020 different tasks that require reading long struc-021 tured documents and finding multiple pieces of 022 evidence, and show D OC H OPPER outperforms 023 Transformer models for plain text input. Addi-024 tionally, D OC H OPPER is efficient at inference 025 time, being 10\u2013250 times faster than baselines. 026",
    "authors": [
        "Arman Cohan"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "D OC H OPPER is evaluated on three different tasks that require reading long struc-021 tured documents and finding multiple pieces of 022 evidence, and shows D OC H OPPER outperforms 023 Transformer models for plain text input."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}