{
    "acronym": "56c20ad9e73a46fb7f2126a1f23f68624b813973",
    "title": "Evaluating Decision Transformer Architecture on Robot Learning Tasks Erkl\u00e4rung zur Abschlussarbeit gem\u00e4\u00df \u00a722 Abs. 7 und \u00a723 Abs. 7 APB der TU Darmstadt Abbreviations, Symbols and Operators",
    "seed_ids": [
        "gpt",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "56c20ad9e73a46fb7f2126a1f23f68624b813973",
    "abstract": "Decision Transformer (DT) is a recently proposed architecture for Reinforcement Learning (RL) that frames RL as an auto-regressive sequence modeling problem and uses a Transformer model to predict the next action in a sequence of states, actions and rewards. Despite of the appealing performance of DT in the original paper, our empirical evaluations of DT on real-time continuous control tasks show two issues of the original DT architecture: the DT architecture yields bad performances in fine-grained stabilization tasks around unstable equilibriums, and long response times are necessary to generate actions, which precludes using DT for real-world tasks. To address these issues, we propose an extension of DT, called Decision LSTM (DLSTM), an architecture that replaces the Transformer model inside DT by an Long Short-Term Memory Network (LSTM) architecture. We evaluate the performance of DT, DLSTM and a Behavioral Cloning (BC) architecture in offline RL stabilization tasks, as well as their real-time capabilities in a real-world pendulum setting. Empirically, we show that DLSTM outperforms both BC and DT and achieves expert level in the stabilization tasks. We conclude that framing RL as a sequence modeling problem in principle enables solving the fine-grained stabilization tasks, as demonstrated by the good performances of DLSTM. Also, significant improvements regarding faster response times can be achieved by using DLSTM instead of DT. However, the performance of these models proves to be highly dependent on the sequence modeling architectures that are used to make predictions.",
    "authors": [
        "Jan Peters"
    ],
    "venue": "",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An extension of DT is proposed, called Decision LSTM (DLSTM), an architecture that replaces the Transformer model inside DT by an Long Short-Term Memory Network (LSTM) architecture, which shows that DLSTM outperforms both BC and DT and achieves expert level in the stabilization tasks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}