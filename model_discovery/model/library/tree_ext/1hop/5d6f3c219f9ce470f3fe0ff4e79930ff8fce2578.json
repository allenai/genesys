{
    "acronym": "5d6f3c219f9ce470f3fe0ff4e79930ff8fce2578",
    "title": "SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models",
    "seed_ids": [
        "gmlp",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5d6f3c219f9ce470f3fe0ff4e79930ff8fce2578",
    "abstract": "Despite the proven effectiveness of Transformer neural networks across multiple domains, their performance with Electronic Health Records (EHR) can be nuanced. The unique, multidimensional sequential nature of EHR data can sometimes make even simple linear models with carefully engineered features more competitive. Thus, the advantages of Transformers, such as efficient transfer learning and improved scalability are not always fully exploited in EHR applications. Addressing these challenges, we introduce SANSformer, an attention-free sequential model designed with specific inductive biases to cater for the unique characteristics of EHR data. In this work, we aim to forecast the demand for healthcare services, by predicting the number of patient visits to healthcare facilities. The challenge amplifies when dealing with divergent patient subgroups, like those with rare diseases, which are characterized by unique health trajectories and are typically smaller in size. To address this, we employ a self-supervised pretraining strategy, Generative Summary Pretraining (GSP), which predicts future summary statistics based on past health records of a patient. Our models are pretrained on a health registry of nearly one million patients, then fine-tuned for specific subgroup prediction tasks, showcasing the potential to handle the multifaceted nature of EHR data. In evaluation, SANSformer consistently surpasses robust EHR baselines, with our GSP pretraining method notably amplifying model performance, particularly within smaller patient subgroups. Our results illuminate the promising potential of tailored attention-free models and self-supervised pretraining in refining healthcare utilization predictions across various patient demographics.",
    "authors": [
        "Yogesh Kumar",
        "A. Ilin",
        "H. Salo",
        "S. Kulathinal",
        "M. Leinonen",
        "P. Marttinen"
    ],
    "venue": "",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "SANSformer, an attention-free sequential model designed with specific inductive biases to cater for the unique characteristics of EHR data, is introduced, aiming to forecast the demand for healthcare services, by predicting the number of patient visits to healthcare facilities."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}