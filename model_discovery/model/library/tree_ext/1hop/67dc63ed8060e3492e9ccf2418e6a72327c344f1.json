{
    "acronym": "67dc63ed8060e3492e9ccf2418e6a72327c344f1",
    "title": "Online Continual Knowledge Learning for Language Models",
    "seed_ids": [
        "gpt2",
        "465471bb5bf1a945549d6291c2d23367966b4957",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "67dc63ed8060e3492e9ccf2418e6a72327c344f1",
    "abstract": "Large Language Models (LLMs) serve as repositories of extensive world knowledge, enabling them to perform tasks such as question-answering and fact-checking. However, this knowledge can become obsolete as global contexts change. In this paper, we introduce a novel problem in the realm of continual learning: Online Continual Knowledge Learning (OCKL). This problem formulation aims to manage the dynamic nature of world knowledge in LMs under real-time constraints. We propose a new benchmark and evaluation metric designed to measure both the rate of new knowledge acquisition and the retention of previously learned knowledge. Our empirical evaluation, conducted using a variety of state-of-the-art methods, establishes robust base-lines for OCKL. Our results reveal that existing continual learning approaches are unfortunately insufficient for tackling the unique challenges posed by OCKL. We identify key factors that influence the trade-off between knowledge acquisition and retention, thereby advancing our understanding of how to train LMs in a continually evolving environment.",
    "authors": [
        "Yuhao Wu",
        "Tongjun Shi",
        "Karthick Sharma",
        "Chun Seah",
        "Shuhao Zhang"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel problem in the realm of continual learning: Online Continual Knowledge Learning (OCKL), which aims to manage the dynamic nature of world knowledge in LMs under real-time constraints and proposes a new benchmark and evaluation metric designed to measure both the rate of new knowledge acquisition and the retention of previously learned knowledge."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}