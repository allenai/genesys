{
    "acronym": "80a56c659dd1ab8675f01560ac5757a7f872ffa2",
    "title": "ParNet: Position-aware Aggregated Relation Network for Image-Text matching",
    "seed_ids": [
        "universaltrans"
    ],
    "s2id": "80a56c659dd1ab8675f01560ac5757a7f872ffa2",
    "abstract": "Exploring fine-grained relationship between entities(e.g. objects in image or words in sentence) has great contribution to understand multimedia content precisely. Previous attention mechanism employed in image-text matching either takes multiple self attention steps to gather correspondences or uses image objects (or words) as context to infer image-text similarity. However, they only take advantage of semantic information without considering that objects' relative position also contributes to image understanding. To this end, we introduce a novel position-aware relation module to model both the semantic and spatial relationship simultaneously for image-text matching in this paper. Given an image, our method utilizes the location of different objects to capture spatial relationship innovatively. With the combination of semantic and spatial relationship, it's easier to understand the content of different modalities (images and sentences) and capture fine-grained latent correspondences of image-text pairs. Besides, we employ a two-step aggregated relation module to capture interpretable alignment of image-text pairs. The first step, we call it intra-modal relation mechanism, in which we computes responses between different objects in an image or different words in a sentence separately; The second step, we call it inter-modal relation mechanism, in which the query plays a role of textual context to refine the relationship among object proposals in an image. In this way, our position-aware aggregated relation network (ParNet) not only knows which entities are relevant by attending on different objects (words) adaptively, but also adjust the inter-modal correspondence according to the latent alignments according to query's content. Our approach achieves the state-of-the-art results on MS-COCO dataset.",
    "authors": [
        "Yaxian Xia",
        "Lun Huang",
        "Wenmin Wang",
        "Xiao-Yong Wei",
        "Jie Chen"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel position-aware relation module is introduced to model both the semantic and spatial relationship simultaneously for image-text matching simultaneously in this paper and achieves the state-of-the-art results on MS-COCO dataset."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}