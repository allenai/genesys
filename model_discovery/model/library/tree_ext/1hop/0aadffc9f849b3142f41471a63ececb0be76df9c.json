{
    "acronym": "0aadffc9f849b3142f41471a63ececb0be76df9c",
    "title": "Do Vision-Language Models Understand Compound Nouns?",
    "seed_ids": [
        "gpt2",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "0aadffc9f849b3142f41471a63ececb0be76df9c",
    "abstract": "Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., *lab coat*) as well as they understand nouns (e.g., *lab*)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs\u2019 limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun. Code and benchmark are available.",
    "authors": [
        "Sonal Kumar",
        "Sreyan Ghosh",
        "S. Sakshi",
        "Utkarsh Tyagi",
        "Dinesh Manocha"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs, and proposes an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}