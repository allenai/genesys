{
    "acronym": "c39925ddfb326ead2623d4cf7c5753784ba590a5",
    "title": "Understanding Long Programming Languages with Structure-Aware Sparse Attention",
    "seed_ids": [
        "bigbird",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28"
    ],
    "s2id": "c39925ddfb326ead2623d4cf7c5753784ba590a5",
    "abstract": "Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have achieved great success in many downstream code-related tasks. Since the memory and computational complexity of self-attention in the Transformer grow quadratically with the sequence length, PPLMs typically limit the code length to 512. However, codes in real-world applications are generally long, such as code searches, which cannot be processed efficiently by existing PPLMs. To solve this problem, in this paper, we present SASA, a Structure-Aware Sparse Attention mechanism, which reduces the complexity and improves performance for long code understanding tasks. The key components in SASA are top-k sparse attention and Abstract Syntax Tree (AST)-based structure-aware attention. With top-k sparse attention, the most crucial attention relation can be obtained with a lower computational cost. As the code structure represents the logic of the code statements, which is a complement to the code sequence characteristics, we further introduce AST structures into attention. Extensive experiments on CodeXGLUE tasks show that SASA achieves better performance than the competing baselines.",
    "authors": [
        "Tingting Liu",
        "Chengyu Wang",
        "Cen Chen",
        "Ming Gao",
        "Aoying Zhou"
    ],
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper presents SASA, a Structure-Aware Sparse Attention mechanism, which reduces the complexity and improves performance for long code understanding tasks, and introduces AST structures into attention."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}