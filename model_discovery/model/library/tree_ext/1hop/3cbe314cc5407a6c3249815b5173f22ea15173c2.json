{
    "acronym": "3cbe314cc5407a6c3249815b5173f22ea15173c2",
    "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding",
    "seed_ids": [
        "linformer",
        "longformer",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "2e1db8cb373f4d4a51d44308b7a457886d855fbb",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "3cbe314cc5407a6c3249815b5173f22ea15173c2",
    "abstract": "This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which significantly enhances the ViT of [12] for encoding high-resolution images using two techniques. The first is the multi-scale model structure, which provides image encodings at multiple scales with manageable computational cost. The second is the attention mechanism of Vision Long-former, which is a variant of Longformer [3], originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work [47], on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at https://github.com/microsoft/vision-longformer.",
    "authors": [
        "Pengchuan Zhang",
        "Xiyang Dai",
        "Jianwei Yang",
        "Bin Xiao",
        "Lu Yuan",
        "Lei Zhang",
        "Jianfeng Gao"
    ],
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work, on a range of vision tasks, including image classification, object detection, and segmentation."
    },
    "citationCount": 275,
    "influentialCitationCount": 20,
    "code": null,
    "description": null,
    "url": null
}