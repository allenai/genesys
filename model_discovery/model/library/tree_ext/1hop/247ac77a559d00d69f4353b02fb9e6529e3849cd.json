{
    "acronym": "247ac77a559d00d69f4353b02fb9e6529e3849cd",
    "title": "Adapting Pretrained Text-to-Text Models for Long Text Sequences",
    "seed_ids": [
        "bigbird",
        "longformer",
        "longt5",
        "3b39efe6c91ae432dd35bb79431edb8a6719f906",
        "6edccbd83a9aae204785d4821f97855677c33866",
        "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
        "e96493b4181de6c60b761dc66492db8e66fd784f",
        "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "13850aabbe8b70e99560a42504773a76ab5a4f47",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "0a41cb292242a82b2b09b3bf23b48349b981a640",
        "ac95a18762133d4065ac8af518c33084d83c5582",
        "e32a12b14e212506115cc6804667b3d8297917e1",
        "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1"
    ],
    "s2id": "247ac77a559d00d69f4353b02fb9e6529e3849cd",
    "abstract": "We present an empirical study of adapting an existing pretrained text-to-text model for long-sequence inputs. Through a comprehensive study along three axes of the pretraining pipeline -- model architecture, optimization objective, and pretraining corpus, we propose an effective recipe to build long-context models from existing short-context models. Specifically, we replace the full attention in transformers with pooling-augmented blockwise attention, and pretrain the model with a masked-span prediction task with spans of varying length. In terms of the pretraining corpus, we find that using randomly concatenated short-documents from a large open-domain corpus results in better performance than using existing long document corpora which are typically limited in their domain coverage. With these findings, we build a long-context model that achieves competitive performance on long-text QA tasks and establishes the new state of the art on five long-text summarization datasets, often outperforming previous methods with larger model sizes. Our code has been released at https://github.com/facebookresearch/bart_ls.",
    "authors": [
        "Wenhan Xiong",
        "Anchit Gupta",
        "Shubham Toshniwal",
        "Yashar Mehdad",
        "Wen-tau Yih"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A long-context model is built that achieves competitive performance on long-text QA tasks and establishes the new state of the art on five long- Text-to-text summarization datasets, often outperforming previous methods with larger model sizes."
    },
    "citationCount": 25,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}