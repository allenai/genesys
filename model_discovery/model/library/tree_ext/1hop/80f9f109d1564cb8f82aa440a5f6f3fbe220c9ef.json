{
    "acronym": "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
    "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding",
    "seed_ids": [
        "gpt",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
    "abstract": "Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.",
    "authors": [
        "Yu Sun",
        "Shuohuan Wang",
        "Yukun Li",
        "Shikun Feng",
        "Hao Tian",
        "Hua Wu",
        "Haifeng Wang"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A continual pre-training framework named ERNIE 2.0 which incrementally builds pre- training tasks and then learns pre-trained models on these constructed tasks via continual multi-task learning is proposed."
    },
    "citationCount": 692,
    "influentialCitationCount": 130,
    "code": null,
    "description": null,
    "url": null
}