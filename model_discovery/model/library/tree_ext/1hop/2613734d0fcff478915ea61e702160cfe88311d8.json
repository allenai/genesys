{
    "acronym": "2613734d0fcff478915ea61e702160cfe88311d8",
    "title": "EVA2.0: Investigating Open-domain Chinese Dialogue Systems with Large-scale Pre-training",
    "seed_ids": [
        "gpt",
        "a3184d40d390793232c99c89b57b8f65c16320b2",
        "2d29e1e684f8db8a143b3313cee991c4c786d340",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "2613734d0fcff478915ea61e702160cfe88311d8",
    "abstract": null,
    "authors": [
        "Yuxian Gu",
        "Jiaxin Wen",
        "Hao Sun",
        "Yi Song",
        "Pei Ke",
        "Chujie Zheng",
        "Zheng Zhang",
        "Jianzhu Yao",
        "Lei Liu",
        "Xiaoyan Zhu",
        "Jie Tang",
        "Minlie Huang"
    ],
    "venue": "Machine Intelligence Research",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and will make the models and codes publicly available."
    },
    "citationCount": 44,
    "influentialCitationCount": 8,
    "code": null,
    "description": null,
    "url": null
}