{
    "acronym": "59f55c9234bb79b64ebfc225744df590d407a0b2",
    "title": "Improving Personalized Sentiment Representation with Knowledge-enhanced and Parameter-efficient Layer Normalization",
    "seed_ids": [
        "bigbird",
        "longformer",
        "84daddd294fa3cc12596b5785f81c2a153d2fb1d",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "a022bda79947d1f656a1164003c1b3ae9a843df9",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0"
    ],
    "s2id": "59f55c9234bb79b64ebfc225744df590d407a0b2",
    "abstract": "Existing studies on personalized sentiment classification consider a document review as an overall text unit and incorporate backgrounds (i.e., user and product information) to learn sentiment representation. However, it is difficult when these methods meet the current pretrained language models (PLMs) owing to quadratic costs that increase with text length and heterogeneous mixes of randomly initialized background information and textual information initialized from well-pretrained checkpoints during information incorporation. To address these problems, we propose a knowledge-enhanced and parameter-efficient layer normalization (E2LN) for efficient and effective review modeling via leveraging LN in transformer structures. Initially, a knowledge base is introduced that stores well-pretrained checkpoints, structured text information, and background information. Based on such a knowledge base, the ability of LN can be magnified as being a crucial component of transformer structure and then improve the performance of PLMs in downstream tasks. Moreover, the proposed E2LN can make PLMs capable of modeling long document reviews and incorporating background information with parameter-efficient fine-tuning and knowledge injecting. Extensive experimental results were obtained for three document-level sentiment classification benchmark datasets. By comparing the results, the effectiveness and efficiency of the proposed model was demonstrated. Code and Data are released at https://github.com/yoyo-yun/E2LN.",
    "authors": [
        "Youjia Zhang",
        "Jin Wang",
        "Liang-Chih Yu",
        "Dan Xu",
        "Xuejie Zhang"
    ],
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A knowledge-enhanced and parameter-efficient layer normalization (E2LN) is proposed for efficient and effective review modeling via leveraging LN in transformer structures and can make PLMs capable of modeling long document reviews and incorporating background information with parameter-efficient fine-tuning and knowledge injecting."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}