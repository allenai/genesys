{
    "acronym": "59a48bf399484a2e5b28c1f1168639acc8cf3412",
    "title": "Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models",
    "seed_ids": [
        "gpt3",
        "7a29fb7a37869126840ed71ac7671db2e985f443",
        "0e3d1457a66e442fae46c8f96886dc76aef3b085",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "8659bf379ca8756755125a487c43cfe8611ce842",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "59a48bf399484a2e5b28c1f1168639acc8cf3412",
    "abstract": "Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters. However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model. Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in few-shot settings. In this paper, we prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately. Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based parameter-efficient tuning and derivative-free optimization methods in few-shot settings.",
    "authors": [
        "Feihu Jin",
        "Yin Liu",
        "Ying Tan"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately, demonstrating substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based parameter-efficient tuning and derivative-free optimization methods in few-shot settings."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}