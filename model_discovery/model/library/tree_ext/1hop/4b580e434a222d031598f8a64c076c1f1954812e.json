{
    "acronym": "4b580e434a222d031598f8a64c076c1f1954812e",
    "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
    "seed_ids": [
        "bert",
        "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "b76e98a0a023d37c6534aa2ead09c8ff595f0bae",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "4b580e434a222d031598f8a64c076c1f1954812e",
    "abstract": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.",
    "authors": [
        "Junliang Luo",
        "Tianyu Li",
        "Di Wu",
        "Michael R. M. Jenkin",
        "Steve Liu",
        "Gregory Dudek"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation for large language models and applying them to real world tasks."
    },
    "citationCount": 6,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}