{
    "acronym": "fb9dfabf5e91ed4e903da01f9c34dcbfecb11efc",
    "title": "MediSwift: Efficient Sparse Pre-trained Biomedical Language Models",
    "seed_ids": [
        "bert",
        "44279244407a64431810f982be6d0c7da4429dd7",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "8326dba15f6b8ee6e43c23eea3265a05e59e8135",
        "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed",
        "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "fb9dfabf5e91ed4e903da01f9c34dcbfecb11efc",
    "abstract": "Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense fine-tuning and strategic soft prompting, MediSwift models outperform existing LLMs up to 7B parameters on biomedical tasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as PubMedQA. Our results show that sparse pre-training, along with dense fine-tuning and soft prompting, offers an effective method for creating high-performing, computationally efficient models in specialized domains.",
    "authors": [
        "Vithursan Thangarasa",
        "Mahmoud Salem",
        "Shreyas Saxena",
        "Kevin Leong",
        "Joel Hestness",
        "Sean Lie"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The results show that sparse pre-training, along with dense fine-tuning and soft prompting, offers an effective method for creating high-performing, computationally efficient models in specialized domains."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}