{
    "acronym": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
    "title": "Ethical and social risks of harm from Language Models",
    "seed_ids": [
        "gpt",
        "319b84be7a843250bc81d7086f79a4126d550277",
        "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
    "abstract": "This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.",
    "authors": [
        "Laura Weidinger",
        "John F. J. Mellor",
        "Maribeth Rauh",
        "Conor Griffin",
        "J. Uesato",
        "Po-Sen Huang",
        "Myra Cheng",
        "Mia Glaese",
        "Borja Balle",
        "Atoosa Kasirzadeh",
        "Zachary Kenton",
        "S. Brown",
        "W. Hawkins",
        "T. Stepleton",
        "Courtney Biles",
        "Abeba Birhane",
        "Julia Haas",
        "Laura Rimell",
        "Lisa Anne Hendricks",
        "William S. Isaac",
        "Sean Legassick",
        "G. Irving",
        "Iason Gabriel"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences to help structure the risk landscape associated with large-scale Language Models."
    },
    "citationCount": 684,
    "influentialCitationCount": 32,
    "code": null,
    "description": null,
    "url": null
}