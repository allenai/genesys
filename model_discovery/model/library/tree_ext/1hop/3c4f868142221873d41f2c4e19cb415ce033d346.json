{
    "acronym": "3c4f868142221873d41f2c4e19cb415ce033d346",
    "title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More",
    "seed_ids": [
        "gpt2",
        "bert",
        "55f1cde49846c58b0bedebde15b8f7d939f39432",
        "b21670e8061a06ab97e7d6052c9345a326e84ff8",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "3c4f868142221873d41f2c4e19cb415ce033d346",
    "abstract": "Today's best language models still struggle with hallucinations: factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The reversal curse, where models cannot recall information when probed in a different order than was encountered during training, exemplifies this in information retrieval. We reframe the reversal curse as a factorization curse - a failure of models to learn the same joint distribution under different factorizations. Through a series of controlled experiments with increasing levels of realism including WikiReversal, a setting we introduce to closely simulate a knowledge intensive finetuning task, we find that the factorization curse is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.",
    "authors": [
        "O. Kitouni",
        "Niklas Nolte",
        "Diane Bouchacourt",
        "Adina Williams",
        "Michael G. Rabbat",
        "Mark Ibrahim"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is found that the factorization curse is an inherent failure of the next-token prediction objective used in popular large language models, and factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}