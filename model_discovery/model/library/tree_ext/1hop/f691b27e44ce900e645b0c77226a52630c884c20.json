{
    "acronym": "f691b27e44ce900e645b0c77226a52630c884c20",
    "title": "Towards Explainability in Legal Outcome Prediction Models",
    "seed_ids": [
        "bert",
        "fd33e77884e69f6bc099990fc2790248af2749d9",
        "c67843e9ccb8221abb5d2feecc4f3ce2708e9cf2",
        "61a5ae33fc34efcdbc710004554ec57e607ce75e",
        "063bfb1cb74a8150a5378125cd34d998aac9dd21",
        "29584ed6d68a06fdf91440a018f6bc83a44fd177",
        "f6fbb6809374ca57205bd2cf1421d4f4fa04f975"
    ],
    "s2id": "f691b27e44ce900e645b0c77226a52630c884c20",
    "abstract": "Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand the model\u2019s decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and neural models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.",
    "authors": [
        "Josef Valvoda",
        "Ryan Cotterell"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "tldr": null,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}