{
    "acronym": "530dbd3247bdec8ee9b2513ebbe6b6df5e4d6055",
    "title": "Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting",
    "seed_ids": [
        "gpt2",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "530dbd3247bdec8ee9b2513ebbe6b6df5e4d6055",
    "abstract": "Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in producing rational and factual responses. However, to establish long-term relationships with users, the KDG model needs the capability to generate responses in a desired style or attribute. Thus, we study a new problem: Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two challenges: (1) How to train a SKDG model where no  triples are available. (2) How to cohere with context and preserve the knowledge when generating a stylized response. In this paper, we propose a novel disentangled template rewriting (DTR) method which generates responses via combing disentangled style templates (from monolingual stylized corpus) and content templates (from KDG corpus). The entire framework is end-to-end differentiable and learned without supervision. Extensive experiments on two benchmarks indicate that DTR achieves a significant improvement on all evaluation metrics compared with previous state-of-the-art stylized dialogue generation methods. Besides, DTR achieves comparable performance with the state-of-the-art KDG methods in standard KDG evaluation setting.",
    "authors": [
        "Qingfeng Sun",
        "Can Xu",
        "Huang Hu",
        "Yujing Wang",
        "Jian Miao",
        "Xiubo Geng",
        "Yining Chen",
        "Fei Xu",
        "Daxin Jiang"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel disentangled template rewriting (DTR) method which generates responses via combing disentangling style templates (from monolingual stylized corpus) and content templates ( from KDG corpus) which is end-to-end differentiable and learned without supervision."
    },
    "citationCount": 8,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}