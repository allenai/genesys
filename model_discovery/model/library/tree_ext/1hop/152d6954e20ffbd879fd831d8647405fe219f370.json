{
    "acronym": "152d6954e20ffbd879fd831d8647405fe219f370",
    "title": "BERTs are Generative In-Context Learners",
    "seed_ids": [
        "gpt3",
        "bert",
        "d8b51d518f2dd62943762ceaa8961d3b1bfbcc1a",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28"
    ],
    "s2id": "152d6954e20ffbd879fd831d8647405fe219f370",
    "abstract": "This paper explores the in-context learning capabilities of masked language models, challenging the common view that this ability does not 'emerge' in them. We present an embarrassingly simple inference technique that enables DeBERTa to operate as a generative model without any additional training. Our findings demonstrate that DeBERTa can match and even surpass GPT-3, its contemporary that famously introduced the paradigm of in-context learning. The comparative analysis reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. This suggests that there is great potential for a hybrid training approach that takes advantage of the strengths of both training objectives.",
    "authors": [
        "David Samuel"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The findings demonstrate that DeBERTa can match and even surpass GPT-3, its contemporary that famously introduced the paradigm of in-context learning."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}