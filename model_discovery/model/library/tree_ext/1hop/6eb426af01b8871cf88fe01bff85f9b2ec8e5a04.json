{
    "acronym": "6eb426af01b8871cf88fe01bff85f9b2ec8e5a04",
    "title": "TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge",
    "seed_ids": [
        "gpt2",
        "7befecfc6df5d3cdae7c9a769dd205d54745a286",
        "8ac324e7f0471ab35f3a59ce5140ac2719b727b6",
        "200050c1f51e2c930e62b078c6ce20f2a6675468",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "6eb426af01b8871cf88fe01bff85f9b2ec8e5a04",
    "abstract": "Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.",
    "authors": [
        "Chao-Hong Tan",
        "Jia-Chen Gu",
        "Chongyang Tao",
        "Zhen-Hua Ling",
        "Can Xu",
        "Huang Hu",
        "Xiubo Geng",
        "Daxin Jiang"
    ],
    "venue": "Findings",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs, and can learn what and how to generate."
    },
    "citationCount": 6,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}