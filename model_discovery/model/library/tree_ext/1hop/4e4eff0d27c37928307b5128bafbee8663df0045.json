{
    "acronym": "4e4eff0d27c37928307b5128bafbee8663df0045",
    "title": "Compressed models are NOT miniature versions of large models",
    "seed_ids": [
        "bert",
        "a76c98c6814ce8de07707b81c18520af508b7184"
    ],
    "s2id": "4e4eff0d27c37928307b5128bafbee8663df0045",
    "abstract": "Large neural models are often compressed before deployment. Model compression is necessary for many practical reasons, such as inference latency, memory footprint, and energy consumption. Compressed models are assumed to be miniature versions of corresponding large neural models. However, we question this belief in our work. We compare compressed models with corresponding large neural models using four model characteristics: prediction errors, data representation, data distribution, and vulnerability to adversarial attack. We perform experiments using the BERT-large model and its five compressed versions. For all four model characteristics, compressed models significantly differ from the BERT-large model. Even among compressed models, they differ from each other on all four model characteristics. Apart from the expected loss in model performance, there are major side effects of using compressed models to replace large neural models.",
    "authors": [
        "Rohit Raj Rai",
        "Rishant Pal",
        "Amit Awekar"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work compares compressed models with corresponding large neural models using four model characteristics: prediction errors, data representation, data distribution, and vulnerability to adversarial attack."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}