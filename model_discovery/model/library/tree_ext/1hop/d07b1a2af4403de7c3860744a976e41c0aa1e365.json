{
    "acronym": "d07b1a2af4403de7c3860744a976e41c0aa1e365",
    "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
    "seed_ids": [
        "bert",
        "465471bb5bf1a945549d6291c2d23367966b4957",
        "563a851106623b9f112d0e2a290d3950a871079c",
        "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1"
    ],
    "s2id": "d07b1a2af4403de7c3860744a976e41c0aa1e365",
    "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.",
    "authors": [
        "Taolin Zhang",
        "Dongyang Li",
        "Qizhou Chen",
        "Chengyu Wang",
        "Longtao Huang",
        "Hui Xue",
        "Xiaofeng He",
        "Junyuan Huang"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes a new pipeline named \"Reinforced Retriever-Reorder-Responder\" to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}