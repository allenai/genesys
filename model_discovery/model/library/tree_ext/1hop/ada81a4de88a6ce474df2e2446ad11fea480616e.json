{
    "acronym": "ada81a4de88a6ce474df2e2446ad11fea480616e",
    "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
    "seed_ids": [
        "performer",
        "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "05bcf9999525656cfaa59bc71f8572d771ff3776",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0",
        "5e00596fa946670d894b1bdaeff5a98e3867ef13",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b",
        "d0086b86103a620a86bc918746df0aa642e2a8a3"
    ],
    "s2id": "ada81a4de88a6ce474df2e2446ad11fea480616e",
    "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
    "authors": [
        "Andy Zeng",
        "Adrian S. Wong",
        "Stefan Welker",
        "K. Choromanski",
        "F. Tombari",
        "Aveek Purohit",
        "M. Ryoo",
        "Vikas Sindhwani",
        "Johnny Lee",
        "Vincent Vanhoucke",
        "Peter R. Florence"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people."
    },
    "citationCount": 447,
    "influentialCitationCount": 36,
    "code": null,
    "description": null,
    "url": null
}