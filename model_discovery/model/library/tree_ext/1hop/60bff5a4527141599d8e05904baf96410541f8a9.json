{
    "acronym": "60bff5a4527141599d8e05904baf96410541f8a9",
    "title": "Learning to Model Editing Processes",
    "seed_ids": [
        "gpt",
        "483336e50c566c5505012d8777b97800a6386113",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "60bff5a4527141599d8e05904baf96410541f8a9",
    "abstract": "Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits.",
    "authors": [
        "Machel Reid",
        "Graham Neubig"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work forms a conceptual framework to describe the likelihood of multi-step edits, and describes neural models that can learn a generative model of sequences based on these multistep edits."
    },
    "citationCount": 32,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}