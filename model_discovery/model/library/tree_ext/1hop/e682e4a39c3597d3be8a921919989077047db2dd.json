{
    "acronym": "e682e4a39c3597d3be8a921919989077047db2dd",
    "title": "Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis",
    "seed_ids": [
        "longformer",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "7cc730da554003dda77796d2cb4f06da5dfd5592",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "e682e4a39c3597d3be8a921919989077047db2dd",
    "abstract": "Recent advances in the area of long document matching have primarily focused on using transformer-based models for long document encoding and matching. There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost \u2013 both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time. In this work, we empirically demonstrate the effectiveness of simple neural models (such as feed-forward networks, and CNNs) and simple embeddings (like GloVe, and Paragraph Vector) over transformer-based models on the task of document matching. We show that simple models outperform the more complex BERT-based models while taking significantly less training time, energy, and memory. The simple models are also more robust to variations in document length and text perturbations.",
    "authors": [
        "Akshita Jha",
        "Adithya Samavedhi",
        "Vineeth Rakesh",
        "J. Chandrashekar",
        "Chandan K. Reddy"
    ],
    "venue": "Findings",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work empirically demonstrate the effectiveness of simple neural models and simple embeddings over transformer-based models on the task of document matching, and shows that simple models outperform the more complex BERT- based models while taking significantly less training time, energy, and memory."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}