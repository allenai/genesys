{
    "acronym": "076d06d66886dedf3cb6e5dc31393dac38fef300",
    "title": "Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation",
    "seed_ids": [
        "reformer",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "076d06d66886dedf3cb6e5dc31393dac38fef300",
    "abstract": "It is critical to obtain high resolution features with long range dependency for dense prediction tasks such as semantic segmentation. To generate high-resolution output of size $H\\times W$ from a low-resolution feature map of size $h\\times w$ ($hw\\ll HW$), a naive dense transformer incurs an intractable complexity of $\\mathcal{O}(hwHW)$, limiting its application on high-resolution dense prediction. We propose a Dual-Flattening Transformer (DFlatFormer) to enable high-resolution output by reducing complexity to $\\mathcal{O}(hw(H+W))$ that is multiple orders of magnitude smaller than the naive dense transformer. Decomposed queries are presented to retrieve row and column attentions tractably through separate transformers, and their outputs are combined to form a dense feature map at high resolution. To this end, the input sequence fed from an encoder is row-wise and column-wise flattened to align with decomposed queries by preserving their row and column structures, respectively. Row and column transformers also interact with each other to capture their mutual attentions with the spatial crossings between rows and columns. We also propose to perform attentions through efficient grouping and pooling to further reduce the model complexity. Extensive experiments on ADE20K and Cityscapes datasets demonstrate the superiority of the proposed dual-flattening transformer architecture with higher mIoUs.",
    "authors": [
        "Ying Wang",
        "C. Ho",
        "Wenju Xu",
        "Ziwei Xuan",
        "Xudong Liu",
        "Guo-Jun Qi"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A Dual-Flattening Transformer (DFlatFormer) is proposed to enable high-resolution output by reducing complexity to $\\mathcal{O}(hw(H+W))$ that is multiple orders of magnitude smaller than the naive dense transformer."
    },
    "citationCount": 4,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}