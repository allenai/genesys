{
    "acronym": "86b6e42e2ce957f6497d4aa578c9bb4d2b4e4ba3",
    "title": "Blockwise Parallel Transformer for Large Context Models",
    "seed_ids": [
        "flashattn",
        "mea",
        "f11044596cf2eaf59f83d82b8167b16ba6a08617",
        "f393aff1593c2d370ec0ae004910d18e40524967",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "a128b1c47e6842605fb95bceae930d2135fc38fc",
        "860bc4f071f35d6d8529a52c2c1858d030779a6a",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "6edccbd83a9aae204785d4821f97855677c33866",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "dc0102a51a9d33e104a4a3808a18cf17f057228c",
        "53c3940f35b8b45d55ed49056282e1961954513d",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "d5e999aae76d5270ef272076979c809817458212",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
        "cec7872b194aadf54140578b9be52939eb1112e9",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "f51497f463566581874c941353dd9d80069c5b77",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "86b6e42e2ce957f6497d4aa578c9bb4d2b4e4ba3",
    "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.",
    "authors": [
        "Hao Liu",
        "P. Abbeel"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods."
    },
    "citationCount": 5,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}