{
    "acronym": "093fe8d04b098d95b1a917ecda6fb27f24e76c7f",
    "title": "Sequentia12D: Organizing Center of Skip Connections for Transformers",
    "seed_ids": [
        "gpt2",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "093fe8d04b098d95b1a917ecda6fb27f24e76c7f",
    "abstract": "Skip connections (SCs) are commonly employed in neural networks to facilitate gradient-based training and often lead to improved performance in deep learning. To implement SCs, a user writes custom modules along with torch.nn.Sequential which limits the extensibility of neural network design with SC. Despite their versatility and user-friendliness, there is an opportunity to create a standardized approach for organizing SCs in various applications. Establishing a clear organizing framework would significantly enhance the widespread adoption of this powerful technique, amplifying its impact. We introduce Sequential2D as an enhanced orchestrator which provides in-sights into not only the width and depth of the network but also a neural network's intra-connections such as SCs. The functionality provided by the Sequential2D architecture is a strict super-set of the functionality found in the common torch.nn.Sequential network. The 2D in Sequential2D refers to the two-dimensional matrix of functions, as opposed to the one-dimensional cascading array of functions in a Pytorch or Keras sequential container. As an example of our proposed approach, we demonstrate the effectiveness of our Sequential2D architecture by substantially improving the fine-tuning performance of the GPT-2 model with less than 1% additional parameters.",
    "authors": [
        "Harsh Nilesh Pathak",
        "Randy Paffenroth",
        "Quincy Hershey"
    ],
    "venue": "International Conference on Machine Learning and Applications",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The Sequential2D architecture is introduced as an enhanced orchestrator which provides in-sights into not only the width and depth of the network but also a neural network's intra-connections such as SCs, and is demonstrated by substantially improving the fine-tuning performance of the GPT-2 model with less than 1% additional parameters."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}