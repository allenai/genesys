{
    "acronym": "4c455b8689f96c452a62671ec16a4c2cef153908",
    "title": "HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding",
    "seed_ids": [
        "bert",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c"
    ],
    "s2id": "4c455b8689f96c452a62671ec16a4c2cef153908",
    "abstract": "Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.",
    "authors": [
        "Rihui Jin",
        "Yu Li",
        "Guilin Qi",
        "Nan Hu",
        "Yuan-Fang Li",
        "Jiaoyan Chen",
        "Jianan Wang",
        "Yongrui Chen",
        "Dehai Min"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "HGT, a framework with a heterogeneous graph-enhanced large language model (LLM) to tackle few-shot TU tasks, empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}