{
    "acronym": "c297684ddba73db32179942714a45f890633ae13",
    "title": "RNA-ViT: Reduced-Dimension Approximate Normalized Attention Vision Transformers for Latency Efficient Private Inference",
    "seed_ids": [
        "cosformer",
        "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "baa4fca5b03862a30ef082341ea97a445d74d3ee",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "c297684ddba73db32179942714a45f890633ae13",
    "abstract": "The concern over data and model privacy in machine learning inference as a service (MLaaS) has led to the development of private inference (PI) techniques. However, existing PI frameworks, especially those designed for large models such as vision transformers (ViT), suffer from high computational and communication overheads caused by the expensive multi-party computation (MPC) protocols. The encrypted attention module that involves the softmax operation contributes significantly to this overhead. In this work, we present a family of models dubbed RNA-ViT, that leverage a novel attention module called reduced-dimension approximate normalized attention and a latency efficient GeLU-alternative layer. In particular, RNA-ViT uses two novel techniques to improve PI efficiency in ViTs: a reduced-dimension normalized attention (RNA) architecture and a high order polynomial (HOP) softmax approximation for latency efficient normalization. We also propose a novel metric, accuracy-to-latency ratio (A2L), to evaluate modules in terms of their accuracy and PI latency. Based on this metric, we perform an analysis to identify a nonlinearity module with improved PI efficiency. Our extensive experiments show that RNA-ViT can achieve average 3.53\u00d7, 3.54\u00d7, 1.66\u00d7 lower PI latency with an average accuracy improvement of 0.93%, 2.04%, and 2.73% compared to the state-of-the-art scheme MPCViT [1], on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively.",
    "authors": [
        "Dake Chen",
        "Yuke Zhang",
        "Souvik Kundu",
        "Chenghao Li",
        "P. Beerel"
    ],
    "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents a family of models, dubbed RNA-ViT, that leverage a novel attention module called reduced-dimension approximate normalized attention and a latency efficient GeLU-alternative layer, and proposes a novel metric, accuracy-to-latency ratio (A2L), to evaluate modules in terms of their accuracy and PI latency."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}