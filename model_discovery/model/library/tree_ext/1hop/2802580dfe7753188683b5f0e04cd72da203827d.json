{
    "acronym": "2802580dfe7753188683b5f0e04cd72da203827d",
    "title": "Pointer-Guided Pre-Training: Infusing Large Language Models with Paragraph-Level Contextual Awareness",
    "seed_ids": [
        "bert",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4"
    ],
    "s2id": "2802580dfe7753188683b5f0e04cd72da203827d",
    "abstract": "We introduce\"pointer-guided segment ordering\"(SO), a novel pre-training technique aimed at enhancing the contextual understanding of paragraph-level text representations in large language models. Our methodology leverages a self-attention-driven pointer network to restore the original sequence of shuffled text segments, addressing the challenge of capturing the structural coherence and contextual dependencies within documents. This pre-training approach is complemented by a fine-tuning methodology that incorporates dynamic sampling, augmenting the diversity of training instances and improving sample efficiency for various downstream applications. We evaluate our method on a diverse set of datasets, demonstrating its efficacy in tasks requiring sequential text classification across scientific literature and financial reporting domains. Our experiments show that pointer-guided pre-training significantly enhances the model's ability to understand complex document structures, leading to state-of-the-art performance in downstream classification tasks.",
    "authors": [
        "L. Hillebrand",
        "Prabhupad Pradhan",
        "Christian Bauckhage",
        "R. Sifa"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The experiments show that pointer-guided pre-training significantly enhances the model's ability to understand complex document structures, leading to state-of-the-art performance in downstream classification tasks."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}