{
    "acronym": "34928ddaf042e883627e18e5e85a143b7321ea70",
    "title": "MemDPT: Differential Privacy for Memory Efficient Language Models",
    "seed_ids": [
        "gpt2",
        "385c2ee0bf829676d1a5aacfc697fc6a9d245ed5",
        "ed38c6b157c11476939c426ec6871c926f2f3524",
        "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "34928ddaf042e883627e18e5e85a143b7321ea70",
    "abstract": "Large language models have consistently demonstrated remarkable performance across a wide spectrum of applications. Nonetheless, the deployment of these models can inadvertently expose user privacy to potential risks. The substantial memory demands of these models during training represent a significant resource consumption challenge. The sheer size of these models imposes a considerable burden on memory resources, which is a matter of significant concern in practice. In this paper, we present an innovative training framework MemDPT that not only reduces the memory cost of large language models but also places a strong emphasis on safeguarding user data privacy. MemDPT provides edge network and reverse network designs to accommodate various differential privacy memory-efficient fine-tuning schemes. Our approach not only achieves $2 \\sim 3 \\times$ memory optimization but also provides robust privacy protection, ensuring that user data remains secure and confidential. Extensive experiments have demonstrated that MemDPT can effectively provide differential privacy efficient fine-tuning across various task scenarios.",
    "authors": [
        "Yanming Liu",
        "Xinyue Peng",
        "Jiannan Cao",
        "Yuwei Zhang",
        "Chen Ma",
        "Songhang Deng",
        "Mengchen Fu",
        "Xuhong Zhang",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin",
        "Tianyu Du"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An innovative training framework MemDPT is presented that not only reduces the memory cost of large language models but also places a strong emphasis on safeguarding user data privacy, ensuring that user data remains secure and confidential."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}