{
    "acronym": "1fcfa7ae5419100607ce3f22fbed7b7e75fd33d3",
    "title": "Long Code for Code Search",
    "seed_ids": [
        "sparsetransformer",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "f6390beca54411b06f3bde424fb983a451789733",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0",
        "2a31319e73d4486716168b65cdf7559baeda18ce"
    ],
    "s2id": "1fcfa7ae5419100607ce3f22fbed7b7e75fd33d3",
    "abstract": "Thanks to the Transformer-based pretraining models, the performance of code search has been improved signi\ufb01cantly. However, due to the restriction of multi-head self-attention and GPU memory, there is a limit on the input token length. The existing pretrained code models, such as GraphCode-BERT, CodeBERT, RoBERTa (code), take the \ufb01rst 256 tokens by default, which makes them unable to represent the complete information of long code (i.e., code that is greater than 256 tokens). Unlike the long text document that can be regarded as a whole with complete semantics, the semantics of long code is discontinuous as a piece of long code may contain different code modules. Therefore, it is unreasonable to directly apply the long text processing methods to long code. To tackle the long code problem, we propose MLCS (Modeling Long Code for Code Search) to obtain a better representation for long code. Our experimental results show the effectiveness of MLCS for long code retrieval. With MLCS, we could use Transformer-based pretraining models to model long code without changing their internal structure and re-pretraining. Through AST-based splitting and attention-based fusion methods, MLCS achieves an overall mean reciprocal ranking (MRR) score of 0.785, outperforming the previous state-of-the-art result of 0.713 on the public CodeSearchNet benchmark.",
    "authors": [
        "Fan Hu",
        "Yanlin Wang",
        "Lun Du",
        "Hongyu Zhang",
        "Shi Han",
        "Dongmei Zhang",
        "Xirong Li"
    ],
    "venue": "arXiv.org",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "With MLCS, Transformer-based pretraining models could be used to model long code without changing their internal structure and re-pretraining, and MLCS achieves an overall mean reciprocal ranking (MRR) score of 0.785, outperforming the previous state-of-the-art result."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}