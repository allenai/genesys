{
    "acronym": "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
    "title": "FUDGE: Controlled Text Generation With Future Discriminators",
    "seed_ids": [
        "transformerxl",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "e04a80263d252a3d8a382ba37a249b9345620570",
        "7a15950dc71079285a4eaf195de5aadd87c41b40",
        "75acc731bdd2b626edc74672a30da3bc51010ae8",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
    "abstract": "We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G\u2019s output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor\u2019s outputs to adjust G\u2019s original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks \u2014 couplet completion in poetry, topic control in language generation, and formality change in machine translation \u2014 and observe gains in all three tasks.",
    "authors": [
        "Kevin Yang",
        "D. Klein"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation that enables conditioning on a desired attribute a while requiring access only to G\u2019s output logits."
    },
    "citationCount": 242,
    "influentialCitationCount": 54,
    "code": null,
    "description": null,
    "url": null
}