{
    "acronym": "47537fec0e6ef936ce4fdb0dadce9bdf4ea8bf73",
    "title": "Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions",
    "seed_ids": [
        "classfreediffu",
        "8d531cb8cf51eec3b8f1106d189295fa3c81c02a",
        "3ff7153fd6bd47d08084c7f50f8fd70026c126e7",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d"
    ],
    "s2id": "47537fec0e6ef936ce4fdb0dadce9bdf4ea8bf73",
    "abstract": "In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: https://compvis.github.io/attribute-control. Code is available at https://github.com/CompVis/attribute-control.",
    "authors": [
        "S. A. Baumann",
        "Felix Krause",
        "Michael Neumayr",
        "Nick Stracke",
        "Vincent Tao Hu",
        "Bjorn Ommer"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "It is shown that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models and it is demonstrated that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}