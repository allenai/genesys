{
    "acronym": "64f94eb97891ca17273464fcb9c507c5a5c7dba7",
    "title": "Acquiring Knowledge from Pre-trained Model to Neural Machine Translation",
    "seed_ids": [
        "gpt",
        "7a09101ac03b74db501648597fa54e992a0fc84f",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "145b8b5d99a2beba6029418ca043585b90138d12",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc"
    ],
    "s2id": "64f94eb97891ca17273464fcb9c507c5a5c7dba7",
    "abstract": "Pre-training and fine-tuning have achieved great success in natural language process field. The standard paradigm of exploiting them includes two steps: first, pre-training a model, e.g. BERT, with a large scale unlabeled monolingual data. Then, fine-tuning the pre-trained model with labeled data from downstream tasks. However, in neural machine translation (NMT), we address the problem that the training objective of the bilingual task is far different from the monolingual pre-trained model. This gap leads that only using fine-tuning in NMT can not fully utilize prior language knowledge. In this paper, we propose an Apt framework for acquiring knowledge from pre-trained model to NMT. The proposed approach includes two modules: 1). a dynamic fusion mechanism to fuse task-specific features adapted from general knowledge into NMT network, 2). a knowledge distillation paradigm to learn language knowledge continuously during the NMT training process. The proposed approach could integrate suitable knowledge from pre-trained models to improve the NMT. Experimental results on WMT English to German, German to English and Chinese to English machine translation tasks show that our model outperforms strong baselines and the fine-tuning counterparts.",
    "authors": [
        "Rongxiang Weng",
        "Heng Yu",
        "Shujian Huang",
        "Shanbo Cheng",
        "Weihua Luo"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An Apt framework for acquiring knowledge from pre-trained model to NMT, which includes a dynamic fusion mechanism to fuse task-specific features adapted from general knowledge into NMT network and a knowledge distillation paradigm to learn language knowledge continuously during the NMT training process."
    },
    "citationCount": 58,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}