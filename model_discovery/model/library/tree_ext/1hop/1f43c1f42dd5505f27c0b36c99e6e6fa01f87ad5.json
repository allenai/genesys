{
    "acronym": "1f43c1f42dd5505f27c0b36c99e6e6fa01f87ad5",
    "title": "Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs",
    "seed_ids": [
        "gpt3",
        "b085968c4362fb286ad6c5ef71a5db9630da0498",
        "3adca9d49eab1a4db6538a995af3636d10e120c3",
        "a8b995f0da78a79447dfb18c2337972b044f4239",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "13a0d8bb38f739990c8cd65a44061c6534f17221"
    ],
    "s2id": "1f43c1f42dd5505f27c0b36c99e6e6fa01f87ad5",
    "abstract": "The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance using LLMs with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2~3x speedup for the decoding speed.",
    "authors": [
        "Luchang Li",
        "Sheng Qian",
        "Jie Lu",
        "Lunxi Yuan",
        "Rui Wang",
        "Qin Xie"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "To facilitate high-efficiency LLM deployment on device GPUs, this work proposes four optimization techniques: a symbolic expression-based approach to support dynamic shape model inference, a symbolic expression-based approach to support dynamic shape model inference, an FP4 quantization method termed M0E4 to reduce dequantization overhead, and a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}