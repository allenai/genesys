{
    "acronym": "af89c2e64cfd12c0553500889d9bd04530b6e7cc",
    "title": "Sim-GPT: Text Similarity via GPT Annotated Data",
    "seed_ids": [
        "gpt3",
        "bert",
        "d84cf745c534c010b8e55e5a4a04878906848dc3",
        "7011bf9aa7e68fabaa1df498da6d2dd8a950f037",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "d56c1fc337fb07ec004dc846f80582c327af717c",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "af89c2e64cfd12c0553500889d9bd04530b6e7cc",
    "abstract": "Due to the lack of a large collection of high-quality labeled sentence pairs with textual similarity scores, existing approaches for Semantic Textual Similarity (STS) mostly rely on unsupervised techniques or training signals that are only partially correlated with textual similarity, e.g., NLI-based datasets. To tackle this issue, in this paper, we propose the strategy of measuring text similarity via GPT annotated data (Sim-GPT for short). The core idea of Sim-GPT is to generate data with STS labels using GPT-4, based on which an STS model is trained. Sim-GPT framework utilizes LLMs to provide a substantial amount of reliable annotated data filling the gap of the lack of training signals for STS. Sim-GPT is trained on a one-time generated dataset using BERT or RoBERTa as the backbone, which offers long-term savings in cost and speed compared to repeatedly invoking LLMs for each sentence pair. Trained on the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the widely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over the current SOTA PromCSE model. To encourage further advancements of the field, we release both models and the 371K annotated examples from GPT-4. Code, models and annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.",
    "authors": [
        "Shuhe Wang",
        "Beiming Cao",
        "Shengyu Zhang",
        "Xiaoya Li",
        "Jiwei Li",
        "Fei Wu",
        "Guoyin Wang",
        "Eduard Hovy"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Sim-GPT framework utilizes LLMs to provide a substantial amount of reliable annotated data filling the gap of the lack of training signals for STS, and is trained on a one-time generated dataset using BERT or RoBERTa as the backbone."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}