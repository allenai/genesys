{
    "acronym": "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
    "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
    "seed_ids": [
        "hyena",
        "bigbird",
        "998ac3e945857cf2676ee7efdbaf443a0c6f820a",
        "54155c2977a977bf129849455dcae3a2b79b3f41",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
        "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
        "7e9ff94476f41041c75e253e84f487db00e9c861"
    ],
    "s2id": "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
    "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level, an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics for simple adaptation to novel tasks without updating pretrained model weights. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 17 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on all 8 datasets on average by +9 accuracy points.",
    "authors": [
        "Eric D Nguyen",
        "Michael Poli",
        "Marjan Faizi",
        "A. Thomas",
        "Callum Birch-Sykes",
        "Michael Wornow",
        "Aman Patel",
        "Clayton M. Rabideau",
        "Stefano Massaroli",
        "Y. Bengio",
        "Stefano Ermon",
        "S. Baccus",
        "Christopher R\u00e9"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level, an up to 500x increase over previous dense attention-based models."
    },
    "citationCount": 90,
    "influentialCitationCount": 16,
    "code": null,
    "description": null,
    "url": null
}