{
    "acronym": "b4ca92e4792cb118b18a1d4f4f6534020a2f403d",
    "title": "DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability",
    "seed_ids": [
        "classfreediffu",
        "e342165a614588878ad0f4bc9bacf3905df34d08",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "82482585e94192b4e9913727e461f89cd08e9725",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "de18baa4964804cf471d85a5a090498242d2e79f"
    ],
    "s2id": "b4ca92e4792cb118b18a1d4f4f6534020a2f403d",
    "abstract": "Recently, large-scale diffusion models, e.g., Stable diffusion and DallE2, have shown remarkable results on image synthesis. On the other hand, large-scale cross-modal pre-trained models (e.g., CLIP, ALIGN, and FILIP) are competent for various downstream tasks by learning to align vision and language embeddings. In this paper, we explore the possibility of jointly modeling generation and discrimination. Specifically, we propose DiffDis to unify the cross-modal generative and discriminative pretraining into one single framework under the diffusion process. DiffDis first formulates the image-text discriminative problem as a generative diffusion process of the text embedding from the text encoder conditioned on the image. Then, we propose a novel dual-stream network architecture, which fuses the noisy text embedding with the knowledge of latent images from different scales for image-text discriminative learning. Moreover, the generative and discriminative tasks can efficiently share the image-branch network structure in the multi-modality model. Benefiting from diffusion-based unified training, DiffDis achieves both better generation ability and cross-modal semantic alignment in one architecture. Experimental results show that DiffDis outperforms single-task models on both the image generation and the image-text discriminative tasks, e.g., 1.65% improvement on average accuracy of zero-shot classification over 12 datasets and 2.42 improvement on FID of zero-shot image synthesis.",
    "authors": [
        "Runhu Huang",
        "Jianhua Han",
        "Guansong Lu",
        "Xiaodan Liang",
        "Yihan Zeng",
        "Wei Zhang",
        "Hang Xu"
    ],
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper proposes DiffDis to unify the cross-modal generative and discriminative pretraining into one single framework under the diffusion process, and proposes a novel dual-stream network architecture, which fuses the noisy text embedding with the knowledge of latent images from different scales for image-text discrim inative learning."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}