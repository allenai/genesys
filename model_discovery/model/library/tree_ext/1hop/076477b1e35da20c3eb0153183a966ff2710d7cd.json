{
    "acronym": "076477b1e35da20c3eb0153183a966ff2710d7cd",
    "title": "CoDA: A Co-Design Framework for Versatile and Efficient Attention Accelerators",
    "seed_ids": [
        "bert",
        "linformer",
        "lstransformer",
        "512ff5037b28be7415d318ae6e8eeb0abb8c7013",
        "690a37a2ba67b44b012bf9aa92e6a7f7670f487f",
        "9b069ba5259d229bfd4fe3ac3768148e2d1092f8",
        "5f895e84c1fea75de07b4f90da518273c2e57291",
        "b97c3c370401dc34d2adbeb24f34de5180a14be6",
        "1a883522f3c0051d70be1f8cbdb8989a77395006",
        "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "076477b1e35da20c3eb0153183a966ff2710d7cd",
    "abstract": "As a primary component of Transformers, attention mechanism suffers from quadratic computational complexity. To achieve efficient implementations, its hardware accelerator designs have aroused great research interest. However, most existing accelerators only support a single type of application and a single type of attention, making it difficult to meet the demands of diverse application scenarios. Additionally, they mainly focus on the dynamic pruning of attention matrices, which requires the deployment of pre-processing units, thereby reducing overall hardware efficiency. This paper presents CoDA which is an algorithm, dataflow and architecture co-design framework for versatile and efficient attention accelerators. The designed accelerator supports both NLP and CV applications, and can be configured into the mode supporting low-rank attention or low-rank plus sparse attention. We apply algorithmic transformations to low-rank attention to significantly reduce computational complexity. To prevent an increase in storage overhead resulting from the proposed algorithmic transformations, we carefully design the dataflows and adopt a block-wise fashion. Down-scaling softmax is further supported by architecture and dataflow co-design. Moreover, we propose a softmax sharing strategy to reduce the area cost. Our experiment results demonstrate that the proposed accelerator outperforms the state-of-the-art designs in terms of throughput, area efficiency and energy efficiency.",
    "authors": [
        "Wenjie Li",
        "Aokun Hu",
        "Ningyi Xu",
        "Guanghui He"
    ],
    "venue": "IEEE transactions on computers",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "CoDA is an algorithm, dataflow and architecture co-design framework for versatile and efficient attention accelerators and demonstrates that the proposed accelerator outperforms the state-of-the-art designs in terms of throughput, area efficiency and energy efficiency."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}