{
    "acronym": "0afdbd7a9719b69b94b1db87cfe2a8f36652b97b",
    "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
    "seed_ids": [
        "nystromformer",
        "performer",
        "linformer",
        "reformer",
        "3725e21357c43404a1f19cfebc330d19734eeec0",
        "dc1b905c0af4dc318b63cd52fbc867c788df4b8c",
        "4cf65e4f358c45476790df1f0ced0fc90c2a405b",
        "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf",
        "4b0541eccd8f98852d6807a14fbac17f775c7b40",
        "a25370452533bf47549243e97852b9cdf7a0ee0e",
        "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "35a9749df07a2ab97c51af4d260b095b00da7676",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
        "955f90930d48750e7239478b4eed440eb84131cd"
    ],
    "s2id": "0afdbd7a9719b69b94b1db87cfe2a8f36652b97b",
    "abstract": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods use a subset of combinations of component functions and weight matrices within the random features paradigm. We identify the need for a systematic comparison of different combinations of weight matrix and component functions for attention learning in Transformer. In this work, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in linearized attention of the Transformer. We experiment with broad classes of component functions and weight matrices for three textual tasks in the LRA benchmark. Our experimentation with multiple combinations of component functions and weight matrices leads us to a novel combination with 23.4% faster training time and 25.2% lower memory consumption over the previous SOTA random feature Transformer, while maintaining the performance, as compared to the Original Transformer. Our code is available at: https://github.com/dukeraphaelng/spectraformer .",
    "authors": [
        "Duke Nguyen",
        "Aditya Joshi",
        "Flora D. Salim"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces Spectraformer, a unified framework for approximating and learning the kernel function in linearized attention of the Transformer, and experiments with broad classes of component functions and weight matrices for three textual tasks in the LRA benchmark."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}