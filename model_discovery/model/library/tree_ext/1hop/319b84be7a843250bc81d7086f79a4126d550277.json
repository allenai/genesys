{
    "acronym": "319b84be7a843250bc81d7086f79a4126d550277",
    "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
    "seed_ids": [
        "gpt",
        "04b40daa1ca74bdbb578beb314bf662538ecd18e",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe",
        "f1957038e9ded19108d3c71340d7462152b70f25",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "319b84be7a843250bc81d7086f79a4126d550277",
    "abstract": "Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).",
    "authors": [
        "Yu Sun",
        "Shuohuan Wang",
        "Shikun Feng",
        "Siyu Ding",
        "Chao Pang",
        "Junyuan Shang",
        "Jiaxiang Liu",
        "Xuyi Chen",
        "Yanbin Zhao",
        "Yuxiang Lu",
        "Weixin Liu",
        "Zhihua Wu",
        "Weibao Gong",
        "Jianzhong Liang",
        "Zhizhou Shang",
        "Peng Sun",
        "Wei Liu",
        "Ouyang Xuan",
        "Dianhai Yu",
        "Hao Tian",
        "Hua Wu",
        "Haifeng Wang"
    ],
    "venue": "arXiv.org",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A unified framework named ERNIE 3.0 is proposed for pre-training large-scale knowledge enhanced models that fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few- shot learning or fine-tuning."
    },
    "citationCount": 307,
    "influentialCitationCount": 54,
    "code": null,
    "description": null,
    "url": null
}