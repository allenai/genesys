{
    "acronym": "aedbe1cab3fc3e84899eb201236fefd3337a0cf4",
    "title": "The Mysterious Case of Neuron 1512: Injectable Realignment Architectures Reveal Internal Characteristics of Meta's Llama 2 Model",
    "seed_ids": [
        "transformer",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "e04a80263d252a3d8a382ba37a249b9345620570"
    ],
    "s2id": "aedbe1cab3fc3e84899eb201236fefd3337a0cf4",
    "abstract": "Large Language Models (LLMs) have an unrivaled and invaluable ability to\"align\"their output to a diverse range of human preferences, by mirroring them in the text they generate. The internal characteristics of such models, however, remain largely opaque. This work presents the Injectable Realignment Model (IRM) as a novel approach to language model interpretability and explainability. Inspired by earlier work on Neural Programming Interfaces, we construct and train a small network -- the IRM -- to induce emotion-based alignments within a 7B parameter LLM architecture. The IRM outputs are injected via layerwise addition at various points during the LLM's forward pass, thus modulating its behavior without changing the weights of the original model. This isolates the alignment behavior from the complex mechanisms of the transformer model. Analysis of the trained IRM's outputs reveals a curious pattern. Across more than 24 training runs and multiple alignment datasets, patterns of IRM activations align themselves in striations associated with a neuron's index within each transformer layer, rather than being associated with the layers themselves. Further, a single neuron index (1512) is strongly correlated with all tested alignments. This result, although initially counterintuitive, is directly attributable to design choices present within almost all commercially available transformer architectures, and highlights a potential weak point in Meta's pretrained Llama 2 models. It also demonstrates the value of the IRM architecture for language model analysis and interpretability. Our code and datasets are available at https://github.com/DRAGNLabs/injectable-alignment-model",
    "authors": [
        "Brenden Smith",
        "Dallin Baker",
        "Clayton Chase",
        "Myles Barney",
        "Kaden Parker",
        "Makenna Allred",
        "Peter Hu",
        "Alex Evans",
        "Nancy Fulda"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work presents the Injectable Realignment Model (IRM) as a novel approach to language model interpretability and explainability, and construct and train a small network -- the IRM -- to induce emotion-based alignments within a 7B parameter LLM architecture."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}