{
    "acronym": "5e1456b25e260951bed9c387932304dd054acc0a",
    "title": "Driving and suppressing the human language network using large language models",
    "seed_ids": [
        "gpt2",
        "66c286df54551baba7351a1ed44019367e5aa7ea",
        "eaee0b647d336c6fc8b844812675ec35cddf14a1",
        "8cef169a76fc8ff2971ff3b6832b5de885d37ad4",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "23e04389f8728a5736382d3662341a1a2a25e171",
        "5a2263092f49540fd0e049050a96882ff29b00c3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5e1456b25e260951bed9c387932304dd054acc0a",
    "abstract": null,
    "authors": [
        "Greta Tuckute",
        "Aalok Sathe",
        "Shashank Srikant",
        "Maya Taliaferro",
        "Mingye Wang",
        "Martin Schrimpf",
        "K. Kay",
        "Evelina Fedorenko"
    ],
    "venue": "Nature Human Behaviour",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The ability of neural network models to not only mimic human language but also non-invasively control neural activity in higher-level cortical areas, such as the language network, is established."
    },
    "citationCount": 5,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}