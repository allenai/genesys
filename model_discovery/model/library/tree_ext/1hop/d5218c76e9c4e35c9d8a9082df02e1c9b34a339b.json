{
    "acronym": "d5218c76e9c4e35c9d8a9082df02e1c9b34a339b",
    "title": "Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction",
    "seed_ids": [
        "bert"
    ],
    "s2id": "d5218c76e9c4e35c9d8a9082df02e1c9b34a339b",
    "abstract": "Few-shot Relation Extraction (FSRE) aims to extract relational facts from a sparse set of labeled corpora. Recent studies have shown promising results in FSRE by employing Pre-trained Language Models (PLMs) within the framework of supervised contrastive learning, which considers both instances and label facts. However, how to effectively harness massive instance-label pairs to encompass the learned representation with semantic richness in this learning paradigm is not fully explored. To address this gap, we introduce a novel synergistic anchored contrastive pre-training framework. This framework is motivated by the insight that the diverse viewpoints conveyed through instance-label pairs capture incomplete yet complementary intrinsic textual semantics. Specifically, our framework involves a symmetrical contrastive objective that encompasses both sentence-anchored and label-anchored contrastive losses. By combining these two losses, the model establishes a robust and uniform representation space. This space effectively captures the reciprocal alignment of feature distributions among instances and relational facts, simultaneously enhancing the maximization of mutual information across diverse perspectives within the same relation. Experimental results demonstrate that our framework achieves significant performance enhancements compared to baseline models in downstream FSRE tasks. Furthermore, our approach exhibits superior adaptability to handle the challenges of domain shift and zero-shot relation extraction. Our code is available online at https://github.com/AONE-NLP/FSRE-SaCon.",
    "authors": [
        "Da Luo",
        "Yanglei Gan",
        "Rui Hou",
        "Run Lin",
        "Qiao Liu",
        "Yuxiang Cai",
        "Wannian Gao"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a novel synergistic anchored contrastive pre-training framework that encompasses both sentence-anchored and label-anchored contrastive losses, and effectively captures the reciprocal alignment of feature distributions among instances and relational facts."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}