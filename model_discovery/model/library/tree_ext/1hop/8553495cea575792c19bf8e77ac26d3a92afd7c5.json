{
    "acronym": "8553495cea575792c19bf8e77ac26d3a92afd7c5",
    "title": "LABOR-LLM: Language-Based Occupational Representations with Large Language Models",
    "seed_ids": [
        "gpt3",
        "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "df602516e28a9ef0ef665ed0aef551984d8d770d"
    ],
    "s2id": "8553495cea575792c19bf8e77ac26d3a92afd7c5",
    "abstract": "Many empirical studies of labor market questions rely on estimating relatively simple predictive models using small, carefully constructed longitudinal survey datasets based on hand-engineered features. Large Language Models (LLMs), trained on massive datasets, encode vast quantities of world knowledge and can be used for the next job prediction problem. However, while an off-the-shelf LLM produces plausible career trajectories when prompted, the probability with which an LLM predicts a particular job transition conditional on career history will not, in general, align with the true conditional probability in a given population. Recently, Vafa et al. (2024) introduced a transformer-based\"foundation model\", CAREER, trained using a large, unrepresentative resume dataset, that predicts transitions between jobs; it further demonstrated how transfer learning techniques can be used to leverage the foundation model to build better predictive models of both transitions and wages that reflect conditional transition probabilities found in nationally representative survey datasets. This paper considers an alternative where the fine-tuning of the CAREER foundation model is replaced by fine-tuning LLMs. For the task of next job prediction, we demonstrate that models trained with our approach outperform several alternatives in terms of predictive performance on the survey data, including traditional econometric models, CAREER, and LLMs with in-context learning, even though the LLM can in principle predict job titles that are not allowed in the survey data. Further, we show that our fine-tuned LLM-based models' predictions are more representative of the career trajectories of various workforce subpopulations than off-the-shelf LLM models and CAREER. We conduct experiments and analyses that highlight the sources of the gains in the performance of our models for representative predictions.",
    "authors": [
        "Tianyu Du",
        "Ayush Kanodia",
        "Herman Brunborg",
        "Keyon Vafa",
        "Susan Athey"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper considers an alternative where the fine-tuning of the CAREER foundation model is replaced by fine-tuning LLMs, and demonstrates that models trained with this approach outperform several alternatives in terms of predictive performance on the survey data, including traditional econometric models, CAREER, and LLMs with in-context learning."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}