{
    "acronym": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
    "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
    "seed_ids": [
        "bigbird",
        "longformer",
        "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
        "6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "01b15017ac59b8d6f2ce3598c4a7d6358c211426",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "2a7023e7d1dbd6ea0d98efd09a1f18d8599fe78f",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
    "abstract": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.",
    "authors": [
        "Mandy Guo",
        "J. Ainslie",
        "David C. Uthus",
        "Santiago Onta\u00f1\u00f3n",
        "Jianmo Ni",
        "Yun-Hsuan Sung",
        "Yinfei Yang"
    ],
    "venue": "NAACL-HLT",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new model, called LongT5, is presented, with which the effects of scaling both the input length and model size at the same time are explored, which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs."
    },
    "citationCount": 229,
    "influentialCitationCount": 41,
    "code": null,
    "description": null,
    "url": null
}