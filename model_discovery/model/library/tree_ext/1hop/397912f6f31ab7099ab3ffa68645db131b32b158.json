{
    "acronym": "397912f6f31ab7099ab3ffa68645db131b32b158",
    "title": "Neural Deep Equilibrium Solvers",
    "seed_ids": [
        "transformerxl",
        "9a618cca0d2fc78db1be1aed70517401cb3f3859",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c"
    ],
    "s2id": "397912f6f31ab7099ab3ffa68645db131b32b158",
    "abstract": "A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer f\u03b8. This structure enables decoupling the internal structure of the layer (which controls representational capacity) from how the fixed point is actually computed (which impacts inference-time efficiency), which is usually via classic techniques such as Broyden\u2019s method or Anderson acceleration. In this paper, we show that one can exploit such decoupling and substantially enhance this fixed point computation using a custom neural solver. Specifically, our solver uses a parameterized network to both guess an initial value of the optimization and perform iterative updates, in a method that generalizes a learnable form of Anderson acceleration and can be trained end-to-end in an unsupervised manner. Such a solution is particularly well suited to the implicit model setting, because inference in these models requires repeatedly solving for a fixed point of the same nonlinear layer for different inputs, a task at which our network excels. Our experiments show that these neural equilibrium solvers are fast to train (only taking an extra 0.9-1.1% over the original DEQ\u2019s training time), require few additional parameters (1-3% of the original model size), yet lead to a 2\u00d7 speedup in DEQ network inference without any degradation in accuracy across numerous domains and tasks.",
    "authors": [
        "Shaojie Bai",
        "V. Koltun",
        "J. Kolter"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper uses a parameterized network to both guess an initial value of the optimization and perform iterative updates, in a method that generalizes a learnable form of Anderson acceleration and can be trained end-to-end in an unsupervised manner."
    },
    "citationCount": 22,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}