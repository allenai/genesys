{
    "acronym": "a99590d39c6ef362f6b788d2720c708e2ceab4bf",
    "title": "LANISTR: Multimodal Learning from Structured and Unstructured Data",
    "seed_ids": [
        "gpt",
        "84b6fecf016d74512869c698c66c83729abdf359",
        "b92628d13e8d090d042232fe6ae0b8998634b893",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "5e00596fa946670d894b1bdaeff5a98e3867ef13",
        "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "a99590d39c6ef362f6b788d2720c708e2ceab4bf",
    "abstract": "Multimodal large-scale pretraining has shown impressive performance for unstructured data such as language and image. However, a prevalent real-world scenario involves structured data types, tabular and time-series, along with unstructured data. Such scenarios have been understudied. To bridge this gap, we propose LANISTR, an attention-based framework to learn from LANguage, Image, and STRuctured data. The core of LANISTR's methodology is rooted in \\textit{masking-based} training applied across both unimodal and multimodal levels. In particular, we introduce a new similarity-based multimodal masking loss that enables it to learn cross-modal relations from large-scale multimodal data with missing modalities. On two real-world datasets, MIMIC-IV (from healthcare) and Amazon Product Review (from retail), LANISTR demonstrates remarkable improvements, 6.6\\% (in AUROC) and 14\\% (in accuracy) when fine-tuned with 0.1\\% and 0.01\\% of labeled data, respectively, compared to the state-of-the-art alternatives. Notably, these improvements are observed even with very high ratio of samples (35.7\\% and 99.8\\% respectively) not containing all modalities, underlining the robustness of LANISTR to practical missing modality challenge. Our code and models will be available at https://github.com/google-research/lanistr",
    "authors": [
        "Sayna Ebrahimi",
        "Sercan \u00d6. Arik",
        "Yihe Dong",
        "Tomas Pfister"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A new similarity-based multimodal masking loss is introduced that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities, underlining the robustness of LANISTR to practical missing modality challenge."
    },
    "citationCount": 2,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}