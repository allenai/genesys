{
    "acronym": "34042e2680e475510a1030b54165a81534ad88d3",
    "title": "HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization",
    "seed_ids": [
        "longformer",
        "9dc624d7258d1a56117ca720aea953ce46b66b21",
        "46e7383e6fb8da77479d0a828c7a24d924302169",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "7cc730da554003dda77796d2cb4f06da5dfd5592",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0"
    ],
    "s2id": "34042e2680e475510a1030b54165a81534ad88d3",
    "abstract": "To capture the semantic graph structure from raw text, most existing summarization approaches are built on GNNs with a pre-trained model. However, these methods suffer from cumbersome procedures and inefficient computations for long-text documents. To mitigate these issues, this paper proposes HetFormer, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization. Specifically, we model different types of semantic nodes in raw text as a potential heterogeneous graph and directly learn heterogeneous relationships (edges) among nodes by Transformer. Extensive experiments on both single- and multi-document summarization tasks show that HetFormer achieves state-of-the-art performance in Rouge F1 while using less memory and fewer parameters.",
    "authors": [
        "Ye Liu",
        "Jianguo Zhang",
        "Yao Wan",
        "Congying Xia",
        "Lifang He",
        "Philip S. Yu"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "HetFormer, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization, model different types of semantic nodes in raw text as a potential heterogeneous graph and directly learn heterogeneous relationships (edges) among nodes by Transformer."
    },
    "citationCount": 24,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}