{
    "acronym": "0fa6bd29f51b82c3fe6e275129e4518ba646b59b",
    "title": "Scalable Hierarchical Self-Attention with Learnable Hierarchy for Long-Range Interactions",
    "seed_ids": [
        "bigbird",
        "sinkhorn",
        "70e91e16eb321067d9402710e14a40cf28311f73",
        "5eda60d4940d4185df45c5703e103458171d465d",
        "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "657329c633709dd1ac34a30d57341b186b1a47c2",
        "34a4e6818d680875ff0bef9a76de0376118446d1",
        "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
        "366244acdd930e488ae224ab6e2a92dc24aa7e06"
    ],
    "s2id": "0fa6bd29f51b82c3fe6e275129e4518ba646b59b",
    "abstract": "Self-attention models have made great strides toward accurately modeling a wide array of data modalities, including, more recently, graph-structured data. This paper demonstrates that adaptive hierarchical attention can go a long way toward successfully applying trans-formers to graphs. Our proposed model Sequoia provides a powerful inductive bias towards long-range interaction modeling, leading to better generalization. We propose an end-to-end mechanism for a data-dependent construction of a hierarchy which in turn guides the self-attention mechanism. Using adaptive hierarchy provides a natural pathway toward sparse attention by constraining node-to-node interactions with the immediate family of each node in the hierarchy (e.g., parent, children, and siblings). This in turn dramatically reduces the computational complexity of a self-attention layer from quadratic to log-linear in terms of the input size while maintaining or sometimes even surpassing the standard transformer\u2019s ability to model long-range dependencies across the entire input. Experimentally, we report state-of-the-art performance on long-range graph benchmarks while remaining computationally efficient. Moving beyond graphs, we also display competitive performance on long-range sequence modeling, point-clouds classification, and segmentation when using a fixed hierarchy. Our source code is",
    "authors": [
        "Thuan Trang",
        "Nhat-Khang Ng\u00f4",
        "Hugo Sonnery",
        "Thieu N. Vo",
        "Siamak Ravanbakhsh",
        "Truong Son"
    ],
    "venue": "",
    "year": null,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper demonstrates that adaptive hierarchical attention can go a long way toward successfully applying trans-formers to graphs and proposes an end-to-end mechanism for a data-dependent construction of a hierarchy which in turn guides the self-attention mechanism."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}