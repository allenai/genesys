{
    "acronym": "08f07767bec6ca374f0ae69268d9846f87d46088",
    "title": "Sentiment analysis with hotel customer reviews using FNet",
    "seed_ids": [
        "bert",
        "fnet",
        "1f133158a8973fb33fea188f20517cd7e69bfe7f",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b"
    ],
    "s2id": "08f07767bec6ca374f0ae69268d9846f87d46088",
    "abstract": "Recent research has focused on opinion mining from public sentiments using natural language processing (NLP) and machine learning (ML) techniques. Transformer-based models, such as bidirectional encoder representations from transformers (BERT), excel in extracting semantic information but are resourceintensive. Google\u2019s new research, mixing tokens with fourier transform, also known as FNet, replaced BERT\u2019s attention mechanism with a non-parameterized fourier transform, aiming to reduce training time without compromising performance. This study fine-tuned the FNet model with a publicly available Kaggle hotel review dataset and investigated the performance of this dataset in both FNet and BERT architectures along with conventional machine learning models such as long short-term memory (LSTM) and support vector machine (SVM). Results revealed that FNet significantly reduces the training time by almost 20% and memory utilization by nearly 60% compared to BERT. The highest test accuracy observed in this experiment by FNet was 80.27% which is nearly 97.85% of BERT\u2019s performance with identical parameters.",
    "authors": [
        "Shovan Bhowmik",
        "Rifat Sadik",
        "Wahiduzzaman Akanda",
        "Juboraj Roy Pavel"
    ],
    "venue": "Bulletin of Electrical Engineering and Informatics",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This study fine-tuned the FNet model with a publicly available Kaggle hotel review dataset and investigated the performance of this dataset in both FNet and BERT architectures along with conventional machine learning models such as long short-term memory (LSTM) and support vector machine (SVM)."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}