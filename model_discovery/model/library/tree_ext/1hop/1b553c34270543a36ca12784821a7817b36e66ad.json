{
    "acronym": "1b553c34270543a36ca12784821a7817b36e66ad",
    "title": "Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments",
    "seed_ids": [
        "gpt",
        "b47381e04739ea3f392ba6c8faaf64105493c196"
    ],
    "s2id": "1b553c34270543a36ca12784821a7817b36e66ad",
    "abstract": "Recent pretrained sentence encoders achieve state of the art results on language understanding tasks, but does this mean they have implicit knowledge of syntactic structures? We introduce a grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), which we use to investigate the grammatical knowledge of three pretrained encoders, including the popular OpenAI Transformer (Radford et al., 2018) and BERT (Devlin et al., 2018). We fine-tune these encoders to do acceptability classification over CoLA and compare the models' performance on the annotated analysis set. Some phenomena, e.g. modification by adjuncts, are easy to learn for all models, while others, e.g. long-distance movement, are learned effectively only by models with strong overall performance, and others still, e.g. morphological agreement, are hardly learned by any model.",
    "authors": [
        "Alex Warstadt",
        "Samuel R. Bowman"
    ],
    "venue": "arXiv.org",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) is introduced, which is used to investigate the grammatical knowledge of three pretrained encoders, including the popular OpenAI Transformer and BERT."
    },
    "citationCount": 19,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}