{
    "acronym": "7ef58be19ee9cf8d683d7209b650094df3902377",
    "title": "StructCoder: Structure-Aware Transformer for Code Generation",
    "seed_ids": [
        "gpt2",
        "05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "0fe2636446cd686830da3d971b31a004d6094b3c",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "7ef58be19ee9cf8d683d7209b650094df3902377",
    "abstract": "There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code\u2019s syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at https://github.com/reddy-lab-code-research/StructCoder/.",
    "authors": [
        "Sindhu Tipirneni",
        "Ming Zhu",
        "Chandan K. Reddy"
    ],
    "venue": "ACM Transactions on Knowledge Discovery from Data",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively."
    },
    "citationCount": 28,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}