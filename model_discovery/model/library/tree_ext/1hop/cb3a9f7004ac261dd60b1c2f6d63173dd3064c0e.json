{
    "acronym": "cb3a9f7004ac261dd60b1c2f6d63173dd3064c0e",
    "title": "A Unified Dialogue User Simulator for Few-shot Data Augmentation",
    "seed_ids": [
        "gpt",
        "2d29e1e684f8db8a143b3313cee991c4c786d340",
        "0f3596364943bb03f14cd75d9595a2c465831edb",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "6ebfbc954b9975d2f2651f380b9bdf46ae963178",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "cb3a9f7004ac261dd60b1c2f6d63173dd3064c0e",
    "abstract": "Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation meth-ods have been developed to augment large-scale task-oriented dialogue corpora. However, they heavily rely on annotated data in the target domain, which require a tremendous amount of data collection and human labeling work. In this paper, we build a unified dialogue user simulation model by pre-training on several publicly available datasets. The model can then be tuned on a target domain with few-shot data. The experiments on a target dataset across multiple domains show that our proposed model brings remarkable performance increases through data augmentation.",
    "authors": [
        "Dazhen Wan",
        "Zheng Zhang",
        "Qi Zhu",
        "Lizi Liao",
        "Minlie Huang"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A unified dialogue user simulation model is built by pre-training on several publicly available datasets and can be tuned on a target domain with few-shot data to bring remarkable performance increases through data augmentation."
    },
    "citationCount": 13,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}