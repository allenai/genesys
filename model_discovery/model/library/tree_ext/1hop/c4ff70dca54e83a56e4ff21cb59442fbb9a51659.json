{
    "acronym": "c4ff70dca54e83a56e4ff21cb59442fbb9a51659",
    "title": "COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks",
    "seed_ids": [
        "lighdynconv",
        "549c682b49fcd29f7404ecfb11fb36467131d3ec",
        "2a218786f4615b82389f78472e7ff22e6ce57490",
        "81c5c35ad1311fb1ebae00f6d87631021fc7d956",
        "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "25db56fc85fe15625c3375064a35e908ba6dfd2a",
        "efe4902a39c8ef332058ae7d156a6bffcd3c1341",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "f4238bd2385a52413ccbacfd9e409a650235bd13",
        "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "b76e98a0a023d37c6534aa2ead09c8ff595f0bae"
    ],
    "s2id": "c4ff70dca54e83a56e4ff21cb59442fbb9a51659",
    "abstract": "Vision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention.\n\n\n\nIn natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context.\n\n\n\nWe present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.\n\n\n\nA comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, including question answering. The proposed approach achieves competitive performance with existing state-of-the-art methods on some tasks.",
    "authors": [
        "Fangyi Zhu",
        "See-Kiong Ng",
        "S. Bressan"
    ],
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}