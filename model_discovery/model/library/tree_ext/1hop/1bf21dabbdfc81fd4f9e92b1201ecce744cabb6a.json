{
    "acronym": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a",
    "title": "SantaCoder: don't reach for the stars!",
    "seed_ids": [
        "mqa",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
        "0fe2636446cd686830da3d971b31a004d6094b3c"
    ],
    "s2id": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a",
    "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.",
    "authors": [
        "Loubna Ben Allal",
        "Raymond Li",
        "Denis Kocetkov",
        "Chenghao Mou",
        "Christopher Akiki",
        "Carlos Mu\u00f1oz Ferrandis",
        "Niklas Muennighoff",
        "Mayank Mishra",
        "A. Gu",
        "Manan Dey",
        "Logesh Kumar Umapathi",
        "Carolyn Jane Anderson",
        "Yangtian Zi",
        "J. Poirier",
        "Hailey Schoelkopf",
        "S. Troshin",
        "Dmitry Abulkhanov",
        "M. Romero",
        "M. Lappert",
        "F. Toni",
        "Bernardo Garc'ia del R'io",
        "Qian Liu",
        "Shamik Bose",
        "Urvashi Bhattacharyya",
        "Terry Yue Zhuo",
        "I. Yu",
        "Paulo Villegas",
        "Marco Zocca",
        "Sourab Mangrulkar",
        "D. Lansky",
        "Huu Nguyen",
        "Danish Contractor",
        "Luisa Villa",
        "Jia Li",
        "Dzmitry Bahdanau",
        "Yacine Jernite",
        "S. Hughes",
        "Daniel Fried",
        "Arjun Guha",
        "H. D. Vries",
        "Leandro von Werra"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The current state of the Personally Identifiable Information (PII) redaction pipeline is outlined, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data are outlined."
    },
    "citationCount": 137,
    "influentialCitationCount": 20,
    "code": null,
    "description": null,
    "url": null
}