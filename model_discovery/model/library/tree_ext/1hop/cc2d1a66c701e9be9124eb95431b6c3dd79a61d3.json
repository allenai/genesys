{
    "acronym": "cc2d1a66c701e9be9124eb95431b6c3dd79a61d3",
    "title": "Revisiting Transformer-based Models for Long Document Classification",
    "seed_ids": [
        "bigbird",
        "longformer",
        "transformerxl",
        "33d15b2d2a434ab33a2a88585604f4728a324baf",
        "2d82ee05b132d4681c3bd517afc17d608fe6e525",
        "fd33e77884e69f6bc099990fc2790248af2749d9",
        "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
        "a022bda79947d1f656a1164003c1b3ae9a843df9",
        "203b543bfa1e564bb80ff4229b43174d7c71b0c0"
    ],
    "s2id": "cc2d1a66c701e9be9124eb95431b6c3dd79a61d3",
    "abstract": "The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text, namely sparse attention and hierarchical encoding methods. We examine several aspects of sparse attention (e.g., size of local attention window, use of\r\nglobal attention) and hierarchical (e.g., document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text, and, based on our results, we derive practical advice of applying Transformer-based models on\r\nlong document classification tasks.",
    "authors": [
        "Xiang Dai",
        "Ilias Chalkidis",
        "S. Darkner",
        "Desmond Elliott"
    ],
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work examines several aspects of sparse attention (e.g., size of local attention window, use of global attention) and hierarchical encoding methods of Transformer-based transformers on four document classification datasets covering different domains."
    },
    "citationCount": 51,
    "influentialCitationCount": 4,
    "code": null,
    "description": null,
    "url": null
}