{
    "acronym": "8f30c30f92fbe1c307ee3c7a68c80a2c0dc8d619",
    "title": "Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules",
    "seed_ids": [
        "deltanet",
        "lineartransformer",
        "ac3c0bfa0e38cbd26aca06cf0fcf7ad6d7deaa4d",
        "4fd61f6b860acc9c5da8766b7c9064f0ec896301",
        "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
        "9ed25f101f19ea735ca300848948ed64064b97ca",
        "1a703f08da01cf737cce3fb9064259b3f4b44e9c",
        "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "6f68e1bb253925d8431588555d3010419f322e04"
    ],
    "s2id": "8f30c30f92fbe1c307ee3c7a68c80a2c0dc8d619",
    "abstract": "Work on fast weight programmers has demonstrated the effectiveness of key/value outer product-based learning rules for sequentially generating a weight matrix (WM) of a neural net (NN) by another NN or itself. However, the weight generation steps are typically not visually interpretable by humans, because the contents stored in the WM of an NN are not. Here we apply the same principle to generate natural images. The resulting fast weight painters (FPAs) learn to execute sequences of delta learning rules to sequentially generate images as sums of outer products of self-invented keys and values, one rank at a time, as if each image was a WM of an NN. We train our FPAs in the generative adversarial networks framework, and evaluate on various image datasets. We show how these generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images. While the performance largely lags behind the one of specialised state-of-the-art image generators, our approach allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images. Finally, we also show that an additional convolutional U-Net (now popular in diffusion models) at the output of an FPA can learn one-step\"denoising\"of FPA-generated images to enhance their quality. Our code is public.",
    "authors": [
        "Kazuki Irie",
        "J. Schmidhuber"
    ],
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work shows how generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images, and allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images."
    },
    "citationCount": 3,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}