{
    "acronym": "436278ce3b85265dc5cface29e63c714fe979d23",
    "title": "LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models",
    "seed_ids": [
        "transformerxl",
        "13a0d8bb38f739990c8cd65a44061c6534f17221",
        "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
        "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "d9f6ada77448664b71128bb19df15765336974a6",
        "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "16c844fd4d97f3c6eb38b0d6527c87d184efedc3",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "436278ce3b85265dc5cface29e63c714fe979d23",
    "abstract": "The Transformer architecture is ubiquitously used as the building block of large-scale autoregressive language models. However, finding architectures with the optimal trade-off between task performance (perplexity) and hardware constraints like peak memory utilization and latency is non-trivial. This is exacerbated by the proliferation of various hardware. We leverage the somewhat surprising empirical observation that the number of decoder parameters in autoregressive Transformers has a high rank correlation with task performance, irrespective of the architecture topology. This observation organically induces a simple Neural Architecture Search (NAS) algorithm that uses decoder parameters as a proxy for perplexity without need for any model training. The search phase of our training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of perplexity versus any hardware performance cost. We evaluate LTS on diverse devices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer backbones: GPT-2 and Transformer-XL. Results show that the perplexity of 16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster runtime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero and one-shot settings, LTS Pareto-frontier models achieve higher average accuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x lower latency. LTS extracts the Pareto-frontier in under 3 hours while running on a commodity laptop. We effectively remove the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.",
    "authors": [
        "Mojan Javaheripi",
        "Gustavo de Rosa",
        "Subhabrata Mukherjee",
        "S. Shah",
        "T. L. Religa",
        "C. C. T. Mendes",
        "S\u00e9bastien Bubeck",
        "F. Koushanfar",
        "Debadeepta Dey"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The search phase of this training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs and effectively removes the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling."
    },
    "citationCount": 9,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}