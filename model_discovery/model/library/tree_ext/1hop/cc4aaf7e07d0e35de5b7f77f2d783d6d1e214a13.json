{
    "acronym": "cc4aaf7e07d0e35de5b7f77f2d783d6d1e214a13",
    "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
    "seed_ids": [
        "gpt2",
        "527d11bae71c4a91f2e66637476e991f4a1d309b",
        "7b396b30c19deae2173c23e67ff5cde8f7069874",
        "42e726e2ea5bbb946001947d1a5b31ccc6b7aef9",
        "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "c57293882b2561e1ba03017902df9fc2f289dea2",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "cc4aaf7e07d0e35de5b7f77f2d783d6d1e214a13",
    "abstract": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported\"emergent abilities\"of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
    "authors": [
        "Mateusz Lajszczak",
        "Guillermo C\u00e1mbara",
        "Yang Li",
        "Fatih Beyhan",
        "Arent van Korlaar",
        "Fan Yang",
        "Arnaud Joly",
        "\u00c1lvaro Mart\u00edn-Cortinas",
        "Ammar Abbas",
        "Adam Michalski",
        "A. Moinet",
        "S. Karlapati",
        "Ewa Muszy'nska",
        "Haohan Guo",
        "Bartosz Putrycz",
        "Soledad L\u00f3pez Gambino",
        "Kayeon Yoo",
        "Elena Sokolova",
        "Thomas Drugman"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, it is shown that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences."
    },
    "citationCount": 19,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}