{
    "acronym": "e750cf66d0e7ade8e21da38df3eb2a174b027f48",
    "title": "Learning-Rate-Free Learning by D-Adaptation",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "e750cf66d0e7ade8e21da38df3eb2a174b027f48",
    "abstract": "D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. An open-source implementation is available.",
    "authors": [
        "Aaron Defazio",
        "Konstantin Mishchenko"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": null
    },
    "citationCount": 51,
    "influentialCitationCount": 5,
    "code": null,
    "description": null,
    "url": null
}