{
    "acronym": "791c3c30f2af10ac06f4fbc5b1e8960064aacbc7",
    "title": "Large-Scale Transfer Learning for Natural Language Generation",
    "seed_ids": [
        "gpt",
        "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "791c3c30f2af10ac06f4fbc5b1e8960064aacbc7",
    "abstract": "Large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. We focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results.",
    "authors": [
        "Sergey Golovanov",
        "Rauf Kurbanov",
        "S. Nikolenko",
        "Kyryl Truskovskyi",
        "Alexander Tselousov",
        "Thomas Wolf"
    ],
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work focuses in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results."
    },
    "citationCount": 86,
    "influentialCitationCount": 11,
    "code": null,
    "description": null,
    "url": null
}