{
    "acronym": "332ec6f2a9cd2b8c18504afa7d278f2d39a8f9cb",
    "title": "Rethinking Masked-Autoencoder-Based 3D Point Cloud Pretraining",
    "seed_ids": [
        "transformer",
        "698b3306698316b95860110d25dc9c846c3528c9",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7"
    ],
    "s2id": "332ec6f2a9cd2b8c18504afa7d278f2d39a8f9cb",
    "abstract": "The BERT-style (Bidirectional Encoder Representations from Transformers) pre-training paradigm has achieved remarkable success in both NLP (Natural Language Processing) and CV (Computer Vision). However, due to the sparsity and lack of semantic information in point cloud data, directly applying this paradigm faces significant obstacles. To address these issues, we propose LSV-MAE, a masked autoencoding pre-training scheme designed for voxel representations. We pre-train the backbone to reconstruct the masked voxels features extracted by PointNN. To enhance the feature extraction capability of the encoder, the point cloud is voxelized with different voxel sizes at different pre-training stages. Meanwhile, to minimize the effect of masking key points during the masking stage, the masked voxel features is re-integrated during the decoder processing. To test the proposed approach, experiments are conducted on well-known datasets. It is shown that the proposed method can improve detection accuracy by 1% to 18%, compared to the model without pre-training, across datasets of different sizes.",
    "authors": [
        "Nuo Cheng",
        "Chuanyu Luo",
        "Xinzhe Li",
        "Ruizhi Hu",
        "Han Li",
        "Sikun Ma",
        "Zhong Ren",
        "Haipeng Jiang",
        "Xiaohan Li",
        "Shengguang Lei",
        "Pu Li"
    ],
    "venue": "2024 IEEE Intelligent Vehicles Symposium (IV)",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work proposes LSV-MAE, a masked autoencoding pre-training scheme designed for voxel representations, which can improve detection accuracy by 1% to 18%, compared to the model without pre-training, across datasets of different sizes."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}