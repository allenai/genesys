{
    "acronym": "1c9d085167947871334f6ec959ecb4299c918880",
    "title": "Importance Guided Query Focused Long-Input Summarization",
    "seed_ids": [
        "longformer",
        "3c5f7e7ee0ab7413ba3bf8ad3400810da542d617",
        "13850aabbe8b70e99560a42504773a76ab5a4f47",
        "0a41cb292242a82b2b09b3bf23b48349b981a640",
        "f8c6d9ed61fdf04cd390dce017a817152cf4d580",
        "ac95a18762133d4065ac8af518c33084d83c5582",
        "faea04bf558c1a8b072bdcfe3cebe06ea4d9d9ca",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481"
    ],
    "s2id": "1c9d085167947871334f6ec959ecb4299c918880",
    "abstract": "Query focused long-document summarization aims to generate a summary depending on both the given query and the given long text. Previous end-to-end method inputs the query and long text into a single model, and generates the summary. However, it\u2019s a difficult task for the end-to-end model to recognize which parts of the long text are more important for answering the query. Besides, the length of the query is too small, so the information of the query is diluted after concatenating with the long text. As a result, many summarized sentences are irrelevant to the query. In this paper, we propose IGQFS (Importance Guided Query Focused long-input Summarization), which predicts an importance score for each sentence as an auxiliary task, and simultaneously generates the summary. To this end, we design an improved cross-attention module for the decoder according to the predicted utterance importance scores. Experimental results on QMSum dataset show that the auxiliary task and improved cross attention module generate summaries more related to the queries, and our method performs better than the end-to-end baseline model.",
    "authors": [
        "Shuaiyao Ning",
        "Caixia Yuan",
        "Xiaojie Wang",
        "Ling Huang"
    ],
    "venue": "International Conference on Cloud Computing and Intelligence Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Experimental results on QMSum dataset show that the auxiliary task and improved cross attention module generate summaries more related to the queries, and the method performs better than the end-to-end baseline model."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}