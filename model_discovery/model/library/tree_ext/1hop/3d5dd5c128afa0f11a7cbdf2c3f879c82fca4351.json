{
    "acronym": "3d5dd5c128afa0f11a7cbdf2c3f879c82fca4351",
    "title": "Memory Enhanced Global-Local Aggregation for Video Object Detection",
    "seed_ids": [
        "transformerxl"
    ],
    "s2id": "3d5dd5c128afa0f11a7cbdf2c3f879c82fca4351",
    "abstract": "How do humans recognize an object in a piece of video? Due to the deteriorated quality of single frame, it may be hard for people to identify an occluded object in this frame by just utilizing information within one image. We argue that there are two important cues for humans to recognize objects in videos: the global semantic information and the local localization information. Recently, plenty of methods adopt the self-attention mechanisms to enhance the features in key frame with either global semantic information or local localization information. In this paper we introduce memory enhanced global-local aggregation (MEGA) network, which is among the first trials that takes full consideration of both global and local information. Furthermore, empowered by a novel and carefully-designed Long Range Memory (LRM) module, our proposed MEGA could enable the key frame to get access to much more content than any previous methods. Enhanced by these two sources of information, our method achieves state-of-the-art performance on ImageNet VID dataset. Code is available at https://github.com/Scalsol/mega.pytorch.",
    "authors": [
        "Yihong Chen",
        "Yue Cao",
        "Han Hu",
        "Liwei Wang"
    ],
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2020,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces memory enhanced global-local aggregation (MEGA) network, which is among the first trials that takes full consideration of both global and local information."
    },
    "citationCount": 225,
    "influentialCitationCount": 56,
    "code": null,
    "description": null,
    "url": null
}