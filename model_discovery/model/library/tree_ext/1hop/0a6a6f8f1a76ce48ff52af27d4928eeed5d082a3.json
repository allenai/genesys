{
    "acronym": "0a6a6f8f1a76ce48ff52af27d4928eeed5d082a3",
    "title": "Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together",
    "seed_ids": [
        "gpt"
    ],
    "s2id": "0a6a6f8f1a76ce48ff52af27d4928eeed5d082a3",
    "abstract": "Neural networks equipped with self-attention have parallelizable computation, light-weight structure, and the ability to capture both long-range and local dependencies. Further, their expressive power and performance can be boosted by using a vector to measure pairwise dependency, but this requires to expand the alignment matrix to a tensor, which results in memory and computation bottlenecks. In this paper, we propose a novel attention mechanism called \u201cMulti-mask Tensorized Self-Attention\u201d (MTSA), which is as fast and as memory-efficient as a CNN, but significantly outperforms previous CNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token) and global (source2token) dependencies by a novel compatibility function composed of dot-product and additive attentions, 2) uses a tensor to represent the feature-wise alignment scores for better expressive power but only requires parallelizable matrix multiplications, and 3) combines multi-head with multi-dimensional attentions, and applies a distinct positional mask to each head (subspace), so the memory and computation can be distributed to multiple heads, each with sequential information encoded independently. The experiments show that a CNN/RNN-free model based on MTSA achieves state-of-the-art or competitive performance on nine NLP benchmarks with compelling memory- and time-efficiency.",
    "authors": [
        "Tao Shen",
        "Tianyi Zhou",
        "Guodong Long",
        "Jing Jiang",
        "Chengqi Zhang"
    ],
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2018,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel attention mechanism called \u201cMulti-mask Tensorized Self-Attention\u201d (MTSA), which is as fast and as memory-efficient as a CNN, but significantly outperforms previous CNN-/RNN-/attention-based models."
    },
    "citationCount": 11,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}