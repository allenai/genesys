{
    "acronym": "5b4685a3efaeee062b898f052a3870c9aa19cab7",
    "title": "Anomaly Detection on Unstable Logs with GPT Models",
    "seed_ids": [
        "gpt2",
        "bert",
        "f5afaccfe90268485a9961c5771ec5e71e9b806c",
        "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "5b4685a3efaeee062b898f052a3870c9aa19cab7",
    "abstract": "Log-based anomaly detection has been widely studied in the literature as a way to increase the dependability of software-intensive systems. In reality, logs can be unstable due to changes made to the software during its evolution. This, in turn, degrades the performance of downstream log analysis activities, such as anomaly detection. The critical challenge in detecting anomalies on these unstable logs is the lack of information about the new logs, due to insufficient log data from new software versions. The application of Large Language Models (LLMs) to many software engineering tasks has revolutionized various domains. In this paper, we report on an experimental comparison of a fine-tuned LLM and alternative models for anomaly detection on unstable logs. The main motivation is that the pre-training of LLMs on vast datasets may enable a robust understanding of diverse patterns and contextual information, which can be leveraged to mitigate the data insufficiency issue in the context of software evolution. Our experimental results on the two-version dataset of LOGEVOL-Hadoop show that the fine-tuned LLM (GPT-3) fares slightly better than supervised baselines when evaluated on unstable logs. The difference between GPT-3 and other supervised approaches tends to become more significant as the degree of changes in log sequences increases. However, it is unclear whether the difference is practically significant in all cases. Lastly, our comparison of prompt engineering (with GPT-4) and fine-tuning reveals that the latter provides significantly superior performance on both stable and unstable logs, offering valuable insights into the effective utilization of LLMs in this domain.",
    "authors": [
        "Fateme Hadadi",
        "Qinghua Xu",
        "D. Bianculli",
        "Lionel C. Briand"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "An experimental comparison of a fine-tuned LLM and alternative models for anomaly detection on unstable logs shows that the fine-tuned LLM (GPT-3) fares slightly better than supervised baselines when evaluated on unstable logs, and a comparison of prompt engineering and fine-tuning reveals that the latter provides significantly superior performance on both stable and unstable logs."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}