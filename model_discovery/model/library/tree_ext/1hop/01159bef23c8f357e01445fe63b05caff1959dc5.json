{
    "acronym": "01159bef23c8f357e01445fe63b05caff1959dc5",
    "title": "Mamba Hawkes Process",
    "seed_ids": [
        "s4",
        "mamba",
        "cbaf689fd9ea9bc939510019d90535d6249b3367",
        "bfd2b76998a0521c12903ef5ced517adf70ad2ba",
        "026b3396a63ed5772329708b7580d633bb86bec9",
        "5a77b508302771fc083bf24e0bcda8553c9b5421",
        "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
        "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "ca9047c78d48b606c4e4f0c456b1dda550de28b2"
    ],
    "s2id": "01159bef23c8f357e01445fe63b05caff1959dc5",
    "abstract": "Irregular and asynchronous event sequences are prevalent in many domains, such as social media, finance, and healthcare. Traditional temporal point processes (TPPs), like Hawkes processes, often struggle to model mutual inhibition and nonlinearity effectively. While recent neural network models, including RNNs and Transformers, address some of these issues, they still face challenges with long-term dependencies and computational efficiency. In this paper, we introduce the Mamba Hawkes Process (MHP), which leverages the Mamba state space architecture to capture long-range dependencies and dynamic event interactions. Our results show that MHP outperforms existing models across various datasets. Additionally, we propose the Mamba Hawkes Process Extension (MHP-E), which combines Mamba and Transformer models to enhance predictive capabilities. We present the novel application of the Mamba architecture to Hawkes processes, a flexible and extensible model structure, and a theoretical analysis of the synergy between state space models and Hawkes processes. Experimental results demonstrate the superior performance of both MHP and MHP-E, advancing the field of temporal point process modeling.",
    "authors": [
        "Anningzhe Gao",
        "Shan Dai",
        "Yan Hu"
    ],
    "venue": "",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This paper introduces the Mamba Hawkes Process (MHP), which leverages the Mamba state space architecture to capture long-range dependencies and dynamic event interactions and proposes the Mamba Hawkes Process Extension (MHP-E), which combines Mamba and Transformer models to enhance predictive capabilities."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}