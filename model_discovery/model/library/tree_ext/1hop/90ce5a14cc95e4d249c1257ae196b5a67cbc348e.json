{
    "acronym": "90ce5a14cc95e4d249c1257ae196b5a67cbc348e",
    "title": "LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding",
    "seed_ids": [
        "roformer",
        "4990f7542f0600e0501a7e7a931b32eb7cb804d5",
        "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
        "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "c8b25fab5608c3e033d34b4483ec47e68ba109b7"
    ],
    "s2id": "90ce5a14cc95e4d249c1257ae196b5a67cbc348e",
    "abstract": "Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm.",
    "authors": [
        "Yuxuan Wang",
        "Yueqian Wang",
        "Pengfei Wu",
        "Jianxin Liang",
        "Dongyan Zhao",
        "Zilong Zheng"
    ],
    "venue": "arXiv.org",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings and large language models demonstrate the superior performance, speed, and versatility of the proposed LSTP paradigm."
    },
    "citationCount": 3,
    "influentialCitationCount": 1,
    "code": null,
    "description": null,
    "url": null
}