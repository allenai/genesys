{
    "acronym": "cfc076ea80f8a900ddb32b9b6808069eef6e5bb1",
    "title": "Observable Propagation: Uncovering Feature Vectors in Transformers",
    "seed_ids": [
        "gpt2",
        "40c34e85dc4558d26bfef7fb54327dbd4a0bebc3",
        "d078b6e88bc4749f4eb83f289537a88b4aaf54e6",
        "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "9405cc0d6169988371b2755e573cc28650d14dfe"
    ],
    "s2id": "cfc076ea80f8a900ddb32b9b6808069eef6e5bb1",
    "abstract": "A key goal of current mechanistic interpretability research in NLP is to find linear features (also called\"feature vectors\") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called\"observable propagation\"(in short: ObProp), for finding linear features used by transformer language models in computing a given task -- using almost no data. Our paradigm centers on the concept of\"observables\", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the coupling coefficient which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models.",
    "authors": [
        "Jacob Dunefsky",
        "Arman Cohan"
    ],
    "venue": "",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work introduces a novel method, called \"observable propagation\", for finding linear features used by transformer language models in computing a given task -- using almost no data, and suggests that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}