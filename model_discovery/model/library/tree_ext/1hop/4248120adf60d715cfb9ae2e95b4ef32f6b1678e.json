{
    "acronym": "4248120adf60d715cfb9ae2e95b4ef32f6b1678e",
    "title": "Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns",
    "seed_ids": [
        "bigbird",
        "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8",
        "7e9ff94476f41041c75e253e84f487db00e9c861",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
        "2a31319e73d4486716168b65cdf7559baeda18ce"
    ],
    "s2id": "4248120adf60d715cfb9ae2e95b4ef32f6b1678e",
    "abstract": "We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks. Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns\u2014during fine-tuning\u2014different attention patterns for each Transformer layer depending on the downstream task. Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens\u2014for each task and model layer\u2014and focuses attention on those. It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.",
    "authors": [
        "Zihan Wang",
        "Jiuxiang Gu",
        "Jason Kuen",
        "Handong Zhao",
        "Vlad I. Morariu",
        "Ruiyi Zhang",
        "A. Nenkova",
        "Tong Sun",
        "Jingbo Shang"
    ],
    "venue": "Findings",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A novel Adaptive Axis Attention method is proposed, which learns\u2014during fine-tuning\u2014different attention patterns for each Transformer layer depending on the downstream task, and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}