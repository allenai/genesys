{
    "acronym": "e9c9d287e98fa543ad533c14c9267163b01d3fc2",
    "title": "Multi-Document Summarization: A Comparative Evaluation",
    "seed_ids": [
        "longformer",
        "memcompress",
        "a4d581724fb604d13719b73f87a0457f007a43a4",
        "274f903041b1a830b37f57929d837c1706e94ec7",
        "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
        "d25121da56c9050137800c69520111b30201d1ed",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "20777da1b38306a398ba4d2ace5bf25d59051b79"
    ],
    "s2id": "e9c9d287e98fa543ad533c14c9267163b01d3fc2",
    "abstract": "This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS^2 datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pretrained Model LED outperforms PRIMERA and PEGASUS on the MS^2 dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can be utilized on demanding datasets with academically and/or scientifically complex data as well as generalized, relatively simple datasets.",
    "authors": [
        "Kushan Hewapathirana",
        "Nisansa de Silva",
        "C. D. A. D. O. C. ScienceEngineering",
        "University of Moratuwa",
        "Sri Lanka",
        "ConscientAI"
    ],
    "venue": "International Conference on Industrial and Information Systems",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The findings show that the General-Purpose Pretrained Model LED outperforms PRIMERA and PEGASUS on the MS^2 dataset and provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains."
    },
    "citationCount": 1,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}