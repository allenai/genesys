{
    "acronym": "5b06056345034e1559ef8680190cdccc79a2196d",
    "title": "VIMA: Robot Manipulation with Multimodal Prompts",
    "seed_ids": [
        "gpt",
        "979810ca765695a481c37126103b8ba256ee2192",
        "60c8d0619481eaafdd1189af610d0e636271fed5",
        "32c9b3859086d15184989454eb878638659e64c6",
        "15190e8b459bd85d546286f7d7da61b4f4f3f58a",
        "ada81a4de88a6ce474df2e2446ad11fea480616e",
        "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "d8d2e574965fe733eb1416e03df2b5c2914fc530",
        "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "8323c591e119eb09b28b29fd6c7bc76bd889df7a"
    ],
    "s2id": "5b06056345034e1559ef8680190cdccc79a2196d",
    "abstract": "Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts , interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We de-sign a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to 2 . 9 \u00d7 task success rate given the same training data. With 10 \u00d7 less training data, VIMA still performs 2 . 7 \u00d7 better than the best competing variant. Code and video demos are available at vimalabs.github.io .",
    "authors": [
        "Yunfan Jiang",
        "Agrim Gupta",
        "Zichen Zhang",
        "Guanzhi Wang",
        "Yongqiang Dou",
        "Yanjun Chen",
        "Fei-Fei Li",
        "Anima Anandkumar",
        "Yuke Zhu",
        "Linxi (Jim) Fan"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work develops a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization and de-signs a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively."
    },
    "citationCount": 15,
    "influentialCitationCount": 2,
    "code": null,
    "description": null,
    "url": null
}