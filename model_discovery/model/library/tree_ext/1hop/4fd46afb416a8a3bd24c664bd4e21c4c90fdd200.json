{
    "acronym": "4fd46afb416a8a3bd24c664bd4e21c4c90fdd200",
    "title": "Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding",
    "seed_ids": [
        "longformer",
        "6fe5cfbe53f14012766240e4b1fd4af000ecb4ba",
        "925ad2897d1b5decbea320d07e99afa9110e09b2",
        "42f0bae2dacba44e9b5d8f050da3cbe41b9fc437",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "d9f6ada77448664b71128bb19df15765336974a6"
    ],
    "s2id": "4fd46afb416a8a3bd24c664bd4e21c4c90fdd200",
    "abstract": null,
    "authors": [
        "Leibo Liu",
        "\u00d3scar P\u00e9rez",
        "Anthony N. Nguyen",
        "Vicki Bennett",
        "Louisa R Jorm"
    ],
    "venue": "Journal of Biomedical Informatics",
    "year": 2022,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "The experiment results show that the F1 scores of the HiLAT+ClinicalplusXLNet outperform the previous state-of-the-art models for the top-50 most frequent ICD-9 codes from MIMIC-III."
    },
    "citationCount": 21,
    "influentialCitationCount": 3,
    "code": null,
    "description": null,
    "url": null
}