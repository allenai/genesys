{
    "acronym": "c247116801c2d55caa9267d11f0c9fa587f75d89",
    "title": "Contrastive Language-knowledge Graph Pre-training",
    "seed_ids": [
        "bert",
        "f1957038e9ded19108d3c71340d7462152b70f25",
        "710d183174844da5b7f392667f3cc25d2b098dde",
        "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "9405cc0d6169988371b2755e573cc28650d14dfe",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "c247116801c2d55caa9267d11f0c9fa587f75d89",
    "abstract": "Recent years have witnessed a surge of academic interest in knowledge-enhanced pre-trained language models (PLMs) that incorporate factual knowledge to enhance knowledge-driven applications. Nevertheless, existing studies primarily focus on shallow, static, and separately pre-trained entity embeddings, with few delving into the potential of deep contextualized knowledge representation for knowledge incorporation. Consequently, the performance gains of such models remain limited. In this article, we introduce a simple yet effective knowledge-enhanced model, College (Contrastive Language-Knowledge Graph Pre-training), which leverages contrastive learning to incorporate factual knowledge into PLMs. This approach maintains the knowledge in its original graph structure to provide the most available information and circumvents the issue of heterogeneous embedding fusion. Experimental results demonstrate that our approach achieves more effective results on several knowledge-intensive tasks compared to previous state-of-the-art methods. Our code and trained models are available at https://github.com/Stacy027/COLLEGE.",
    "authors": [
        "Xiaowei Yuan",
        "Kang Liu",
        "Yequan Wang"
    ],
    "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
    "year": 2024,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "A simple yet effective knowledge-enhanced model, College (Contrastive Language-Knowledge Graph Pre-training), which leverages contrastive learning to incorporate factual knowledge into PLMs and circumvents the issue of heterogeneous embedding fusion."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}