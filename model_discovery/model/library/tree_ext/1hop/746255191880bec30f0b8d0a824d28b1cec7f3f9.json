{
    "acronym": "746255191880bec30f0b8d0a824d28b1cec7f3f9",
    "title": "How to Prune Your Language Model: Recovering Accuracy on the \"Sparsity May Cry\" Benchmark",
    "seed_ids": [
        "bert",
        "92e121c6e114fe3cfb89370df03847c66a9b4e28",
        "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47"
    ],
    "s2id": "746255191880bec30f0b8d0a824d28b1cec7f3f9",
    "abstract": "Pruning large language models (LLMs) from the BERT family has emerged as a standard compression benchmark, and several pruning methods have been proposed for this task. The recent ``Sparsity May Cry'' (SMC) benchmark put into question the validity of all existing methods, exhibiting a more complex setup where many known pruning methods appear to fail. We revisit the question of accurate BERT-pruning during fine-tuning on downstream datasets, and propose a set of general guidelines for successful pruning, even on the challenging SMC benchmark. First, we perform a cost-vs-benefits analysis of pruning model components, such as the embeddings and the classification head; second, we provide a simple-yet-general way of scaling training, sparsification and learning rate schedules relative to the desired target sparsity; finally, we investigate the importance of proper parametrization for Knowledge Distillation in the context of LLMs. Our simple insights lead to state-of-the-art results, both on classic BERT-pruning benchmarks, as well as on the SMC benchmark, showing that even classic gradual magnitude pruning (GMP) can yield competitive results, with the right approach.",
    "authors": [
        "Eldar Kurtic",
        "Torsten Hoefler",
        "Dan Alistarh"
    ],
    "venue": "arXiv.org",
    "year": 2023,
    "tldr": {
        "model": "tldr@v2.0.0",
        "text": "This work revisits the question of accurate BERT-pruning during fine-tuning on downstream datasets, and proposes a set of general guidelines for successful pruning, even on the challenging SMC benchmark."
    },
    "citationCount": 0,
    "influentialCitationCount": 0,
    "code": null,
    "description": null,
    "url": null
}