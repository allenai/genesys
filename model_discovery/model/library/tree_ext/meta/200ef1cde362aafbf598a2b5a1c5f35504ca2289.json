{"paperId": "200ef1cde362aafbf598a2b5a1c5f35504ca2289", "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design", "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various vision tasks. However, ViTs\u2019 self-attention module is still arguably a major bottleneck, limiting their achievable hardware efficiency and more extensive applications to resource constrained platforms. Meanwhile, existing accelerators dedicated to NLP Transformers are not optimal for ViTs. This is because there is a large difference between ViTs and Transformers for natural language processing (NLP) tasks: ViTs have a relatively fixed number of input tokens, whose attention maps can be pruned by up to 90% even with fixed sparse patterns, without severely hurting the model accuracy (e.g., <=1.5% under 90% pruning ratio); while NLP Transformers need to handle input sequences of varying numbers of tokens and rely on on-the-fly predictions of dynamic sparse attention patterns for each input to achieve a decent sparsity (e.g., >=50%). To this end, we propose a dedicated algorithm and accelerator co-design framework dubbed ViTCoD for accelerating ViTs. Specifically, on the algorithm level, ViTCoD prunes and polarizes the attention maps to have either denser or sparser fixed patterns for regularizing two levels of workloads without hurting the accuracy, largely reducing the attention computations while leaving room for alleviating the remaining dominant data movements; on top of that, we further integrate a lightweight and learnable auto-encoder module to enable trading the dominant high-cost data movements for lower-cost computations. On the hardware level, we develop a dedicated accelerator to simultaneously coordinate the aforementioned enforced denser and sparser workloads for boosted hardware utilization, while integrating on-chip encoder and decoder engines to leverage ViTCoD\u2019s algorithm pipeline for much reduced data movements. Extensive experiments and ablation studies validate that ViTCoD largely reduces the dominant data movement costs, achieving speedups of up to 235.3\u00d7, 142.9\u00d7, 86.0\u00d7, 10.1\u00d7, and 6.8\u00d7 over general computing platforms CPUs, EdgeGPUs, GPUs, and prior-art Transformer accelerators SpAtten and Sanger under an attention sparsity of 90%, respectively. Our code implementation is available at https://github.com/GATECH-EIC/ViTCoD.", "venue": "International Symposium on High-Performance Computer Architecture", "year": 2022, "citationCount": 38, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.09573", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A dedicated algorithm and accelerator co-design framework dubbed ViTCoD for accelerating ViTs and develops a dedicated accelerator to simultaneously coordinate the aforementioned enforced denser and sparser workloads for boosted hardware utilization, while integrating on-chip encoder and decoder engines to leverage ViTCs algorithm pipeline for much reduced data movements."}, "embedding": {"model": "specter_v2", "vector": [0.7215093374252319, 0.4852980375289917, -0.38422301411628723, 0.46185794472694397, -0.12075953185558319, -0.19377590715885162, 0.7288986444473267, 0.014815681613981724, -0.5811399817466736, -0.7923375368118286, 0.41958513855934143, -0.3305080533027649, 0.7722659111022949, 0.06493022292852402, -0.03210853040218353, 0.17986828088760376, -0.19877135753631592, 0.13552039861679077, 0.04751924052834511, -0.4447159767150879, 0.1516045778989792, -0.5929399132728577, -1.5347577333450317, 0.0055720810778439045, 0.49353182315826416, 0.8399409055709839, 0.4843710958957672, 0.9425156712532043, -0.5746247172355652, 0.347027987241745, 0.5552712082862854, 0.015419156290590763, 0.4145675003528595, 0.28228873014450073, -0.20688052475452423, -0.1695387214422226, 0.6378628015518188, -0.21619640290737152, -0.5715866088867188, 1.0365046262741089, -0.22330118715763092, 0.24311500787734985, 0.48612499237060547, -0.7197929620742798, -0.6562583446502686, 0.29389500617980957, 0.32107850909233093, 0.8729555010795593, -0.5758872628211975, -0.02049405872821808, 1.2539058923721313, -1.2781517505645752, 0.11716608703136444, 1.0223904848098755, 0.6438214182853699, 0.34331613779067993, 0.04172030836343765, -0.39427444338798523, 0.4863313138484955, 0.21479766070842743, -0.3784032166004181, -0.6235607266426086, -0.10619056224822998, -0.19162617623806, 2.230433225631714, -0.5550869107246399, 0.3155589699745178, 0.5342603921890259, 0.14765049517154694, 1.4703375101089478, -0.28820788860321045, -0.9619793891906738, -0.14347977936267853, -0.23153604567050934, 0.43486347794532776, 0.9348359107971191, -0.16381102800369263, 0.37952885031700134, -1.0204399824142456, 0.03149927407503128, 0.22375988960266113, 0.09870550036430359, 0.485865980386734, -0.016214484348893166, -0.12285465747117996, 0.8543189764022827, 0.5944848656654358, 0.8877968192100525, -0.25853434205055237, 0.7641993761062622, 0.9711805582046509, -0.08358611166477203, -0.2541409134864807, 0.27257096767425537, 0.09343381971120834, 0.3433506488800049, -0.8899062871932983, 0.13223896920681, -0.3522810637950897, 1.160618782043457, -0.32103222608566284, 0.6080330014228821, -1.0295213460922241, 0.09389300644397736, 1.081394076347351, 0.5294541120529175, 0.5919354557991028, -0.3367560803890228, 0.2633953392505646, -0.6360005736351013, -0.18929041922092438, -0.42547568678855896, -0.11484560370445251, -0.2750311493873596, -1.1587231159210205, -1.0243604183197021, -0.5311334133148193, 0.4456382691860199, -1.3658407926559448, 0.45437657833099365, -0.2684214413166046, 0.023957177996635437, 0.14240607619285583, 0.3262900114059448, 0.8277480602264404, 0.5396653413772583, 0.21937055885791779, 0.18770305812358856, 1.6339178085327148, -1.3225120306015015, -0.5229361057281494, -1.2941240072250366, 0.3065740168094635, -0.465964674949646, 0.5381426215171814, -0.06888418644666672, -1.3344027996063232, -1.188051700592041, -0.6595375537872314, -0.6584422588348389, -0.46696656942367554, 0.3302725553512573, 1.0808061361312866, 0.25987422466278076, -1.1651593446731567, 0.377822607755661, -0.29407620429992676, -0.2354298233985901, 0.6562981009483337, 0.024768631905317307, 0.7056437134742737, -0.2527156174182892, -1.1101118326187134, 0.11145935952663422, -0.10888750106096268, -0.5038956999778748, -0.4430348575115204, -0.7107409834861755, -1.3359780311584473, 0.41846463084220886, 0.05585767701268196, -0.7510318160057068, 1.4530586004257202, -0.15948152542114258, -0.8727683424949646, 0.833106279373169, -0.6614289283752441, 0.032420482486486435, -0.01151387207210064, 0.05630243942141533, -0.1922980546951294, -0.34813281893730164, 0.12604734301567078, 0.6805972456932068, 1.0676323175430298, 0.2913123667240143, -0.3623548150062561, 0.2743063271045685, -0.4883909225463867, -0.10292936116456985, -0.6199995875358582, 0.8139005303382874, -0.6860609650611877, -0.16739404201507568, 0.2130834311246872, 0.6144557595252991, -0.3744421601295471, -0.1868341565132141, -0.4112524092197418, -1.0691051483154297, 0.6944609880447388, 0.2547573149204254, 0.619133472442627, -1.117403507232666, -1.02942955493927, -0.25864315032958984, 0.2518562376499176, 0.20427356660366058, -0.7787109613418579, 0.5202688574790955, -0.2706541419029236, -0.1444476842880249, 0.20283640921115875, -1.1039670705795288, 0.18450689315795898, -0.3140007555484772, -1.0290961265563965, -0.3483431935310364, 0.034587882459163666, 1.2656221389770508, -0.9055803418159485, -0.10506216436624527, -0.07584912329912186, 0.4461315870285034, -1.142505168914795, 0.6996960043907166, -0.6279512047767639, -0.14718516170978546, 0.08569962531328201, 0.1939978003501892, -0.07745654881000519, -0.4849175810813904, 0.5099679231643677, -0.8451932072639465, -0.2539348006248474, 0.3749215602874756, 0.231907457113266, 1.384934425354004, -0.34830278158187866, 0.7227723002433777, -0.04293275251984596, -0.7781495451927185, 0.36045387387275696, 0.25490280985832214, -0.39352482557296753, -0.8965787291526794, 0.5325131416320801, 0.24915991723537445, -0.6715993285179138, 0.13024447858333588, 1.2797000408172607, 1.0628235340118408, -0.32185235619544983, -0.18517738580703735, 0.4738316833972931, -0.15674099326133728, 0.4441622793674469, 0.546269416809082, 1.0039247274398804, 0.15006023645401, 0.47011247277259827, -0.048448070883750916, 0.24030983448028564, -0.5507017374038696, -0.02543093077838421, 0.9251441955566406, 0.48780766129493713, 0.5000296235084534, 0.26810595393180847, -0.8986778259277344, -0.49226492643356323, 0.1744714379310608, 0.6091312766075134, 1.376812219619751, -0.3015504479408264, -0.07025431096553802, -0.5404440760612488, -0.3432687819004059, -0.4658133387565613, -0.34589332342147827, -0.05113285034894943, -0.06033223122358322, -0.14842796325683594, -0.7663651704788208, 0.8797516226768494, 0.23500053584575653, 0.9841326475143433, -1.0945340394973755, -0.733135461807251, -0.5146315097808838, 0.1769775152206421, -1.2072521448135376, -0.6711961627006531, 0.48664140701293945, -0.4657984972000122, -0.10187714546918869, 0.33720940351486206, -0.43065452575683594, 0.32725149393081665, -0.14872536063194275, 1.0102399587631226, -0.4394322335720062, -0.7971097826957703, 0.13449303805828094, 0.3654419779777527, -0.5934292078018188, -0.6051031351089478, 0.2983117997646332, 0.051149532198905945, -0.5163520574569702, 0.4362197518348694, 0.21814216673374176, -0.12569637596607208, -0.5818862318992615, -0.31483349204063416, 0.013590091839432716, 0.2652339041233063, 0.2822510600090027, 0.903209388256073, -0.4939843416213989, -0.32249772548675537, -0.9509967565536499, 0.7797773480415344, 0.2824834883213043, -0.582243025302887, -0.08361133188009262, -0.8427718877792358, -0.3302546739578247, 0.7440101504325867, -0.6394485831260681, -0.12064310163259506, -0.4140968322753906, 0.2821884751319885, -0.8024543523788452, -0.29619547724723816, -0.04252242296934128, 0.46119746565818787, 0.028681017458438873, 0.48822182416915894, 0.9876968860626221, 0.2130490094423294, 0.4132918119430542, 0.56707763671875, -0.7202959060668945, 0.7852407693862915, 0.3317019045352936, 0.12331875413656235, -0.05436593294143677, -0.05892065539956093, -0.96999192237854, -0.612619161605835, -0.26923301815986633, 0.1559431105852127, -0.28736209869384766, 0.25190114974975586, -0.6105284690856934, -1.163689374923706, -0.16514046490192413, -1.2436021566390991, -0.12454358488321304, -0.06688208878040314, -0.6269783973693848, -0.24300627410411835, -0.797240138053894, -1.0094764232635498, -0.5378843545913696, -1.4991799592971802, -1.2216564416885376, 0.655653178691864, 0.3123301565647125, -0.16927149891853333, -0.3688744902610779, -0.05019195005297661, -0.2892526388168335, 0.8899979591369629, -0.6491119265556335, 0.5849979519844055, 0.021455910056829453, -0.2595571279525757, 0.07578586786985397, -0.16641248762607574, 0.27281931042671204, -0.47960004210472107, 0.28991496562957764, -1.1753203868865967, 0.28139573335647583, -0.5043302774429321, -0.3572979271411896, 0.5993481874465942, 0.5425661206245422, 0.5528177618980408, -0.02711157500743866, -0.6580856442451477, 0.6237292885780334, 1.6048517227172852, -0.4927210211753845, 0.18012931942939758, 0.06435742229223251, 1.0724910497665405, -0.3474053740501404, -0.14553317427635193, 0.6525986194610596, 0.26040560007095337, 0.4331152141094208, 0.35176390409469604, -0.33781200647354126, -0.2538087069988251, -0.40959253907203674, 0.5678818821907043, 1.226945161819458, 0.7883745431900024, -0.1898956298828125, -1.11992609500885, 0.8311243057250977, -1.1939994096755981, -0.557671308517456, 0.2794206142425537, 0.21592433750629425, 0.21681389212608337, -0.18417136371135712, -0.4270857572555542, -0.006256090477108955, 0.26786887645721436, 0.5874868631362915, -0.7868226766586304, -1.2210811376571655, 0.2712162137031555, 0.6558403968811035, 0.3437560498714447, 0.41317248344421387, -0.353733092546463, 1.0266746282577515, 14.603503227233887, 1.0484079122543335, -0.15843191742897034, 0.5315985679626465, 0.3521249294281006, 0.24534493684768677, -0.21681153774261475, -0.09481070190668106, -1.3132771253585815, -0.37367361783981323, 0.7865660786628723, 0.36162298917770386, 0.5995078682899475, 0.3803612291812897, -0.017428498715162277, 0.22606375813484192, -0.30779123306274414, 0.7307177186012268, 0.6098672151565552, -1.6426810026168823, 0.3891773819923401, 0.2070881724357605, 0.174237921833992, 0.86561119556427, 1.041225790977478, 0.8079392910003662, 0.5937421321868896, -0.16940493881702423, 0.4814298152923584, 0.07117640972137451, 1.1621716022491455, 0.25002244114875793, 0.043781112879514694, 0.5089883804321289, -1.1200841665267944, 0.05758940055966377, -0.6186560988426208, -1.3362233638763428, 0.07244957238435745, 0.3971443474292755, -0.723172128200531, -0.5885087847709656, -0.24352438747882843, 0.9971846342086792, 0.30624258518218994, 0.08306851238012314, -0.20562158524990082, 0.42860954999923706, -0.09086931496858597, -0.010975074023008347, 0.5609710216522217, 0.6514114737510681, 0.1028498113155365, 0.109885573387146, -0.005696459673345089, -0.08758137375116348, 0.2122003734111786, 0.5457091331481934, -0.4921450614929199, -0.4508056640625, -0.47010698914527893, -0.2739032804965973, -0.07475659251213074, 1.1658746004104614, 0.11540007591247559, 0.35262686014175415, -0.4043855369091034, 0.3218911290168762, 0.5650691986083984, -0.014581568539142609, -0.4096314013004303, -0.1364390105009079, 0.3888508379459381, -0.935646116733551, 0.4551539719104767, 0.5812846422195435, -0.500434935092926, -0.6950474381446838, -0.7698914408683777, -0.6141997575759888, 0.22751905024051666, -0.7064026594161987, -0.4674203097820282, 0.7750623822212219, -0.7785394191741943, -0.2607249915599823, 0.34946659207344055, -0.762660562992096, -0.4026094079017639, 0.4127475917339325, -1.6580134630203247, -0.9558678269386292, 0.30732598900794983, -0.33217066526412964, -0.1361386924982071, -0.2520141005516052, 1.185018539428711, -0.03084125928580761, -0.13940562307834625, 0.266886830329895, -0.14478281140327454, -0.12826773524284363, -0.5928582549095154, -0.5038055181503296, 1.41981840133667, 0.5451211333274841, 0.14992168545722961, -0.16563570499420166, 0.36753544211387634, 0.22272750735282898, -1.184474229812622, -0.22066834568977356, 0.7770381569862366, -0.5188626646995544, -0.3776881992816925, -0.8981763124465942, -0.5743889808654785, 0.2538418471813202, 0.5101914405822754, -0.03406349569559097, 0.05318901315331459, 0.30758002400398254, -0.7992700338363647, 0.010397897101938725, -0.8017529845237732, 0.21231038868427277, 0.6404377818107605, -0.9667069911956787, -0.17718902230262756, 0.1518896222114563, 0.46590885519981384, -1.4101519584655762, -0.09738265722990036, -0.33158138394355774, 0.3495629131793976, -0.14110787212848663, 1.2985332012176514, 0.013645937666296959, 1.1193623542785645, 0.8759950995445251, 0.10557786375284195, -0.3613812029361725, -0.051108911633491516, -0.7165053486824036, -0.1765676587820053, 0.052737556397914886, 0.25236329436302185, -0.438555508852005, 0.6178069114685059, 0.5729392170906067, -0.009337992407381535, -0.403954416513443, -0.7374835014343262, -0.19671949744224548, -0.32474127411842346, -0.6867667436599731, 0.2256753295660019, -0.24691061675548553, 0.1179879829287529, 0.3312813341617584, 0.38810455799102783, 0.6204366087913513, -0.00624690018594265, -0.25214868783950806, 0.2697739005088806, 0.005649290978908539, -0.10660874098539352, -0.6372946500778198, -0.5568010807037354, -1.3937450647354126, -0.10366113483905792, -1.3219518661499023, 0.03604838252067566, -0.851375162601471, -0.48356005549430847, 0.054106708616018295, -0.04319902881979942, 0.6549860239028931, 0.34895941615104675, -0.15525296330451965, -0.24994760751724243, -0.5692589282989502, -0.43240758776664734, 0.9875893592834473, 0.983267068862915, -0.9315773844718933, 0.04443870112299919, -0.3669949173927307, -0.258535236120224, 0.33123084902763367, 0.24245475232601166, -0.32005155086517334, -0.736585795879364, -1.2991656064987183, 0.5205787420272827, -0.030182311311364174, -0.06880033016204834, -0.7580548524856567, 1.0334773063659668, 0.5288538336753845, -0.3454671800136566, -0.09273669868707657, 0.1483631730079651, -0.7095733880996704, -0.9983239769935608, 0.5762924551963806, -0.5084137916564941, 0.24953490495681763, 0.7633371949195862, -0.6791805028915405, -0.03227332979440689, 0.5864169597625732, -0.20321212708950043, -0.7700185179710388, -1.049367904663086, 0.5125958323478699, -0.5180654525756836, 0.3123324513435364, -0.47612807154655457, -0.11726740002632141, -1.3154096603393555, -0.11931750923395157, 0.28928127884864807, 0.3682299554347992, -0.6042609214782715, 0.7632236480712891, 0.5851408243179321, -0.8881403803825378, 0.3129066228866577, 0.566069483757019, -0.2074912041425705, 0.34585654735565186, 0.5030166506767273, 0.28996336460113525, -0.27361100912094116, 0.6852401494979858, 0.19639433920383453, 0.14982585608959198, -0.9294219613075256, 0.387933611869812, 0.5078890919685364, -0.7214345335960388, -0.4043363630771637, 0.8528178334236145, -0.16126742959022522, -0.7638156414031982, -0.03327314183115959, -1.4012669324874878, -0.5613256096839905, -0.24649980664253235, 0.589596688747406, -0.19289550185203552, 0.17039965093135834, 0.25055402517318726, -0.7017377018928528, 0.18240182101726532, -0.3289988040924072, -0.40401825308799744, 0.3621564507484436, 0.27028006315231323, -0.13318660855293274, -0.08070886135101318, 0.40585869550704956, -0.760846734046936, -0.7065799832344055, -0.9292068481445312, -0.6886684894561768, 0.07653769850730896, 0.7092912793159485, -0.16635043919086456, -0.8690237402915955, 0.7394518256187439, 0.3766099512577057, 0.40603747963905334, 0.445333868265152, -0.23031620681285858, 0.522804319858551, 0.6686044335365295, -0.15176662802696228, -0.46122539043426514, -0.7186797857284546, 1.7500890493392944, 1.0293292999267578, -0.8007380962371826, 0.26874610781669617, -0.07744575291872025, -0.6505545973777771, 0.685896635055542, 0.24369259178638458, -0.18367642164230347, 0.6559227705001831, 0.17797885835170746, -0.1361275166273117, -0.06347472220659256, -0.911472499370575, -0.6060803532600403, 0.7454677820205688, 1.0320149660110474, 0.5789425373077393, -0.03334539756178856, 0.19657684862613678, 0.5990315079689026, 0.12514320015907288, 0.319841206073761, 0.22742871940135956, 0.37408915162086487, 0.11765563488006592, 0.025348801165819168, -0.060160234570503235, 0.7423415780067444, -0.3774266541004181, -1.1795384883880615, 0.6541252136230469, 0.37499260902404785, 0.33424240350723267, 0.4357302784919739, 1.0876678228378296, 0.028475375846028328, 0.6775179505348206, -0.1678941547870636, 0.5728651285171509, -0.5057123303413391, -0.4398762285709381, -0.21908089518547058, -0.9216604828834534, -0.12583842873573303, -0.02573155239224434, -0.6838205456733704, -0.4055156409740448, -0.14877308905124664, 0.49688032269477844, -0.3784409165382385, 0.42563554644584656, 1.0375174283981323, 0.8338203430175781, 0.8896204233169556, -0.48640504479408264, -0.6745539903640747, -0.2152377814054489, -0.6570734977722168, 0.33940786123275757, -0.855417013168335, 0.03120800107717514, -0.0019262094283476472, -0.05294083431363106, -0.2194012701511383]}, "authors": [{"authorId": "47113848", "name": "Haoran You"}, {"authorId": "2118548098", "name": "Zhanyi Sun"}, {"authorId": "30984015", "name": "Huihong Shi"}, {"authorId": "10385489", "name": "Zhongzhi Yu"}, {"authorId": "2118834108", "name": "Yang Zhao"}, {"authorId": "2108455959", "name": "Yongan Zhang"}, {"authorId": "28987646", "name": "Chaojian Li"}, {"authorId": "2145518289", "name": "Baopu Li"}, {"authorId": "3138925", "name": "Yingyan Lin"}], "references": [{"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "3be4cd7626e2be58849fcbd5edc2a4e8cfc08158", "title": "VAQF: Fully Automatic Software-hardware Co-design Framework for Low-bit Vision Transformer"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "d291149a75d7ac194382bd61e515eb40ed0aa106", "title": "Predicting Attention Sparsity in Transformers"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "100f2e2a810394503472f50938522930bd07b834", "title": "Rethinking the Self-Attention in Vision Transformers"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "ea76c776b3cd3ad448572999ad3b4323abe47453", "title": "Gamma: leveraging Gustavson\u2019s algorithm to accelerate sparse matrix multiplication"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "3063b14c89886d2dd2da18cc37d024178831034d", "title": "Exploiting Temporal Contexts With Strided Transformer for 3D Human Pose Estimation"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "44b67c61fea56e7b132a447d19a4cdb064b42470", "title": "MatRaptor: A Sparse-Sparse Matrix Multiplication Accelerator Based on Row-Wise Product"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "a0185d4f32dde88aa1749f3a8000ed4721787b65", "title": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision"}, {"paperId": "ac8af4202f8ac179e727013929a95f6ecc4936e8", "title": "FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function"}, {"paperId": "0449c88867dec952945fc99749cc656df25c38b2", "title": "MobileDets: Searching for Object Detection Architectures for Mobile Accelerators"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "fb93ca1e004cbdcb93c8ffc57357189fa4eb6770", "title": "ResNeSt: Split-Attention Networks"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "b1c39d042fdf8f00a407b0df734764beb6c3b062", "title": "Low-Rank Bottleneck in Multi-head Attention Models"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345", "title": "SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training"}, {"paperId": "7af3992959ff24e369da7c514c041062f5249b1e", "title": "SpArch: Efficient Architecture for Sparse Matrix Multiplication"}, {"paperId": "7d32ff65fb0e144ffdfc12540a0207c60f38df9c", "title": "Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "232d04b1cc0e9ad535a35e00aaa80360ec3e72c4", "title": "ExTensor: An Accelerator for Sparse Tensor Algebra"}, {"paperId": "5e19eba1e6644f7c83f607383d256deea71f87ae", "title": "Searching for MobileNetV3"}, {"paperId": "27ac832ee83d8b5386917998a171a0257e2151e2", "title": "Attention Augmented Convolutional Networks"}, {"paperId": "a821a00c54749a69a9d9fe0a7411de590d247715", "title": "DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs"}, {"paperId": "061edcbe54d23e6c8688525503ebc643c30d44a0", "title": "OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c1fe35e3ca313c0308d722366181f617c7fd9c11", "title": "Maximizing CNN accelerator efficiency through resource partitioning"}, {"paperId": "bac377d3a051899dbe0d7249ed5d3d0b22d57310", "title": "Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "deace80ef638b4987bd473cbbf28bde73e15bde5", "title": "\u57fa\u4e8eNVIDIA Jetson TX2\u7684\u9053\u8def\u573a\u666f\u5206\u5272"}, {"paperId": null, "title": "Synopsys design compiler . \u201d [ Online ] Synopsys ic compiler ii . \u201d [ Online ]"}, {"paperId": null, "title": "\u201cSynopsys ic compiler ii.\u201d"}]}