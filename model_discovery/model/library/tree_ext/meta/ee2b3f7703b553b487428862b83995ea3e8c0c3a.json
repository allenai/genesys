{"paperId": "ee2b3f7703b553b487428862b83995ea3e8c0c3a", "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers", "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning."}, "embedding": {"model": "specter_v2", "vector": [0.11187523603439331, 0.7142021656036377, -0.15464700758457184, -0.32162418961524963, -0.38059866428375244, -0.11939048767089844, 0.4309540092945099, -0.16092899441719055, 0.2645919919013977, -0.34172123670578003, 0.3997824192047119, 0.23161382973194122, 0.4627709984779358, 0.17057077586650848, -0.2944429814815521, 0.293002188205719, -1.0835368633270264, 0.47069624066352844, 0.22983181476593018, -0.5689151883125305, 0.001252995803952217, -0.9318503141403198, -1.0948352813720703, 0.0646677315235138, 0.5559023022651672, 0.7638675570487976, 0.37924501299858093, 0.5698773264884949, -0.6008247137069702, 0.3333154320716858, 0.14499470591545105, -0.5785148739814758, 0.14799116551876068, -0.2509121894836426, -0.28598925471305847, -0.4574041962623596, 0.7153936624526978, -0.07945723086595535, -0.24254333972930908, 0.6228656768798828, -0.10368282347917557, 0.13661658763885498, 0.4377272129058838, -0.4926404058933258, -0.5080143809318542, 0.9620466232299805, 0.542072057723999, 0.7744598984718323, -0.3096556067466736, -0.7910416722297668, 1.7167216539382935, -1.4807703495025635, -0.04304882884025574, 1.4764970541000366, 0.3448874354362488, -0.0038401924539357424, -0.2033703327178955, -0.6408734321594238, 1.146880865097046, 0.34444761276245117, -0.9409410357475281, -0.6740615963935852, 0.158429354429245, 0.0004955973709002137, 1.9796746969223022, -0.18177427351474762, 0.3715139627456665, 0.41381892561912537, -0.07745981961488724, 1.4737308025360107, -0.4469199478626251, -0.7015365958213806, -0.18675407767295837, 0.01861560344696045, 0.2773478925228119, 0.6431553363800049, -0.58176589012146, 0.07573191076517105, -1.147377848625183, -0.33120590448379517, 0.3979727029800415, -0.4038049280643463, 0.3298584818840027, -0.2241022139787674, 0.1902637630701065, 0.9908744096755981, 0.3137171268463135, 0.9123489856719971, -0.23688563704490662, 0.7796715497970581, 0.4031776785850525, 0.3748590648174286, 0.07784228771924973, 0.4070444703102112, -0.33218660950660706, 0.2651267945766449, -1.2157431840896606, 0.25725695490837097, -0.16124361753463745, 1.0416538715362549, -0.01958242803812027, 0.784967303276062, -0.7228115200996399, 0.46020087599754333, 1.3410478830337524, 0.2815951406955719, 0.7626502513885498, -0.7941810488700867, 0.21348349750041962, -0.8270514607429504, -0.2842249572277069, -0.9746819138526917, 0.09490383416414261, -0.336906373500824, -0.5922053456306458, -1.5950701236724854, -0.5558801889419556, 0.2585992217063904, -0.42922431230545044, 0.947415292263031, -0.019001396372914314, 0.4644792973995209, -0.13481304049491882, 0.3988788425922394, 0.5273508429527283, 0.8578406572341919, 0.09537161886692047, -0.08898403495550156, 0.9618155360221863, -0.9170924425125122, -0.5530369877815247, -1.2900789976119995, 0.764704167842865, -0.10699670016765594, 0.223077192902565, -0.15577246248722076, -1.2927428483963013, -0.7136860489845276, -0.47576865553855896, 0.05064268410205841, -0.21299058198928833, 0.25171464681625366, 1.07024085521698, 0.211416557431221, -1.109385371208191, 0.5474877953529358, -0.0694005936384201, 0.02946186624467373, 0.32170942425727844, 0.18388870358467102, 0.37228915095329285, -0.22001947462558746, -1.7795923948287964, 0.23240035772323608, -0.11680155247449875, -0.19045597314834595, 0.011863553896546364, -0.8533223867416382, -1.4521759748458862, 0.05153774470090866, 0.4911327660083771, -0.12521833181381226, 1.3937273025512695, -0.0902123749256134, -1.5072964429855347, 0.5533043146133423, -0.6178441643714905, -0.03807742893695831, -0.3474322259426117, -0.2839578688144684, -0.47448763251304626, -0.5460007786750793, 0.002209726022556424, 0.12686103582382202, 0.8044321537017822, 0.19643962383270264, -0.10529451817274094, 0.0064325896091759205, -0.5934386253356934, -0.11186864227056503, -0.16098040342330933, 1.0384360551834106, -0.44795480370521545, -0.17104142904281616, 0.13816653192043304, 0.5585566759109497, -0.04530807211995125, -0.6126649975776672, -0.192998468875885, -1.2369425296783447, 0.6366040706634521, -0.1424354761838913, 1.2478975057601929, -0.8286910653114319, -0.7341046929359436, -0.13575953245162964, -0.3382064998149872, 0.06794577091932297, -0.7597415447235107, 0.6025264859199524, -0.46776628494262695, 0.12083602696657181, -0.08947335928678513, -0.9126572608947754, -0.12236687541007996, -0.10467315465211868, -0.9970194697380066, -0.3668726682662964, 0.36920759081840515, 0.9804312586784363, -1.036234974861145, -0.2670678496360779, -0.053588833659887314, 0.2965741455554962, -1.0397461652755737, 1.4003021717071533, -0.356355756521225, -0.041325461119413376, -0.1949433535337448, -0.04429025575518608, -0.14165137708187103, -0.37947845458984375, 0.3533152937889099, -0.1931842565536499, -0.201022669672966, 0.727704644203186, -0.2690331041812897, 1.2851499319076538, -0.4929395914077759, 0.39245423674583435, -0.22993841767311096, -0.7851020097732544, 0.40754827857017517, 0.3141651153564453, -0.17162254452705383, -0.5713554620742798, -0.19790448248386383, 0.439125120639801, -0.7820004820823669, 0.20477794110774994, 0.537748396396637, 0.9318086504936218, -0.4259987771511078, -0.29260244965553284, 0.6288754940032959, -0.4761686325073242, 0.39354372024536133, 0.5168988108634949, 0.7522667646408081, 0.3969991207122803, 0.7268429398536682, -0.06052197888493538, 0.32514336705207825, -0.7846758961677551, 0.1401997059583664, 0.5254958868026733, 0.7750622034072876, 0.7488164901733398, 0.417316734790802, -0.7277854681015015, -0.13196241855621338, 0.31790047883987427, 0.6625451445579529, 1.704210638999939, -0.3963511288166046, -0.27631962299346924, -0.349731981754303, -0.04863009601831436, -0.2237582802772522, 0.21407614648342133, -0.37278783321380615, 0.043786127120256424, -0.8468431830406189, -0.7074275612831116, 0.523423969745636, 0.15317797660827637, 0.4492379426956177, -0.8497174382209778, -0.10322554409503937, 0.04770886152982712, 0.2752985954284668, -0.7094129323959351, -0.7627028822898865, 0.41891244053840637, -0.36547768115997314, 0.15466788411140442, 0.0066460431553423405, -0.19796016812324524, -0.19273611903190613, -0.7554823756217957, 1.1897499561309814, -0.6341313719749451, -0.2680833339691162, -0.023713786154985428, 0.3995307683944702, -0.7070830464363098, -0.2833980917930603, 0.3691572844982147, 0.2265712022781372, -0.11840406805276871, 0.23174221813678741, 0.15181349217891693, 0.12643186748027802, -0.1995873600244522, 0.18980976939201355, -0.18673349916934967, 0.1520681083202362, -0.0048218038864433765, 0.7023200392723083, -0.41093751788139343, 0.08054991066455841, -1.2762757539749146, 0.34881535172462463, -0.0653342753648758, -0.8447432518005371, 0.08514555543661118, -0.7267734408378601, -0.47717365622520447, 0.589084267616272, -0.5352756977081299, -0.23707641661167145, -0.6051680445671082, 0.43652409315109253, -0.35222122073173523, -0.06367543339729309, 0.3216339647769928, 0.2860083281993866, 0.1819027215242386, -0.022203730419278145, 0.7600851655006409, 0.22502487897872925, -0.031266842037439346, 0.6279780268669128, -0.8964707851409912, 0.47543421387672424, 0.26164647936820984, 0.23647820949554443, -0.335296094417572, -0.2854396402835846, -1.2159254550933838, -0.6661340594291687, -0.4599055051803589, 0.056062232702970505, -0.04780939221382141, 0.1494249403476715, -0.656738817691803, -0.6169466972351074, -0.13602368533611298, -1.0982534885406494, -0.2568686306476593, 0.02573663555085659, -0.4058217406272888, -0.10792655497789383, -1.0701431035995483, -1.196882963180542, -0.892290472984314, -0.7539204955101013, -0.6556974053382874, 0.7471303939819336, -0.030185777693986893, -0.36173245310783386, -0.46364516019821167, 0.23257499933242798, -0.6563272476196289, 1.1892958879470825, -0.9758099317550659, 0.6236372590065002, -0.14216554164886475, -0.4844297170639038, -0.17884191870689392, 0.46951523423194885, 0.2767338752746582, -0.1129411980509758, 0.1073555126786232, -0.583949089050293, 0.350116103887558, -0.49810487031936646, -0.19405870139598846, -0.06821417808532715, 0.7002833485603333, 0.6473838686943054, -0.1787337213754654, -0.40369516611099243, 0.556998074054718, 1.3415861129760742, -0.5183489918708801, 0.05069822445511818, 0.040707267820835114, 1.0788826942443848, 0.3610762059688568, -0.013787119649350643, 0.6452972292900085, 0.643609344959259, 0.7686411738395691, 0.4742528200149536, 0.09966598451137543, 0.25318464636802673, -0.4022194445133209, 0.6914176344871521, 1.9777952432632446, 0.30302664637565613, 0.19631654024124146, -0.7551384568214417, 0.93398517370224, -1.1187695264816284, -0.9362297058105469, 0.4514067471027374, 0.6077113747596741, 0.46224111318588257, -0.8880975842475891, -0.2754826247692108, -0.565036952495575, 0.17104797065258026, 0.5799099802970886, -0.3694048821926117, -0.8127985596656799, -0.14306612312793732, 0.6240924000740051, -0.20515790581703186, 0.5824865698814392, -0.1715320646762848, 0.9058669209480286, 14.988537788391113, 0.5564541816711426, -0.03070939891040325, 0.4700324237346649, 0.6565569043159485, 0.1491057276725769, -0.2849195897579193, -0.1746065765619278, -1.4109631776809692, -0.006534728687256575, 1.186850666999817, 0.11739682406187057, 0.6050872802734375, 0.48296400904655457, 0.3771863877773285, 0.54782634973526, -0.3438979983329773, 0.7038437724113464, 0.5685534477233887, -1.1178532838821411, 0.30746278166770935, -0.07441015541553497, 0.2511623501777649, 0.533193051815033, 0.7609410881996155, 0.9557230472564697, 0.8603524565696716, -0.31824055314064026, 0.38592052459716797, 0.20325742661952972, 0.8562150597572327, 0.13297408819198608, 0.22198472917079926, 0.11118543148040771, -1.118754506111145, -0.10410954058170319, -0.6680719256401062, -1.0314061641693115, 0.3320426344871521, 0.3615128993988037, -0.4350784718990326, -0.47422510385513306, 0.08371683955192566, 0.7122024297714233, 0.02365829050540924, 0.2782764136791229, -0.25189679861068726, 0.9263224601745605, -0.009859452024102211, -0.06861454993486404, 0.18553894758224487, 0.3302040994167328, 0.051690537482500076, -0.00873178243637085, 0.45774829387664795, 0.10689200460910797, 0.02408185787498951, 0.26590099930763245, -0.03844377398490906, 0.310822457075119, -0.3582298755645752, -0.2614132761955261, 0.1920422464609146, 0.712823748588562, 1.0627371072769165, 0.05517695099115372, -0.5170825719833374, 0.387434184551239, 0.9209182858467102, 0.19657258689403534, -0.245651975274086, 0.06291096657514572, 0.5627304911613464, -0.49944642186164856, 0.403580904006958, 0.6060605645179749, -0.031183209270238876, -0.464919775724411, -0.7863476872444153, -0.4503885805606842, 0.6203938722610474, -0.973050057888031, -0.8695223331451416, 0.738722026348114, -0.28783297538757324, -0.1051076203584671, -0.27457329630851746, -0.40098637342453003, -0.28328150510787964, 0.5785417556762695, -1.2995283603668213, -0.6315096616744995, 0.28005972504615784, -0.45239976048469543, -0.09448645263910294, -0.05613262578845024, 1.381260633468628, -0.10418005287647247, -0.3971346616744995, 0.039366547018289566, 0.022040290758013725, -0.39096274971961975, -0.29255130887031555, -0.8450804352760315, 0.9131677746772766, 0.32484322786331177, -0.009750082157552242, 0.4139038920402527, 0.09970159083604813, 0.16392871737480164, -0.671873927116394, -0.10193508863449097, 0.9318801164627075, -0.9608175158500671, -0.21711651980876923, -0.7916032671928406, -1.0463670492172241, 0.29536962509155273, 0.5250178575515747, -0.07427340000867844, 0.30297359824180603, 0.29775553941726685, -0.5732153654098511, -0.41930797696113586, -0.258166640996933, -0.1776433140039444, 0.5864216685295105, -0.777995228767395, -0.20930421352386475, -0.1449732482433319, 0.721222460269928, -0.8719080686569214, -0.3421616554260254, -0.4611930549144745, 0.11798828840255737, -0.03545135259628296, 0.9705677032470703, -0.36606818437576294, 0.4767570495605469, 0.8425472974777222, -0.04765653610229492, -0.9192872643470764, -0.444847971200943, -1.095151662826538, -0.4507472515106201, 0.261400431394577, 0.6998400688171387, -0.3874835669994354, 0.32111549377441406, 0.6722765564918518, 0.2085280865430832, -0.5925925374031067, -0.7721542716026306, -0.35513970255851746, -0.32807737588882446, -0.4295497536659241, 0.5776938796043396, 0.18242020905017853, -0.2067331075668335, 0.25433841347694397, 0.04487825930118561, 0.7169997692108154, -0.11911874264478683, -0.5886122584342957, 0.24632595479488373, -0.07380934059619904, -0.22776485979557037, -0.38531947135925293, -0.47653502225875854, -1.1740443706512451, -0.07847343385219574, -0.9252040982246399, -0.02977360598742962, -0.8239850401878357, -0.5845417380332947, 0.028318356722593307, -0.3147020637989044, 0.08619906008243561, 0.2459147572517395, -0.3534187376499176, -0.555007815361023, -0.5396245121955872, -0.5360614061355591, 0.9019516110420227, 0.4526655077934265, -0.7225489616394043, -0.11334916204214096, -0.018459158018231392, -0.17460529506206512, -0.08132103085517883, 0.33538904786109924, -0.4786040484905243, -0.6498989462852478, -1.3177071809768677, 0.4785499572753906, -0.10303200036287308, -0.2424890249967575, -0.37397322058677673, 0.40579938888549805, 0.15571168065071106, -0.38921573758125305, -0.12224307656288147, 0.387857049703598, -0.716047465801239, -0.32591331005096436, 0.07246684283018112, -0.6648039817810059, 0.33922967314720154, -0.12275098264217377, -0.5408079624176025, -0.5210132598876953, 0.6913773417472839, -0.09871909767389297, -1.3272535800933838, -0.6551429033279419, 0.7148063778877258, -0.5278096795082092, 0.45241668820381165, -0.5262472629547119, -0.1013968214392662, -0.8917674422264099, -0.7145357728004456, 0.29163217544555664, 0.39361444115638733, -0.6596329808235168, 0.9862149953842163, 0.28075262904167175, -1.1294617652893066, 0.06696910411119461, 0.41070225834846497, 0.03462342917919159, 0.2444075047969818, 0.5389241576194763, 0.3141941726207733, -0.08894369006156921, 0.5811338424682617, 0.7180038690567017, 0.3142573833465576, -0.8260686993598938, -0.05170352756977081, 0.6781681776046753, -0.6501215696334839, 0.01982985995709896, 1.3240338563919067, -0.026737753301858902, -0.8475829362869263, -0.10275605320930481, -1.2304034233093262, -0.7756556272506714, -0.18085749447345734, 0.5807427763938904, -0.03684861212968826, -0.1343105584383011, -0.30272895097732544, -0.7709835767745972, 0.046967145055532455, -0.15921655297279358, -0.2170954793691635, 0.9795994758605957, -0.06710957735776901, -0.5226104855537415, 0.8128470182418823, 0.8874768018722534, -0.5712339878082275, -0.35352373123168945, -0.6545004844665527, -0.4150952696800232, -0.04333864524960518, 0.21740122139453888, -0.11799663305282593, -0.5746384263038635, 0.7463126182556152, 0.2618725895881653, 0.5235795974731445, -0.16145142912864685, 0.1455666869878769, 0.1258082538843155, 0.6656587719917297, 0.14846570789813995, -0.6870245933532715, -0.3531720042228699, 1.3903919458389282, 1.3790682554244995, -0.6945406198501587, -0.060703344643116, -0.3904813826084137, -0.8984777927398682, 0.36001864075660706, 0.409879207611084, 0.09331341832876205, 0.771848738193512, -0.45448803901672363, 0.27892372012138367, -0.2565281093120575, -1.3034451007843018, -0.2310597449541092, 0.8825085759162903, 1.1719036102294922, 0.8780235052108765, 0.08783020079135895, 0.3299480080604553, 0.7763595581054688, -0.3061679005622864, -0.1871906965970993, 0.21470382809638977, 0.07502836734056473, -0.1329677551984787, 0.134139284491539, 0.20863528549671173, 0.895256757736206, -0.5884724855422974, -0.9799749851226807, 0.19098375737667084, 0.2961837649345398, -0.143800750374794, 0.4396877586841583, 0.7971484661102295, 0.49631771445274353, 0.5815351009368896, 0.1450044810771942, 0.22669969499111176, -0.684158980846405, 0.03340030089020729, -0.2119992971420288, -0.5835129618644714, -0.41579025983810425, 0.020019637420773506, -0.7659688591957092, -0.10004033893346786, 0.4300541281700134, -0.06332992762327194, 0.09479742497205734, 0.28639307618141174, 0.9601367115974426, 0.8369696736335754, 0.30571338534355164, 0.007862781174480915, -0.3068806529045105, -0.5335649251937866, -0.8688715100288391, -0.26129791140556335, -0.5456134676933289, -0.24699074029922485, -0.13217037916183472, -0.15460219979286194, -0.41961413621902466]}, "authors": [{"authorId": "2061806821", "name": "Chao Lou"}, {"authorId": "1453587987", "name": "Zixia Jia"}, {"authorId": "2266032392", "name": "Zilong Zheng"}, {"authorId": "40341553", "name": "Kewei Tu"}], "references": [{"paperId": "4b879f069d023e03bf537309a99bdaeb39916ea5", "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training"}, {"paperId": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"paperId": "f288e2238ac8725baa7ca9874bbc3fed1e89a632", "title": "Data Engineering for Scaling Language Models to 128K Context"}, {"paperId": "f4a0c4154203808f362e4678f3741b3d317fdc82", "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry"}, {"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "482ffdfc1e3594f5a22f51e3b9ee31e1c74f85de", "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "b6346f9fa093b8e85df712485a2b851b9f680dac", "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"}, {"paperId": "a7fc585cc4c2b6822646b2c410e0c427a20798f2", "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "2a09ebbfcca1a6994eeb472cd4159f5f3858dbf9", "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "d203c764fb5dec2b053be667c8b06e516ea6ef10", "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "148644bf4ccef7e022b965304e8b3178be8af0fa", "title": "Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "title": "Do Long-Range Language Models Actually Use Long-Range Context?"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "c1a4278f969acfc6682a924e31b95e1ade9703ee", "title": "Memory-efficient Transformers via Top-k Attention"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "0cd82dfae930ac4b57c0e959f744f2d10bf87649", "title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "3df83a60f55c64b40e6dbcd99cf9f67894a0736e", "title": "Do Transformers Need Deep Long-Range Memory?"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "3cee801d10f410f0feb1a2390776a01ba2765001", "title": "Sparse Sequence-to-Sequence Models"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "title": "Fixing Weight Decay Regularization in Adam"}, {"paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498", "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "89e93daca5e76a909579037f51623cf63a190fe8", "title": "Foundation"}, {"paperId": "b20c0758a38bd5a4083f64eff53af924499a8e29", "title": "Possible generalization of Boltzmann-Gibbs statistics"}, {"paperId": null, "title": "Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama"}, {"paperId": null, "title": "better large language"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning"}, {"paperId": null, "title": "xformers: A modular and hackable transformer modelling library"}, {"paperId": null, "title": "OpenCompass Contributors"}]}