{"paperId": "ff5eb1bd55d61ae919865f6b4dab84e6ae1974f3", "title": "Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression", "abstract": "In training of modern large natural language processing (NLP) models, it has become a common practice to split models using 3D parallelism to multiple GPUs. Such technique, however, suffers from a high overhead of inter-node communication. Compressing the communication is one way to mitigate the overhead by reducing the inter-node traffic volume; however, the existing compression techniques have critical limitations to be applied for NLP models with 3D parallelism in that 1) only the data parallelism traffic is targeted, and 2) the existing compression schemes already harm the model quality too much. In this paper, we present Optimus-CC, a fast and scalable distributed training framework for large NLP models with aggressive communication compression. Optimus-CC differs from existing communication compression frameworks in the following ways: First, we compress pipeline parallel (inter-stage) traffic. In specific, we compress the inter-stage backpropagation and the embedding synchronization in addition to the existing data-parallel traffic compression methods. Second, we propose techniques to avoid the model quality drop that comes from the compression. We further provide mathematical and empirical analyses to show that our techniques can successfully suppress the compression error. Lastly, we analyze the pipeline and opt to selectively compress those traffic lying on the critical path. This further helps reduce the compression error. We demonstrate our solution on a GPU cluster, and achieve superior speedup from the baseline state-of-the-art solutions for distributed training without sacrificing the model quality.", "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems", "year": 2023, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2301.09830", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Optimus-CC is presented, a fast and scalable distributed training framework for large NLP models with aggressive communication compression that achieves superior speedup from the baseline state-of-the-art solutions for distributed training without sacrificing the model quality."}, "embedding": {"model": "specter_v2", "vector": [0.019324272871017456, 0.14879748225212097, -0.6121854186058044, -0.0986989364027977, -0.13506296277046204, 0.2441478818655014, 0.5617700815200806, 0.1839367300271988, -1.0356799364089966, -0.26708805561065674, 0.553623616695404, -0.6024405360221863, 0.4148597717285156, -0.07058965414762497, -0.7263023853302002, 0.29180893301963806, -0.9277677536010742, 0.6400222778320312, 0.007972865365445614, -0.23890487849712372, -0.19166703522205353, -0.027405785396695137, -0.9896305203437805, 0.3482185900211334, 0.6666135191917419, 0.32428571581840515, 0.11380770802497864, 1.1207019090652466, -0.7943108677864075, 0.04772228002548218, 0.3744524419307709, 0.004824344534426928, 0.1307927668094635, 0.11595108360052109, -0.36328646540641785, -0.09158077836036682, 0.22972342371940613, -0.3057141602039337, -0.4125567674636841, 0.7245659828186035, -0.15705035626888275, 0.27332037687301636, 0.14406843483448029, -0.6044948101043701, 0.2713097929954529, 0.2725563049316406, -0.13455380499362946, 0.843551516532898, -0.35694998502731323, -0.8112182021141052, 0.6021748185157776, -1.071354627609253, 0.246649369597435, 1.446999192237854, 0.7690244913101196, 0.5576975345611572, -0.06946786493062973, -0.3593531847000122, 0.26159125566482544, 0.08106862753629684, -0.4301050901412964, -0.20732557773590088, -0.37146028876304626, -0.22341863811016083, 2.0167858600616455, 0.010687467642128468, 0.4047742486000061, 0.37396928668022156, -0.08203823864459991, 1.247924566268921, -0.10679271072149277, -1.0360150337219238, -0.22067607939243317, -0.22820386290550232, 0.31897851824760437, 0.8923149704933167, -0.3105517029762268, 0.6554415822029114, -1.0604100227355957, -0.28860077261924744, 0.13072316348552704, 0.11202146857976913, 0.17472170293331146, 0.17303137481212616, 0.18254230916500092, 0.8532930612564087, 0.5087504982948303, 0.5534843802452087, -0.04750869423151016, 0.8282336592674255, 0.8055412173271179, 0.19150033593177795, 0.5540660619735718, -0.12142807990312576, -0.13469481468200684, 0.20136991143226624, -1.2874079942703247, 0.2883259654045105, 0.5366257429122925, 0.64653480052948, -0.4441496431827545, -0.12810030579566956, -0.6127071380615234, 0.11386185884475708, 1.444050669670105, -0.051961302757263184, 0.505739152431488, -0.32530543208122253, 0.7545093894004822, -0.7908795475959778, -0.12983575463294983, -0.6043745875358582, -0.33866259455680847, -0.18298912048339844, -0.9471168518066406, -0.9049915075302124, -0.7872236371040344, -0.28939834237098694, -0.7804818749427795, 0.18034179508686066, -0.04702967405319214, 0.6864284873008728, 0.3548784554004669, 0.6172049045562744, 0.3711091876029968, 1.0209293365478516, 0.1761389672756195, 0.19744661450386047, 1.2668803930282593, -1.4424283504486084, -0.4680672287940979, -1.0126250982284546, 0.8589655756950378, -0.1887558549642563, -0.06845889240503311, -0.7883870005607605, -1.5452954769134521, -0.6816361546516418, -0.7969216108322144, -0.12412501871585846, -0.29009538888931274, -0.11287251859903336, 1.021309733390808, 0.22623947262763977, -1.0060129165649414, 1.0959758758544922, -0.5785782933235168, -0.2781757712364197, 0.7011417150497437, 0.26553791761398315, 0.37321537733078003, -0.1976088285446167, -1.2972588539123535, 0.3232845366001129, 0.2861844003200531, -0.6125916242599487, 0.12030868232250214, -0.6199147701263428, -0.9057183861732483, 0.20605158805847168, 0.07730331271886826, -0.6215381622314453, 1.0458132028579712, 0.294453889131546, -1.0516555309295654, 0.5212496519088745, -0.5270189642906189, -0.36001402139663696, 0.3209341764450073, -0.06833643466234207, -0.33714309334754944, -0.2935775816440582, -0.43397989869117737, 0.8026749491691589, 0.3133622407913208, 0.1543007344007492, -0.28401684761047363, 0.12235809862613678, -0.40474212169647217, 0.3216623365879059, -0.731667697429657, 1.059386968612671, -0.8312597274780273, 0.03877630829811096, 0.35923078656196594, 0.5691593289375305, -0.46800702810287476, 0.038360241800546646, -0.33503270149230957, -0.6571588516235352, 1.105600118637085, 0.02915921062231064, 0.6498737931251526, -0.9966520071029663, -0.8212098479270935, -0.017322978004813194, -0.0035350299440324306, 0.1265077441930771, -0.9120550751686096, 0.5462345480918884, -0.213083416223526, 0.4808312654495239, -0.28888019919395447, -1.219603419303894, 0.023870710283517838, 0.12615597248077393, -0.661291778087616, -0.3091331422328949, -0.024459784850478172, 1.0685279369354248, -0.6770797967910767, -0.06869692355394363, -0.44997844099998474, 0.4623222053050995, -1.196794033050537, 1.1261589527130127, -0.8190061450004578, 0.07718595862388611, 0.22418150305747986, -0.47481852769851685, 0.3313635289669037, -0.27430304884910583, 0.511313259601593, -0.25473782420158386, -0.21291513741016388, 0.5647165775299072, -0.1710330694913864, 1.3034478425979614, -0.3955399990081787, 0.30190378427505493, 0.16155126690864563, -0.7039406895637512, 0.1485380232334137, 0.5317397117614746, -0.1580982208251953, -0.3579380512237549, 0.3164992928504944, 0.7250181436538696, -0.46491169929504395, 0.3301510214805603, 0.9952324032783508, 0.8194655776023865, -0.15096133947372437, 0.5104960799217224, 0.399321973323822, -0.3030623197555542, 0.7632952332496643, 0.587255597114563, 0.5742624998092651, 0.19025930762290955, -0.0696764811873436, -0.003126785857602954, 0.45117801427841187, -0.6419320106506348, -0.18632814288139343, 0.24572131037712097, 0.32852789759635925, 0.368588924407959, 0.5293219685554504, -0.7285032272338867, -0.6127493381500244, 0.8197740316390991, 0.6862106323242188, 1.2481685876846313, -0.46577534079551697, -0.14074762165546417, -0.673231303691864, -0.8520352840423584, -0.07940715551376343, -0.37810707092285156, 0.021675340831279755, 0.32061028480529785, -0.5051581263542175, -1.2587336301803589, 0.9952784180641174, 0.2561239004135132, 1.0352486371994019, -0.3959648311138153, -0.24246838688850403, -0.693492591381073, 0.3098193109035492, -1.1108860969543457, -0.6962990760803223, 0.5393662452697754, -0.8778442740440369, -0.09433947503566742, 0.03574378043413162, -0.0832817554473877, 0.36441996693611145, -0.2171620875597, 0.8753188252449036, -0.30203500390052795, -0.36475563049316406, -0.08649149537086487, 0.45482102036476135, -0.7523547410964966, -1.061699390411377, 0.5288311243057251, -0.08511538803577423, -0.7649042010307312, 0.5327968001365662, 0.4972451627254486, 0.3868522346019745, -0.3197823166847229, -0.35701262950897217, 0.3020398020744324, -0.18239279091358185, -0.30188116431236267, 0.739418089389801, -0.405437171459198, -0.16076861321926117, -1.3123093843460083, 1.1684147119522095, 0.06399556249380112, -0.6476607918739319, 0.010519275441765785, -0.35898348689079285, -0.3551795184612274, 0.564177930355072, -0.5470675826072693, -0.03667609021067619, -0.8149104118347168, -0.2619704604148865, -0.5551854968070984, -0.4109533727169037, 0.38036131858825684, 1.049871802330017, -0.09549031406641006, 0.49287644028663635, 0.2965259552001953, 0.7074804306030273, -0.1804317682981491, 0.6871801018714905, -0.8502882719039917, 0.14651669561862946, 0.10568591952323914, 0.09024811536073685, -0.10981686413288116, 0.12835454940795898, -0.8666106462478638, -0.24005275964736938, -0.3384292423725128, -0.3261887729167938, -0.0026780888438224792, 0.12748247385025024, -0.43291664123535156, -0.7856524586677551, -0.5429855585098267, -1.516403079032898, -0.09520598500967026, -0.08266452699899673, 0.14509980380535126, -0.06910271942615509, -0.851162314414978, -1.8640862703323364, -0.3391125500202179, -1.0277658700942993, -1.0288866758346558, 0.5012928247451782, 0.07336821407079697, -0.04146769270300865, -0.7677743434906006, -0.04536387324333191, -0.537347137928009, 1.0504318475723267, -0.4796682596206665, 0.9318230748176575, -0.085963673889637, 0.2618856132030487, -0.3249148726463318, -0.2900010943412781, 0.16794154047966003, -0.759188175201416, 0.3855920732021332, -0.5559508800506592, 0.41288116574287415, -0.33424580097198486, -0.5603536367416382, 0.4453350305557251, 0.4661250412464142, 0.6670243144035339, 0.21670599281787872, -0.819758951663971, 0.6328004598617554, 1.2593202590942383, -0.9746577143669128, 0.06233169883489609, -0.24417853355407715, 0.837622344493866, 0.15563459694385529, -0.6819635033607483, 0.4361763298511505, -0.039049338549375534, 0.32628706097602844, 0.29945439100265503, -0.16287507116794586, -0.33425745368003845, -0.35392841696739197, 0.12845708429813385, 2.2679736614227295, 0.5689211487770081, -0.14500552415847778, -0.9319179058074951, 0.14340722560882568, -1.2758499383926392, -0.35606083273887634, 0.3166019320487976, 0.33568599820137024, 0.3903842270374298, -0.4138752520084381, -0.25458574295043945, -0.21071267127990723, 0.5670703649520874, 0.9125394225120544, -0.16285763680934906, -0.993407130241394, 0.19149993360042572, 0.35889145731925964, 0.38114452362060547, 0.7513971328735352, -0.25462818145751953, 0.5290961861610413, 14.527233123779297, 1.1477258205413818, -0.082978256046772, 0.601318895816803, 0.6956207156181335, 0.29448455572128296, -0.3217505216598511, -0.45979562401771545, -1.4894286394119263, -0.10721010714769363, 1.5906728506088257, 0.2621051073074341, 0.37683025002479553, -0.0242777056992054, 0.17264248430728912, 0.3034457564353943, -0.12414999306201935, 0.5030235052108765, 0.3261013925075531, -1.6435143947601318, 0.10608840733766556, 0.1327911913394928, 0.45309171080589294, 0.9766483306884766, 0.4390278160572052, 0.5743333697319031, 0.4243915379047394, -0.35693466663360596, -0.18610727787017822, 0.06542161852121353, 0.9886762499809265, -0.0242379829287529, 0.4336298406124115, 1.011742353439331, -0.768113911151886, 0.06763766705989838, -0.693411648273468, -1.0039646625518799, 0.41820529103279114, 0.355724960565567, -0.4235657751560211, -0.3028601109981537, -0.4440736174583435, 0.9085700511932373, 0.49662742018699646, 0.39725515246391296, -0.19681963324546814, 0.5630870461463928, -0.7320024371147156, -0.11420158296823502, 0.5469474792480469, 0.3005259037017822, 0.14817562699317932, 0.20634862780570984, 0.05744560807943344, -0.22715243697166443, 0.2682788670063019, 0.41607314348220825, -0.9373208284378052, -0.18713097274303436, -0.32330870628356934, -0.17887181043624878, -0.24534481763839722, 0.6677854657173157, 0.4311656057834625, 0.26498422026634216, -0.6541197299957275, 0.5629326701164246, 1.1525599956512451, -0.09314940869808197, -0.2577323913574219, 0.1387999802827835, -0.036841440945863724, -0.3867809772491455, -0.02634717896580696, 0.17304575443267822, -0.47757458686828613, -0.3020159900188446, -0.6628331542015076, -0.1647552102804184, 0.17589716613292694, -0.2421901971101761, -0.7526243925094604, 0.9251543879508972, -0.2911640703678131, -0.21220558881759644, 0.43915843963623047, -0.5734038949012756, -0.2818254828453064, 0.5140573978424072, -1.6737418174743652, -0.31619003415107727, 0.6262204647064209, -0.373056560754776, -0.5766019225120544, 0.09664899855852127, 1.5510306358337402, 0.7863233089447021, -0.27046364545822144, -0.09511256217956543, 0.15651920437812805, -0.14661742746829987, -0.2888907194137573, -0.3309882879257202, 1.2708560228347778, 0.22353419661521912, 0.13302654027938843, -0.5931029915809631, -0.4886852204799652, -0.03257884085178375, -0.7587537169456482, -0.6189701557159424, 1.134177565574646, -0.12515036761760712, -0.49148282408714294, -0.6005467772483826, -0.992671549320221, 0.2277018129825592, 0.39777347445487976, 0.017816560342907906, 0.5616705417633057, -0.013472444377839565, -0.527162492275238, 0.17331933975219727, -0.8159217238426208, 0.1711428314447403, 0.33270740509033203, -0.5398805141448975, 0.1576867550611496, 0.3678259551525116, 0.9169321060180664, -1.453488826751709, -0.581815779209137, -0.09970734268426895, -0.060194291174411774, 0.2332320660352707, 0.8286616802215576, -0.4095674157142639, 1.0701667070388794, 1.0811837911605835, -0.07762305438518524, -0.4779495894908905, 0.4342498481273651, -0.644994854927063, -0.43432772159576416, -0.4800206422805786, 0.674627423286438, -0.21366363763809204, 1.0969033241271973, 0.8101025819778442, 0.17628374695777893, -0.5852692127227783, -0.4663431942462921, -0.21226531267166138, 0.007615276612341404, -0.5756208300590515, 0.13113203644752502, -0.18191970884799957, -0.04220753535628319, 0.042855873703956604, 0.34737610816955566, 0.5963497757911682, -0.22353236377239227, -0.7073842287063599, 0.3618830740451813, 0.16367493569850922, -0.2188226729631424, -0.37679606676101685, -0.27929556369781494, -1.6532031297683716, 0.06913747638463974, -1.369480848312378, -0.27832508087158203, -0.72843337059021, -0.515895426273346, -0.06296125054359436, 0.4276936650276184, -0.04149635136127472, 0.46510469913482666, -0.35734236240386963, -0.1947201043367386, -0.37916305661201477, -0.39908868074417114, 1.0348035097122192, 1.0195952653884888, -0.35909196734428406, -0.14524897933006287, 0.1206536814570427, 0.6742646098136902, 0.427460253238678, 0.669393002986908, -0.5410845279693604, -0.6625580191612244, -1.5595062971115112, 0.15527351200580597, 0.10818194597959518, -0.19563962519168854, -0.8280715942382812, 1.1748400926589966, 0.61802077293396, -0.3414638042449951, -0.21475671231746674, 0.32602328062057495, -1.2192037105560303, -0.8060833215713501, 0.7601823806762695, -0.6496578454971313, 0.04472829028964043, 0.7601192593574524, -0.4767305850982666, -0.3798811435699463, 0.5162200331687927, -0.17900624871253967, -0.7542711496353149, -0.47707927227020264, 0.6901818513870239, -0.3433220684528351, 0.11935704201459885, -0.2757814824581146, 0.27840152382850647, -1.1134788990020752, -0.09639763087034225, -0.21145139634609222, 0.3050541877746582, -0.8573601841926575, 0.4892744719982147, 0.2951434254646301, -1.18507981300354, -0.05242916941642761, 0.5770037770271301, -0.7872600555419922, 0.2910551130771637, 0.7673099637031555, 0.5162489414215088, -0.9252362251281738, 0.6756240129470825, 0.39414986968040466, 0.19331714510917664, -1.0650783777236938, -0.1882449835538864, 0.8174633979797363, -0.8071086406707764, -0.42018449306488037, 1.0487370491027832, -1.0867629051208496, -1.591857671737671, 0.21007712185382843, -1.0417110919952393, -0.6007745862007141, -0.6326418519020081, 0.737989604473114, 0.41774141788482666, 0.1073286309838295, 0.2177671641111374, -0.4671611785888672, 0.11114421486854553, -0.24358512461185455, -0.6232831478118896, 0.45793721079826355, 0.1894272416830063, -0.5534920692443848, 0.07626516371965408, 1.057537317276001, -0.8020527362823486, -0.6008639335632324, -0.9915932416915894, -0.37920910120010376, -0.1994020640850067, 0.24141746759414673, -0.2249850183725357, -0.8557550311088562, 1.007912516593933, 0.28241342306137085, 0.4894244968891144, -0.1752842217683792, -0.32056182622909546, 0.958981454372406, 0.38897064328193665, -0.28255710005760193, -0.32847660779953003, -0.7204388380050659, 1.7706429958343506, 1.1448520421981812, -0.6989936232566833, 0.555982232093811, -0.5922250747680664, -0.6002885103225708, 1.023345708847046, 0.25944024324417114, -0.15924011170864105, 1.2628331184387207, 0.3254278600215912, -0.1071019098162651, 0.04719965159893036, -0.9798684120178223, -0.20284539461135864, 0.6858816742897034, 0.7827096581459045, 0.701120138168335, -0.026443276554346085, -0.512408971786499, 0.5972311496734619, 0.36048102378845215, 0.1893686205148697, -0.00045556272380053997, 0.563494086265564, -0.364132821559906, -0.3111923336982727, -0.1221061572432518, 0.8102871179580688, -0.693601667881012, -0.6491137146949768, 0.43479880690574646, 0.43769335746765137, 0.06878256052732468, 0.6487274765968323, 1.0755200386047363, -0.07093438506126404, 0.6362432837486267, -0.004315878730267286, 0.4173460900783539, -0.49394649267196655, -0.5849950909614563, -0.021628493443131447, -0.75853031873703, 0.041505977511405945, -0.37110620737075806, -0.055939316749572754, -0.9505971670150757, -0.8798972964286804, 0.6996352672576904, 0.2298041135072708, 0.7707017660140991, 0.8145995140075684, 1.1031948328018188, 0.8164312839508057, -0.42595532536506653, -0.6462968587875366, -0.382321298122406, -0.7359576225280762, -0.049149878323078156, -0.6731368899345398, -0.720349907875061, 0.6368192434310913, 0.007099047303199768, -0.4114813208580017]}, "authors": [{"authorId": "70440407", "name": "Jaeyong Song"}, {"authorId": "2202481081", "name": "Jinkyu Yim"}, {"authorId": "2122355016", "name": "Jaewon Jung"}, {"authorId": "2202358191", "name": "Hongsun Jang"}, {"authorId": "2109608651", "name": "H. Kim"}, {"authorId": "2567514", "name": "Youngsok Kim"}, {"authorId": "2108602459", "name": "Jinho Lee"}], "references": [{"paperId": "286d371febea99ec19044e69e163e8bd53137a7f", "title": "PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "9e4a8177823d044deca512f5849a3f61af553a15", "title": "Near-optimal sparse allreduce for distributed deep learning"}, {"paperId": "10f3ca78e194552427ebe9173b19d1b910469e27", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"}, {"paperId": "91b29761840442005da39bc258e2298b528f31aa", "title": "Breaking the computation and communication abstraction barrier in distributed machine learning workloads"}, {"paperId": "181198db16562aec7d0409871345027b406242a9", "title": "ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "6b3870e25416772a38e8a79ad7d661545bb29373", "title": "Canary: Decentralized Distributed Deep Learning Via Gradient Sketch and Partition in Multi-Interface Networks"}, {"paperId": "8c518d1859d892640247fc6b4beb75545864ebc9", "title": "Pufferfish: Communication-efficient Models At No Extra Cost"}, {"paperId": "d99ae9e262dbba2d0232cac12bb1a3bfadeabb53", "title": "On the Utility of Gradient Compression in Distributed Training Systems"}, {"paperId": "4066d78b637c2b8e57de5ffd53950134a551de85", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"}, {"paperId": "8b2e1137639aa764aa3c6ecbd1c1aa9787126c7e", "title": "An Efficient Statistical-based Gradient Compression Technique for Distributed Training Systems"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "a9bebc22b682906a9ebb4d4cce4b31d230131385", "title": "Adaptive Gradient Quantization for Data-Parallel SGD"}, {"paperId": "f57db358459390590bc838663025dae0f8d51ebf", "title": "Towards Scalable Distributed Training of Deep Learning on Public Cloud Clusters"}, {"paperId": "8bb8a1a340eae6d50059bbb70af2e5562b6c46d0", "title": "Error Compensated Distributed SGD Can Be Accelerated"}, {"paperId": "46780b67ec795d143750bcba94406928c6973488", "title": "Step-Ahead Error Feedback for Distributed Training with Compressed Gradient"}, {"paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d", "title": "DAPPLE: a pipelined data parallel approach for training large models"}, {"paperId": "de65c34bfb7e16ef9dec1d24893d54e08e7ccf34", "title": "FlexReduce: Flexible All-reduce for Distributed Deep Learning on Asymmetric Network Topology"}, {"paperId": "c75390f8138e2a422956d0e1b00cb6a39579bc95", "title": "PyTorch distributed"}, {"paperId": "0ffe2dde5dc78775bcf0c116661664845937b499", "title": "A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning"}, {"paperId": "1c61525e1e95d29549d57417b3dc28243561a140", "title": "Scaling Distributed Training with Adaptive Summation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "408d0580d7e2befabf542119d7fc8318c684572b", "title": "Pipelined Backpropagation at Scale: Training Large Models without Batches"}, {"paperId": "9d5dbe7230161db2d3573c3d5e7868dda42a6d3e", "title": "Improving the Accuracy, Scalability, and Performance of Graph Neural Networks with Roc"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6909b61fd4edaa6b4f9078a59f337c606fc6baae", "title": "Channel and filter parallelism for large-scale CNN training"}, {"paperId": "1e009f755503bffd7644fcd0a45939c54b838b37", "title": "BlueConnect: Decomposing all-reduce for deep learning on heterogeneous network hierarchy"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "76c929af6735cdff2c4badc9a9c8f39d15ea3e70", "title": "A generic communication scheduler for distributed DNN training acceleration"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "8221b23813858ee536997385097c1ef611184346", "title": "Blink: Fast and Generic Collectives for Distributed ML"}, {"paperId": "1340978c92e7cbcf9abe87888150c60984e2964b", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "fd075bcdf2d7e13d23f7c249a8eded343d5bbe3b", "title": "Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs"}, {"paperId": "6bb165ed0eefc382bf957771a89a133012bbac54", "title": "Optimizing Multi-GPU Parallelization Strategies for Deep Learning Training"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "8000a4a63ac97ffb84917f910e2ce747e48d409f", "title": "Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "90cbe7f340a8de92143e5b464e6e963bb95f6129", "title": "Priority-based Parameter Propagation for Distributed DNN Training"}, {"paperId": "b4a632a7097e7d0631250884dfc6e1f76b376996", "title": "PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"}, {"paperId": "cc14e2e99ef12b01ecb1e869b46b9eb50e2179bd", "title": "HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "7f12f77cd04dab6116a6ea1db73bf9425d94d8af", "title": "Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash"}, {"paperId": "dbca9dbe14e9933515d2005dc1163ae2c24d9afd", "title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant"}, {"paperId": "d3b0d4b2ea111f9b560815487e65c2619d9cf15d", "title": "Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU Platform"}, {"paperId": "93ef5b740fa1b54929ead6eb177e0698d7f19719", "title": "Don't Use Large Mini-Batches, Use Local SGD"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "8ae94896468b0e5006b96d3af25c0c05a3b31e3b", "title": "Double Quantization for Communication-Efficient Distributed Optimization"}, {"paperId": "89b2ddc042215f4ac1121114f73a30505f44a838", "title": "Unifying Data, Model and Hybrid Parallelism in Deep Learning via Tensor Tiling"}, {"paperId": "2229ac756f89c3db017293918548555734d2f891", "title": "TicTac: Accelerating Distributed Deep Learning with Communication Scheduling"}, {"paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54", "title": "signSGD: compressed optimisation for non-convex problems"}, {"paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5", "title": "Regularized Evolution for Image Classifier Architecture Search"}, {"paperId": "92495abbac86394cb759bec15a763dbf49a8e590", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training"}, {"paperId": "3193c82ae0598f7df7527b6f1a3836eee345d122", "title": "AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training"}, {"paperId": "e4edc414773e709e8eb3eddd77b519637f26f9a5", "title": "Scale out for large minibatch SGD: Residual network training on ImageNet-1K with improved accuracy and reduced time to train"}, {"paperId": "6cd621c0973c915895d88a391783063b20e98855", "title": "Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes"}, {"paperId": "6fc2ccc1cbb555955291b0989251bd77240dd551", "title": "ImageNet Training in Minutes"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "079932bf6ff8b99c899172ba60071818f6b5dfcb", "title": "Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "4bdb91a6e47385292ab7a18e8901a6a25f50cc6b", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "f4e5578dfe6483fd221b7118817297f7b70d3a27", "title": "Distributed Deep Learning Using Synchronous Stochastic Gradient Descent"}, {"paperId": "d1e4365de165463e51134f10bf3939f2b00a6667", "title": "Deep learning with Elastic Averaging SGD"}, {"paperId": "50684b147b752a07c313cb73d864f7b21bd8b703", "title": "Scaling Distributed Machine Learning with the Parameter Server"}, {"paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "title": "One weird trick for parallelizing convolutional neural networks"}, {"paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e", "title": "Large Scale Distributed Deep Networks"}, {"paperId": "b471f0b45d69c3fd3333f0322bab64b2a4ae9369", "title": "Optimization of Collective Communication Operations in MPICH"}, {"paperId": "23977ef6c3cf6df8f54347f16543638fb3e5abbb", "title": "Understanding some simple processor-performance limits"}, {"paperId": "ca1402619a80c140c650961d899319c2928744f0", "title": "Piper: Multidimensional Planner for DNN Parallelization"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "NVIDIA/Megatron-LM: v2.5"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "72bf9a21d336881697a2c8dbe5537e97737e0f13", "title": "GradZip: Gradient Compression using Alternating Matrix Factorization for Large-scale Deep Learning"}, {"paperId": null, "title": "Better Language Models and Their Implications"}, {"paperId": null, "title": "Hyung-Jin Him, Youngsok Kim, and Jinho Lee. 2022. jaeyong-song/Optimus-CC: v0.2"}]}