{"paperId": "d163cca5cfea5d967873d34023554e3d1771716b", "title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers", "abstract": "Vision transformers using self-attention or its proposed alternatives have demonstrated promising results in many image related tasks. However, the underpinning inductive bias of attention is not well understood. To address this issue, this paper analyzes attention through the lens of convex duality. For the non-linear dot-product self-attention, and alternative mechanisms such as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent finite-dimensional convex problems that are interpretable and solvable to global optimality. The convex programs lead to {\\it block nuclear-norm regularization} that promotes low rank in the latent feature and token dimensions. In particular, we show how self-attention networks implicitly clusters the tokens, based on their latent similarity. We conduct experiments for transferring a pre-trained transformer backbone for CIFAR-100 classification by fine-tuning a variety of convex attention heads. The results indicate the merits of the bias induced by attention compared with the existing MLP or linear heads.", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 27, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2205.08078", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper analyzes attention through the lens of convex duality, and shows how self-attention networks implicitly clusters the tokens, based on their latent similarity, in order to derive equivalent finite-dimensional convex problems that are interpretable and solvable to global optimality."}, "embedding": {"model": "specter_v2", "vector": [0.1344131976366043, 0.999218761920929, -0.19259950518608093, 0.25059598684310913, -0.3228954076766968, 0.04578786715865135, 0.742760419845581, -0.2408127635717392, -0.35977014899253845, -0.5226625800132751, 0.7603527903556824, 0.5038555264472961, 0.23013673722743988, -0.07524186372756958, -0.4231676161289215, -0.5980493426322937, -0.6912388205528259, 0.40953752398490906, -0.5081284642219543, -0.2696252763271332, 0.21735025942325592, -0.765879213809967, -1.2255313396453857, 0.2384456992149353, 0.5201364159584045, 1.0168622732162476, 0.4367920160293579, 0.58156418800354, -0.3253178000450134, 0.72733473777771, 0.6165812015533447, -0.312286376953125, 0.6788738369941711, 0.10743219405412674, -0.5415915250778198, -0.07605206966400146, 0.4802495539188385, -0.18449667096138, -0.7948080897331238, 1.1397767066955566, -0.2788533568382263, 0.10073590278625488, 0.7781797647476196, -0.966306209564209, -0.7676516175270081, 0.3803945481777191, 0.5553338527679443, 0.8779876828193665, -0.596944272518158, -0.5615459084510803, 1.6795302629470825, -1.312334656715393, 0.06724386662244797, 1.2184319496154785, 0.2523340880870819, 0.09609151631593704, -0.02214995212852955, -0.5197382569313049, 0.4983631670475006, 0.5635963082313538, -0.585289716720581, -0.3770894408226013, 0.21548902988433838, -0.05581536889076233, 1.5132449865341187, -0.4569520354270935, -0.025634560734033585, 0.22469596564769745, -0.09537182748317719, 1.579978585243225, 0.6565855741500854, -0.8813459277153015, -0.2408541887998581, 0.27071520686149597, 0.41735249757766724, 1.0958421230316162, -0.6869848966598511, 0.39246270060539246, -1.009570598602295, 0.07170557975769043, 0.3719853460788727, 0.003973702434450388, -0.014753971248865128, -0.34173399209976196, -0.22184287011623383, 0.8739215135574341, 0.8762645721435547, 0.5900194048881531, -0.5260263681411743, 0.9430199265480042, 0.7668594717979431, -0.06200907379388809, 0.007561518810689449, 0.4225345551967621, 0.7243037819862366, 0.4489585757255554, -0.5179142951965332, 0.018354253843426704, -0.09841752797365189, 0.923829972743988, 0.017374353483319283, 0.427816778421402, -0.5373318791389465, 0.03483547642827034, 1.2736670970916748, 0.3363441228866577, 0.5107041001319885, -0.6371408104896545, -0.048256680369377136, -0.8797130584716797, -0.2931270897388458, -1.2662700414657593, 0.1651258021593094, -0.5761628746986389, -1.0348079204559326, -1.0952041149139404, -0.5341778993606567, 0.8019843697547913, -0.7343726754188538, 0.48627620935440063, -0.3756876289844513, 0.08347467333078384, -0.026666458696126938, 0.7508217692375183, 0.42457127571105957, 0.6374140977859497, 0.2883528769016266, 0.2937985062599182, 1.3411271572113037, -0.9786423444747925, -0.7918084263801575, -0.555186927318573, 0.2115059792995453, -0.11487288773059845, 0.31680047512054443, -0.12039283663034439, -1.268744707107544, -0.9722210168838501, -0.7455830574035645, -0.029495831578969955, -0.5165545344352722, 0.4204254448413849, 1.1759024858474731, 0.18190701305866241, -0.9598456621170044, 0.6819478869438171, -0.1742536723613739, -0.11039064824581146, 1.0047674179077148, 0.5159815549850464, 0.20201705396175385, -0.1379927694797516, -0.9095568656921387, 0.6024782657623291, 0.1222003772854805, -0.26377636194229126, -0.29910317063331604, -0.712125837802887, -1.1579786539077759, 0.3237621784210205, 0.20226062834262848, -0.7061976790428162, 0.7804377675056458, -0.454367071390152, -0.8927202224731445, 0.917584240436554, -0.40671706199645996, -0.04115680605173111, 0.16461153328418732, -0.014125348068773746, -0.05302794277667999, -0.08930052071809769, 0.3858143091201782, 0.5535621047019958, 0.8074891567230225, -0.7511023879051208, -0.33039751648902893, 0.02548852376639843, -0.5882895588874817, 0.11770233511924744, -0.5299766063690186, 0.8003539443016052, -0.08326465636491776, -0.2010442018508911, 0.1614525020122528, 0.4933406114578247, 0.532630980014801, -0.49605071544647217, -0.3953617215156555, -1.2848180532455444, 0.6144624948501587, 0.4879783093929291, 0.37487223744392395, -0.9885504841804504, -0.8352082967758179, 0.019995031878352165, 0.3249886631965637, -0.15906427800655365, -0.5715239644050598, 0.45660528540611267, -0.7930262088775635, 0.153046652674675, -0.03986496850848198, -0.7353913187980652, 0.29730188846588135, 0.09032529592514038, -1.1106512546539307, 0.07866792380809784, 0.386640340089798, 1.218996286392212, -1.0882142782211304, -0.12461010366678238, 0.14999790489673615, 0.17201541364192963, -0.8910980224609375, 0.967712938785553, 0.3639691174030304, -0.17555102705955505, 0.04135604947805405, -0.3479202687740326, 0.22794513404369354, -0.6032835841178894, 0.06670632213354111, -0.8905718326568604, 0.05943313241004944, 0.41198956966400146, -0.07789494842290878, 0.7600913643836975, -0.18339090049266815, 0.9434784650802612, -0.12239953130483627, -1.5633758306503296, -0.09382058680057526, 0.22531303763389587, 0.15569163858890533, -0.9132891893386841, 0.30272430181503296, -0.001659535919316113, -0.6055629849433899, -0.002711508423089981, 0.5073816776275635, 0.5295003056526184, -0.04251612350344658, 0.2467660903930664, 0.7576306462287903, -0.07598457485437393, 0.060623712837696075, 0.35082074999809265, 0.6316215991973877, 0.22060120105743408, 0.28997787833213806, -0.5519999861717224, 0.017965633422136307, -0.5875588059425354, -0.3441515564918518, 0.5256516337394714, 0.40427884459495544, 0.862654447555542, 0.32313141226768494, -0.8585972785949707, -0.4630356729030609, -0.30609947443008423, 0.47165969014167786, 1.4889390468597412, -0.122578464448452, -0.193428173661232, -0.7382602691650391, -0.14157989621162415, -0.4838329255580902, -0.38380151987075806, -0.48380714654922485, -0.5252587795257568, -0.41865143179893494, -1.380108118057251, 0.778451681137085, 0.32522523403167725, 1.4404016733169556, -0.5111793875694275, -0.41293108463287354, -0.37904518842697144, 0.09727345407009125, -0.6696504354476929, -0.8203374147415161, 0.5763468742370605, 0.09700673818588257, -0.11752399802207947, -0.21040840446949005, -0.426078736782074, 0.40537944436073303, -0.3044664263725281, 0.8970944881439209, -0.8773618936538696, -0.5410986542701721, 0.5035223364830017, 0.7288742065429688, -1.0830104351043701, -0.2776826322078705, 0.017002779990434647, 0.1989273875951767, 0.01031420473009348, 0.2353176325559616, 0.22274035215377808, -0.15185242891311646, -0.06087363138794899, -0.33057263493537903, -0.16222764551639557, 0.20110434293746948, 0.5959911942481995, 0.5177865624427795, -0.0521094873547554, -0.1220911517739296, -0.8978250026702881, 0.9205164313316345, 0.25462520122528076, -0.07890807092189789, 0.20955859124660492, -0.5842129588127136, 0.016949258744716644, 0.5851395130157471, -0.45800119638442993, -0.20039154589176178, -0.6219714283943176, 0.7718637585639954, -0.7612645030021667, -0.12883779406547546, -0.011250782757997513, 0.2747838497161865, -0.078892782330513, 0.5774945616722107, 0.5300433039665222, 0.1423632651567459, 0.7461860179901123, 0.7013847827911377, -0.8987595438957214, 0.976742684841156, -0.1598285436630249, 0.31790581345558167, 0.555354654788971, -0.057680170983076096, -0.8404330611228943, -0.589002788066864, -0.498967707157135, 0.01908806897699833, -0.4091518223285675, 0.4830697774887085, -0.7590038776397705, -1.58163583278656, -0.03775212541222572, -0.9501293897628784, 0.015774592757225037, -0.4569571316242218, -0.24057994782924652, -0.5853970646858215, -1.0692839622497559, -0.9127745032310486, -0.900725781917572, -0.5282214879989624, -0.8162003755569458, 0.2508823871612549, 0.0832299217581749, -0.14898811280727386, -0.46233025193214417, -0.4637317657470703, -0.21988445520401, 0.9818167090415955, -0.629132866859436, 0.6144711971282959, 0.027024894952774048, -0.408880352973938, 0.24148781597614288, -0.3215622007846832, 0.48384717106819153, -0.3047674596309662, -0.028555039316415787, -1.0645023584365845, 0.48548656702041626, -0.05344100296497345, -0.4888503849506378, 0.30602630972862244, 0.5061395764350891, 0.7512884736061096, -0.2633991837501526, -0.37757694721221924, 0.4812989830970764, 1.3988134860992432, -0.39772653579711914, 0.3384992182254791, 0.17925411462783813, 1.3129771947860718, 0.4739092290401459, -0.44480323791503906, 0.350038081407547, 0.8590642213821411, -0.028407815843820572, 0.4213328957557678, -0.5843139290809631, 0.05063053220510483, -0.5808271169662476, 0.27994436025619507, 1.4287227392196655, -0.178084135055542, 0.4106200635433197, -1.0855329036712646, 0.7744846940040588, -1.4231747388839722, -1.2341519594192505, 0.7113727331161499, 0.3475680947303772, -0.25919023156166077, -0.6049556136131287, -0.23734594881534576, -0.08953172713518143, 0.6583858132362366, 0.06562202423810959, -0.16583207249641418, -0.3562086522579193, -0.055475521832704544, 0.7656040787696838, 0.7843790650367737, 0.22602996230125427, -0.6832834482192993, 0.6336077451705933, 14.949325561523438, 0.8239385485649109, -0.04981948807835579, 0.701892614364624, 0.7792778611183167, 0.46299344301223755, -0.12170812487602234, 0.18204912543296814, -0.6256465315818787, -0.12879133224487305, 0.6207296252250671, 0.4551214277744293, 0.6576868295669556, 0.24116934835910797, -0.23652604222297668, 0.2721691131591797, -0.5157424211502075, 1.235507607460022, 0.8109288811683655, -1.3522785902023315, 0.21727757155895233, 0.5450047850608826, 0.10837580263614655, 0.4203512966632843, 1.1096504926681519, 0.5625831484794617, 0.643355667591095, -0.4402664303779602, 0.5685981512069702, 0.2854195535182953, 0.9584164023399353, -0.03125636652112007, -0.009596966207027435, 0.34384727478027344, -0.7727651000022888, -0.4073300361633301, -0.4285311698913574, -1.0286970138549805, -0.19409729540348053, 0.1820829212665558, -0.1699269413948059, -0.4203386902809143, 0.110803984105587, 0.5349698662757874, 0.536486804485321, 0.03077150322496891, 0.05773766338825226, 0.3177322447299957, -0.00798022747039795, 0.12773790955543518, 0.41331902146339417, 0.4548310935497284, 0.0905219167470932, 0.007012987043708563, -0.10771743953227997, 0.295737087726593, 0.10481386631727219, 0.8814292550086975, -0.4402318298816681, -0.3399147391319275, -0.1806093007326126, -0.17482273280620575, -0.3808151185512543, 1.2161214351654053, 0.4176357686519623, 0.03146280720829964, 0.1933332234621048, 0.3271538317203522, 0.6055046319961548, 0.002141509437933564, -0.5453916192054749, -0.49520382285118103, 0.12876732647418976, -0.3313235342502594, -0.02661186084151268, 0.67041015625, -0.29137298464775085, -0.5419111251831055, -0.690758228302002, -0.2542734146118164, 0.0751551017165184, -1.1669175624847412, -1.2083897590637207, 0.7947453260421753, -0.2278003990650177, -0.28687432408332825, 0.8414543867111206, -0.9793519377708435, -0.44186297059059143, 0.05931493639945984, -1.2538037300109863, -0.8828291296958923, 0.04375522583723068, -0.1692018359899521, -0.5601864457130432, -0.1922241747379303, 0.7520859241485596, 0.051152002066373825, -0.019793560728430748, 0.2047358602285385, -0.09214606136083603, 0.18407803773880005, -0.23072032630443573, -1.022911548614502, 0.37044745683670044, -0.05767667293548584, -0.014533509500324726, 0.3735579252243042, 0.17382563650608063, 0.7034622430801392, -0.696619987487793, 0.5432306528091431, 0.4881865382194519, -0.7330228090286255, -0.3466755449771881, -0.8419313430786133, -0.6238185167312622, -0.38443389534950256, 0.7139350175857544, 0.2535489499568939, 0.28889861702919006, 0.004859158769249916, -0.6279879212379456, -0.3561853766441345, -0.7015435695648193, -0.16782839596271515, 0.5356809496879578, -1.184436559677124, -0.5866668224334717, 0.1994481384754181, -0.29822516441345215, -0.819129228591919, -0.3259030878543854, -0.618298351764679, 0.18446652591228485, -0.2359391748905182, 1.0662529468536377, -0.4601829946041107, 0.8479951620101929, 0.6186158061027527, 0.006956703029572964, -0.9028196930885315, -0.563107430934906, -0.8698858022689819, 0.07399030029773712, 0.393878698348999, -0.17776177823543549, -0.4924308955669403, 0.35816988348960876, 0.7457311749458313, -0.028596997261047363, -0.4314899444580078, -0.6646308302879333, -0.017628828063607216, -0.408429354429245, -0.39777258038520813, 0.01836692914366722, 0.4337710440158844, 0.43518802523612976, 0.2640150189399719, 0.6592483520507812, 0.20048514008522034, 0.3891220986843109, -0.6834377646446228, 0.061787236481904984, -0.10622801631689072, 0.18700149655342102, -0.5530232787132263, -0.6710913181304932, -1.3747169971466064, 0.2923557460308075, -1.5517756938934326, 0.030601102858781815, -0.8879023790359497, -0.13956192135810852, 0.07876809686422348, -0.3430153727531433, 0.027494318783283234, 0.5437081456184387, -0.2160281240940094, -0.01745869591832161, -0.28891581296920776, -0.5521290302276611, 0.8020825982093811, 0.6742389798164368, -1.0027778148651123, 0.26492202281951904, -0.32501110434532166, -0.7624657154083252, 0.44244951009750366, 0.2110188752412796, -0.4255819022655487, -0.48674389719963074, -1.0147989988327026, 0.3579728305339813, -0.3680320978164673, 0.3859993815422058, -0.9657555818557739, 1.0087528228759766, 0.6054474711418152, -0.03670826181769371, 0.041709382086992264, 0.2928038239479065, -1.0338422060012817, -0.7567003965377808, 0.19748656451702118, -0.7084715366363525, 0.20391154289245605, -0.4343266189098358, -0.14302705228328705, -0.5091363191604614, 0.7978018522262573, 0.019070886075496674, -1.4345933198928833, -0.2422252595424652, 0.3857837915420532, -0.5692318081855774, 0.4584086537361145, -0.15872438251972198, -0.3805141746997833, -1.2900043725967407, -0.18050067126750946, 0.08813181519508362, 0.18651124835014343, -0.69929438829422, 0.6624476313591003, 0.25965484976768494, -1.4056130647659302, 0.352189838886261, 0.28531718254089355, 0.016660571098327637, 0.10115518420934677, 0.5128771662712097, 0.27399203181266785, -0.009415676817297935, 0.5917260646820068, -0.12269110977649689, 0.03352335840463638, -0.4479769468307495, 0.2646723687648773, 0.6803524494171143, -0.4223000407218933, -0.13192333281040192, 0.7526662349700928, 0.3548630177974701, -0.7443175315856934, 0.07023845613002777, -1.1279484033584595, -0.3811449706554413, -0.0411759689450264, 0.27580106258392334, 0.35447272658348083, -0.3288578987121582, -0.5266487002372742, -0.5484739542007446, 0.34532082080841064, -0.09669163078069687, -0.4970572888851166, 0.7572588920593262, 0.01917898841202259, -0.2814805805683136, 0.16859520971775055, 0.9706745743751526, -1.0675257444381714, -0.7275918126106262, -0.7863405346870422, -0.6019391417503357, -0.2143186777830124, 0.5190861225128174, 0.07679368555545807, -0.7152780294418335, 0.9902165532112122, 0.8003405928611755, 0.4392378628253937, 0.2491304576396942, -0.04211890324950218, 0.0033305762335658073, 0.7148869633674622, -0.4605659544467926, -1.167012095451355, -0.2742854356765747, 1.3927823305130005, 0.9945605993270874, -0.5911102890968323, 0.1790180504322052, -0.42259302735328674, -0.9248297214508057, 0.6127336025238037, 0.18981431424617767, -0.538611114025116, 0.8206469416618347, -0.07759353518486023, -0.21644841134548187, -0.20873796939849854, -0.7305406332015991, -0.6552090048789978, 0.8182145953178406, 1.572919487953186, 0.27512773871421814, 0.1489839255809784, 0.44370782375335693, 0.6544740200042725, 0.011966345831751823, 0.047773465514183044, 0.3998367488384247, 0.13819478452205658, -0.017195729538798332, 0.4462307095527649, -0.2510750889778137, 0.7055339217185974, -0.6809611916542053, -0.5168097615242004, 0.3460843861103058, 0.4406522512435913, 0.43549108505249023, 0.2006738781929016, 1.092810869216919, 0.18516308069229126, 0.5417914390563965, -0.028628021478652954, 0.6096770167350769, -0.1306803673505783, -0.2895487844944, -0.14262166619300842, -0.910880446434021, 0.21764029562473297, -0.7078387141227722, -0.5608188509941101, -0.5705231428146362, 0.03667623922228813, -0.04650817811489105, -0.18526063859462738, 0.4345925450325012, 0.9898073077201843, 0.5446075201034546, 0.6115112900733948, 0.04434577748179436, -0.7032119631767273, -0.2609195411205292, -0.36094608902931213, 0.08675862103700638, -0.17724329233169556, -0.12788116931915283, -0.45630943775177, -0.27098777890205383, 0.07738363742828369]}, "authors": [{"authorId": "2742407", "name": "Arda Sahiner"}, {"authorId": "19278348", "name": "Tolga Ergen"}, {"authorId": "2027017491", "name": "Batu Mehmet Ozturkler"}, {"authorId": "2060416777", "name": "J. Pauly"}, {"authorId": "33002157", "name": "M. Mardani"}, {"authorId": "3173667", "name": "Mert Pilanci"}], "references": [{"paperId": "14a3aae8060338e3fbefc2af694890b019874d4f", "title": "Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations"}, {"paperId": "10a24023707e5e307856bde478385f4e1d6f5b9a", "title": "Fast Convex Optimization for Two-Layer ReLU Networks: Equivalent Model Classes and Cone Decompositions"}, {"paperId": "63664d0ef6c8d0b8f7768e9e5535cb911e565fcb", "title": "Efficient Global Optimization of Two-layer ReLU Networks: Quadratic-time Algorithms and Adversarial Training"}, {"paperId": "427fe0e6e2e2d6f927c62e80c75706e02c2a747f", "title": "Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "621bfee510b99059e394c857d954158c814c6385", "title": "Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs"}, {"paperId": "f829a355de02c08567927154d3045a6eb5425c91", "title": "Is Attention Better Than Matrix Decomposition?"}, {"paperId": "39b492db00faead70bc3f4fb4b0364d94398ffdb", "title": "Do Vision Transformers See Like Convolutional Neural Networks?"}, {"paperId": "d6206e3cc9fdefcaa901cd9634a892216b6a98d4", "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions"}, {"paperId": "9b6af0e358e76d22f209c75b1702c3e6ea7815b1", "title": "Global Filter Networks for Image Classification"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "f0747c025e4557104231d1ebd88372c316630388", "title": "Training Quantized Neural Networks to Global Optimality via Semidefinite Programming"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "a7721b6523971394a8bd4bfda139122ef59b22cd", "title": "Sparse Attention with Linear Units"}, {"paperId": "560c2ea251a2818008c55a69fdd2599617699b66", "title": "Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "cc8dcd83a36ac228fc401f4f0d727534ae541fe1", "title": "Neural Spectrahedra and Semidefinite Lifts: Global Convex Optimization of Polynomial Activation Neural Networks in Fully Polynomial-Time"}, {"paperId": "49d4a9e019b19b7a294d5590fcf43154663f2524", "title": "Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms"}, {"paperId": "0acd7ff5817d29839b40197f7a4b600b7fba24e4", "title": "Transformer Interpretability Beyond Attention Visualization"}, {"paperId": "a0886a4e611b211c4baf7b2fa5971efcb505c9de", "title": "Convex Regularization Behind Neural Reconstruction"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2f7dc1ee85e9f6a97810c66016e09ffeed684f03", "title": "Fourier Neural Operator for Parametric Partial Differential Equations"}, {"paperId": "a51547aabbb94e7347fdad77ad8ff3c76182995a", "title": "Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference"}, {"paperId": "1fde90108b8984c5d7c7f6d06fa4919a2a5bbd36", "title": "Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time"}, {"paperId": "aa607c6535e3e1953e5bd48f36efc432a0c167f8", "title": "The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions"}, {"paperId": "45988d39ab1b0e5199e1f0f31952760bc763e611", "title": "The Lipschitz Constant of Self-Attention"}, {"paperId": "8b9d77d5e52a70af37451d3db3d32781b83ea054", "title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "fd9517a8d258dfb91a9b07141485f7cdfd38fa6b", "title": "Convex Geometry and Duality of Over-parameterized Neural Networks"}, {"paperId": "9705b124f6d3bb77c4bc3ce3d83c2f2ebe8ee770", "title": "Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-Layer Networks"}, {"paperId": "6aa86110a83febcb6064348644470d90678aa7cd", "title": "Revealing the Structure of Deep Neural Networks via Convex Duality"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "4d9b1fd31ba5f74e0707f654691fe71aa83df4b0", "title": "How do infinite width bounded norm networks look in function space?"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "0629e13627f14ff0a1d0f534270278fbd5c90b5b", "title": "Decoupling Gating from Linearity"}, {"paperId": "f723eb3e7159f07b97464c8d947d15e78612abe4", "title": "AutoAugment: Learning Augmentation Policies from Data"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "4e8917e73e02c76d55ded62e43541d44684a4c8a", "title": "Implicit Regularization in Matrix Factorization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "b6292128b04f878fe620bfc5d45b78444c1774bd", "title": "Subspace Learning and Imputation for Streaming Big Data Matrices and Tensors"}, {"paperId": "961eabeaebd7035cd7668c9917fa9c39462e1113", "title": "Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization"}, {"paperId": "aaaa97ec983ecc3c602a1674bb04dbb2508541a3", "title": "Decentralized Sparsity-Regularized Rank Minimization: Algorithms and Applications"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "c369d9192c9754fb7a04529c68f2fb286f646df7", "title": "The Power of Convex Relaxation: Near-Optimal Matrix Completion"}, {"paperId": "f946854137c46e173c9511c62aca5ffb10f59eda", "title": "Semi-infinite programming, duality, discretization and optimality conditions"}, {"paperId": "0c9bb579d8ad6ac987f7a16b66ddace671fc57c5", "title": "Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization"}, {"paperId": "5012fc8bb64a6911a2e58aab8a7ad4df9ae07af8", "title": "Matrix Differential Calculus with Applications"}, {"paperId": "0c53dd35f92534faabcf4f1689cc12195d8a46ff", "title": "RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?"}, {"paperId": "dacf7ac8924ee959a1096fcc31da6b568ad4093e", "title": "S IMPLE T RON : E LIMINATING S OFTMAX FROM A TTENTION C OMPUTATION"}, {"paperId": "6bbaaee20e891e38a8a6fed489a277dc46c7134d", "title": "Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices"}, {"paperId": null, "title": "Simpletron: Eliminating softmax from attention computation"}, {"paperId": "4f1b02e150444eb3158de9e7b301a3c348786a4e", "title": "Exact and Relaxed Convex Formulations for Shallow Neural Autoregressive Models"}, {"paperId": null, "title": "Pytorch image models"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "6d389485b399ae7b60c1f426f1168f4eacaba64f", "title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems"}, {"paperId": "283943f3c1390f8641dd34ef7d00fa30cfa89713", "title": "An Introduction to Hyperplane Arrangements"}, {"paperId": "5ced94fdad3c0dc591935aa2dce06608474cee9d", "title": "Digital Object Identifier (DOI) 10.1007/s10107-004-0564-1"}, {"paperId": null, "title": "Unraveling Attention via Convex Duality"}]}