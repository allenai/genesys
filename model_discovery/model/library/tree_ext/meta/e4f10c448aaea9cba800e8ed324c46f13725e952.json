{"paperId": "e4f10c448aaea9cba800e8ed324c46f13725e952", "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers", "abstract": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.14858", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "The equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference is formally established, implying that Pre- LN Transformers can be substituted with Pre-(C)RMSorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement."}, "embedding": {"model": "specter_v2", "vector": [0.6919413208961487, 0.9996774196624756, -0.7334992289543152, -0.44440770149230957, -0.20104919373989105, 0.1378238946199417, 0.6997578740119934, -0.17046503722667694, -0.7859339714050293, -0.2554294764995575, 0.7903246283531189, -0.23086845874786377, 0.5024689435958862, -0.07415387779474258, -0.2864726185798645, -0.26726827025413513, -0.8267999291419983, 0.01809803396463394, -0.22713685035705566, -0.3992605209350586, -0.25098174810409546, -0.4884190857410431, -1.1901432275772095, 0.30360546708106995, 0.16380105912685394, 0.9712699055671692, -0.22693085670471191, 0.5089091062545776, -0.9647010564804077, 0.8602667450904846, 0.6456528902053833, -0.5927425026893616, 0.3083473742008209, 0.0414397157728672, -0.19049778580665588, 0.06621456891298294, 0.4680110216140747, -0.5905995965003967, -0.3497268855571747, 1.158676266670227, -0.44791552424430847, 0.5375884771347046, 0.4808778762817383, -0.711275577545166, 0.019098250195384026, 1.0888309478759766, 0.567055881023407, 0.4528401792049408, -0.8072814345359802, -0.7069510817527771, 1.086828589439392, -1.1047720909118652, -0.05581330135464668, 1.2264395952224731, 0.6895241737365723, 0.381178617477417, -0.32190820574760437, -0.7626233696937561, 0.4967091679573059, -0.2491122931241989, -0.9036402702331543, -0.6806134581565857, -0.05823127552866936, -0.1261439174413681, 1.7363393306732178, 0.08681222051382065, -0.42272424697875977, 0.3345087766647339, -0.06000529229640961, 1.0225824117660522, -0.10503530502319336, -0.5182809829711914, -0.13030852377414703, 0.19614383578300476, -0.009167078882455826, 0.7744436860084534, -0.36924293637275696, 0.3051387071609497, -0.9746553897857666, 0.07304057478904724, 0.20554152131080627, 0.1520177572965622, 0.22597090899944305, -0.18628482520580292, -0.16336354613304138, 0.7831133604049683, 0.38598379492759705, 0.9118471741676331, -0.28524985909461975, 0.6624008417129517, 0.7276116013526917, 0.3905036151409149, 0.5004721283912659, 0.16255852580070496, -0.15825118124485016, 0.2205091416835785, -1.2208690643310547, -0.05063946172595024, -0.5169206857681274, 0.7090902328491211, -0.1622690111398697, 0.6318156123161316, -0.7660298943519592, 0.29354727268218994, 1.259865164756775, 0.15501558780670166, 0.49767765402793884, -0.7626168727874756, 0.4774141311645508, -0.5184624195098877, -0.49375617504119873, -0.5494422912597656, -0.045205798000097275, -0.6179670691490173, -1.3245007991790771, -1.0526174306869507, -0.7983819246292114, 0.2522442042827606, -0.8690816164016724, 0.7661799192428589, -0.593717634677887, 0.10723575204610825, 0.3106234073638916, 0.22439156472682953, 0.33421334624290466, 0.5742011070251465, 0.28625980019569397, -0.19648821651935577, 0.7400099635124207, -0.5374144911766052, -1.1271518468856812, -0.9906779527664185, 0.6833943128585815, -0.23663219809532166, 0.3447245955467224, -0.41444095969200134, -0.8839741945266724, -0.6240599155426025, -0.9266742467880249, 0.1186109334230423, -0.6077243089675903, 0.02675965055823326, 1.1172131299972534, 0.4624991714954376, -0.9378049373626709, 1.1804473400115967, -0.296138733625412, -0.13618150353431702, 0.755892813205719, 0.3120628297328949, 0.0686585083603859, -0.09312505275011063, -1.2005467414855957, 0.38082075119018555, 0.42276424169540405, -0.423705130815506, 0.16301745176315308, -0.7433764338493347, -1.254918098449707, -0.019960694015026093, 0.23708325624465942, -0.19346638023853302, 1.4581856727600098, -0.04314350709319115, -1.3170188665390015, 0.4822703003883362, -0.314640074968338, -0.017451945692300797, 0.39007139205932617, -0.1211966946721077, -0.47214704751968384, -0.6149224638938904, -0.2817867696285248, 0.04587027430534363, 0.5977810025215149, -0.18047787249088287, -0.1666579693555832, 0.15810942649841309, -0.5965732932090759, -0.015638265758752823, -0.6613703370094299, 0.879631757736206, -0.25267094373703003, -0.36441895365715027, 0.6043551564216614, 1.0864711999893188, -0.03147967532277107, -0.21774861216545105, -0.4131309986114502, -0.6970102787017822, 0.31780922412872314, 0.14890427887439728, 1.0714024305343628, -0.8382615447044373, -1.107593059539795, 0.29035061597824097, -0.03090827353298664, -0.05185253545641899, -0.5593613386154175, 0.2613458037376404, -0.5949658751487732, 0.3857437074184418, -0.2007303088903427, -1.3536550998687744, 0.1480865329504013, 0.12020882219076157, -0.964870274066925, 0.20811118185520172, -0.11396131664514542, 0.93602454662323, -0.8939109444618225, -0.08372996002435684, 0.06233219802379608, 0.53534996509552, -0.8994004130363464, 1.3645809888839722, 0.002486118348315358, -0.3852786421775818, 0.3288581371307373, -0.3782274127006531, 0.28073111176490784, -0.284292072057724, 0.5457080006599426, -0.5034468173980713, -0.07625216990709305, 0.6252032518386841, -0.6034961342811584, 1.04728364944458, -0.19640184938907623, 0.5547889471054077, -0.061535317450761795, -0.6046261787414551, -0.05025956407189369, 0.33328938484191895, 0.19289849698543549, -0.388484925031662, 0.7993106842041016, 0.19023288786411285, -0.8486915826797485, 0.6109194159507751, 0.7180721163749695, 0.1993124932050705, -0.018436050042510033, 0.19845858216285706, 0.8168126344680786, -0.10397960990667343, 0.5850167274475098, 0.19462823867797852, 0.3595045804977417, -0.00952035840600729, 0.30859270691871643, 0.27848875522613525, 0.21995492279529572, -0.9979711771011353, -0.1761133074760437, 0.49703866243362427, 1.0356745719909668, 1.023449420928955, 0.35824865102767944, -0.2425183653831482, -0.17634573578834534, -0.19454413652420044, 0.48524874448776245, 1.85137140750885, -0.6648022532463074, -0.5112087726593018, -0.6010063290596008, -0.14782404899597168, -0.26923316717147827, 0.18927130103111267, -0.36732766032218933, -0.3505817651748657, -0.4527725875377655, -1.381980538368225, 1.003301978111267, 0.40765973925590515, 0.8100892901420593, -0.2450285404920578, 0.1977176070213318, -0.3830520212650299, 0.24841047823429108, -0.8715020418167114, -0.5243005752563477, 0.6666671633720398, -1.058416485786438, -0.0725330114364624, 0.19783005118370056, 0.14641138911247253, 0.5309074521064758, -1.0666391849517822, 0.6174662113189697, -0.5873191356658936, 0.23985294997692108, -0.2793516516685486, 0.7423689365386963, -0.44465941190719604, -0.7862992286682129, 0.17166589200496674, 0.27837786078453064, 0.20119181275367737, 0.3346881866455078, 0.2515420615673065, 0.4926184117794037, -0.4100029468536377, -0.576274037361145, 0.0202406607568264, 0.31355878710746765, -0.3468993008136749, 0.6823049187660217, -0.11355629563331604, -0.4667085111141205, -1.4230520725250244, 1.1116905212402344, 0.20966355502605438, -0.3892934322357178, -0.028316693380475044, -1.2630890607833862, 0.1498698890209198, 0.4761621952056885, -0.5400491952896118, -0.031091105192899704, -0.914617657661438, 0.09264706820249557, -0.538536012172699, 0.1606781929731369, 0.2322758138179779, 0.29302549362182617, 0.35787349939346313, 0.07189185917377472, 0.5214545726776123, 0.6187312602996826, -0.5181574821472168, 0.7177755236625671, -0.40082642436027527, 0.7932698726654053, 0.4277316629886627, 0.6526391506195068, -0.1934075653553009, -0.21951404213905334, -0.5925700664520264, -0.5183688402175903, -0.4489304721355438, 0.0716891661286354, 0.017183752730488777, -0.04550971835851669, -0.5471123456954956, -0.9421340823173523, -0.2178851217031479, -0.6762259602546692, -0.08104266971349716, -0.21842660009860992, -0.06981411576271057, -0.12299712002277374, -1.1086370944976807, -1.2402808666229248, -0.7301855087280273, -0.7904981970787048, -0.6666955947875977, 0.17239414155483246, -0.1528106927871704, -0.1508023887872696, -0.3815014362335205, -0.5325217843055725, -0.5986050367355347, 1.3957798480987549, -0.5850484371185303, 0.9495202302932739, -0.3004503846168518, 0.20303019881248474, -0.19643819332122803, 0.10548557341098785, 1.2132207155227661, -0.17162121832370758, 0.07292338460683823, -0.9967766404151917, 0.27799779176712036, -0.12066511809825897, 0.039346467703580856, 0.15124155580997467, -0.011761683039367199, 0.6749119758605957, -0.4065992832183838, -0.12066467851400375, 0.8238057494163513, 1.4293993711471558, -1.0012528896331787, 0.15082739293575287, 0.13512903451919556, 0.7591325044631958, -0.06267429143190384, -0.417803019285202, 0.4804002344608307, -0.053533099591732025, 0.07371804863214493, 0.4174549877643585, -0.3130262792110443, -0.04070661589503288, -0.47848671674728394, 0.6490566730499268, 2.0341367721557617, 0.02970908209681511, 0.21611864864826202, -0.7046883702278137, 0.3374689817428589, -0.8173169493675232, -1.059204339981079, 0.7987856268882751, 0.6721738576889038, 0.7284290194511414, -0.21992334723472595, -0.6708975434303284, 0.20747746527194977, 0.09186495840549469, 0.7201547026634216, -0.05506564676761627, -0.7197776436805725, -0.09777998924255371, 0.5071074366569519, 0.8714945912361145, 0.6391516327857971, -0.41504478454589844, 0.2979535162448883, 14.730679512023926, 0.6116636991500854, -0.20750528573989868, 0.5387107729911804, 0.40775537490844727, 0.01618128828704357, -0.2839714586734772, -0.6003242135047913, -1.1634976863861084, -0.0024774956982582808, 1.0227185487747192, -0.10574925690889359, 0.8024364113807678, 0.3578435778617859, -0.10264069586992264, 0.49456506967544556, -0.285075306892395, 0.7606082558631897, 0.5929122567176819, -1.3819580078125, 0.3330704867839813, 0.114383265376091, 0.30298730731010437, 0.6317803263664246, 0.6760578751564026, 0.9634780883789062, 0.3321162760257721, -0.5970882177352905, 0.36839985847473145, 0.3283500671386719, 1.2260799407958984, -0.20360881090164185, 0.8811739087104797, 0.5996252298355103, -0.8167664408683777, -0.029432568699121475, -0.5396278500556946, -1.1234768629074097, -0.24175173044204712, 0.3368651568889618, -0.6939952373504639, -0.4575134217739105, -0.1778048723936081, 0.7354491949081421, -0.0740247368812561, -0.09266985207796097, -0.09159175306558609, 0.6533563137054443, -0.38633087277412415, 0.44698449969291687, 0.2402142435312271, 0.20773617923259735, -0.24003757536411285, 0.0740683302283287, 0.1217387244105339, -0.3704644441604614, -0.0005403492250479758, 0.3681178092956543, -0.8175994157791138, 0.10994362831115723, -0.06978031992912292, -0.5619658827781677, -0.1906706988811493, 0.6396121382713318, 0.7125870585441589, 0.06533868610858917, -0.28201165795326233, 0.19228670001029968, 0.48763564229011536, 0.31184300780296326, -0.2413664311170578, -0.3682152330875397, 0.9355258941650391, 0.14191864430904388, -0.14334607124328613, 0.8195740580558777, -0.4091443121433258, -0.5482785105705261, -0.6915273070335388, -0.3324628472328186, 0.23507267236709595, -0.43447157740592957, -0.6914812922477722, 0.9345858693122864, -0.2748641073703766, -0.5373141169548035, 0.025625403970479965, -0.7945356369018555, 0.05795640870928764, 0.42114847898483276, -1.490163803100586, -0.22236567735671997, 0.17272543907165527, -0.3110787272453308, -0.8604997396469116, -0.37889063358306885, 1.2314974069595337, 0.4088880717754364, -0.3534873425960541, 0.1673247218132019, 0.39537662267684937, 0.4594138562679291, -0.3366275727748871, -0.662808358669281, 0.6060270071029663, 0.45139214396476746, -0.3005644381046295, 1.0002758502960205, 0.15295735001564026, 0.41961008310317993, -0.42899617552757263, -0.4980613887310028, 0.9132557511329651, -0.5284571647644043, -0.3196277618408203, -0.9558646082878113, -0.695892333984375, 0.586526095867157, 0.37479346990585327, -0.029439326375722885, 0.4767351746559143, 0.24864161014556885, -0.5781385898590088, -0.3606667220592499, -0.519309937953949, 0.10169605165719986, 0.675745964050293, -0.9432224631309509, -0.3873421847820282, -0.13546422123908997, 0.1403532475233078, -0.8464210033416748, -0.8932108283042908, 0.16167771816253662, -0.11558044701814651, -0.011938688345253468, 0.9532883763313293, -0.6714026927947998, 0.4787818193435669, 0.7201076149940491, -0.4972262978553772, -0.580328106880188, 0.27952539920806885, -0.8174846172332764, -0.06762818992137909, 0.1953701674938202, 0.6498358249664307, -0.464398056268692, 0.5884649157524109, 0.5227454304695129, 0.262462854385376, -0.5295905470848083, -0.7139230966567993, -0.37137851119041443, -0.29943618178367615, -0.9568313956260681, 0.5496369004249573, 0.01720082014799118, -0.30225691199302673, 0.027677001431584358, 0.6074163913726807, 0.2238650619983673, 0.0021100114099681377, -0.9936131238937378, 0.03604953736066818, -0.1309565007686615, 0.3055266737937927, -0.6050541996955872, -0.5053952932357788, -1.0655667781829834, -0.06600888818502426, -1.5269300937652588, -0.02666453830897808, -1.1297686100006104, -0.4361410439014435, 0.3259270489215851, -0.4932742416858673, 0.40434694290161133, 0.5608309507369995, -0.08260733634233475, -0.29362455010414124, -0.4776582419872284, -0.4023449122905731, 0.9797301888465881, 0.573900043964386, -0.7262049913406372, 0.29644033312797546, 0.08321397751569748, 0.02527797780930996, 0.34116944670677185, 0.6134048700332642, -0.6925525665283203, -1.0771619081497192, -0.9486492872238159, 0.39325180649757385, -0.6646409034729004, -0.3326779007911682, -0.5451866388320923, 0.6573652625083923, 0.3270735740661621, -0.0354345478117466, 0.12765303254127502, 0.3420094847679138, -1.103088140487671, -0.588243842124939, 0.15675164759159088, -0.9069609642028809, 0.09117350727319717, -0.07678867131471634, -0.8380824327468872, -0.47855138778686523, 0.828075647354126, 0.1805620789527893, -0.5717838406562805, -0.19991521537303925, 0.5803134441375732, -0.5781866908073425, 0.20426404476165771, -0.5154273509979248, -0.17099642753601074, -1.0080311298370361, -0.21969693899154663, 0.21663415431976318, -0.060950495302677155, -0.5342061519622803, 0.9984318614006042, 0.49860328435897827, -1.4519977569580078, 0.30389177799224854, 0.7719932794570923, -0.10326020419597626, -0.10781469941139221, -0.038102053105831146, 0.5550737977027893, -0.2873377799987793, 0.5122093558311462, 0.4396718740463257, 0.3590044379234314, -1.0217478275299072, -0.27539992332458496, 0.7506242394447327, -0.4077022671699524, -0.17600488662719727, 1.0932515859603882, -0.7288505434989929, -1.1993995904922485, 0.48773321509361267, -1.6881225109100342, -0.6473981142044067, -0.06480148434638977, 0.7426408529281616, 0.06164579465985298, 0.3661446273326874, -0.36134475469589233, -0.5496468544006348, -0.0600365549325943, 0.23119235038757324, -0.275790274143219, 0.9175153374671936, -0.025588802993297577, -0.5772321224212646, 0.7959778308868408, 1.3708155155181885, -0.668615460395813, -0.36271214485168457, -0.8185765743255615, -0.27348634600639343, -0.19135166704654694, 0.536173939704895, -0.041438713669776917, -0.9544789791107178, 0.6646181344985962, 0.6155371069908142, 0.0554913729429245, 0.33190619945526123, -0.4800204634666443, 0.06329711526632309, 0.677728533744812, -0.2104484885931015, -0.9949029684066772, -0.9779815673828125, 1.2267998456954956, 1.0067999362945557, -0.6558719873428345, 0.654480516910553, -0.1289670765399933, -0.5613870620727539, 0.7365705966949463, -0.08002842962741852, 0.2869148552417755, 0.9746121168136597, 0.38708606362342834, 0.37064358592033386, 0.2228558361530304, -1.0182397365570068, -0.705967128276825, 0.897617518901825, 0.8106738328933716, 1.050386905670166, -0.031139086931943893, 0.45851582288742065, 1.1260119676589966, -0.2151464819908142, 0.05586618557572365, 0.18302206695079803, 0.5857107043266296, -0.36230358481407166, 0.11163562536239624, -0.07621113955974579, 1.1409257650375366, -0.691157341003418, -0.7688101530075073, 0.40093404054641724, 0.6199986338615417, 0.33043572306632996, 0.5794603824615479, 0.9219743013381958, -0.4104292094707489, 0.8266048431396484, 0.43064218759536743, 0.1450609713792801, -0.5033659934997559, -0.47688451409339905, -0.40541520714759827, -0.7363894581794739, -0.3523484468460083, -0.08895936608314514, -0.23814673721790314, -0.7177330255508423, -0.6549444198608398, 0.6484485864639282, 0.05517129972577095, 0.5485896468162537, 1.009564995765686, 0.12695570290088654, 0.8155756592750549, -0.36440399289131165, -0.2052072435617447, -0.9740742444992065, -1.0850732326507568, -0.34053441882133484, -0.254840612411499, -0.059748172760009766, 0.0903572365641594, -0.29528817534446716, -0.2339494377374649]}, "authors": [{"authorId": "1471708315", "name": "Zixuan Jiang"}, {"authorId": "2098601", "name": "Jiaqi Gu"}, {"authorId": "2115315923", "name": "Hanqing Zhu"}, {"authorId": "1681705", "name": "D. Pan"}], "references": [{"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "12c6be503e4e5b7c9cb1810152d4364f26628a8d", "title": "Data-centric Artificial Intelligence: A Survey"}, {"paperId": "a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5", "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "1415479215dcfec5e2ee0d33f1e1565ae2c65bb9", "title": "Examining Scaling and Transfer of Language Model Architectures for Machine Translation"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0", "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "2b8088253e2378fce001a090fe923b81e8dedf25", "title": "RepVGG: Making VGG-style ConvNets Great Again"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2b2056bf5763e32811a69769fa8c223160125f9e", "title": "DNNFusion: accelerating deep neural networks execution with advanced operator fusion"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "40922d386116975853a743b1d810c1e0f03e886a", "title": "Understanding and Improving Layer Normalization"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "50684b147b752a07c313cb73d864f7b21bd8b703", "title": "Scaling Distributed Machine Learning with the Parameter Server"}, {"paperId": null, "title": "Pytorch distributed: Experiences on accelerating data parallel training"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "Chatgpt could cost over $700,000 per day to operate."}, {"paperId": null, "title": "Llm-analysis: Latency and memory analysis of transformer models for training and inference"}]}