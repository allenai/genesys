{"paperId": "5c577988ccebfea96de86678d04fd94fad367d2e", "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models", "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat", "venue": "arXiv.org", "year": 2023, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.16149", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "Jais and Jais-chat are introduced, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs) based on the GPT-3 decoder-only architecture that demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin."}, "embedding": {"model": "specter_v2", "vector": [-0.01863110065460205, 0.9764655828475952, -0.15369674563407898, 0.10180173814296722, -0.13236595690250397, -0.2026367485523224, 0.9790846705436707, -0.2804725170135498, 0.17123493552207947, -0.3568097949028015, 0.6308701038360596, -0.4772017300128937, 0.5823939442634583, -0.18372070789337158, -0.4263761639595032, -0.2663171887397766, -1.216246485710144, 0.4988349974155426, -0.20740757882595062, -0.6742616295814514, -0.31977033615112305, -0.7612452507019043, -0.5930068492889404, 0.23540915548801422, 0.5500919818878174, 0.05682213231921196, -0.18038301169872284, 1.123513102531433, -0.14585496485233307, 0.4033825695514679, 0.41525062918663025, -0.8056490421295166, -0.1376936286687851, -0.54023277759552, -0.26342618465423584, 0.1471794992685318, -0.11245934665203094, -0.6310878992080688, -0.22401851415634155, 0.31437796354293823, -0.34688663482666016, -0.07971256971359253, 0.8046960830688477, -0.7896347641944885, -0.6056454181671143, 1.3994660377502441, 0.8095701932907104, 0.9020695090293884, -0.3079109191894531, -0.35689154267311096, 1.0486857891082764, -1.47517991065979, 0.08073824644088745, 1.616603136062622, 0.15409177541732788, 0.5578053593635559, -0.17382898926734924, -0.3494212329387665, 0.4728930592536926, -0.21501149237155914, -0.7646273970603943, -0.41387200355529785, 0.08013919740915298, -0.11506064236164093, 1.6518393754959106, -0.7001345157623291, -0.016548695042729378, 0.856265127658844, -0.05734536424279213, 1.2904431819915771, -0.3180156946182251, -1.0562365055084229, -0.25460511445999146, -0.08947387337684631, -0.13943898677825928, 1.1175918579101562, -0.36540326476097107, 0.23838892579078674, -0.6174831986427307, -0.0460655577480793, 0.6335251927375793, -0.4192952513694763, -0.059761274605989456, -0.023292116820812225, -0.713712751865387, 1.0896821022033691, 0.1882649064064026, 1.0381613969802856, 0.192722886800766, 0.39336445927619934, 0.026317721232771873, 0.060950204730033875, -0.11797142773866653, -0.10161427408456802, -0.21298228204250336, 0.4409838020801544, -0.7676937580108643, 0.18379145860671997, 0.17658424377441406, 0.8093653321266174, -0.47281840443611145, 0.5926820039749146, -0.41604477167129517, -0.15716108679771423, 1.8382182121276855, 0.42158710956573486, 0.7545403242111206, -0.6159166693687439, 0.3134549558162689, -0.6760681867599487, -0.05900717154145241, -0.5706667304039001, -0.12771256268024445, -0.28373128175735474, -0.5058371424674988, -1.236990213394165, -0.09189426898956299, 0.03629162535071373, -1.0071802139282227, 0.7012699842453003, -0.14446435868740082, -0.19313426315784454, 0.5333197116851807, 0.4070415198802948, 0.6129685044288635, 1.0361601114273071, 0.24112512171268463, 0.15316307544708252, 0.8372052907943726, -1.033364176750183, -0.708074688911438, -1.4104242324829102, 0.7499752044677734, -0.18336662650108337, 0.2066514641046524, -0.2098773717880249, -1.4058632850646973, -0.6010573506355286, -1.0583006143569946, 0.08866053819656372, -0.396596223115921, 0.6565240621566772, 0.7764880657196045, 0.9091601371765137, -0.8648349642753601, 0.21892192959785461, -0.3575640320777893, -0.05179412662982941, 0.09816458821296692, -0.07612252235412598, 0.4791397750377655, -0.3970087468624115, -1.3733183145523071, 0.17710798978805542, 0.3852759003639221, -0.4173703193664551, -0.49767395853996277, -0.08168738335371017, -1.250213861465454, -0.1389358639717102, 0.10547850281000137, -0.1009647473692894, 1.3410289287567139, -0.20283722877502441, -1.431278944015503, 0.5025946497917175, -0.2998610734939575, -0.008103502914309502, -0.3543904423713684, 0.14045150578022003, -0.7269886136054993, -0.5149924159049988, -0.35890671610832214, 0.5910354852676392, 0.7057496309280396, -0.24827313423156738, 0.22214064002037048, 0.1206790953874588, -0.20074135065078735, -0.17035959661006927, -0.09691181033849716, 1.1399210691452026, -0.6241374015808105, -0.4808492660522461, 0.10524319112300873, 0.28288373351097107, 0.024940788745880127, -0.3911944031715393, -0.3265039026737213, -0.7536211013793945, 0.6222097277641296, -0.41499537229537964, 1.1517239809036255, -0.9808318614959717, -0.34724336862564087, -0.2722404897212982, -0.45529964566230774, -0.10018426924943924, -0.6375806927680969, 0.3448396325111389, -0.39420607686042786, 0.4294690191745758, -0.6436861157417297, -0.8908644914627075, -0.05579449236392975, -0.042972639203071594, -0.784736156463623, -0.18720103800296783, 0.7579579949378967, 1.1219967603683472, -0.7192316055297852, -0.0005296584567986429, -0.36948683857917786, 0.393116295337677, -1.169398307800293, 1.0894858837127686, -0.7335205078125, 0.2800029218196869, -0.211199089884758, 0.07681620121002197, -0.2667745053768158, -0.35498905181884766, 0.4852665662765503, -0.09070176631212234, 0.19337986409664154, 0.3432738482952118, -0.48279181122779846, 1.4789884090423584, -0.08000628650188446, 0.005334567278623581, -0.1615651547908783, -0.6860703825950623, 0.20842556655406952, 0.5282439589500427, -0.7678421139717102, -0.11250337213277817, 0.605304479598999, 0.7360196113586426, -0.07153302431106567, 0.5962230563163757, 0.28234440088272095, 0.45424044132232666, -0.4647163152694702, 0.6140187382698059, 1.0208561420440674, -0.317050963640213, 0.3632889986038208, 0.6657365560531616, 0.18252000212669373, 0.47244325280189514, 0.35111773014068604, 0.011617039330303669, 0.4588907063007355, -0.7498497366905212, -0.41018766164779663, -0.011236226186156273, 0.4672539234161377, 1.0672564506530762, 0.4690728485584259, -0.8066168427467346, -0.04902027174830437, -0.020842192694544792, 0.7112559080123901, 1.3392913341522217, -0.43905436992645264, -0.16599829494953156, -0.8335265517234802, -0.2818637490272522, -0.5746670365333557, 0.5174992680549622, -0.5701590180397034, -0.323646605014801, -0.8129424452781677, -1.3168948888778687, 0.8423982858657837, 0.07798846065998077, 0.7741217613220215, -0.7135487794876099, -0.1989395022392273, -0.18687547743320465, -0.17451098561286926, -0.5910705924034119, -0.7465429902076721, 0.09505204111337662, -0.6874094605445862, 0.11621038615703583, -0.13749051094055176, -0.1645166277885437, 0.17046166956424713, -1.1159093379974365, 0.9213258028030396, -0.29614683985710144, -0.1496368795633316, 0.15196488797664642, 0.324538916349411, -0.7627039551734924, -1.4158415794372559, -0.00046814072993583977, 0.016155585646629333, 0.22827573120594025, 0.1698986291885376, 0.5707252025604248, 0.5059163570404053, 0.02804448828101158, -0.45278200507164, -0.031002389267086983, 0.016893655061721802, -0.15812048316001892, 0.26655474305152893, -0.5245627164840698, -0.21844688057899475, -1.0204418897628784, 0.3798713982105255, 0.05655814707279205, -0.36091598868370056, 0.2751889228820801, -0.3770662248134613, -0.04505590349435806, 0.15270884335041046, -0.48604828119277954, -0.5266983509063721, -0.8410329818725586, 0.20059223473072052, 0.2542002201080322, -0.32835838198661804, 0.32427623867988586, 0.11036648601293564, 0.5007994771003723, 0.577610969543457, 0.560213029384613, 0.3601004481315613, 0.011920034885406494, 0.981522798538208, -0.9057519435882568, 0.46344587206840515, 0.13088476657867432, 0.2873230278491974, -0.08928830921649933, -0.41874372959136963, -0.21859359741210938, -0.34123679995536804, 0.3431381583213806, -0.43875500559806824, -0.44249001145362854, 0.43604251742362976, -0.7831518650054932, -0.3001689612865448, 0.1287374496459961, -0.9914954304695129, -0.3552851378917694, 0.47121548652648926, -0.5791489481925964, -0.26690882444381714, -0.7913910746574402, -1.2716851234436035, -0.43651095032691956, -0.4677257537841797, -1.091050386428833, 0.6027953028678894, 0.09765680134296417, -0.11067516356706619, -0.7830675840377808, 0.1848156601190567, -0.20053565502166748, 0.9447066783905029, -0.5229198336601257, 1.081283450126648, -0.02807779051363468, -0.24154126644134521, -0.030309932306408882, 0.7887013554573059, 0.17565152049064636, -0.3085848093032837, 0.7722446918487549, -0.46608811616897583, 0.30093592405319214, -0.356000691652298, -0.5982679724693298, -0.14082874357700348, 0.2963242530822754, 0.05023850500583649, -0.11232586205005646, -0.2440272867679596, 0.16868260502815247, 0.9007079601287842, -0.6376842856407166, 0.08402164280414581, 0.16423867642879486, 0.630622148513794, 0.25111123919487, -0.3028196394443512, 0.5471991300582886, 0.5300226807594299, 0.22991299629211426, 0.416511207818985, 0.24256740510463715, -0.027516914531588554, -0.43590086698532104, 1.100734829902649, 1.6059578657150269, 0.004972327034920454, -0.06400727480649948, -1.6006298065185547, 0.6891475915908813, -0.8442521691322327, -0.36382925510406494, 0.6112979054450989, 0.640004575252533, 0.7393689155578613, -0.5442627668380737, -0.14588956534862518, -0.03677954897284508, 0.5614972710609436, 0.25447380542755127, 0.1912204772233963, -0.9040459990501404, -0.1270475685596466, 0.38435500860214233, -0.5197870135307312, 0.8627272844314575, -0.5377718210220337, 0.45872583985328674, 14.931817054748535, 0.5251955389976501, 0.11553357541561127, 0.7714470028877258, 0.964236855506897, 0.22542980313301086, -0.5040076375007629, -0.1607883870601654, -1.1723740100860596, -0.2651207149028778, 1.8315149545669556, 0.48809853196144104, 0.7430660724639893, 0.11415913701057434, -0.4755978286266327, 0.4034986197948456, -0.18960171937942505, 0.3087821900844574, 0.8830238580703735, -1.2641353607177734, 0.572432816028595, 0.10873953998088837, 0.2259603589773178, 0.5703529119491577, 0.8973695039749146, 1.026794672012329, 0.5604487061500549, -0.43664517998695374, 0.7329568266868591, 0.10615183413028717, 0.5645790100097656, -0.3722740411758423, 0.30301493406295776, 0.6403998732566833, -0.6317721009254456, -0.4397912621498108, -0.5406408905982971, -1.008762240409851, 0.11593685299158096, 0.11443427950143814, -0.4106696546077728, -0.4846918284893036, -0.21281857788562775, 0.5202575922012329, -0.15531325340270996, -0.20982789993286133, -0.38612109422683716, 1.1835342645645142, -0.033636488020420074, 0.14046476781368256, 0.13863857090473175, 0.13241907954216003, 0.28718799352645874, -0.23236671090126038, 0.47846099734306335, 0.16865241527557373, -0.09570677578449249, 0.7333882451057434, -0.27249234914779663, -0.11549041420221329, -0.3022770583629608, -0.640564501285553, -0.41501665115356445, 0.844741702079773, 0.1731283962726593, 0.3736603260040283, -0.6396552324295044, 0.2915719449520111, 0.39222222566604614, 0.1273966133594513, -0.5508761405944824, 0.1236472949385643, -0.1551351696252823, -0.5145648121833801, 0.3457607924938202, 0.266177237033844, 0.17961052060127258, -0.46626153588294983, -0.8527926802635193, -0.7250120639801025, 0.4278407394886017, -0.8148921728134155, -0.3515137732028961, 0.9914810061454773, -0.5333991050720215, -0.34288540482521057, -0.09026612341403961, -0.7748332023620605, -0.2494431436061859, 0.48814326524734497, -1.3073201179504395, -1.2043057680130005, 0.5636246204376221, 0.0228340495377779, -0.5155320167541504, -0.26431548595428467, 1.431244969367981, -0.23536595702171326, -0.43195533752441406, -0.07322005182504654, -0.022293321788311005, 0.40755295753479004, -0.2522548735141754, -0.7085601687431335, 1.0550457239151, 0.6046227812767029, 0.4507167339324951, 0.04734243080019951, 0.04818159341812134, 0.07432708889245987, -0.9991830587387085, -0.03563937172293663, 0.8053693175315857, -1.433273434638977, -0.292917400598526, -0.8807030320167542, -0.5708634853363037, 0.5650196075439453, 0.8247936964035034, -0.004155687056481838, 0.3827984631061554, -0.06102754548192024, -0.7121958136558533, -0.007638970389962196, -0.47483837604522705, -0.15977902710437775, -0.005989964585751295, -0.29781144857406616, -0.4652966856956482, -0.06019888073205948, 0.39934131503105164, -0.8604756593704224, -0.38227730989456177, -0.4446527659893036, 0.08090575039386749, -0.08483218401670456, 0.7949143648147583, -0.26127198338508606, 0.855354368686676, 0.8237804770469666, -0.19900669157505035, -0.9187151193618774, -0.13338710367679596, -0.9162198305130005, -0.24875694513320923, 0.19101569056510925, 0.9945068955421448, -0.22481787204742432, -0.22996556758880615, 0.862445592880249, 0.39698609709739685, -0.25565558671951294, -0.7693812847137451, -0.034485217183828354, 0.5858374238014221, -0.31750714778900146, 0.2301330864429474, -0.19447554647922516, -0.01282346062362194, 0.11374517530202866, 0.34088703989982605, 0.9252792000770569, -0.31226009130477905, -0.5677489638328552, 0.3416261672973633, 0.0451820082962513, -0.42628195881843567, -0.4012683033943176, -0.3888845145702362, -0.8030641674995422, -0.38024139404296875, -1.0889538526535034, 0.07660768181085587, -0.668131947517395, -0.3644639253616333, 0.1814824491739273, 0.0969635397195816, 0.30432453751564026, 0.19520533084869385, -0.005051083862781525, -0.495211660861969, -0.6266295313835144, -0.1706836223602295, 1.0437202453613281, 0.853466272354126, -0.6666796207427979, 0.3343343436717987, -0.21149611473083496, 0.11374136805534363, 0.10971728712320328, 0.30981531739234924, -0.4952988028526306, -0.8334205150604248, -1.3905729055404663, 0.39468857645988464, -0.1310887187719345, -0.1860828697681427, -0.8130010962486267, 0.13054552674293518, 0.568463921546936, -0.14756585657596588, 0.45177197456359863, 0.11149553209543228, -0.37169915437698364, -0.5907336473464966, 0.5391704440116882, -0.44621366262435913, 0.1549990326166153, 0.39722779393196106, -0.5059180855751038, -0.33990752696990967, 0.6023451089859009, -0.010758798569440842, -1.0035463571548462, -0.2360765039920807, 0.5173701047897339, -1.0069942474365234, 0.0710570216178894, -0.15599574148654938, 0.14804330468177795, -0.9072185754776001, -0.5992235541343689, -0.32169532775878906, -0.21722564101219177, -0.5048263072967529, 1.2148230075836182, 0.20661430060863495, -1.0423699617385864, -0.35451066493988037, 0.7246182560920715, -0.08875297009944916, -0.10489904880523682, 0.1297830045223236, 0.417287141084671, -0.5659164190292358, 0.815568208694458, 0.32962000370025635, 0.2579200565814972, -0.6025773882865906, -0.19571015238761902, 0.6691126823425293, -0.2528446614742279, -0.09829504787921906, 1.2784174680709839, -0.42192909121513367, -1.5021734237670898, -0.11402382701635361, -1.2023411989212036, -0.8230609893798828, -0.5531926155090332, 0.7554201483726501, -0.3450298607349396, -0.04421326145529747, -0.552589476108551, -0.37311288714408875, 0.13576768338680267, 0.21554340422153473, -0.663219153881073, 0.5197424292564392, -0.07388702780008316, -0.6174423694610596, 0.7567282319068909, 0.23969660699367523, -0.2773717939853668, -0.5658446550369263, -0.3408331871032715, -0.5477823615074158, 0.412418931722641, 0.1722198724746704, -0.7589420676231384, -0.6043012142181396, 1.1424328088760376, 0.351728618144989, 0.1436208039522171, 0.2900952398777008, -0.03907501697540283, -0.005767389666289091, 0.5323033928871155, 0.23686179518699646, -0.337467759847641, -0.31758585572242737, 1.5479017496109009, 1.3798177242279053, -0.8867166638374329, -0.023449629545211792, -0.25083810091018677, -1.0055381059646606, 1.2031222581863403, 0.18099984526634216, 0.04384154453873634, 1.1157619953155518, -0.26744070649147034, 0.18455706536769867, 0.4195574223995209, -0.9727118611335754, 0.15102554857730865, 0.745701253414154, 0.9527021050453186, 1.0096769332885742, 0.3575540781021118, 0.1713952273130417, 1.3059488534927368, 0.27660098671913147, -0.17532868683338165, 0.8498910665512085, 0.6930892467498779, -0.2534763216972351, -0.2460281103849411, -0.2582772672176361, 0.3453582227230072, -0.4299011826515198, -0.6009378433227539, 0.08320118486881256, 0.6378035545349121, 0.31578904390335083, 0.6454563140869141, 0.6312814354896545, 0.5632148385047913, 0.03514842316508293, 0.5609120726585388, 0.7196050882339478, -0.4216672480106354, -0.03710461035370827, 0.015893958508968353, -0.7203110456466675, 0.04540927708148956, -0.24578562378883362, -0.46473225951194763, -0.6850400567054749, -0.08775591850280762, 0.2784023582935333, -0.17878276109695435, 0.35686439275741577, 1.448362112045288, 0.6673110723495483, 0.07100949436426163, -0.28763505816459656, -0.5963003635406494, -0.1872757226228714, -1.1056184768676758, -0.05087055638432503, -0.9131946563720703, -0.18464410305023193, -0.21636110544204712, -0.13844114542007446, 0.17473867535591125]}, "authors": [{"authorId": "1880394", "name": "Neha Sengupta"}, {"authorId": "3422905", "name": "Sunil Kumar Sahu"}, {"authorId": "2087720002", "name": "Bokang Jia"}, {"authorId": "2235818050", "name": "Satheesh Katipomu"}, {"authorId": "49404498", "name": "Haonan Li"}, {"authorId": "2789148", "name": "Fajri Koto"}, {"authorId": "49843300", "name": "Osama Mohammed Afzal"}, {"authorId": "2203791403", "name": "Samta Kamboj"}, {"authorId": "22171629", "name": "O. Pandit"}, {"authorId": "2235794681", "name": "Rahul Pal"}, {"authorId": "2076256459", "name": "Lalit Pradhan"}, {"authorId": "123838298", "name": "Zainul Mujahid"}, {"authorId": "1380273855", "name": "Massa Baali"}, {"authorId": "2110982198", "name": "Xudong Han"}, {"authorId": "8129718", "name": "Alham Fikri Aji"}, {"authorId": "100468503", "name": "Zhengzhong Liu"}, {"authorId": "2235826325", "name": "Andy Hock"}, {"authorId": "77917645", "name": "Andrew Feldman"}, {"authorId": "2235945609", "name": "Jonathan Lee"}, {"authorId": "2064974174", "name": "A. Jackson"}, {"authorId": "1683562", "name": "Preslav Nakov"}, {"authorId": "145465286", "name": "Timothy Baldwin"}, {"authorId": "2064963077", "name": "Eric P. Xing"}], "references": [{"paperId": "288063323dddad9bea7eb1230a2048546435687e", "title": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "7ace46ab8e71c4304682ab126b1212deb54b9b03", "title": "Style Over Substance: Evaluation Biases for Large Language Models"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "8d84148c12983f3015e66ddf5655402d2cb21157", "title": "Privacy- and Utility-Preserving NLP with Anonymized data: A case study of Pseudonymization"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "5e07cb357ea8078af34c6e9154a0c746af3a4d9e", "title": "Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages"}, {"paperId": "647f520eea08cf8e98b99319dcb7e32cc3a78c72", "title": "Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation"}, {"paperId": "879a7f5abdb7ab803d48172d4f0830965f989d46", "title": "Language Model Tokenizers Introduce Unfairness Between Languages"}, {"paperId": "6354f2639d07bf8d5b08a4dcaef4c5db5fe19fdb", "title": "ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "82e440220e29d6c2c5866f9cb40e522ca0c8a22d", "title": "Human-like Summarization Evaluation with ChatGPT"}, {"paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f", "title": "Self-Refine: Iterative Refinement with Self-Feedback"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552", "title": "The Capacity for Moral Self-Correction in Large Language Models"}, {"paperId": "cb29cf52f0f7d2e4324c68690a55b22890f2212d", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"}, {"paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "title": "Large language models encode clinical knowledge"}, {"paperId": "d78df936a717f278e89221f74f3f894e90d8c1fe", "title": "JASMINE: Arabic GPT Models for Few-Shot Learning"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "6f4cc536f9ed83d0dbf7e919dc609be12aa0848a", "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "title": "Large Language Models Are Human-Level Prompt Engineers"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "f72413a441737e1f944a4346221db0ab9baec32d", "title": "AraBART: a Pretrained Arabic Sequence-to-Sequence Model for Abstractive Summarization"}, {"paperId": "cc3b41c4d2ededb94b481089633ecb43d6b2162f", "title": "Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "37c9f2f1dc2b32590502c5fd0702ee45f94b041d", "title": "JABER and SABER: Junior and Senior Arabic BERt"}, {"paperId": "35eb6755b63c45ff44d05ace8ea1b3b8c8db9eee", "title": "On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "8e3b613f8a6d775a5bf97aa73328a2f4795dd407", "title": "SituatedQA: Incorporating Extra-Linguistic Contexts into QA"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "a05973d939f5a2b63f6b052e8f059d5fd73c25a1", "title": "AraT5: Text-to-Text Transformers for Arabic Language Generation"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "a458ea049f87db9d2c856f3554f1819a27d4e676", "title": "ALUE: Arabic Language Understanding Evaluation"}, {"paperId": "a4ab1bd1501668e932c986725b33d065e4f0a233", "title": "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models"}, {"paperId": "bcc14c3a878c7f155a52a07be8b26eec2f46ec66", "title": "Pre-Training BERT on Arabic Tweets: Practical Considerations"}, {"paperId": "c342798bafc1eaaa60c652fc90fd738941542133", "title": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "aea9f18d70a78d39ca927f2baa143e084c486086", "title": "AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding"}, {"paperId": "1e4cda8be54999ced1324777fa462a85e2c9746c", "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"}, {"paperId": "7f809204b5fa96c4631899d44cebd61c5057ee53", "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "f9886e992260cf2bd0ec8195d0e804f851fb8e98", "title": "KUISAIL at SemEval-2020 Task 12: BERT-CNN for Offensive Speech Identification in Social Media"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "1898d36d4fb8b2cd388305c67ff1952fa53b8ff0", "title": "Overview of OSACT4 Arabic Offensive Language Detection Shared Task"}, {"paperId": "995ec006ac98a697ea38bd4eea8c1f3170a8adb4", "title": "CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing"}, {"paperId": "96873c758c47a555b4e0b4b66b84b04cb251e9ef", "title": "An Empirical Study of Pre-trained Transformers for Arabic Information Extraction"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "1359d2ef45f1550941e22bf046026c89f6edf315", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "2626495b7743759c6a7a9738bc21b6694092eafe", "title": "IDAT at FIRE2019: Overview of the Track on Irony Detection in Arabic Tweets"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6", "title": "Analysing Mathematical Reasoning Abilities of Neural Models"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "13ff28f98474858d84c339e5301cb3bf468ac68b", "title": "Predicting the Type and Target of Offensive Posts in Social Media"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "7365f887c938ca21a6adbef08b5a520ebbd4638f", "title": "Model Cards for Model Reporting"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "66c8ab0b3b7698cf5333c3f7350a6eaa5302294b", "title": "SemEval-2018 Task 1: Affect in Tweets"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "21ebc8c6bd7adcdced2f2abb432174b71d0e862d", "title": "Towards a Map of the Syntactic Similarity of Languages"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "88f7ae268140634653ebadabfe8726c1c2041774", "title": "ArabicWeb16: A New Crawl for Today's Arabic Web"}, {"paperId": "800366078f063a637e6a4880c0c49c217c7905ea", "title": "The United Nations Parallel Corpus v1.0"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "74a7b457f84fad22daefbfa7dac675807c5d2c16", "title": "More Effective Boilerplate Removal - the GoldMiner Algorithm"}, {"paperId": "25ca4a36df2955b345634b5f8a6b6bb66a774b3c", "title": "Parallel Data, Tools and Interfaces in OPUS"}, {"paperId": "1b560f892432fb853d233c92f9294640bc91de3c", "title": "Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages"}, {"paperId": "984e762b35a564daecfba84ed52d91f9aa289f7f", "title": "ANERsys: An Arabic Named Entity Recognition System Based on Maximum Entropy"}, {"paperId": "7d1c803184ee60fae6368ff620141d25be1c184e", "title": "Source Language Markers in EUROPARL Translations"}, {"paperId": "98d7207066b61c15e007dac9758530b99ac06cb9", "title": "The Enron Corpus: A New Dataset for Email Classification Research"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "a1066659ec1afee9dce586f6f49b7d44527827e1", "title": "A Statistical Approach to Machine Translation"}, {"paperId": "2db77485736cf29778a4464fe500a289bd46e7ac", "title": "ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": null, "title": "GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5Turbo"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality"}, {"paperId": null, "title": "Instruction in the wild: A user-based instruction dataset"}, {"paperId": null, "title": "Alpaca-CoT: An instruction fine-tuning platform with instruction data collection and unified large language models interface"}, {"paperId": "1189abbda1f88753db8e917f159b8c2db9f62cde", "title": "Transformer-based Architecture for Empathy Prediction and Emotion Classification"}, {"paperId": "a6b6eedb0559cb44bd3fcab64151529019bae42a", "title": "Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"}, {"paperId": null, "title": "A framework for few-shot language model evaluation v0.0.1"}, {"paperId": null, "title": "ELECTRA: pretraining text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "El-Khair Corpus: A modern standard Arabic corpus"}, {"paperId": "4c88a3f34f1d268dc1bf5c12ea3b18e03e4da50a", "title": "Hybridity in MT. Experiments on the Europarl Corpus"}, {"paperId": "694b3c58712deefb59502847ba1b52b192c413e5", "title": "Europarl: A Parallel Corpus for Statistical Machine Translation"}, {"paperId": null, "title": "Free Dolly: Introducing the world\u2019s first truly open instruction-tuned LLM"}, {"paperId": null, "title": "Stanford Alpaca: An instruction-following LLaMA model"}, {"paperId": null, "title": "Falcon-40B: an open large language model with state-of-the-art performance"}]}