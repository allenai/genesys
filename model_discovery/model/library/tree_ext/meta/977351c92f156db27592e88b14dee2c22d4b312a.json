{"paperId": "977351c92f156db27592e88b14dee2c22d4b312a", "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference", "abstract": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "citationCount": 14, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2211.10526", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear- angular attention during inference."}, "embedding": {"model": "specter_v2", "vector": [0.44773346185684204, 0.6305183172225952, -0.6630614995956421, 0.07971388101577759, -0.32541197538375854, -0.7795557379722595, 0.9081281423568726, 0.15828631818294525, -0.1787996143102646, -1.0927668809890747, 0.2262192964553833, 0.1404702365398407, 0.511128306388855, -0.1437702178955078, -0.20948821306228638, 0.3285871744155884, -0.4045599400997162, 0.1386682689189911, 0.4275013208389282, -0.4626977741718292, 0.16426868736743927, -0.7106525301933289, -1.1379408836364746, 0.2880391478538513, 0.22524872422218323, 1.2068030834197998, 7.784006447764114e-05, 1.056779146194458, -0.4894600808620453, 0.4483959972858429, 0.16351033747196198, -0.8455505967140198, 0.41899311542510986, 0.20640727877616882, -0.4483873248100281, -0.25014713406562805, 0.7587336897850037, -0.36890456080436707, -0.5167055726051331, 0.8144139647483826, -0.1305045485496521, -0.019293615594506264, 0.5713203549385071, -0.854972779750824, -1.0267363786697388, 0.4053342640399933, 0.8396040201187134, 0.40299269556999207, -0.13362346589565277, -0.5202125906944275, 1.7826136350631714, -1.318688988685608, 0.029338639229536057, 1.176900029182434, 0.4612200856208801, -0.005915354005992413, 0.17441925406455994, -0.3371272683143616, 0.640316903591156, 0.8037185072898865, -0.7144520878791809, -0.535461962223053, 0.030728211626410484, 0.040547631680965424, 1.6814870834350586, -0.3216429352760315, 0.21250471472740173, 0.2977663576602936, -0.35706090927124023, 1.4724291563034058, 0.049871984869241714, -0.7316055297851562, -0.21243241429328918, 0.3151389956474304, -0.1187606006860733, 0.9939561486244202, -0.5298832058906555, 0.13630272448062897, -1.0706301927566528, -0.09504012763500214, 0.33719927072525024, 0.0975104570388794, 0.3516978919506073, -0.5242844223976135, 0.019465483725070953, 0.8214187622070312, 0.6792218089103699, 0.31160545349121094, -0.2550324499607086, 0.7173852324485779, 0.5930861234664917, 0.2915796935558319, -0.22726784646511078, 0.39057475328445435, -0.003819643286988139, 0.2944803237915039, -0.9651192426681519, 0.030543770641088486, -0.09686727821826935, 0.6443386077880859, -0.20434939861297607, 0.13420164585113525, -0.41001713275909424, 0.20535075664520264, 1.0279017686843872, 0.19302630424499512, 0.25086694955825806, -0.6731661558151245, -0.08357568085193634, -0.6091791391372681, -0.15632788836956024, -0.9088679552078247, 0.4497038722038269, -0.07628567516803741, -0.71329265832901, -1.0451910495758057, -0.6369239091873169, 0.8430893421173096, -1.3215323686599731, 0.45184871554374695, -0.21908122301101685, 0.3050640821456909, -0.11113450676202774, 0.3190121054649353, 0.6597253680229187, 0.383735716342926, -0.049119167029857635, 0.3367077112197876, 1.7071830034255981, -0.7438737154006958, -0.39000919461250305, -1.0222655534744263, 0.03939821198582649, -0.6151811480522156, 0.22780074179172516, 0.10035260766744614, -0.9690062999725342, -1.1522457599639893, -0.9804544448852539, -0.13445527851581573, -0.8266456127166748, 0.1980949491262436, 0.6195172071456909, 0.3995605409145355, -1.2898155450820923, 0.25216448307037354, -0.13631898164749146, -0.38829290866851807, 0.5295296311378479, -0.10136931389570236, 0.37913575768470764, -0.2676509618759155, -1.197497010231018, 0.2849915325641632, -0.02035737782716751, -0.5300748944282532, -0.21963010728359222, -0.4746301472187042, -1.4649471044540405, 0.13769535720348358, 0.6344342827796936, -0.6147689819335938, 1.3917373418807983, -0.05636722594499588, -0.896388053894043, 1.0206891298294067, -0.6957505345344543, 0.40534481406211853, 0.19641315937042236, -0.06181398779153824, -0.4991611838340759, -0.29607948660850525, 0.20315515995025635, 0.4544314444065094, 1.0816692113876343, -0.12471470981836319, -0.06591455638408661, 0.3045983910560608, -0.2138102948665619, -0.19640931487083435, -0.3631051182746887, 0.5999115109443665, -0.7671064138412476, -0.4989011883735657, 0.4155943989753723, 0.6011255979537964, 0.4359061121940613, 0.10445360094308853, -0.37103283405303955, -1.4639712572097778, 1.19407057762146, 0.2539524435997009, 0.48684608936309814, -1.0282961130142212, -0.7182790637016296, -0.12410715967416763, 0.0585864782333374, 0.2485182285308838, -0.5924957394599915, 0.8490447998046875, -0.26877570152282715, 0.1624198704957962, 0.26752224564552307, -1.0650070905685425, 0.15473486483097076, -0.11056377738714218, -1.201684832572937, -0.42323431372642517, 0.4302459955215454, 1.1888123750686646, -0.7909026145935059, -0.10344911366701126, 0.2503598928451538, 0.4383317828178406, -0.7807406187057495, 1.3611741065979004, -0.5097411870956421, -0.3933485746383667, 0.1458311378955841, 0.4149748682975769, -0.02964370884001255, -0.42738285660743713, 0.3384472131729126, -0.7893714308738708, 0.07261957973241806, 0.7141637206077576, -0.5233503580093384, 1.039095163345337, -0.18928462266921997, 0.6804534196853638, -0.32354432344436646, -1.0282057523727417, 0.19947682321071625, 0.1597670465707779, -0.22575651109218597, -0.4363888204097748, -0.06987570971250534, -0.1676732897758484, -1.0054161548614502, 0.25845804810523987, 1.0033262968063354, 1.2093849182128906, -0.34622618556022644, -0.374179869890213, 0.6580824851989746, -0.7380161881446838, -0.18236936628818512, 0.6276670098304749, 0.6179677248001099, 0.38678866624832153, 0.5921576023101807, 0.3329939842224121, 0.044807318598032, -0.6441073417663574, -0.006520458497107029, 0.8101471662521362, 0.5765472054481506, 0.8816155195236206, 0.5255507826805115, -0.8242108225822449, -0.40768641233444214, -0.2561183571815491, 0.4905843138694763, 1.528582215309143, 0.2627978026866913, -0.7592912912368774, -0.40235254168510437, -0.18821485340595245, -0.17056608200073242, -0.4968539774417877, -0.5327006578445435, -0.276400089263916, 0.06086688116192818, -0.6069870591163635, 1.0686901807785034, 0.392553448677063, 1.0766026973724365, -0.5449658632278442, -0.2017846703529358, -0.36518722772598267, 0.07011570036411285, -0.997111976146698, -0.6711410284042358, 0.4411057233810425, 0.28695496916770935, -0.2046857327222824, 0.3060099184513092, -0.4669243395328522, -0.08488086611032486, -0.4957374334335327, 0.7161977291107178, -0.5970747470855713, -0.876491129398346, 0.3499593734741211, 0.29181361198425293, -0.8403745293617249, -0.3050291836261749, 0.12284452468156815, 0.011975284665822983, -0.040285151451826096, 0.45619338750839233, 0.5357705950737, -0.3472290635108948, 0.10519634932279587, -0.35901087522506714, -0.38079434633255005, 0.06661525368690491, 0.09838981926441193, 0.7037475109100342, -0.5185368061065674, -0.2195161134004593, -0.6496981978416443, 0.2836020588874817, 0.20952485501766205, -0.17443972826004028, 0.11028385907411575, -1.099369764328003, -0.3469221889972687, 0.512503445148468, -0.715238630771637, 0.15939700603485107, -0.7458222508430481, 0.4030647873878479, -0.708591103553772, -0.27973297238349915, -0.24408309161663055, 0.26734694838523865, 0.04271065443754196, 0.43138644099235535, 0.8538050055503845, 0.1313144862651825, 0.30172035098075867, 0.9966282248497009, -0.7457108497619629, 1.1398972272872925, 0.08058762550354004, 0.5257498621940613, -0.0065483758226037025, -0.28546142578125, -0.6971420049667358, -0.2618686854839325, -0.7360488176345825, -0.01989264041185379, -0.13406118750572205, 0.4001242220401764, -0.571931779384613, -1.3029518127441406, 0.2603686451911926, -0.9950995445251465, -0.07099929451942444, -0.21976730227470398, -0.7006924152374268, -0.14635154604911804, -0.5508604049682617, -0.9893549680709839, -0.6441584229469299, -0.5385051369667053, -0.8729386329650879, 0.5355026721954346, 0.1386944055557251, -0.4497787058353424, -0.17841005325317383, 0.06953899562358856, -0.23866289854049683, 0.9849410653114319, -0.7713279724121094, 0.3275740444660187, 0.028064418584108353, -0.6369880437850952, -0.3159625828266144, -0.3336507976055145, 0.41168439388275146, 0.21827834844589233, 0.2607722282409668, -1.2395211458206177, 0.35706815123558044, -0.6313565969467163, -0.30720463395118713, 0.4711424708366394, 0.07859044522047043, 0.6284183859825134, -0.30422070622444153, -0.2135772705078125, 0.6512258648872375, 1.4667255878448486, -0.3183095157146454, 0.1777302324771881, 0.12607789039611816, 0.8401423096656799, 0.08546686172485352, -0.06524919718503952, 0.6927396059036255, 0.8743491172790527, 0.5955408215522766, 0.6062813401222229, -0.21944142878055573, -0.4102781414985657, -0.708763599395752, 0.45191487669944763, 0.9714238047599792, 0.2281956970691681, 0.09743154048919678, -0.7237964868545532, 1.2510851621627808, -1.0497502088546753, -1.1177875995635986, 0.9127600789070129, 0.3331136703491211, 0.2987976670265198, -0.610281229019165, -0.18320778012275696, -0.2641895115375519, 0.2665219306945801, 0.8201823234558105, -0.5976992845535278, -0.9732722640037537, 0.02187390811741352, 0.8932759761810303, 0.2991204261779785, 0.5994718670845032, -0.4978054463863373, 0.8078613877296448, 14.905409812927246, 0.7356847524642944, -0.46378061175346375, 0.5073603987693787, 0.3765011429786682, 0.654181718826294, -0.14311617612838745, 0.14154887199401855, -1.0390433073043823, -0.29917314648628235, 0.5118479132652283, 0.5602685809135437, 0.3941667377948761, 0.3436630070209503, -0.5254185199737549, 0.3059195578098297, -0.2654412090778351, 0.8461039066314697, 0.8388164639472961, -1.2167935371398926, -0.09218718111515045, 0.18469294905662537, -0.005843582097440958, 0.3990691006183624, 1.2439208030700684, 0.4900350570678711, 0.886210560798645, -0.2878228724002838, 0.7351047396659851, 0.13731569051742554, 1.3757708072662354, 0.3457723557949066, 0.2732589840888977, 0.05494091287255287, -1.3818402290344238, -0.0426860973238945, -0.37505093216896057, -0.8073937892913818, -0.10705164074897766, -0.05257630720734596, -0.8973589539527893, -0.24686500430107117, 0.3485780954360962, 0.5055880546569824, 0.17744328081607819, 0.2145729511976242, -0.23142744600772858, 0.41287630796432495, -0.020393874496221542, 0.06804252415895462, 0.17257890105247498, 0.6602587103843689, -0.14607301354408264, 0.14196866750717163, 0.018522918224334717, -0.11189422011375427, 0.15925051271915436, 0.5517611503601074, -0.3291758894920349, -0.38595131039619446, -0.3962758779525757, -0.238401398062706, -0.012533317320048809, 0.9303813576698303, 0.5555970668792725, 0.1959274262189865, -0.14726807177066803, 0.9626497030258179, 0.3620572090148926, 0.05576231703162193, -0.47824737429618835, -0.3812738358974457, 0.6775184273719788, -0.4460263252258301, 0.47225266695022583, 0.5728276371955872, -0.14873720705509186, -0.508528470993042, -0.5053042769432068, -0.2467288225889206, 0.6016799807548523, -1.0231925249099731, -0.6979485750198364, 0.6032427549362183, -0.48026609420776367, -0.2227933704853058, -0.02231714501976967, -0.7335659861564636, -0.5252727270126343, 0.49002137780189514, -1.4610295295715332, -0.9561977982521057, -0.10042458772659302, -0.22023648023605347, -0.24298155307769775, -0.10407049208879471, 1.2377554178237915, -0.11435043066740036, 0.40829482674598694, 0.3748842477798462, -0.42709535360336304, 0.34485360980033875, 0.44754666090011597, -1.1129361391067505, 1.1383854150772095, 0.4079556167125702, 0.3063780665397644, 0.24727877974510193, 0.1170230284333229, 0.28527286648750305, -0.7015392184257507, 0.1391066014766693, 0.4979747235774994, -1.0256463289260864, -0.4415503740310669, -0.7324880957603455, -0.5553310513496399, 0.3293551504611969, 0.691029965877533, 0.12195885181427002, 0.07847768068313599, 0.06905420124530792, -1.37002432346344, -0.24276500940322876, -0.6752418875694275, -0.11731639504432678, 0.37187260389328003, -1.2615312337875366, -0.39713186025619507, -0.4567157030105591, 0.3339877724647522, -1.084949016571045, -0.45277342200279236, -0.011608376167714596, 0.474624365568161, -0.059303127229213715, 1.4454784393310547, -0.24298502504825592, 0.4079226553440094, 0.6797448396682739, -0.16331346333026886, -0.7028689980506897, -0.11191621422767639, -0.6680962443351746, -0.1598092019557953, 0.2760840058326721, 0.2022472769021988, -0.4123975932598114, 0.5408517122268677, 0.22810454666614532, 0.4243207573890686, -0.37922003865242004, -0.3247416615486145, -0.013598578050732613, -0.306208997964859, -0.7596361637115479, 0.219236820936203, 0.6591732501983643, -0.075562983751297, -0.008958496153354645, 0.33517521619796753, 0.8830091953277588, -0.06841230392456055, -0.6853976249694824, 0.29998108744621277, -0.3272434175014496, -0.39079564809799194, -0.3205529451370239, -1.0870469808578491, -1.3797415494918823, -0.4292878210544586, -0.869920551776886, 0.010751837864518166, -0.9912521243095398, -0.8574153780937195, 0.15117333829402924, -0.22572699189186096, 0.4612424969673157, 0.251810222864151, 0.05054526403546333, -0.17062027752399445, -0.7029170393943787, -0.5131725072860718, 0.3328090012073517, 0.5146272778511047, -0.9628636240959167, 0.25585320591926575, -0.011359211057424545, -0.639162540435791, 0.003962196409702301, 0.25737878680229187, -0.2458353042602539, -0.6823365688323975, -0.836795449256897, 0.41445496678352356, -0.38428130745887756, 0.3332483768463135, -0.31152433156967163, 1.00192129611969, 0.334343284368515, -0.14818024635314941, -0.24057431519031525, 0.6810188889503479, -0.7760095596313477, -0.7465434670448303, 0.35280120372772217, -0.41807547211647034, -0.0712653174996376, 0.04428775608539581, -0.4583743214607239, -0.24053359031677246, 1.054294228553772, -0.04664825648069382, -0.7522563934326172, -1.1814578771591187, 0.4425957500934601, -0.4111472964286804, 0.12039846181869507, -0.364043653011322, -0.30142974853515625, -1.5066472291946411, -0.6937967538833618, -0.030899792909622192, 0.26453596353530884, -0.513239324092865, 1.1084743738174438, 0.5341230630874634, -1.1649696826934814, 0.240417018532753, 0.6392072439193726, 0.3930162191390991, 0.2612050175666809, 0.2720080018043518, 0.32241377234458923, -0.23175454139709473, 0.3812270164489746, 0.24114197492599487, 0.04884931072592735, -1.0128364562988281, 0.24948516488075256, 0.674447238445282, -0.15811187028884888, 0.2310054749250412, 1.1626864671707153, -0.30454835295677185, -1.0440667867660522, 0.03918436914682388, -1.1299232244491577, -0.14246150851249695, 0.2411458045244217, 0.05211566761136055, -0.2180037796497345, 0.1380983591079712, 0.054466407746076584, -0.8032588958740234, 0.5401606559753418, -0.13235177099704742, -0.4631739556789398, 0.06391993165016174, 0.23833300173282623, -0.561549186706543, 0.0588596910238266, 0.7729581594467163, -0.543833315372467, -0.6573209762573242, -1.1747969388961792, -0.7520517706871033, -0.027040308341383934, -0.010108946822583675, -0.30406391620635986, -0.31585004925727844, 0.7540640234947205, 0.5570728778839111, 0.812078058719635, 0.21488818526268005, 0.4722382128238678, 0.27331990003585815, 0.8159433603286743, -0.4579818546772003, -0.48502588272094727, -0.20618374645709991, 1.031489610671997, 0.9320290088653564, -0.92209792137146, 0.017300035804510117, -0.38695764541625977, -0.34971579909324646, 0.6984331011772156, 0.3511028289794922, 0.090582937002182, 0.6374413967132568, 0.029695432633161545, 0.36434826254844666, 0.12081807106733322, -0.6355255842208862, -0.624781608581543, 1.1034427881240845, 1.3101544380187988, 0.04691549390554428, 0.15085095167160034, 0.7808941602706909, 0.3953576982021332, -0.15198153257369995, -0.16503196954727173, 0.08348752558231354, 0.4142836928367615, -0.27352556586265564, -0.13696075975894928, -0.30149123072624207, 0.5010631084442139, -0.6945520043373108, -0.6206137537956238, -0.2725897431373596, 0.33046266436576843, 0.4306800961494446, 0.6355460286140442, 0.993309736251831, -0.3632977604866028, 0.7018033266067505, 0.22253772616386414, 0.4695781171321869, -0.3922041952610016, -0.0017247319919988513, -0.3898301124572754, -1.0484496355056763, -0.0670420452952385, -0.2296653836965561, -0.6212179660797119, -0.01839488185942173, -0.2774908244609833, 0.21615810692310333, -0.2951274812221527, 0.4831252694129944, 0.8791248202323914, 0.5551104545593262, 0.5994029641151428, -0.1406622976064682, -0.31516316533088684, -0.3239687979221344, -0.6093719601631165, 0.38123807311058044, -0.5731560587882996, 0.12103685736656189, -0.09019400924444199, -0.09679258614778519, -0.16871343553066254]}, "authors": [{"authorId": "47113848", "name": "Haoran You"}, {"authorId": "2760194", "name": "Yunyang Xiong"}, {"authorId": "4527324", "name": "Xiaoliang Dai"}, {"authorId": "3130257", "name": "Bichen Wu"}, {"authorId": "2918780", "name": "Peizhao Zhang"}, {"authorId": "146884473", "name": "Haoqi Fan"}, {"authorId": "48682997", "name": "P\u00e9ter Vajda"}, {"authorId": "3138925", "name": "Yingyan Lin"}], "references": [{"paperId": "9b069ba5259d229bfd4fe3ac3768148e2d1092f8", "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention"}, {"paperId": "200ef1cde362aafbf598a2b5a1c5f35504ca2289", "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design"}, {"paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f", "title": "Token Merging: Your ViT But Faster"}, {"paperId": "2475b38a76a9c2dc67f74446e2e686815764b0f2", "title": "EcoFormer: Energy-Saving Attention with Linear Complexity"}, {"paperId": "ec139916edd6feb9b3cb3a0325ca96e21dbb0147", "title": "Hydra Attention: Efficient Attention with Many Heads"}, {"paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a", "title": "EfficientFormer: Vision Transformers at MobileNet Speed"}, {"paperId": "f6abdedf5927295776d638ac53ad6cb9c507dede", "title": "ConvMAE: Masked Convolution Meets Masked Autoencoders"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "a09cbcaac305884f043810afc4fa4053099b5970", "title": "Exploring Plain Vision Transformer Backbones for Object Detection"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "77a90bb2fe31a53f64221519c60af32e32141c8b", "title": "Pyramid Fusion Transformer for Semantic Segmentation"}, {"paperId": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5", "title": "QuadTree Attention for Vision Transformers"}, {"paperId": "0a0c204919ec72c6c335296ebf639ebc379d3ac5", "title": "Learned Queries for Efficient Local Attention"}, {"paperId": "b6e9f1189fd46dabd6f1059c546cd2f4589fe65d", "title": "MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation"}, {"paperId": "c2a0c18e810535db52e5ebaf180c64ce70356748", "title": "A-ViT: Adaptive Tokens for Efficient Vision Transformer"}, {"paperId": "658a017302d29e4acf4ca789cb5d9f27983717ff", "title": "Masked-attention Mask Transformer for Universal Image Segmentation"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "cb99f6f2bdd72b6a23be6af5488ed61e4fdc4fe3", "title": "FBNetV5: Neural Architecture Search for Multiple Tasks in One Run"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "e5c32ac6cb785832d5fe186cca654c6e41828f1c", "title": "PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "4ba435bca5a006ee5b63a1355c5f323d8a8359b3", "title": "ViT-YOLO:Transformer-Based YOLO for Object Detection"}, {"paperId": "4ad3b17fcd582f944ebff35d91dd27d6849d9d0a", "title": "PP-LCNet: A Lightweight CPU Convolutional Neural Network"}, {"paperId": "c01b385205e488a731c8c8c11c0c494d426beb03", "title": "YOLOX: Exceeding YOLO Series in 2021"}, {"paperId": "260ad39a1dac4b451019e2bf17925f4df8e3b69a", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "d645bd08fc19d52164695f9cd5ae863345459a06", "title": "AutoFormer: Searching Transformers for Visual Recognition"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "100f2e2a810394503472f50938522930bd07b834", "title": "Rethinking the Self-Attention in Vision Transformers"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "ac8af4202f8ac179e727013929a95f6ecc4936e8", "title": "FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "0449c88867dec952945fc99749cc656df25c38b2", "title": "MobileDets: Searching for Object Detection Architectures for Mobile Accelerators"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "41c67d04be2d1632c0d3b0880c21c9fe797cdab8", "title": "EfficientDet: Scalable and Efficient Object Detection"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "5e19eba1e6644f7c83f607383d256deea71f87ae", "title": "Searching for MobileNetV3"}, {"paperId": "d34b3bb6d5b611e6117e7e25f0b5419d3a99fdf1", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "a5ade6b1cd5065fe242af4c95376b4acb049bc11", "title": "The angular kernel in machine learning for hyperspectral data classification"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "092a2818c81504ad7fb6d3479272c344267f16fd", "title": "Distance metrics and band selection in hyperspectral processing with applications to material identification and spectral libraries"}, {"paperId": "4e476e992c559e28731e53c472b59fe05bbc2047", "title": "Moore's law: past, present and future"}, {"paperId": "7d2a78a1f713b71c3a337247d042c5c2f0b2da84", "title": "EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition"}, {"paperId": "2a23ffef0b4f5f8689ffffb4cd9f515cb28336bd", "title": "HRFormer: High-Resolution Vision Transformer for Dense Predict"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "9af62668cb87f11fffb53a194588c8158fde6b00", "title": "DynamicViT: Ef\ufb01cient Vision Transformers with Dynamic Token Sparsi\ufb01cation"}, {"paperId": null, "title": "Scene parsing through 14441 Authorized licensed use limited to the terms of the applicable license agreement with IEEE"}, {"paperId": null, "title": "Learning with localized receptive fields"}, {"paperId": null, "title": "2%) w/ linear attention [17] for 300 epochs and observe that: (1) + Lin.: 68.3%"}, {"paperId": null, "title": "license agreement with IEEE. Restrictions apply"}]}