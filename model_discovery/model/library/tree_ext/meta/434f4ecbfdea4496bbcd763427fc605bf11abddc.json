{"paperId": "434f4ecbfdea4496bbcd763427fc605bf11abddc", "title": "Token Dropping for Efficient BERT Pretraining", "abstract": "Transformer-based models generally allocate the same amount of computation for each token in a given sequence. We develop a simple but effective \u201ctoken dropping\u201d method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks. In particular, we drop unimportant tokens starting from an intermediate layer in the model to make the model focus on important tokens more efficiently if with limited computational resource. The dropped tokens are later picked up by the last layer of the model so that the model still produces full-length sequences. We leverage the already built-in masked language modeling (MLM) loss to identify unimportant tokens with practically no computational overhead. In our experiments, this simple approach reduces the pretraining cost of BERT by 25% while achieving similar overall fine-tuning performance on standard downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "citationCount": 35, "influentialCitationCount": 4, "openAccessPdf": {"url": "http://arxiv.org/pdf/2203.13240", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A simple but effective \u201ctoken dropping\u201d method is developed to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks."}, "embedding": {"model": "specter_v2", "vector": [0.2983298599720001, 0.682155430316925, -0.34477531909942627, 0.10640459507703781, -0.38158342242240906, -0.2592338025569916, 0.43266561627388, -0.022510191425681114, -0.6037102937698364, -0.6542500257492065, 0.5164926648139954, -0.15919090807437897, 0.667136013507843, 0.4357246160507202, -0.1552007496356964, -0.006825899239629507, -1.0477088689804077, 0.3478679358959198, -0.04817097261548042, -0.39959800243377686, -0.31955447793006897, -0.6731842756271362, -0.962731659412384, 0.4290749728679657, 0.5333968997001648, 0.43045470118522644, 0.35903483629226685, 0.5995507836341858, -0.6204087734222412, 0.6579378843307495, 0.25484099984169006, -0.3348168432712555, 0.4376189410686493, -0.044613804668188095, -0.12630052864551544, 0.16739408671855927, -0.048310115933418274, -0.21384896337985992, -0.4642728269100189, 0.6868751049041748, -0.0803915411233902, 0.25872156023979187, 0.2607457935810089, -0.9176245331764221, -0.19276082515716553, 1.6314449310302734, 0.40459898114204407, 0.9925264716148376, -0.49047985672950745, -0.7288273572921753, 1.119399070739746, -1.533113718032837, -0.01914619281888008, 1.4622999429702759, 0.6266821026802063, 0.6598181128501892, -0.2261340469121933, -0.7916965484619141, 1.0856131315231323, 0.22837209701538086, -1.0250271558761597, -0.5242629051208496, 0.1778237223625183, -0.33856135606765747, 1.6028521060943604, -0.3916645646095276, 0.28700223565101624, 0.4738333821296692, -0.034550487995147705, 1.8223870992660522, -0.43757426738739014, -0.703078031539917, -0.5918923020362854, 0.37178826332092285, 0.03858611360192299, 0.9074175953865051, -0.6475207209587097, 0.702517569065094, -1.1878403425216675, 0.25223639607429504, 0.1242990717291832, -0.0914292186498642, 0.4201508164405823, -0.035452019423246384, -0.044237710535526276, 0.6176877617835999, 0.1682983785867691, 0.9314801096916199, -0.12416723370552063, 0.5060717463493347, 0.4760388731956482, 0.1944313496351242, 0.04594844952225685, 0.00742843234911561, 0.018663734197616577, 0.2924521863460541, -0.9064146280288696, 0.02464757114648819, -0.5472070574760437, 0.8338643908500671, -0.2677210867404938, 0.8526137471199036, -1.1199101209640503, 0.17962215840816498, 1.3101377487182617, -0.13528305292129517, 0.7367739081382751, -0.28795772790908813, 0.06717123091220856, -0.6820001602172852, 0.18314054608345032, -0.18550877273082733, -0.004104253835976124, -0.3024662733078003, -0.4568588137626648, -1.226639986038208, -0.5159344673156738, 0.14602188766002655, -1.3817027807235718, 0.911432683467865, -0.33558931946754456, 0.4318126142024994, -0.0942341536283493, 0.16606734693050385, 0.3781949281692505, 0.33472785353660583, 0.5472537875175476, 0.18234378099441528, 0.9495246410369873, -0.884371817111969, -0.5830997824668884, -1.1270560026168823, 0.3853752315044403, -0.4932701587677002, -0.028556959703564644, -0.18733683228492737, -1.1591947078704834, -1.0726243257522583, -0.6828861236572266, 0.2912105321884155, -0.5938341021537781, 0.10069701075553894, 1.0300954580307007, 0.599678635597229, -1.1815624237060547, 0.5853289365768433, -0.21937018632888794, 0.35327771306037903, 0.30784568190574646, 0.14251266419887543, 0.6156711578369141, -0.13948094844818115, -1.4898121356964111, 0.4229792356491089, 0.27960240840911865, -0.30107396841049194, -0.7199481129646301, -1.0167524814605713, -1.2963721752166748, -0.03283880650997162, 0.15983648598194122, 0.19548189640045166, 1.6421921253204346, -0.13988935947418213, -1.059085488319397, 0.7632706761360168, -0.6381656527519226, 0.05616991966962814, 0.5425578951835632, -0.17019765079021454, -0.6278021335601807, -0.42939293384552, -0.027609067037701607, 0.7597362995147705, 0.7087920308113098, -0.3949204385280609, -0.3239399492740631, 0.4906787574291229, -0.39971923828125, -0.05903606116771698, -0.23211009800434113, 1.053723692893982, 0.023854248225688934, -0.5540416240692139, 0.38182738423347473, 1.0077102184295654, -0.08529821783304214, -0.6515881419181824, -0.29146280884742737, -0.8983293771743774, 0.6298884749412537, -0.3042478859424591, 0.7343780994415283, -0.8818230628967285, -0.8826022744178772, -0.2609919011592865, -0.14623834192752838, -0.159526064991951, -0.9359515309333801, 0.8127605319023132, -0.692230224609375, 0.42792612314224243, 0.15442441403865814, -1.216672658920288, 0.1873674839735031, -0.6842942833900452, -0.8548498749732971, -0.4215010702610016, 0.4245147705078125, 1.0185495615005493, -0.9783003330230713, -0.08905858546495438, 0.24127472937107086, 0.5912867188453674, -1.0137567520141602, 1.042029619216919, -0.04487929865717888, 0.30885425209999084, 0.23615631461143494, 0.05708266794681549, 0.3141307830810547, 0.10742062330245972, 0.1498013287782669, -0.18335109949111938, -0.1467728167772293, 0.6382592916488647, -0.4131907820701599, 0.9197105765342712, -0.2327173799276352, 0.3455990254878998, 0.05767615884542465, -0.8273767828941345, 0.04756884276866913, 0.49381840229034424, 0.3721487820148468, -0.2775411903858185, 0.5105034708976746, 0.107082799077034, -0.3156461715698242, 0.561811089515686, 0.997190535068512, 0.07946454733610153, -0.3120812773704529, -0.1641373336315155, 0.48863884806632996, 0.12246616184711456, 0.6527631282806396, 0.5267374515533447, 0.6000851988792419, 0.3890153765678406, 0.061301734298467636, 0.02722950093448162, 0.1914183646440506, -0.6431699991226196, 0.42825138568878174, 0.6804637312889099, 0.32255277037620544, 0.5299475193023682, 0.22133371233940125, -0.4391373097896576, -0.08240532875061035, -0.2642272412776947, 0.6282330751419067, 1.6183665990829468, -0.17764073610305786, -0.2613410949707031, -0.4469048082828522, -0.2682386338710785, -0.04969922453165054, 0.07414283603429794, -0.18686984479427338, -0.5615531206130981, -0.5971589684486389, -0.8413411974906921, 0.8812568187713623, 0.35024136304855347, 0.53190678358078, -0.9186016917228699, -0.05593213811516762, 0.07544959336519241, 0.0704379752278328, -0.8878198862075806, -0.7956258654594421, 0.7651105523109436, -0.5070867538452148, 0.15619005262851715, 0.12802492082118988, -0.5187556147575378, 0.19326327741146088, -0.6714999675750732, 1.0072013139724731, -0.19923292100429535, -0.48949819803237915, -0.4288511574268341, 0.2726021409034729, -0.8272489905357361, -1.0518133640289307, 0.08242969214916229, 0.3882976770401001, -0.2459244430065155, 0.5570220947265625, 0.5446555018424988, 0.2842825651168823, 0.07423421740531921, -0.34606480598449707, 0.3186749815940857, 0.20654937624931335, 0.09453395009040833, 0.385121613740921, 0.12803597748279572, -0.032971594482660294, -0.9571167230606079, 0.5080614686012268, 0.43252089619636536, -0.4367988407611847, -0.02720574289560318, -0.38896000385284424, -0.4251292049884796, 0.6095331311225891, -0.7808050513267517, -0.1505073606967926, -0.7788787484169006, 0.019012345001101494, -0.4732021987438202, -0.019759442657232285, 0.3778612017631531, 0.2544468939304352, 0.36521345376968384, -0.17138230800628662, 0.8879123330116272, 0.07957766205072403, -0.3570072650909424, 0.3088289201259613, -0.8658595681190491, 0.32791295647621155, 0.2898201048374176, 0.5653628706932068, -0.26399898529052734, -0.779313862323761, -0.6760977506637573, -0.4341917634010315, -0.6739837527275085, -0.24125267565250397, -0.5083679556846619, -0.26544901728630066, -0.708034336566925, -1.0109457969665527, 0.1882435381412506, -0.7806711792945862, -0.47915396094322205, -0.07655618339776993, -0.26974520087242126, -0.07238016277551651, -0.993368923664093, -1.4955686330795288, -1.1101174354553223, -0.17217975854873657, -0.9101293683052063, 0.3683888614177704, -0.07446321099996567, -0.13082531094551086, -0.17245416343212128, -0.3919636607170105, -0.24131141602993011, 0.8858248591423035, -0.9749992489814758, 0.6963030099868774, 0.04760477691888809, -0.39217448234558105, -0.2281792163848877, 0.5556066632270813, 0.7404528856277466, 0.0048395744524896145, 0.2368459701538086, -1.0825328826904297, -0.046455200761556625, -0.43810781836509705, 0.03389662504196167, 0.494548499584198, 0.35007548332214355, 0.6489718556404114, 0.2782285809516907, -0.6379736065864563, 0.12745234370231628, 1.1436841487884521, -0.22339606285095215, -0.06355132162570953, 0.029360981658101082, 0.8034310936927795, 0.2569301426410675, -0.000984982936643064, 0.3567028045654297, 0.083725206553936, 0.2760463356971741, -0.02467774972319603, -0.34363260865211487, 0.12081277370452881, -0.6301873326301575, 1.0051137208938599, 1.6020101308822632, 0.6970635056495667, 0.10290386527776718, -1.1435472965240479, 1.058193325996399, -1.1248265504837036, -0.9179404377937317, 1.08401620388031, 0.5416937470436096, 0.5498796701431274, -0.21496598422527313, -0.07680016756057739, 0.10979661345481873, 0.7779338955879211, 0.17093373835086823, -0.2382829189300537, -0.6782245635986328, 0.4764096140861511, 0.4428832232952118, 0.07260213047266006, 0.8155626654624939, -0.4825949966907501, 1.0968388319015503, 14.862716674804688, 0.6609300374984741, -0.07660111784934998, 0.48346924781799316, 0.42308279871940613, 0.3113570213317871, -0.4309421479701996, -0.20483505725860596, -1.6245275735855103, 0.039038851857185364, 1.0945895910263062, 0.054494693875312805, 0.7843939065933228, 0.2580517828464508, 0.26548895239830017, 0.20763026177883148, -0.4992196261882782, 0.39344000816345215, 0.34947043657302856, -0.9621738791465759, 0.4535278081893921, -0.0002553905942477286, -0.1933196485042572, 0.4707722067832947, 0.7826949954032898, 1.2157841920852661, 0.6540209054946899, -0.5128246545791626, 1.121712565422058, -0.20176568627357483, 0.6567341685295105, -0.3805150091648102, 0.44462770223617554, 0.6049689650535583, -1.1490076780319214, -0.1417890340089798, -0.5166131258010864, -1.0600930452346802, 0.45397233963012695, 0.4192400574684143, -1.0309457778930664, -0.4826621413230896, -0.1928456872701645, 0.5963349938392639, 0.14812901616096497, 0.4253500998020172, -0.14913122355937958, 0.9478169083595276, 0.12685930728912354, 0.14884163439273834, 0.2904760539531708, 0.41001856327056885, 0.3444066345691681, 0.19785451889038086, -0.11997506022453308, -0.275429904460907, 0.36953380703926086, 0.3918844163417816, -0.46901872754096985, -0.4021610617637634, -0.13603255152702332, -0.6352126002311707, -0.13624897599220276, 0.8705939054489136, -0.004653202835470438, 0.18906253576278687, -0.20373548567295074, 0.2015138417482376, 0.34465715289115906, 0.024422282353043556, -0.7150905132293701, 0.021257735788822174, 0.7876688241958618, -0.37889355421066284, 0.2889903485774994, 0.5572720766067505, 0.07017067074775696, -0.6138039231300354, -0.7608699798583984, -0.22763103246688843, 0.34423816204071045, -0.5298149585723877, -0.6758583784103394, 0.897847056388855, -0.24563676118850708, -0.44999492168426514, 0.04973423108458519, -0.6783440709114075, -0.043752651661634445, 0.6598785519599915, -1.7824556827545166, -0.39785441756248474, 0.2748410701751709, -0.3651747405529022, -0.4114604592323303, 0.16011987626552582, 0.9397997260093689, 0.4355839788913727, -0.23167456686496735, 0.17464208602905273, 0.029416510835289955, 0.5704486966133118, -0.34055501222610474, -0.8463781476020813, 0.9913963079452515, 0.6382805109024048, 0.10829029232263565, 0.380397230386734, 0.15580354630947113, 0.011815708130598068, -0.33152124285697937, -0.25324365496635437, 0.9691885709762573, -0.8839223384857178, -0.5371955633163452, -1.325614094734192, -0.687557578086853, 0.6498872637748718, 0.3229006230831146, -0.4714926481246948, 0.47035813331604004, 0.25275230407714844, -0.34704601764678955, -0.2016850858926773, -0.796043872833252, 0.19294866919517517, 0.6506872773170471, -0.700949490070343, -0.3620901107788086, -0.24354888498783112, 0.3087487816810608, -0.9719854593276978, -0.14776580035686493, -0.19567033648490906, -0.1711500585079193, -0.12325891852378845, 1.2107930183410645, -0.5716454386711121, 0.38112518191337585, 0.7766625881195068, 0.15284724533557892, -1.0559152364730835, -0.3131406009197235, -0.7713571190834045, 0.2516961097717285, 0.308100163936615, 0.5141829252243042, -0.12066800892353058, 0.5228602886199951, 0.5299460291862488, -0.004314893390983343, -0.011338043957948685, -0.6255857348442078, -0.5326142907142639, 0.07141993939876556, -0.6914892792701721, 0.3452457785606384, 0.11393957585096359, 0.10010980814695358, 0.005659706424921751, 0.4967316687107086, 0.5426097512245178, -0.26326659321784973, -0.7556649446487427, -0.02515718899667263, 0.10552596300840378, 0.21628835797309875, -0.6745707988739014, -0.5354758501052856, -1.0119554996490479, 0.1659068465232849, -1.378117322921753, 0.6211670637130737, -0.7938816547393799, -0.4061019718647003, -0.02438351884484291, -0.7697282433509827, 0.34516188502311707, -0.061044760048389435, -0.26157161593437195, -0.24141082167625427, -0.9075484871864319, -0.26964086294174194, 0.7621608376502991, 0.6474826335906982, -0.932113528251648, 0.09146728366613388, -0.14540302753448486, -0.25909510254859924, 0.33423227071762085, 0.361363023519516, -0.8724929690361023, -0.601830780506134, -1.0814651250839233, -0.10014335066080093, -0.26344573497772217, -0.2890455424785614, -0.6059656143188477, 0.2588586211204529, 0.26335564255714417, 0.011148623190820217, -0.34324246644973755, 0.4290357828140259, -0.898863673210144, -0.5899627208709717, 0.564414381980896, -0.587333083152771, 0.20119766891002655, 0.3841429054737091, -0.8232813477516174, -0.42177724838256836, 0.5702606439590454, 0.2838882803916931, -1.212110161781311, -1.0313620567321777, 0.5102260112762451, -1.1539088487625122, 0.17450249195098877, -0.2648308277130127, -0.2924504578113556, -0.5428817272186279, -0.37121546268463135, -0.08258633315563202, 0.45902878046035767, -0.6832971572875977, 1.2847594022750854, 0.565093994140625, -1.0024687051773071, -0.12723702192306519, 0.608271598815918, -0.23003214597702026, -0.20158132910728455, 0.3295719623565674, 0.39795491099357605, -0.01786215789616108, 0.4495399296283722, 0.21847493946552277, 0.23016725480556488, -1.0563299655914307, 0.1074468269944191, 0.7784150838851929, -0.540422797203064, -0.27851369976997375, 1.3830512762069702, -0.32626011967658997, -1.1302452087402344, 0.4130783975124359, -1.2887331247329712, -0.5019786357879639, -0.11634288728237152, 0.8825237154960632, -0.010939748026430607, 0.19845175743103027, -0.28827717900276184, -1.006831169128418, -0.22556710243225098, -0.26007208228111267, -0.7156786918640137, 0.46525296568870544, -0.0623721145093441, -0.111290842294693, 0.6603022217750549, 0.3342447578907013, -0.4520826041698456, -0.7392827272415161, -0.45038142800331116, -0.19491195678710938, -0.1773407757282257, 0.3303014636039734, -0.38179805874824524, -0.7283991575241089, 0.7855396270751953, 0.29662325978279114, 0.30356258153915405, -0.17616869509220123, -0.44923752546310425, 0.11917909979820251, 0.2965490520000458, 0.3692611753940582, -0.7918292284011841, -0.5267528295516968, 1.3640283346176147, 0.9038090109825134, -0.8021931648254395, -0.3117104470729828, -0.3370330333709717, -0.8218900561332703, 0.4592002034187317, 0.44008344411849976, 0.10437250882387161, 0.8796663284301758, 0.1284504532814026, 0.7646580934524536, 0.49134981632232666, -0.7064019441604614, -0.6257603764533997, 0.484487384557724, 1.166744589805603, 0.8700209856033325, 0.3013845384120941, 0.53754061460495, 0.8082533478736877, -0.021675657480955124, -0.047796353697776794, 0.5102487802505493, 0.7830663323402405, -0.19738651812076569, -0.3074679374694824, 0.08023060858249664, 0.6716060042381287, -0.8542966842651367, -0.9791086912155151, 0.0955205112695694, 0.6593251824378967, 0.2992510497570038, 0.7977410554885864, 0.7144908905029297, -0.09401075541973114, 0.4498659670352936, 0.6492685079574585, 0.4028221368789673, -0.7528069019317627, -0.5780629515647888, -0.03741087764501572, -0.785081148147583, -0.05727221071720123, -0.11226432025432587, -0.5621710419654846, -0.4775142967700958, -0.26203566789627075, 0.23315201699733734, 0.3941188454627991, 0.2095298320055008, 1.0199625492095947, 0.250914990901947, 0.7751346826553345, -0.26422256231307983, -0.6172322034835815, -0.44776251912117004, -0.997259795665741, -0.3186916410923004, -0.5076952576637268, 0.2477693110704422, -0.2918453812599182, -0.1389262080192566, -0.2454770803451538]}, "authors": [{"authorId": "2153400663", "name": "Le Hou"}, {"authorId": "46230016", "name": "Richard Yuanzhe Pang"}, {"authorId": "1805655", "name": "Tianyi Zhou"}, {"authorId": "9287688", "name": "Yuexin Wu"}, {"authorId": "2831106", "name": "Xinying Song"}, {"authorId": "2118945016", "name": "Xiaodan Song"}, {"authorId": "65855107", "name": "Denny Zhou"}], "references": [{"paperId": "8745260ea20bf671bdd388ef25979fc82083951b", "title": "Amortized Noisy Channel Neural Machine Translation"}, {"paperId": "3c209e0703ffff26231b1145268c935df494631a", "title": "QuALITY: Question Answering with Long Input Texts, Yes!"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "069c109507ee685dfe04534f0461b837c5cb224d", "title": "Pre-trained Language Model based Ranking in Baidu Search"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "ad9b8672d48eaefa67a5f7a7e7f14e3a2a7b5d72", "title": "Reducing BERT Computation by Padding Removal and Curriculum Learning"}, {"paperId": "837ac4ed6825502f0460caec45e12e734c85b113", "title": "Dynamic Neural Networks: A Survey"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "f6245b3e6270e4dc2e279c4b728030523dffcff4", "title": "LEGAL-BERT: \u201cPreparing the Muppets for Court\u2019\u201d"}, {"paperId": "480815ff593a7c36f64db8ca197014e2ad4a2a5f", "title": "Taking Notes on the Fly Helps BERT Pre-training"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "f2c0b478a30e653157dcdfe879b3082c6bbb0913", "title": "ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation"}, {"paperId": "8747f028acccde9ee7c35c858da8091613d3e574", "title": "Faster Depth-Adaptive Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "1359d2ef45f1550941e22bf046026c89f6edf315", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding"}, {"paperId": "4df543d7bc5dc9ec87057857266760f0c9ebd809", "title": "Improving Joint Training of Inference Networks and Structured Prediction Energy Networks"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "4585611042d2be0d997ee135e3fe219d668db9ec", "title": "Depth-Adaptive Transformer"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "4ac62731b802c727f916e8deefda1a992991505d", "title": "Are All Layers Created Equal?"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "dc784e10a1be4cc73e9039cd7ccce019bf042828", "title": "A Stable and Effective Learning Strategy for Trainable Greedy Decoding"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454", "title": "Non-Autoregressive Neural Machine Translation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": null, "title": "The FLOPs calculus of language model training"}, {"paperId": "8988742cb5658634e2a173d0d4baaaab17304229", "title": "Taking Notes on the Fly Helps Language Pre-Training"}, {"paperId": "31cf97601ab6cd812cfdcf58dde869a92285ae52", "title": "Packing: Towards 2x NLP BERT Acceleration"}, {"paperId": null, "title": "In our case, given that we are dropping 50% of the tokens in 50% of the layers, we would save around 25% of the FLOPs A.2 Potential Limitations and Other Considerations"}, {"paperId": null, "title": "Given that the community is paying more attention to long-document tasks"}, {"paperId": null, "title": "The total MLP compute is proportional where T is the number of tokens in each and L is the number of total layers"}, {"paperId": null, "title": "2021b), it is worth investigating whether token dropping can be used to pretrain transformers with a much larger context length, like Longformer encoder decoder (LED) (Beltagy"}, {"paperId": null, "title": "ELECTRA: Pretraining text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Linguistics (Volume 2: Short Papers)"}]}