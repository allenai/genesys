{"paperId": "33222cf8be8ed749bd5af6390b5a31599ac99a92", "title": "Towards Effective Utilization of Pre-trained Language Models", "abstract": "In the natural language processing (NLP) literature, neural networks are becoming increasingly deeper and more complex. Recent advancements in neural NLP are large pretrained language models (e.g. BERT), which lead to significant performance gains in various downstream tasks. Such models, however, require intensive computational resource to train and are difficult to deploy in practice due to poor inference-time efficiency. In this thesis, we are trying to solve this problem through knowledge distillation (KD), where a large pretrained model serves as teacher and transfers its knowledge to a small student model. We also want to demonstrate the competitiveness of small, shallow neural networks. We propose a simple yet effective approach that transfers the knowledge of a large pretrained network (namely, BERT) to a shallow neural architecture (namely, a bidirectional long short-term memory network). To facilitate this process, we propose heuristic data augmentation methods, so that the teacher model can better express its knowledge on the augmented corpus. Experimental results on various natural language understanding tasks show that our distilled model achieves high performance comparable to the ELMo model (a LSTM based pretrained model) in both single-sentence and sentence-pair tasks, while using roughly 60\u2013100 times fewer parameters and 8\u201315 times less inference time. Although experiments show that small BiLSTMs are more expressive on natural language tasks than previously thought, we wish to further exploit its capacity through a different KD framework. We propose MKD, a Multi-Task Knowledge Distillation Approach. It distills the student model from different tasks jointly, so that the distilled model learns a more universal language representation by leveraging cross-task data. Furthermore, we evaluate our approach on two different student model architectures, one is bi-attentive LSTM based network, another uses three layer Transformer models. For LSTM based student, our approach keeps the advantage of inference speed while maintaining comparable performance as those specifically designed for Transformer methods. For our Transformerbased student, it does provide a modest gain, and outperforms other KD methods without using external training data.", "venue": "", "year": 2020, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This thesis proposes MKD, a Multi-Task Knowledge Distillation Approach, where a large pretrained model serves as teacher and transfers its knowledge to a small student model, which distills the student model from different tasks jointly, so that the distilled model learns a more universal language representation by leveraging cross-task data."}, "embedding": {"model": "specter_v2", "vector": [0.13476409018039703, 0.9749264121055603, -0.1374179869890213, -0.25265729427337646, -0.718430757522583, -0.7622774839401245, 0.5223993062973022, -0.07004140317440033, -0.6869019865989685, 0.17068864405155182, 0.56979900598526, -0.601344645023346, 0.4001632034778595, 0.0872718095779419, -0.3475671708583832, 0.40545251965522766, -0.30740469694137573, 0.3183726668357849, -0.08904063701629639, -0.6284931898117065, -0.24421361088752747, -0.8131222128868103, -0.2840505838394165, -0.10835759341716766, 0.6615397334098816, 0.2769806683063507, 0.21007271111011505, 0.8098651766777039, -0.46350404620170593, 0.41085878014564514, 0.5376479029655457, -0.5886740684509277, 0.2898919880390167, 0.02658148854970932, -0.688207745552063, -0.36569973826408386, 0.5469093322753906, -0.6727203726768494, -0.29500922560691833, 0.7423129677772522, -0.1587778925895691, 0.3464277386665344, 0.2795228660106659, -0.5302743315696716, -0.8052462935447693, 1.2430589199066162, 0.8176035284996033, 0.8160381317138672, -0.17370463907718658, -0.2866317629814148, 1.4023841619491577, -1.1466960906982422, 0.26083824038505554, 1.5233800411224365, 0.4619346559047699, 0.5314891934394836, -0.3174138367176056, -0.8704124689102173, 0.606988787651062, 0.16988089680671692, -0.730456531047821, -0.3029644191265106, -0.13871948421001434, 0.06560513377189636, 1.876128911972046, -0.6946778893470764, -0.16130013763904572, 0.40169334411621094, -0.6116761565208435, 1.7338839769363403, -0.020817967131733894, -0.7034656405448914, -0.9239667654037476, 0.34982770681381226, 0.67692631483078, 0.887176513671875, -0.2895194888114929, 0.04451045021414757, -0.644098162651062, 0.19676965475082397, 0.44522127509117126, -0.021734805777668953, -0.4470982849597931, 0.04380342736840248, -0.32642728090286255, 0.8385443091392517, 0.5677353143692017, 0.783244252204895, -0.17249907553195953, 0.42485278844833374, 0.7241869568824768, 0.7170949578285217, -0.29471880197525024, 0.8100024461746216, -0.6930527687072754, 0.3892071545124054, -0.8942502737045288, 0.10379733890295029, 0.09264098107814789, 0.4271959662437439, 0.08589459210634232, 0.3273451328277588, -0.5575951933860779, -0.1148851290345192, 1.1767146587371826, -0.04739612713456154, 0.5193824172019958, -0.7616251111030579, 0.34511351585388184, -0.663848340511322, 0.032309602946043015, -0.6375582218170166, 0.1112109050154686, -0.18041394650936127, -0.6147061586380005, -1.610455870628357, -0.1260433793067932, -0.021523354575037956, -0.8270660638809204, 1.1736942529678345, -0.49004414677619934, 0.42301738262176514, 0.6682435274124146, 0.1527920961380005, 0.8494403958320618, 0.8599566221237183, 0.22255562245845795, 0.19615353643894196, 0.8403885960578918, -0.5182067155838013, -0.5978816747665405, -1.0767580270767212, 0.8490476012229919, 0.0634678453207016, 0.12309017777442932, -0.12895116209983826, -1.208459496498108, -1.069710612297058, -0.6051485538482666, -0.30210351943969727, -0.846862256526947, 0.15749554336071014, 0.8133857250213623, 0.4943470358848572, -1.1070506572723389, 0.687421441078186, -0.08284246921539307, -0.17015965282917023, 0.21368585526943207, 0.12247239798307419, 0.14960633218288422, -0.6615066528320312, -1.5025392770767212, 0.6263339519500732, 0.7801794409751892, -0.2768349349498749, -0.42849910259246826, -0.7829043865203857, -1.2960206270217896, -0.09307758510112762, 0.3403204381465912, -0.4587453603744507, 1.4387625455856323, -0.018529217690229416, -0.9886577129364014, 0.8610942959785461, -0.4926838278770447, -0.09228566288948059, 0.025820579379796982, -0.5348387360572815, -0.3333854079246521, -0.5542340874671936, 0.006279855966567993, 0.4718771278858185, 0.11266544461250305, -0.20413704216480255, 0.030241506174206734, 0.25562188029289246, 0.17007657885551453, -0.3817540109157562, -0.466615229845047, 0.6825246810913086, -0.03538714721798897, -0.116378054022789, 0.3413834571838379, 0.9780460596084595, 0.09110822528600693, -0.33000531792640686, -0.5373290181159973, -1.1595871448516846, 0.774408221244812, -0.4234524369239807, 0.693540096282959, -0.6780350804328918, -0.5822649598121643, -0.15800486505031586, -0.010249592363834381, 0.33439886569976807, -0.995466411113739, 0.5667590498924255, -0.5979283452033997, 0.479705274105072, -0.1038723737001419, -1.1664963960647583, 0.03488646820187569, -0.07809793204069138, -0.956566333770752, -0.7109352350234985, 0.112485371530056, 1.3183295726776123, -0.9222285747528076, 0.33026638627052307, 0.08965454995632172, 0.21922683715820312, -0.8234022259712219, 1.371074914932251, -0.3900851309299469, 0.39979201555252075, -0.17793114483356476, -0.35800349712371826, 0.27020758390426636, -0.2571316361427307, 0.07601945847272873, -0.5392948389053345, -0.1578846424818039, 0.789341390132904, -0.4860781729221344, 1.40858793258667, -0.4695521891117096, 0.8016180396080017, 0.09593688696622849, -1.134406566619873, 0.23988908529281616, 0.6252573728561401, -0.6738962531089783, -0.19254837930202484, 0.1742987185716629, 0.48870113492012024, -0.4529882073402405, 0.18464870750904083, 0.7406821846961975, 0.3808809220790863, -0.25822457671165466, 0.38839849829673767, 0.5238392949104309, -0.5654969215393066, 0.5522883534431458, 0.6951888203620911, 0.31032970547676086, 0.09038679301738739, 0.2097853124141693, 0.08838868886232376, 0.6813180446624756, -0.6967084407806396, 0.002834566170349717, 0.4133352041244507, 0.826115608215332, 0.5417076349258423, 0.5040767192840576, -0.4282795786857605, -0.21637660264968872, -0.20499922335147858, 0.7593180537223816, 1.4613704681396484, -0.46393465995788574, 0.07126815617084503, -0.557405412197113, -0.48112016916275024, -0.30922529101371765, 0.36622974276542664, -0.03894603252410889, -0.1724277287721634, -0.6744241118431091, -0.777280330657959, 0.8774816393852234, 0.6127288937568665, 1.2615655660629272, -0.3428187668323517, 0.19641096889972687, -0.26214325428009033, -0.007159585598856211, -1.0302308797836304, -0.3395518362522125, 0.4175272285938263, -0.46763697266578674, -0.45166686177253723, 0.04999072477221489, -0.35534101724624634, 0.21768821775913239, -0.6867791414260864, 1.0095877647399902, -0.5787085890769958, -0.33021846413612366, 0.2834332287311554, 0.6692602634429932, -0.6666761040687561, -0.6180456876754761, 0.11452014744281769, 0.1703123152256012, -0.5320304036140442, 0.5819151997566223, 0.6744834780693054, 0.08376769721508026, -0.03369686007499695, -0.5294451713562012, 0.06464572995901108, 0.07553113251924515, -0.07525677978992462, 0.5596484541893005, -0.3087875247001648, 0.37493273615837097, -1.1363365650177002, 0.9726701378822327, 0.26186880469322205, -0.4237551987171173, 0.38155999779701233, -0.2935570180416107, -0.21041111648082733, 0.4930289387702942, -0.4893220365047455, -0.29725149273872375, -1.2232123613357544, 0.024620961397886276, 0.0028245241846889257, -0.12675608694553375, 0.6393522620201111, 0.22850917279720306, 0.24881017208099365, 0.046378906816244125, 0.5953061580657959, 0.6976227760314941, -0.3085780143737793, 0.7576020359992981, -0.8839167356491089, 0.5340612530708313, 0.24061869084835052, 0.5614139437675476, -0.4810430109500885, -0.5108882188796997, -0.6249383687973022, -0.9206011295318604, -0.278787225484848, -0.244932159781456, 0.17946071922779083, 0.22132883965969086, -0.7471448183059692, -0.7350090742111206, 0.1023988202214241, -1.3304401636123657, -0.4275599718093872, -0.11479739844799042, -0.3005535304546356, 0.18272612988948822, -1.0966711044311523, -1.3458846807479858, -0.379544198513031, -0.370430052280426, -0.9089459776878357, 0.18785713613033295, 0.3380030691623688, -0.42718714475631714, -0.8681647181510925, 0.08209256082773209, 0.0011716933222487569, 1.0846112966537476, -0.6528458595275879, 0.9642270803451538, -0.34221985936164856, -0.25094953179359436, -0.3849143385887146, -0.17863063514232635, 1.0645323991775513, -0.1699959933757782, -0.046458154916763306, -0.7710227966308594, 0.15244030952453613, -0.524398148059845, -1.0251418352127075, 0.40765586495399475, 0.27975520491600037, 0.5000050663948059, 0.2230122983455658, -0.2964778542518616, 0.07740311324596405, 1.492279291152954, -1.0081731081008911, 0.20446841418743134, 0.06258954852819443, 1.280466914176941, 0.16389049589633942, -0.4494606852531433, -0.15057314932346344, 0.6934330463409424, -0.266974538564682, 0.007526513189077377, -0.1908879429101944, -0.034148283302783966, -0.6531612277030945, 0.47781088948249817, 1.4326696395874023, 0.09576299041509628, 0.02575940079987049, -1.2254531383514404, 0.790212869644165, -1.0479923486709595, -0.2879117727279663, 0.4585094451904297, 0.506393313407898, 0.7834962606430054, -0.5343701839447021, -0.787352442741394, -0.21338951587677002, 0.3087506890296936, 0.2691628932952881, -0.40181222558021545, -0.68331378698349, 0.2762184739112854, 0.6682918667793274, 0.09528358280658722, 0.7813156843185425, -0.1294998973608017, 0.9020151495933533, 14.438408851623535, 0.8779319524765015, 0.14125099778175354, 0.571692168712616, 0.5269286036491394, 0.5412531495094299, -0.4068206250667572, -0.30415835976600647, -1.7344658374786377, -0.6826944947242737, 1.0703363418579102, 0.1029624342918396, 0.2303326576948166, -0.06863657385110855, 0.007471833378076553, 0.030489442870020866, -0.6662143468856812, 0.6650224924087524, 0.6800698637962341, -1.1029751300811768, 0.6484046578407288, 0.15145300328731537, 0.415367066860199, 0.4102628827095032, 0.7780351042747498, 1.240511178970337, 0.530038595199585, -0.6274169683456421, 0.1691930741071701, 0.19146224856376648, 0.6060705184936523, 0.08625495433807373, 0.8498128056526184, 0.6862974166870117, -0.794567883014679, -0.2587725520133972, -0.4053509831428528, -1.001511573791504, 0.18583783507347107, -0.005120133515447378, -0.24227501451969147, -0.6629139184951782, -0.39589154720306396, 0.812361478805542, 0.17488600313663483, 0.2880013585090637, -0.7666037678718567, 0.47824031114578247, -0.24537570774555206, 0.018061190843582153, 0.5437093377113342, 0.5812252163887024, 0.1908201277256012, -0.15706610679626465, -0.04144861549139023, 0.01921749673783779, 0.37432461977005005, 0.6555800437927246, -0.7701063752174377, -0.31480199098587036, -0.3186763525009155, -0.44374391436576843, -0.2150070071220398, 0.6082992553710938, 0.8012299537658691, 0.03426631540060043, -0.28756752610206604, 0.3927329480648041, 0.7390408515930176, 0.24779118597507477, -0.1118931919336319, -0.27627333998680115, 0.42072272300720215, -0.4209095239639282, 0.09956783056259155, 0.16631504893302917, -0.036172401160001755, -0.6058741211891174, -0.7369024157524109, -0.3449479937553406, 0.3047267496585846, -0.9942255020141602, -0.6424605846405029, 0.8343100547790527, -0.40961217880249023, -0.020471518859267235, -0.042304590344429016, -1.1075584888458252, -0.4007396697998047, 0.6542821526527405, -1.9177316427230835, -0.752435028553009, 0.25565317273139954, 0.04408790171146393, -0.422493577003479, -0.01204417273402214, 1.3901705741882324, 0.012809053994715214, -0.8329985737800598, -0.06239562854170799, 0.24197892844676971, 0.5488691329956055, -0.14942678809165955, -0.9665237069129944, 0.6133207082748413, 0.20862925052642822, -0.23175854980945587, 0.31460732221603394, -0.05910485237836838, 0.21616317331790924, -0.6116894483566284, -0.12023545056581497, 0.950272262096405, -1.0101829767227173, -0.13933825492858887, -0.5666841268539429, -0.7344046831130981, 0.6473305225372314, 0.7457305192947388, -0.4451955258846283, 0.5176583528518677, 0.26595544815063477, -0.7782473564147949, -0.06799827516078949, -0.9917343854904175, 0.12364511936903, -0.0005207157228142023, -0.9581592679023743, -0.7255527377128601, -0.11746057868003845, 0.4468837082386017, -0.7368221282958984, -0.6561553478240967, -0.3163717985153198, -0.2808201313018799, 0.21794761717319489, 1.0614246129989624, -0.46041157841682434, 0.7350382208824158, 1.2674736976623535, 0.06949954479932785, -1.041745662689209, 0.3669242858886719, -0.6258805394172668, 0.07026971876621246, 0.5878125429153442, 0.9443023204803467, -0.6589251160621643, 0.1910458356142044, 1.3488584756851196, 0.37131786346435547, -0.26996928453445435, -0.28657466173171997, -0.2755762040615082, 0.30408450961112976, -0.6278528571128845, 0.016410937532782555, 0.3678456246852875, -0.04701670631766319, 0.3543614149093628, 0.8728067278862, 0.5874230861663818, -0.15275922417640686, -0.8495404124259949, 0.27873069047927856, -0.31842678785324097, -0.02254650928080082, -0.25163552165031433, -0.40513768792152405, -1.7611411809921265, 0.39204058051109314, -1.3169043064117432, -0.0208998192101717, -1.3322466611862183, -0.44738513231277466, -0.07069167494773865, -0.10510548204183578, 0.41939663887023926, 0.052896976470947266, -0.1709146797657013, -0.5832109451293945, -0.9663761258125305, -0.26073530316352844, 0.6345496773719788, 0.9211174845695496, -0.4976651966571808, 0.0725003257393837, -0.28309398889541626, -0.39359793066978455, 0.5073636174201965, 0.4086531400680542, -0.032345663756132126, -0.8930948972702026, -1.6280395984649658, 0.282370924949646, -0.1855737715959549, 0.1735045462846756, -0.3499498963356018, 0.74093097448349, 0.7176418900489807, -0.4104400873184204, 0.035643402487039566, 0.391508013010025, -0.8794794082641602, -1.2434229850769043, 0.32832014560699463, -0.611987292766571, -0.14245496690273285, 0.49101048707962036, -0.6051775813102722, -0.4744280278682709, 0.41871777176856995, -0.09717901796102524, -1.3879213333129883, -0.9913960099220276, 0.2919085621833801, -0.6384779214859009, 0.447001576423645, -0.4692513346672058, -0.2580067217350006, -0.994632363319397, -0.3317408859729767, -0.17538775503635406, 0.6730659008026123, -1.027508020401001, 0.4184565246105194, 0.5765790343284607, -0.8529888987541199, -0.25081613659858704, 0.3677453100681305, -0.1631028801202774, 0.17671121656894684, 0.40645888447761536, 0.5029041171073914, -0.2522841691970825, 0.5334476232528687, 0.5009969472885132, 0.4332400858402252, -0.862409770488739, -0.3454428017139435, 0.9387452602386475, -0.6097767949104309, -0.01327522937208414, 1.39894700050354, -0.6041393280029297, -1.6250978708267212, 0.3126929998397827, -1.3613327741622925, -0.5072914958000183, -0.005976542830467224, 0.883461594581604, -0.05875442177057266, -0.15877772867679596, 0.10425393283367157, 0.05394880846142769, 0.32977187633514404, -0.2798619270324707, -0.37879666686058044, 0.8284737467765808, -0.190394327044487, -0.6259286999702454, 0.7033220529556274, 0.998066782951355, -0.984144389629364, -0.24023348093032837, -0.9419129490852356, -0.3615154027938843, 0.11361907422542572, 0.5700538158416748, -0.470952570438385, -0.5655425190925598, 0.9583299160003662, 0.1498262882232666, 0.2931958734989166, -0.002314129378646612, -0.16777193546295166, 0.7320968508720398, 1.0199506282806396, -0.11213371902704239, -0.221877783536911, -0.3088059425354004, 1.7409324645996094, 1.0532023906707764, -1.2603273391723633, -0.1484697163105011, -0.5685604810714722, -0.5080655813217163, 0.9454742670059204, 0.5155730247497559, -0.030212244018912315, 0.967593789100647, -0.3561267554759979, -0.0608709417283535, 0.13878674805164337, -0.5170237421989441, -0.5646712779998779, 0.6304355263710022, 1.0992121696472168, 0.9161447882652283, 0.051806725561618805, 0.46298861503601074, 1.0239672660827637, -0.11831632256507874, 0.12658074498176575, 0.4483353793621063, 0.36290866136550903, -0.4875805079936981, -0.13191156089305878, -0.01646474003791809, 0.6186859607696533, -0.6608228087425232, -1.0395307540893555, -0.08405905961990356, 0.8007265329360962, 0.13269053399562836, 0.5201332569122314, 0.730353832244873, 0.45897698402404785, 0.7526026368141174, 0.602652370929718, 0.47980043292045593, -0.6163171529769897, -0.4187031388282776, -0.6862130165100098, -0.3735649287700653, 0.055385392159223557, -0.26232749223709106, -0.20009607076644897, -0.42829692363739014, -0.3171972632408142, 0.25864073634147644, -0.099769726395607, 0.2110181450843811, 1.3622418642044067, 0.12235312908887863, 0.6269293427467346, -0.6226037740707397, -0.1146155297756195, -0.43431463837623596, -0.9090591073036194, -0.1339341700077057, -0.6547524333000183, 0.05219731107354164, 0.13638798892498016, -0.360005259513855, -0.2592860460281372]}, "authors": [{"authorId": "2111591", "name": "Linqing Liu"}], "references": [{"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "560b8e1dde0b38770ed29a07d4c3006164cd80f2", "title": "Attentive Student Meets Multi-Task Teacher: Improved Knowledge Distillation for Pretrained Models"}, {"paperId": "0563230acc891263057e28bf4df21582c81ddfa8", "title": "Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling"}, {"paperId": "a67955df15688a908190e57420df5cea9278cdcb", "title": "Evaluating BERT for natural language inference: A case study on the CommitmentBank"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "740e4599b0e3113ad804cee4394c7fa7c0e96ca5", "title": "Extreme Language Model Compression with Optimal Subwords and Shared Projections"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb", "title": "Text Summarization with Pretrained Encoders"}, {"paperId": "335613303ebc5eac98de757ed02a56377d99e03a", "title": "What Does BERT Learn about the Structure of Language?"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "ef6948edae12eba6f1d486b8600108b9762f36ab", "title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "145b8b5d99a2beba6029418ca043585b90138d12", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"paperId": "7ebed46b7f3ec913e508e6468304fcaea832eda1", "title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"}, {"paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"paperId": "403227333329b36183004f04db72362b604adef3", "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning"}, {"paperId": "2fe7dba5a58aee5156594b4d78634ecd6c7dcabd", "title": "End-to-End Open-Domain Question Answering with BERTserini"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "309e4a8d1ac0a32832447ac0ffb09df8da9894c7", "title": "FLOPs as a Direct Optimization Objective for Learning Sparse Neural Networks"}, {"paperId": "a4ddfafe1bf1f06979aca5e2e85bbc76601f1f2b", "title": "Constraint-Aware Deep Neural Network Compression"}, {"paperId": "c586ee4c556e8e7f24c156e1fb982ed2170e2ff1", "title": "On-Device Neural Language Model Based Word Prediction"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "acdf151b8efc2c6b05662d69f27531afc557dc85", "title": "Training and Inference with Integers in Deep Neural Networks"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "90a16f34d109b63d95ab4da2d491cbe3a1c8b656", "title": "Learning Efficient Convolutional Networks through Network Slimming"}, {"paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "e978d832a4d86571e1b52aa1685dc32ccb250f50", "title": "Dynamic Coattention Networks For Question Answering"}, {"paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "title": "Bidirectional Attention Flow for Machine Comprehension"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets"}, {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"}, {"paperId": "e2dba792360873aef125572812f3673b1a85d850", "title": "Enriching Word Vectors with Subword Information"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "6eecc808d4c74e7d0d7ef6b8a4112c985ced104d", "title": "Binarized Neural Networks"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "a122dbde40c9cd976badd99fdc820156908b7ee8", "title": "The Benefit of Multitask Representation Learning"}, {"paperId": "c3b8367a80181e28c95630b9b63060d895de08ff", "title": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval"}, {"paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "title": "Fast R-CNN"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "1a08e135ac11db0249c6afb4540672c5a349495e", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "a058935fd019c2367fd32c16cd1ce6983a29aafb", "title": "Efficient mini-batch training for stochastic optimization"}, {"paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "title": "A Convolutional Neural Network for Modelling Sentences"}, {"paperId": "d770060812fb646b3846a7d398a3066145b5e3c8", "title": "Do Deep Nets Really Need to be Deep?"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "eb9243a3b98a819539ad57b7b4f05b969510d075", "title": "New types of deep neural network learning for speech recognition and related applications: an overview"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "8729441d734782c3ed532a7d2d9611b438c0a09a", "title": "ADADELTA: An Adaptive Learning Rate Method"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "8492070dc4031ed825e95e4803781752bb5e909f", "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "57458bc1cffe5caa45a885af986d70f723f406b4", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"}, {"paperId": "3a7f94f79e30bb8f647d54c8a94d9fe8e9548a57", "title": "Document preparation system"}, {"paperId": "5536d42ce80e129be8cae172ed1b7659c769d31d", "title": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"}, {"paperId": "727e1e16ede6eaad241bad11c525da07b154c688", "title": "A Model of Inductive Bias Learning"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "title": "Bidirectional recurrent neural networks"}, {"paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49", "title": "Multitask Learning"}, {"paperId": "997dc5d9a058753f034422afe7bd0cc0b8ad808b", "title": "Signature Verification Using A \"Siamese\" Time Delay Neural Network"}, {"paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "title": "Learning representations by back-propagating errors"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "f216444d4f2959b4520c61d20003fa30a199670a", "title": "Siamese Neural Networks for One-Shot Image Recognition"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "2e5f2b57f4c476dd69dc22ccdf547e48f40a994c", "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": "c79a3522c03d3c16d8aec4b84e0efeea72b8114c", "title": "The TeXbook"}, {"paperId": null, "title": "Learning representations by back-propagating"}, {"paperId": null, "title": "Ran El-Yaniv, and Yoshua Bengio"}, {"paperId": null, "title": "Signature verification using a"}, {"paperId": null, "title": "Multitask learning. Machine learning"}, {"paperId": null, "title": "First Quora dataset release: Question pairs"}]}