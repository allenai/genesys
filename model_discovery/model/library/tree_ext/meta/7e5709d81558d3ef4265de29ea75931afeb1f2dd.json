{"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey", "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.", "venue": "ACM Computing Surveys", "year": 2020, "citationCount": 898, "influentialCitationCount": 76, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3530811", "status": "BRONZE"}, "tldr": {"model": "tldr@v2.0.0", "text": "This article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains."}, "embedding": {"model": "specter_v2", "vector": [0.23141799867153168, 0.5826670527458191, -0.5233388543128967, -0.13177436590194702, -0.12354627251625061, -0.13247524201869965, 0.7431999444961548, -0.23664605617523193, -0.39454981684684753, -0.3748936951160431, 0.5192748308181763, -0.02952512912452221, 0.35888218879699707, -0.1659400463104248, -0.2476377636194229, -0.03386920318007469, -0.6355975270271301, 0.2687292993068695, -0.06064724922180176, -0.3327741026878357, 0.04126801714301109, -0.2041093111038208, -1.061387062072754, -0.0997755229473114, 0.27281737327575684, 0.7860933542251587, -0.07402974367141724, 0.4961499273777008, -0.35583168268203735, 0.9071157574653625, 0.792669415473938, -0.3763470947742462, 0.33301103115081787, 0.3234443664550781, -0.2840288579463959, -0.22811752557754517, 0.45625749230384827, -0.8305457234382629, -0.5991373658180237, 0.8198373913764954, -0.33196863532066345, 0.33454588055610657, 0.1205536425113678, -0.942590057849884, -0.015482592396438122, 0.8100798726081848, 0.8268439769744873, 0.8072271347045898, -0.6272110939025879, -0.27900004386901855, 1.7123966217041016, -1.4118484258651733, 0.05599471926689148, 1.1914558410644531, 0.6322222948074341, 0.30737268924713135, -0.4826350808143616, -0.666700541973114, 0.6196541786193848, -0.005071711726486683, -0.3496885299682617, -0.644137442111969, 0.16246530413627625, 0.17227120697498322, 1.9722490310668945, -0.4146241545677185, 0.18433111906051636, 0.613575279712677, 0.077761709690094, 1.5215779542922974, 0.5609428882598877, -0.5128668546676636, -0.15357957780361176, -0.10927723348140717, 0.5725213289260864, 1.1641600131988525, -0.5647839903831482, 0.7003817558288574, -1.1931712627410889, -0.10364119708538055, 0.4461613893508911, 0.3425067663192749, -0.07254229485988617, -0.4482780694961548, -0.14872455596923828, 0.8409987092018127, 0.624443531036377, 1.0187296867370605, -0.38469913601875305, 0.7892063856124878, 0.4452855587005615, 0.3851066827774048, -0.23159444332122803, 0.6338878273963928, 0.002449328312650323, 0.232606902718544, -0.7978509664535522, 0.15623492002487183, -0.14155100286006927, 0.6466489434242249, -0.037980370223522186, 0.6889849901199341, -0.8005520701408386, 0.28433817625045776, 1.309720516204834, 0.18824400007724762, 0.3533961772918701, -0.8996152281761169, -0.06741134077310562, -0.4337550103664398, -0.014200435020029545, -0.11024041473865509, -0.046115171164274216, -0.5692505836486816, -0.8428561687469482, -0.9248381853103638, -0.4434490501880646, 0.7530161738395691, -0.8743541240692139, 0.7360190153121948, -0.8842124938964844, 0.08215764909982681, -0.2294185906648636, 0.4158098101615906, 0.08460686355829239, 0.5544342994689941, 0.47257158160209656, -0.07680775225162506, 0.7078990340232849, -1.0599160194396973, -0.6428736448287964, -0.9214301705360413, 0.5067858695983887, -0.09219124913215637, 0.1433166265487671, 0.14981956779956818, -1.1072286367416382, -0.9138708114624023, -0.8005304336547852, 0.14272402226924896, -0.42038431763648987, 0.09468135237693787, 1.053314447402954, 0.5048736333847046, -1.4813660383224487, 0.6161409020423889, -0.45077118277549744, 0.20973935723304749, 0.5369125008583069, 0.4108082354068756, 0.38022229075431824, -0.2475937455892563, -0.9180991053581238, 0.3569612205028534, 0.027777111157774925, -0.5747984051704407, -0.7439129948616028, -0.8224673867225647, -1.0178786516189575, 0.358920693397522, -0.0019220623653382063, -0.4206271767616272, 1.3689383268356323, -0.27382075786590576, -1.13076651096344, 0.6096996068954468, -0.3503374755382538, -0.0814141258597374, -0.03272860869765282, -0.36339741945266724, -0.10722599178552628, -0.3580009937286377, -0.08211551606655121, 0.4228650629520416, 0.6196154952049255, -0.344018816947937, -0.2454156130552292, 0.2688409686088562, -0.04956093057990074, -0.21155501902103424, -0.2855241298675537, 0.6944236755371094, -0.05559185519814491, -0.28190934658050537, 0.45923513174057007, 0.6803104281425476, -0.22940762341022491, 0.20676586031913757, -0.22751812636852264, -1.0895171165466309, 0.6241317391395569, 0.17397354543209076, 1.0522868633270264, -1.0767898559570312, -0.9013999104499817, 0.10501818358898163, 0.3058567941188812, -0.06569050252437592, -0.704396665096283, 0.3048323392868042, -0.7395783066749573, 0.06286008656024933, -0.1489766240119934, -0.9747644662857056, 0.3723144829273224, -0.05897149816155434, -1.0954515933990479, 0.05348336324095726, 0.04817347973585129, 0.9931944608688354, -1.0327812433242798, 0.21175549924373627, 0.3746643662452698, 0.1971300095319748, -0.9171493053436279, 1.5490869283676147, -0.02342490293085575, -0.2910183072090149, 0.15075449645519257, -0.12112776935100555, 0.16734912991523743, -0.39744624495506287, 0.49560850858688354, -0.6356057524681091, -0.25572121143341064, 0.6266108751296997, -0.2993742823600769, 1.1982479095458984, -0.4347275495529175, 0.5151759386062622, -0.06154424324631691, -0.8272181749343872, 0.33199915289878845, 0.4540153443813324, -0.13576671481132507, -0.8746710419654846, 0.33245930075645447, 0.4310222268104553, -0.6440292596817017, 0.7404932379722595, 0.8717265725135803, 0.46734753251075745, 0.04141146317124367, 0.3336889445781708, 0.6434913873672485, -0.2240307629108429, 0.3145495653152466, 0.5275185108184814, 0.7528209686279297, 0.2725307047367096, 0.39880308508872986, -0.15740546584129333, 0.21266600489616394, -1.1458103656768799, -0.06447339057922363, 0.6381059885025024, 0.21063187718391418, 0.4768809974193573, 0.2738990783691406, -0.6148678064346313, -0.48522353172302246, -0.5813310146331787, 0.7536701560020447, 1.612180233001709, -0.2886408269405365, -0.2741956114768982, -0.27527105808258057, -0.23219114542007446, -0.822841465473175, 0.172671839594841, -0.04476785287261009, -0.4621248245239258, -0.6685714721679688, -0.8696984648704529, 0.9100385308265686, 0.7257776856422424, 1.1520788669586182, -0.3564882278442383, -0.635750412940979, -0.30613160133361816, 0.2772059738636017, -0.786687433719635, -0.48591670393943787, 0.7495498657226562, -0.9910081624984741, -0.3420359194278717, 0.22381377220153809, -0.1968763768672943, 0.26422426104545593, -0.5030627846717834, 1.1007345914840698, -0.3764514923095703, 0.21517327427864075, 0.19892817735671997, 0.7114591002464294, -0.35681813955307007, -0.5701128840446472, -0.03345013037323952, 0.154962420463562, -0.13474822044372559, 0.2102619856595993, 0.36484554409980774, 0.13721080124378204, 0.03527241572737694, -0.44186773896217346, 0.021770423278212547, 0.06564345210790634, 0.46640998125076294, 0.8250216841697693, -0.4993722438812256, -0.1938719004392624, -1.0662202835083008, 0.9522350430488586, 0.44447341561317444, -0.5087093114852905, 0.27387091517448425, -1.031246304512024, -0.0561375766992569, 0.6027235984802246, -0.2499915063381195, -0.1588430255651474, -0.8221367597579956, 0.05567922443151474, -0.639447808265686, 0.09422729164361954, -0.10341660678386688, 0.3589046895503998, 0.1246531680226326, 0.0418081171810627, 0.6970635056495667, 0.06692951917648315, 0.1524970531463623, 0.5199132561683655, -0.92475825548172, 0.3765709102153778, 0.06681758910417557, 0.33002981543540955, -0.1617680788040161, -0.2992974519729614, -0.11392607539892197, -0.470259428024292, -0.3454851508140564, 0.21223877370357513, -0.4453129470348358, -0.23972029983997345, -0.6918816566467285, -0.9308702349662781, 0.3320251703262329, -0.8484571576118469, -0.2208550125360489, -0.24681948125362396, -0.5353452563285828, -0.37768691778182983, -1.2103177309036255, -1.236743450164795, -0.782042920589447, -0.8591769337654114, -1.2528070211410522, 0.4081035852432251, 0.17293456196784973, 0.08140319585800171, -0.48816540837287903, -0.20614440739154816, -0.38488200306892395, 1.1940118074417114, -0.8139636516571045, 1.0700795650482178, -0.16410355269908905, -0.6695798635482788, 0.13991296291351318, 0.032568152993917465, 0.5008053183555603, -0.35833540558815, -0.037832632660865784, -1.3989955186843872, 0.06988218426704407, -0.08667661994695663, -0.3259504437446594, 0.17867368459701538, 0.12852105498313904, 0.763952374458313, -0.05974310263991356, -0.31683796644210815, 0.22282469272613525, 1.2920176982879639, -0.4767991304397583, -0.0018601523479446769, 0.048937417566776276, 0.8376898169517517, -0.22676382958889008, -0.24950338900089264, 0.48696208000183105, 0.29675424098968506, 0.25133782625198364, 0.39800500869750977, -0.1095413938164711, -0.019555727019906044, -0.6172879338264465, 0.6100708246231079, 1.517408013343811, 0.2676970660686493, 0.1361895501613617, -0.9293203949928284, 0.6723827719688416, -1.1608860492706299, -0.8250563740730286, 0.9640458822250366, 0.64106285572052, 0.42627042531967163, -0.11583419144153595, -0.4432293176651001, 0.42708417773246765, 0.42936447262763977, 0.7811259627342224, -0.22963470220565796, -0.8364290595054626, -0.134895920753479, 1.0590287446975708, 0.30309197306632996, 0.6223793625831604, -0.18276867270469666, 0.43450090289115906, 14.9963960647583, 0.8543052673339844, -0.09661823511123657, 0.7240831851959229, 0.7255426645278931, 0.3463299572467804, -0.6893361210823059, 0.19587209820747375, -0.8265286087989807, -0.371966689825058, 1.0029287338256836, 0.27437227964401245, 0.6555032730102539, 0.17676272988319397, -0.23445570468902588, 0.20526286959648132, -0.6515122652053833, 0.8488838076591492, 0.3244152069091797, -1.443719744682312, 0.31437361240386963, -0.026338854804635048, 0.09226611256599426, 0.5496444702148438, 0.9618818163871765, 0.8986930847167969, 0.6158690452575684, -0.6041267514228821, 0.8060392141342163, 0.29929718375205994, 0.8490235209465027, 0.01769161783158779, 0.15404431521892548, 0.5302736759185791, -1.0117969512939453, -0.3747740387916565, -0.20454175770282745, -1.1911643743515015, 0.02759438194334507, 0.11930301040410995, -0.320727676153183, -0.6871268153190613, -0.27629464864730835, 1.0706983804702759, 0.2959561049938202, 0.3699820637702942, -0.3995799124240875, 0.8212664127349854, -0.4880259037017822, 0.2571720480918884, 0.3653320372104645, 0.32335734367370605, 0.19386935234069824, -0.32809653878211975, -0.04871620982885361, 0.02407767064869404, 0.13291509449481964, 0.5552152991294861, -0.7032896876335144, -0.30973902344703674, -0.40831997990608215, -0.3766239881515503, -0.015353672206401825, 0.7893954515457153, 0.39973631501197815, 0.4912109673023224, -0.20369736850261688, -0.09830951690673828, 0.5854971408843994, 0.13269910216331482, -0.41566699743270874, 0.020229501649737358, 0.22768133878707886, -0.3590448796749115, -0.11284136027097702, 0.6416978240013123, -0.14320939779281616, -0.6321067214012146, -0.8480703234672546, -0.6168063879013062, 0.25930219888687134, -0.9050374627113342, -0.5259197354316711, 0.9035975337028503, -0.37789273262023926, -0.321601003408432, 0.582655668258667, -0.7549975514411926, -0.19449111819267273, 0.010982419364154339, -1.6603848934173584, -0.6581975221633911, 0.2816017270088196, -0.14800669252872467, -0.4745897948741913, -0.36760419607162476, 1.150412678718567, 0.2113586813211441, -0.1872282326221466, 0.24364697933197021, -0.24913960695266724, 0.07612297683954239, -0.5282436013221741, -0.882256805896759, 0.5807489156723022, 0.42078307271003723, 0.24393223226070404, 0.4522212743759155, 0.2214614450931549, 0.7832364439964294, -0.6441832780838013, -0.118681900203228, 0.7999377250671387, -0.6486837267875671, -0.26363593339920044, -0.6220117807388306, -0.4421263337135315, 0.4126906991004944, 0.5600854754447937, -0.2385721206665039, 0.20831526815891266, 0.058965105563402176, -0.957028865814209, -0.404289186000824, -0.6902016401290894, 0.1328422725200653, 0.7007321119308472, -1.117752194404602, -0.4786030054092407, -0.21939212083816528, 0.01695350743830204, -0.8662870526313782, -0.3908049166202545, -0.13274845480918884, -0.08663474023342133, -0.5728175044059753, 0.9769965410232544, -0.2116120606660843, 0.35985276103019714, 0.7256707549095154, -0.2979069948196411, -0.7056849598884583, 0.010869763791561127, -1.008202075958252, -0.09853310137987137, -0.06667053699493408, 0.32620930671691895, -0.5978896021842957, 0.2642710506916046, 0.6462793946266174, 0.0002502274001017213, -0.6436457633972168, -0.9283520579338074, 0.048571858555078506, -0.24858693778514862, -0.5056301951408386, 0.37496882677078247, -0.15333965420722961, -0.2434500753879547, 0.41179242730140686, 0.575623631477356, 0.10843630135059357, -0.1929382085800171, -0.430988609790802, -0.14229083061218262, -0.014326496049761772, 0.10284265130758286, -0.6159729957580566, -0.24342596530914307, -1.5755844116210938, 0.2733698785305023, -1.3497161865234375, -0.013759894296526909, -0.9086010456085205, -0.3291630446910858, -0.26581916213035583, -0.4193401336669922, 0.45053690671920776, 0.47054779529571533, -0.31833773851394653, -0.19392946362495422, -0.3892061114311218, -0.6647814512252808, 0.9694058299064636, 0.6451467275619507, -0.8559491634368896, 0.4478970766067505, -0.10510794818401337, 0.04896510764956474, 0.20213094353675842, 0.42475733160972595, -0.5130593776702881, -0.9181575179100037, -1.135796308517456, 0.4125629961490631, -0.1786176562309265, -0.005738270469009876, -0.7616140246391296, 0.6382861137390137, 0.41729143261909485, -0.44065818190574646, 0.27269092202186584, 0.6201139092445374, -1.2842172384262085, -0.46004045009613037, 0.6805075407028198, -0.797696053981781, 0.6047675013542175, 0.10182970017194748, -0.6008141040802002, -0.4471578299999237, 0.6544286608695984, 0.3954967260360718, -0.8304708003997803, -0.6539450883865356, 0.45730769634246826, -0.7813120484352112, 0.10367331653833389, -0.19493365287780762, -0.39055493474006653, -1.1595572233200073, -0.24330629408359528, 0.03834571689367294, 0.27777236700057983, -0.2892827093601227, 0.5659698247909546, 0.5781975984573364, -1.154107928276062, 0.4022614657878876, 0.5291926264762878, -0.09144371747970581, -0.3693728744983673, -0.0770752802491188, 0.4938088655471802, -0.273116797208786, 0.6546444892883301, -0.10724334418773651, 0.368515282869339, -0.6271272301673889, -0.20048175752162933, 1.2896584272384644, -0.9112778306007385, -0.05590993911027908, 0.9849153161048889, -0.3292979300022125, -0.9992310404777527, 0.15624941885471344, -1.2787476778030396, -0.5503884553909302, -0.44507917761802673, 0.2727026045322418, -0.053362902253866196, -0.26913365721702576, -0.038252171128988266, -0.4214031398296356, 0.0347173847258091, -0.06570863723754883, -0.40231409668922424, 0.6815944910049438, 0.3891719877719879, -0.5264057517051697, 0.5336729884147644, 0.508634626865387, -0.8998881578445435, -0.6676440834999084, -0.7149310111999512, -0.3270135223865509, -0.11644358932971954, 0.44473642110824585, -0.21943403780460358, -1.0616888999938965, 1.024416446685791, 0.6593419909477234, 0.15110154449939728, 0.6298041939735413, -0.45695245265960693, 0.05954127386212349, 0.6189615726470947, 0.3708514869213104, -0.7100182771682739, -0.4114764332771301, 1.6089125871658325, 1.002251386642456, -0.4879607558250427, 0.2540222406387329, -0.2203367054462433, -0.5484261512756348, 0.5859717726707458, 0.36907562613487244, -0.23916974663734436, 1.0562835931777954, 0.025640249252319336, 0.011482371017336845, 0.18968023359775543, -1.1362736225128174, -0.18854616582393646, 0.22865904867649078, 1.1455693244934082, 0.8087493181228638, 0.1008797436952591, 0.22853650152683258, 0.9974601864814758, 0.16482095420360565, 0.13767887651920319, 0.5457882881164551, 0.68039870262146, -0.07205410301685333, -0.09479960054159164, 0.16319029033184052, 0.7995039820671082, -0.532418966293335, -0.7912701368331909, 0.5085912346839905, 0.5161749720573425, 0.1841271072626114, 0.4481405019760132, 0.693876326084137, 0.18880201876163483, 0.6160550713539124, 0.16462616622447968, 0.7017419934272766, -0.42903009057044983, -0.8064253330230713, -0.20109973847866058, -0.6672279238700867, -0.21818850934505463, -0.746632993221283, -0.20102539658546448, -0.34599512815475464, -0.2066628336906433, 0.01997491344809532, 0.0047658896073699, 0.0025213893968611956, 0.9457079768180847, 0.3263700306415558, 0.5110337734222412, -0.28614240884780884, -0.1937834471464157, -0.5939894914627075, -0.7470844984054565, -0.055528461933135986, -0.5251777172088623, 0.08184885233640671, -0.3872171938419342, 0.02996896393597126, -0.30879175662994385]}, "authors": [{"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "3226635", "name": "Mostafa Dehghani"}, {"authorId": "11774695", "name": "Dara Bahri"}, {"authorId": "1680617", "name": "Donald Metzler"}], "references": [{"paperId": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17", "title": "Designing Effective Sparse Expert Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "9d40837175577bb0009b138269b422f6d5820d00", "title": "Transformer Memory as a Differentiable Search Index"}, {"paperId": "2d82ee05b132d4681c3bd517afc17d608fe6e525", "title": "Simple Local Attentions Remain Competitive for Long-Context Tasks"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "66d735987a31d666a6459566ae026c40ab9a1c3a", "title": "The Efficiency Misnomer"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "9933a5af7895354087baf6c96b64dc8a8973eaed", "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "e79d1206292bc5e67ba19737d87d4b2ea4a37105", "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "e32a12b14e212506115cc6804667b3d8297917e1", "title": "Poolingformer: Long Document Modeling with Pooling Attention"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "0cd82dfae930ac4b57c0e959f744f2d10bf87649", "title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "03d8cd1d4bbdc0f6f07bdd0412997606b3648e78", "title": "HyperGrid: Efficient Multi-Task Transformers with Grid-wise Decomposable Hyper Projections"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "26299d5fdc5137291dc6a091573b3d18aba1d1c2", "title": "MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"}, {"paperId": "baed71eed57ad462f3ab138d4b1700a738cd5414", "title": "ETC: Encoding Long and Structured Data in Transformers"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39", "title": "Training with Quantization Noise for Extreme Model Compression"}, {"paperId": "37127a02d129cb733377458d2f155997e3fd622f", "title": "Training with Quantization Noise for Extreme Fixed-Point Compression"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "59c7f27be8b09596714384f4a35face7cb74ad11", "title": "NAT: Neural Architecture Transformer for Accurate and Compact Architectures"}, {"paperId": "49e5b09480189fc9b2316a54f9d1e55cf0097c8b", "title": "Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "5240bad304d5e9dd6a7ab1e089e024119ae55567", "title": "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks"}, {"paperId": "e763fdc9ae56826ff799163ea035b29bffd8ea6f", "title": "Scaling Autoregressive Video Models"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5132500b23d2da47129b3f4f68dd30947a29e502", "title": "CCNet: Criss-Cross Attention for Semantic Segmentation"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "3861ae2a6bdd2a759c2d901a6583e63a216bc2fc", "title": "Weighted Transformer Network for Machine Translation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83", "title": "Depthwise Separable Convolutions for Neural Machine Translation"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "a456265138c088a894301c0433dae938705a9bec", "title": "Deep Sets"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a", "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "37fa065905442974682e4aca19c8108075d4de44", "title": "Ranking via Sinkhorn Propagation"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "441f33fab0614fa0696be54a046cbc692b7e70a2", "title": "A Relationship Between Arbitrary Positive Matrices and Doubly Stochastic Matrices"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "eaa09c607780373cc809bce89b6b28b17e301f27", "title": "TokenLearner: Adaptive Space-Time Tokenization for Videos"}, {"paperId": "ec28cb6f488a0e7f0d67f62a70a142f4601b7f7f", "title": "Scaling Laws vs Model Architectures : How does Inductive Bias In\ufb02uence Scaling? An Extensive Empirical Study on Language Tasks"}, {"paperId": null, "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning"}, {"paperId": null, "title": "(cid:32)Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "Are pre-trained convolutions better than pre-trained transformers?"}]}