{"paperId": "c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e", "title": "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention", "abstract": "Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2310.07911", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head, and empirically demonstrates that this MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks."}, "embedding": {"model": "specter_v2", "vector": [0.16498856246471405, 0.712307870388031, -0.09577485173940659, -0.030347120016813278, -0.10822881013154984, 0.3778464198112488, 0.5830885767936707, -0.21828442811965942, -0.486369252204895, -0.3167416453361511, 0.9364455938339233, -0.017951298505067825, 0.29293668270111084, 0.18433423340320587, 0.027583327144384384, -0.0006250459118746221, -0.8720850944519043, 0.47748878598213196, -0.03663294389843941, -0.33698877692222595, -0.04450201243162155, -0.845352292060852, -0.87282395362854, -0.0625658631324768, 0.013836733996868134, 0.12416861951351166, 0.4346400499343872, 0.8191509246826172, -0.2356041818857193, 0.2542210817337036, 0.6277143359184265, -0.4805662930011749, 0.07236561179161072, 0.03978903591632843, -0.2764110267162323, -0.41673392057418823, 0.445868581533432, -0.44678065180778503, -0.1907651126384735, 0.919712245464325, -0.020517274737358093, 0.350864440202713, 0.43458184599876404, -0.6610312461853027, -0.49490517377853394, 1.1019625663757324, 0.40334054827690125, 0.7588217854499817, -0.30773597955703735, -0.6166968941688538, 1.734419345855713, -1.669884204864502, 0.20257239043712616, 1.706372857093811, 0.3002166152000427, 0.19191482663154602, -0.38195905089378357, -0.5021982789039612, 0.7949806451797485, 0.32175302505493164, -0.9422147274017334, -0.4582623541355133, 0.320613831281662, 0.12435250729322433, 2.0864808559417725, -0.3182762861251831, 0.23435233533382416, 0.3907783627510071, 0.17251884937286377, 1.5628644227981567, -0.31026631593704224, -0.8655827641487122, -0.5480614304542542, 0.05604667216539383, 0.5854740738868713, 0.7357618808746338, -0.3862418234348297, 0.09091295301914215, -1.0632047653198242, -0.06633970141410828, 0.20364871621131897, -0.02780206874012947, -0.09096573293209076, -0.27843374013900757, -0.6654711961746216, 0.9975722432136536, 0.45382750034332275, 0.7569481730461121, -0.40871337056159973, 0.46733972430229187, 0.5286728739738464, 0.013546490110456944, -0.3515773117542267, 0.3932264447212219, -0.2013738602399826, 0.7239983081817627, -0.8821203708648682, 0.13524256646633148, 0.22242850065231323, 0.9794244766235352, -0.08237399905920029, 0.09529977291822433, -0.6894823908805847, -0.11663243174552917, 1.634015679359436, 0.39294159412384033, 0.3219519555568695, -0.44692909717559814, 0.2925945520401001, -0.8171657919883728, -0.3126765191555023, -0.9470105171203613, -0.36072155833244324, -0.09491747617721558, -0.7686077952384949, -1.4505938291549683, -0.45896124839782715, 0.298008531332016, -0.4248976707458496, 0.7640011310577393, -0.006724849808961153, -0.014760449528694153, -0.2179996520280838, 0.3056721091270447, 0.5718810558319092, 0.8592680096626282, 0.7472623586654663, 0.18897320330142975, 1.1844139099121094, -1.0963349342346191, -0.7688829302787781, -1.3835220336914062, 0.701464056968689, -0.2332981377840042, 0.49792423844337463, -0.11112292855978012, -0.9536696672439575, -0.7273144125938416, -0.8145289421081543, -0.3169667422771454, -0.528989315032959, 0.21890076994895935, 0.8625854849815369, 0.12892134487628937, -1.1747809648513794, 0.6023276448249817, 0.04887039586901665, -0.25813183188438416, 0.04679929092526436, 0.4142100512981415, 0.047865014523267746, -0.38230282068252563, -1.4330767393112183, 0.4740425646305084, 0.4357820153236389, -0.12947632372379303, 0.17315976321697235, -0.45329710841178894, -1.2758105993270874, 0.005283012520521879, 0.18569985032081604, -0.7469499707221985, 1.2033593654632568, 0.22860027849674225, -1.5728309154510498, 0.527959942817688, -0.5897589325904846, 0.5006482005119324, -0.2662278413772583, -0.4517749547958374, -0.47356903553009033, -0.6820189356803894, -0.15501579642295837, 0.6541836857795715, 0.5341633558273315, 0.3085923194885254, -0.048015616834163666, -0.2782435119152069, -0.7042572498321533, 0.17194420099258423, -0.5848046541213989, 1.4058531522750854, -0.6568851470947266, 0.1095866858959198, 0.5651832818984985, 0.5175571441650391, 0.13895323872566223, -0.3692294657230377, -0.8385055661201477, -1.12562894821167, 0.723131000995636, 0.12728287279605865, 1.0219794511795044, -0.7868024110794067, -0.1716562658548355, -0.25872814655303955, 0.053682077676057816, -0.1943989098072052, -0.8262947797775269, 0.43417078256607056, -0.46235695481300354, 0.18034301698207855, 0.022720377892255783, -1.3455713987350464, 0.3508353531360626, -0.39704179763793945, -0.4573999345302582, -0.060329798609018326, -0.07316069304943085, 1.0705435276031494, -0.8486331105232239, -0.2970905303955078, 0.03379157930612564, 0.5288135409355164, -1.0392377376556396, 1.257413387298584, -0.04178227111697197, -0.07642048597335815, 0.12351527810096741, -0.3259267508983612, 0.0037328677717596292, -0.34301528334617615, 0.48444342613220215, -0.6318233609199524, -0.02164115570485592, 0.47942104935646057, -0.4973827600479126, 1.0630900859832764, -0.17371900379657745, 0.4847070872783661, 0.09068576246500015, -0.2752012610435486, 0.13712316751480103, 0.29357263445854187, -0.5260251760482788, -0.4282432794570923, 0.3694647550582886, 0.41875138878822327, -0.3603099584579468, 0.4048275053501129, 0.8229726552963257, 0.7641496062278748, -0.4059790372848511, 0.10159796476364136, 0.44735413789749146, -0.174323171377182, 0.10131092369556427, 0.22399349510669708, 0.25413966178894043, 0.30438971519470215, 1.0326108932495117, -0.14888527989387512, 0.4797872304916382, -1.1658854484558105, -0.09978899359703064, 0.0822795033454895, 0.3299698233604431, 0.7077207565307617, 0.4079569876194, -0.6642323136329651, -0.14268842339515686, 0.14874869585037231, 0.7420294284820557, 2.112766742706299, -0.4845068156719208, -0.20910422503948212, -0.4739207327365875, 0.08644983172416687, -0.3232266306877136, 0.18228526413440704, -0.3752951920032501, 0.01168510876595974, -0.7389246821403503, -1.0319085121154785, 0.5244995951652527, 0.39505329728126526, 0.7008832097053528, -0.8303384780883789, -0.3038213551044464, -0.262065052986145, 0.5285022854804993, -0.7184923887252808, -0.6257462501525879, 0.5661766529083252, -0.5748893022537231, 0.061867065727710724, 0.08723852783441544, -0.15337686240673065, -0.21715451776981354, -0.8776877522468567, 0.9286409616470337, -0.7705095410346985, 0.10262976586818695, -0.14095830917358398, 0.8602735996246338, -0.64117032289505, -0.22577719390392303, 0.4169977903366089, 0.27546823024749756, -0.15488116443157196, 0.7295761704444885, 0.4644508957862854, 0.12734054028987885, -0.3162060081958771, -0.024758165702223778, 0.2534883916378021, 0.24191784858703613, -0.15039408206939697, 0.5527017712593079, -0.7478101253509521, -0.26618948578834534, -1.6396572589874268, 0.6467599868774414, -0.11497234553098679, -0.335814893245697, -0.02819139137864113, -0.6916703581809998, -0.3430686295032501, 0.4375915825366974, -0.369574636220932, -0.17254680395126343, -0.5286537408828735, 0.3122197985649109, -0.3267790973186493, -0.2600363790988922, 0.3566172122955322, -0.1614495813846588, 0.47876670956611633, -0.03023264929652214, 0.45041197538375854, 0.1261117309331894, -0.052854184061288834, 0.8258834481239319, -1.0001505613327026, 0.5034891366958618, 0.5407184958457947, 0.28363555669784546, -0.48836517333984375, -0.0928739681839943, -1.0794659852981567, -0.22653675079345703, -0.5334166288375854, -0.4323697090148926, 0.1489076018333435, 0.3496061861515045, -0.4352249205112457, -0.5952020883560181, 0.02537146769464016, -1.5016347169876099, 0.12397722154855728, 0.5013463497161865, -0.14222368597984314, 0.24490967392921448, -1.2442041635513306, -1.2295318841934204, -0.21697135269641876, -0.9694516062736511, -1.1560370922088623, 0.45601022243499756, 0.1713910698890686, -0.5854225158691406, -0.7501511573791504, 0.0638994574546814, -0.49845850467681885, 1.202121376991272, -1.0305521488189697, 1.0526593923568726, -0.42186471819877625, -0.25241684913635254, -0.14299613237380981, 0.12358760088682175, 0.11876718699932098, -0.41847360134124756, 0.2154022753238678, -0.899420976638794, 0.35704952478408813, -0.36520588397979736, 0.1091509759426117, 0.1484343409538269, 0.45854413509368896, 0.5339896082878113, -0.0977567508816719, -0.5189455151557922, 0.41051343083381653, 1.140810489654541, -0.8762325048446655, 0.16723380982875824, 0.24541611969470978, 1.3934537172317505, 0.29483723640441895, -0.540062665939331, 0.5018081068992615, 1.0157934427261353, 0.343922883272171, 0.1828712522983551, 0.04398798942565918, -0.15241236984729767, -0.6195113658905029, 0.7245436906814575, 2.037587881088257, 0.36738133430480957, -0.07772170752286911, -1.1712509393692017, 0.5673502683639526, -0.989592969417572, -0.5938162803649902, 0.2934052646160126, 0.6079034805297852, 0.20998430252075195, -0.7294474840164185, -0.36118483543395996, -0.3449421525001526, 0.5815765261650085, 0.5621500611305237, -0.24760758876800537, -0.9027156233787537, 0.17139309644699097, 0.42031562328338623, 0.06061748042702675, 0.7538871765136719, -0.6082437038421631, 0.7496132254600525, 14.646921157836914, 0.013792595826089382, -0.06747276335954666, 0.4490196406841278, 0.4156358540058136, 0.28966978192329407, -0.338249146938324, -0.10003843903541565, -1.5272737741470337, -0.17083965241909027, 1.3871811628341675, 0.24631249904632568, 0.4357725977897644, 0.28034520149230957, -0.13999056816101074, 0.39794403314590454, -1.0062063932418823, 0.7869058847427368, 0.8679981827735901, -0.9846686720848083, 0.40118804574012756, 0.2134629637002945, -0.04317956790328026, 0.5362964868545532, 0.7426081299781799, 0.8875025510787964, 0.435956209897995, -0.35459181666374207, 0.27315449714660645, 0.15077254176139832, 0.5301051735877991, -0.09045576304197311, 0.24041223526000977, 0.20493446290493011, -0.9216236472129822, -0.3582659959793091, -0.5407533645629883, -1.4093003273010254, 0.18011942505836487, 0.15393497049808502, -0.402563214302063, -0.8708112835884094, -0.1883654147386551, 0.7116020321846008, -0.1468544602394104, 0.4325447082519531, -0.5919986963272095, 0.42136576771736145, -0.07332319766283035, -0.3318467140197754, 0.12133067846298218, 0.3507586717605591, 0.3156678080558777, 0.19161637127399445, 0.19439058005809784, -0.09379518777132034, -0.057432446628808975, 0.5406880378723145, -0.3378848135471344, 0.17856605350971222, -0.449677437543869, -0.08430743962526321, 0.41529348492622375, 0.9859633445739746, 0.707207977771759, 0.4048643112182617, -0.5443782210350037, 0.22797688841819763, 0.7748594880104065, 0.03129536285996437, -0.003441342618316412, -0.279048353433609, 0.3259226083755493, -0.572851300239563, 0.1395183801651001, 0.505887508392334, -0.1731058955192566, -0.2552771270275116, -1.0221389532089233, -0.23907586932182312, 0.6080883741378784, -0.7768340110778809, -0.6932461261749268, 0.5341004133224487, -0.37830719351768494, 0.002697717398405075, 0.06804661452770233, -0.9254960417747498, -0.44039186835289, 0.6002258658409119, -1.335945963859558, -0.8624340891838074, 0.5273126363754272, -0.3488864600658417, -0.42698946595191956, 0.04923345521092415, 1.4219062328338623, 0.19779513776302338, -0.9974566102027893, 0.13055945932865143, -0.26947322487831116, 0.08229576796293259, -0.019953174516558647, -0.7806544899940491, 0.7406273484230042, 0.42907005548477173, -0.20448653399944305, 0.6877439618110657, 0.06645358353853226, 0.2885533571243286, -1.2562741041183472, -0.04429711401462555, 1.191062331199646, -0.801064670085907, -0.18869735300540924, -0.5620576739311218, -0.8064020872116089, 0.5651336312294006, 0.891467809677124, -0.14315295219421387, 0.775131344795227, 0.41635334491729736, -0.6415004134178162, -0.08069039136171341, -0.3099527955055237, 0.09253965318202972, 0.08060584217309952, -0.971809983253479, -0.5887612104415894, -0.33169883489608765, 0.3774963319301605, -0.9077644348144531, -0.3882690966129303, -0.6185489892959595, -0.048956308513879776, 0.14585323631763458, 1.0800687074661255, -0.389141708612442, 0.759446918964386, 0.6402915120124817, -0.3421001732349396, -1.022510290145874, -0.5126873254776001, -0.6271956562995911, -0.3864772319793701, 0.09925299137830734, 0.8688302636146545, -0.46218225359916687, -0.11603106558322906, 1.213834524154663, 0.3642178773880005, -0.19295206665992737, -0.4298435151576996, 0.05856873840093613, 0.07346642762422562, -0.4424799084663391, 0.4723803400993347, -0.047683849930763245, 0.04867237061262131, 0.7728415131568909, 0.3850341737270355, 0.7067641019821167, 0.22844164073467255, -0.6754050254821777, 0.27604132890701294, -0.45037299394607544, -0.16738596558570862, -0.5009295344352722, -0.4374997615814209, -1.0467188358306885, 0.20629149675369263, -1.2349377870559692, 0.10642320662736893, -0.9946394562721252, 0.023911288008093834, 0.3000255823135376, -0.23276060819625854, 0.43279731273651123, -0.19068072736263275, -0.25307396054267883, -0.1933852583169937, -0.4462524950504303, -0.46535956859588623, 0.9763306379318237, 0.7711925506591797, -0.44692811369895935, 0.09587345272302628, -0.31409814953804016, -0.18452750146389008, 0.31382039189338684, 0.254021018743515, -0.31662067770957947, -0.49093833565711975, -1.5668004751205444, 0.5692566633224487, -0.18126384913921356, -0.07607697695493698, -0.4653906524181366, 1.1559813022613525, 0.4095141589641571, -0.15555830299854279, -0.19464357197284698, 0.429502934217453, -0.8678693175315857, -0.6309164762496948, 0.2628393769264221, -0.6174505352973938, 0.5131274461746216, 0.13200625777244568, -0.6438413262367249, -0.27489104866981506, 0.9227967858314514, -0.06002028286457062, -0.9204878807067871, -1.2607747316360474, 0.5003336071968079, -0.4494348466396332, 0.23380763828754425, -0.31835660338401794, -0.010776132345199585, -1.1875518560409546, -0.14426115155220032, 0.44469717144966125, 0.13814595341682434, -0.5796176791191101, 0.6423052549362183, 0.6150434017181396, -0.9641756415367126, 0.06104610115289688, 0.6933948993682861, 0.01574825495481491, -0.10060400515794754, 0.26540860533714294, 0.48174282908439636, -0.07349979877471924, 0.6546014547348022, 0.24000321328639984, 0.1860211342573166, -0.8385793566703796, -0.11867953836917877, 0.5268085598945618, -0.3450123965740204, -0.325754314661026, 1.3419018983840942, -0.36056938767433167, -1.517282485961914, 0.0522305853664875, -1.078795075416565, -0.7482495903968811, -0.01855524443089962, 0.8533804416656494, 0.29111596941947937, -0.021497661247849464, -0.4238208532333374, -0.844746470451355, 0.2532530725002289, -0.1551397144794464, -0.4070051610469818, 0.7868184447288513, -0.1430164873600006, -0.7464615106582642, 0.5036909580230713, 1.1814476251602173, -0.772786021232605, -0.26444917917251587, -0.8630931973457336, -0.20356035232543945, 0.21430689096450806, 0.758922815322876, -0.19780240952968597, -0.6334825158119202, 0.9367215037345886, 0.2669852077960968, -0.02533680386841297, -0.019571442157030106, -0.2367039918899536, 0.33547210693359375, 0.7904093265533447, -0.04026765748858452, -0.4643738269805908, -0.8399967551231384, 1.6017823219299316, 1.1446239948272705, -0.6578891277313232, 0.2532109022140503, -0.29209455847740173, -0.8312816619873047, 0.6456015706062317, 0.4631798267364502, 0.14929988980293274, 0.976508617401123, -0.16425038874149323, -0.021221790462732315, -0.033241190016269684, -0.886188805103302, -0.13609595596790314, 0.967812716960907, 1.0227552652359009, 1.0611437559127808, 0.6028883457183838, 0.37276801466941833, 0.746347963809967, -0.06490268558263779, -0.1133221760392189, 0.4035777449607849, 0.13139107823371887, -0.24514172971248627, 0.10081296414136887, -0.16846638917922974, 0.6757384538650513, -0.2894924581050873, -0.9806644320487976, 0.203562930226326, 0.6392543911933899, -0.24792064726352692, 0.27880463004112244, 0.6481146216392517, 0.2194063663482666, 0.6171368956565857, 0.4600563645362854, 0.21216365694999695, -0.7056952118873596, -0.14909464120864868, -0.35518285632133484, -0.6647490859031677, -0.26677757501602173, 0.1290309876203537, -0.7035391330718994, -0.4202103316783905, -0.12567000091075897, 0.3927347958087921, -0.5316480994224548, 0.29818737506866455, 1.078847050666809, 0.5366314053535461, 0.46062976121902466, -0.5663259029388428, -0.8284321427345276, -0.5165058970451355, -1.0153238773345947, -0.2950669229030609, -0.7650854587554932, -0.18169964849948883, -0.0999559685587883, -0.24801428616046906, -0.6696372628211975]}, "authors": [{"authorId": "2152263068", "name": "Huiyin Xue"}, {"authorId": "3238627", "name": "Nikolaos Aletras"}], "references": [{"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "feeed68c23932fda1cc9f222b5b7690b2ad7e706", "title": "HashFormers: Towards Vocabulary-independent Pre-trained Transformers"}, {"paperId": "f2c17758e74707d379b87372528221656d14b697", "title": "Taxonomy of Risks posed by Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "05d70085d1b580b2369942410ae77c48d1eeacca", "title": "Exploring Extreme Parameter Compression for Pre-trained Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "4b5e4948a572bd8d5045fdb532fa1391cb0b51eb", "title": "Dynamic Knowledge Distillation for Pre-trained Language Models"}, {"paperId": "cddf40e579a596d0110b260313adf43470617c4c", "title": "Datasets: A Community Library for Natural Language Processing"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"}, {"paperId": "ce6b6046ad26e0f29932b539919c58333aedf1eb", "title": "EL-Attention: Memory Efficient Lossless Attention for Generation"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "a978fcb10817abe8bc91ab2ba0c0bb4605add1d9", "title": "Discourse Probing of Pretrained Language Models"}, {"paperId": "969287b8a96e242793b11f0dbb99ec341228106f", "title": "Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"paperId": "3af8a493cf756f9fe72623204a11e378a9cd71a5", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"}, {"paperId": "8fd0b70cfd6bbdaef9fbe1073afb3920cb61f80b", "title": "BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover\u2019s Distance"}, {"paperId": "9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0", "title": "Probing Pretrained Language Models for Lexical Semantics"}, {"paperId": "8fa19377b9cd6d2e9292522774c3a13108cd2ff5", "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "48745e3485f84cc5a2dab8e1ce41de0a38afb490", "title": "Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "389036b1366b64579725457993c1f63a4f3370ba", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "a81674f480dba239e12c80910528cae5d3a28e97", "title": "schuBERT: Optimizing Elements of BERT"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "268b4c0cbbce656a582b6903fb85e3c517bd137c", "title": "Fixed-Point Optimization of Transformer Neural Network"}, {"paperId": "91ac65431b2dc46919e1673fde67671c29446812", "title": "When BERT Plays the Lottery, All Tickets Are Winning"}, {"paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39", "title": "Training with Quantization Noise for Extreme Model Compression"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "159dc82a5ee901716b0154051988b5408acfc861", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "738215a396f6eee1709c6b521a6199769f0ce674", "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187", "title": "Quantifying the Carbon Emissions of Machine Learning"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b", "title": "Green AI"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "ec4ba9b4743b7092205d967300032191f02f7c14", "title": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "d55d1d035e91220335edff0fe8f5d249d8c4a00b", "title": "Measuring the Intrinsic Dimension of Objective Landscapes"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "677523614d4eeffdd3b34e36875b0e623810723e", "title": "Elasticity of Supply"}, {"paperId": "427a53b4471b57220bb3bee7e4bc1f0631fd6066", "title": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": "a584ee1c72619ffee435311ad11d9e02af08c28b", "title": "KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation"}, {"paperId": null, "title": "2022) and report the memory usage saving ratio (%) during the attention calculation"}, {"paperId": "b60a16d2978b10ead102a6a6cd03dc940b1194cc", "title": "Compressing Pre-trained Language Models by Matrix Decomposition"}, {"paperId": null, "title": "ELECTRA: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Underlined values denote the best performing method and bold values denote the method with best PEoP in each task"}]}