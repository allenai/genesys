{"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers", "abstract": "Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 22, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.15805", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs."}, "embedding": {"model": "specter_v2", "vector": [0.35231852531433105, 0.7638205885887146, -0.0661204531788826, -0.105667345225811, -0.3676895201206207, -0.18847855925559998, 0.40103381872177124, -0.2507617771625519, 0.07802725583314896, -0.3269246518611908, 0.6296411752700806, -0.06119221821427345, 0.4138071537017822, 0.01803787611424923, -0.06740862131118774, 0.03632394224405289, -1.1773475408554077, 0.1876160353422165, -0.15007907152175903, -0.2778933048248291, -0.14925990998744965, -0.5592954158782959, -1.2344088554382324, 0.18136820197105408, 0.3473722040653229, 0.8405970335006714, 0.18148447573184967, 0.768361508846283, -0.49395620822906494, 0.6736822724342346, 0.12455999106168747, -0.39706459641456604, 0.1640303134918213, -0.3941880464553833, -0.20755627751350403, -0.17212866246700287, 0.191132590174675, -0.22746217250823975, -0.1399410367012024, 0.4874994158744812, -0.3177293837070465, 0.2055269181728363, -0.028191782534122467, -0.6677747368812561, -0.3872438967227936, 1.3952276706695557, 0.5824291110038757, 0.764343798160553, -0.31231820583343506, -0.8459374308586121, 1.6362160444259644, -1.2616699934005737, -0.03905661776661873, 1.5975399017333984, 0.49310559034347534, 0.3000204265117645, -0.11446961760520935, -0.7420850992202759, 1.3547005653381348, 0.17663253843784332, -0.7355858683586121, -0.5519477725028992, 0.5283305048942566, 0.045507341623306274, 1.8590620756149292, -0.06596940755844116, 0.4627058207988739, 0.6347046494483948, -0.12568633258342743, 1.507294774055481, -0.266857385635376, -0.9934691190719604, 0.09912006556987762, -0.15302546322345734, 0.2114432007074356, 0.8584539294242859, -0.48049992322921753, 0.26669222116470337, -0.9458041787147522, -0.35848522186279297, 0.6168659329414368, -0.21512869000434875, 0.4404933750629425, 0.01126013696193695, 0.42384833097457886, 0.8007595539093018, 0.06973632425069809, 0.8688196539878845, -0.31913796067237854, 0.6520682573318481, 0.43239912390708923, 0.2669849693775177, 0.028224149718880653, 0.46741148829460144, -0.1385062336921692, 0.421236127614975, -0.7058477997779846, 0.1094871312379837, -0.2011418640613556, 0.835887610912323, -0.49266931414604187, 1.1740810871124268, -0.7459859848022461, 0.4690265655517578, 1.3865925073623657, -0.04727029427886009, 0.5942638516426086, -1.0392546653747559, 0.13480553030967712, -0.677668571472168, 0.09122183918952942, -0.3470194637775421, 0.20090176165103912, -0.3868834376335144, -0.5761285424232483, -1.3226158618927002, -0.6018146276473999, -0.1550396829843521, -0.8509002923965454, 0.9136184453964233, -0.44067680835723877, 0.666816771030426, -0.058361172676086426, -0.03298589959740639, 0.05695563182234764, 0.5006864070892334, 0.4435528516769409, -0.26157498359680176, 0.891464114189148, -0.6715720891952515, -0.6283212304115295, -1.3840328454971313, 0.5605200529098511, 0.08979158103466034, 0.07893700152635574, -0.21221868693828583, -1.4333772659301758, -0.7674666047096252, -0.7589152455329895, 0.02392042987048626, -0.5305073857307434, 0.22413590550422668, 1.1380181312561035, 0.6391125917434692, -1.1225286722183228, 0.39417821168899536, -0.23807965219020844, 0.4033292531967163, 0.2741587460041046, 0.1675581932067871, 0.6120420098304749, -0.08887060731649399, -1.4314109086990356, 0.018364708870649338, -0.21264755725860596, -0.03505183756351471, 0.18530002236366272, -0.8766482472419739, -1.3595755100250244, 0.15910275280475616, 0.2252758890390396, -0.4896255135536194, 1.5136276483535767, -0.021195154637098312, -1.731655240058899, 0.28513166308403015, -0.61696857213974, -0.2843126356601715, -0.25023284554481506, -0.3306429386138916, -0.4651619493961334, -0.45181605219841003, -0.18963611125946045, 0.23780985176563263, 0.46392762660980225, -0.16627265512943268, -0.5809726715087891, 0.039234459400177, -0.2916259765625, -0.32674166560173035, 0.31218141317367554, 1.1073553562164307, -0.2187872976064682, -0.5173827409744263, 0.4150925874710083, 0.6569950580596924, -0.44214603304862976, -0.8655266165733337, -0.3679663836956024, -0.9752558469772339, 0.4283806383609772, -0.308014839887619, 1.3162182569503784, -0.4472854435443878, -0.9272907376289368, -0.14934587478637695, -0.4131212830543518, -0.02183379791676998, -0.8061841726303101, 0.35619044303894043, -0.5667202472686768, 0.5638891458511353, 0.008421558886766434, -1.1166331768035889, -0.04974498599767685, -0.30172473192214966, -1.2697818279266357, -0.6462175846099854, 0.26612576842308044, 0.8444072008132935, -0.8650728464126587, 0.03815777599811554, 0.17192085087299347, 0.35114115476608276, -1.1082687377929688, 1.0475839376449585, -0.2121751308441162, 0.17648716270923615, -0.2976323962211609, 0.14170655608177185, -0.2010372132062912, 0.11404109001159668, 0.415168434381485, 0.2833594083786011, -0.1719876229763031, 0.7379046678543091, -0.2311359941959381, 0.8985639810562134, -0.8112525343894958, 0.41240543127059937, -0.40701454877853394, -0.4442470371723175, 0.2561732828617096, 0.4166754186153412, -0.15047159790992737, -0.5381128191947937, 0.08447093516588211, 0.3263894021511078, -0.5849258899688721, 0.511075496673584, 0.898459792137146, 0.9899391531944275, -0.7701890468597412, -0.3120949864387512, 0.38284197449684143, -0.2887652814388275, 0.4671400487422943, -0.00862983986735344, 0.8907991051673889, 0.37267038226127625, 0.4542559087276459, 0.37691158056259155, 0.3550117313861847, -0.9955227971076965, 0.622096836566925, 0.7551684379577637, 1.1867905855178833, 0.801123857498169, 0.6055741906166077, -0.6981702446937561, -0.1978512406349182, 0.21070821583271027, 0.8134936690330505, 1.541634202003479, -0.601492166519165, -0.2604045569896698, -0.5346422791481018, 0.026568027213215828, -0.5216079950332642, 0.5150849223136902, -0.21354112029075623, -0.07658281922340393, -0.8493821024894714, -0.6085829138755798, 1.061677098274231, 0.42227914929389954, 0.7283292412757874, -0.8041846752166748, -0.3157772421836853, 0.041609250009059906, 0.2319439947605133, -0.8371350765228271, -0.22326849400997162, 0.42629751563072205, -0.6667022705078125, 0.2153879851102829, 0.2232225090265274, -0.030476536601781845, -0.22980628907680511, -1.209408164024353, 1.0075527429580688, -0.4586699306964874, -0.2379830777645111, -0.13302527368068695, 0.5856838822364807, -0.853081464767456, -0.6991592049598694, -0.013738513924181461, -0.22669243812561035, -0.30079707503318787, 0.2680036425590515, 0.22811473906040192, 0.2696555554866791, -0.40072089433670044, 0.0765974372625351, -0.10450270771980286, -0.03799545764923096, 0.09127926081418991, 0.8935192823410034, -0.34335383772850037, -0.05697140470147133, -1.2585519552230835, 0.7206367254257202, 0.34334155917167664, -0.8110980987548828, 0.2591915428638458, -0.5452304482460022, -0.32468563318252563, 0.6521783471107483, -0.23752929270267487, -0.5695140361785889, -0.735329806804657, 0.21279862523078918, -0.4927656054496765, -0.299593061208725, 0.24854986369609833, 0.23061956465244293, 0.4909743368625641, -0.43347880244255066, 0.4940367043018341, -0.1768626719713211, -0.33194226026535034, 0.3609929084777832, -0.8549696803092957, 0.35997992753982544, 0.31759440898895264, -0.2358839064836502, -0.3603897988796234, 0.03608705848455429, -0.8160739541053772, -0.2879181206226349, -0.2978834807872772, 0.039683807641267776, -0.07251947373151779, -0.20750239491462708, -0.6677618026733398, -0.632244348526001, 0.10730292648077011, -1.1196796894073486, -0.3840115964412689, -0.13048063218593597, -0.33735325932502747, -0.14612740278244019, -1.0951305627822876, -1.2370598316192627, -1.1089918613433838, -0.5089672207832336, -0.7527878880500793, 0.6733840703964233, -0.014038066379725933, -0.37777549028396606, -0.5715391039848328, 0.06176605075597763, -0.35864847898483276, 0.699515163898468, -0.5717137455940247, 1.0550918579101562, -0.08049886673688889, -0.4919404685497284, -0.3839249014854431, 0.8043683767318726, 0.3312557637691498, -0.3175649344921112, 0.30937907099723816, -1.006966471672058, 0.6372842192649841, -0.29136890172958374, 0.0922837108373642, -0.07349594682455063, 0.6490263938903809, 0.8355513215065002, -0.20201465487480164, -0.4212895631790161, 0.19079695641994476, 1.2206803560256958, -0.1683845818042755, 0.35701656341552734, -0.05610981956124306, 0.7873803377151489, 0.012653129175305367, 0.02429237775504589, 0.6946914196014404, 0.15202923119068146, 0.5137373805046082, 0.20736564695835114, 0.03297223895788193, 0.1495410054922104, -0.6475993990898132, 0.5275736451148987, 1.7383192777633667, 0.0794449970126152, -0.2331005483865738, -0.7816990613937378, 0.7709682583808899, -1.1375701427459717, -0.7521495223045349, 0.6654665470123291, 0.928964376449585, 0.3113968074321747, -0.5141708254814148, -0.6112753748893738, 0.025097912177443504, 0.5188553333282471, 0.46997272968292236, -0.21825473010540009, -1.1869909763336182, 0.18800514936447144, 0.6762502193450928, -0.049113091081380844, 0.7220322489738464, -0.2763669788837433, 0.7676133513450623, 14.855470657348633, 0.4375632703304291, 0.1795985996723175, 0.10596680641174316, 0.9117485284805298, 0.1930236965417862, -0.33568477630615234, 0.007372644729912281, -1.4448087215423584, -0.003712451783940196, 1.2558491230010986, 0.17635555565357208, 0.5832962393760681, 0.36579596996307373, 0.4495416581630707, 0.3081096410751343, -0.5318922996520996, 0.4323906898498535, 0.37021732330322266, -1.0835762023925781, 0.5589758157730103, 0.21245373785495758, 0.19795960187911987, 0.24087302386760712, 0.6173691153526306, 1.11388099193573, 0.7698472142219543, -0.4088858664035797, 0.4138430655002594, 0.3456293046474457, 0.49560898542404175, -0.27206137776374817, 0.3726258873939514, 0.16560155153274536, -1.1500911712646484, -0.419359415769577, -0.5486976504325867, -1.2592222690582275, 0.329509437084198, 0.21795518696308136, -0.5730209946632385, -0.5895591974258423, -0.1740216761827469, 0.4051889479160309, -0.13545191287994385, 0.4746325612068176, -0.4972010552883148, 1.005250334739685, -0.15559622645378113, 0.16624264419078827, 0.17332687973976135, 0.12071128934621811, 0.42190811038017273, -0.143605574965477, 0.4786655902862549, 0.1686430126428604, -0.11660159379243851, 0.3528485894203186, -0.48056477308273315, -0.24859502911567688, -0.5641745328903198, -0.5296579003334045, 0.4525582194328308, 0.5627256035804749, 0.5211365222930908, 0.26935234665870667, -0.05077045038342476, 0.022896014153957367, 0.575545072555542, 0.3005823791027069, -0.40629565715789795, -0.15114718675613403, 0.5926250219345093, -0.6613137125968933, 0.40737220644950867, 0.7742623686790466, -0.2559907138347626, -0.3902932107448578, -0.6705864667892456, -0.40861964225769043, 0.5509476065635681, -1.128114938735962, -0.42961424589157104, 0.8571391105651855, -0.16696377098560333, -0.20310233533382416, -0.6297469139099121, -0.5050913691520691, -0.39029598236083984, 0.4137592911720276, -1.1148611307144165, -0.7788288593292236, 0.5435984134674072, -0.5248217582702637, 0.12245795130729675, -0.004227022640407085, 1.3976799249649048, -0.2456800788640976, -0.6522697806358337, 0.11103878915309906, -0.1482006460428238, -0.028356371447443962, -0.718438446521759, -0.944303572177887, 0.964336633682251, 0.7287437319755554, 0.16011540591716766, 0.5512408018112183, 0.06691833585500717, 0.1207156777381897, -0.4185575842857361, -0.039824310690164566, 1.162792682647705, -0.6797223091125488, -0.30154454708099365, -0.9321050047874451, -0.6953622102737427, 0.8133398294448853, 0.3930031359195709, -0.24855460226535797, 0.26265424489974976, 0.4662958085536957, -0.26130935549736023, -0.19915588200092316, -0.5281740427017212, 0.29055753350257874, 0.6404935717582703, -0.6439763307571411, -0.32422375679016113, -0.5489462614059448, 0.507379949092865, -1.022454023361206, -0.4240335524082184, -0.22703683376312256, -0.0006066065980121493, -0.22673551738262177, 1.0404797792434692, -0.48989927768707275, 0.6478006839752197, 1.1182959079742432, 0.11732234805822372, -0.864190399646759, -0.3497500419616699, -1.042830467224121, -0.03134414181113243, 0.6430566310882568, 0.6974108219146729, -0.4770420491695404, 0.1282944232225418, 0.9877244234085083, -0.037061676383018494, -0.3577117323875427, -0.8724464178085327, -0.14110992848873138, -0.05759327858686447, -0.7159010171890259, 0.7570284008979797, -0.2914793789386749, -0.14780329167842865, 0.33343783020973206, 0.32398295402526855, 0.9138404130935669, -0.5192415118217468, -0.4160597622394562, 0.018456310033798218, 0.0062315259128808975, -0.029837550595402718, -0.46356016397476196, -0.5934680700302124, -1.2198548316955566, 0.20561929047107697, -0.5572295188903809, 0.15752138197422028, -0.7979371547698975, -1.0424470901489258, -0.10559585690498352, -0.3245125710964203, 0.2556988596916199, 0.4124239981174469, -0.46929997205734253, -0.5255454182624817, -0.8792897462844849, -0.304714173078537, 0.6623300313949585, 0.15704353153705597, -0.559257984161377, -0.1340292990207672, 0.32084548473358154, 0.024991940706968307, 0.4032652974128723, 0.4225626587867737, -0.404746949672699, -0.8941584825515747, -1.3330596685409546, 0.3445158898830414, 0.07359147071838379, -0.05428527668118477, -0.524109423160553, 0.40198832750320435, 0.3591049909591675, -0.0820750743150711, 0.0006463039317168295, 0.5010913014411926, -0.7521602511405945, -0.190365731716156, -0.1017383337020874, -0.8441540002822876, 0.3364507853984833, 0.2210293710231781, -0.4485757350921631, -0.480558842420578, 0.4345599412918091, -0.2949375510215759, -1.0470558404922485, -0.9649753570556641, 0.29510265588760376, -0.8669381737709045, 0.18289631605148315, -0.5812714695930481, -0.18546761572360992, -0.7969130277633667, -0.37289193272590637, 0.10733719915151596, 0.1456739604473114, -0.6764976978302002, 1.1913455724716187, 0.4621216654777527, -0.8128271698951721, 0.32181525230407715, 0.4110255837440491, -0.361496239900589, 0.0547855943441391, 0.21597953140735626, 0.12621836364269257, 0.048655081540346146, 0.5987153053283691, 0.45635122060775757, 0.4198615550994873, -1.012157917022705, -0.37748101353645325, 0.7201300859451294, -0.8191751837730408, -0.16717618703842163, 1.059009313583374, -0.28600212931632996, -0.6013935804367065, 0.09454459697008133, -1.7554515600204468, -0.6960776448249817, -0.36225706338882446, 0.509900689125061, 0.32236722111701965, 0.01927386038005352, -0.07397427409887314, -0.35800766944885254, -0.08256201446056366, -0.050675854086875916, -0.4328272044658661, 0.5920981168746948, -0.2665961980819702, -0.30509376525878906, 0.7493271827697754, 0.23978662490844727, -0.3270338773727417, -0.5458739399909973, -0.38429102301597595, -0.259075790643692, -0.05186089873313904, 0.5118376612663269, -0.2748440206050873, -0.6873258352279663, 0.6779801845550537, 0.3865746259689331, 0.417344868183136, 0.0016004974022507668, -0.31384730339050293, -0.04398017376661301, 0.6436553597450256, 0.23596400022506714, -0.7665182948112488, -0.7413893938064575, 1.4016029834747314, 1.3470453023910522, -0.454018235206604, 0.22260676324367523, -0.3875914216041565, -0.874944269657135, 0.5476316213607788, 0.21371302008628845, -0.08665135502815247, 0.6992594003677368, -0.0477323979139328, 0.22605417668819427, 0.21166303753852844, -1.3173651695251465, -0.40245139598846436, 0.37194520235061646, 1.317838430404663, 0.8248274922370911, 0.14630287885665894, 0.3950355648994446, 1.2530643939971924, -0.17994676530361176, 0.20149406790733337, 0.6239377856254578, 0.5239742398262024, 0.13458594679832458, -0.15373890101909637, 0.08007891476154327, 0.7290360331535339, -0.9083064794540405, -0.9944550395011902, -0.028178686276078224, 0.6978468298912048, 0.10913289338350296, 0.791574239730835, 0.9750449061393738, 0.5525316596031189, 0.5681787133216858, 0.4734620451927185, 0.37564170360565186, -0.49531877040863037, -0.32031702995300293, 0.14986661076545715, -0.2928289473056793, -0.2511262893676758, 0.012168259359896183, -0.5818366408348083, -0.23298406600952148, 0.353903204202652, 0.22398099303245544, 0.16634660959243774, 0.20394501090049744, 0.9853664636611938, 0.7645368576049805, 0.5648969411849976, -0.18809814751148224, -0.3955298662185669, -0.28512826561927795, -0.7129465341567993, -0.2285764515399933, -0.7603898048400879, -0.2701586186885834, -0.07365262508392334, 0.023684890940785408, -0.16649645566940308]}, "authors": [{"authorId": "2051417741", "name": "Sotiris Anagnostidis"}, {"authorId": "41018093", "name": "Dario Pavllo"}, {"authorId": "2008318441", "name": "Luca Biggio"}, {"authorId": "1781789550", "name": "Lorenzo Noci"}, {"authorId": "40401747", "name": "Aur\u00e9lien Lucchi"}, {"authorId": "143936663", "name": "Thomas Hofmann"}], "references": [{"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f", "title": "Token Merging: Your ViT But Faster"}, {"paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3", "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "5eeb80dc67590422db64ca95ec0aded24799cfb6", "title": "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0", "title": "A Fast Post-Training Pruning Framework for Transformers"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "title": "Do Long-Range Language Models Actually Use Long-Range Context?"}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences"}, {"paperId": "c156b1b30e3dd9284615e5304f2fb2826c09d0ff", "title": "Learned Token Pruning for Transformers"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5", "title": "Hopfield Networks is All You Need"}, {"paperId": "3836ccb33191799e748e8e96f85a813eaf650ff8", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "09e69bf0926e55cd277a3ef5b1450ba083719cb9", "title": "Sparse and Continuous Attention Mechanisms"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "1686203adc5f2dbc18627ce64f66d33eb81432a5", "title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "3cee801d10f410f0feb1a2390776a01ba2765001", "title": "Sparse Sequence-to-Sequence Models"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "642c1b4a9da95ea4239708afc5929a5007a1870d", "title": "Tensor2Tensor for Neural Machine Translation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning"}, {"paperId": "b20c0758a38bd5a4083f64eff53af924499a8e29", "title": "Possible generalization of Boltzmann-Gibbs statistics"}, {"paperId": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "title": "A Maximum Likelihood Approach to Continuous Speech Recognition"}, {"paperId": "8b16dc5b4c0728147eef1647a6ab7f786333b76c", "title": "Sparse Token Transformer with Attention Back Tracking"}, {"paperId": null, "title": "2022], that has exhibited very successful results for transformer encoder models out of the box, i.e. without any additional fine-tuning"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": null, "title": "Gardening is the practice of growing and cultivating plants as part of horticulture"}, {"paperId": null, "title": "medicinal or cosmetic use"}, {"paperId": null, "title": "our results"}, {"paperId": null, "title": ", or overall appearance ; useful plants , such as root vegetables , leaf vegetables , fruits , and herbs , are grown for consumption , for use as dyes, or for"}, {"paperId": null, "title": "GPT-2-small GPT-2-medium GPT-2-large GPT-2-xl Dense"}]}