{"paperId": "98850975e574e08695a9f32b4c8747dc7f8bcc17", "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam", "abstract": "1-bit gradient compression and local steps are two representative techniques that enable drastic communication reduction in distributed SGD. Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose 0/1 Adam that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation. 0/1 Adam performs an Adam-like step to preserve the adaptivity, while its linearity allows utilizing 1-bit compression and local steps simultaneously for wall-clock time speed up. We provide convergence guarantee for 0/1 Adam on smooth non-convex objectives. On various large-scale benchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we demonstrate on up to 128 GPUs that 0/1 Adam is able to reduce up to 87% of data volume, 54% of communication rounds, and achieve up to 2$\\times$ higher training throughput and end-to-end training time reduction compared to the state-of-the-art baseline 1-bit Adam; while enjoying the same statistical convergence speed and end task model accuracy on GLUE dataset and ImageNet validation set.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 14, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied, so 0/1 Adam is proposed that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation."}, "embedding": {"model": "specter_v2", "vector": [0.003960791509598494, 0.7833396792411804, -0.21813000738620758, -0.1350414901971817, -0.15069229900836945, 0.4156036078929901, 0.5609138011932373, -0.22005701065063477, -1.0429376363754272, -0.11023426055908203, 0.14412660896778107, 0.02757749892771244, 0.6538485884666443, 0.0580233670771122, -0.5914183855056763, -0.3373474180698395, -1.5943540334701538, 0.6589816808700562, 0.08557749539613724, -0.2110016942024231, -0.3329698443412781, -0.6484149694442749, -0.41283082962036133, -0.12320221215486526, 0.3773501217365265, 0.9928315877914429, -0.23956230282783508, 1.4037597179412842, -0.08413629233837128, 0.40254026651382446, 0.09714222699403763, -0.25615742802619934, 0.5361242294311523, -0.25027740001678467, -0.5153737664222717, -0.12935924530029297, 0.14997778832912445, -0.794636607170105, -0.36850684881210327, 0.6944772005081177, 0.2652437090873718, 0.4932313561439514, 0.3610692322254181, -0.38487154245376587, 0.3027551770210266, 0.2075139433145523, 0.11457951366901398, 0.6998159289360046, -0.7611422538757324, -0.21171291172504425, 0.8569086790084839, -1.177316427230835, 0.11292371153831482, 1.0005345344543457, 0.7304807305335999, 0.016846513375639915, -0.33307433128356934, 0.09532547742128372, 0.792924702167511, 0.016028372570872307, -0.5397778153419495, -0.15454450249671936, -0.14152780175209045, -0.0006400046404451132, 1.6712571382522583, -0.3456248342990875, 0.05414624884724617, 0.5318190455436707, 0.24938464164733887, 1.1430495977401733, -0.11504959315061569, -0.4253173768520355, 0.227069690823555, -0.1179109439253807, 0.27568408846855164, 0.6155461072921753, -0.12010832130908966, 0.3635462820529938, -1.010303020477295, -0.20580820739269257, 0.2502609193325043, -0.032539211213588715, 0.16798578202724457, -0.10646061599254608, -0.29257041215896606, 0.5979804992675781, 0.3189939558506012, -0.22874858975410461, -0.3693845570087433, 1.3650184869766235, 0.6902480125427246, 0.6108385324478149, 0.8345338106155396, 0.03155861794948578, -0.16117461025714874, -0.27447429299354553, -0.5546462535858154, 0.03278757631778717, 0.3585757613182068, 0.6996012926101685, -0.2238929569721222, 0.44182029366493225, -0.24740010499954224, 0.4083203971385956, 1.0305291414260864, 0.06282805651426315, 0.5607027411460876, -0.24733014404773712, 0.5424222946166992, -0.8663240075111389, -0.0156114362180233, -0.5299461483955383, -0.30283108353614807, -0.4439868927001953, -1.3741295337677002, -0.46837687492370605, -1.0011422634124756, -0.4218296706676483, -0.7825478315353394, 0.3710334300994873, 0.3827519416809082, 0.6580812335014343, 0.2531317472457886, 1.1680694818496704, 0.5321663022041321, 0.9135870933532715, -0.07522518932819366, 0.33588728308677673, 0.5403369665145874, -1.344180703163147, -0.042007897049188614, -0.9943571090698242, 0.5128181576728821, -0.16084791719913483, -0.04430488869547844, -0.15172086656093597, -1.6096550226211548, -0.9447796940803528, -0.9569486975669861, 0.3530544936656952, -0.33160048723220825, 0.21786600351333618, 0.998684823513031, 0.18503864109516144, -0.7706879377365112, 0.9903619289398193, -1.0832630395889282, 0.08932793140411377, 0.4282339811325073, 0.6271263957023621, 0.3202224671840668, 0.12119264155626297, -0.8691121935844421, 0.2694430649280548, 0.25440120697021484, -0.24087099730968475, -0.4588870406150818, -0.9201478958129883, -0.5740875601768494, -0.22489914298057556, 0.06383774429559708, -0.6162770986557007, 1.1111571788787842, -0.35254237055778503, -1.5837221145629883, 0.6386786103248596, 0.21772433817386627, -0.7865445017814636, 1.00668466091156, -0.4149857759475708, 0.062449581921100616, -0.05473845452070236, -0.8648793697357178, 0.41118013858795166, 0.8873836994171143, 0.026688437908887863, -0.0834963470697403, 0.159467414021492, -0.2026037573814392, 0.14171254634857178, -0.24374110996723175, 0.5785438418388367, -0.36830273270606995, -0.3665572702884674, 0.16277216374874115, 0.4210789203643799, -0.4293016195297241, -0.18152834475040436, -0.19761718809604645, -0.6631326675415039, 0.8537247776985168, 0.2990795969963074, 0.2281322330236435, -0.9025739431381226, -0.6334533095359802, 0.14790308475494385, 0.15768076479434967, -0.006536522880196571, -0.5170453786849976, 0.1497591733932495, -0.22198346257209778, 0.0018132133409380913, -0.38616588711738586, -1.5366618633270264, 0.04701818898320198, -0.01177250687032938, -0.7928040027618408, 0.44239264726638794, 0.48012775182724, 0.9162572026252747, -0.8063115477561951, 0.1426546275615692, -0.13567736744880676, 0.4638303816318512, -1.1752249002456665, 1.6085947751998901, -0.44852200150489807, 0.031809817999601364, 0.1667291522026062, -0.1516907960176468, 0.39708593487739563, -0.4761022925376892, 0.5546825528144836, -0.13103759288787842, 0.4009961187839508, 0.5628346800804138, -0.8692513108253479, 0.9934565424919128, -0.4162829518318176, 0.14191938936710358, 0.42971011996269226, -1.1822936534881592, 0.07212495803833008, 0.07427888363599777, 0.07118869572877884, -0.16888734698295593, 0.5552782416343689, 0.02551141567528248, -0.5726063847541809, 0.49002423882484436, 0.7194287180900574, 0.9510138630867004, -0.06092747300863266, 0.3580765128135681, 0.5727782249450684, -0.17825005948543549, 0.5139839053153992, 0.43905404210090637, 0.555239200592041, 0.5033500790596008, 0.030021974816918373, 0.00482879439368844, -0.004849666263908148, -0.7004555463790894, -0.0530233271420002, 0.5270001292228699, 0.8570022583007812, 1.0635056495666504, 0.578208327293396, -1.2781308889389038, -0.5881593227386475, 0.1045665591955185, 0.6492199897766113, 1.415075421333313, 0.13027194142341614, 0.16267603635787964, -0.6824327707290649, -0.5440933108329773, -0.049992043524980545, -0.4225076735019684, -0.4585663676261902, -0.026655832305550575, -0.9155407547950745, -2.14327073097229, 0.6381263136863708, 0.19534167647361755, 1.1698524951934814, -0.08135104924440384, -0.6621171832084656, -0.2384832203388214, 0.8276926875114441, -0.7555873394012451, -0.5554366111755371, 0.5841259360313416, -0.9492838978767395, 0.16596440970897675, -0.18159687519073486, 0.18521136045455933, 0.8992372751235962, -0.6545253992080688, 1.030625581741333, -0.11994224041700363, -0.3687748610973358, 0.29882746934890747, 0.3994913697242737, -0.688029408454895, -0.6164175271987915, 0.4785066545009613, 0.0478910394012928, 0.13271427154541016, -0.5773444175720215, 0.04942017048597336, -0.28344786167144775, -0.009071632288396358, -0.1962672919034958, 0.27240046858787537, 0.08422234654426575, 0.23476924002170563, 0.5900833606719971, -0.6224938035011292, 0.09690400958061218, -1.1997989416122437, 0.9809663891792297, -0.14483296871185303, -0.4647994935512543, -0.245642751455307, -0.9031843543052673, -0.3351532816886902, 0.7031424641609192, -1.1608507633209229, -0.11509078741073608, -0.9819448590278625, -0.10044772922992706, -0.5056855082511902, -0.14031116664409637, -0.22843393683433533, 1.0119215250015259, -0.24730484187602997, 0.33856889605522156, 0.31333616375923157, 0.676024854183197, -0.37916919589042664, 0.4042898714542389, -1.2064170837402344, 0.5393803715705872, -0.20721566677093506, 0.21351274847984314, 0.266934871673584, 0.14979341626167297, -0.6695970892906189, -0.22716408967971802, -0.14807169139385223, -0.22476078569889069, -0.5428067445755005, 0.20337815582752228, -0.5937854051589966, -0.7873934507369995, -0.12254857271909714, -0.858400821685791, -0.649162769317627, -0.08121909201145172, 0.16227005422115326, -0.5046047568321228, -0.9042636752128601, -1.3959119319915771, -0.36628037691116333, -1.2975858449935913, -0.9915938973426819, 0.39231544733047485, 0.1127043291926384, -0.364779531955719, -0.7335764765739441, -0.035532351583242416, -0.7199920415878296, 1.176192045211792, -0.435373455286026, 0.21281851828098297, -0.03175776079297066, -0.1670120507478714, -0.3414894938468933, 0.4960103929042816, 0.6280069947242737, -0.9063032269477844, 0.06534440070390701, -0.6024006605148315, -0.11417079716920853, -0.5603542923927307, -0.6796180605888367, 0.36175692081451416, 0.14673514664173126, 0.45406901836395264, -0.25155967473983765, 0.00045907896128483117, 1.0542618036270142, 1.3178246021270752, -1.0174369812011719, -0.19789153337478638, -0.1340028941631317, 0.7971043586730957, -0.4206257164478302, -0.25394508242607117, 0.9381654262542725, -0.14295102655887604, 0.017092658206820488, 0.5562542080879211, -0.8522148728370667, -0.2546791434288025, -0.23574082553386688, 0.588348925113678, 1.4387080669403076, 0.14250189065933228, 0.16587838530540466, -0.5720948576927185, 0.5319159626960754, -1.3724603652954102, -0.5383251905441284, 0.6795800924301147, 0.7783960700035095, 0.4324865937232971, -0.023334471508860588, -0.005495870485901833, -0.7757062911987305, 0.2182166427373886, 0.661263644695282, -0.8104384541511536, -0.6907368302345276, -0.1601254940032959, 0.5726302862167358, 0.5712218880653381, 0.4473660886287689, -0.21761688590049744, 0.35167333483695984, 14.6428861618042, 0.6713443398475647, -0.25768810510635376, 0.9053559899330139, 1.3666894435882568, -0.4939620792865753, 0.36796805262565613, -0.32235774397850037, -1.015382170677185, 0.24226002395153046, 1.3584632873535156, 0.2822106182575226, 0.49930936098098755, 0.7875780463218689, 0.29746368527412415, 0.020805345848202705, -0.6263527870178223, 0.7678846716880798, 0.49089691042900085, -1.5547255277633667, -0.012422993779182434, 0.08266149461269379, 0.949337363243103, 1.0806885957717896, 0.7314440608024597, 0.44325393438339233, 0.8299069404602051, -0.27043798565864563, 0.44874730706214905, 0.24013474583625793, 1.153433918952942, -0.16280195116996765, 0.8751097321510315, 0.36412209272384644, -0.8106523156166077, 0.401732474565506, -0.6746223568916321, -1.2457518577575684, 0.44902700185775757, 0.5160606503486633, -0.7295872569084167, -0.23643440008163452, -0.1728048026561737, 0.6080400347709656, 0.6200577616691589, 0.39824607968330383, -0.1423284411430359, 0.9497115612030029, -0.6553540229797363, 0.050685446709394455, 0.2886751890182495, -0.235197052359581, 0.016681203618645668, 0.1163068488240242, -0.20991280674934387, 0.029443124309182167, 0.33641478419303894, 0.2918831706047058, -0.8665589690208435, -0.24477285146713257, 0.2060454934835434, 0.16843578219413757, -0.49462413787841797, 0.6677090525627136, 0.612991452217102, -0.3286917805671692, -0.5150789022445679, 0.6561710834503174, 0.8278303742408752, -0.0366596095263958, -0.5366408228874207, 0.08591367304325104, 0.8774354457855225, -0.6394573450088501, -0.03507330268621445, 0.19530588388442993, -0.4279775321483612, -0.5868020057678223, -0.42027318477630615, -0.38786938786506653, 0.37503328919410706, -0.30996617674827576, -0.7643573880195618, 0.6534672379493713, -0.2684301435947418, -0.28083574771881104, 0.4517396092414856, -0.5197510123252869, -0.5886841416358948, 0.3937419354915619, -1.4041707515716553, -0.09574534744024277, 0.5146952867507935, -0.5412760376930237, -0.20287594199180603, 0.2072252780199051, 0.8314583897590637, 0.19405092298984528, -0.6415395736694336, 0.23919066786766052, 0.4492635726928711, 0.04456877335906029, -0.08850137144327164, -0.5620880722999573, 1.273803472518921, 0.43972310423851013, -0.31162014603614807, -0.10515062510967255, -0.27805647253990173, 0.37301841378211975, -1.05779230594635, -0.1931314766407013, -0.009814199060201645, -0.5480275750160217, -0.1037183403968811, -0.5500194430351257, -0.5880711078643799, 0.24407660961151123, 0.5852692127227783, 0.1738135665655136, 0.12389279156923294, -0.15590573847293854, -0.19698278605937958, -0.773562490940094, -0.5304152369499207, -0.1297895312309265, 0.5050129890441895, -0.09398408234119415, -0.4092077910900116, 0.3203880190849304, 0.25566622614860535, -1.3079270124435425, -0.9308086037635803, -0.1634211242198944, -0.2718288004398346, 0.0715768113732338, 1.278914213180542, -0.568227231502533, 0.8378497362136841, 0.9661950469017029, 0.3477863073348999, -0.8684075474739075, 0.700093150138855, -1.3064969778060913, -0.2840504050254822, -0.16292445361614227, 0.4093734622001648, -0.6803141832351685, 0.40549325942993164, 0.8734049201011658, 0.3123587369918823, -0.8925387263298035, -0.9391582012176514, -0.15397055447101593, -0.019961252808570862, -0.3372592628002167, 0.2744021415710449, -0.2397143691778183, -0.09125759452581406, -0.160590261220932, -0.021111182868480682, 0.6856335997581482, 0.13512299954891205, -0.4802960753440857, 0.9601635336875916, 0.28949815034866333, -0.3385516107082367, -0.5082653164863586, -0.5590039491653442, -2.0544614791870117, -0.37621718645095825, -1.0283132791519165, -0.21967633068561554, -0.5341335535049438, -0.5546514987945557, -0.04901633411645889, -0.04143392667174339, -0.16841904819011688, 0.2906668484210968, -0.042714688926935196, -0.24257725477218628, -0.2956692576408386, -0.8209264874458313, 1.2714684009552002, 0.6011350750923157, -0.5953744649887085, 0.159902885556221, 0.1616169959306717, 0.5563288331031799, 0.3529015779495239, 0.6709344983100891, -0.5287930369377136, -0.5596666932106018, -1.509865403175354, 0.37071913480758667, -0.48632362484931946, 0.08251363039016724, -0.9298150539398193, 0.5725285410881042, 0.43898525834083557, 0.12370806187391281, 0.17839005589485168, 0.5690310597419739, -0.802577555179596, -0.6480314135551453, 0.5455025434494019, -0.5840157270431519, 0.2784484922885895, 0.03427336364984512, -0.365940660238266, -0.07417016476392746, 0.8191895484924316, 0.371894896030426, -0.836188018321991, -0.34874415397644043, 0.5325010418891907, -0.3626139461994171, 0.5490375757217407, -0.2824876308441162, 0.09566053748130798, -1.2046215534210205, -0.12601937353610992, -0.3151073455810547, 0.40474700927734375, -0.33528000116348267, 0.27863404154777527, 0.3187905251979828, -0.8597257137298584, -0.02988487295806408, 0.42254117131233215, -0.735079288482666, 0.6082462072372437, 0.5971875786781311, 0.9314337968826294, -0.7882466912269592, -0.23828095197677612, 0.5370411276817322, 0.11690129339694977, -0.6304718852043152, -0.3004968464374542, 1.0247650146484375, -0.6983396410942078, -0.6840717196464539, 1.2139451503753662, -0.6020575761795044, -1.042959213256836, 0.3675810396671295, -1.024335265159607, 0.22549918293952942, -0.5627435445785522, 0.5216597318649292, 0.6556890606880188, 0.1670158952474594, 0.34893178939819336, -0.6401063203811646, 0.2527804374694824, -0.22499525547027588, -0.4308682084083557, 0.8415494561195374, -0.2868887484073639, -0.3606261610984802, 0.3826349973678589, 0.99098801612854, -0.9673756957054138, -1.2520933151245117, -0.6704061031341553, -0.9156287908554077, -0.14147214591503143, 0.3190078139305115, -0.16135437786579132, -0.8951846361160278, 0.8225492835044861, 0.3573322594165802, 0.024030406028032303, 0.12472155690193176, 0.018906723707914352, 0.4788026511669159, 0.6899121999740601, 0.08896201848983765, -0.9601868987083435, -0.643864631652832, 1.0416864156723022, 1.118175983428955, -1.0926902294158936, 0.9036908149719238, 0.06716644018888474, -0.7110750675201416, 0.8517709970474243, 0.36542776226997375, -0.49921053647994995, 0.9754897952079773, -0.020507069304585457, -0.029523542150855064, 0.21703486144542694, -1.2136260271072388, -0.046806659549474716, 0.4405113160610199, 0.4694879353046417, 0.33273017406463623, 0.29068097472190857, -0.12762224674224854, 0.7406191229820251, 0.024890761822462082, -0.5605384707450867, 0.26733770966529846, 0.4743909239768982, 0.2290901392698288, 0.35326382517814636, -0.14451059699058533, 0.7514106631278992, -0.7241312265396118, -0.41789510846138, 0.2649243175983429, 0.42164504528045654, -0.10655482858419418, 0.3591682016849518, 1.0784105062484741, 0.2013421654701233, 0.2463303506374359, -0.4783054292201996, 0.06045904383063316, -0.07360386103391647, -0.5538214445114136, -0.27220696210861206, -0.4525616765022278, -0.4958714246749878, 0.08966114372015, -0.1950370967388153, -0.5938482284545898, -0.9852790832519531, 0.7565738558769226, 0.04503876343369484, 0.3821931481361389, 0.9061595797538757, 0.9373904466629028, 0.6562805771827698, -0.07164661586284637, -1.0848602056503296, -0.8991637229919434, -0.9192657470703125, 0.16479821503162384, -0.17975415289402008, -0.2783626914024353, 0.25154033303260803, -0.294966459274292, 0.06550490111112595]}, "authors": [{"authorId": "1454006122", "name": "Yucheng Lu"}, {"authorId": "2609325", "name": "Conglong Li"}, {"authorId": "2112111675", "name": "Minjia Zhang"}, {"authorId": "1801197", "name": "Christopher De Sa"}, {"authorId": "2145020341", "name": "Yuxiong He"}], "references": [{"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "fc52a514c99a1d1c596df98aaa6e5961e1c878e3", "title": "Federated Learning via Plurality Vote"}, {"paperId": "d71af4de02081831f89cb0ffefdd3d8311cd4675", "title": "Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD"}, {"paperId": "5ba6e84ace3a9ba1ab6a212997bf771b81f62810", "title": "On the Convergence of Decentralized Adaptive Gradient Methods"}, {"paperId": "7327b236d28954a33515366ddd01885b1a6c09b7", "title": "DP-SIGNSGD: When Efficiency Meets Privacy and Robustness"}, {"paperId": "5d21acef02a2035d0c5e3b8f70d4d3c9a164855f", "title": "1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB\u2019s Convergence Speed"}, {"paperId": "4066d78b637c2b8e57de5ffd53950134a551de85", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"}, {"paperId": "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4", "title": "Recent Theoretical Advances in Non-Convex Optimization"}, {"paperId": "1e04ca1998c04040c9c10685fc0daa4ecc13855b", "title": "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients"}, {"paperId": "d118ef4117208b0d0b1e54c37e9cd19a64a2a9ce", "title": "Optimal Complexity in Decentralized Training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0227e5071528d47e9b2bbe654ce48da66f20db2f", "title": "MixML: A Unified Analysis of Weakly Consistent Parallel Learning"}, {"paperId": "59600463ca4f6b5d02900022b29b6de278154c6b", "title": "A new regret analysis for Adam-type algorithms"}, {"paperId": "05b4436d504d5615801639a120a2c8eca7cbaabd", "title": "A Simple Convergence Proof of Adam and Adagrad"}, {"paperId": "73e728ef0905e48233270e5e151d2e00c7637801", "title": "Moniqua: Modulo Quantized Communication in Decentralized SGD"}, {"paperId": "37abd9e6f679ac8bc3702c5ff897522860151e5f", "title": "Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "a24d4170ad15c943805639fb5470f885d6338173", "title": "signADAM++: Learning Confidences for Deep Neural Networks"}, {"paperId": "95d87c3eece7e3f48a8377cdbe2a0d357e8a5c8b", "title": "ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization"}, {"paperId": "dd0d962e46bddab07f7d66648653afe68743909d", "title": "Election Coding for Distributed Learning: Protecting SignSGD against Byzantine Attacks"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "6105fc6d1058cd883037ed89f42332c56eef8160", "title": "Decentralized Deep Learning with Arbitrary Communication Compression"}, {"paperId": "8000a4a63ac97ffb84917f910e2ce747e48d409f", "title": "Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations"}, {"paperId": "edb19d0c5a422ee890f3799d217d9dc456cf4769", "title": "Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms"}, {"paperId": "a14ecc04274f4ef67ed5943aa53fde1bae7b95d0", "title": "Stochastic Sign Descent Methods: New Algorithms and Better Theory"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "b4cc26d67c0643257bc578f048a0c66e9a4b2de0", "title": "Convergence Analyses of Online ADAM Algorithm in Convex Setting and Two-Layer ReLU Neural Network"}, {"paperId": "596f0411d1bd2db054d9ba1a4521ad37ffbeba35", "title": "DoubleSqueeze: Parallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression"}, {"paperId": "1e13454c3388ccc2ae44ff307979256b09bed1bf", "title": "On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization"}, {"paperId": "628e35c16b4aafdb166ba9f12222be7631044f12", "title": "Zeno++: Robust Fully Asynchronous SGD"}, {"paperId": "03af562fb8e69677865dbe94910e464443dd4623", "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate"}, {"paperId": "7c22a6a07e89461178b794681c675b209332ee15", "title": "Error Feedback Fixes SignSGD and other Gradient Compression Schemes"}, {"paperId": "27f197e0401b854d14a41829e09209e38fe920b6", "title": "A Sufficient Condition for Convergences of Adam and RMSProp"}, {"paperId": "dbca9dbe14e9933515d2005dc1163ae2c24d9afd", "title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant"}, {"paperId": "9f88722cbc4107e3c3d0e1c7934cc7f1d5ae4fdb", "title": "AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods"}, {"paperId": "93ef5b740fa1b54929ead6eb177e0698d7f19719", "title": "Don't Use Large Mini-Batches, Use Local SGD"}, {"paperId": "551505614e025831a80c2d67bd854b1441eac03c", "title": "On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization"}, {"paperId": "c215b9ac79f07c8a43782b224f4416943837ffa8", "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization"}, {"paperId": "aa0672cf958973184297e87fb5800e8f87a81963", "title": "SignProx: One-bit Proximal Algorithm for Nonconvex Stochastic Optimization"}, {"paperId": "b611636f3cfe7b9aa41a606bec1d9fa72e1359ae", "title": "ATOMO: Communication-efficient Learning via Atomic Sparsification"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "7cfa76a82be96c74b2eff514265b7fd271a179cd", "title": "Local SGD Converges Fast and Communicates Little"}, {"paperId": "9c3621d816bd178586d0f8c0eb0bfb0ed0b0b83f", "title": "Nostalgic Adam: Weighting More of the Past Gradients When Designing the Adaptive Learning Rate"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "c983653841b6987d9959318f074a595783838576", "title": "On the Convergence of Adam and Beyond"}, {"paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54", "title": "signSGD: compressed optimisation for non-convex problems"}, {"paperId": "1caff87770b1cbddcf94edc0a9b1bce029324765", "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization"}, {"paperId": "3f1ab8b484f7881a68c8562ff908390742e4ba90", "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent"}, {"paperId": "e4da4fb310df2bfa5b9aab217982723634bda4bc", "title": "Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients"}, {"paperId": "4bdb91a6e47385292ab7a18e8901a6a25f50cc6b", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning"}, {"paperId": "c9d64aaa2007b60ef7814acc895dd90f15578a20", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "c0d4ca87ff248812de726e668120534b37783104", "title": "Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "3439a127e45fb763881f03ef3ec735a1db0e0ccc", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"paperId": "36f49b05d764bf5c10428b082c2d96c13c4203b9", "title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "d931f84abfc4550c10ceb113b142c8eb3e07571e", "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training"}, {"paperId": "461ef5b0affedba425e4b42acf35ad210dad22c4", "title": "Supplementary for: Momentum Centering and Asynchronous Update for Adaptive Gradient Methods"}, {"paperId": "80b497c6a39d1d5007c968d4db8b960e6936c35b", "title": "Distributed SignSGD With Improved Accuracy and Network-Fault Tolerance"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "3ae1544865a18d8649a5c4939f9eb17165b23bea", "title": "signSGD via Zeroth-Order Oracle"}, {"paperId": null, "title": "Sadam: A variant of adam for strongly convex functions"}, {"paperId": null, "title": "Better language models and their implications"}, {"paperId": "90e2842c06b8d4a1fa3c146aabf07a63de4bdc2f", "title": "Adaptive Methods for Nonconvex Optimization"}, {"paperId": null, "title": "Torchvision 0.11.0 documentation \u2014 pytorch.org"}, {"paperId": null, "title": "Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs"}]}