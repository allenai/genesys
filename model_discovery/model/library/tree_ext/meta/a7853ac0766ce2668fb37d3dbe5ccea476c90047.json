{"paperId": "a7853ac0766ce2668fb37d3dbe5ccea476c90047", "title": "Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models", "abstract": "The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning. Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs. These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (e.g., charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions. Drawing from the multifaceted information, we present a new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities. To embed lengthy rationales containing abundant information, we employ the Mamba architecture, capable of processing sequential data with linear time complexity. We introduce a new concept of traversal of rationale that facilitates efficient embedding of rationale. Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale. Through these steps, Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models.", "venue": "arXiv.org", "year": 2024, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities, achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities."}, "embedding": {"model": "specter_v2", "vector": [-0.15528400242328644, 0.11538950353860855, -0.2658940851688385, -0.3918725848197937, -0.2803793251514435, -0.346981018781662, 0.5147332549095154, 0.4075249433517456, -0.6019744873046875, -0.4254550039768219, 0.330474853515625, -0.5367320775985718, 0.48673635721206665, 0.3289220333099365, 0.011216515675187111, 0.5882465839385986, -0.4739706218242645, 0.1294935941696167, 0.10434725880622864, -0.6967489123344421, 0.5319724082946777, -0.6497707366943359, -1.6019771099090576, 0.6138792634010315, 0.19846417009830475, 0.48349153995513916, 0.24233490228652954, 1.6478636264801025, -0.5445801019668579, 0.64719158411026, 0.21555960178375244, 0.08978366106748581, -0.7180489301681519, 0.3932764232158661, -0.5769453644752502, 0.05182972550392151, 0.7097464203834534, -0.6237155795097351, -0.4135746657848358, 0.37298086285591125, -0.20345784723758698, 0.22528758645057678, 0.6954010128974915, -0.8903665542602539, -0.40955713391304016, 0.3329339325428009, 0.3588768243789673, 0.5607026815414429, 0.16536743938922882, -0.06997871398925781, 1.3957428932189941, -1.6497355699539185, 0.25754761695861816, 1.4852148294448853, 0.11767373979091644, 0.7425954937934875, 0.17786283791065216, 0.05142321065068245, 0.8775370717048645, 0.33125144243240356, -0.8018054366111755, -0.34710630774497986, -0.46285244822502136, -0.43294090032577515, 1.6822266578674316, -0.17130476236343384, -0.19253943860530853, 0.27149471640586853, -0.27554333209991455, 1.2892882823944092, -0.21350985765457153, -1.0878463983535767, 0.0580020546913147, -0.2759276330471039, 0.48626717925071716, 1.296607494354248, -0.17848782241344452, -0.17296180129051208, -0.9219244122505188, 0.04131850227713585, 0.7126476764678955, -0.2876961827278137, -0.20494601130485535, -0.6051168441772461, -0.6338015794754028, 0.902559757232666, 0.531858503818512, 0.4577938914299011, -0.059700023382902145, 0.5996230840682983, 0.10249442607164383, 0.16189731657505035, -1.0055744647979736, 0.30068448185920715, -0.4216304421424866, 0.4377032518386841, -0.5426830649375916, 0.40683335065841675, 0.21795853972434998, 0.7513787150382996, -0.37665724754333496, -0.049594294279813766, -0.7679697275161743, -0.15783853828907013, 1.9043720960617065, 0.20653614401817322, 0.480780690908432, -0.714538037776947, 0.5177013278007507, -0.31173884868621826, 0.4146939814090729, -0.6826974153518677, -0.47293809056282043, -0.21652892231941223, -0.12645544111728668, -0.7228942513465881, -0.3146878778934479, 0.20865371823310852, -0.9654197692871094, 0.6845870018005371, 0.009482708759605885, -0.285274475812912, 0.2702130079269409, 0.5937141180038452, 1.386967420578003, 0.321316123008728, 0.6751073598861694, 0.6272327899932861, 1.0811209678649902, -0.6827819347381592, -0.1898920089006424, -1.1901036500930786, 1.2412595748901367, -0.3150750696659088, 0.8233842253684998, -0.4570533335208893, -0.9718104600906372, -1.2590361833572388, -1.0160424709320068, -0.3922969400882721, -0.6648590564727783, 0.5380927920341492, 0.8519554138183594, 0.42214521765708923, -1.5959688425064087, -0.04358379542827606, 0.26563963294029236, -0.3436935842037201, 0.08815901726484299, 0.29787683486938477, 0.41837233304977417, -0.9863941073417664, -0.7357717752456665, 0.12885789573192596, 0.15869921445846558, -0.5558419227600098, -0.33341842889785767, -0.1982434093952179, -1.7430779933929443, -0.024373909458518028, 0.25589385628700256, -0.9919337034225464, 1.6303164958953857, 0.10856398195028305, -0.831542432308197, 0.6592400670051575, -0.45416930317878723, 0.5207733511924744, 0.0035719340667128563, -0.03100392408668995, -0.6060972809791565, -0.025164397433400154, -0.3179751932621002, 1.4438575506210327, 0.8273857235908508, -0.38591742515563965, -0.39182230830192566, 0.38162943720817566, 0.23586101830005646, 0.02176504395902157, -0.0765424519777298, 0.808438241481781, -0.3623839020729065, 0.20885109901428223, 0.5028155446052551, 0.8977422118186951, 0.012614614330232143, 0.029983004555106163, -0.2091626077890396, -0.9300008416175842, 0.7277290225028992, 0.33409300446510315, 1.1247656345367432, -0.8072770237922668, -0.6636320352554321, -0.3849484622478485, 0.3311031758785248, -0.24722535908222198, -1.1578341722488403, 0.3862321674823761, -0.3170452117919922, 0.4757154881954193, -0.23759278655052185, -1.04995596408844, -0.1415235996246338, -0.4063588082790375, -0.6932767033576965, -0.4837430417537689, 0.23257096111774445, 1.274659514427185, -0.8020318150520325, -0.2794754207134247, 0.24963495135307312, 0.1273164302110672, -0.8148587346076965, 1.146602749824524, -0.9336151480674744, 0.30710628628730774, -0.34423336386680603, 0.07934074103832245, -0.196012943983078, -0.962843656539917, 0.1973952203989029, -0.5358839631080627, -0.10602231323719025, 0.021211199462413788, -0.41958749294281006, 1.5213112831115723, -0.10758274048566818, 0.5962941646575928, -0.28498104214668274, -0.10402630269527435, 0.13570870459079742, 0.337212473154068, -0.759181559085846, -0.34727805852890015, 0.32145676016807556, 0.45615023374557495, -0.5478368401527405, -0.3746853172779083, 0.8460561633110046, 1.2529425621032715, -0.5096727609634399, -0.0359237864613533, -0.0584498792886734, -0.5535358786582947, 0.5944599509239197, 0.5853253602981567, 0.8705990314483643, 0.34890878200531006, 0.5460875630378723, 0.3458116054534912, 0.845572292804718, -0.1629442274570465, -0.7925264239311218, 0.7076545357704163, 0.6969549059867859, 0.8756770491600037, 0.08783452957868576, -0.8531765341758728, -0.0079533401876688, 0.20549559593200684, 0.8856315612792969, 1.755808711051941, 0.23697419464588165, -0.02477133460342884, -0.6594732999801636, -0.3382382392883301, -0.456845760345459, 0.10350219905376434, -0.1653551608324051, 0.19154439866542816, -0.5766939520835876, -0.5918186902999878, 0.6325093507766724, 0.8056333065032959, 1.0938550233840942, -0.7453480362892151, -0.8524539470672607, -0.728486180305481, 0.07381050288677216, -0.8257418274879456, -0.519244372844696, -0.28373926877975464, -0.7657323479652405, -0.6826626062393188, 0.4149014353752136, -0.6112545132637024, 0.5908922553062439, -0.19778306782245636, 1.4685500860214233, 0.13236403465270996, -0.5502715706825256, 0.8080143928527832, 0.6972990036010742, -0.3102077543735504, -0.4609706997871399, -0.13345856964588165, -0.48830950260162354, -0.5163396596908569, 0.5508931279182434, 1.23949134349823, -0.14226889610290527, 0.07695260643959045, -0.7875787019729614, 0.38128960132598877, 0.5572705268859863, -0.1159631535410881, 0.6844358444213867, -0.9575036764144897, -0.02528654783964157, -0.6128589510917664, 0.6966308355331421, 0.08845915645360947, -0.42020484805107117, 0.3851003050804138, -0.4533933699131012, -0.16760237514972687, -0.14414948225021362, -0.5395544767379761, -0.0837237760424614, -0.6500065326690674, 0.5940818786621094, -0.1496472805738449, -0.876957893371582, 0.3758721947669983, 0.2748483717441559, -0.07996953278779984, 0.21906636655330658, 0.02638683095574379, 0.024731257930397987, 0.03302139788866043, 1.108590006828308, -0.6098362803459167, 0.7396605610847473, 0.19935786724090576, -0.149159774184227, -0.18592731654644012, 0.02017086185514927, -0.7580643892288208, -0.3377962112426758, -0.5687167048454285, -0.193484365940094, -0.3754851222038269, 0.5005271434783936, -0.6909527778625488, -1.3869513273239136, -0.3119190037250519, -1.4900157451629639, -0.1874357908964157, 0.4327157437801361, -0.33824825286865234, 0.0047317249700427055, -0.9071474671363831, -0.889312744140625, -0.14445829391479492, -0.01751497946679592, -1.0038197040557861, 0.6476287245750427, 0.2874058783054352, -0.809208333492279, -0.3928634226322174, 0.09048619866371155, -0.16261164844036102, 0.6105248332023621, -0.22504061460494995, 1.3760788440704346, 0.29490575194358826, -0.6802142858505249, -0.1485920548439026, 0.053675245493650436, -0.1329416185617447, -0.2358233779668808, 0.37480512261390686, -0.6597237586975098, 0.14426802098751068, -0.06806940585374832, -0.7346125841140747, 0.426981121301651, 0.24010764062404633, 0.5368072390556335, 0.5489652156829834, -0.3579188287258148, -0.08419323712587357, 1.6269341707229614, -0.7558377385139465, -0.203587606549263, 0.13036520779132843, 1.194926381111145, 0.613252580165863, -0.14593066275119781, 0.4482710659503937, 0.878084123134613, 0.004499873612076044, 0.8072118759155273, -0.060013700276613235, -0.35215291380882263, -0.174115851521492, 0.4390001595020294, 1.0586705207824707, 0.6954115033149719, -0.16546520590782166, -1.1939115524291992, 0.5261182188987732, -1.1666170358657837, -0.019336869940161705, 0.03027033805847168, 0.7969368696212769, 0.10976066440343857, -0.5900169014930725, -0.454604834318161, -0.30399090051651, 0.5881381034851074, 0.20697173476219177, -0.352618932723999, -0.6675642728805542, -0.009283766150474548, 0.09024914354085922, -0.13811051845550537, 0.648272693157196, -0.6402075886726379, 0.20205654203891754, 14.229681968688965, 0.47222769260406494, -0.13888481259346008, 0.2500150203704834, 0.8043065071105957, 0.35346272587776184, -0.49220752716064453, -0.23641449213027954, -1.3676331043243408, -0.7725153565406799, 1.2024188041687012, 0.8414463400840759, -0.17593204975128174, 0.1315172165632248, -0.2641823887825012, 0.010010749101638794, -0.8981434106826782, 0.9273754954338074, 0.9005141854286194, -1.1918416023254395, 0.6053140163421631, -0.1321212202310562, 0.13643144071102142, 0.12413276731967926, 1.1994677782058716, 0.9668061137199402, 0.3438761830329895, -1.051560878753662, 0.5754916071891785, 0.13108298182487488, 1.1763626337051392, 0.2200850546360016, 0.17712809145450592, 0.4177040755748749, -1.0646523237228394, -0.48712003231048584, -0.26939448714256287, -1.0495017766952515, -0.29552003741264343, -0.6586925983428955, -0.8626719117164612, -0.6027360558509827, -0.6032272577285767, 0.23375296592712402, -0.28883096575737, 0.5105830430984497, -0.44628503918647766, 0.023509129881858826, 0.42038026452064514, -0.22235335409641266, 0.2045729011297226, 1.1347568035125732, -0.05106326565146446, -0.5201097726821899, 0.0658261850476265, -0.25265437364578247, 0.3361758291721344, 0.328315794467926, -0.485866516828537, -0.0046411664225161076, -0.27602651715278625, -0.1920958012342453, -0.07844372093677521, 0.7450036406517029, 0.1968195140361786, 0.4824821650981903, -0.8254951238632202, 0.08277419954538345, 0.5646386742591858, 0.3595949411392212, -0.19448807835578918, 0.21565169095993042, 0.017558205872774124, -0.5208494663238525, 0.5889368057250977, 0.2511039078235626, -0.41778793931007385, -0.38236042857170105, -0.5871027708053589, -0.4154597222805023, 0.9015323519706726, -1.1117035150527954, -0.3672291338443756, 0.7623904347419739, -0.23906169831752777, -0.6985957026481628, -0.09857922047376633, -1.322317361831665, -0.3255309760570526, 0.27661219239234924, -1.7020857334136963, -1.02596914768219, -0.29935112595558167, -0.1707526594400406, 0.10301071405410767, 0.3050903379917145, 1.5369399785995483, -0.6279594898223877, -0.16782225668430328, -0.3904217481613159, -0.43706026673316956, -0.22033248841762543, 0.054028332233428955, -0.7243815660476685, 0.5331032872200012, 0.09020184725522995, 0.010148064233362675, 0.25096675753593445, -0.047982294112443924, -0.17384441196918488, -0.6952182054519653, 0.1481255292892456, 0.6377238631248474, -1.3835515975952148, -0.6694114208221436, -0.36963316798210144, -0.7273463606834412, 0.07682289183139801, 0.4746282398700714, -0.2622798681259155, 0.44413885474205017, -0.12297331541776657, -0.8343312740325928, 0.2773455083370209, -0.8913301825523376, 0.3182603418827057, 0.09663493186235428, -0.5023998022079468, -0.7674912214279175, -0.29154953360557556, 0.5798969268798828, -0.9066063761711121, 0.04438193887472153, -0.3346066474914551, 0.00815332680940628, 0.020512519404292107, 1.020100474357605, -0.70648193359375, 1.2039000988006592, 0.4151175022125244, -0.3548009395599365, -0.4130542576313019, 0.42635250091552734, -0.4974290430545807, -0.42412909865379333, -0.3017568588256836, 0.6846256852149963, -0.02817200869321823, -0.0008571050711907446, 1.0975712537765503, 0.4712772071361542, -0.38536712527275085, -0.23397597670555115, 0.3298581838607788, -0.006459466647356749, -0.4607156813144684, 0.5373111367225647, -0.3437325656414032, -0.35762104392051697, 0.2580016255378723, 1.0116149187088013, 1.0495774745941162, -0.41471025347709656, -0.2911558449268341, 0.5122801661491394, -0.24211928248405457, -0.23426856100559235, -0.6729025840759277, -0.4501151740550995, -1.42545485496521, -0.36368393898010254, -0.5182150602340698, 0.1641886979341507, -1.1700383424758911, -0.7363669872283936, 0.5069943070411682, -0.37772637605667114, 0.025782126933336258, 0.02727581188082695, -0.27158716320991516, -0.6413292288780212, -0.38736581802368164, -0.8656253814697266, 0.21103288233280182, 0.8000058531761169, -0.5472166538238525, 0.13111837208271027, -0.38966962695121765, -0.3650124967098236, 0.42220866680145264, 0.30011722445487976, -0.018183080479502678, -0.9663472175598145, -1.5454943180084229, 0.5672188401222229, 0.025487424805760384, 0.12696804106235504, -0.7591126561164856, 0.9653714895248413, 0.3429144322872162, 0.18324093520641327, -0.18387840688228607, 0.4351145029067993, -0.3576832711696625, -1.3015024662017822, 0.5977456569671631, -0.9138547778129578, 0.5782744288444519, 0.5810231566429138, -0.5132853388786316, -0.24882666766643524, 0.46133148670196533, -0.33496543765068054, -0.9763838052749634, -1.3647267818450928, -0.01106029748916626, -0.4985128939151764, 0.05730462446808815, -0.3893387019634247, 0.020163919776678085, -1.3846551179885864, -0.40987563133239746, 0.31367895007133484, 0.6415119767189026, -0.1740664392709732, 0.797180712223053, 1.2468715906143188, -0.7537997364997864, -0.02994743548333645, 0.40685299038887024, 0.3377821147441864, 0.1739170253276825, 0.6038516163825989, 0.18765155971050262, -0.6348731517791748, 0.6928566694259644, 0.2001018524169922, 0.04573451727628708, -1.1749048233032227, 0.07741092145442963, 0.3911498785018921, -0.2036382406949997, 0.20040279626846313, 1.327215313911438, -0.09510016441345215, -0.8591853976249695, 0.10138638317584991, -1.4079762697219849, -1.244606375694275, -0.6714664697647095, 0.9518021941184998, -0.19633182883262634, -0.017147855833172798, -0.05867390334606171, -0.6957058310508728, 0.6097827553749084, 0.0776909813284874, -0.7081786394119263, -0.09216039627790451, -0.26358357071876526, -0.4542653560638428, 0.46378788352012634, 0.38657909631729126, -0.5422326922416687, -0.5814828872680664, -0.19731275737285614, 0.030819658190011978, 0.016461580991744995, 0.08603618294000626, -0.4296075403690338, -0.07123751193284988, 0.9636610150337219, 0.4872708022594452, -0.09492136538028717, 0.26802191138267517, -0.19973132014274597, -0.007191997487097979, 1.058585524559021, -0.339595764875412, 0.0438648983836174, 0.06911752372980118, 1.0591601133346558, 1.40883207321167, -1.1959457397460938, -0.07173094898462296, -0.27342066168785095, -0.5140518546104431, 1.4207459688186646, 0.8169360160827637, 0.3759478032588959, 0.3146473169326782, -0.4315979778766632, 0.4540388584136963, -0.2097097784280777, -1.0011444091796875, -0.21196705102920532, 1.1033164262771606, 1.1099265813827515, 0.85838383436203, 0.24701374769210815, 0.483047753572464, 0.4583805203437805, 0.2803097367286682, 0.15504367649555206, 0.7404198050498962, 0.8512071967124939, -0.5296432971954346, -0.15672250092029572, -0.21871939301490784, 0.3474515974521637, 0.0003882255114149302, -0.8087831139564514, 0.21761305630207062, 0.7343155145645142, 0.0017106870654970407, 1.0919572114944458, 0.9083961844444275, 0.4620220363140106, 0.4336389899253845, 0.39940720796585083, 0.7243221998214722, -1.0061718225479126, -0.13068519532680511, -0.5417648553848267, -0.6857987642288208, -0.047990817576646805, -0.33398452401161194, -0.1833677440881729, -0.9369074702262878, 0.032420214265584946, 0.8005933165550232, -0.3823118507862091, 0.6837677955627441, 1.1450896263122559, 0.5372115969657898, 0.6973639130592346, -0.7339761853218079, 0.1870357245206833, -0.529765784740448, -0.9415748119354248, 0.7596269845962524, -0.749011218547821, -0.032774459570646286, -0.44502773880958557, -0.03342784568667412, -0.10548380762338638]}, "authors": [{"authorId": "2152440079", "name": "Byung-Kwan Lee"}, {"authorId": "2284733446", "name": "Chae Won Kim"}, {"authorId": "2284809413", "name": "Beomchan Park"}, {"authorId": "2073193069", "name": "Yonghyun Ro"}], "references": [{"paperId": "4fcb8b6c466937025d315be6a83b624b10e860b4", "title": "MAmmoTH2: Scaling Instructions from the Web"}, {"paperId": "887306ee7ebd9eebd7faea24106ec8e8f1a50987", "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"}, {"paperId": "727282aad52cd4c1ebb3a0a4678d84bd1ec42dc8", "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context"}, {"paperId": "26cc508635f30328b66403dd78436c0cc1ef9bfb", "title": "OmniFusion Technical Report"}, {"paperId": "a1f23f04421cdc62e80fe9f04c1ce60f4a6af9f0", "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"}, {"paperId": "8a9e11addba791860a9dbf15de75cc28d2cf844c", "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?"}, {"paperId": "b38845e9adbeeeab37519a2fc30e899411b4a36a", "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models"}, {"paperId": "75b2ae5ee35611ecfbd3dc2c3d0799cfb4fd98e4", "title": "InternLM2 Technical Report"}, {"paperId": "6d017adda6b2b1ea627dde2f0e85401ebb9fe566", "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?"}, {"paperId": "15ef0417570a190241caac0615eabdff11abb4de", "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding"}, {"paperId": "b6648235f437c5be722db822f1f29a6a05984cd2", "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images"}, {"paperId": "6675bcf6dc97c87da7afda223938ec7e51ecc3b2", "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"}, {"paperId": "6c6f63643eb97a8c309a881e4ba21461454478fe", "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models"}, {"paperId": "b14e5138d4d1f3577b2390541dc7b730a41bb651", "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding"}, {"paperId": "c0b454e0a6aa51ff3ba56778787d0c43932ef6ba", "title": "Yi: Open Foundation Models by 01.AI"}, {"paperId": "437fbf13aa6c141c26d328921fae1dc40a2f234d", "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World"}, {"paperId": "e96c86407b3acf09e5d7729bec201824b717b476", "title": "Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset"}, {"paperId": "f6b9ccd7533b58e14d284191f1a576b0c764b3d5", "title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models"}, {"paperId": "86f6fc1f96d189a314f10680eed4e7e9807c4f5e", "title": "CoLLaVO: Crayon Large Language and Vision mOdel"}, {"paperId": "ec8e2b45c4601730015608a58e33409224a81228", "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"}, {"paperId": "8c8527f7615d53cbc21b9c3536486540f1c75000", "title": "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study"}, {"paperId": "a3ca77456142b78367dd5d53138b50dfac8086ca", "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities"}, {"paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0", "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"}, {"paperId": "3713112311efbcf785de17fa86e5bf42e4360f77", "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model"}, {"paperId": "2141ed804636a1cf339d606cd03fd3b3e9582133", "title": "VILA: On Pre-training for Visual Language Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560", "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"}, {"paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada", "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"}, {"paperId": "bf14244669d5505f63343d4365d99d24aa6c5e82", "title": "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models"}, {"paperId": "2f566575a246752d59438e2bde22f88680927af9", "title": "OtterHD: A High-Resolution Multi-modality Model"}, {"paperId": "ad13b213681b6f634bc83a264df246e83dd9a9d9", "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"}, {"paperId": "2313afae52d98e569da2dedbf14daf9efc74e7cf", "title": "CogVLM: Visual Expert for Pretrained Language Models"}, {"paperId": "819fce5234439ed94be76a0c1329e7990ad5c396", "title": "Causal Unsupervised Semantic Segmentation"}, {"paperId": "7014b6700cfb74a6f5bb408dde553b9de7d1bb90", "title": "Rationale-Enhanced Language Models are Better Continual Relation Learners"}, {"paperId": "c6038b271fe2180a7b40ed27539e869672c3156a", "title": "Mitigating Dataset Bias in Image Captioning Through Clip Confounder-Free Captioning Network"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "8946891e94831adc8cddb0d32311cce2445c96d2", "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"}, {"paperId": "c1e450284e7d6cac1855330a1197df8537df653f", "title": "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"}, {"paperId": "593b42c628b49937bcd7f7c4a7d54d5f97e6b414", "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision"}, {"paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9", "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"}, {"paperId": "94972e30504017156ef5b5debc419bf6edc67384", "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"}, {"paperId": "4309d572a37d655779f9dce6a2c98c66334132de", "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "f0da004c20245fd654484679d25c37b3a96edd05", "title": "Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning"}, {"paperId": "b37b1dc72b1882858f5120f2cd6883134089a6ed", "title": "MMBench: Is Your Multi-modal Model an All-around Player?"}, {"paperId": "e2a58fd18961c3941102989e3a3d0d27c615e015", "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"}, {"paperId": "7a6a298efb965ce9a351a3212f6f536e94dbbb03", "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step"}, {"paperId": "697e0add95e880bd42e00bef838181e105f91981", "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"}, {"paperId": "948e8cfae92c2004f2dd5c9316f5972f8baaea21", "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"}, {"paperId": "ebf3a59aacdd9982283d7f41229ee2a93800d6ef", "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "e5754bb65a648f319a02d47c356df0db1e936b7f", "title": "Post Hoc Explanations of Language Models Can Improve Language Models"}, {"paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773", "title": "Evaluating Object Hallucination in Large Vision-Language Models"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "d6d3604f369bb0415cbe814e43ca3131323b03e2", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning"}, {"paperId": "aad167be3c902388ea625da4117fcae4325b8b7d", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"paperId": "56fa65d8dc41708082f9b2ef7752c49cee9ebe01", "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891", "title": "DINOv2: Learning Robust Visual Features without Supervision"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba", "title": "Sigmoid Loss for Language Image Pre-Training"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "2810fd6761ed2532961e89efeb2a27477cc13cbc", "title": "Demystifying Causal Features on Adversarial Examples and Causal Inoculation for Robust Network by Adversarial Instrumental Variable Regression"}, {"paperId": "1358f90705b05cdb20ebe6799b02196205e7e9f0", "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "title": "Automatic Chain of Thought Prompting in Large Language Models"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "e23d0bf71547029968504de2ee1bb8888344c3b7", "title": "Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network"}, {"paperId": "430090a9feae89e7fc89dc234d886cd8ec5a3225", "title": "Distilling Robust and Non-Robust Features in Adversarial Examples by Information Bottleneck"}, {"paperId": "5f8943a8eb911e2c6976861422a35425b9de4c78", "title": "A Rationale-Centric Framework for Human-in-the-loop Machine Learning"}, {"paperId": "b611c501269224702d1a9942c8600a31ec66ab28", "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df", "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "3df5343edf4da012e39a5ed79b6ceb723f38b2bf", "title": "Kleister: Key Information Extraction Datasets Involving Long Documents with Complex Layouts"}, {"paperId": "d5611a92619548e7f2af5adb04070574c0dacac1", "title": "InfographicVQA"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "394be105b87e9bfe72c20efe6338de10604e1a11", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"}, {"paperId": "f05126c1a792ea64a7af0c8c68b03bcddec5b297", "title": "VisualMRC: Machine Reading Comprehension on Document Images"}, {"paperId": "b40bfcf339de3f0dba08fabb2b58b9368ff4c51a", "title": "DocVQA: A Dataset for VQA on Document Images"}, {"paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96", "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "46bfca498faf4964b693e3cb70e17389075168d2", "title": "Do Human Rationales Improve Machine Explanations?"}, {"paperId": "e65c84e2778d7b13b7541e6b14ff790b624a24ec", "title": "A Study of BFLOAT16 for Deep Learning Training"}, {"paperId": "0b5d7a79205b44952e24025ce5d46e9f3aa401a1", "title": "Low-Memory Neural Network Training: A Technical Report"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "7289a240c9425bc7cad87b3b835e5f0cac22f488", "title": "DVQA: Understanding Data Visualizations via Question Answering"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "a8e4580471908d17e279000d328f39654359bd6e", "title": "Beam Search Strategies for Neural Machine Translation"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "e18ec2c9f0b4a817b8cf0435822bbc879d7db698", "title": "A Diagram is Worth a Dozen Images"}, {"paperId": "b41e95c8c97846d5ca4c11ef79d7814499cc9663", "title": "Compositional Semantic Parsing on Semi-Structured Tables"}, {"paperId": "51fa7c573fcc98b05c2d15685d64463c40d57cff", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": null, "title": "\u201cLlava-next: Improved reasoning, ocr, and world knowledge,\u201d"}, {"paperId": "5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a", "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"}, {"paperId": "a37153a5f42ee2951ad8a2c9ec86b52c4bf81c77", "title": "Retrieval-Augmented Multimodal Language Modeling"}, {"paperId": "65d5728ea17f016382870aa27aac1e78d590b50c", "title": "HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models"}, {"paperId": null, "title": "\u201cTowards adversarial robustness of bayesian neural network through hierarchical variational inference,\u201d"}, {"paperId": null, "title": "\u201cDeepform: Understand structured documents at scale,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "de8ba9b01c9ab7cbabf5c33b80b7bbc618857627", "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku"}, {"paperId": null, "title": "\u201cAm-radio: Agglomerative model\u2013 reduce all domains into one,\u201d"}, {"paperId": null, "title": "\u201cXtuner: A toolkit for efficiently fine-tuning llm.\u201d"}, {"paperId": null, "title": "\u201cGemini: a family of highly capable multimodal models,\u201d"}, {"paperId": null, "title": "\u201cInternlm: A multilingual language model with progressively enhanced capabilities.\u201d"}]}