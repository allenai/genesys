{"paperId": "d6a0dfd5f39222d8924b7727a0a49f81fa247d71", "title": "Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning", "abstract": "Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 14, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2205.14794", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines."}, "embedding": {"model": "specter_v2", "vector": [0.5893833041191101, 0.32454827427864075, -0.12307983636856079, -0.06762638688087463, -0.10927954316139221, 0.21114645898342133, 0.9359187483787537, -0.014655890874564648, -0.71993088722229, -0.5999314188957214, 0.6490136384963989, 0.13966813683509827, 0.4313971996307373, -0.1689649522304535, 0.23147623240947723, -0.1423504501581192, -1.0325278043746948, -0.2261655181646347, 0.24958285689353943, -0.23694291710853577, 0.38902613520622253, -0.4576283097267151, -1.6377559900283813, 0.4938167631626129, -0.18627013266086578, 0.5996706485748291, 0.18486671149730682, 1.086617350578308, -0.3940656781196594, 0.9020939469337463, 0.7487719058990479, 0.030804362148046494, 0.47261467576026917, -0.12384913116693497, -0.5734235644340515, -0.47874727845191956, 0.2742275893688202, -0.3279525935649872, -0.6918321251869202, 0.0705132782459259, -0.2260141670703888, 0.4967535436153412, 0.38050004839897156, -0.38577449321746826, 0.5673481225967407, 0.6463876366615295, 0.547654926776886, 1.094423770904541, 0.03595962002873421, -0.8794775605201721, 1.318508267402649, -1.0931988954544067, 0.4837476909160614, 1.44736909866333, 0.6381023526191711, 0.6139715909957886, -0.3440282940864563, -0.6881914138793945, 1.355759620666504, 0.4757336378097534, -0.7420612573623657, -0.21504010260105133, -0.068641796708107, -0.23153561353683472, 1.694434642791748, -0.4277113378047943, 0.2694481611251831, 0.5515349507331848, 0.015590363182127476, 1.656614899635315, -0.02414344996213913, -0.6461852192878723, -0.017036058008670807, 0.04288792237639427, 0.513114869594574, 0.7673092484474182, -0.6981933116912842, 0.7370631098747253, -1.5848618745803833, 0.10848548263311386, 0.5329486131668091, 0.1666555106639862, -0.15071704983711243, -0.29628515243530273, -0.14970067143440247, 0.581469714641571, 0.5124694108963013, 0.5848408937454224, -0.4471210837364197, 0.7223219275474548, 0.8358373045921326, 0.6386671662330627, -0.1600048542022705, 0.03526783734560013, 0.3970872461795807, 0.08881659060716629, -0.6871215105056763, 0.3232634663581848, -0.07971359044313431, 0.824857234954834, -0.1057049036026001, 0.27177515625953674, -0.7770786881446838, 0.26345157623291016, 1.0926872491836548, -0.7052819132804871, 0.9108266234397888, -0.5500982999801636, 0.3603716492652893, -0.7864195704460144, 0.04434003308415413, -0.7254607677459717, -0.017093690112233162, -0.06036502122879028, -0.7876473069190979, -0.9292385578155518, -0.272565633058548, 0.579984188079834, -0.8364494442939758, 0.5496777892112732, -0.5479254722595215, 0.24521347880363464, 0.003030133666470647, 0.5384204983711243, -0.031607575714588165, 0.69649338722229, 0.46177393198013306, 0.029182665050029755, 0.6571327447891235, -0.4603615701198578, -0.5473384857177734, -0.6957300305366516, 0.34631139039993286, 0.04725903272628784, 0.0749705508351326, -0.1242811307311058, -1.2871403694152832, -1.5192925930023193, -0.9531007409095764, 0.08133679628372192, -0.6938430666923523, -0.2857867181301117, 1.3921773433685303, 0.1314825862646103, -0.7718698382377625, 1.527296543121338, -0.6014031767845154, -0.5628314018249512, 0.6079991459846497, 0.008545706048607826, 0.4515336751937866, -0.2828039526939392, -0.8432407379150391, 0.3396611511707306, 0.16996711492538452, -0.5247374773025513, -0.42216920852661133, -0.541812002658844, -1.0602684020996094, -0.0708160325884819, 0.08382813632488251, -0.835131049156189, 1.4351997375488281, -0.022910241037607193, -1.104118824005127, 0.31263887882232666, -0.5593473315238953, -0.01287422887980938, 0.5527303218841553, -0.2887197732925415, -0.2272980809211731, 0.11187771707773209, -0.29465252161026, 0.38566339015960693, 0.38076260685920715, -0.010704555548727512, -0.7538169026374817, -0.10970685631036758, -0.9120995998382568, -0.024467825889587402, -0.14057657122612, 0.6647108793258667, -0.25772422552108765, -0.39448708295822144, 0.6575393676757812, 0.8895763158798218, -0.14912906289100647, -0.2802639603614807, 0.5473523139953613, -1.2417834997177124, 0.3433040380477905, 0.14441385865211487, 1.0693492889404297, -0.9278438091278076, -0.448209673166275, -0.1389855146408081, 0.009841489605605602, -0.5089434385299683, -1.0159401893615723, 0.8799523115158081, -0.3751765191555023, 0.509178876876831, -0.03467641770839691, -0.9993471503257751, -0.24138768017292023, -0.2517721354961395, -1.1582484245300293, -0.07390730828046799, 0.06693635880947113, 0.8852265477180481, -1.1146799325942993, -0.07303927838802338, -0.07382278144359589, -0.0234542153775692, -0.6927220821380615, 1.4351931810379028, 0.08756304532289505, -0.34382161498069763, -0.2288629412651062, -0.2754864990711212, 0.12777481973171234, -0.09557652473449707, 0.43891623616218567, -0.40334078669548035, -0.5395773649215698, 0.689454972743988, -0.33770304918289185, 1.6258068084716797, -0.2990121841430664, 0.9554421305656433, -0.472067266702652, -0.8575676083564758, 0.026744166389107704, 0.6169145107269287, -0.020512890070676804, -0.8133333325386047, 0.12464175373315811, -0.05992390587925911, -0.9408183097839355, 0.2536761462688446, 0.6368192434310913, 0.525829017162323, -0.18860521912574768, 0.489080011844635, 0.6439148783683777, -0.3252299427986145, 0.8385627269744873, 0.7434170842170715, 0.7667703032493591, 0.7735930681228638, 0.13487282395362854, 0.43352964520454407, -0.1483859121799469, -0.9568343758583069, 0.21859417855739594, 0.9873780608177185, 0.7083210945129395, 1.036858320236206, 0.295705109834671, -1.1212846040725708, -0.6372546553611755, -0.20066842436790466, 0.6729660034179688, 1.3489140272140503, -0.2111026793718338, -0.7093914747238159, -0.07759705930948257, 0.00222264532931149, -0.30766424536705017, 0.11273398995399475, -0.6405950784683228, -0.3637191951274872, -0.7789278030395508, -0.7831531763076782, 0.5069418549537659, 0.7657637000083923, 1.0540350675582886, -0.5973921418190002, -0.8047685623168945, 0.04174182191491127, 0.35236215591430664, -0.5754169821739197, -0.3307911157608032, 0.24400851130485535, -0.9126459360122681, -0.2151060849428177, 0.0034193620085716248, 0.27568498253822327, -0.17649830877780914, -0.2545276880264282, 0.846760094165802, -0.5148195624351501, -0.6054348945617676, 0.2126169055700302, 0.45785635709762573, -0.8085623383522034, -0.1909225881099701, 0.04729331284761429, 0.004019224550575018, -0.05841509625315666, 0.3759535849094391, 0.17792223393917084, -0.36401593685150146, 0.10311822593212128, -0.41413769125938416, 0.5547183156013489, 0.2674282193183899, -0.002332038711756468, 0.385844349861145, -0.2714172303676605, -0.08701533079147339, -0.9419422745704651, 0.7175959348678589, 0.1253933608531952, -0.28836387395858765, 0.019806042313575745, -0.7415608763694763, -0.20797713100910187, 0.31674250960350037, -0.770309329032898, -0.4857805073261261, -0.9614929556846619, 0.6086366772651672, -0.46935904026031494, -0.22877269983291626, 0.22792793810367584, 1.0406241416931152, 0.22334569692611694, 0.17204433679580688, 0.514205276966095, 0.2057836651802063, -0.20710334181785583, 0.04057972878217697, -0.9122297167778015, 0.2513687014579773, 0.30231204628944397, 0.03157922625541687, 0.17865104973316193, -0.12347511947154999, -0.7933764457702637, -0.46358543634414673, -0.8994475603103638, -0.4450240433216095, -0.41210445761680603, -0.1760128140449524, -0.5473079085350037, -1.1969319581985474, -0.04524825140833855, -0.785262942314148, -0.5172399282455444, -0.15955959260463715, 0.022281311452388763, -0.5871028304100037, -1.1319611072540283, -1.2822084426879883, -0.7770147323608398, -0.0057936254888772964, -0.19617050886154175, -0.0740174800157547, -0.1302449256181717, -0.5617765188217163, -0.11400289088487625, 0.004445299506187439, -0.7585702538490295, 0.9706417322158813, -0.45845142006874084, 0.6422094106674194, 0.465030699968338, -0.564649760723114, -0.20599806308746338, 0.1698097288608551, 0.3380610942840576, -0.3988952934741974, 0.033868223428726196, -1.0170308351516724, -0.01670871675014496, -0.10314001142978668, -0.28762903809547424, 0.7692903876304626, 0.11215359717607498, 0.8195644617080688, 0.23186585307121277, -0.5761368274688721, 0.36975404620170593, 1.4696153402328491, -0.19205133616924286, 0.16369545459747314, -0.074544258415699, 0.7876101732254028, 0.6785528063774109, -0.02502269670367241, 0.5636142492294312, 0.029954027384519577, 0.14562664926052094, -0.29236558079719543, -0.211467906832695, -0.22089223563671112, -0.8292633295059204, 0.22313351929187775, 1.4087793827056885, -0.2978983223438263, 0.5726051926612854, -0.9470872282981873, 0.5470582246780396, -1.5284764766693115, -0.9873595833778381, 1.1379990577697754, 0.9282307624816895, 0.11012274026870728, -0.35898539423942566, -0.07617517560720444, 0.09150981903076172, 0.11341623216867447, 0.30365094542503357, -0.2862333059310913, -0.6029470562934875, 0.2617689371109009, 0.5290277600288391, 0.3354184627532959, 0.5239721536636353, -0.12947513163089752, 0.5782579779624939, 14.889774322509766, 0.45506271719932556, 0.010558008216321468, 0.36306262016296387, 0.31044918298721313, -0.08916415274143219, -0.07887905836105347, -0.035810939967632294, -0.9794623851776123, -0.4146634638309479, 1.084841012954712, -0.19560499489307404, 0.466743528842926, -0.28594473004341125, 0.016408449038863182, 0.02671358361840248, -1.0134577751159668, 0.7888056635856628, 0.13931921124458313, -1.258041501045227, 0.5191547870635986, -0.008311801590025425, -0.15606307983398438, 0.3561234772205353, 0.6455094814300537, 0.9020395278930664, 0.285593718290329, -0.5197924375534058, 0.5006443858146667, 0.42172548174858093, 0.7137278914451599, -0.12464046478271484, 0.06218724325299263, 0.25919055938720703, -1.4492052793502808, -0.5784608125686646, -0.6144794225692749, -1.2368500232696533, 0.12709812819957733, -0.270957350730896, -0.5185630321502686, -0.36031603813171387, -0.0012638348853215575, 1.0711860656738281, 0.2756700813770294, 0.40844398736953735, -0.04423806071281433, 0.472365140914917, 0.08456627279520035, -0.08223122358322144, 0.2588902711868286, 0.5609821677207947, 0.16859260201454163, 0.17125585675239563, -0.27368298172950745, 0.3843269348144531, -0.02345142513513565, 0.2949784994125366, -0.13504371047019958, -0.12831003963947296, -0.5143051743507385, -0.47899875044822693, -0.12376723438501358, 0.33660435676574707, 0.3227796256542206, 0.040698591619729996, -0.13564687967300415, 0.20221108198165894, 0.7181887626647949, 0.42292800545692444, 0.16122840344905853, -0.3850979208946228, 0.23262940347194672, -0.5800157189369202, 0.1916048675775528, 0.31411120295524597, -0.2575308084487915, -0.6545039415359497, -0.8537482619285583, -0.04051220044493675, 0.8152894377708435, -0.6963959336280823, -0.7530719041824341, 0.9409807920455933, 0.09722072631120682, -0.41964414715766907, 0.02121642604470253, -0.5748448967933655, -0.12802472710609436, 0.5470353364944458, -1.5092027187347412, -0.0928192138671875, -0.18658602237701416, 0.0528709851205349, 0.18800482153892517, 0.1781473457813263, 1.2633094787597656, -0.21983613073825836, -0.25182878971099854, -0.17260125279426575, -0.5488759279251099, -0.34937533736228943, -0.4272782802581787, -0.7503046989440918, 0.6551666855812073, 0.26987317204475403, -0.19685828685760498, 0.500840425491333, -0.035594530403614044, 0.1710459440946579, -0.6655887961387634, 0.16990216076374054, 0.6361299753189087, -0.33582866191864014, -0.49280765652656555, -0.7140552401542664, -1.0469416379928589, 0.14185191690921783, 0.01036540325731039, -0.4844406247138977, 0.2535759508609772, 0.25343549251556396, -0.7176956534385681, -0.32241368293762207, -0.3316735327243805, 0.3783237040042877, 0.5860733985900879, -1.003275990486145, -0.6289146542549133, -0.36965030431747437, 0.16353942453861237, -0.29755693674087524, -0.35451844334602356, -0.21397526562213898, 0.494087815284729, -0.5052990317344666, 0.960408627986908, -0.44047605991363525, 0.5090813636779785, 0.8494566082954407, 0.0027784062549471855, -0.44738510251045227, -0.39874759316444397, -1.028404951095581, 0.05298570170998573, 0.029478078708052635, -0.07924670726060867, -0.27851447463035583, 0.41858363151550293, 0.6862229704856873, 0.40993350744247437, -0.1998809278011322, -0.34640341997146606, 0.11155547201633453, -0.4801453948020935, -0.7267929315567017, -0.0296426210552454, -0.4106409251689911, -0.015545859932899475, 0.05597502738237381, 0.9023939967155457, 0.2687583267688751, -0.1624680906534195, -0.35950812697410583, 0.03797326982021332, 0.09858284890651703, 0.4897105097770691, -0.8430949449539185, -0.687343418598175, -1.4162825345993042, -0.3557977080345154, -0.8945479989051819, -0.11569289863109589, -0.7749003767967224, -0.8736811280250549, -0.020363103598356247, -0.7957732677459717, -0.13022537529468536, 0.5825797915458679, -0.41481053829193115, -0.11375098675489426, -0.4561064541339874, -0.2996516525745392, 0.6855555176734924, 0.7945228815078735, -0.5423299074172974, -0.29055339097976685, 0.17879022657871246, 0.334462434053421, 0.2856369614601135, 0.3682372272014618, -0.4639619290828705, -0.8489382266998291, -0.8233763575553894, 0.5139482021331787, 0.12343621253967285, -0.1853308230638504, -0.9508203268051147, 0.949223518371582, 0.39749065041542053, 0.22686776518821716, -0.5514376163482666, 0.5132916569709778, -0.7420102953910828, -0.6324849724769592, 0.31084170937538147, -1.19851553440094, 0.07132846862077713, 0.25968924164772034, -0.2779267728328705, -0.5056291222572327, 0.9982874393463135, 0.12187131494283676, -1.341964602470398, -0.9868351221084595, 0.588970422744751, -0.8392027616500854, -0.23904842138290405, -0.3166888654232025, -0.5022048950195312, -0.780376672744751, 0.11946213245391846, -0.05617688596248627, 0.5495816469192505, -0.6085098385810852, 0.9443602561950684, 1.165630578994751, -0.8705310225486755, 0.06685672700405121, 0.421936571598053, 0.04893189296126366, -0.05718553438782692, 0.40505215525627136, 0.12706728279590607, 0.06314443051815033, 0.4035125970840454, -0.0348845012485981, 0.312953919172287, -0.9133669137954712, 0.29532843828201294, 0.7952762246131897, 0.10689326375722885, -0.5924278497695923, 1.0582389831542969, -0.19692613184452057, -0.40158611536026, 0.5673903822898865, -1.0555315017700195, -0.8272162079811096, 0.47800835967063904, 0.9830073118209839, 0.7352577447891235, -0.26855912804603577, 0.5885501503944397, -0.20045287907123566, 0.5006645321846008, -0.46604418754577637, -0.7952315211296082, 0.5988112688064575, -0.3924529552459717, -0.14587070047855377, 1.4723008871078491, 0.9115949869155884, -1.329774260520935, -0.7618772387504578, -0.9089803099632263, -0.09617556631565094, -0.6345382332801819, 0.29541847109794617, 0.009131895378232002, -0.27161267399787903, 0.9584174156188965, 0.7445421814918518, 0.2642318606376648, 0.21629731357097626, -0.35333651304244995, -0.14534446597099304, 0.6961502432823181, 0.02030317671597004, -0.7421879172325134, -0.3599640727043152, 0.901246964931488, 1.3455157279968262, -0.37982967495918274, 0.3950903117656708, -0.2020908147096634, -0.48737436532974243, 0.9812535643577576, 0.19127410650253296, -0.339495986700058, 0.5812893509864807, -0.43551379442214966, 0.19041778147220612, 0.11744435876607895, -1.5934268236160278, -0.27874132990837097, 0.7064059972763062, 0.9518710374832153, 0.6118794679641724, 0.019750704988837242, 0.08621807396411896, 0.2792121171951294, 0.08163468539714813, 0.5796340703964233, 0.3219233453273773, 0.8361288905143738, 0.21107181906700134, 0.267627090215683, 0.4349271357059479, 0.5518054366111755, -0.6953451633453369, -0.2580612897872925, 0.7201012372970581, 0.39692655205726624, -0.2850441336631775, 0.5765962600708008, 1.091332197189331, 0.031337589025497437, 0.6471750140190125, 0.24325905740261078, 0.842574417591095, -0.4823526442050934, -0.2834426760673523, -0.06504035741090775, -0.6177735328674316, -0.5935075879096985, -0.5974019169807434, -0.8698892593383789, -0.2445121556520462, 0.019769247621297836, 0.7000210285186768, 0.2445225715637207, 0.34884974360466003, 0.697676420211792, 0.314264178276062, 0.5832799673080444, -0.03803660348057747, -0.7706086039543152, -0.36018258333206177, -0.7702911496162415, 0.0662866085767746, -0.16293317079544067, 0.2591996490955353, -0.1086832582950592, -0.31518128514289856, 0.09352176636457443]}, "authors": [{"authorId": "150010392", "name": "Aniket Didolkar"}, {"authorId": "2066789106", "name": "Kshitij Gupta"}, {"authorId": "1996705", "name": "Anirudh Goyal"}, {"authorId": "49071560", "name": "Alex Lamb"}, {"authorId": "145604319", "name": "Nan Rosemary Ke"}, {"authorId": "1865800402", "name": "Y. Bengio"}], "references": [{"paperId": "4247f45a5730e3bda5836e2bc7941e30f5b91cb7", "title": "Board"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "164e41a60120917d13fb69e183ee3c996b6c9414", "title": "Vision Transformer for Small-Size Datasets"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "b70bb1855e217edffb5dfa0632e8216860821870", "title": "Efficient Self-supervised Vision Transformers for Representation Learning"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "36b9d0f8610a82fd25854889d9327a04da4ff8fd", "title": "MST: Masked Self-Supervised Transformer for Visual Representation"}, {"paperId": "2d98048c2d2fcd3f6b989d2a54003808906ab4b7", "title": "Efficient Training of Visual Transformers with Small Datasets"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "166e98317ed9c4687e71bef55a6800431e00b8fa", "title": "SiT: Self-supervised vIsion Transformer"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "2984ab83ade26639c3a82d29628d0d9e4abbebb0", "title": "Incorporating Convolution Designs into Visual Transformers"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "78ea232dbabc67ca4d6d4a7c1bbf568e9b47cb8a", "title": "Coordination Among Neural Modules Through a Shared Global Workspace"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "1c39625ed65389cfb9d268f93f406455665f201b", "title": "Grounded Language Learning Fast and Slow"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "74e85e72392208b9a1d135dcb00fe28c61ddb9c7", "title": "Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "67a9dde04f367efc903b6d06097df9bdd9887ae7", "title": "Recurrent Independent Mechanisms"}, {"paperId": "cc4435c2c1ea079721f48d08d0ae3436599d1533", "title": "Striving for Simplicity in Off-policy Deep Reinforcement Learning"}, {"paperId": "549c9dfb32e85d9ef5a48566767be42ad132a3c4", "title": "Information asymmetry in KL-regularized RL"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "bf7f1ada5feecc0992f71b39c1ebeccb19ae631b", "title": "InfoBot: Transfer and Exploration via the Information Bottleneck"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "b29362fc22bf17848dab2f0dcb0b2ca02bab1724", "title": "BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "033dd6cf61a6017e9aa9b46068d3c89082849cf3", "title": "Tracking the World State with Recurrent Entity Networks"}, {"paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55", "title": "Using Fast Weights to Attend to the Recent Past"}, {"paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233", "title": "Hierarchical Multiscale Recurrent Neural Networks"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}, {"paperId": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "title": "A Clockwork RNN"}, {"paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17", "title": "Generating Sequences With Recurrent Neural Networks"}, {"paperId": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "title": "Speech recognition with deep recurrent neural networks"}, {"paperId": "f82e4ff4f003581330338aaae71f60316e58dd26", "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"}, {"paperId": "4a843f0c1909403201c1e6b13dfcf8515f1db78d", "title": "Working memory, long-term memory, and medial temporal lobe function."}, {"paperId": "be9a17321537d9289875fe475b71f4821457b435", "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"}, {"paperId": "7cfed21c37a2f1e142b10e7ce0594a516caba1ae", "title": "The mind and brain of short-term memory."}, {"paperId": "5536d42ce80e129be8cae172ed1b7659c769d31d", "title": "2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "title": "Bidirectional recurrent neural networks"}, {"paperId": "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"}, {"paperId": "e141d68065ce638f9fc4f006eab2f66711e89768", "title": "Induction of Multiscale Temporal Structure"}, {"paperId": "5d99174e5bad291614b26dc8ef5b4b0503c56701", "title": "Attention and retrieval from long-term memory."}, {"paperId": "4c50b9d5794dbb3893f8fbfb42803c1cd27502f2", "title": "Maintaining Organization in a Dynamic Long-Term Memory"}, {"paperId": "0466739b5e0ad0522c218c3289e912346542bf98", "title": "The control of short-term memory."}, {"paperId": null, "title": "Improving long-horizon imitation through language prediction, 2022"}, {"paperId": null, "title": "Fiedel. Palm: Scaling language modeling with pathways, 2022"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "2068b4d5c95ea5c66c8a81e73337fc52466b8b18", "title": "Reinforcement Learning as One Big Sequence Modeling Problem"}, {"paperId": null, "title": "Addressing some limitations of transformers with feedback memory"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks"}, {"paperId": "c69201d091dd92699fd90a17b9e3407319726791", "title": "Neural sequence chunkers"}, {"paperId": "7257eacd80458e70c74494eb1b6759b52ff21399", "title": "Using fast weights to deblur old memories"}, {"paperId": "ca645409a33b0d00e7e44d3ede3de88e3ee7274e", "title": "The long and the short of long\u2013term memory\u2014a molecular framework"}, {"paperId": "dc87946183592af5fdce73e53dfb4247e2d431d4", "title": "Short-term memory in vision"}, {"paperId": null, "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, {"paperId": null, "title": "b) Did you describe any potential participant risks, with links to Institutional Review"}, {"paperId": null, "title": "Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating?"}, {"paperId": null, "title": "Acknowledgement The authors would like to thank Compute Canada for providing the computational resources used in this project"}, {"paperId": null, "title": "If you used crowdsourcing or conducted research with human subjects."}, {"paperId": null, "title": "Did you include any new assets either in the supplemental material or as a URL?"}, {"paperId": null, "title": "(a) Did you include the full text of instructions given to participants and screenshots"}]}