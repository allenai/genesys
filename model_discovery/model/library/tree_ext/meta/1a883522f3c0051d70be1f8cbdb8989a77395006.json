{"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision", "abstract": "Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .", "venue": "Neural Information Processing Systems", "year": 2021, "citationCount": 107, "influentialCitationCount": 13, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks, and proposes a dual normalization strategy to account for the scale mismatch between the two attention mechanisms."}, "embedding": {"model": "specter_v2", "vector": [0.37435382604599, 0.2823033928871155, 0.24914492666721344, -0.20026607811450958, 0.08627523481845856, -0.024508021771907806, 0.9689925909042358, -0.3656545579433441, -0.6887866258621216, -0.610741376876831, 0.6381615400314331, 0.40829354524612427, 0.7477424740791321, -0.15586306154727936, -0.22552905976772308, 0.07457217574119568, -0.7804486155509949, 0.15151022374629974, 0.2821054756641388, -0.5705876350402832, 0.12245826423168182, -0.5773294568061829, -1.0922331809997559, -0.10160283744335175, 0.19271934032440186, 0.5822594165802002, 0.6077261567115784, 1.1109626293182373, -0.6572675108909607, 0.534453809261322, 0.38412320613861084, -0.31931012868881226, 0.1628286987543106, 0.18877793848514557, -0.2602182924747467, -0.10513973236083984, 0.693130373954773, -0.4283443093299866, -0.44319313764572144, 0.8432518839836121, -0.18346823751926422, 0.2562728524208069, 0.35275694727897644, -0.3382483422756195, -0.5432891845703125, 0.5623541474342346, 0.4575243890285492, 0.8043429851531982, -0.43640974164009094, -0.5188736319541931, 1.196761131286621, -1.291955590248108, 0.09339269995689392, 1.4672694206237793, 0.401522696018219, 0.392934650182724, 0.13971993327140808, -0.8498014807701111, 0.9807988405227661, 0.01323047187179327, -0.6566699147224426, -0.32814791798591614, 0.055981993675231934, -0.38034021854400635, 1.7869318723678589, -0.6473249793052673, 0.3975765109062195, 1.014257788658142, 0.2577354311943054, 0.8633435368537903, 0.16409838199615479, -0.567925214767456, -0.41193872690200806, -0.2757158577442169, 0.7541177272796631, 0.8519346117973328, -0.5963517427444458, 0.2699849009513855, -0.7992785573005676, 0.06239417940378189, 0.635003924369812, 0.18819749355316162, 0.18755730986595154, -0.04543933644890785, -0.24384883046150208, 0.7005800604820251, 0.5414707064628601, 0.9212096333503723, -0.13103225827217102, 0.7617528438568115, 0.5378961563110352, 0.1819634735584259, 0.08419407159090042, 0.013949529267847538, 0.2554786205291748, 0.44176870584487915, -0.665181577205658, 0.22690634429454803, -0.15072518587112427, 1.2190132141113281, -0.18009372055530548, 0.4559730291366577, -0.589754045009613, 0.35962074995040894, 1.2149163484573364, 0.4532527029514313, 0.5712890625, -0.6508492827415466, -0.2451619803905487, -0.9220830798149109, -0.1790616512298584, -0.922297477722168, -0.04500824585556984, -0.42283016443252563, -1.0150835514068604, -1.1941859722137451, -0.3411320149898529, 0.7030596733093262, -0.7465656399726868, 0.8672746419906616, -0.48601943254470825, 0.28056880831718445, 0.06207064539194107, 0.24998655915260315, 0.6317206025123596, 0.7894963026046753, 0.3304953873157501, 0.14710162580013275, 1.22422456741333, -1.1884939670562744, -0.504362165927887, -1.2029153108596802, 0.05985105410218239, -0.29319924116134644, 0.6632618308067322, -0.23589704930782318, -0.907843828201294, -1.1218900680541992, -0.757838785648346, -0.16891543567180634, -0.6392869353294373, -0.046353135257959366, 0.6888402700424194, 0.1709362268447876, -1.1772805452346802, 0.6838651299476624, -0.201381117105484, -0.6200000047683716, 0.3203287422657013, -0.1696488857269287, 0.4500635266304016, -0.17530372738838196, -1.3443169593811035, 0.3787779211997986, -0.29907163977622986, -0.4044452905654907, -0.30293431878089905, -0.3531983196735382, -1.428976058959961, -0.01905384287238121, 0.11638286709785461, -0.2506089210510254, 1.1182210445404053, -0.3759307265281677, -1.1456117630004883, 0.7851253747940063, -0.4018685519695282, -0.09559771418571472, -0.07278097420930862, -0.47748175263404846, -0.3768940269947052, -0.5741069912910461, -0.0051799057982862, 0.5567032694816589, 0.545121431350708, 0.09034205228090286, -0.05703795701265335, -0.0012246454134583473, -0.47323518991470337, -0.2181982398033142, -0.4889911413192749, 1.1961160898208618, -0.6073871850967407, -0.5034156441688538, -0.13901494443416595, 0.34787964820861816, -0.027508949860930443, -0.36465686559677124, 0.06889672577381134, -1.0153838396072388, 0.7178016901016235, 0.1893264204263687, 0.6115462183952332, -1.033312201499939, -1.023245930671692, -0.5273342728614807, -0.2996331751346588, -0.22656305134296417, -1.1897757053375244, 0.3228766620159149, -0.38362404704093933, 0.18037360906600952, 0.05127346143126488, -0.9353538751602173, -0.14216163754463196, -0.006171358749270439, -1.1388952732086182, -0.12030721455812454, -0.03748351335525513, 1.0871293544769287, -1.0850224494934082, -0.09139703959226608, -0.15563800930976868, 0.4207558333873749, -0.6742136478424072, 1.2045518159866333, -0.2785404622554779, 0.05020491033792496, -0.05240272358059883, -0.4000023603439331, 0.02584211155772209, -0.5222774744033813, 0.11342748999595642, -0.3208594024181366, -0.18393641710281372, 0.42011964321136475, 0.2801303267478943, 1.2487226724624634, -0.356636643409729, 0.7968588471412659, -0.3404138386249542, -0.44424861669540405, 0.46081340312957764, 0.016456322744488716, -0.07622568309307098, -0.5734491944313049, 0.2219449281692505, -0.003979420755058527, -0.8477696180343628, 0.25154367089271545, 0.47580260038375854, 1.0971089601516724, -0.4005154073238373, 0.13892683386802673, 0.5534418225288391, -0.19806109368801117, 0.44932085275650024, 0.47970691323280334, 0.7604976892471313, 0.48519474267959595, 0.5583191514015198, -0.1992160528898239, 0.4034230411052704, -1.0527364015579224, -0.06342295557260513, 0.604422926902771, 0.43375539779663086, 0.9774821400642395, 0.1319795697927475, -0.8306050896644592, -0.606245756149292, 0.018770333379507065, 0.9310348629951477, 1.5999982357025146, -0.38049551844596863, -0.13161802291870117, -0.8106059432029724, 0.021564442664384842, -0.6895937323570251, -0.16688594222068787, -0.3801833987236023, -0.09005361050367355, -0.6954866051673889, -0.7001150846481323, 0.6846171617507935, 0.31803032755851746, 1.0985896587371826, -0.9128966331481934, -0.5967205166816711, -0.38077864050865173, 0.15849769115447998, -1.2029085159301758, -1.1000975370407104, -0.03017459809780121, -0.20564383268356323, 0.11163102090358734, -0.3876369893550873, -0.17291460931301117, -0.04588470607995987, -0.46544942259788513, 1.0103976726531982, -0.8493286967277527, -0.4326411187648773, 0.5417050719261169, 0.40788355469703674, -0.8959667682647705, -0.5917907357215881, -0.02169645205140114, 0.15596243739128113, -0.045752815902233124, 0.39090457558631897, 0.2877122163772583, 0.023801812902092934, -0.17424365878105164, -0.14725147187709808, -0.20555098354816437, 0.23952722549438477, -0.039618268609046936, 0.5547729730606079, -0.38240575790405273, -0.1538591980934143, -0.9027473330497742, 0.7727007865905762, 0.5167714357376099, -0.6641762256622314, 0.275461882352829, -0.7474963068962097, -0.41782599687576294, 0.369964599609375, -0.7316416501998901, -0.27171406149864197, -0.8674277663230896, 0.45692434906959534, -0.6026056408882141, -0.13347189128398895, -0.012918144464492798, 0.46352168917655945, 0.24821266531944275, 0.1346588283777237, 0.7116639018058777, 0.39694419503211975, 0.13320483267307281, 0.35478904843330383, -0.82889723777771, 0.6131699085235596, 0.5863634943962097, 0.02609788440167904, 0.16271235048770905, -0.1776428520679474, -1.2179816961288452, -0.9217586517333984, -0.5631969571113586, -0.35399311780929565, -0.4041779339313507, 0.4095053970813751, -0.6191656589508057, -0.7980082631111145, 0.36256152391433716, -1.3583195209503174, -0.2609483599662781, 0.045729588717222214, -0.462068110704422, -0.31140750646591187, -0.8490782976150513, -1.0775994062423706, -0.6181573867797852, -0.8247774839401245, -0.6527153849601746, 0.325651079416275, 0.09293636679649353, -0.26349857449531555, -0.45948997139930725, 0.09320256859064102, -0.579200804233551, 1.0603035688400269, -0.6723126769065857, 0.2908473610877991, -0.09422247856855392, -0.40412887930870056, -0.04411979019641876, 0.08148074150085449, 0.6459424495697021, -0.07242527604103088, 0.1547754555940628, -0.8432921171188354, 0.14682532846927643, -0.07071897387504578, -0.17742183804512024, 0.5820428133010864, 0.5035168528556824, 0.5716143250465393, -0.06620617210865021, -0.2813200354576111, 0.3176058828830719, 1.2048676013946533, -0.47817185521125793, 0.22694051265716553, 0.14171965420246124, 0.9140136241912842, 0.27966299653053284, -0.26531508564949036, 0.4430651068687439, 0.6682401895523071, 0.09435337781906128, 0.07089224457740784, -0.09945456683635712, -0.12472297251224518, -0.4738183617591858, 0.6345508098602295, 1.5713428258895874, 0.18232077360153198, 0.10039719939231873, -1.0312288999557495, 0.8434602618217468, -0.8964184522628784, -0.9842106699943542, 0.8020683526992798, 0.6153358221054077, 0.26042482256889343, -0.40565189719200134, -0.3975539207458496, -0.670153021812439, 0.4277122914791107, 0.8440314531326294, -0.25000450015068054, -0.7873149514198303, -0.39013662934303284, 0.30350691080093384, 0.09999088197946548, 0.5990094542503357, -0.4845457375049591, 0.781485378742218, 14.97509479522705, 0.42909178137779236, -0.3487764596939087, 0.6621403694152832, 0.6959951519966125, 0.3756761848926544, -0.16828623414039612, -0.11496178060770035, -1.2247849702835083, -0.2870396077632904, 1.015486478805542, 0.18758772313594818, 0.5727782845497131, 0.04821166396141052, 0.026656867936253548, 0.43286168575286865, -0.2566622495651245, 0.7686960101127625, 0.9656555652618408, -0.9119811058044434, 0.41425755620002747, 0.011352564208209515, 0.06918025016784668, 0.7378993630409241, 0.987650454044342, 0.5824448466300964, 0.6748148202896118, -0.18015778064727783, 0.3546150028705597, 0.6161283254623413, 0.9897336959838867, 0.2789388597011566, 0.12087499350309372, 0.34436264634132385, -0.9526023268699646, -0.21411238610744476, -0.7821448445320129, -1.1874022483825684, 0.07656347006559372, 0.17542333900928497, -0.2833603024482727, -0.7516135573387146, 0.2096315175294876, 0.9870943427085876, -0.17383918166160583, 0.0797022357583046, -0.0069305445067584515, 0.5307939648628235, 0.14132365584373474, -0.3532411456108093, 0.5046492218971252, 0.24601323902606964, 0.5690388083457947, 0.15864941477775574, 0.24031400680541992, 0.29563918709754944, 0.08854590356349945, 0.4127410352230072, -0.11509110778570175, -0.04323124140501022, -0.5047256946563721, -0.5404936671257019, -0.4202287495136261, 0.653506338596344, 0.4187586009502411, 0.15359218418598175, -0.27377811074256897, 0.28304505348205566, 0.4541444778442383, 0.5394489169120789, -0.44336432218551636, 0.11275912821292877, 0.13624387979507446, -0.3847862184047699, 0.14537839591503143, 0.12724509835243225, -0.020960766822099686, -0.5398399233818054, -0.6923457384109497, -0.09303365647792816, 0.44516152143478394, -1.096038579940796, -0.6499615907669067, 1.2198065519332886, -0.3388177454471588, -0.17277036607265472, 0.28474676609039307, -0.7360761165618896, -0.28727978467941284, 0.38751333951950073, -1.263753056526184, -0.838708758354187, -0.2691252529621124, -0.3312007784843445, 0.24352186918258667, -0.27780112624168396, 1.078867793083191, -0.005516848526895046, 0.018939174711704254, -0.24191264808177948, -0.22338902950286865, 0.022257395088672638, -0.12600873410701752, -0.5860167145729065, 0.9429473280906677, 0.2866118252277374, -0.10850697755813599, 0.2921571433544159, 0.3114807903766632, 0.20247633755207062, -0.580260694026947, -0.1597902923822403, 0.8425412774085999, -0.7196599245071411, -0.17411072552204132, -1.0868406295776367, -0.8467711210250854, 0.2804846465587616, 0.7248814702033997, -0.1047850027680397, -0.08737701922655106, 0.005825842265039682, -0.8659741878509521, -0.4151727557182312, -0.17267045378684998, 0.1510167270898819, 0.5512061715126038, -0.9557980895042419, -0.21531616151332855, -0.20838549733161926, 0.6952844262123108, -0.7134358882904053, 0.01353074423968792, -0.09175775945186615, 0.46914970874786377, -0.3502792716026306, 1.085754632949829, -0.17979826033115387, 0.7146385312080383, 1.0867726802825928, -0.1180369108915329, -0.5915483832359314, -0.2888743579387665, -0.9143698215484619, 0.3334088921546936, 0.3497004806995392, 0.2284429669380188, -0.5444892048835754, -0.022458823397755623, 0.3820572793483734, 0.4382038414478302, -0.5044410824775696, -0.44534721970558167, -0.5422982573509216, -0.04255526885390282, -0.39430147409439087, 0.19950005412101746, -0.18685296177864075, -0.3758334517478943, 0.5157169699668884, 0.2765328586101532, 0.30767080187797546, 0.035729166120290756, -0.5411068797111511, 0.23024536669254303, -0.05005143582820892, 0.2946893870830536, -0.7496594190597534, -0.4086569547653198, -1.4880475997924805, 0.19074030220508575, -0.8488249778747559, -0.040302980691194534, -0.9189743995666504, -0.22991123795509338, 0.27853068709373474, -0.23163101077079773, -0.1582830399274826, 0.5376529097557068, -0.4755164086818695, -0.2905668020248413, -0.5129906535148621, -0.45965293049812317, 1.0007119178771973, 0.8932988047599792, -0.9255889058113098, 0.2725100517272949, -0.07349442690610886, 0.23263005912303925, 0.401841938495636, 0.11750441044569016, -0.3258078098297119, -0.768798291683197, -1.171530842781067, 0.35164421796798706, -0.45281100273132324, 0.14063787460327148, -0.8197048306465149, 0.79995197057724, 0.21291238069534302, -0.3782554864883423, -0.2332984060049057, 0.6077306866645813, -0.7692002654075623, -0.7257533669471741, 0.48307064175605774, -0.8405545353889465, 0.32020074129104614, 0.08166125416755676, -0.3263220489025116, -0.30267763137817383, 0.8931336998939514, 0.24236740171909332, -1.3691829442977905, -0.793179452419281, 0.7126047015190125, -0.532310426235199, 0.2911515533924103, -0.3056177794933319, -0.13319723308086395, -1.0257238149642944, -0.5802367329597473, 0.12480770796537399, 0.27515679597854614, -0.6036109924316406, 1.3138114213943481, 0.7110840082168579, -0.9731983542442322, 0.02049606665968895, 0.40470343828201294, -0.03149954602122307, -0.024431167170405388, 0.6024708151817322, 0.28877177834510803, -0.08867479860782623, 0.7358850836753845, 0.2915131449699402, 0.1450975388288498, -1.1093950271606445, 0.34791073203086853, 0.8150036334991455, -0.46321091055870056, -0.24678541719913483, 1.0259132385253906, 0.2899947464466095, -0.978724479675293, 0.12840470671653748, -0.9575258493423462, -1.031458854675293, -0.17481671273708344, 0.6165148019790649, -0.005575769115239382, -0.5464619398117065, -0.3329169750213623, -0.4903002977371216, 0.5440791249275208, -0.029595911502838135, -0.7700533866882324, 0.5833248496055603, -0.3610152006149292, -0.5452985763549805, 0.9604906439781189, 0.9317113161087036, -0.6544773578643799, -0.7163611054420471, -0.9564425945281982, -0.6592167019844055, -0.03641059249639511, 0.09606501460075378, -0.07376866787672043, -0.4353223443031311, 0.9977521300315857, 0.5987292528152466, 0.23150497674942017, 0.27565649151802063, -0.1510097235441208, 0.21089518070220947, 0.8110508918762207, -0.04445286840200424, -0.6647094488143921, -0.1900181621313095, 2.0124568939208984, 1.7170772552490234, -0.5630354881286621, 0.09245616942644119, -0.37118932604789734, -0.7903439402580261, 0.6041510701179504, 0.5125522613525391, -0.3020097017288208, 1.0459319353103638, 0.0012134502176195383, 0.16597548127174377, 0.03725191205739975, -1.1038486957550049, -0.27444663643836975, 0.8300405144691467, 0.9361405968666077, 0.7437345385551453, 0.010409530252218246, 0.2871905267238617, 0.6937782764434814, -0.002756828209385276, 0.028948644176125526, 0.4955742061138153, 0.12588933110237122, -0.22485190629959106, 0.41554728150367737, 0.07573370635509491, 0.4033260941505432, -0.4463972747325897, -0.7018493413925171, 0.1881546825170517, 0.16291439533233643, -0.1772739738225937, 0.33159711956977844, 1.2904267311096191, 0.30110785365104675, 0.5664376020431519, 0.0152747156098485, 0.502495288848877, -0.47574982047080994, 0.16526731848716736, 0.11164145171642303, -0.8338795900344849, -0.16697753965854645, -0.35862627625465393, -0.7999540567398071, -0.1694120168685913, 0.17608164250850677, 0.4636343717575073, -0.3768637180328369, 0.2200886607170105, 1.049796462059021, 0.5275490880012512, 0.562335729598999, -0.2984732389450073, -0.19351419806480408, -0.4765254259109497, -1.0925512313842773, 0.0936858057975769, -0.5852228999137878, 0.10517474263906479, -0.17247594892978668, 0.10854052752256393, -0.0836571455001831]}, "authors": [{"authorId": "1431754650", "name": "Chen Zhu"}, {"authorId": "2056440915", "name": "Wei Ping"}, {"authorId": "2723309", "name": "Chaowei Xiao"}, {"authorId": "1911755", "name": "M. Shoeybi"}, {"authorId": "1962083", "name": "T. Goldstein"}, {"authorId": "2047844", "name": "Anima Anandkumar"}, {"authorId": "2301680", "name": "Bryan Catanzaro"}], "references": [{"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "8514e90a83b2609d188c07be1cde90307b2c6afe", "title": "OmniNet: Omnidirectional Representations from Transformers"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "022622e024890d6e044ac50e2da6b44c59bdf418", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "5c63fc87400a4d3afea63ab8a068a47249f815c2", "title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition"}, {"paperId": "f5c8464032a936451b222be1984cabf42d6adfa8", "title": "Are we done with ImageNet?"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "46b3ba0f3cb8340bc94f26e0fdf6dc4e38f68948", "title": "Hierarchical Transformers for Long Document Classification"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "45557cc70cd6989ab6b03e5aeb787e34299099f7", "title": "Natural Adversarial Examples"}, {"paperId": "6be216d93421bf19c1659e7721241ae73d483baf", "title": "Generating Diverse High-Fidelity Images with VQ-VAE-2"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "49b64383fe36268410c430352637ed23b16820c5", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"}, {"paperId": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "title": "Do ImageNet Classifiers Generalize to ImageNet?"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "512b8ef0002e0bfd0ecb5ab17d533c1762eb9786", "title": "Set Transformer"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "1fc97231d1a2139ae9f54c15a530b4714b6701b4", "title": "Centered Weight Normalization in Accelerating Training of Deep Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649", "title": "Understanding the difficulty of training deep feedforward neural networks"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "986e339f3ab16c44b57b0544fe7aecb96679eeb3", "title": "Improve Vision Transformers Training by Suppressing Over-smoothing"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Flop counter for pytorch models"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Micro-batch training with batch-channel normalization and weight standardization"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": null, "title": "MIXED-RAND refers to replace the image background with a random background of the random class"}, {"paperId": null, "title": "Model Params (M) ImageNet (%)"}, {"paperId": null, "title": "ImageNet-A is the natural adversarial example dataset. It contains naturally collected images from online that mislead the ImageNet classifiers"}, {"paperId": null, "title": "ImageNet-R (Rendition) aims to evaluate the model generalization performance on out-of-distribution data"}, {"paperId": null, "title": "LS-21S has fewer parameters than CvT * -LS-21, more FLOPs, and 0.4% higher accuracy, demonstrating the advantage of focusing the computation on higher-resolution feature maps. The effect of DualLN"}, {"paperId": null, "title": "believe that our observation opens new directions for designing robust vision Transformers. We the in-depth study as an important future work"}]}