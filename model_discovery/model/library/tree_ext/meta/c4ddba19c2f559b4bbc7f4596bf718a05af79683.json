{"paperId": "c4ddba19c2f559b4bbc7f4596bf718a05af79683", "title": "Towards Optimal Learning of Language Models", "abstract": "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.", "venue": "arXiv.org", "year": 2024, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A theory for the optimal learning of LMs is presented, and it is empirically verified that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods."}, "embedding": {"model": "specter_v2", "vector": [0.36578741669654846, 0.6537673473358154, -0.7668984532356262, -0.12105211615562439, -0.7409735321998596, 0.3493841886520386, 0.3308066427707672, -0.13252167403697968, -0.5685795545578003, -0.24920572340488434, 0.39844784140586853, -0.3314031660556793, 0.22341084480285645, 0.3038182556629181, -0.19001498818397522, 0.005146403796970844, -0.795615017414093, 0.06318823993206024, -0.47700875997543335, -0.3191516697406769, -0.4246327579021454, -0.5255798101425171, -0.8560033440589905, 0.048246487975120544, 0.5357677340507507, 0.628841757774353, 0.211909681558609, 1.284042239189148, -0.5579836368560791, 0.16377116739749908, 1.0586698055267334, -0.5103153586387634, 0.5334851741790771, -0.19120831787586212, -0.10748652368783951, 0.03150644153356552, 0.19640038907527924, -0.5394060015678406, -0.6498416066169739, 0.9079554080963135, -0.3942575454711914, 0.4318488538265228, 0.4876323342323303, -0.5069208741188049, -0.04894145950675011, 0.520838737487793, 0.2845611274242401, 0.6699175238609314, -0.4521716237068176, -0.6374853849411011, 1.2502968311309814, -1.2679741382598877, -0.17633017897605896, 1.574469804763794, 0.6975411772727966, 0.37419819831848145, 0.03777044638991356, -0.4012187719345093, 0.47173115611076355, -0.31354033946990967, -1.0119775533676147, -0.4811131954193115, -0.4446457624435425, 0.21669337153434753, 1.3127572536468506, -0.3155560791492462, -0.44639068841934204, 0.1470179706811905, 0.06794257462024689, 1.4232230186462402, -0.15676479041576385, -0.9614905714988708, -0.3194146156311035, 0.4328911006450653, 0.19829821586608887, 0.8702363967895508, -0.2264864295721054, 0.3581487834453583, -1.1673617362976074, -0.08053861558437347, 0.014729403890669346, -0.13081131875514984, -0.18095752596855164, -0.36028003692626953, 0.5093016624450684, 0.7005907297134399, 0.28735435009002686, 0.5688835978507996, 0.0730978474020958, 1.002698302268982, 0.5423911213874817, 0.5552781820297241, 0.7209446430206299, 0.2558617889881134, -0.19986404478549957, 0.24049700796604156, -1.0519219636917114, 0.007666883058845997, -0.18675720691680908, 0.7975433468818665, -0.494777649641037, 0.21841177344322205, -0.5149187445640564, 0.6864454746246338, 1.4920684099197388, 0.24323610961437225, 0.42113688588142395, -0.6640194654464722, 0.7366335391998291, -1.0090090036392212, -0.027939705178141594, -0.5367139577865601, -0.09847959876060486, -0.8345840573310852, -0.9026510715484619, -1.1272333860397339, -0.6731906533241272, 0.19593296945095062, -0.3772631287574768, 0.8778981566429138, -0.2735677659511566, -0.05843733623623848, 0.4180167317390442, 0.3098175525665283, -0.19876818358898163, 1.111283540725708, 0.07156173139810562, -0.5967692136764526, 0.46136605739593506, -0.7757607698440552, -0.8622694611549377, -0.8260169625282288, 1.1081676483154297, -0.2695600688457489, 0.6255326867103577, -0.18955133855342865, -1.239925742149353, -1.1315486431121826, -1.0528678894042969, 0.023141000419855118, 0.003041943535208702, 0.5552012324333191, 0.6454223990440369, 0.5670357346534729, -1.115033745765686, 0.9819843769073486, -0.28622329235076904, -0.1812264770269394, 0.48100778460502625, 0.3100818395614624, 0.45038995146751404, -0.4428497552871704, -0.7527082562446594, 0.2979207932949066, 0.43388429284095764, -0.9244763851165771, -0.06781036406755447, -0.6683140993118286, -0.994464635848999, 0.00738556869328022, 0.3388673961162567, -0.6598506569862366, 1.3829923868179321, -0.22230784595012665, -1.3778793811798096, 0.13365790247917175, -0.2268742173910141, 0.10839610546827316, 0.6456539630889893, -0.5666343569755554, -0.4334675371646881, -0.33510062098503113, -0.4993800222873688, 0.6143304109573364, 0.4374980032444, -0.29760652780532837, -0.5353617072105408, 0.4235357344150543, -0.5757353901863098, -0.10694970190525055, -0.755337655544281, 0.4571513235569, -0.7226928472518921, -0.7076908349990845, 0.2413005232810974, 0.3988296389579773, -0.35501739382743835, 0.17078423500061035, -0.07812950015068054, -0.6778121590614319, 0.926933228969574, -0.7258273363113403, 1.153152585029602, -0.9966129064559937, -0.7143520712852478, 0.005543433129787445, -0.15980438888072968, 0.11329586803913116, -0.96207195520401, 0.34995609521865845, 0.0029823570512235165, 0.43448546528816223, -0.5311285257339478, -1.5652594566345215, -0.09284566342830658, 0.0020363517105579376, -0.7602145075798035, -0.2811524271965027, 0.2383650839328766, 0.7910603284835815, -0.9138997793197632, 0.22765366733074188, -0.1545446217060089, 0.44144803285598755, -1.021349549293518, 1.3282190561294556, -0.2602522373199463, 0.31039872765541077, 0.18131913244724274, -0.4082944989204407, 0.3212369382381439, -0.12366504967212677, 0.43936687707901, -0.06342615187168121, 0.34357962012290955, 0.1362430453300476, -0.49163496494293213, 1.2590875625610352, -0.6148188710212708, 0.8475441336631775, 0.0784008651971817, -0.7579450607299805, -0.038852740079164505, 0.31255683302879333, -0.16471894085407257, -0.304043710231781, 0.4664461016654968, 0.5414922833442688, -0.7480558753013611, 0.3119679093360901, 0.5779065489768982, 0.529191255569458, -0.05915692821145058, 0.5226697325706482, 0.702040433883667, -0.2912549078464508, 0.5343813300132751, 0.41623741388320923, 0.13835260272026062, 0.22263705730438232, 0.5526170134544373, 0.1379382461309433, 0.43673214316368103, -1.1844396591186523, -0.51738041639328, 0.7399846315383911, 0.7961019277572632, 0.45309823751449585, -0.09313110262155533, -0.5323962569236755, -0.6076327562332153, -0.4528979957103729, 0.6949933767318726, 1.384535551071167, -0.4060266613960266, -0.29588162899017334, -0.9002398252487183, -0.36298441886901855, -0.15242619812488556, 0.4275556206703186, 0.06105635315179825, -0.5109261870384216, -0.3383263647556305, -1.3357038497924805, 0.8520479202270508, 0.15673020482063293, 1.050737977027893, 0.3075491487979889, 0.2877002954483032, -0.5294489860534668, 0.41012802720069885, -1.034633755683899, -0.9093064665794373, 0.2715967893600464, -1.1046721935272217, -0.019784249365329742, -0.1615884006023407, 0.08866512030363083, 0.2709698975086212, -0.3925281763076782, 0.38285544514656067, -0.036947984248399734, -0.00954787153750658, 0.35727623105049133, 0.3181926906108856, -0.999751091003418, -1.534777045249939, 0.1493813693523407, 0.5929601192474365, 0.007650051731616259, -0.15886521339416504, 0.5500017404556274, 0.7127227187156677, -0.39359086751937866, -0.5164305567741394, 0.2974073588848114, 0.20392471551895142, -0.06669066101312637, 0.6578521132469177, -0.21504679322242737, -0.04489580914378166, -1.0229393243789673, 1.3557227849960327, 0.21961964666843414, -0.42709261178970337, 0.4730619788169861, -1.2517611980438232, 0.23765596747398376, 0.6843367218971252, -0.5384673476219177, -0.2560315728187561, -0.8560231924057007, -0.07356228679418564, -0.324099063873291, -0.14239244163036346, 0.05211693048477173, 0.6803686022758484, -0.043379150331020355, 0.14426912367343903, 0.5711601972579956, 0.7254837155342102, -0.29351580142974854, 0.6795825958251953, -0.3903660476207733, 0.542460560798645, 0.40803492069244385, 0.33213096857070923, -0.28760361671447754, -0.13741804659366608, -0.8163712620735168, -0.5799835324287415, -0.47502601146698, -0.498486191034317, -0.3674408197402954, -0.2136949747800827, -0.4383469521999359, -0.18686795234680176, -0.6853916049003601, -0.7490864992141724, -0.013012212701141834, 0.07209280878305435, 0.10335980355739594, -0.4943358302116394, -0.8005141019821167, -1.443436622619629, -0.876549243927002, -0.8419768214225769, -0.9656583070755005, 0.2724333107471466, 0.12311044335365295, -0.3138390779495239, -0.5815603137016296, -0.40334516763687134, -0.2200247347354889, 1.007364273071289, -1.0918668508529663, 1.0526899099349976, -0.024076152592897415, 0.024047721177339554, -0.15849635004997253, 0.3039466142654419, 0.9913223385810852, -0.11280913650989532, 0.2550661265850067, -1.0508606433868408, -0.24128182232379913, -0.22216296195983887, -0.13820114731788635, 0.15000084042549133, 0.36696967482566833, 0.6317062973976135, -0.26638031005859375, -0.5217744708061218, 0.7431797981262207, 1.344332218170166, -0.6751585602760315, -0.31297147274017334, -0.04870154708623886, 0.9286091327667236, 0.08043866604566574, -0.3201994001865387, 0.622933030128479, -0.18634578585624695, 0.4796466827392578, -0.09133283793926239, -0.23946180939674377, -0.3768806755542755, -0.5394250154495239, 0.6234849691390991, 2.097501039505005, 0.4820766746997833, -0.18573054671287537, -0.7268657684326172, 0.026522347703576088, -0.9733837842941284, -0.44973617792129517, 0.6728109121322632, 0.8806349635124207, 0.3034152090549469, -0.2428668588399887, -0.19958563148975372, 0.09710332006216049, 0.22970734536647797, 0.3216445744037628, -0.2736070156097412, -0.6693999171257019, 0.12162329256534576, 0.22455547749996185, 0.14009611308574677, 0.9991757869720459, -0.31290844082832336, 0.3327004015445709, 14.870519638061523, 1.114333987236023, -0.13243423402309418, 1.0910255908966064, 0.8124570846557617, -0.0779087245464325, 0.023112226277589798, -0.521385669708252, -1.0042548179626465, -0.10100200027227402, 1.4647572040557861, 0.16789333522319794, 0.8140982985496521, 0.12081675976514816, 0.2529193162918091, 0.3795260190963745, -0.26463913917541504, 1.0682783126831055, 0.3696887195110321, -1.2973252534866333, 0.8415468335151672, 0.23238399624824524, 0.5635843873023987, 0.5475132465362549, 0.7182044386863708, 0.8506742119789124, 0.03062480315566063, -0.2493641972541809, 0.8518639802932739, 0.16843898594379425, 0.8577017188072205, -0.1857072114944458, 0.5255980491638184, 0.9576244354248047, -0.9312306642532349, -0.37799540162086487, -0.5517532825469971, -0.789733350276947, 0.28461816906929016, 0.3432292640209198, -0.7580662965774536, -0.32363542914390564, -0.30526164174079895, 0.4558066725730896, -0.03910728543996811, 0.29021352529525757, -0.006374801974743605, 0.9705160856246948, -0.3380936086177826, 0.11474999785423279, 0.18643875420093536, 0.3374980390071869, 0.1769414246082306, 0.36991503834724426, 0.2889222502708435, -0.22897447645664215, 0.10641574114561081, 0.35176700353622437, -0.8330169916152954, 0.3712924122810364, -0.2929301857948303, -0.2834525406360626, -0.1369204819202423, 0.20913144946098328, 0.7057638764381409, 0.1111159399151802, -0.10715346038341522, 0.30273982882499695, 0.5573278665542603, 0.1495351642370224, 0.12470044940710068, 0.09507952630519867, 0.40306535363197327, -0.3172261714935303, -0.49319496750831604, 0.379371702671051, -0.4417758882045746, -0.7449319958686829, -0.5536839962005615, -0.6598008871078491, 0.3638632297515869, -0.6713042259216309, -0.727046012878418, 0.5309674143791199, -0.08440345525741577, -0.45726478099823, 0.3404994606971741, -0.7236843705177307, 0.05452701821923256, 0.22629299759864807, -1.268320083618164, -0.30364471673965454, 0.6645342111587524, -0.677000105381012, 0.029356811195611954, -0.21144656836986542, 1.0005019903182983, 0.2267886996269226, -0.323409765958786, 0.6527367234230042, 0.7546278238296509, -0.2685259282588959, -0.48789554834365845, -0.3318129777908325, 0.5817111730575562, 0.514478325843811, 0.11770093441009521, 0.19815495610237122, 0.012794842012226582, 0.33250582218170166, -0.5893993377685547, -0.1108618900179863, 0.6184356212615967, -0.42405688762664795, -0.5058283805847168, -0.6465035080909729, -0.771690845489502, -0.09094248712062836, 0.1275336593389511, -0.5689049959182739, 0.4186117947101593, -0.004350749775767326, -0.40209630131721497, -0.21512512862682343, -0.6859131455421448, 0.1992308795452118, 0.5082302093505859, -1.0034912824630737, -0.09343407303094864, 0.25712886452674866, 0.653843879699707, -0.8049883842468262, -0.18983431160449982, -0.045218367129564285, -0.11583170294761658, 0.22873516380786896, 0.875628650188446, -0.6027867197990417, 0.5800273418426514, 0.770248293876648, -0.4425349533557892, -0.722368061542511, 0.3381485641002655, -0.9649117588996887, 0.07992857694625854, -0.0391891673207283, 0.4594658613204956, -0.14841428399085999, 0.05472169071435928, 0.5537911653518677, 0.17858421802520752, -0.5545133352279663, -0.9771813154220581, -0.33450984954833984, -0.11383598297834396, -1.007900595664978, -0.19869700074195862, -0.2888427972793579, -0.33662059903144836, -0.2558644413948059, 0.18407058715820312, 0.7771610617637634, 0.0431043803691864, -1.075739860534668, 0.3697460889816284, 0.07140649855136871, -0.28157928586006165, -0.7848709225654602, -0.269090473651886, -1.7202590703964233, -0.056370809674263, -1.573865532875061, -0.024985376745462418, -0.5090413093566895, -0.4378123879432678, 0.15428082644939423, -0.184824600815773, -0.37601929903030396, 0.16984383761882782, -0.5799975395202637, 0.29984602332115173, -0.21456536650657654, -0.6169775724411011, 1.0600358247756958, 0.6538821458816528, -0.6173714399337769, 0.09595433622598648, 0.10936211794614792, 0.23235532641410828, 0.38524115085601807, 0.4335397183895111, -0.76388019323349, -0.9769020080566406, -1.431809425354004, 0.07331427186727524, -0.20139148831367493, -0.30268609523773193, -0.7225762605667114, 0.44901445508003235, 0.39245110750198364, -0.1721184253692627, 0.5107329487800598, 0.27523329854011536, -0.8378171920776367, -0.6223928332328796, 0.889938473701477, -1.441338062286377, 0.1628076136112213, 0.03399541229009628, -0.2425137311220169, -0.30621013045310974, 0.23125091195106506, 0.3345770537853241, -0.6190276145935059, -0.24945731461048126, 0.4219924211502075, -0.38846737146377563, 0.1287800669670105, -0.23401150107383728, 0.2166564166545868, -0.552981436252594, -0.6287866234779358, -0.1274958997964859, 0.1500067114830017, -0.2962828278541565, 0.9798879623413086, 0.31230559945106506, -1.1162232160568237, 0.022025294601917267, 0.519399106502533, -0.3406481146812439, -0.24890975654125214, 0.32898977398872375, 0.3632223606109619, -0.5932170152664185, 0.574298083782196, 0.7070640325546265, 0.15787947177886963, -1.1390248537063599, -0.22225363552570343, 0.9508065581321716, -0.6804144978523254, -0.0783587247133255, 1.2785407304763794, -0.484435498714447, -1.1551941633224487, 0.37855011224746704, -1.1005055904388428, -0.1692681461572647, -0.6525411605834961, 0.6676909923553467, 0.1945788711309433, -0.05536718666553497, 0.1839720606803894, -0.0008791807922534645, 0.2936140298843384, 0.25127407908439636, -0.42968013882637024, 0.572516679763794, -0.6191073656082153, -0.2889988422393799, 0.6221377849578857, 1.1114217042922974, -0.5973948240280151, -0.7693395018577576, -0.7873589396476746, -0.19961144030094147, -0.42422354221343994, 0.28217852115631104, 0.18836428225040436, -0.5804020762443542, 0.9458713531494141, 1.0145007371902466, -0.08001438528299332, 0.3455936312675476, -0.2256416380405426, 0.11159069836139679, 0.5634779334068298, 0.38437601923942566, -0.835092306137085, -0.419210284948349, 1.0637494325637817, 1.2896854877471924, -0.9690220952033997, 0.5385788679122925, -0.15863308310508728, -0.570522129535675, 1.0759403705596924, -0.024655083194375038, 0.14204823970794678, 1.0545982122421265, -0.06867291778326035, 0.06366518139839172, 0.6287435293197632, -1.080853819847107, 0.08273199200630188, 1.2058804035186768, 0.9442514777183533, 0.6798548698425293, 0.3381282389163971, -0.1383126825094223, 1.011301040649414, 0.01647154800593853, 0.2886103391647339, 0.461503803730011, 0.4360915720462799, -0.5062255859375, -0.32488271594047546, -0.20442798733711243, 0.6528356671333313, -0.8230488896369934, -0.5782086253166199, 0.38257893919944763, 0.4943104088306427, 0.26702165603637695, 0.7026246190071106, 0.3880460560321808, -0.1912192404270172, 0.3998183608055115, 0.28973373770713806, 0.21934981644153595, -0.3977819085121155, 0.04034070298075676, -0.1285824328660965, -0.7192639708518982, 0.26973316073417664, -0.09049638360738754, -0.4020978510379791, -0.4634496569633484, -0.5219982862472534, 0.3865419328212738, -0.04075746610760689, 0.4856552183628082, 1.1130127906799316, 0.5519823431968689, 0.29932790994644165, -0.1434251219034195, -0.27912649512290955, -0.9404171705245972, -0.6936189532279968, -0.11058292537927628, -0.33745911717414856, -0.22734764218330383, 0.17826396226882935, -0.285105437040329, 0.08107468485832214]}, "authors": [{"authorId": "2116405624", "name": "Yuxian Gu"}, {"authorId": "2286153844", "name": "Li Dong"}, {"authorId": "34128716", "name": "Y. Hao"}, {"authorId": "2287927238", "name": "Qingxiu Dong"}, {"authorId": "2285704485", "name": "Minlie Huang"}, {"authorId": "2253471545", "name": "Furu Wei"}], "references": [{"paperId": "5851121df5ce46be5faea265c868ec0beabfce96", "title": "Efficient Large Language Models: A Survey"}, {"paperId": "194e57aee2d936f5f8ffa038e663bcb3bb2fdc1f", "title": "Efficient Online Data Mixing For Language Model Pre-Training"}, {"paperId": "21091f8133ab034baacb92fdb958e14989eb427f", "title": "Language Modeling Is Compression"}, {"paperId": "11cf88dce827bd67cbfa60400306318022e736d5", "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification"}, {"paperId": "4b474c1f42eefbf14ca85c951f2a22ce031b6cb7", "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "4eeefda3d7cc40d51183a1066200dae6a545874d", "title": "h2oGPT: Democratizing Large Language Models"}, {"paperId": "013f9d16ae286b2ad0179bc1a59b4ce3b5de2a93", "title": "LLMZip: Lossless Text Compression using Large Language Models"}, {"paperId": "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d", "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d", "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"}, {"paperId": "28085f480ce456a376ebace9b899e3bc93dbc048", "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"}, {"paperId": "638b08154fbb71fd34db2aae6cb40045577fe0de", "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "74013b7cfa0fc524803350fca51341004565eb22", "title": "Data Selection for Language Models via Importance Resampling"}, {"paperId": "8a4e2828777c9b3703e8e2b68ac27d9af496261a", "title": "Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "6a8db14262ca2017cb253e12b8daeb57989a38df", "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "70ac362fe4aeca7f699eedec166698c2e3bb01c4", "title": "TRACE: A Fast Transformer-based General-Purpose Lossless Compressor"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "f49065750931c1c3c9edaf7d2f4bc8ea1342450a", "title": "Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling"}, {"paperId": "e553407be283d018e275f472d4d2fd709a6c9248", "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "3dbc347d3ac74de517d148efca4da8d57323f232", "title": "Continuous vs. Discrete Optimization of Deep Neural Networks"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "4fa32fec61c50f8339a05e097dacebe71cf9ab8e", "title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent"}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "c94e49617f569204f989643e5462691b9b3a482b", "title": "Estimating Training Data Influence by Tracking Gradient Descent"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c", "title": "Neural Text Generation with Unlikelihood Training"}, {"paperId": "e658741baefd2c4da4742bb43155f69d4cbd79fa", "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "title": "An Empirical Model of Large-Batch Training"}, {"paperId": "f29e7684a25a2fc880d049a03147decae475e1ca", "title": "The Eighty Five Percent Rule for optimal learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1", "title": "Understanding Black-box Predictions via Influence Functions"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "eb3cda683c79ad11bb46ed741117461a4e046f71", "title": "The Goldilocks Effect: Human Infants Allocate Attention to Visual Sequences That Are Neither Too Simple Nor Too Complex"}, {"paperId": "8af74907c5d0b825c3e9419da64903f614aefd7c", "title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces"}, {"paperId": "aa12af4231669db1b8f978b829079ca2684df5cc", "title": "AUC Optimization vs. Error Rate Minimization"}, {"paperId": "1384d42b8df88bab0ce38c21144e154d1afed238", "title": "Statistical Mechanics of Learning"}, {"paperId": "319f22bd5abfd67ac15988aa5c7f705f018c3ccd", "title": "Learning internal representations by error propagation"}, {"paperId": "4f2703652205bb4e146aee4dd48c78ed68b83ff3", "title": "The Mathematical Theory of Optimal Processes"}, {"paperId": "d4e8835e29418fcffc7c52e7cdf63ca207a206db", "title": "Towards Tracing Knowledge in Language Models Back to the Training Data"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": "8213dbed4db44e113af3ed17d6dad57471a0c048", "title": "The Nature of Statistical Learning Theory"}, {"paperId": "090c5a5df345ab60c41d6de02b3e366e1a27cf43", "title": "A logical calculus of the ideas immanent in nervous activity"}, {"paperId": "17bbcead4eb107b3bc9e9dbb3acb3e64c44e128b", "title": "Calculus of Variations"}, {"paperId": null, "title": "Compression for agi"}, {"paperId": null, "title": "Nonlinear programming , volume 4"}, {"paperId": "ac4611700a27ffae31f6a119a6da47704529aef2", "title": "CURRENT DIRECTIONS IN PSYCHOLOGICAL SCIENCE Metacognitive Judgments and Control of Study"}, {"paperId": null, "title": "Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-dra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b"}, {"paperId": null, "title": "Bilevel optimization to learn training distributions for language modeling under domain shift"}]}