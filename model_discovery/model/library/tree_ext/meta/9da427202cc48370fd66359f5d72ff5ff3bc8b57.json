{"paperId": "9da427202cc48370fd66359f5d72ff5ff3bc8b57", "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks", "abstract": "State-space models (SSMs), such as Mamba (Gu&Dao, 2023), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.", "venue": "arXiv.org", "year": 2024, "citationCount": 24, "influentialCitationCount": 4, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A hybrid model is introduced, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently, and suggests that hybrid architectures offer promising avenues for enhancing ICL in language models."}, "embedding": {"model": "specter_v2", "vector": [0.21193628013134003, 0.2828015089035034, -0.29808908700942993, -0.024089481681585312, -0.6531988978385925, -0.09883331507444382, 0.9719595909118652, -0.05342152342200279, -0.20994950830936432, 0.269766628742218, 0.32510924339294434, -0.2858009338378906, 0.3021928668022156, 0.21105971932411194, -0.08367540687322617, -0.022348599508404732, -1.0089194774627686, 0.5244030952453613, -0.49043214321136475, -0.3597571551799774, -0.21463331580162048, -0.6152960062026978, -0.9493823051452637, 0.04135719686746597, -0.051445312798023224, 0.8281689882278442, 0.34700536727905273, 0.6734076738357544, -0.6056370139122009, 0.3456249237060547, 0.8019686937332153, -0.0441443957388401, 0.2741560935974121, -0.2076854109764099, -0.4616038501262665, -0.5755669474601746, 0.7975381016731262, -0.12841391563415527, -0.6754682064056396, 0.6136835217475891, -0.2762690782546997, 0.4081403613090515, 0.5544368028640747, -0.38402777910232544, -0.13046316802501678, 0.6378046870231628, 0.9025516510009766, 0.7642604112625122, -0.08978071063756943, -0.3838632106781006, 1.5452197790145874, -1.5138657093048096, -0.21740123629570007, 1.277246356010437, 0.36086899042129517, 0.547260046005249, -0.3128349483013153, -0.6944032907485962, 0.9390090107917786, 0.4340677857398987, -0.7789329290390015, -0.45172423124313354, -0.19401216506958008, 0.0130690922960639, 1.6593313217163086, 0.04547299072146416, 0.44548499584198, 0.4578191339969635, -0.10771994292736053, 1.433242678642273, 0.1743520051240921, -0.9199860692024231, -0.22921593487262726, 0.11870365589857101, 0.2729912996292114, 0.7906796932220459, -0.5138152241706848, 0.1639324426651001, -1.0089764595031738, 0.030796773731708527, 0.19918866455554962, -0.049917031079530716, 0.26114028692245483, 0.027282750234007835, -0.5993079543113708, 0.6776426434516907, 0.3851637840270996, 0.8565975427627563, -0.27587172389030457, 1.2104580402374268, 0.12577687203884125, 0.4017266631126404, -0.12460385262966156, 0.6923420429229736, -0.2351853996515274, 0.3413446843624115, -0.916098952293396, 0.1441810131072998, -0.028271814808249474, 0.6862167119979858, -0.20611554384231567, 0.4106542766094208, -0.6137323975563049, -0.1201358512043953, 1.5776457786560059, 0.16886870563030243, 0.36172136664390564, -0.5505228638648987, 0.17512933909893036, -0.6736651062965393, 0.00137931143399328, -0.47743988037109375, -0.046688262373209, -0.31143683195114136, -0.44831573963165283, -1.4897209405899048, -0.5412465929985046, 0.5235884189605713, -0.6170636415481567, 0.9815629720687866, -0.49224624037742615, 0.6030070781707764, -0.05195092409849167, 0.27161961793899536, 0.4363459050655365, 0.785910427570343, 0.3747306168079376, -0.1927395761013031, 0.8538853526115417, -0.6827194690704346, -0.8722471594810486, -1.4112390279769897, 0.7139277458190918, -0.06565265357494354, 0.474098801612854, -0.009098578244447708, -1.17258882522583, -0.7911905646324158, -1.0011085271835327, 0.1260276436805725, -0.5673675537109375, -0.14602647721767426, 1.178090214729309, 0.41869834065437317, -0.9343882203102112, 0.2945699095726013, -0.44988778233528137, -0.28723350167274475, 0.1847444325685501, 0.31931328773498535, 0.10444184392690659, -0.4880720376968384, -1.3826594352722168, 0.883262574672699, 0.37261855602264404, -0.08737289905548096, -0.6916390061378479, -0.3434889316558838, -1.2305686473846436, 0.20425525307655334, 0.23595267534255981, -0.6307317018508911, 1.1571552753448486, -0.20461729168891907, -0.8634564876556396, 0.513849675655365, -0.29677248001098633, 0.20536606013774872, 0.11497215181589127, -0.2088356465101242, -0.5618168115615845, -0.8120488524436951, -0.27286049723625183, 0.5220928192138672, 0.30971047282218933, 0.04476573318243027, -0.18958641588687897, -0.06561572104692459, -0.5028700828552246, -0.0882963016629219, -0.30524390935897827, 0.5339893102645874, -0.5250409841537476, 0.059428099542856216, 0.5357968211174011, 0.7058444023132324, -0.2116168886423111, -0.201652392745018, -0.08220473676919937, -0.8457977175712585, 0.9032668471336365, -0.1860540509223938, 1.1619577407836914, -0.7226609587669373, -0.7377028465270996, 0.0393342562019825, -0.4386678636074066, -0.05691622942686081, -0.7945042252540588, 0.5228179693222046, -0.12243618816137314, 0.19397327303886414, -0.31510069966316223, -0.9937184453010559, -0.044018227607011795, -0.04350999742746353, -0.6703880429267883, -0.24291551113128662, 0.3657936453819275, 1.122009515762329, -1.1277735233306885, 0.07790400832891464, 0.08613505959510803, 0.14467380940914154, -0.9863728284835815, 1.3043628931045532, -0.5041840672492981, 0.39889001846313477, -0.13582319021224976, 0.12482987344264984, 0.07252556830644608, -0.3147219121456146, 0.8207777142524719, -0.3071648180484772, 0.07929805666208267, 0.6693601012229919, -0.3827904164791107, 1.5972293615341187, -0.8959119915962219, 0.8896574378013611, 0.0641477033495903, -0.9623875021934509, 0.06938383728265762, 0.38975989818573, -0.4058646857738495, -0.601672351360321, -0.07980698347091675, 0.5729172825813293, -0.5515048503875732, 0.13359616696834564, 0.6028143167495728, 0.7474233508110046, -0.5000237822532654, -0.04874587431550026, 0.7236742377281189, -0.515009880065918, 0.1462026685476303, 0.25995948910713196, 0.3818427324295044, 0.3124953806400299, 0.6515411734580994, 0.04334075003862381, 0.41779765486717224, -1.0135349035263062, -0.3415623605251312, 0.8452804684638977, 0.4827055335044861, 0.6556423306465149, 0.4972590506076813, -0.7134042978286743, 0.036696381866931915, -0.2580662965774536, 0.5950965285301208, 1.3817708492279053, -0.21043150126934052, -0.03292984142899513, -0.5359115600585938, -0.36613160371780396, -0.302416056394577, 0.199032723903656, -0.46074146032333374, -0.28001856803894043, -0.5130012035369873, -1.0332006216049194, 0.4487711787223816, 0.4055759608745575, 0.9477220177650452, -0.7565966248512268, 0.23882122337818146, -0.16412614285945892, 0.32540786266326904, -0.5661085844039917, -0.4975908398628235, 0.4658846855163574, -0.561167299747467, -0.28992754220962524, 0.4945448935031891, -0.2521865665912628, -0.09831443428993225, -0.6220517158508301, 0.8790234327316284, -0.5446704626083374, -0.26869505643844604, 0.2928520441055298, 0.8717683553695679, -0.5706060528755188, -0.5808058977127075, 0.3039554953575134, 0.2128637135028839, -0.10716161876916885, 0.41706395149230957, 0.32217901945114136, 0.0631984993815422, -0.36290591955184937, -0.26437658071517944, 0.19686894118785858, 0.1316082775592804, 0.24022702872753143, 0.5655383467674255, -0.767598569393158, 0.44332394003868103, -1.1406733989715576, 0.6738296151161194, -0.5497859716415405, -0.6661378145217896, 0.33069783449172974, -1.0694622993469238, -0.4459529519081116, 0.1890915036201477, -0.40920987725257874, -0.12006815522909164, -0.4961966574192047, 0.27388280630111694, -0.3187185823917389, -0.2208181768655777, 0.6669213175773621, 0.27463892102241516, 0.3913494646549225, 0.02880118414759636, 0.48424142599105835, 0.3703076243400574, -0.0511903278529644, 0.64464271068573, -0.9248612523078918, 0.5054541230201721, 0.2579542100429535, 0.22308877110481262, -0.15257464349269867, 0.11359129101037979, -0.4639442563056946, -0.5689154863357544, -0.22558529675006866, 0.01193406991660595, -0.16134856641292572, 0.1115214079618454, -0.9131182432174683, -0.7664135098457336, -0.240423321723938, -0.7553099393844604, -0.4518507719039917, 0.2910064458847046, -0.18372440338134766, -0.22150252759456635, -1.3494415283203125, -0.8335488438606262, -0.7993587851524353, -0.5596153140068054, -0.8714693188667297, 0.04518487676978111, 0.2427174597978592, -0.2516789436340332, -0.827140748500824, -0.19784046709537506, -0.3961065411567688, 1.2946200370788574, -0.7739580869674683, 0.5952858924865723, -0.22535723447799683, -0.4262517988681793, -0.33454087376594543, 0.3872397840023041, 0.33686307072639465, -0.014694617129862309, -0.03321828320622444, -1.2386456727981567, 0.2170979231595993, -0.43740177154541016, -0.28852513432502747, 0.37726864218711853, 0.35059109330177307, 0.8737515807151794, -0.013206678442656994, -0.4743243455886841, 0.4950033128261566, 1.1143133640289307, -0.5032768249511719, 0.04561173543334007, 0.04234511777758598, 1.0755424499511719, 0.09612550586462021, 0.0820235162973404, 0.17223142087459564, 0.2594783306121826, 0.49592798948287964, 0.4147809147834778, 0.2393377721309662, 0.21992795169353485, -0.6262372136116028, 0.49981939792633057, 1.5857185125350952, 0.46802839636802673, 0.24807043373584747, -1.1127054691314697, 0.6884754300117493, -1.361976146697998, -0.5452162623405457, 0.7170053720474243, 0.7892128229141235, 0.5209051966667175, -0.5772459506988525, -0.3406713604927063, -0.27472010254859924, -0.14167067408561707, 0.352822482585907, -0.04676394909620285, -0.6577574014663696, 0.10799720138311386, 0.7771005034446716, 0.10365564376115799, 0.8145925402641296, -0.5602429509162903, 0.6536511182785034, 15.135462760925293, 0.823956310749054, -0.3092471659183502, 0.8788567781448364, 0.851442813873291, -0.11200759559869766, -0.3599343001842499, 0.08683864027261734, -1.037690281867981, -0.0835014134645462, 1.2851321697235107, 0.6667022705078125, 0.658964216709137, 0.13072432577610016, 0.025228746235370636, 0.24314747750759125, -0.9941570162773132, 0.8255255818367004, 0.4845292866230011, -1.3433148860931396, 0.23976290225982666, -0.02805919386446476, 0.2767595648765564, 0.3911542594432831, 0.9961387515068054, 1.1284208297729492, 0.1342572122812271, -0.6566053628921509, 0.3803568482398987, 0.5674725770950317, 1.064111351966858, 0.14339162409305573, 0.24162143468856812, 0.4688640832901001, -0.8383063673973083, -0.423235148191452, -0.1377733051776886, -1.052314043045044, -0.033218592405319214, -0.3578011691570282, -0.6639031171798706, -0.5841216444969177, -0.2913411557674408, 0.49543970823287964, 0.039363473653793335, 0.26966592669487, -0.13450443744659424, 0.976634681224823, -0.06364689022302628, 0.1252783089876175, 0.27020108699798584, 0.293473482131958, -0.09705173969268799, 0.2678270637989044, 0.13287080824375153, 0.04829086735844612, 0.09728202223777771, 0.5956578254699707, -0.3823931813240051, -0.16681838035583496, -0.3096124231815338, -0.26438769698143005, 0.22628703713417053, 0.7201128602027893, 0.9120628833770752, 0.1871972680091858, -0.4724515378475189, -0.13801078498363495, 0.35572099685668945, 0.4366694390773773, 0.10890019685029984, 0.157710000872612, 0.6340039372444153, -0.551444411277771, -0.0935530886054039, 0.5905525088310242, -0.21628661453723907, -0.508887529373169, -0.8271746039390564, -0.4329327344894409, 0.5112529993057251, -0.7681636214256287, -0.865300714969635, 0.4672350585460663, -0.12487621605396271, -0.44057467579841614, 0.12491367012262344, -0.6919311285018921, -0.3824969232082367, 0.649867057800293, -1.6951476335525513, -0.6188419461250305, 0.13842295110225677, -0.35703426599502563, -0.2642754912376404, -0.2975057363510132, 1.2343894243240356, 0.29995641112327576, -0.3633648753166199, 0.0397818386554718, 0.1965951770544052, -0.2316833734512329, -0.022741856053471565, -0.9531903862953186, 0.7118015885353088, -0.03066123090684414, 0.3165801167488098, 0.33420783281326294, -0.1764765828847885, 0.46316710114479065, -0.9098925590515137, 0.045715101063251495, 0.7369139790534973, -1.0316565036773682, -0.4826602339744568, -0.749359667301178, -0.60810387134552, 0.6525081396102905, 0.3011050224304199, 0.1768290102481842, 0.43437978625297546, 0.19240841269493103, -0.5507540702819824, -0.40056750178337097, -0.4140036404132843, 0.031899258494377136, 0.24331849813461304, -0.9229151010513306, -0.4097253084182739, -0.20245493948459625, 0.26743385195732117, -0.8588971495628357, -0.19644629955291748, -0.31754958629608154, 0.15215866267681122, 0.2826099395751953, 1.121749758720398, -0.849288821220398, 0.4567570984363556, 0.7017711997032166, -0.33628275990486145, -0.6850495934486389, -0.037579137831926346, -0.6371026635169983, -0.4632142186164856, -0.19826112687587738, 0.5778310894966125, -0.5166876316070557, 0.2638700306415558, 0.6116947531700134, 0.16227266192436218, -0.5584519505500793, -0.8662340044975281, -0.09754011034965515, -0.12468299269676208, -0.5406363010406494, 0.37453386187553406, 0.4607725441455841, -0.20053336024284363, 0.018313316628336906, 0.45854994654655457, 0.5116043090820312, -0.4602327048778534, -0.7048918008804321, 0.272052526473999, -0.18543857336044312, -0.14764894545078278, -0.5548924207687378, -0.38653168082237244, -1.074076771736145, -0.06152026727795601, -1.2141624689102173, 0.0011914598289877176, -1.0012201070785522, -0.40906041860580444, -0.3224204480648041, -0.8775385618209839, 0.087949238717556, -0.14723671972751617, -0.40359002351760864, -0.11461644619703293, -0.5378020405769348, -0.7864223718643188, 0.6052683591842651, 0.7408628463745117, -0.8066776990890503, 0.14183874428272247, -0.06423816084861755, 0.11380813270807266, 0.0941062942147255, 0.27206382155418396, -0.24832600355148315, -0.7065249085426331, -1.0203255414962769, 0.3709326684474945, 0.15427656471729279, -0.15719765424728394, -0.5999971032142639, 0.5022215843200684, 0.05699877068400383, 0.000622773019131273, 0.2012687772512436, 0.7826652526855469, -0.9101771116256714, -0.28967273235321045, 0.3995397984981537, -1.0135841369628906, 0.015072036534547806, 0.2674669027328491, -0.3970116972923279, -0.516429603099823, 0.6528160572052002, -0.3579484522342682, -1.0325802564620972, -0.7412609457969666, 0.3722962737083435, -0.6680838465690613, 0.07518267631530762, -0.3365248143672943, 0.031843844801187515, -0.7604584097862244, -0.6388956904411316, -0.1250157505273819, 0.4499097764492035, -0.8307815194129944, 0.7194750308990479, 0.30018678307533264, -1.2517149448394775, 0.1599135547876358, 0.3976808786392212, 0.13514122366905212, 0.058368898928165436, 0.2983729839324951, 0.5438815355300903, -0.33469337224960327, 0.5000325441360474, 0.24931447207927704, 0.34278368949890137, -0.3966487646102905, -0.08554267883300781, 0.9681932330131531, -0.6440258026123047, 0.5600268840789795, 0.9206535816192627, -0.17940421402454376, -1.067099928855896, 0.06647638231515884, -0.8506407737731934, -0.5803157091140747, -0.1394624263048172, 0.4592704474925995, -0.09402590245008469, 0.08247370272874832, 0.3312416970729828, -0.45988789200782776, -0.298185795545578, -0.036222219467163086, -0.46643364429473877, 0.5140873193740845, 0.08072038739919662, -0.42863476276397705, 0.7445351481437683, 0.8717893362045288, -0.7182507514953613, -0.48516184091567993, -0.9125431776046753, -0.26398080587387085, -0.23272155225276947, 0.3693825602531433, -0.4769192039966583, -0.6637864112854004, 0.7802708148956299, 0.7333948016166687, 0.18598011136054993, 0.23217469453811646, -0.17414331436157227, 0.03785030171275139, 0.879371702671051, -0.15635228157043457, -0.44350624084472656, -0.610735297203064, 1.3252593278884888, 1.072548747062683, -1.3495043516159058, 0.3074478805065155, 0.03876039758324623, -0.1518886387348175, 0.7281907796859741, 0.6543602347373962, 0.04371821507811546, 0.9642423987388611, -0.5141965746879578, 0.013883446343243122, 0.10203663259744644, -1.3173314332962036, -0.18558654189109802, 0.9192684888839722, 1.0856844186782837, 0.9022364020347595, 0.42143571376800537, 0.20816676318645477, 0.8827707171440125, 0.41696083545684814, -0.269426554441452, 0.13512861728668213, 0.4736040234565735, -0.14184530079364777, -0.1775454580783844, -0.1424432396888733, 0.8506038188934326, -0.42298248410224915, -0.8155823945999146, 0.20432771742343903, 0.7115249633789062, -0.0020176118705421686, 0.2613184452056885, 1.242348313331604, 0.33264824748039246, 0.2798177897930145, 0.5512591004371643, 0.40740105509757996, -0.7035406231880188, -0.4927750527858734, -0.5177325010299683, -0.6331342458724976, 0.06688300520181656, -0.01198664866387844, -0.5498499870300293, -0.043966494500637054, 0.2214612066745758, 0.22984255850315094, -0.0036975250113755465, 0.4248001277446747, 1.1200873851776123, 0.3833342492580414, 0.2580881118774414, -0.49110496044158936, -0.44770383834838867, -0.4670129716396332, -1.0840256214141846, 0.28471726179122925, -0.6703246235847473, 0.0531814806163311, -0.30036476254463196, -0.17128071188926697, -0.5852841138839722]}, "authors": [{"authorId": "2283136866", "name": "Jongho Park"}, {"authorId": "2262529747", "name": "Jaeseung Park"}, {"authorId": "2282947576", "name": "Zheyang Xiong"}, {"authorId": "2221365914", "name": "Nayoung Lee"}, {"authorId": "2262403775", "name": "Jaewoong Cho"}, {"authorId": "3103394", "name": "Samet Oymak"}, {"authorId": "2267745065", "name": "Kangwook Lee"}, {"authorId": "1740595", "name": "Dimitris Papailiopoulos"}], "references": [{"paperId": "85447eeb6e5276e713957835125a2273f9ac0694", "title": "In-Context Language Learning: Architectures and Algorithms"}, {"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "1be73fa3e856c33d0aed1d9e46693523e7fa3c60", "title": "Zoology: Measuring and Improving Recall in Efficient Language Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "ed3ec3ba82f7ba7adf8aa47a73c685ac023d6efd", "title": "Looped Transformers are Better at Learning Learning Algorithms"}, {"paperId": "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b", "title": "What Algorithms can Transformers Learn? A Study in Length Generalization"}, {"paperId": "d4c1517ca5e550ce43515b54e475082fba80bd56", "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions"}, {"paperId": "9bb3deca32af8d632e0d916c587cca6c185a6576", "title": "Uncovering mesa-optimization algorithms in Transformers"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "cbec8bf16a459b0ae38856f604a6a14cd1343477", "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention"}, {"paperId": "0a067fab18c67d4a386efa846c080f8afff5e8f3", "title": "Block-State Transformers"}, {"paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3", "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"}, {"paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2", "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"}, {"paperId": "d40dbe668d5b68419e934dfa4c5851ffa1c24aa2", "title": "Exposing Attention Glitches with Flip-Flop Language Modeling"}, {"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "title": "Scaling Data-Constrained Language Models"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "0ea7fc93d4947d9024ccaa202987a2070683bc1f", "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "d4c60620570801a231a7756f931dda1740288fb9", "title": "Looped Transformers as Programmable Computers"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "661e8d555c4424b5953f17434f2ba910bfcf3afe", "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6", "title": "Transformers Learn Shortcuts to Automata"}, {"paperId": "e070ff286709db28312e08b52b05539debe88146", "title": "Measuring and Narrowing the Compositionality Gap in Language Models"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "f5f5616f39493566a9d502f611adcc8f1ceb394e", "title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7", "title": "MetaICL: Learning to Learn In Context"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "10c86505de83647c7b4157595ab10f64e97c94ef", "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "b3564be8b79f25585acb035f3deaf4ae93c26d8f", "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "aa0d9ad6adf19aa6bafedcb11fd4de92a5f2a3a8", "title": "Time-space hardness of learning sparse parities"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "29c887794eed2ca9462638ff853e6fe1ab91d5d8", "title": "Optimization as a Model for Few-Shot Learning"}, {"paperId": "e7f48efb3f4a6ea4dd8117c20156c5660e6323c6", "title": "Fast Learning Requires Good Memory: A Time-Space Lower Bound for Parity Learning"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "6e7241121c688abbd9329bdcebce4b6320fc619d", "title": "Shifting Inductive Bias with Success-Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement"}, {"paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af", "title": "Are Emergent Abilities of Large Language Models a Mirage?"}, {"paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725", "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": null, "title": "Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning"}, {"paperId": null, "title": "An 800gb dataset of diverse text"}, {"paperId": null, "title": "Linformer"}]}