{"paperId": "ea686f0d933b8e6e4c753a45e433ad386e421272", "title": "Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket", "abstract": "Spiking Neural Networks (SNNs), known for their biologically plausible architecture, face the challenge of limited performance. The self-attention mechanism, which is the cornerstone of the high-performance Transformer and also a biologically inspired structure, is absent in existing SNNs. To this end, we explore the potential of leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA) and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for softmax and captures the sparse visual feature employing spike-based Query, Key, and Value. This sparse computation without multiplication makes SSA efficient and energy-saving. Further, we develop a Spiking Convolutional Stem (SCS) with supplementary convolutional layers to enhance the architecture of Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer V2. To train larger and deeper Spikformer V2, we introduce a pioneering exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we pre-train Spikformer V2 with masking and reconstruction style inspired by the mainstream self-supervised Transformer, and then finetune the Spikformer V2 on the image classification on ImageNet. Extensive experiments show that Spikformer V2 outperforms other previous surrogate training and ANN2SNN methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of 81.10% with just 1 time step. To the best of our knowledge, this is the first time that the SNN achieves 80+% accuracy on ImageNet. The code will be available at Spikformer V2.", "venue": "arXiv.org", "year": 2024, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work introduces a pioneering exploration of Self-Supervised Learning (SSL) within the SNN, and proposes a novel Spiking Self-Attention (SSA) and Spiking Transformer (Spikformer) that achieves 80+% accuracy on ImageNet."}, "embedding": {"model": "specter_v2", "vector": [0.6105367541313171, 0.6554948091506958, -0.04391208663582802, -0.15992969274520874, 0.22858193516731262, 0.05274125561118126, 0.3764950633049011, -0.24450251460075378, -0.2899569869041443, -0.406917929649353, 0.15525393187999725, 0.8256332278251648, 0.8086401224136353, -0.18364955484867096, -0.3116286098957062, -0.07301986962556839, -0.683566689491272, -0.2849253714084625, 0.8371595740318298, -0.3094261586666107, 0.2128593623638153, -0.6806541085243225, -1.5668563842773438, 0.5100785493850708, -0.05377735570073128, 1.5910372734069824, 0.22772859036922455, 0.6177620887756348, -0.16513074934482574, 0.3812119960784912, 0.5351101160049438, 0.043333832174539566, 0.5216068625450134, -0.1099129468202591, 0.08098669350147247, -0.17339521646499634, 0.6552891135215759, 0.27500206232070923, -0.681792140007019, 1.105604887008667, -0.022884998470544815, -0.036273662000894547, 0.2723346948623657, -0.9065709114074707, -0.035541944205760956, 0.47631871700286865, 0.7948974967002869, 1.007171392440796, -0.9414313435554504, 0.1570739895105362, 1.0557576417922974, -1.021354079246521, 0.01704617217183113, 1.1155184507369995, 0.4296937584877014, 0.7318838834762573, -0.11136126518249512, -1.0989044904708862, 0.5178139209747314, -0.4640507400035858, -0.44968342781066895, -0.4264160990715027, 0.12417124211788177, -0.03195739537477493, 1.5287573337554932, -0.5209559202194214, 0.03352511301636696, 1.2656192779541016, 0.4861358106136322, 1.3560210466384888, 0.3717091977596283, -0.06144250929355621, 0.24556659162044525, 0.22850048542022705, 0.4136999249458313, 0.46445992588996887, -0.1530160903930664, 0.3997220993041992, -1.1987804174423218, 0.007023362908512354, 1.0426994562149048, 0.13662078976631165, 0.5140865445137024, -0.4157866835594177, -0.336709588766098, 0.5622923970222473, 0.9884033799171448, 0.7890278697013855, -0.48062416911125183, 1.1441415548324585, 0.5376320481300354, 0.16176381707191467, -0.34505322575569153, 0.5318787693977356, 0.1056637316942215, 0.5700523853302002, -0.829140841960907, -0.053293026983737946, -0.1528017371892929, 0.9792647957801819, -0.44903242588043213, 0.8379673361778259, -0.4032672345638275, -0.07851807773113251, 1.2566473484039307, 0.17283856868743896, 0.5726740956306458, -0.757344126701355, -0.347678005695343, -0.48275026679039, -0.1644909530878067, -0.8577746152877808, -0.02174549177289009, -0.9592841267585754, -1.097437858581543, -0.48317626118659973, -0.08525490015745163, 0.49703940749168396, -0.7386806607246399, 0.7612684369087219, -0.6286063194274902, 0.09130992740392685, -0.0005587063496932387, 0.41641703248023987, 0.9202146530151367, 0.11412341892719269, 0.382964551448822, 0.6724837422370911, 1.3746728897094727, -0.7256525158882141, -0.7303076386451721, -1.3043626546859741, -0.5077095627784729, 0.31601205468177795, -0.14009593427181244, -0.03583928197622299, -1.290152668952942, -1.0378671884536743, -0.8795602321624756, 0.45069620013237, -0.5055257081985474, -0.20503346621990204, 1.395453929901123, 0.12358008325099945, -1.1076152324676514, 1.2470543384552002, -0.5194169282913208, -0.6731233596801758, 1.111087679862976, 0.375243216753006, 0.3730260133743286, 0.7872986197471619, -1.1879730224609375, 0.35165172815322876, -0.1011013314127922, -0.6343197822570801, -0.419120192527771, -0.16548208892345428, -1.0278409719467163, 0.23585063219070435, 0.00477291364222765, -0.7000289559364319, 0.8742941617965698, -0.8493781685829163, -0.8162301182746887, 1.2064687013626099, -0.29864001274108887, -0.48479190468788147, -0.06558473408222198, 0.552571177482605, -0.5953092575073242, -0.3267914950847626, -0.08748246729373932, 0.9354172348976135, 0.6941788792610168, -0.6022986173629761, -0.02422628365457058, -0.22895753383636475, -0.7943534255027771, -0.6240074634552002, -0.37637361884117126, 0.9663911461830139, -0.17961296439170837, -0.5513573288917542, 0.5454984307289124, 0.8546593189239502, 0.2129330337047577, 0.32619771361351013, -0.0676463320851326, -0.9737825989723206, 0.2758556008338928, 0.7353289723396301, 0.3754962682723999, -1.1818474531173706, -1.2785780429840088, 0.13700909912586212, 0.055037278681993484, -0.3586792051792145, -0.8059332966804504, 0.13166865706443787, -0.5215457081794739, -0.02965536341071129, -0.14846386015415192, -0.440774530172348, -0.36967381834983826, -0.21078640222549438, -0.5952563881874084, -0.09123236685991287, 0.6442924737930298, 1.235085129737854, -1.19127357006073, 0.09663324803113937, -0.4381905496120453, 0.15169326961040497, -0.7000824809074402, 0.8140575289726257, -0.20937703549861908, -0.0782894641160965, -0.05451478809118271, 0.07164653390645981, -0.20409761369228363, -0.6562648415565491, 0.23548024892807007, -0.8121235966682434, 0.05550294369459152, 0.37603339552879333, -0.36773931980133057, 1.5528644323349, 0.08859814703464508, 0.5098026990890503, -0.04186667129397392, -0.8144504427909851, 0.703953742980957, 0.30587825179100037, 0.08544381707906723, -0.9193625450134277, 0.878207266330719, 0.3492140471935272, -0.43414393067359924, 0.15351900458335876, -0.19787931442260742, 1.1408841609954834, -0.14509736001491547, -0.31276968121528625, 1.0025063753128052, -0.370503693819046, -0.21671360731124878, 0.28015515208244324, 0.8520140647888184, 0.31543225049972534, 0.49324333667755127, -0.9109891653060913, -0.2748831510543823, -1.0929745435714722, 0.1172633022069931, 0.7468451261520386, 0.04430755600333214, 1.026340126991272, 0.4906148910522461, -0.8927106857299805, -0.12485211342573166, -0.2515536844730377, 0.13532957434654236, 1.0899059772491455, 0.035167351365089417, 0.6318984031677246, -0.8720335960388184, 0.018996167927980423, -0.2187819629907608, -0.26334622502326965, -0.6782362461090088, -0.4669781029224396, 0.0892757773399353, -1.017966866493225, 0.868930995464325, 0.3253847062587738, 1.0312490463256836, -1.3446247577667236, -0.6511754989624023, 0.037519387900829315, 0.727470338344574, -0.6861740946769714, -0.6362361311912537, 0.9029597640037537, -0.6022247672080994, 0.0014603540766984224, 0.22207440435886383, -0.42477864027023315, 0.20438505709171295, -0.7130189538002014, 0.776284396648407, -0.788881778717041, -0.2115601897239685, 0.06496661901473999, 1.1391266584396362, -0.995198130607605, -0.09574734419584274, 0.06427264213562012, 0.12869077920913696, 0.41660088300704956, 0.34586068987846375, -0.14756864309310913, -0.43928205966949463, 0.4067930579185486, -0.5644466280937195, -0.3646491467952728, 0.19283635914325714, 0.03376639261841774, 0.8183457255363464, -0.4776545763015747, 0.8357639312744141, -0.8910651803016663, 0.42533907294273376, -0.2586064338684082, -0.2948150932788849, -0.21078668534755707, -0.5005682110786438, 0.04020584002137184, 0.04678026959300041, -0.7687117457389832, 0.04370133578777313, -0.13823282718658447, 0.7158247828483582, -0.28272175788879395, -0.20604915916919708, -0.37103694677352905, 0.526671290397644, -0.45471006631851196, 0.9621002674102783, 0.7181191444396973, 0.4782734513282776, 0.7021346092224121, -0.17360490560531616, -0.9627394676208496, 1.0346568822860718, 0.5231776237487793, 0.052022602409124374, 0.3893412947654724, -0.18137788772583008, -0.5434067845344543, -0.9421282410621643, -0.5243755578994751, 0.3136069178581238, -0.5530693531036377, 0.38744258880615234, -0.6451488137245178, -1.6628583669662476, 0.4580555856227875, -0.7468911409378052, -0.4809030592441559, -0.12061027437448502, -0.38206538558006287, -0.6291741728782654, -1.1565145254135132, -0.8102105855941772, -0.954929530620575, -0.7090325355529785, -0.570856511592865, -0.03737768903374672, 0.4782557189464569, -0.44631001353263855, -0.2228962630033493, -0.5756998658180237, -0.6938535571098328, 1.7857497930526733, -0.5029927492141724, -0.05392511188983917, -0.5337964296340942, -0.2724911570549011, -0.2170858234167099, -0.13342788815498352, 0.30926987528800964, -0.17391707003116608, -0.03959331288933754, -1.302008032798767, 0.4726884067058563, 0.04366328567266464, -0.638084888458252, 0.7121052742004395, 0.5540661811828613, 1.2539112567901611, 0.39651957154273987, -0.37877827882766724, 0.5086873173713684, 1.5822653770446777, -0.3337584435939789, 0.3319167494773865, 0.16231536865234375, 0.30401286482810974, 0.1246681660413742, -0.7707509994506836, 0.3448295295238495, 0.5068699717521667, 0.08225362002849579, 0.9433416128158569, -0.09628075361251831, -0.19466552138328552, -0.4178132712841034, -0.002763910684734583, 0.6565048098564148, 0.18604406714439392, 0.18391206860542297, -0.7062565684318542, 0.9636766910552979, -1.2227495908737183, -0.8855100870132446, 0.6928455233573914, 0.9587646126747131, 0.6698964238166809, 0.1747327744960785, -0.4154611825942993, -0.33146464824676514, 0.5128926038742065, 0.15531526505947113, -0.8293530344963074, -0.4415183961391449, -0.6224683523178101, 0.5768448710441589, 0.622067391872406, -0.12041369825601578, -0.5585110187530518, 0.48193228244781494, 14.30620002746582, 0.4298851191997528, -0.2192789614200592, 0.38407793641090393, 0.8670084476470947, 0.5533479452133179, -0.37517985701560974, -0.3283199369907379, -1.279983639717102, 0.17613056302070618, 0.9266732335090637, 0.7760304808616638, 0.7599745988845825, 0.36772865056991577, -0.17611435055732727, 0.23884132504463196, -0.2544383108615875, 0.8502102494239807, 0.4998958706855774, -1.359392762184143, -0.1415373682975769, -0.1979210525751114, 0.39957642555236816, 0.7520624399185181, 0.7607043981552124, 0.6096423268318176, 0.6525712609291077, -0.18797685205936432, 0.36845287680625916, 0.46592965722084045, 0.9786881804466248, 0.7098564505577087, 0.19072014093399048, 0.22095751762390137, -0.80849289894104, -0.1515650749206543, -0.5131036043167114, -0.8882304430007935, -0.24320071935653687, 0.07984311878681183, 0.4733785390853882, -1.2154490947723389, 0.4906269907951355, 0.8354626297950745, 0.18643125891685486, 0.4923411011695862, 0.14481306076049805, 0.48250308632850647, -0.13378635048866272, -0.4180595576763153, 0.17064283788204193, 0.9053787589073181, 0.011140172369778156, -0.11290126293897629, -0.12537893652915955, 0.3529939353466034, -0.2545945942401886, 1.0431822538375854, -0.41985777020454407, -0.4318619668483734, -0.10762123763561249, -0.03000211901962757, -0.160934716463089, 1.1017340421676636, 0.21482181549072266, 0.09982456266880035, 0.10884752869606018, 0.3310092091560364, 0.09877292811870575, 0.05822482332587242, -0.30661776661872864, -0.2296343892812729, 0.2582385540008545, -0.1758304089307785, 0.03358563035726547, 0.8902342915534973, -0.8734849095344543, -0.6660549640655518, -0.7374323010444641, -0.10139605402946472, 0.13410541415214539, -1.2362496852874756, -0.901518702507019, 1.3131482601165771, -0.9905266761779785, -0.3092356026172638, 1.067468285560608, -0.6570079326629639, -0.5522727966308594, 0.09835565835237503, -1.3063420057296753, -0.3845037817955017, -0.4860188364982605, -0.11330495774745941, -0.36789754033088684, -0.17755642533302307, 0.9041159152984619, -0.28257888555526733, -0.24051184952259064, 0.07558117806911469, -0.5424383282661438, -0.22527724504470825, 0.04789440706372261, -0.8375206589698792, 1.1574445962905884, -0.15853188931941986, -0.2101636826992035, -0.015519802458584309, 0.24778254330158234, 0.2139829695224762, -0.4892539083957672, 0.07988828420639038, 0.5905348658561707, -0.7728687524795532, 0.13963600993156433, -0.8307303190231323, -1.1169971227645874, 0.07860559225082397, 0.7993353605270386, 0.4842934012413025, -0.24400091171264648, -0.42627790570259094, -0.9055048227310181, -0.4578339457511902, -1.1164379119873047, -0.11488628387451172, 0.7554240822792053, -0.5398903489112854, -0.3654823303222656, -0.2491878718137741, -0.014094761572778225, -0.6985240578651428, -0.5385298728942871, -0.1133345440030098, 0.33416393399238586, -0.3690434992313385, 1.018356204032898, -0.35656794905662537, 0.08143878728151321, 0.6559258699417114, 0.011128775775432587, -0.5884230136871338, -0.2538326382637024, -0.9238568544387817, 0.3772575259208679, 0.03505217656493187, 0.4016961455345154, -1.0462340116500854, 0.6607205867767334, 0.44102099537849426, 0.09912966936826706, -0.420409619808197, -0.7518748641014099, -0.4493979513645172, -0.5589913129806519, -0.3561266362667084, 0.1588757038116455, 0.0869106650352478, 0.05051659792661667, -0.1629456877708435, 0.6487401723861694, 0.23226474225521088, 0.35503897070884705, -0.8027920722961426, -0.21993012726306915, -0.4429609179496765, 0.08186274766921997, -0.660486102104187, -0.8442989587783813, -1.1378052234649658, -0.5857440829277039, -1.3171136379241943, -0.05795389041304588, -0.7387178540229797, -0.3564883768558502, 0.11433988064527512, -1.137737512588501, 0.2483150213956833, 0.6680006980895996, -0.11613035947084427, -0.7083753943443298, -0.5279467701911926, -0.546850860118866, 0.5316925048828125, 0.7811475992202759, -0.7121521830558777, 0.23272526264190674, 0.1923694610595703, -0.4398704767227173, 0.3351021409034729, 0.6763613224029541, -0.7664282321929932, -0.21352845430374146, -0.7548918724060059, 0.3286168873310089, 0.02205408737063408, 0.3448134958744049, -1.4959455728530884, 1.196824073791504, 0.4889402389526367, 0.08243292570114136, 0.15743887424468994, 0.20791353285312653, -1.1630398035049438, -0.5587266087532043, 0.8229187726974487, -0.6248573064804077, -0.1403232365846634, 0.22387585043907166, -0.31411540508270264, -0.33763381838798523, 1.0247983932495117, 0.49234727025032043, -1.265626311302185, -0.696850061416626, 0.7068671584129333, -0.6750488877296448, 0.3158174753189087, 0.017546292394399643, -0.5828222036361694, -1.3927632570266724, -0.19451944530010223, -0.520072340965271, 0.40106722712516785, -0.5261993408203125, 0.9807624816894531, 0.5231155753135681, -1.1797720193862915, 0.31644490361213684, 0.7832857370376587, 0.3832736909389496, -0.005279480013996363, 0.39863911271095276, 0.30589020252227783, -0.011848179623484612, 0.296365350484848, -0.48931121826171875, 0.1902698278427124, 0.04970348998904228, 0.13628333806991577, 1.151888370513916, 0.3126683831214905, 0.28954991698265076, 1.045454740524292, 0.18999679386615753, -0.9231663346290588, 0.47521844506263733, -1.4488672018051147, -0.7027018070220947, -0.14064249396324158, 0.15773552656173706, -0.2912520170211792, -0.2311408668756485, 0.11101324111223221, -0.4457504451274872, 0.271061509847641, 0.4538438618183136, 0.026294050738215446, 0.5462501645088196, 0.4129926264286041, 0.0064913989044725895, 0.6163812279701233, 0.3542150557041168, -1.0530827045440674, -1.4260385036468506, -1.0563418865203857, -0.2973548471927643, 0.367891401052475, 0.31727075576782227, -0.4451866149902344, -1.5457713603973389, 0.5096732974052429, 0.9178739786148071, 0.46928870677948, 0.030426468700170517, 0.030202317982912064, -0.0017067864537239075, 0.43889620900154114, 0.09001724421977997, -0.35635119676589966, 0.01924757845699787, 0.9234121441841125, 1.1279757022857666, -1.047410011291504, -0.4432496130466461, -0.21799063682556152, -0.6751862168312073, 0.6911799311637878, 0.6372523903846741, -0.7886085510253906, 1.0273624658584595, -0.15837596356868744, -0.1970837414264679, -0.2669137120246887, -1.2915648221969604, -0.2713109850883484, 0.6407700777053833, 0.9189868569374084, 0.42719846963882446, 0.03136157616972923, 0.4506378173828125, 0.976720929145813, 0.4435596466064453, 0.3292666971683502, 0.4035835862159729, 0.1117311492562294, -0.6477530598640442, 0.7841173410415649, 0.44233405590057373, 0.9960981607437134, -0.6890061497688293, -0.5889063477516174, 0.37112465500831604, -0.039403848350048065, -0.09492302685976028, 0.39152002334594727, 1.200237512588501, 0.07680491358041763, 0.7739471793174744, -0.03520118072628975, 0.44996997714042664, -0.40629467368125916, -0.6274085640907288, -0.09704692661762238, -1.0337283611297607, -0.4158235192298889, -0.2260497510433197, -0.7765036821365356, -0.4815398156642914, 0.511921763420105, 0.15014848113059998, -0.5264062881469727, 0.85813307762146, 0.4892723262310028, 0.6073617339134216, 0.6601774096488953, 0.012036922387778759, -0.6469823122024536, -0.5101064443588257, -0.7355971336364746, 0.08547268062829971, -0.40610453486442566, -0.2297026664018631, -0.2922104001045227, -0.522912859916687, -0.4059475064277649]}, "authors": [{"authorId": "2215228167", "name": "Zhaokun Zhou"}, {"authorId": "2277741313", "name": "Kaiwei Che"}, {"authorId": "2277871560", "name": "Wei Fang"}, {"authorId": "2277741366", "name": "Keyu Tian"}, {"authorId": "2277813840", "name": "Yuesheng Zhu"}, {"authorId": "2277903531", "name": "Shuicheng Yan"}, {"authorId": "2275270707", "name": "Yonghong Tian"}, {"authorId": "2152193547", "name": "Liuliang Yuan"}], "references": [{"paperId": "0127ff209c2ee559c0a68daefc3fc9f263a6941f", "title": "Spike-driven Transformer"}, {"paperId": "8fa44efea525b8bd930440221c334ff7646efa98", "title": "Fast-SNN: Fast Spiking Neural Network by Converting Quantized ANN"}, {"paperId": "cc562c1b86202ba6f80a9218df4be5bb3a12464b", "title": "Enhancing the Performance of Transformer-based Spiking Neural Networks by SNN-optimized Downsampling with Precise Gradient Backpropagation"}, {"paperId": "ecf25d6ea576663ad148de9c721b69e29f0a2db0", "title": "Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "5682daa63e1ffa2153384ba40a4a204b2bdc5446", "title": "Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks"}, {"paperId": "9f52317ea9c5a6804b978987ff2a6557f98b5b2c", "title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "5556d6e4c24f1793459407eb1ee623d9549281c9", "title": "Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes"}, {"paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458", "title": "Symbolic Discovery of Optimization Algorithms"}, {"paperId": "9b0187d4bc20bc7ede89d28e7567c5728b095831", "title": "Meta neurons improve spiking neural networks for efficient spatio-temporal learning"}, {"paperId": "e45c0a415b01b13d25e7dbced9f651261be0feaf", "title": "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling"}, {"paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d", "title": "RT-1: Robotics Transformer for Real-World Control at Scale"}, {"paperId": "ffd91f85d6d19e75309ececfd190afa6bb562284", "title": "Spikformer: When Spiking Neural Network Meets Transformer"}, {"paperId": "9c5cd94bbe92b831b12149b9504c25afc8d00d3d", "title": "Attention Spiking Neural Networks"}, {"paperId": "82f2c390ba52228b48254423821b40c6b25dccb2", "title": "Signed Neuron with Memory: Towards Simple, Accurate and High-Efficient ANN-SNN Conversion"}, {"paperId": "7d0fb504fa780cdb5312afab79c00643dd5b9a02", "title": "Toward a realistic model of speech processing in the brain with self-supervised learning"}, {"paperId": "dad156b4f365dd32fa3d57dbaa4095ed7f9154e5", "title": "Spiking Transformers for Event-based Single Object Tracking"}, {"paperId": "e95e4ee85cbe79ef6f5bd0acf3a06be13d8b4fbe", "title": "Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation"}, {"paperId": "92d9762cbd367bd2a7a37c60e3b1546b77ab9f95", "title": "Neuromorphic Data Augmentation for Training Spiking Neural Networks"}, {"paperId": "0faf7b08fc18a22ccd0eac8fae0eee13913755ba", "title": "Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "c74f0e3bc7eaf29a628e7ffe71a4f31b714f53b9", "title": "Spiking Transformer Networks: A Rate Coded Approach for Processing Sequential Data"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "4fa7547167d67be36f80502086a9a954f7408a60", "title": "Spiking Deep Residual Networks"}, {"paperId": "6da4a1004918a4d94e549b71ce459cc40b4f7afe", "title": "Optimizing Deeper Spiking Neural Networks for Dynamic Vision Sensing"}, {"paperId": "a8ae5a8ebb77b4790f4c087f57340760dbd780fa", "title": "HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise"}, {"paperId": "649b706ba282de4eb5a161137f80eb49ed84a0a8", "title": "UFO-ViT: High Performance Linear Vision Transformer without Softmax"}, {"paperId": "09bbd25646371bb4c58e26866be7c374a829b84e", "title": "Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State"}, {"paperId": "238c2c55bf23dd516e6c1de6aa2f0c25516440c1", "title": "Temporal-wise Attention Spiking Neural Networks for Event Streams Classification"}, {"paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1", "title": "Early Convolutions Help Transformers See Better"}, {"paperId": "1fb10189c500e4902cd1b5afd406f57323d21be8", "title": "VOLO: Vision Outlooker for Visual Recognition"}, {"paperId": "93538fea732a77e7ff8a5281eeba0aaaa9094876", "title": "The functional specialization of visual cortex emerges from training parallel pathways with self-supervised predictive learning"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "aadb056dafbc7a5d3900dbf4b8549da76d56000c", "title": "Sparse Spiking Gradient Descent"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "4b06c7e29280b1c6bc05c9df39023b48fef02c93", "title": "Escaping the Big Data Paradigm with Compact Transformers"}, {"paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae", "title": "An Empirical Study of Training Self-Supervised Vision Transformers"}, {"paperId": "96da196d6f8c947db03d13759f030642f8234abf", "title": "DeepViT: Towards Deeper Vision Transformer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "35fcc65db264d9c73ed19d0f3f9d53a43991bdbf", "title": "Deep Residual Learning in Spiking Neural Networks"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "6f6f73e69ee0d9d5d7d088bb882db1851d98175a", "title": "Pre-Trained Image Processing Transformer"}, {"paperId": "251795393f3f94279d531367da1fe81e0ab65b9e", "title": "LIAF-Net: Leaky Integrate and Analog Fire Network for Lightweight and Efficient Spatiotemporal Information Processing"}, {"paperId": "e5ac393af264eb99484973c565ce2e1f12c85e18", "title": "Going Deeper With Directly-Trained Larger Spiking Neural Networks"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "52f0a2d68c39342230be7c9ab46dea71738949a9", "title": "DIET-SNN: Direct Input Encoding With Leakage and Threshold Optimization in Deep Spiking Neural Networks"}, {"paperId": "445fd15f64650d2cfd58833ec6642235888ee584", "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks"}, {"paperId": "4d45165f3c82b6bb52fe16ffcdaf877cb1e4b994", "title": "Progressive Tandem Learning for Pattern Recognition With Deep Spiking Neural Networks"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "e692ab8e6d1b1efb11b24ca31bd9ac53caa021a6", "title": "Unsupervised neural network models of the ventral visual stream"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "ba9b36b17874b2f03d6247bba9bdc7fba8fb18f2", "title": "Efficient Processing of Spatio-Temporal Data Streams With Spiking Neural Networks"}, {"paperId": "6dca2ee4cda6c75ba18e5efbcf8b1930c7c04ed8", "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation"}, {"paperId": "8aad18f5c474cd25e283a7b3c956894d94f82873", "title": "RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network"}, {"paperId": "0289d774bd0dc8025005110973d5b8e6439197d8", "title": "Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "26051e0070a59444eefe5bed36627703235405b7", "title": "Segment"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "0ebca477733c79e13900d1b4e00b67a9b7bf8abd", "title": "Towards spike-based machine intelligence with neuromorphic computing"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "56559fcd8b588357051e0d3c46631bdb435b963b", "title": "Towards artificial general intelligence with hybrid Tianjic chip architecture"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "eaaaed86d1b811fb4690e20ec532d4298c10e324", "title": "Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures"}, {"paperId": "409e5a3b7c25cd0cfcf0ff37cc47f7625c09f2fb", "title": "Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based optimization to spiking neural networks"}, {"paperId": "2ed7baf3dc26de1f934c938e2a806c28eea445a7", "title": "Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)"}, {"paperId": "67b12aa7cde36523780c6be84afb9b95ddbd078a", "title": "Direct Training for Spiking Neural Networks: Faster, Larger, Better"}, {"paperId": "3fe019f5ee31a4be3fe24b687f555006e17d27c5", "title": "SLAYER: Spike Layer Error Reassignment in Time"}, {"paperId": "a67b7966cd8cdff0c9195cf719ac8ebf38c40b8e", "title": "Loihi: A Neuromorphic Manycore Processor with On-Chip Learning"}, {"paperId": "915cc4b359863f256957485c8a60f2cceb78ab5f", "title": "Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1d1802480eeb320d2f48bccee669c02804dbaa99", "title": "Spatio-Temporal Backpropagation for Training High-Performance Spiking Neural Networks"}, {"paperId": "e72b7962133921fa3e84299cd6a4a2aeb60bab19", "title": "CIFAR10-DVS: An Event-Stream Dataset for Object Classification"}, {"paperId": "c99537a3f0ce446df31942b78fb8337edb0c0c07", "title": "Training Deep Spiking Neural Networks Using Backpropagation"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "0a76cdb10b244d90770888f72ee46ca3ea1d918d", "title": "Spiking Deep Networks with LIF Neurons"}, {"paperId": "07e89af6769c4ed9c77dcaf034c8c458b76e8294", "title": "Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "680a38e8f025685b192e9e0cf755c6b664963551", "title": "A million spiking-neuron integrated circuit with a scalable communication network and interface"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "689bc24f71f8f22784534c764d59baa93a62c2e0", "title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer"}, {"paperId": "4dac091ce96bb8c945c20cf3c5902bea40e6cee4", "title": "How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?"}, {"paperId": "4d90f6a0c79f62bf9a607766c9c0e7a8d36215b9", "title": "MCMAE: Masked Convolution Meets Masked Autoencoders"}, {"paperId": "cd8d745c792037c57a352390906f933c5288511c", "title": "Spike Transformer: Monocular Depth Estimation for Spiking Camera"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "47cb7e4ceb17d6edebefd0ea62cca9274e538502", "title": "Advancing Residual Learning towards Powerful Deep Spiking Neural Networks"}, {"paperId": "5a9bc55f6332e38f62eb509b684147a1d4f10fd9", "title": "Focal Attention for Long-Range Interactions in Vision Transformers"}, {"paperId": "abe49fdbee367d332008501bd07c950e7ce2df38", "title": "Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "9ef2c863159e9f9a79000e3c35cb3f41536897fd", "title": "Networks of Spiking Neurons: The Third Generation of Neural Network Models"}]}