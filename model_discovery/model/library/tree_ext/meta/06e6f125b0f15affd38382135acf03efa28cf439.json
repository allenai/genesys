{"paperId": "06e6f125b0f15affd38382135acf03efa28cf439", "title": "A Quantitative Review on Language Model Efficiency Research", "abstract": "Language models (LMs) are being scaled and becoming powerful. Improving their efficiency is one of the core research topics in neural information processing systems. Tay et al. (2022) provided a comprehensive overview of efficient Transformers that have become an indispensable staple in the field of NLP. However, in the section of\"On Evaluation\", they left an open question\"which fundamental efficient Transformer one should consider,\"answered by\"still a mystery\"because\"many research papers select their own benchmarks.\"Unfortunately, there was not quantitative analysis about the performances of Transformers on any benchmarks. Moreover, state space models (SSMs) have demonstrated their abilities of modeling long-range sequences with non-attention mechanisms, which were not discussed in the prior review. This article makes a meta analysis on the results from a set of papers on efficient Transformers as well as those on SSMs. It provides a quantitative review on LM efficiency research and gives suggestions for future research.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.01768", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "A meta analysis on the results from a set of papers on efficient Transformers as well as those on SSMs provides a quantitative review on LM efficiency research and gives suggestions for future research."}, "embedding": {"model": "specter_v2", "vector": [-0.01323429774492979, 0.5278913974761963, -0.35832837224006653, 0.12992563843727112, -0.2710070312023163, -0.3890479505062103, 0.8631112575531006, -0.33412739634513855, -0.7953963279724121, -0.003467625007033348, 0.3110025227069855, -0.4092482328414917, 0.15197642147541046, 0.06696946918964386, -0.13683773577213287, 0.21817727386951447, -0.30704763531684875, 0.46726125478744507, -0.34087124466896057, 0.14925773441791534, -0.13369248807430267, -0.3101969361305237, -0.914853036403656, -0.5750792026519775, 0.37644925713539124, 0.36684176325798035, 0.4718088209629059, 0.7006215453147888, -0.747435450553894, 0.4962177276611328, 0.5338636040687561, -0.20383019745349884, -0.26203933358192444, 0.0055392528884112835, -0.11679326742887497, -0.577217161655426, 0.16091172397136688, -0.04183666408061981, -0.677500307559967, 0.957675039768219, 0.02477320097386837, 0.535333514213562, 0.3996925354003906, -0.6023918390274048, -0.018873214721679688, 0.5994275212287903, 0.585693895816803, 0.8089043498039246, -0.17644451558589935, -0.10147516429424286, 1.085192084312439, -0.9901262521743774, 0.3214662969112396, 1.2762527465820312, 0.45179691910743713, 0.1890278309583664, -0.12184247374534607, -0.5587650537490845, 0.2438362091779709, -0.028732188045978546, -1.0506082773208618, -0.4129350185394287, -0.3270126283168793, 0.194546639919281, 2.1684134006500244, -0.025765063241124153, 0.022887451574206352, 0.25084012746810913, -0.2139938324689865, 1.0117839574813843, 0.23563025891780853, -0.6463164687156677, -0.14070577919483185, 0.39811211824417114, 0.5502925515174866, 0.7922682762145996, -0.29781457781791687, 0.19395935535430908, -0.8406476974487305, -0.08713005483150482, -0.012681023217737675, -0.24827946722507477, -0.1497061550617218, -0.08429785817861557, -0.21311867237091064, 0.7413347959518433, 0.04720127582550049, 0.7647659182548523, -0.09254536777734756, 0.6696879267692566, 0.4695703089237213, 0.30871161818504333, 0.2283838391304016, 0.5561302304267883, -0.1947551667690277, 0.06951404362916946, -0.948773205280304, -0.01567949913442135, 0.12027665972709656, 0.8031112551689148, -0.5742648243904114, 0.46982207894325256, -0.36610445380210876, 0.33818289637565613, 1.4705309867858887, 0.44186994433403015, 0.5205091238021851, -0.9343246221542358, 0.03736165910959244, -0.3964671492576599, 0.03208419680595398, -0.4551079571247101, -0.4020748734474182, -0.4173082411289215, -0.6718801259994507, -1.3465166091918945, -0.6949924230575562, 0.4228716194629669, -0.5603870153427124, 0.8920258283615112, -0.6677345037460327, -0.0746842473745346, -0.05136340484023094, 0.3227417767047882, 0.6408605575561523, 1.184159278869629, 0.4812852740287781, -0.4490516781806946, 0.7876178622245789, -0.7943845391273499, -1.1904844045639038, -0.7417541146278381, 0.9593325257301331, -0.06747603416442871, 0.6424180865287781, -0.2129105031490326, -1.2761362791061401, -0.7656750082969666, -0.2987693250179291, 0.0053949737921357155, -0.28643321990966797, 0.5404537916183472, 0.7427279949188232, 0.4876609444618225, -1.0658719539642334, 0.2354450523853302, -0.5417364239692688, -0.684390664100647, 0.017350081354379654, 0.31796157360076904, 0.1617288589477539, -0.14683909714221954, -1.0983766317367554, 1.1156755685806274, 0.18301035463809967, -0.5249812602996826, -0.4269031584262848, -0.2840029001235962, -1.279012680053711, 0.17012450098991394, 0.12712007761001587, -0.7150481939315796, 1.107763648033142, -0.04748854041099548, -1.6064484119415283, 0.3935655355453491, -0.22675776481628418, -0.029596379026770592, 0.4671628177165985, -0.17974218726158142, -0.5387921929359436, -0.5972026586532593, -0.31847983598709106, 0.38914239406585693, -0.10151814669370651, 0.08835180103778839, -0.2486969381570816, 0.582242488861084, -0.48086702823638916, -0.11229011416435242, -0.3776288330554962, 1.2237573862075806, -0.46593692898750305, 0.08948928862810135, 0.16449099779129028, 0.32247015833854675, -0.36568936705589294, -0.007156175561249256, -0.2931598424911499, -1.0215438604354858, 0.5546405911445618, -0.39558783173561096, 1.4825729131698608, -0.6499917507171631, -0.5463548302650452, -0.33678650856018066, -0.27324217557907104, 0.036342281848192215, -0.7314401865005493, 0.46214163303375244, -0.1628824919462204, 0.7072981595993042, -0.4542671740055084, -1.1039295196533203, 0.1918070912361145, -0.2088211476802826, -0.8274908065795898, 0.14133694767951965, -0.04929313063621521, 0.8969248533248901, -0.6541492938995361, 0.19892604649066925, 0.47093281149864197, 0.3970496356487274, -0.658474326133728, 1.2970296144485474, -0.11834260821342468, -0.09744396805763245, 0.44076356291770935, -0.5823022127151489, 0.2864634096622467, -0.5173158049583435, 0.4021066725254059, -0.21295461058616638, -0.5361946821212769, 0.4881961941719055, -0.4165306091308594, 1.3728578090667725, -0.7745012640953064, 0.9300118088722229, -0.05141926184296608, -0.2826060652732849, -0.034861642867326736, 0.39766067266464233, -0.4873664379119873, -0.7556336522102356, 0.20182304084300995, 0.37067458033561707, -0.48230496048927307, 0.24525579810142517, 0.4888099431991577, 0.975554883480072, -0.18682527542114258, 0.6241810917854309, 0.3900298476219177, -0.30501043796539307, 0.6095628142356873, 0.5108328461647034, 0.3883418142795563, 0.20048968493938446, 0.6078468561172485, -0.08376418054103851, 0.4576735496520996, -0.7845202088356018, -0.3285605013370514, 0.5285668969154358, 0.47732073068618774, 0.4185944199562073, 0.13450489938259125, -0.5162810683250427, -0.5963000655174255, -0.3476809859275818, 0.8607187867164612, 1.3369619846343994, -0.1763860285282135, -0.36045143008232117, -0.8754130005836487, -0.19000299274921417, -0.7775501608848572, 0.6388749480247498, 0.05759987607598305, -0.14962607622146606, -0.7547231912612915, -0.9794541001319885, 1.0631388425827026, 0.13134579360485077, 0.8683580160140991, -0.7631954550743103, -0.22023265063762665, -0.6430566310882568, 0.15198329091072083, -0.9655094742774963, -0.18808607757091522, -0.008695990778505802, -0.7950921654701233, -0.24890834093093872, 0.13871639966964722, 0.18362705409526825, 0.04621778056025505, -0.4783477783203125, 0.9902653694152832, -0.7061246037483215, 0.17553216218948364, 0.16281913220882416, 0.8396632075309753, -0.7362917065620422, -1.0345343351364136, 0.2346639335155487, 0.5382755994796753, -0.7027236819267273, 0.12724363803863525, 0.707848072052002, 0.44209274649620056, -0.07649299502372742, -0.4225272238254547, 0.2845618426799774, 0.3149886131286621, -0.04411466792225838, 0.27794113755226135, -0.24870766699314117, 0.048639487475156784, -0.8224077224731445, 1.1606346368789673, 0.1824098825454712, -0.8277126550674438, 0.7042618989944458, -1.0074702501296997, 0.08671892434358597, 0.21457163989543915, -0.2562759518623352, -0.33343273401260376, -0.7926833629608154, 0.154556006193161, -0.36775198578834534, -0.060328226536512375, 0.1913674920797348, 0.4446280002593994, 0.34604349732398987, -0.13288073241710663, 0.9561826586723328, 0.4426621198654175, -0.3868931233882904, 0.27399492263793945, -0.2139539271593094, 0.052831992506980896, 0.8903225660324097, 0.0011732431594282389, -0.3609280288219452, -0.4806210398674011, -0.8203240633010864, -0.04346057027578354, -0.009401449002325535, 0.47762417793273926, -0.18850025534629822, -0.15048037469387054, -0.6720900535583496, -0.8310297131538391, -0.09167158603668213, -1.1144872903823853, -0.08311256021261215, 0.29085633158683777, 0.000993563560768962, -0.22100356221199036, -0.87115877866745, -1.0487496852874756, -1.1339857578277588, -0.7895854115486145, -0.9115834832191467, 0.3133118748664856, 0.034628260880708694, -0.5268819332122803, -0.5928229689598083, 0.37485018372535706, -0.208334282040596, 1.0299265384674072, -1.0417840480804443, 0.658807098865509, -0.08374757319688797, -0.05685150995850563, -0.09633390605449677, 0.2628263533115387, 0.41312846541404724, -0.26656317710876465, 0.4012241065502167, -1.0027533769607544, 0.1873951554298401, -0.23862522840499878, 0.10834251344203949, 0.17486749589443207, 0.7548747062683105, 0.7148140072822571, -0.09099239856004715, -0.5321271419525146, 0.21563690900802612, 1.0310282707214355, -0.9172060489654541, 0.021016577258706093, 0.16160723567008972, 0.90665203332901, 0.30475661158561707, -0.27956533432006836, 0.33760973811149597, 0.06250425428152084, 0.6253134608268738, -0.0880890041589737, -0.18111513555049896, 0.23543068766593933, -0.41671305894851685, 0.6810548901557922, 1.8687015771865845, 0.322746604681015, -0.3331402540206909, -0.8011916279792786, 0.2552243173122406, -0.9887606501579285, -0.6544457674026489, 0.5798359513282776, 0.7562852501869202, 0.44970080256462097, -0.3678582012653351, -0.008904721587896347, 0.10443273186683655, 0.6764534115791321, 0.46736443042755127, -0.21412381529808044, -0.6785281300544739, -0.13253499567508698, 0.16692017018795013, 0.3746180832386017, 0.7178098559379578, -0.38140955567359924, 0.6158091425895691, 15.318037033081055, 1.2362852096557617, 0.01734798774123192, 0.7764286994934082, 0.5719619393348694, -0.11473476886749268, -0.18635383248329163, -0.1462325155735016, -0.8977682590484619, 0.26187393069267273, 1.5667550563812256, -0.05885881185531616, 0.6808632612228394, 0.14056476950645447, 0.45330187678337097, 0.024908151477575302, -0.37512996792793274, 1.148945689201355, 0.24893134832382202, -1.1647051572799683, 0.3674951195716858, 0.04877076670527458, 0.24206621944904327, 0.633061408996582, 0.7404149770736694, 0.7900831699371338, 0.22010481357574463, -0.6416634917259216, 0.7588990330696106, 0.4737699329853058, 1.1407506465911865, -0.21385815739631653, 0.48925119638442993, 0.9176703095436096, -0.5803648829460144, -0.18458965420722961, -0.3784860074520111, -1.0997899770736694, 0.24406932294368744, 0.17538653314113617, -0.5451087951660156, -0.7277771830558777, -0.4047558307647705, 0.5704912543296814, 0.18470534682273865, 0.4022522568702698, -0.23831920325756073, 0.9521288275718689, -0.6337339878082275, -0.08492554724216461, 0.10927664488554001, 0.311801552772522, 0.14261788129806519, 0.4051225185394287, 0.13346129655838013, 0.10319911688566208, -0.1317276656627655, 0.6293538212776184, -0.6384245753288269, 0.14453133940696716, -0.3885568082332611, -0.7271605730056763, 0.3134934902191162, 0.6660496592521667, 0.3551640510559082, 0.19440995156764984, -0.20027899742126465, -0.14637897908687592, 0.25730445981025696, 0.12919369339942932, -0.40506649017333984, 0.031039351597428322, 0.4946821331977844, -0.40694060921669006, -0.5469167828559875, 0.6241357922554016, -0.23059341311454773, -0.45909130573272705, -0.5719701051712036, -0.5616996884346008, 0.18648137152194977, -1.0194480419158936, -0.5750522613525391, 0.6999720335006714, -0.3618500530719757, -0.22856956720352173, 0.35384175181388855, -0.47110748291015625, 0.11769687384366989, 0.43964725732803345, -1.4292858839035034, -0.5455347895622253, 0.5527617931365967, -0.5897454619407654, -0.17293541133403778, -0.387057363986969, 1.00770103931427, 0.06565766036510468, -0.6556817889213562, -0.042168110609054565, 0.26047268509864807, -0.22255872189998627, -0.41515055298805237, -0.3169327676296234, 0.8181613683700562, 0.16067636013031006, 0.02951933443546295, 0.4744785726070404, 0.45025384426116943, 0.21880130469799042, -0.5264832377433777, -0.027939876541495323, 0.841139554977417, -0.9426069259643555, -0.38409262895584106, -0.49309682846069336, -0.5768851637840271, 0.17824488878250122, 0.1044832319021225, -0.5188499689102173, 0.21974800527095795, -0.23960061371326447, -0.26439255475997925, 0.2171725034713745, -0.6391721963882446, 0.12290358543395996, 0.5595585703849792, -0.9141972064971924, -0.23477323353290558, 0.0707118883728981, 0.3597140908241272, -0.5406075716018677, -0.23356226086616516, 0.2415514439344406, -0.12466991692781448, 0.34899622201919556, 0.6532986164093018, -0.49200135469436646, 0.3935343623161316, 0.40803614258766174, 0.07314489036798477, -0.5134744048118591, 0.04714211821556091, -0.768815815448761, -0.3991290032863617, -0.4691709578037262, 0.5604926347732544, -0.4307180345058441, -0.04068003222346306, 0.7769900560379028, 0.16545359790325165, -0.15189729630947113, -0.8175374269485474, -0.45835599303245544, -0.2675086557865143, -0.37523820996284485, 0.2587626576423645, -0.10685490816831589, -0.33437106013298035, 0.2531279921531677, 0.5211673974990845, 0.47721144556999207, -0.4002334773540497, -0.664735734462738, 0.16582094132900238, -0.3773256242275238, 0.15837065875530243, -0.9970264434814453, -0.16708612442016602, -1.3092366456985474, 0.4453822672367096, -0.9722955822944641, -0.08854355663061142, -1.1741373538970947, 0.09403456747531891, 0.1370059996843338, -0.21529793739318848, -0.2913728952407837, 0.4317009449005127, -0.6563936471939087, -0.355798602104187, -0.11514972150325775, -1.0404852628707886, 0.8262840509414673, 0.6188125610351562, -0.7217522859573364, -0.027319112792611122, -0.20064300298690796, 0.5686008334159851, 0.530465304851532, 0.40949517488479614, -0.8017699122428894, -0.806084930896759, -1.0776524543762207, 0.5773359537124634, -0.10770760476589203, -0.4089744985103607, -0.47043994069099426, 0.5734794735908508, 0.29702815413475037, -0.45328637957572937, 0.11359689384698868, 0.41112956404685974, -0.743516743183136, 0.08410285413265228, 0.503463089466095, -1.0894134044647217, 0.39748474955558777, 0.09432553499937057, -0.526155948638916, -0.2106335610151291, 0.4114183187484741, 0.06665945798158646, -0.7373800873756409, -0.42632779479026794, 0.23401477932929993, -0.8107090592384338, 0.10696032643318176, -0.03974587470293045, -0.2913939654827118, -0.7206922769546509, -0.3484498858451843, -0.2510337233543396, 0.0668313056230545, -0.32317468523979187, 0.7173451781272888, 0.16727864742279053, -0.9150674939155579, 0.20677411556243896, 0.04469799995422363, -0.16038115322589874, -0.565373957157135, -0.005246122367680073, 0.6122214794158936, -0.7066464424133301, 1.1101038455963135, 0.4910070598125458, 0.29764634370803833, -1.0683672428131104, -0.275001585483551, 1.1299352645874023, -0.5446833372116089, 0.0058263554237782955, 0.8857871294021606, -0.13207727670669556, -1.2728722095489502, 0.16747233271598816, -1.000037431716919, -0.533544659614563, -0.48761656880378723, 0.38033124804496765, 0.5124704837799072, -0.2137402892112732, 0.003661439986899495, -0.46860501170158386, -0.09064042568206787, 0.4015752971172333, -0.6034883260726929, 0.7435379028320312, -0.239688903093338, -0.3705187737941742, 0.6939250826835632, 0.6431812047958374, -0.7846927046775818, -0.44471341371536255, -0.5747578740119934, 0.0372186042368412, 0.06902331113815308, 0.41865959763526917, -0.3903794288635254, -0.45974001288414, 0.7280957698822021, 0.7440938353538513, 0.0647834911942482, 0.2694525122642517, -0.23788630962371826, 0.023533685132861137, 0.8169326186180115, 0.29695937037467957, -0.9679468274116516, -0.9906976222991943, 1.4493623971939087, 1.2510384321212769, -1.116578221321106, 0.547476589679718, -0.1034328043460846, -0.5858776569366455, 0.521416425704956, 0.028228018432855606, 0.22231945395469666, 0.8076208829879761, -0.12898218631744385, -0.338882714509964, 0.212431862950325, -0.9788033366203308, -0.05227240175008774, 0.4011303782463074, 0.41770312190055847, 0.7753154635429382, 0.34275904297828674, -0.2330930531024933, 0.9658157825469971, 0.06653021275997162, 0.34068983793258667, 0.37074726819992065, 0.3889109194278717, -0.17655377089977264, 0.02722645364701748, 0.06719931215047836, 0.24580667912960052, -0.4070075452327728, -0.7402918338775635, -0.11727681010961533, 0.4840056002140045, -0.4134094715118408, 0.45980802178382874, 0.8815377950668335, 0.29107168316841125, 0.3523515462875366, 0.36054834723472595, 0.281497597694397, -0.5903760194778442, -0.34366077184677124, -0.2846267521381378, -0.3635624051094055, -0.13143712282180786, -0.46280741691589355, -0.46496257185935974, -0.20375795662403107, -0.5097427368164062, -0.22283321619033813, 0.343548446893692, 0.7018847465515137, 0.9221555590629578, 0.8429507613182068, 0.253185898065567, -0.4984126091003418, -7.140467641875148e-05, -0.3953571915626526, -1.3176920413970947, 0.08587969094514847, -0.6029707193374634, -0.4492023289203644, -0.15243245661258698, -0.41299763321876526, -0.611549437046051]}, "authors": [{"authorId": "144812586", "name": "Meng Jiang"}, {"authorId": "1791955943", "name": "Hy Dang"}, {"authorId": "2147222008", "name": "Lingbo Tong"}], "references": [{"paperId": "b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc", "title": "Liquid Structural State-Space Models"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "238e4958773a5d9d3260f05e2532996b8b7dbaea", "title": "Towards a General Purpose CNN for Long Range Dependencies in ND"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c", "title": "Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "7083181d36fa22a35a511f3eb361fa4ab312de24", "title": "KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering"}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "e79d1206292bc5e67ba19737d87d4b2ea4a37105", "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "e32a12b14e212506115cc6804667b3d8297917e1", "title": "Poolingformer: Long Document Modeling with Pooling Attention"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "1ccf25e9e59ec74c899b7182a619972a196fc138", "title": "No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "c828f4bf1a752700dd2c4a96fdd08ba938cda43d", "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "ea8c46e193d5121e440daf96edfd15a47151c293", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "a93e6c7762125c465c467d4b07b2872369db4ce5", "title": "RikiNet: Reading Wikipedia Pages for Natural Question Answering"}, {"paperId": "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0", "title": "MPNet: Masked and Permuted Pre-training for Language Understanding"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "01b15017ac59b8d6f2ce3598c4a7d6358c211426", "title": "A Divide-and-Conquer Approach to the Summarization of Long Documents"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "af34ea4242ca8725ea739ec1bef674bec10c1fa9", "title": "Time-aware Large Kernel Convolutions"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c7e04335452e988e2be5f1d132e7f6eadad13fd3", "title": "Frustratingly Easy Natural Question Answering"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "eb606d9ce65139754232cee62f6ab77f3e0c665f", "title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "b496b11fb2091678cc2d299cc778046d9a64b0a4", "title": "A BERT Baseline for the Natural Questions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "a14af711aaa3ae83eb64d1f517b024b8c3094a8a", "title": "Trellis Networks for Sequence Modeling"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "fd4ae71916cf400bfd1490f275e91b154eb69160", "title": "Relational recurrent neural networks"}, {"paperId": "853d4d94651c6d9f8ed4d114e1eb21f15f786daa", "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"}, {"paperId": "69ac3b35887eb42e8fe554619fc7255e6e95a4cb", "title": "Fast Parametric Learning with Activation Memorization"}, {"paperId": "921196c32213a229245a9705ee4768bc941e7a26", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"}, {"paperId": "3c78c6df5eb1695b6a399e346dde880af27d1016", "title": "Simple and Effective Multi-Paragraph Reading Comprehension"}, {"paperId": "13395213d47f78672ab4e81573f2b0fa0cfc8c6d", "title": "Challenges in Data-to-Document Generation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "87a913817503379547bec61a5f010abac5b0f76b", "title": "Fast-Slow Recurrent Neural Networks"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "104715e1097b7ebee436058bfd9f45540f269845", "title": "Reading Wikipedia to Answer Open-Domain Questions"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "title": "Quasi-Recurrent Neural Networks"}, {"paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache"}, {"paperId": "98445f4172659ec5e891e031d8202c102135c644", "title": "Neural Machine Translation in Linear Time"}, {"paperId": "563783de03452683a9206e85fe6d661714436686", "title": "HyperNetworks"}, {"paperId": "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "title": "Multiplicative LSTM for sequence modelling"}, {"paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233", "title": "Hierarchical Multiscale Recurrent Neural Networks"}, {"paperId": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "title": "Recurrent Highway Networks"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17", "title": "Generating Sequences With Recurrent Neural Networks"}, {"paperId": "c6dbbd796b7b5f6f12f5518ff2fc5344e8f13de4", "title": "Reformer"}, {"paperId": "44fca068eecce2203d111213e3691647914a3945", "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization"}, {"paperId": "d5f388abda4928b21d6636c05d0487bc41d72e8f", "title": "Table 2"}, {"paperId": null, "title": "Local Attention / Local Attention / Local Attn"}, {"paperId": null, "title": "Model Acc \u2191 Sources Performer in"}, {"paperId": null, "title": "Sinkhorn Trans"}, {"paperId": null, "title": "Transformer (re-impl) / XFM (re-impl)"}, {"paperId": "5665805becad6c87b194b260f2270d86d560bd3f", "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "XFM / Transformer / Transformer / Transformer / Transformer"}, {"paperId": "676b1549adae511164c1b5343f10260fd42035b4", "title": "The Impact of Frequency on Summarization"}]}