{"paperId": "af579b774f9efa6a295b8c81379672aea96d3d32", "title": "An Invitation to Deep Reinforcement Learning", "abstract": "Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning, if the target objective is differentiable. For many interesting problems, this is however not the case. Common objectives like intersection over union (IoU), bilingual evaluation understudy (BLEU) score or rewards cannot be optimized with supervised learning. A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years. Examples include aligning large language models via human feedback, code generation, object detection or control problems. This makes RL techniques relevant to the larger machine learning audience. The subject is, however, time intensive to approach due to the large range of methods, as well as the often very theoretical presentation. In this introduction, we take an alternative approach, different from classic reinforcement learning textbooks. Rather than focusing on tabular problems, we introduce reinforcement learning as a generalization of supervised learning, which we first apply to non-differentiable objectives and later to temporal problems. Assuming only basic knowledge of supervised learning, the reader will be able to understand state-of-the-art deep RL algorithms like proximal policy optimization (PPO) after reading this tutorial.", "venue": "arXiv.org", "year": 2023, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This tutorial introduces reinforcement learning as a generalization of supervised learning, which is first applied to non-differentiable objectives and later to temporal problems."}, "embedding": {"model": "specter_v2", "vector": [-0.0402514711022377, 1.1672815084457397, -0.37646037340164185, 0.14414113759994507, -0.13015766441822052, -0.06464093923568726, 0.9299945831298828, -0.18574847280979156, -0.5670883059501648, 0.4336572587490082, 0.38151654601097107, -0.11619435250759125, 0.23755158483982086, 0.013020763173699379, -0.6020951867103577, -0.30728521943092346, -0.782469630241394, 0.1786898672580719, -0.43987321853637695, -0.26649948954582214, -0.02260972559452057, -0.7279946208000183, -0.8076226115226746, -0.4126812219619751, 0.013865197077393532, 0.3503277003765106, 0.10679814219474792, 0.7437387704849243, -0.227325901389122, 0.6790868043899536, 0.25779059529304504, -0.21646802127361298, 0.6573366522789001, 0.04915665090084076, -0.8077439069747925, -0.004360208287835121, -0.22150659561157227, -0.899381160736084, -0.2840993404388428, 1.0789663791656494, -0.37195050716400146, 0.48270145058631897, 0.24207371473312378, -0.7918633222579956, 0.033764857798814774, 0.6931800246238708, 0.4322507083415985, 0.5000671744346619, -0.009021401405334473, -0.41065332293510437, 1.3936406373977661, -0.526706337928772, 0.3899250626564026, 1.156909465789795, 0.0016118260100483894, 0.40075328946113586, -0.27790963649749756, -0.15763461589813232, 0.8504473567008972, -0.37188369035720825, -0.07944343239068985, 0.45424190163612366, -0.06077102944254875, -0.11510614305734634, 1.3523740768432617, -0.48672589659690857, 0.1221015676856041, 0.9340261220932007, -0.32031744718551636, 1.4942058324813843, 0.43433183431625366, -0.8415367007255554, 0.05679602175951004, 0.35728031396865845, 0.3173123002052307, 0.7842804193496704, -0.41207563877105713, 1.371579647064209, -0.5058788061141968, 0.011407885700464249, 0.4035743474960327, 0.11718077212572098, 0.1403142511844635, -0.6049537658691406, -0.08085273206233978, 0.8443461656570435, 0.24639375507831573, 0.37241801619529724, -0.28068825602531433, 0.9564815759658813, 0.6573811769485474, 0.3221050500869751, -0.38208359479904175, 0.5366659164428711, 0.18953171372413635, 0.19385048747062683, 0.030808323994278908, 0.020946908742189407, -0.2339501827955246, 0.431240975856781, -0.04103908687829971, 0.345267653465271, -0.8392103910446167, 0.10599285364151001, 1.5786136388778687, 0.04215625301003456, 0.17942756414413452, -1.2927031517028809, 0.12703172862529755, -0.7578599452972412, 0.6513740420341492, -0.2916364371776581, -0.005143249407410622, -0.22633719444274902, -0.4081706702709198, 0.053564492613077164, 0.04408862069249153, 0.037645675241947174, -0.5091348886489868, 0.986305832862854, -0.35001838207244873, -0.3923354148864746, 0.403198778629303, 1.2696471214294434, -0.284645140171051, 0.7721298933029175, -0.004368667956441641, 0.006721824873238802, 0.3659989535808563, -0.671638011932373, -0.8970993161201477, -0.5929595828056335, 0.21601814031600952, 0.19716130197048187, 0.25575628876686096, 0.3505263030529022, -1.0418187379837036, -0.890664279460907, -1.2190529108047485, 0.6335499882698059, -0.40207263827323914, 0.2331986427307129, 1.150287389755249, 0.6580102443695068, -0.659698486328125, 1.0204905271530151, -0.2176484763622284, -0.12710681557655334, 0.329584002494812, 0.4997057616710663, 0.33436185121536255, -0.325658917427063, -0.9446552395820618, 0.3918403685092926, 0.5146536231040955, -0.7741823196411133, -0.8909547924995422, 0.0067626554518938065, -0.9596971273422241, -0.3767704665660858, 0.5411796569824219, -0.18445764482021332, 1.5130137205123901, -0.6077826023101807, -1.7010530233383179, 0.4404152035713196, 0.5925244688987732, -0.09069979935884476, 0.6334806680679321, -0.3494007885456085, -0.41077250242233276, -0.49857887625694275, -0.005508938804268837, 0.4848250448703766, 0.43976032733917236, -0.2834402620792389, -0.35392746329307556, 0.23582300543785095, 0.17964735627174377, -0.12572872638702393, -0.5106095671653748, 0.6531020998954773, -0.0304681658744812, -0.18079879879951477, -0.3631461262702942, 0.8013460636138916, -0.09558551013469696, -0.23502786457538605, -0.4483809173107147, -1.2293590307235718, 0.596913754940033, 0.44141602516174316, 0.4160497188568115, -0.6831308603286743, -0.8871697187423706, 0.003858785843476653, -0.27495646476745605, -0.0016762486193329096, -0.8841110467910767, 0.12145198881626129, -0.451930433511734, 0.08827314525842667, -0.0643828958272934, -0.9101790189743042, 0.3111720383167267, 0.15433183312416077, -0.6507927775382996, 0.2604111135005951, 0.16914114356040955, 0.6579740047454834, -1.3972233533859253, 0.36397337913513184, 0.18513835966587067, -0.1572902947664261, -0.7388485074043274, 1.7998229265213013, -0.561874270439148, 0.5144487619400024, -0.28051650524139404, -0.6454587578773499, 0.23336230218410492, -0.12091576308012009, 0.5403712391853333, 0.4115122854709625, -0.12583373486995697, 0.6293942332267761, -0.7802959680557251, 1.387366771697998, 0.07691112160682678, 0.344119668006897, 0.0972486361861229, -1.4070661067962646, 0.5094530582427979, 0.2958332598209381, 0.24281807243824005, -0.6169083118438721, 0.35814377665519714, 0.09050247073173523, -0.9791727066040039, 0.20039860904216766, 0.34753066301345825, 0.7098395228385925, -0.1121814027428627, 0.8770237565040588, 0.49541741609573364, 0.11539172381162643, 0.45700356364250183, 0.45544305443763733, 0.7326238751411438, 1.009714961051941, 0.3393543064594269, -0.014474715106189251, 0.2303905338048935, -0.9206178188323975, 0.06428369134664536, 0.5357573628425598, 0.1344073861837387, 0.6792407631874084, 0.7720059156417847, -0.7960458993911743, -0.24257542192935944, -0.5398159027099609, 1.0296961069107056, 1.286566138267517, 0.15031880140304565, 0.029309075325727463, -0.5558852553367615, -0.9458441138267517, -0.27524325251579285, 0.18677891790866852, -0.6417961716651917, -0.1988944560289383, -0.7897213101387024, -1.0119649171829224, 0.5287176370620728, 0.349136620759964, 0.8853570818901062, -0.6671623587608337, -0.21142980456352234, -0.3684751093387604, 0.4634265601634979, -0.526777982711792, -0.8714163899421692, 0.5457501411437988, -0.24724379181861877, -0.4917272627353668, -0.4182308614253998, 0.006865174043923616, 0.30092138051986694, -0.832966685295105, 0.8177968263626099, -0.7816800475120544, 0.3173602521419525, 0.3500380516052246, 0.575208842754364, -0.8246816992759705, -0.8469145894050598, -0.010036716237664223, 0.26224395632743835, -0.20005059242248535, -0.1667153686285019, 0.7843230962753296, 0.3081195652484894, 0.07327651977539062, -0.47085434198379517, 0.022291578352451324, -0.06865964829921722, 0.44877317547798157, 0.5449346899986267, -0.6196425557136536, 0.2703414261341095, -0.8196918964385986, 0.983833909034729, 0.04040064290165901, -1.113749623298645, 0.13821177184581757, -1.0450642108917236, 0.21911129355430603, 0.39237844944000244, -0.4931252896785736, -0.3146288990974426, -0.5314968228340149, 0.2104036509990692, -0.3759765625, -0.3003281056880951, 0.0161096453666687, 0.6024317741394043, 0.46216967701911926, 0.2475593537092209, -0.1425311118364334, 0.8253780603408813, -0.0938950702548027, 0.5706257224082947, -0.9477925300598145, 0.4715788960456848, -0.0798913985490799, 0.006511852145195007, -0.07982123643159866, -0.18961501121520996, 0.024849751964211464, -0.5761744976043701, -0.07966553419828415, 0.26448750495910645, -0.6946320533752441, 0.08095746487379074, -0.44651103019714355, -1.2150347232818604, -0.18033507466316223, -1.0435324907302856, -0.3583008348941803, -0.6353138089179993, 0.1678583323955536, -0.8594837784767151, -0.8574498891830444, -0.5889766812324524, -0.8654015064239502, -0.1059812381863594, -1.0421030521392822, -0.3311217725276947, 0.4259985387325287, -0.11381179839372635, -0.43224048614501953, 0.4742579758167267, -0.4628392457962036, 0.5548290610313416, -0.6670735478401184, 0.41910746693611145, 0.11155327409505844, -0.3725681006908417, 0.19077101349830627, 0.4053318202495575, 0.6969088912010193, -0.06219379976391792, -0.14829805493354797, -0.6937859058380127, -0.13605399429798126, -0.42613181471824646, -0.9798697829246521, -0.05960820987820625, 0.24784207344055176, 0.6200718283653259, -0.021690599620342255, -0.1345900446176529, 0.18172287940979004, 1.285538673400879, -0.559943437576294, 0.10007131099700928, 0.49487537145614624, 0.7494889497756958, 0.54121333360672, 0.2437930405139923, 0.5733803510665894, 0.004931740462779999, 0.6058164238929749, 0.8139843344688416, -0.23457889258861542, -0.01919325441122055, -0.8133505582809448, 0.7712758183479309, 0.635415256023407, 0.40006232261657715, 0.22834868729114532, -0.6174264550209045, 0.7347100377082825, -1.4382027387619019, -0.5510715246200562, 0.9676299095153809, 0.7592333555221558, 0.14665476977825165, -0.09324608743190765, 0.2786597013473511, -0.01948493719100952, 0.8378350138664246, 0.2885269522666931, -0.3770068287849426, -0.9636479616165161, 0.01381754595786333, 0.14820550382137299, 0.23356716334819794, 0.9141933917999268, -0.45720717310905457, 0.11647573113441467, 15.09907054901123, 0.6414726972579956, 0.02547907643020153, 0.35213565826416016, 0.5955148935317993, 0.17048045992851257, -0.1377706527709961, -0.22531236708164215, -0.9643313884735107, -0.22577650845050812, 0.8743433952331543, 0.27258580923080444, 0.9385700821876526, 0.15031780302524567, -0.06743010878562927, -0.12132718414068222, -0.4331929385662079, 0.586312472820282, 0.1871856302022934, -1.334307074546814, -0.10372333973646164, 0.3528843820095062, 0.4057261049747467, 0.7685362100601196, 0.7410684823989868, 0.8724991679191589, 1.0435092449188232, -0.21848353743553162, 0.8411701917648315, 0.10724236816167831, 0.8748547434806824, -0.14929582178592682, 0.3784635663032532, 0.6461934447288513, -0.3204028904438019, -0.40750524401664734, -0.3495151698589325, -1.2325589656829834, 0.27905890345573425, -0.07039815187454224, -0.3811594843864441, -0.3311763107776642, -0.40295082330703735, 0.5541285872459412, 0.8213698267936707, 0.04892132431268692, -0.533530592918396, 0.48566415905952454, -0.4659188687801361, -0.3141714036464691, 0.5308876037597656, 0.10522004216909409, 0.6315155029296875, -0.3047885298728943, -0.3298179805278778, -0.195278599858284, 0.13507334887981415, 0.7606461644172668, -0.6732597351074219, -0.38173672556877136, -0.5268501043319702, -0.13051678240299225, -0.5007529854774475, 0.45250582695007324, 0.94007408618927, 0.49473801255226135, -0.1766311079263687, -0.030187439173460007, 0.814115583896637, 0.24526429176330566, -0.3986017107963562, -0.130798801779747, 0.7641843557357788, -0.4657573699951172, -0.1979614496231079, 0.6676759123802185, -0.25937795639038086, -0.30573251843452454, -0.7208324074745178, -0.02073390781879425, 0.2287876307964325, -0.7987393736839294, -0.6518647074699402, 0.5722379088401794, -0.37127625942230225, -0.7240347266197205, -0.04252113029360771, -0.6881939768791199, 0.00125696521718055, 0.23723982274532318, -1.5566003322601318, -0.002816535532474518, 0.5448107719421387, -0.26944756507873535, -0.9461233615875244, -0.37137794494628906, 1.0463051795959473, -0.11631225049495697, -0.7867578864097595, -0.0234120674431324, 0.2745840549468994, 0.20505277812480927, -0.2512352168560028, -1.2885241508483887, 0.2698208689689636, 0.3195812404155731, -0.10766509175300598, -0.187690868973732, 0.06411798298358917, 0.4694191515445709, -0.17634393274784088, -0.16967087984085083, -0.045415814965963364, -0.7676265239715576, -0.5765888094902039, -0.44769999384880066, -0.4074181616306305, 0.044879693537950516, 0.6337655782699585, -0.03502480685710907, -0.10495630651712418, -0.03129490837454796, -0.3902601897716522, -0.5168831944465637, -0.7129414677619934, 0.07872380316257477, 0.23390547931194305, -0.2404288500547409, -0.4221191704273224, -0.006312365643680096, 0.3257952630519867, -0.7806726694107056, -0.3410241901874542, -0.3437052369117737, -0.12221336364746094, -0.2520545721054077, 1.000256896018982, -0.8771095871925354, 0.5138314962387085, 0.6216458678245544, 0.20960840582847595, -0.851120114326477, -0.12130418419837952, -1.5033599138259888, 0.12479580193758011, 0.24368642270565033, 0.3619380295276642, -0.5362115502357483, 0.6039775013923645, 0.5877891182899475, 0.34919971227645874, -0.1652696579694748, -0.7788991332054138, -0.6634711623191833, 0.17185306549072266, -0.6257815957069397, 0.3286443054676056, -0.24333718419075012, 0.20320604741573334, 0.054669253528118134, 0.03959095850586891, 0.5593019127845764, -0.029198933392763138, -0.6926575303077698, 0.19261054694652557, 0.329969584941864, -0.019081545993685722, -0.6496966481208801, -0.20547513663768768, -1.897124171257019, -0.10880494117736816, -1.264703631401062, 0.13487641513347626, -0.9144318699836731, -0.6230888962745667, 0.04326833784580231, -0.712121307849884, 0.1388915479183197, 0.5558758974075317, -0.7112292051315308, -0.08494574576616287, -0.4299497902393341, -0.7257078886032104, 1.1924233436584473, 0.866129457950592, -0.8048134446144104, 0.18239982426166534, -0.027150411158800125, -0.039331499487161636, 0.301779568195343, 0.6918694972991943, -0.7274672985076904, -0.7189489006996155, -1.1042472124099731, 0.2814544141292572, 0.12795627117156982, -0.06435518711805344, -0.8373696804046631, 0.1684539020061493, 0.179948091506958, -0.05101919174194336, 0.21653218567371368, 0.3535456359386444, -0.8687038421630859, -0.6589509844779968, 0.3686906397342682, -0.9419971108436584, -0.19277937710285187, -0.05353056266903877, -0.13573983311653137, -0.20869627594947815, 0.5330976247787476, 0.28482937812805176, -0.6359520554542542, -0.1809411495923996, 0.39926546812057495, -0.9228847622871399, 0.14499185979366302, 0.026737576350569725, -0.32060423493385315, -1.124140977859497, -0.22066554427146912, -0.11753357201814651, 0.52619868516922, -0.5032026767730713, 0.8129065632820129, 0.40790635347366333, -1.311192274093628, 0.02921989932656288, 0.2955031096935272, 0.009477400220930576, -0.0633544921875, -0.21156471967697144, 0.10334617644548416, -0.1774628460407257, 0.5522933006286621, -0.29774826765060425, 0.30153167247772217, -0.5654204487800598, 0.10553884506225586, 1.072906255722046, -1.0074268579483032, -0.12301933765411377, 0.7588548064231873, -0.40615877509117126, -1.4528794288635254, 0.607930600643158, -0.7175027132034302, -0.7972518801689148, -0.7390193939208984, 0.4389544427394867, 0.18504761159420013, -0.44762343168258667, 0.33756962418556213, -0.14450576901435852, 0.07256579399108887, 0.11812150478363037, -0.5919771194458008, 0.40327826142311096, -0.26545098423957825, 0.13549292087554932, 0.48238176107406616, 0.8390049934387207, -0.9068654775619507, -1.288219690322876, -0.5240548253059387, -0.32795479893684387, -0.28048643469810486, 0.056589867919683456, -0.6840085983276367, -0.9040839672088623, 0.8466222286224365, 1.176235318183899, 0.22128325700759888, 0.1069219559431076, -0.06481830775737762, -0.19416922330856323, 0.4467288553714752, -0.14120350778102875, -1.0937446355819702, 0.021516624838113785, 1.3858307600021362, 1.4943090677261353, -1.0385160446166992, 0.333536833524704, -0.055064596235752106, -0.7229431867599487, 0.7487355470657349, 0.5936815738677979, -0.22281226515769958, 0.661228358745575, -0.2863132953643799, 0.13396014273166656, 0.26931408047676086, -0.6607282161712646, -0.5368184447288513, 0.27411767840385437, 1.3605766296386719, 0.4105735719203949, 0.41713422536849976, 0.011074255220592022, 0.9676218628883362, 0.49044069647789, 0.2595509886741638, 0.7970660924911499, 0.5494208335876465, 0.04853661358356476, -0.4927518367767334, -0.2309720814228058, 0.4504857361316681, -0.6575897336006165, 0.2617369592189789, 0.3697252869606018, 0.38859984278678894, 0.05057411268353462, 0.3749898076057434, 0.6658748388290405, 0.16253474354743958, 0.5010577440261841, -0.541956901550293, 0.5095146894454956, -0.2048282027244568, -0.39420387148857117, -0.1844622641801834, -0.45144718885421753, -0.34769243001937866, -0.38299527764320374, -0.9142318964004517, -0.583067774772644, -0.17504674196243286, 0.3120577037334442, 0.10411915183067322, 0.20770889520645142, 1.0211682319641113, 0.1992577314376831, 0.5285545587539673, 0.05798472464084625, -0.24951352179050446, -0.8491291403770447, -0.8014253377914429, -0.13717716932296753, -0.38331761956214905, 0.03399084880948067, -0.4837341010570526, 0.04151478409767151, -0.3990679383277893]}, "authors": [{"authorId": "2167151828", "name": "Bernhard Jaeger"}, {"authorId": "2258718328", "name": "Andreas Geiger"}], "references": [{"paperId": "f5275c61736781d236abe6700b822f1ea62f982e", "title": "Diffusion Model Alignment Using Direct Preference Optimization"}, {"paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3", "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"}, {"paperId": "7fac180a463c3969921381b2ed5f077163f603ff", "title": "Champion-level drone racing using deep reinforcement learning"}, {"paperId": "d91591feb96936237167c9e569b8f74e0b2bfcc3", "title": "Benchmarking Offline Reinforcement Learning on Real-Robot Hardware"}, {"paperId": "13f775d1b7f9de1168b5d37a2622ee1938bf43cd", "title": "Faster sorting algorithms discovered using deep reinforcement learning"}, {"paperId": "d364f8cb20daf958ebca53a807e79082eff98e78", "title": "Bigger, Better, Faster: Human-level Atari with human-level efficiency"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "a553bf27d801d09f667fe121c0ba9632257f364b", "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"}, {"paperId": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d", "title": "Training Diffusion Models with Reinforcement Learning"}, {"paperId": "b5405ea4dcdd35c0facdbfcad38b4b105535c89a", "title": "A tutorial introduction to reinforcement learning"}, {"paperId": "efa06fe7c6a4abbe465dbea4f7130f45720ac6f0", "title": "Tuning computer vision models with task rewards"}, {"paperId": "f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e", "title": "Mastering Diverse Domains through World Models"}, {"paperId": "ea14465601f45bf50a148563c73c4d6e1971dcb1", "title": "Redeeming Intrinsic Rewards via Constrained Optimization"}, {"paperId": "442ab95eb9cfbc03bb17a27b52313b5d25eaa738", "title": "Discovering faster matrix multiplication algorithms with reinforcement learning"}, {"paperId": "b3e59caf23dd121048779b2edcdff0dd2872cd43", "title": "Human-level Atari 200x faster"}, {"paperId": "02d9dc238ae825e45e728607c3c83b77d07f4017", "title": "Stabilizing Off-Policy Deep Reinforcement Learning from Pixels"}, {"paperId": "b947cfc64517ac2e1b809e74b506e40eb4e76235", "title": "Mastering the game of Stratego with model-free multiagent reinforcement learning"}, {"paperId": "b63be6032f99b6041e92e1fec305f0afba0e1429", "title": "The Phenomenon of Policy Churn"}, {"paperId": "69b80ce5ab6c2263b145b1eaa23664145938f351", "title": "The Primacy Bias in Deep Reinforcement Learning"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "61e7a3d5606043594a8ce377870479f77a6b58c2", "title": "A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems"}, {"paperId": "58adc31df514e1f5816e961856cf1a7fb4795971", "title": "Outracing champion Gran Turismo drivers with deep reinforcement learning"}, {"paperId": "88e488a08dcd629c8ce90099bd8ab0a87f10cafb", "title": "Magnetic control of tokamak plasmas through deep reinforcement learning"}, {"paperId": "25a1732e1166db3b1d74fc934318e9b4c2d16771", "title": "GRI: General Reinforced Imitation and its Application to Vision-Based Autonomous Driving"}, {"paperId": "558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd", "title": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"}, {"paperId": "010ac9acd45c0073e79130e855aad3b65c51a5e1", "title": "End-to-End Urban Driving by Imitating a Reinforcement Learning Coach"}, {"paperId": "e06c005e98281af455c454ce2478285f6f3afeca", "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "3e9fa14b16c3092718f3f9468553e106aa4ac818", "title": "MetricOpt: Learning to Optimize Black-Box Evaluation Metrics"}, {"paperId": "f4fd4a5a0901675e19bb4054d8861d2697194fd5", "title": "Deep Reinforcement Learning: A State-of-the-Art Walkthrough"}, {"paperId": "0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d", "title": "Exploring Simple Siamese Representation Learning"}, {"paperId": "16e83f3f0f78ceb203746eeb88f1f5aae9ba3092", "title": "Deep reinforcement learning: a survey"}, {"paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af", "title": "Mastering Atari with Discrete World Models"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "cbe9be3a9731c13dd18fc7bdfaf8dcedfe7a5544", "title": "What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d415b724fbc35afcc8dd91738123edfa6a5db634", "title": "Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO"}, {"paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890", "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"}, {"paperId": "744139d65c3bf6da6a6acd384a32d94a06f44f62", "title": "Reinforcement Learning with Augmented Data"}, {"paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b", "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"}, {"paperId": "616ac6772508e72084360158078058dfa8bda7e7", "title": "First return, then explore"}, {"paperId": "1b04936c2599e59b120f743fbb30df2eed3fd782", "title": "Shortcut learning in deep neural networks"}, {"paperId": "9651e1987a39d100dc0f6696a2077199e5075ea2", "title": "Agent57: Outperforming the Atari Human Benchmark"}, {"paperId": "273c84b64e16da4842d3c4b438713a81254caf90", "title": "An empirical investigation of the challenges of real-world reinforcement learning"}, {"paperId": "24e0cec8c71421fdb5d002a7776d2b17c5dc975b", "title": "Reward-Conditioned Policies"}, {"paperId": "b19729b27a1b4c24b52f87308c907653300afa7f", "title": "Dota 2 with Large Scale Deep Reinforcement Learning"}, {"paperId": "7a7a7847041e7b25febb1491d65d842a6c65927e", "title": "Training Agents using Upside-Down Reinforcement Learning"}, {"paperId": "72973e49f453f678eb0b79b5fa5311b158f3909d", "title": "Reinforcement Learning Upside Down: Don't Predict Rewards - Just Map Them to Actions"}, {"paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9", "title": "Dream to Control: Learning Behaviors by Latent Imagination"}, {"paperId": "42d0b94c8cf71541e5b52f45ab845b6d47dc4efe", "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances"}, {"paperId": "3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b", "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"}, {"paperId": "361c00b22e29d0816ca896513d2c165e26399821", "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"}, {"paperId": "893a16bfc2e14c4eec45470f76083632470fc41c", "title": "Neural Policy Gradient Methods: Global Optimality and Rates of Convergence"}, {"paperId": "895735cace0de940aa647dbafc046b7f30316fe5", "title": "A survey on intrinsic motivation in reinforcement learning"}, {"paperId": "2ee463bba9d4db6aec0eab17e54431a6dc80bf17", "title": "Superhuman AI for multiplayer poker"}, {"paperId": "2b92bdb31369cea84a294b4e16636f6975b13a1a", "title": "Learning Surrogate Losses"}, {"paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc", "title": "Model-Based Reinforcement Learning for Atari"}, {"paperId": "6dae703128d9caff2623eb8dfe2526dc6ad7aff5", "title": "A Theoretical Analysis of Deep Q-Learning"}, {"paperId": "12c0751b4f51ed833172a713b7e32390032ead93", "title": "Soft Actor-Critic Algorithms and Applications"}, {"paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608", "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"}, {"paperId": "4b61c25a86083c20730c9b12737ac6ac4178c364", "title": "An Introduction to Deep Reinforcement Learning"}, {"paperId": "8ede7ddf99986d69562455bc8d69222fc3e27350", "title": "Recurrent Experience Replay in Distributed Reinforcement Learning"}, {"paperId": "8e372ed2b688de0e4dcffbec1d2abdd0fc7ea27a", "title": "Dopamine: A Research Framework for Deep Reinforcement Learning"}, {"paperId": "41cca0b0a27ba363ca56e7033569aeb1922b0ac9", "title": "Recurrent World Models Facilitate Policy Evolution"}, {"paperId": "2d15a7546c16d5821ffa8f769eb7ec18e435e64d", "title": "Recognition in Terra Incognita"}, {"paperId": "e3088dd1ab6993e3e9bc75e5b9b266139a2570b0", "title": "CIRL: Controllable Imitative Reinforcement Learning for Vision-based Self-driving"}, {"paperId": "d85623ffae865f9ef386644dd02d0ea2d6a8c8de", "title": "Implicit Quantile Networks for Distributional Reinforcement Learning"}, {"paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b", "title": "Addressing Function Approximation Error in Actor-Critic Methods"}, {"paperId": "601a2d349fc26d7b82f905e924e2f91b0ac4b310", "title": "Distributed Prioritized Experience Replay"}, {"paperId": "8a05d72b6f54479c5ab3ceaccd221b2119f0ea7d", "title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning"}, {"paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386", "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"}, {"paperId": "eaabb78d0bc44ed132e4d077e9486c86a9e4cda9", "title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals"}, {"paperId": "811df72e210e20de99719539505da54762a11c6d", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"}, {"paperId": "b542d5f3973970902eab247154f74cc5abb5cbb4", "title": "Time Limits in Reinforcement Learning"}, {"paperId": "36fd42195d46bdc0dc94327f66846505e979e25d", "title": "Deep Reinforcement Learning: A Brief Survey"}, {"paperId": "fe3e91e40a950c6b6601b8f0a641884774d949ae", "title": "Distributional Reinforcement Learning with Quantile Regression"}, {"paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c", "title": "Mastering the game of Go without human knowledge"}, {"paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33", "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"}, {"paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6", "title": "Deep Reinforcement Learning that Matters"}, {"paperId": "1bead9000a719cb258bac7320228055aee650d2c", "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e", "title": "A Distributional Perspective on Reinforcement Learning"}, {"paperId": "3fab4d28be5ff73f42c8b307117aa8d680345ad9", "title": "Noisy Networks for Exploration"}, {"paperId": "d65ce2b8300541414bfe51d03906fca72e93523c", "title": "On Calibration of Modern Neural Networks"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "9f1e9e56d80146766bc2316efbc54d8b770a23df", "title": "Deep Reinforcement Learning: An Overview"}, {"paperId": "a2155552ca5afb784a3c1d67a5bcbd4e688b6e05", "title": "DeepStack: Expert-level artificial intelligence in heads-up no-limit poker"}, {"paperId": "0d24a0695c9fc669e643bad51d4e14f056329dec", "title": "An Actor-Critic Algorithm for Sequence Prediction"}, {"paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd", "title": "Asynchronous Methods for Deep Reinforcement Learning"}, {"paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490", "title": "Mastering the game of Go with deep neural networks and tree search"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "69b647afe6526256a93033eac14ce470204e7bae", "title": "Training Deep Neural Networks via Direct Loss Minimization"}, {"paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca", "title": "Prioritized Experience Replay"}, {"paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e", "title": "Deep Reinforcement Learning with Double Q-Learning"}, {"paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f", "title": "Continuous control with deep reinforcement learning"}, {"paperId": "f5f323e62acb75f785e00b4c90ace16f1690076f", "title": "Deep Recurrent Q-Learning for Partially Observable MDPs"}, {"paperId": "d00e7779c39dc7b06d7d43cf6de6d734c8edc4b8", "title": "Language Understanding for Text-based Games using Deep Reinforcement Learning"}, {"paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8", "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"}, {"paperId": "340f48901f72278f6bf78a04ee5b01df208cc508", "title": "Human-level control through deep reinforcement learning"}, {"paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199", "title": "Trust Region Policy Optimization"}, {"paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94", "title": "Deterministic Policy Gradient Algorithms"}, {"paperId": "f82e4ff4f003581330338aaae71f60316e58dd26", "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"}, {"paperId": "644a079073969a92674f69483c4a85679d066545", "title": "Double Q-learning"}, {"paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e", "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"}, {"paperId": "e60f3c1cb857daa3233f2c5b17b6f111ff86698c", "title": "Algorithms for Reinforcement Learning"}, {"paperId": "4a5f0c5c7d1b404cbb3a717e5581d86ff29025de", "title": "Reinforcement Learning: A Tutorial Survey and Recent Advances"}, {"paperId": "668b1277fbece28c4841eeab1c97e4ebd0079700", "title": "Pattern Recognition and Machine Learning"}, {"paperId": "02cc6a5944d57d2353a55639c7b77336b94f29b6", "title": "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search"}, {"paperId": "c513ca9dd4bbcb62660015ca3e29483a7d403c64", "title": "A Short Tutorial on Reinforcement Learning"}, {"paperId": "4d92df4a844c94fbb31b95157488e4b562b4f681", "title": "The Optimal Reward Baseline for Gradient-Based Reinforcement Learning"}, {"paperId": "33de518ff6ab4261d94544dc029460c6cf00117c", "title": "Exploration in Gradient-Based Reinforcement Learning"}, {"paperId": "1187a77f857ad029168863ba0005ddf6d2b957c8", "title": "Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning"}, {"paperId": "3b90b73fa0f904a2dc84bca4b3f80cbb51d7025f", "title": "Reinforcement Learning with Long Short-Term Memory"}, {"paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b", "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"}, {"paperId": "94066dc12fe31e96af7557838159bde598cb4f10", "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"}, {"paperId": "f608268033a797a38047575e6b4de65899eedd5f", "title": "Simulation-based optimization of Markov reward processes"}, {"paperId": "12d1d070a53d4084d88a77b8b143bad51c40c38f", "title": "Reinforcement Learning: A Survey"}, {"paperId": "5ed59f49c1bb7de06cfa2a9467d5efb535103277", "title": "Temporal difference learning and TD-Gammon"}, {"paperId": "621c03cd67b0b7bac665f3c7887481b4b42f269c", "title": "Asynchronous Stochastic Approximation and Q-Learning"}, {"paperId": "f3e10675b2ef79d8431b8011f909ee0d05e92d92", "title": "Incremental multi-step Q-learning"}, {"paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa", "title": "Q-learning"}, {"paperId": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b", "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching"}, {"paperId": "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772", "title": "Some Studies in Machine Learning Using the Game of Checkers"}, {"paperId": "4a06ec48e4b413ab2563273981018df82faac32e", "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning"}, {"paperId": "c375c33093d2be25bafd894866ad959fab910c00", "title": "CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms"}, {"paperId": "e3fc5b5627af62ee6981a02090cf6bae368202d7", "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations"}, {"paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f", "title": "AUTO-ENCODING VARIATIONAL BAYES"}, {"paperId": null, "title": "Deepmimic: example-guided deep reinforcement learning of physics-based character skills"}, {"paperId": "f8ad22941c633b62573aaee9cee651a9bc895fe5", "title": "DEEP REINFORCEMENT LEARNING"}, {"paperId": null, "title": "Faulty reward functions in the wild"}, {"paperId": "49f67c0ee24f28cf2fe5670e37d195330d833524", "title": "Dynamic"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "9b13e4862e7e914bdaf4928b3fe1e6dcecbc0155", "title": "The Monte-Carlo Method"}, {"paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614", "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"}, {"paperId": "ccecdf227475db9be6f0fa8f431c1db51e9dcd83", "title": "Reinforcement Learning and Its Relationship to Supervised Learning"}, {"paperId": "3d55f759ca4281d96f906cdbec8123cf50a70004", "title": "Off\u2010Policy Actor\u2010Critic\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u5f37\u5316\u5b66\u7fd2"}, {"paperId": "bee570503aaa0ed5bc5dd4cf6aa742df0b5cef87", "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition"}, {"paperId": "ac4af1df88e178386d782705acc159eaa0c3904a", "title": "Actor-Critic Algorithms"}, {"paperId": "26b8747eb4d7fb4d4fc45707606d5e969b9afb0c", "title": "Issues in Using Function Approximation for Reinforcement Learning"}, {"paperId": "97efafdb4a3942ab3efba53ded7413199f79c054", "title": "Reinforcement Learning: An Introduction"}, {"paperId": "5312b96a62d4f942e3896521bdf6d8c8cc8b50ad", "title": "Reinforcement Learning: A Tutorial."}, {"paperId": null, "title": "Advantage updating"}, {"paperId": "6bc8db0c7444d9c07aad440393b2fd300fb3595c", "title": "Function Optimization using Connectionist Reinforcement Learning Algorithms"}, {"paperId": "cc442ec39825c9715f697ff4b025a3d2b6be75e4", "title": "Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy"}, {"paperId": null, "title": "Simplified PPO-Clip Objective"}, {"paperId": null, "title": "The 37 implementation details of proximal policy optimization"}]}