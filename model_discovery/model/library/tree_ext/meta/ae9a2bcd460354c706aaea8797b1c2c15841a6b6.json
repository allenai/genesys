{"paperId": "ae9a2bcd460354c706aaea8797b1c2c15841a6b6", "title": "A Survey on Vision-Language-Action Models for Embodied AI", "abstract": "Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components through pretraining. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.", "venue": "arXiv.org", "year": 2024, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is imperative to capture the evolving landscape of embodied AI through a comprehensive survey of vision-language-action models, reflecting the rapid advancement of embodied AI."}, "embedding": {"model": "specter_v2", "vector": [0.25050732493400574, 0.7243552803993225, -0.34143856167793274, -0.07301721721887589, -0.03182018920779228, 0.03227626532316208, 0.7240022420883179, -0.458769291639328, -0.7176018953323364, -0.3079856336116791, 0.557158887386322, 0.1554049253463745, 0.16698281466960907, 0.1361229419708252, -0.5106742978096008, 0.18345700204372406, -0.7278317213058472, 0.18519680202007294, 0.0788300558924675, -0.6733002662658691, -0.05234277620911598, -0.6041016578674316, -0.9617358446121216, 0.09971654415130615, -0.034335412085056305, 0.1400814801454544, 0.5069831609725952, 1.344710350036621, 0.17545124888420105, 0.7535519003868103, 0.6899235844612122, 0.1746724545955658, 0.1677834391593933, 0.11077660322189331, -0.5927566289901733, -0.04715899005532265, 0.09887184947729111, -0.801956057548523, -1.2058477401733398, 0.896564781665802, -0.5736912488937378, 0.14881740510463715, 0.6127697825431824, -0.8559277057647705, -0.3363640606403351, 0.4630667269229889, 0.8770776987075806, 0.4115445911884308, 0.27773356437683105, 0.05284689739346504, 1.2890416383743286, -0.6997531056404114, 0.36359986662864685, 1.939787745475769, -0.02738744392991066, 1.1494216918945312, -0.06302893906831741, -0.22962762415409088, 0.9748565554618835, -0.14431779086589813, -0.1568300575017929, -0.046490032225847244, 0.08690173178911209, -0.19865289330482483, 1.2859785556793213, -0.6769379377365112, 0.34977567195892334, 1.0240470170974731, 0.548241138458252, 1.7381095886230469, -0.09098877012729645, -1.0343555212020874, -0.14057567715644836, -0.2780495882034302, 0.27804192900657654, 1.0058845281600952, -0.5158882141113281, 0.9316861629486084, -0.7476296424865723, 0.25833144783973694, 0.8122596740722656, 0.06518637388944626, 0.03197959437966347, -0.6584292054176331, -0.7447724938392639, 0.7535440325737, 0.5661870241165161, 0.565041184425354, -0.5611215233802795, 0.94456547498703, 0.26877033710479736, 0.4303397834300995, -0.9034993648529053, 0.42587733268737793, 0.18051119148731232, 0.4993758201599121, 0.003802139777690172, 0.23820534348487854, 0.022326407954096794, 0.6380488276481628, -0.3365037143230438, 0.2890169620513916, -0.8927541375160217, 0.10098890215158463, 1.652702808380127, -0.02088640257716179, 0.188810795545578, -1.1036263704299927, 0.09311061352491379, -0.52699214220047, 0.7169632911682129, -0.5416255593299866, -0.35598310828208923, 0.2825632393360138, -0.2935568690299988, -0.4280775189399719, -0.17025160789489746, 0.09239906072616577, -1.152442216873169, 0.9316174983978271, -0.3037106394767761, -0.4428386092185974, 0.31207165122032166, 0.8541991114616394, -0.022410036996006966, 0.5680227875709534, 0.3816567063331604, 0.5288938283920288, 0.8554721474647522, -0.9031376242637634, -0.9013993740081787, -1.1961785554885864, 0.06725658476352692, 0.2943975329399109, 0.001631594030186534, -0.12393558025360107, -0.8470116853713989, -0.9675989151000977, -1.001389980316162, -0.09198018163442612, -0.233724445104599, 0.5272583961486816, 1.1844418048858643, -0.09986607730388641, -1.0990087985992432, 0.38820189237594604, -0.3198126256465912, -0.2790078818798065, 0.2049230933189392, 0.404812753200531, 0.0026470914017409086, -0.3355502486228943, -0.8526573777198792, 0.5109360814094543, 0.5146998167037964, -0.3097594082355499, -1.073785662651062, 0.4900378882884979, -1.7251157760620117, -0.45402517914772034, 0.1926468014717102, -0.8200274109840393, 1.5944373607635498, -0.3547576367855072, -1.4869545698165894, 0.4508242905139923, -0.2213936299085617, 0.02559414692223072, 0.2554091215133667, -0.5058197379112244, -0.60507732629776, 0.10292821377515793, -0.35551607608795166, 1.3061648607254028, 0.5550428032875061, -0.555356502532959, -0.2702181339263916, 0.1343935877084732, 0.4881468713283539, -0.006608597934246063, -0.07064858078956604, 0.7491883039474487, -0.32738566398620605, -0.13028016686439514, 0.5067492723464966, 0.4472711384296417, -0.05164393037557602, 0.2744128406047821, -0.31850478053092957, -1.0924049615859985, 0.6016839146614075, 0.37629812955856323, 0.44401437044143677, -0.8859541416168213, -0.26686859130859375, -0.4749756157398224, -0.19630111753940582, -0.32676395773887634, -1.2627853155136108, 0.44859474897384644, -0.38441041111946106, -0.040422145277261734, -0.22556214034557343, -0.9693266749382019, 0.03633672744035721, 0.27501577138900757, -0.49435776472091675, 0.16721130907535553, 0.41598770022392273, 1.2814162969589233, -1.1610990762710571, -0.15338240563869476, 0.2929529547691345, -0.12622351944446564, -0.9353334903717041, 1.4519801139831543, -0.8961001038551331, 0.40855926275253296, -0.38228845596313477, -0.40628039836883545, -0.4623109698295593, 0.06329470872879028, 0.3525530695915222, -0.4414087235927582, -0.060349322855472565, 0.2625417709350586, -0.43714383244514465, 1.8697779178619385, -0.15457594394683838, 0.7306684851646423, 0.16316430270671844, -0.5592408180236816, -0.03602767735719681, 0.48928529024124146, -0.32087472081184387, -0.5011427998542786, 0.42947468161582947, -0.08847566694021225, -0.4272013008594513, -0.5106328725814819, 0.24255838990211487, 0.5497297048568726, -0.29788896441459656, 0.2815605103969574, 0.28322193026542664, -0.3765626847743988, 0.3225950598716736, 0.5356878638267517, 0.6080535054206848, 0.9308375120162964, 0.5223449468612671, 0.26244789361953735, 0.3198552131652832, -0.8265275955200195, -0.2592611312866211, 0.6824969053268433, 0.22613446414470673, 0.7213833332061768, 0.008927959017455578, -1.0024610757827759, -0.09702631086111069, -0.1731671392917633, 1.0855573415756226, 1.311153531074524, 0.41030368208885193, 0.43369951844215393, -0.5764850974082947, -0.3451971113681793, -0.5864455103874207, 0.3348456919193268, -0.6898728609085083, -0.3614669144153595, -0.6671703457832336, -0.29516276717185974, 0.687315046787262, 0.7059383988380432, 1.0539913177490234, -1.1759308576583862, -0.5379940271377563, -0.24913617968559265, 0.374517023563385, -0.7799956798553467, -0.7138815522193909, 0.0962119773030281, -0.23277029395103455, -0.46785762906074524, -0.012493075802922249, -0.5050914287567139, 0.36992010474205017, -0.6476839184761047, 0.6946483254432678, -0.7692756056785583, -0.104069285094738, 0.7561939358711243, 0.6272455453872681, -0.9617167115211487, -0.9005423188209534, -0.37780341506004333, 0.1448599100112915, -0.13583345711231232, -0.009816758334636688, 0.9442088603973389, 0.09386055916547775, -0.17838694155216217, -0.7030691504478455, 0.3097552955150604, 0.4423254132270813, 0.1895613670349121, 0.4635748863220215, -1.0664188861846924, 0.2556212842464447, -0.5027306079864502, 1.0406889915466309, 0.48933565616607666, -0.24392737448215485, 0.370206743478775, -0.20623791217803955, -0.028154170140624046, 0.07981731742620468, -0.778321385383606, -0.6947543621063232, -0.28701314330101013, 0.2650439143180847, -0.583137035369873, -1.0052937269210815, 0.2793402671813965, 0.6700366139411926, 0.16617202758789062, 0.23152200877666473, 0.5739293098449707, 0.7512153387069702, 0.3589056432247162, 0.5599415302276611, -0.7818739414215088, 0.7829944491386414, 0.2227104902267456, 0.07996524125337601, -0.011101036332547665, -0.1590820699930191, -0.3971202075481415, -0.49937573075294495, -0.4634513556957245, -0.0054335822351276875, -0.8151077628135681, 0.5990296006202698, -0.43058332800865173, -1.2508649826049805, 0.0949472188949585, -1.217829704284668, -0.5454980731010437, 0.20896349847316742, -0.06318047642707825, -0.7677994966506958, -0.8681500554084778, -0.8477560877799988, -0.9382742643356323, -0.13414320349693298, -0.9521182179450989, 0.110919289290905, 0.4366419315338135, -0.4622633755207062, -0.24641849100589752, 0.2880164384841919, -0.17066259682178497, 0.19140617549419403, -0.7245759963989258, 0.7727846503257751, 0.25342410802841187, -0.19308792054653168, -0.31976813077926636, 0.5935627818107605, 0.24460133910179138, -0.03306439518928528, -0.019293975085020065, -0.6393564343452454, 0.0393608883023262, -0.4814207851886749, -0.6459836959838867, 0.17484714090824127, 0.13768789172172546, 0.6361945867538452, 0.26505497097969055, -0.33946457505226135, -0.15938067436218262, 1.242121696472168, -0.06290172040462494, -0.112986721098423, 0.10368990898132324, 0.9173480868339539, 0.33385610580444336, 0.08045802265405655, 0.35607677698135376, 0.7917410731315613, 0.43354740738868713, 0.8172435164451599, 0.16328977048397064, -0.20150887966156006, -0.6845460534095764, 0.6842108368873596, 0.5613797307014465, 0.3772631287574768, -0.05814617872238159, -0.8349975347518921, 0.9191928505897522, -1.333318829536438, -0.5879175662994385, 1.0698373317718506, 0.3128575086593628, 0.2245427817106247, -0.23134145140647888, -0.08724456280469894, -0.43367838859558105, 0.783663809299469, 0.31160324811935425, -0.5347549319267273, -0.792217493057251, 0.35888680815696716, -0.19277624785900116, -0.570891261100769, 0.8943834900856018, -0.8360286951065063, 0.31741419434547424, 14.476643562316895, 0.062691830098629, 0.11042644828557968, 0.2674579620361328, 0.04901862516999245, 0.5501094460487366, -0.348191499710083, -0.2602645754814148, -0.8305019736289978, -0.5500530004501343, 0.8939794898033142, 0.7627324461936951, 0.6775690317153931, 0.04207444563508034, -0.08470042049884796, -0.23284554481506348, -1.0853458642959595, 0.7557457685470581, 0.5867655873298645, -1.0630358457565308, 0.3747992217540741, -0.19023697078227997, 0.08787887543439865, 0.39121320843696594, 0.8058415651321411, 1.198522686958313, 0.4963262677192688, -0.11142335087060928, 1.0831491947174072, 0.5136381387710571, 0.6638229489326477, 0.2917361557483673, -0.02741776965558529, 1.0953264236450195, -0.8920612931251526, -0.9200733304023743, -0.0398169606924057, -1.085426688194275, 0.3911163806915283, -0.7867246866226196, 0.10532520711421967, -0.33939358592033386, -0.523967444896698, 0.5136493444442749, 0.7364403605461121, 0.24592038989067078, -0.5376372933387756, 0.11192505806684494, -0.2244941145181656, -0.6154309511184692, 0.5058639049530029, 0.26892051100730896, 0.4975350797176361, -0.33939871191978455, -0.37072041630744934, 0.533467173576355, 0.1797141283750534, 0.5724270343780518, -0.23873303830623627, -0.8057724833488464, -0.5724269151687622, -0.3970060646533966, -0.36199286580085754, 0.2744406461715698, 0.6784040331840515, 0.7137851715087891, -0.27565351128578186, 0.18076364696025848, 0.40604135394096375, 0.4880444407463074, -0.2283625304698944, -0.03575562313199043, 0.4326075315475464, -1.0018078088760376, 0.041880421340465546, 0.1907481998205185, 0.08746915310621262, -0.5297437310218811, -0.7068191766738892, -0.3960295617580414, 0.3472064733505249, -1.008523941040039, -0.49119386076927185, 0.6923162937164307, -0.2314169555902481, -0.4368131458759308, 0.17595164477825165, -1.07949960231781, -0.3336199223995209, -0.1311107873916626, -1.2627955675125122, -0.631939172744751, 0.14563581347465515, -0.2259446382522583, 0.002598527120426297, -0.049432966858148575, 1.2324234247207642, -0.3214401304721832, -0.6752161383628845, -0.2114640474319458, -0.5510968565940857, 0.02511429786682129, -0.48721468448638916, -0.6631108522415161, 0.12807326018810272, 0.2836700975894928, 0.02562078833580017, -0.01216977834701538, 0.1385696530342102, -0.03220284357666969, -0.9288177490234375, 0.14601287245750427, 0.2829933166503906, -0.8664087057113647, -0.5342696905136108, -0.2512819468975067, -0.37791240215301514, 0.50754714012146, 0.6724199056625366, -0.1434793472290039, -0.24062414467334747, -0.3145105540752411, -0.3786170780658722, 0.012787370942533016, -0.7295687794685364, 0.3920401930809021, 0.2692053020000458, -0.7845237255096436, -1.0932084321975708, -0.2767990827560425, 0.3168354332447052, -0.9792929291725159, -0.02090359851717949, -0.1940469741821289, 0.3458317220211029, -0.08615893125534058, 1.0233474969863892, -0.9556174874305725, 0.4796158969402313, 0.40202102065086365, 0.2289455533027649, -0.7217369675636292, -0.2639264464378357, -0.7955552935600281, 0.013580030761659145, -0.309247225522995, 0.29719218611717224, -0.4038298428058624, -0.02547808364033699, 0.8320486545562744, 0.14358936250209808, -0.4554600119590759, -0.6323179602622986, 0.014333714731037617, -0.08459803462028503, -0.47729185223579407, -0.013882078230381012, -0.6969199180603027, -0.12375331670045853, 0.16697204113006592, 0.36986076831817627, 0.9116047024726868, -0.38514211773872375, -0.34045931696891785, 0.28372418880462646, 0.2693314552307129, 0.0610392726957798, -0.41565266251564026, -0.05093330517411232, -1.9049352407455444, 0.2432813048362732, -0.9135990142822266, 0.7486127614974976, -1.3784970045089722, -0.4101710915565491, 0.5991091132164001, -0.33048123121261597, 0.4831222593784332, 0.36978137493133545, -0.7586193084716797, -0.05674798786640167, -0.43200919032096863, -0.7228366732597351, 0.9774280190467834, 1.2520627975463867, -0.8514249920845032, 0.1907062530517578, -0.06977003067731857, 0.2805326282978058, 0.5112056732177734, 0.45447903871536255, -0.25448012351989746, -0.8446136713027954, -1.3556599617004395, 0.36738571524620056, 0.29224830865859985, 0.1955457478761673, -1.2952407598495483, 0.9032986760139465, 0.08315203338861465, -0.19566862285137177, -0.0618826188147068, 0.8088356256484985, -1.1738989353179932, -0.8718344569206238, 0.7431639432907104, -1.1968334913253784, -0.13475632667541504, 0.27186843752861023, 0.12325179576873779, -0.4561418294906616, 0.6030287742614746, -0.09094402939081192, -0.8526563048362732, -1.2416473627090454, 0.4733133316040039, -1.1022140979766846, -0.25876379013061523, 0.3613269329071045, -0.018655333667993546, -0.7688286304473877, -0.5134977698326111, -0.2647961676120758, 0.6547106504440308, -0.738314151763916, 0.9320623874664307, 1.156373381614685, -1.0004857778549194, 0.016639603301882744, 0.39203017950057983, 0.287121057510376, 0.14489662647247314, 0.3074667155742645, 0.36619099974632263, -0.13071951270103455, 0.6587139964103699, -0.187705397605896, 0.5592560172080994, -0.5512760281562805, -0.05740654841065407, 1.3051244020462036, -0.11526158452033997, 0.02629268541932106, 0.946199893951416, -0.18205493688583374, -1.5196236371994019, 0.47441384196281433, -0.7675721645355225, -1.0485639572143555, -0.6257649660110474, 0.434782475233078, -0.1458948850631714, -0.9523022174835205, -0.15637020766735077, -0.29764246940612793, 0.42825859785079956, 0.042446449398994446, -0.5247011780738831, 0.16810350120067596, -0.1721530705690384, 0.04221053794026375, 0.7190181612968445, 0.5233704447746277, -1.2868229150772095, -1.3290534019470215, -0.1566358357667923, -0.6696957349777222, 0.3055785298347473, -0.15503866970539093, -0.49062228202819824, -0.6949570775032043, 0.8721399307250977, 0.681705892086029, -0.07330590486526489, -0.07412297278642654, 0.1362394541501999, -0.2825228273868561, 1.0132417678833008, 0.09314900636672974, -0.5193111300468445, 0.20969076454639435, 1.4139550924301147, 1.9094778299331665, -1.0793722867965698, 0.019482629373669624, -0.2514705955982208, -0.4817873537540436, 0.9980713725090027, 0.8547816872596741, -0.1888626217842102, 0.6323196887969971, -0.4930943548679352, 0.1553608924150467, 0.16464775800704956, -0.7059361934661865, -0.3441928029060364, 0.7723504304885864, 1.0797357559204102, 0.31759992241859436, 0.5103879570960999, 0.1708163321018219, 0.6441988348960876, 0.5720432996749878, 0.45299655199050903, 0.5663460493087769, 0.735816478729248, -0.07172133028507233, 0.21747541427612305, 0.004940190352499485, 0.6223828792572021, -0.17097355425357819, 0.22078020870685577, 0.13172373175621033, 0.6053133010864258, 0.010076035745441914, 0.8223476409912109, 0.9215735197067261, 0.03057130053639412, 0.648296058177948, -0.009035954251885414, 1.0126596689224243, -0.6807163953781128, 0.03798254206776619, -0.18778109550476074, -0.63059401512146, -0.41033098101615906, -0.8616515398025513, -0.821185827255249, -0.8835304975509644, 0.6458940505981445, 0.3720659017562866, -0.5507226586341858, 0.33746108412742615, 1.4277094602584839, 0.40089917182922363, 0.4736708104610443, -0.5665519833564758, -0.730986475944519, -0.6990321278572083, -1.0048426389694214, 0.5267730951309204, -0.5894333124160767, 0.2454431653022766, -0.6485303640365601, -0.14283396303653717, -0.31531623005867004]}, "authors": [{"authorId": "115643437", "name": "Yueen Ma"}, {"authorId": "2114791973", "name": "Zixing Song"}, {"authorId": "8773733", "name": "Yuzheng Zhuang"}, {"authorId": "2287738662", "name": "Jianye Hao"}, {"authorId": "2266399557", "name": "Irwin King"}], "references": [{"paperId": "1d2753d74025e7a71594506623be81f18b073adb", "title": "Octo: An Open-Source Generalist Robot Policy"}, {"paperId": "66d7d369b00d4855455ce55dd7c9c1419da0c1d0", "title": "BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1, 000 Everyday Activities and Realistic Simulation"}, {"paperId": "2bbc0b48052f1f3f52cbd46ece378baa61fa711b", "title": "RT-H: Action Hierarchies Using Language"}, {"paperId": "55f64d78fd7a2913b40f9708952ddad604650be1", "title": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents"}, {"paperId": "4443c9a43bff8dcd717e5c75115ec6497af2b953", "title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation"}, {"paperId": "13d12b26db345f62e8e512db181b96a7f8763b47", "title": "An Embodied Generalist Agent in 3D World"}, {"paperId": "ad13b213681b6f634bc83a264df246e83dd9a9d9", "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"}, {"paperId": "1ba3adf8f2049e672c0b8786c18a1f2ffcd21fa0", "title": "RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches"}, {"paperId": "857efca0d4cbece97da4b904de1d506c2af25f9c", "title": "Vision-Language Foundation Models as Effective Robot Imitators"}, {"paperId": "3a610110d0464b8d47b9685466795a9eb653a242", "title": "A Dataset of Relighted 3D Interacting Hands"}, {"paperId": "bcb197654f39bb9312d8d0333646b71254d29239", "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning"}, {"paperId": "ef7d31137ef06c5be8c2824ecc5af6ce3358cc8f", "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models"}, {"paperId": "5343715cb2b7f7ac5c0acf92d1fc5657b40850b6", "title": "RoboHive: A Unified Framework for Robot Learning"}, {"paperId": "174a3290ff0040e0f6a7a0b43bcd752c456350a0", "title": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning"}, {"paperId": "bf89341e16fc0a379ab5e4c6370cf7ea4e9afd03", "title": "Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions"}, {"paperId": "4fb3695d7a3cba3db438cda198c724225ab48a38", "title": "Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods"}, {"paperId": "7fbc502441d66daf1f53765d5d86a8dfba9ab0ce", "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"}, {"paperId": "38939304bb760473141c2aca0305e44fbe04e6e8", "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"}, {"paperId": "af6d0ba799213cbbcbfceb1fb9b78d2858486308", "title": "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"}, {"paperId": "2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f", "title": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9", "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"}, {"paperId": "6eddfde78d5f6b10206f00675102fe3ddef8ff89", "title": "Structured World Models from Human Videos"}, {"paperId": "3c8de7a0a37fcdea5cabe0f0848319f1bbcfa75a", "title": "SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "3b0c02955e88f5862e61b560c7f70ba8cf235b1d", "title": "HomeRobot: Open-Vocabulary Mobile Manipulation"}, {"paperId": "2b806bc0a075f9088021f7362ffa5b8b86fd75ab", "title": "Robot Learning with Sensorimotor Pre-training"}, {"paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"}, {"paperId": "ed8ac4ff13d32a291bbe74f3e5a138800bba47fd", "title": "Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks"}, {"paperId": "3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a", "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model"}, {"paperId": "d3f79210b54e168c76b8c311488f42d7d1048b81", "title": "PandaGPT: One Model To Instruction-Follow Them All"}, {"paperId": "dedfe929d182cc3537a9ed765d589b4735ce062a", "title": "On the Planning Abilities of Large Language Models - A Critical Investigation"}, {"paperId": "00cb69a9f280317d1c59ac5827551ee9b10642b8", "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "d48cb91b9e555194f7494c4d4bb9815021d3ee45", "title": "VideoChat: Chat-Centric Video Understanding"}, {"paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91", "title": "ImageBind One Embedding Space to Bind Them All"}, {"paperId": "43e6e8d6663d83f1b74cf5a2be7b040b0928f867", "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"}, {"paperId": "570079bbdd8758dfe865097e05719313c9c1301a", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "690df0820f35a47e1ce44f90e6ddb4132aa09267", "title": "Vision-Language Models for Vision Tasks: A Survey"}, {"paperId": "326f6a8011e43322c433751b9cc31fd56564621c", "title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "f15a8105f5fb5444ef3685a893f85073af175bc4", "title": "Transformer-based World Models Are Happy With 100k Interactions"}, {"paperId": "af997821231898a5f8d0fd78dad4eec526acabe5", "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"}, {"paperId": "bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2", "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "9976672fd95dd1b7579117a01957f5a0c46e9d01", "title": "Open-World Object Manipulation using Pre-trained Vision-Language Models"}, {"paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "3396609b96dd24cac3b1542aec686ce362f32fe2", "title": "Language-Driven Representation Learning for Robotics"}, {"paperId": "0ba581718f294db1d7b3dbc159cc3d3380f74606", "title": "ChatGPT for Robotics: Design Principles and Model Abilities"}, {"paperId": "746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7", "title": "Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "ccb1ccc4deacc4fb18000f0e1ce24329548963ae", "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents"}, {"paperId": "da2fe6cd385194b0274d04d04ee72e8caf3854d4", "title": "Learning Universal Policies via Text-Guided Video Generation"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "fc918d6f8e2523696c34fa1be5aabdb42e9648d2", "title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling"}, {"paperId": "abba9a6f99d877fdd1b8412ddfcc26fdac6163dc", "title": "SMART: Self-supervised Multi-task pretrAining with contRol Transformers"}, {"paperId": "f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e", "title": "Mastering Diverse Domains through World Models"}, {"paperId": "638b5c76d96e32f54475a8327a9c68e0167156a9", "title": "A Survey on Transformers in Reinforcement Learning"}, {"paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d", "title": "RT-1: Robotics Transformer for Real-World Control at Scale"}, {"paperId": "8ee45aeb7c97e3346cc62f216f673b91277ac718", "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"}, {"paperId": "0703c5c7f737574d708babf48cdc876271415802", "title": "Masked Autoencoding for Scalable and Generalizable Decision Making"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "0e34addae55a571d7efd3a5e2543e86dd7d41a83", "title": "Interactive Language: Talking to Robots in Real Time"}, {"paperId": "07099fe26ee8850c9ccba6fe2ee139d67289b67c", "title": "Foundation Transformers"}, {"paperId": "eac1108cc22fa1510259fcf2c62d29f05bf1d21b", "title": "EgoTaskQA: Understanding Human Tasks in Egocentric Videos"}, {"paperId": "99832586d55f540f603637e458a292406a0ed75d", "title": "ReAct: Synergizing Reasoning and Acting in Language Models"}, {"paperId": "25425e299101b13ec2872417a14f961f4f8aa18e", "title": "VIMA: General Robot Manipulation with Multimodal Prompts"}, {"paperId": "979810ca765695a481c37126103b8ba256ee2192", "title": "Real-World Robot Learning with Masked Visual Pre-training"}, {"paperId": "498ac9b2e494601d20a3d0211c16acf2b7954a54", "title": "Imagen Video: High Definition Video Generation with Diffusion Models"}, {"paperId": "8f84dcbad8cd3b5b4d9229c56bc95f24be859a35", "title": "Grounding Language with Visual Affordances over Unstructured Data"}, {"paperId": "3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3", "title": "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training"}, {"paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"}, {"paperId": "626d405aa05d96f8c1f27fa09e93b292084670b0", "title": "PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training"}, {"paperId": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c", "title": "Code as Policies: Language Model Programs for Embodied Control"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "60c8d0619481eaafdd1189af610d0e636271fed5", "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation"}, {"paperId": "682649e54113007724168c3920efe9a919f2d3d8", "title": "Instruction-driven history-aware policies for robotic manipulations"}, {"paperId": "235303a8bc1e4892efd525a38ead657422d8a519", "title": "Transformers are Sample Efficient World Models"}, {"paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8", "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"}, {"paperId": "31d629bb161d8199e18b6f2ed7e4ecbda10b6797", "title": "Masked World Models for Visual Control"}, {"paperId": "65fc1f1c567801fee3788974e753cdbf934f07e9", "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos"}, {"paperId": "e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc", "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change"}, {"paperId": "e9c371f05cb1144211ba22f2bb48aba72f49a811", "title": "Object Scene Representation Transformer"}, {"paperId": "220a21fbc074ef9d24af4cdd51465f74809a4317", "title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models"}, {"paperId": "60ee030773ba1b68eb222a265b052ca028353362", "title": "GIT: A Generative Image-to-text Transformer for Vision and Language"}, {"paperId": "9dae204dad41633188022002a04c8aa67c79a4e1", "title": "Simple Open-Vocabulary Object Detection with Vision Transformers"}, {"paperId": "5922f437512158970c417f4413bface021df5f78", "title": "A Generalist Agent"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "15ac70d077bb735eed4a8502ce49aa7782c803fd", "title": "What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data"}, {"paperId": "a58b3f2ab75fdbda082e684d027ab4f552b0b5d3", "title": "Correcting Robot Plans with Natural Language Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"}, {"paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204", "title": "R3M: A Universal Visual Representation for Robot Manipulation"}, {"paperId": "7b3d26bd1d65ed5937c76043b5cd058260d8469f", "title": "The Unsurprising Effectiveness of Pre-Trained Vision Models for Control"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "24ed74ed29c057cba8b52fff4edd2c0d7f408716", "title": "VLP: A Survey on Vision-language Pre-training"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "1d803f07e4591bd67c358eef715bcd443e821894", "title": "BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning"}, {"paperId": "b9b220b485d2add79118ffdc2aaa148b67fa53ef", "title": "Pre-Trained Language Models for Interactive Decision-Making"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02", "title": "FLAVA: A Foundational Language And Vision Alignment Model"}, {"paperId": "4be02694125b71876552900a53c85c47a2a83614", "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks"}, {"paperId": "21ec90872abd986c12afe39bebe807732ffa70c9", "title": "Florence: A New Foundation Model for Computer Vision"}, {"paperId": "826383e18568c9c37b5fc5dd7e2913352db22b47", "title": "Simple but Effective: CLIP Embeddings for Embodied AI"}, {"paperId": "197d5867a45a2988f4dd159063cdfbfe90164962", "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "f675c62abfa788ea0be85d3124eba15a14d5e9d6", "title": "FILIP: Fine-grained Interactive Language-Image Pre-Training"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "ae83ca7901aba565604b146911d17ae3ef4d7393", "title": "SE(3) Equivariant Graph Neural Networks with Complete Local Frames"}, {"paperId": "2d4ca959cb3d544473cb661cefe76daabebcdff3", "title": "Skill Induction and Planning with Latent Language"}, {"paperId": "6bae20930eaa0d9d489317f3b3b1aaaf18205ef8", "title": "Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets"}, {"paperId": "69ee9b3a915951cc84b74599a3a2699a66d4004f", "title": "CLIPort: What and Where Pathways for Robotic Manipulation"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "aeae048c7d8d4ea03bb9cb1b75c65903c915909a", "title": "iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks"}, {"paperId": "c0c9f77cb097f2ce53feb91802bcfbae57fcc42f", "title": "BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments"}, {"paperId": "9933a5af7895354087baf6c96b64dc8a8973eaed", "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "6536f36648d39f0f9f6105562f76704fcc0b19e8", "title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "4aa88c1406414cda3ce9cf76c8af0abaa8391760", "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat"}, {"paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d", "title": "Multimodal Few-Shot Learning with Frozen Language Models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "f864d4d2267abba15eb43db54f58286aef78292b", "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "68f080e0ac836ea230cb5316fbed273c70422d75", "title": "Segmenter: Transformer for Semantic Segmentation"}, {"paperId": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14", "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation"}, {"paperId": "a2f6930b37febd2b88bbea7778f442ef3621b8a3", "title": "STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation"}, {"paperId": "7ba9c013988eaff5cd186d73704af329d027872d", "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"}, {"paperId": "6b45c8d03a20c5ae4fe5d6136cf01748b4bd7489", "title": "ManipulaTHOR: A Framework for Visual Object Manipulation"}, {"paperId": "3e85d208b1b927fdb69ecf8336c70995818aaebd", "title": "MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale"}, {"paperId": "6500df35e30461d857103b3e3fa72b2913049a56", "title": "Visual Room Rearrangement"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "d85d16b003955c6996fafacea7f3c075c531225f", "title": "Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "9c404d02aefd850ac3d5a8bdc5860738e6cd2b04", "title": "A Survey of Embodied AI: From Simulators to Research Tasks"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0", "title": "E(n) Equivariant Graph Neural Networks"}, {"paperId": "c16835c8e535ebd9c10a550ca9455fe384a14449", "title": "High-Performance Large-Scale Image Recognition Without Normalization"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "806725595f04849b3b4cc9f6c28d4a84744e8d95", "title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes"}, {"paperId": "5f975172aa9088f24236ccc8fe4bfc01ce6fb9b9", "title": "Object Rearrangement Using Learned Implicit Collision Functions"}, {"paperId": "b4caa67681cbe4973b21e96e69ad7b213b19f28f", "title": "Rearrangement: A Challenge for Embodied AI"}, {"paperId": "3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2", "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af", "title": "Mastering Atari with Discrete World Models"}, {"paperId": "54244dc6c62f0ae98b6ebe0a5bfa9f39ffdad809", "title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation"}, {"paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a", "title": "Denoising Diffusion Probabilistic Models"}, {"paperId": "6075091294a0fe0fe5c6c7b7a0df9029b6a965cb", "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks"}, {"paperId": "a9a4e8e631890a14257539948e1813b5214c60dd", "title": "Self-Supervised Graph Transformer on Large-Scale Molecular Data"}, {"paperId": "28db20a81eec74a50204686c3cf796c42a020d2e", "title": "Conservative Q-Learning for Offline Reinforcement Learning"}, {"paperId": "b539b61fbeb3a8514fd82069b35884a4fcac4985", "title": "Understanding Human Hands in Contact at Internet Scale"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "1301e9d11b728268ed1ff3f1a9adc155308d5250", "title": "Language Conditioned Imitation Learning Over Unstructured Data"}, {"paperId": "7b9b756ab509cb9f52dbac95e3e901d571f0784f", "title": "A Survey of the Usages of Deep Learning for Natural Language Processing"}, {"paperId": "e0d2852be2a3f5952a6600eaa933f23bee448210", "title": "SAPIEN: A SimulAted Part-Based Interactive ENvironment"}, {"paperId": "0311ace1d499cadd1cc0c515a625d1d045f60d25", "title": "Deep Learning for 3D Point Clouds: A Survey"}, {"paperId": "f4cf4246f3882aa6337e9c05d5675a3b8463a32e", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"}, {"paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9", "title": "Dream to Control: Learning Behaviors by Latent Imagination"}, {"paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d", "title": "Self-Training With Noisy Student Improves ImageNet Classification"}, {"paperId": "2b10c90d2aef5ad62e746d809065251768108541", "title": "Interactive Gibson Benchmark: A Benchmark for Interactive Navigation in Cluttered Environments"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "8c54e8575e7c17a4097838305915e6e7b00fd4af", "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"}, {"paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9", "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"}, {"paperId": "3e519d85cdcefdd1d2ad89829d6ad445695d8c58", "title": "RoboNet: Large-Scale Multi-Robot Learning"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "320b227027030fc291de2896fc3c6da49d7614be", "title": "Solving Rubik's Cube with a Robot Hand"}, {"paperId": "71f3a2632d924f29ca6eb2e789f8ff6d46250c82", "title": "EgoVQA - An Egocentric Video Question Answering Benchmark Dataset"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "a2fdfda785b3a2a0178d174daa515377c531f222", "title": "RLBench: The Robot Learning Benchmark & Learning Environment"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "834fb308ca1a0177d303691d6003e3b0fded021e", "title": "Reasoning Over Semantic-Level Graph for Fact Checking"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "dd20f00b0121075d2373e60025d59d9bddf89ea1", "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "43ddb69219ce12b0ed213dcae7a16dda40b2c2e6", "title": "Multilingual Universal Sentence Encoder for Semantic Retrieval"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd", "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "68a2eb5890eb67989df5fb42b929d10e5e8e5a47", "title": "Multi-Target Embodied Question Answering"}, {"paperId": "d2d3580faaa56be0eb7243fa119e6a7191924927", "title": "Embodied Question Answering in Photorealistic Environments With Point Cloud Perception"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "b4a35e548de27b6924e5f2ee41d37238a5c4a1d5", "title": "Habitat: A Platform for Embodied AI Research"}, {"paperId": "99a7df93a2e16bd7ac3349d52cc34417cda7909d", "title": "Learning Latent Plans from Play"}, {"paperId": "8452941a175c899628c679523da952b18f63e335", "title": "A survey of the recent architectures of deep convolutional neural networks"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d", "title": "Off-Policy Deep Reinforcement Learning without Exploration"}, {"paperId": "35bb00d21bfc8e13959b7ddb863e6254f778bebe", "title": "MeshNet: Mesh Neural Network for 3D Shape Representation"}, {"paperId": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9", "title": "How Powerful are Graph Neural Networks?"}, {"paperId": "ae09e1df06d7fdf7bece8c454085ddbf5fedec2a", "title": "Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "b20a5427d79c660fe55282da2533071629bfc533", "title": "Deep Learning Advances on Different 3D Data Representations: A Survey"}, {"paperId": "d37a34c204a8beefcaef4dddddb7a90c16e973d4", "title": "Learning dexterous in-hand manipulation"}, {"paperId": "eb37e7b76d26b75463df22b2a3aa32b6a765c672", "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"}, {"paperId": "7139a5f730652abbeabf9e140009907d2c7da3e5", "title": "VirtualHome: Simulating Household Activities Via Programs"}, {"paperId": "e89a4fe6e8286eccedd702216153f0f248adb151", "title": "Gibson Env: Real-World Perception for Embodied Agents"}, {"paperId": "fc50c9392fd23b6c88915177c6ae904a498aacea", "title": "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "811df72e210e20de99719539505da54762a11c6d", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"}, {"paperId": "a9a3ed69c94a3e1c08ef1f833d9199f57736238b", "title": "DeepMind Control Suite"}, {"paperId": "89c8aad71433f7638d2e2c009e1ea20e039f832d", "title": "AI2-THOR: An Interactive 3D Environment for Visual AI"}, {"paperId": "b0cd469a06fb2eae3a5cc0c860aa592f71b13f6d", "title": "IQA: Visual Question Answering in Interactive Environments"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "7cfa5c97164129ce3630511f639040d28db1d4b7", "title": "FiLM: Visual Reasoning with a General Conditioning Layer"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8", "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2", "title": "Hindsight Experience Replay"}, {"paperId": "b68811a9b5cafe4795a11c1048541750068b7ad0", "title": "The \u201cSomething Something\u201d Video Database for Learning and Evaluating Visual Common Sense"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "8674494bd7a076286b905912d26d47f7501c4046", "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"}, {"paperId": "6b7d6e6416343b2a122f8416e69059ce919026ef", "title": "Inductive Representation Learning on Large Graphs"}, {"paperId": "cc188b35108d089b31cc0cedcfed100e4e7ae8a1", "title": "Deep Learning Advances in Computer Vision with 3D Data"}, {"paperId": "e24cdf73b3e7e590c2fe5ecac9ae8aa983801367", "title": "Neural Message Passing for Quantum Chemistry"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "9c4082bfbd46b781e70657f14895306c57c842e3", "title": "Robust Adversarial Reinforcement Learning"}, {"paperId": "049c6e5736313374c6e594c34b9be89a3a09dced", "title": "FeUdal Networks for Hierarchical Reinforcement Learning"}, {"paperId": "9f1e9e56d80146766bc2316efbc54d8b770a23df", "title": "Deep Reinforcement Learning: An Overview"}, {"paperId": "6b1793ece5993523855ce67c646de408318d1b12", "title": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks"}, {"paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878", "title": "Feature Pyramid Networks for Object Detection"}, {"paperId": "d997beefc0922d97202789d2ac307c55c2c52fba", "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"}, {"paperId": "54906484f42e871f7c47bbfe784a358b1448231f", "title": "Variational Graph Auto-Encoders"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "3fe70222056286c4241488bf687afbd6db02d207", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Data Collection"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4", "title": "Modeling Context in Referring Expressions"}, {"paperId": "c41eb895616e453dcba1a70c9b942c5063cc656c", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"}, {"paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1", "title": "Generative Adversarial Imitation Learning"}, {"paperId": "2b10281297ee001a9f3f4ea1aa9bea6b638c27df", "title": "OpenAI Gym"}, {"paperId": "4cdcf2ae5e1fafebd9b3613247a7b1962584da34", "title": "Volumetric and Multi-view CNNs for Object Classification on 3D Data"}, {"paperId": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"}, {"paperId": "d358d41c69450b171327ebd99462b6afef687269", "title": "Continuous Deep Q-Learning with Model-based Acceleration"}, {"paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "1a37f07606d60df365d74752857e8ce909f700b3", "title": "Deep Neural Networks for Learning Graph Representations"}, {"paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd", "title": "Asynchronous Methods for Deep Reinforcement Learning"}, {"paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490", "title": "Mastering the game of Go with deep neural networks and tree search"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "b0c065cd43aa7280e766b5dcbcc7e26abce59330", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"}, {"paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e", "title": "Deep Reinforcement Learning with Double Q-Learning"}, {"paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f", "title": "Continuous control with deep reinforcement learning"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "af88ce6116c2cd2927a4198745e99e5465173783", "title": "Bidirectional LSTM-CRF Models for Sequence Tagging"}, {"paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd", "title": "You Only Look Once: Unified, Real-Time Object Detection"}, {"paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8", "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "title": "Fast R-CNN"}, {"paperId": "b6b8a1b80891c96c28cc6340267b58186157e536", "title": "End-to-End Training of Deep Visuomotor Policies"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "340f48901f72278f6bf78a04ee5b01df208cc508", "title": "Human-level control through deep reinforcement learning"}, {"paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199", "title": "Trust Region Policy Optimization"}, {"paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "title": "Show and tell: A neural image caption generator"}, {"paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0", "title": "Fully convolutional networks for semantic segmentation"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94", "title": "Deterministic Policy Gradient Algorithms"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "5e925a9f1e20df61d1e860a7aa71894b35a1c186", "title": "Spectral Networks and Locally Connected Networks on Graphs"}, {"paperId": "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "244539f454800697ed663326b7cfba337ca0c2ec", "title": "Guided Policy Search"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "ec2b2569b3a0d70a5b45d48b041dec9060d85eb7", "title": "Neural Network for Graphs: A Contextual Constructive Approach"}, {"paperId": "f020b61789112fe7241b871907268f0bdc5c84fa", "title": "ArnetMiner: extraction and mining of academic social networks"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260", "title": "Finding Structure in Time"}, {"paperId": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "title": "Backpropagation Applied to Handwritten Zip Code Recognition"}, {"paperId": "319f22bd5abfd67ac15988aa5c7f705f018c3ccd", "title": "Learning internal representations by error propagation"}, {"paperId": null, "title": "Introducing ernie 3.5: Baidu\u2019s knowledge-enhanced foundation model takes a giant leap forward"}, {"paperId": "da923d1ccfd4927fae7c2a835c7979e3a4dec159", "title": "Graph Neural Networks for Natural Language Processing: A Survey"}, {"paperId": "369b34826e23cb43bea9a91395e9603eacfa7420", "title": "EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models"}, {"paperId": null, "title": "S. contributors"}, {"paperId": "baf821eba97d799562ed9789c87fc650e689c614", "title": "PDSketch: Integrated Domain Programming, Learning, and Planning"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "ser. Proceedings of Machine Learning Research, A. Faust, D. Hsu, and G. Neumann, Eds., vol. 164"}, {"paperId": null, "title": "\u201cELECTRA: pre-training text encoders as discriminators rather than generators,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "81a4fd3004df0eb05d6c1cef96ad33d5407820df", "title": "A Comprehensive Survey on Graph Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "\u201cSet transformer: A framework for attention-based permutation-invariant neural networks,\u201d"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "\u201cStereo magnification: learning view synthesis using multiplane images,\u201d"}, {"paperId": "3efd851140aa28e95221b55fcc5659eea97b172d", "title": "The Graph Neural Network Model"}, {"paperId": null, "title": "Introducing chatgpt"}, {"paperId": null, "title": "\u201cVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\u201d"}, {"paperId": "920731b589af90a5b79236f4939ac117bbb939f2", "title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models"}, {"paperId": null, "title": "\u201cStanford alpaca: An instruction-following llama model,\u201d"}, {"paperId": null, "title": "\u201cVideo generation models as world simulators,\u201d"}, {"paperId": null, "title": "\u201cEQA-MX: Embodied question answering using multimodal expression,\u201d"}, {"paperId": null, "title": "\u201cMinigpt-4: En-hancing vision-language understanding with advanced large language models,\u201d"}, {"paperId": null, "title": "\u201cTask planning for visual room rearrangement under partial observability,\u201d"}, {"paperId": null, "title": "\u201cMultimodal learning with trans-formers: A survey,\u201d"}, {"paperId": null, "title": "\u201cBaize: An open-source chat model with parameter-efficient tuning on self-chat data,\u201d"}, {"paperId": null, "title": "\u201cRobocat: A self-improving foundation agent for robotic manipulation,\u201d"}, {"paperId": null, "title": "(2023) Introducing mpt-7b: A new standard for open-source, commercially usable llms"}]}