{"paperId": "3c6f8b14a1ed05c7228a37c32968c23a133bf28f", "title": "SlipStream: Adapting Pipelines for Distributed Training of Large DNNs Amid Failures", "abstract": "Training large Deep Neural Network (DNN) models requires thousands of GPUs for days or weeks at a time. At these scales, failures are frequent and can have a big impact on training throughput. Restoring performance using spare GPU servers becomes increasingly expensive as models grow. SlipStream is a system for efficient DNN training in the presence of failures, without using spare servers. It exploits the functional redundancy inherent in distributed training systems -- servers hold the same model parameters across data-parallel groups -- as well as the bubbles in the pipeline schedule within each data-parallel group. SlipStream dynamically re-routes the work of a failed server to its data-parallel peers, ensuring continuous training despite multiple failures. However, re-routing work leads to imbalances across pipeline stages that degrades training throughput. SlipStream introduces two optimizations that allow re-routed work to execute within bubbles of the original pipeline schedule. First, it decouples the backward pass computation into two phases. Second, it staggers the execution of the optimizer step across pipeline stages. Combined, these optimizations enable schedules that minimize or even eliminate training throughput degradation during failures. We describe a prototype for SlipStream and show that it achieves high training throughput under multiple failures, outperforming recent proposals for fault-tolerant training such as Oobleck and Bamboo by up to 1.46x and 1.64x, respectively.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A prototype for SlipStream is described and it is shown that it achieves high training throughput under multiple failures, outperforming recent proposals for fault-tolerant training such as Oobleck and Bamboo by up to 1.46x and 1.64x, respectively."}, "embedding": {"model": "specter_v2", "vector": [0.10356239229440689, 0.1260850578546524, -0.3330546021461487, -0.07943888753652573, -0.24517273902893066, 0.10522523522377014, 0.38270455598831177, -0.35052040219306946, -0.5450923442840576, -0.5209215879440308, 0.07905977219343185, 0.013280988670885563, 0.7593955397605896, 0.22738412022590637, -0.3997064232826233, 0.028488676995038986, -1.0738633871078491, 0.14468839764595032, 0.9708996415138245, -0.023197488859295845, -0.14112302660942078, -0.12336862087249756, -1.8284579515457153, 0.5084347724914551, 0.48009002208709717, 1.000097393989563, -0.23838630318641663, 1.1871843338012695, 0.019140614196658134, 0.46882498264312744, 0.08371936529874802, 0.46686792373657227, 0.3115100562572479, -0.04562901705503464, -0.24933074414730072, -0.055441901087760925, 0.579333484172821, -0.7528344392776489, -0.5271646976470947, 0.40413910150527954, -0.06189137324690819, 0.1477646380662918, -0.10093601047992706, -1.089057207107544, 0.7123699188232422, 0.003917199559509754, 0.4562036097049713, 1.0889655351638794, -1.1462769508361816, -0.3194049000740051, 0.6607090830802917, -1.286482572555542, -0.0022426098585128784, 1.1056090593338013, 0.9882402420043945, 0.2006271928548813, -0.3519588112831116, -0.4420945346355438, 0.48368874192237854, -0.12480560690164566, -0.4221533536911011, -0.8728750944137573, -0.6461957693099976, -0.24791136384010315, 1.9057694673538208, -0.15111994743347168, -0.09243209660053253, 0.17711257934570312, 0.06895708292722702, 1.278698205947876, -0.24867035448551178, -0.5451862812042236, 0.3050638437271118, -0.3195802867412567, 0.45300737023353577, 0.5117923617362976, 0.11622354388237, 0.05427847057580948, -1.5219179391860962, -0.2972695529460907, 0.3082277774810791, 0.30277201533317566, 0.3980620801448822, -0.34777843952178955, 0.20215919613838196, 0.46644705533981323, 0.5866490006446838, 0.5860267281532288, -0.44493815302848816, 1.1144174337387085, 0.9892097115516663, 0.23845334351062775, 0.7405292391777039, 0.05989786237478256, -0.01861473172903061, 0.11109720170497894, -1.0675727128982544, 0.08351894468069077, 0.36435455083847046, 0.6875690817832947, -0.25355643033981323, 0.08541519939899445, -0.5287797451019287, 0.14797396957874298, 1.167974829673767, -0.24086491763591766, 0.46602311730384827, -0.6158648729324341, 0.4659666121006012, -0.6602073907852173, -0.01305865217000246, -0.3695794939994812, -0.351388156414032, -0.20995841920375824, -0.821196973323822, -0.39815300703048706, -0.6747986674308777, -0.13033010065555573, -0.7215703725814819, -0.08566643297672272, 0.17330048978328705, 0.6886234283447266, -0.03905152156949043, 0.7011008262634277, 0.4567601978778839, 0.36334893107414246, 0.19954872131347656, 0.5983110666275024, 0.7100358009338379, -1.4046356678009033, -0.1163230687379837, -1.1068899631500244, 0.451324462890625, -0.22115390002727509, -0.19322946667671204, -0.3602668046951294, -1.3201947212219238, -0.8019601106643677, -1.2110650539398193, 0.2800792157649994, -0.2980824410915375, -0.1537340134382248, 1.1449705362319946, 0.018575463443994522, -1.2756558656692505, 1.7489032745361328, -1.0418756008148193, -0.2028895914554596, 0.7090407013893127, 0.5228968858718872, 0.5575593709945679, -0.26653745770454407, -0.6147980690002441, -0.31571075320243835, -0.018458791077136993, -0.5297083258628845, -0.21053369343280792, -0.9272506237030029, -0.1398771107196808, 0.12224724143743515, 0.11887956410646439, -0.8108194470405579, 1.750710129737854, -0.287352979183197, -0.9443210363388062, 0.26542893052101135, 0.14860393106937408, 0.15674912929534912, 0.6369515061378479, -0.11031902581453323, -0.24693217873573303, 0.22987310588359833, -0.45843788981437683, 0.35657528042793274, 0.7634736895561218, -0.14919625222682953, -0.27942752838134766, -0.09678413718938828, -0.6514118909835815, 0.14419537782669067, -0.6121907234191895, 0.9245267510414124, -0.5802753567695618, 0.10533353686332703, 0.668710470199585, 0.5997111201286316, -0.08763739466667175, 0.05365612357854843, -0.21332481503486633, -0.8202839493751526, 1.069114089012146, 0.159776970744133, 0.7996671199798584, -1.0216490030288696, -0.9475355744361877, -0.009139367379248142, 0.07291162014007568, 0.11963832378387451, -0.5294148325920105, 0.681999146938324, -0.24981693923473358, 0.3878251016139984, 0.10539572685956955, -1.3025767803192139, -0.15297897160053253, -0.14958740770816803, -0.5352039933204651, -0.4035705327987671, 0.11347712576389313, 0.6734170913696289, -0.5366905927658081, 0.20903640985488892, -0.6557559967041016, 0.2918441891670227, -1.2418849468231201, 1.4312442541122437, -0.4814426898956299, 0.024117810651659966, 0.04366239905357361, -0.20671547949314117, 0.30212393403053284, -0.4528645873069763, 0.7748379707336426, -0.4302457869052887, -0.02856498956680298, 0.5712636709213257, -0.1794818639755249, 0.9791496396064758, -0.3614623546600342, 0.08576945960521698, 0.15595050156116486, -0.8781622052192688, -0.3008619546890259, 0.5041323304176331, 0.05968118831515312, -0.42343926429748535, 0.5024543404579163, 0.76532381772995, -0.7104789614677429, 0.3077952265739441, 1.0067111253738403, 1.1136822700500488, -0.03216424584388733, 0.34934571385383606, 0.6283088326454163, -0.2857152223587036, 0.21375752985477448, 0.38067543506622314, 1.1611870527267456, 0.4357815384864807, -0.5934556126594543, -0.1360110193490982, -0.3320361375808716, -0.7240203619003296, -0.21256327629089355, 0.5027393102645874, 0.2862486243247986, 0.7434223890304565, 1.0576095581054688, -0.8840934038162231, -0.7006509304046631, 0.23863783478736877, 0.6359108090400696, 1.6851571798324585, -0.2749518156051636, 0.3867551386356354, -0.8733845353126526, -0.8964004516601562, 0.10134841501712799, -0.4402645230293274, -0.028633926063776016, -0.3077012896537781, -0.6117700338363647, -1.3399522304534912, 0.7730635404586792, 0.2312457114458084, 1.0403354167938232, -0.886053740978241, -0.8314617872238159, -0.6133604049682617, 0.9481992125511169, -0.7487074136734009, -0.5469177961349487, 0.7249595522880554, -0.986240804195404, -0.3644772171974182, 0.31121888756752014, -0.08107830584049225, 0.4375308156013489, -0.06267688423395157, 1.3193342685699463, 0.21652550995349884, -0.6418859362602234, -0.13454872369766235, 0.39616450667381287, -0.34802931547164917, -0.5671225190162659, 0.2765226364135742, 0.23178596794605255, -0.3578328788280487, -0.36998897790908813, 0.047320131212472916, -0.320235013961792, 0.18745717406272888, -0.6979590058326721, 0.09404829889535904, -0.009183566085994244, -0.119334876537323, 0.6428595781326294, -0.5585705041885376, 0.38289400935173035, -1.3586522340774536, 0.813513994216919, -0.2863512635231018, -0.42737987637519836, -0.14936207234859467, -0.671481192111969, -0.14057424664497375, 0.6065089702606201, -0.5193918347358704, -0.04765234887599945, -1.370605230331421, 0.07816671580076218, -0.8294524550437927, -0.0032461306545883417, -0.47115761041641235, 0.7074135541915894, -0.36844244599342346, 0.6083300113677979, 0.104779452085495, 0.8736593127250671, -0.3618070185184479, 0.07529791444540024, -0.8147793412208557, 0.2591249644756317, 0.2164738029241562, -0.3418058454990387, -0.22876709699630737, -0.06283794343471527, -0.6840741038322449, -0.34748587012290955, -0.4024173319339752, -0.20989131927490234, -0.3357832729816437, -0.19468428194522858, -0.34775832295417786, -1.1180520057678223, 0.08472949266433716, -1.0441614389419556, -0.8482829332351685, 0.4447576105594635, -0.28108149766921997, -0.1086253672838211, -1.2264196872711182, -1.6126880645751953, -0.018583297729492188, -1.308039903640747, -1.1038893461227417, 0.49830809235572815, 0.36358776688575745, -0.45715752243995667, -0.3362714648246765, -0.16957184672355652, -0.7859110832214355, 1.2036746740341187, -0.2893354892730713, 0.2953099012374878, -0.12819287180900574, -0.18445181846618652, -0.08700678497552872, -0.3018634021282196, 0.7980626225471497, -0.8601136207580566, 0.25938495993614197, -0.859648585319519, -0.09893783181905746, -0.5303838849067688, -0.8150948286056519, 0.3517507016658783, -0.12532374262809753, 0.9237203598022461, -0.1907515823841095, -0.6109220385551453, 0.7403761148452759, 1.5758510828018188, -0.7601141333580017, -0.06363382935523987, -0.11915348470211029, 0.9882627725601196, -0.08243007957935333, -0.4083305895328522, 0.588980495929718, -0.4594933092594147, 0.2265760898590088, 0.5430837869644165, -0.2510594427585602, -0.5361138582229614, -0.13285279273986816, 0.0990174189209938, 1.4854373931884766, 1.0998793840408325, -0.11230084300041199, -0.9361456036567688, 0.25571227073669434, -1.4267818927764893, 0.02812703512609005, 0.4962804913520813, 1.0991806983947754, 0.03424578532576561, -0.3582685887813568, -0.02576358988881111, -0.4125736355781555, 0.29038292169570923, 0.3275768756866455, -0.9542003870010376, -0.8074836730957031, 0.7932701706886292, 0.44272106885910034, 0.6196879148483276, 0.18493340909481049, -0.2575059235095978, 0.3297880291938782, 14.475829124450684, 0.634879469871521, -0.16009214520454407, 0.784177839756012, 0.7819594740867615, -0.14327102899551392, 0.03817751631140709, -0.38110071420669556, -1.1344767808914185, 0.29283976554870605, 1.72265625, 0.5029448866844177, 0.05163826048374176, 0.3383823335170746, -0.0485689640045166, -0.3702719807624817, -0.5059192180633545, 0.4458613693714142, 0.5542869567871094, -1.3454945087432861, -0.014717716723680496, 0.10757090896368027, 0.426662415266037, 1.259955883026123, 0.5921235084533691, 0.42886024713516235, 0.839352548122406, -0.1793888956308365, 0.23247836530208588, 0.34020811319351196, 1.1184576749801636, -0.5463025569915771, 0.6156826019287109, 0.360610693693161, -0.40922486782073975, 0.281516969203949, -0.32830819487571716, -1.0440254211425781, -0.01809546910226345, 0.18042947351932526, -0.7127688527107239, -0.2671714723110199, 0.3251679241657257, 0.557384192943573, 0.0874866172671318, 0.64790278673172, 0.24768872559070587, 0.533147931098938, -0.3411993086338043, 0.06559708714485168, -0.08172431588172913, 0.718863844871521, -0.37855735421180725, -0.034448493272066116, -0.27400916814804077, -0.26813384890556335, 0.31653836369514465, -0.07141794264316559, -0.831265389919281, -0.5715926289558411, -0.10136257857084274, -0.022703414782881737, -0.07526067644357681, 0.6922724843025208, -0.03518267348408699, 0.03172825276851654, -0.35445520281791687, 0.8325222730636597, 0.7631146311759949, -0.1992017775774002, 0.029476650059223175, -0.032507751137018204, 0.6899741291999817, -0.36165136098861694, -0.19806289672851562, 0.48886576294898987, -0.6254643797874451, -0.17872247099876404, -0.706501305103302, -0.38184380531311035, 0.1509941816329956, -0.4736727178096771, -0.5971728563308716, 1.03872549533844, -0.4430674910545349, -0.2559519410133362, 0.5252623558044434, -0.6608622670173645, -0.44253408908843994, 0.6736814379692078, -1.0246028900146484, -0.03932249918580055, 0.10058165341615677, -0.7860047817230225, -0.48539331555366516, 0.23632171750068665, 1.317260503768921, 0.369867742061615, -0.7698846459388733, 0.062106117606163025, 0.3532874286174774, -0.39710062742233276, 0.07245461642742157, -0.5292424559593201, 1.584627628326416, 0.3911198675632477, -0.7417073845863342, -0.6599318385124207, 0.11773012578487396, 0.04269517958164215, -0.5322635173797607, -0.3739991784095764, 0.4832070767879486, -0.5585774183273315, -0.07568994909524918, -0.46923917531967163, -0.9157286286354065, 0.31520146131515503, 0.38899436593055725, 0.48344144225120544, 0.4463701546192169, 0.05639715865254402, -0.4157675504684448, -0.4334266185760498, -0.8320854902267456, -0.08950478583574295, 0.7992089986801147, -0.35739320516586304, 0.22729410231113434, 0.32944267988204956, 0.6969218850135803, -1.2615336179733276, -0.5750003457069397, -0.47369059920310974, 0.08910003304481506, -0.5864071846008301, 0.8881401419639587, -0.16175469756126404, 0.8825168609619141, 1.2372435331344604, 0.30372047424316406, -0.8132247924804688, 0.4641053080558777, -0.6235629320144653, 0.09143245220184326, -0.030694425106048584, -0.059154365211725235, -0.6078641414642334, 0.7475327849388123, 0.7577874660491943, -0.3857860863208771, -0.48321273922920227, -0.5281602740287781, -0.12064308673143387, -0.2680761516094208, -0.39593642950057983, 0.15843826532363892, -0.28932076692581177, -0.04874696955084801, -0.12312506884336472, 0.6896104216575623, 0.5001256465911865, 0.10530173033475876, -0.8198166489601135, 0.35221871733665466, -0.20696265995502472, -0.26601916551589966, -0.565041184425354, -0.3659764230251312, -1.1145174503326416, -0.3016422986984253, -0.9501333832740784, 0.23645037412643433, -0.5612407326698303, -0.9119819402694702, -0.46631255745887756, -0.08465813845396042, 0.29710450768470764, 0.36012139916419983, 0.15716028213500977, -0.1815272867679596, -0.044371169060468674, -1.0101919174194336, 0.7104570269584656, 0.8009129762649536, 0.09816550463438034, -0.046132851392030716, 0.013731558807194233, 0.23182998597621918, 0.34625399112701416, 0.6658371686935425, -0.31923964619636536, -0.47938188910484314, -1.4110201597213745, 0.04858211800456047, -0.0014063958078622818, 0.028802568092942238, -1.2699230909347534, 0.8584725260734558, 0.6121926307678223, -0.1313239187002182, 0.11617857217788696, -0.24242690205574036, -0.8947148323059082, -0.6338817477226257, 0.4580635130405426, -0.1735246777534485, 0.4541162848472595, 0.549793004989624, -0.6103419661521912, -0.0743638351559639, 0.954433023929596, -0.07470912486314774, -0.6531079411506653, -0.9824111461639404, 0.4833061993122101, -0.3704086244106293, 5.497307938640006e-05, -0.22181269526481628, 0.22622761130332947, -1.2994954586029053, 0.3525356352329254, -0.05963210016489029, 0.6486753821372986, -0.15791071951389313, 0.7840833067893982, 0.17276786267757416, -0.9326111674308777, 0.35932472348213196, 0.4724593758583069, -0.5628775954246521, 0.7263731956481934, 1.2535415887832642, 0.8067736029624939, -1.2881392240524292, 0.015637632459402084, 0.023487607017159462, 0.18794582784175873, -0.8770791292190552, -0.019060835242271423, 0.9513514637947083, -0.3319695293903351, -0.2889225482940674, 1.327355980873108, -0.4719613194465637, -0.7677819132804871, 0.16712097823619843, -0.9728777408599854, -0.12281448394060135, -0.13949580490589142, 0.7157024145126343, 0.6395108699798584, 0.722834587097168, 0.41172197461128235, -0.7696263194084167, -0.32416754961013794, -0.18883167207241058, -0.2274271696805954, 0.5935003161430359, -0.0323457233607769, -0.5238121151924133, 0.5983515381813049, 0.7525590062141418, -1.120695948600769, -1.4273205995559692, -0.953510046005249, -0.41231560707092285, 0.03127935528755188, 0.43225911259651184, -0.3667624294757843, -0.8001598119735718, 0.6696339845657349, 0.4169883728027344, 0.3397679030895233, 0.14560766518115997, -0.3364920914173126, 0.45122548937797546, 0.047370295971632004, -0.041944533586502075, -0.6950427293777466, -0.1391850858926773, 1.042489767074585, 1.2135411500930786, -0.6263554692268372, 0.45498737692832947, -0.13998393714427948, -0.397715300321579, 0.8166439533233643, 0.7610465884208679, -0.5592145919799805, 0.9744911789894104, 0.013980001211166382, 0.024541513994336128, 0.2213459610939026, -1.1746262311935425, 0.11112268269062042, 0.4226225018501282, 0.431103378534317, 0.336006760597229, 0.30550089478492737, 0.10359679162502289, 0.5713307857513428, 0.4508640468120575, 0.1733359843492508, 0.3653246760368347, 0.9873520731925964, -0.060295309871435165, 0.10394967347383499, -0.025595473125576973, 0.7649434804916382, -0.5650056600570679, -0.45266953110694885, 0.3030204474925995, 0.4024001359939575, 0.3420368731021881, 0.590994119644165, 1.3968024253845215, -0.19727382063865662, 0.8512875437736511, 0.020654931664466858, 0.13445067405700684, -0.8146446347236633, -0.44538137316703796, -0.1506333202123642, -0.5638336539268494, -0.4713360369205475, 0.27119868993759155, -0.597754955291748, -0.49625128507614136, -1.0111318826675415, 0.8853319883346558, -0.0356752909719944, 0.5334240794181824, 0.8100273609161377, 1.110226035118103, 1.6281471252441406, 0.2080952376127243, -1.3769930601119995, -0.6869945526123047, -0.8258567452430725, -0.25188279151916504, -0.5462765097618103, -0.29311537742614746, 0.2990040183067322, -0.39183473587036133, -0.5480048656463623]}, "authors": [{"authorId": "2302770513", "name": "Swapnil Gandhi"}, {"authorId": "2279780592", "name": "Mark Zhao"}, {"authorId": "1580175613", "name": "Athinagoras Skiadopoulos"}, {"authorId": "2279756528", "name": "Christos Kozyrakis"}], "references": [{"paperId": "6dc5c6190dfbe55c8b45b7b23800614c21e5b51c", "title": "MegaScale: Scaling Large Language Model Training to More Than 10, 000 GPUs"}, {"paperId": "6cdf32647b158b6308aac7a650f756a27227f0f7", "title": "Unicron: Economizing Self-Healing LLM Training at Scale"}, {"paperId": "a2eba36b34833621fa70bcc63ba239846bfd529a", "title": "GEMINI: Fast Failure Recovery in Distributed Training with In-Memory Checkpoints"}, {"paperId": "e97addc2c9d137ca53a73d41ad59083c1a4cf214", "title": "Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da", "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"}, {"paperId": "7c25adf2ddb35df05a61c697da97efb8583d77df", "title": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "041edc8b14bdd0e5627377956fd0e6c6c011146a", "title": "Machine Learning Model Sizes and the Parameter Gap"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "5ce7d930b87a16ae9a7b5f789181a6e021f4df5d", "title": "Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "10f3ca78e194552427ebe9173b19d1b910469e27", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "96a3afc3e3fd6ef727b6aea846f54f59283a473e", "title": "TensorOpt: Exploring the Tradeoffs in Distributed DNN Training with Auto-Parallelism"}, {"paperId": "584cb9c2f7c2ad1f3bd4a4491b65155b90e93f0c", "title": "Deep Learning Training in Facebook Data Centers: Design of Scale-up and Scale-out Systems"}, {"paperId": "b6dd99f857e61a57cf4940c088ed99cf8514ebc6", "title": "Resource Elasticity in Distributed Deep Learning"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "6b6237e90ba4678ee4647191523af7c655ccdd5f", "title": "Lineage stash: fault tolerance off the critical path"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4c3ae07aab62ad4aaa7448be0a0ed834b624b5a0", "title": "Themis: Fair and Efficient GPU Cluster Scheduling for Machine Learning Workloads"}, {"paperId": "64957205022593d904070a399a8d5675a08ffaf4", "title": "Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the Limbo of Resources"}, {"paperId": "54ddd67944520f249b906ba4e817188686eae94d", "title": "Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "121ec08eea488c8294b07de72190375f8675e166", "title": "Data Dropout: Optimizing Training Data for Convolutional Neural Networks"}, {"paperId": "7dd7198bb8a61dd22879068e8c4619b32f0470f8", "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "93a06eb066fe58ed7d036e46e4cee53483e16bb8", "title": "Optimus: an efficient dynamic resource scheduler for deep learning clusters"}, {"paperId": "e2c8726d092aea573e69f5b0a2654225883cfacf", "title": "Horovod: fast and easy distributed deep learning in TensorFlow"}, {"paperId": "45cf7c4c7b3a0d95d51ba9e6c9a22a6a95d787b3", "title": "Online Job Scheduling in Distributed Machine Learning Clusters"}, {"paperId": "78522d5ab004d27241bc4e34e5cf96d0e5e2630b", "title": "Failures in Large Scale Systems: Long-term Measurement, Analysis, and Implications"}, {"paperId": "6fc2ccc1cbb555955291b0989251bd77240dd551", "title": "ImageNet Training in Minutes"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "25fb5a6abcd88ee52bdb3165b844c941e90eb9bf", "title": "Revisiting Distributed Synchronous SGD"}, {"paperId": "e69c8b5df8a4178b1c8c7f154a761147a6f030be", "title": "Project Adam: Building an Efficient and Scalable Deep Learning Training System"}, {"paperId": "50684b147b752a07c313cb73d864f7b21bd8b703", "title": "Scaling Distributed Machine Learning with the Parameter Server"}, {"paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "title": "One weird trick for parallelizing convolutional neural networks"}, {"paperId": "a191f0343a960f5a78bb3eec9732371386173d4e", "title": "Introducing"}, {"paperId": "1b5de61ad031cd382436829b6dbd3f47f26dbde1", "title": "Resiliency at Scale: Managing Google's TPUv4 Machine Learning Supercomputer"}, {"paperId": "283e56e843edc2ab599b0cf8218b936a89a11875", "title": "BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models"}, {"paperId": "aa6ddad0a84eaa004e49142981d05c5f36cc585e", "title": "Hanayo: Harnessing Wave-Like Pipeline Parallelism for Enhanced Large Model Training Efficiency"}, {"paperId": null, "title": "2023. GPT-4 Technical Report"}, {"paperId": "08cdaf650064922991a6004a362fb6d4a0ac66b2", "title": "Detection and Prevention of Silent Data Corruption in an Exabyte-scale Database System"}, {"paperId": null, "title": "Large Scale OpenCLIP: L/14, H/14 AND G/14 Trained on LAION-2B"}, {"paperId": null, "title": ". ChatGPT: Language models for task-oriented dialogue"}, {"paperId": null, "title": ". The Technology Behind BLOOM Training"}, {"paperId": "ca1402619a80c140c650961d899319c2928744f0", "title": "Piper: Multidimensional Planner for DNN Parallelization"}, {"paperId": "920241246c82176173d442fd43214b2762be6e56", "title": "Elastic Resource Sharing for Distributed Deep Learning"}, {"paperId": null, "title": "CheckFreq: Frequent, Fine-Grained DNN Checkpointing"}, {"paperId": "08588107b9e37f9601bb5c801aa46b918cc3c8ec", "title": "A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters"}, {"paperId": null, "title": "PyTorch distributed: experiences on accelerating data parallel training"}, {"paperId": "5adca3c33b9fc1b76bb57053530f7993722b73aa", "title": "A case for redundant arrays of inexpensive disks (RAID)"}, {"paperId": null, "title": "LargeScaleDistributedDeepNet-works"}, {"paperId": null, "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups"}, {"paperId": "627be67feb084f1266cfc36e5aed3c3e7e6ce5f0", "title": "MapReduce: simplified data processing on large clusters"}, {"paperId": null, "title": "2022. Alpa: Au-tomating Inter-and Intra-Operator Parallelism for Distributed Deep Learning"}, {"paperId": null, "title": "2022. Check-N-Run: a Check-pointing System for Training Deep Learning Recommendation Models"}, {"paperId": null, "title": "2022. MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters"}, {"paperId": null, "title": "2022. Detecting silent data corruptionsinthewild"}, {"paperId": null, "title": "2023. TPU v4: An Optically Reconfigurable Supercomputer"}, {"paperId": null, "title": "2022. Training Compute-Optimal Large Language Models"}, {"paperId": null, "title": "MetaAI"}, {"paperId": null, "title": "2022. Varuna: scalable, low-cost training of massive deep learning models"}]}