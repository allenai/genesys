{"paperId": "fd38e1f3c5fa7e770c327180fd68bf8bbfc33a70", "title": "On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code", "abstract": "Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen APIs over time. We study two widely used PLM architectures, i.e., a GPT2 decoder and a RoBERTa encoder, on two downstream tasks, API call and API usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of APIs, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in PLMs across both downstream tasks while achieving comparable or superior performance.", "venue": "ESEC/SIGSOFT FSE", "year": 2023, "citationCount": 5, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario, and to implement five continual learning approaches, including replay-based and regularization-based methods."}, "embedding": {"model": "specter_v2", "vector": [-0.19223880767822266, 0.22458481788635254, -0.5323643684387207, -0.1796187311410904, 0.02491791546344757, -0.3748796284198761, 0.640674352645874, -0.2443462461233139, -0.22666500508785248, -0.08730240911245346, 0.08032648265361786, -0.5595827102661133, 0.6418928503990173, -0.014216724783182144, -0.8333131074905396, 0.3122312128543854, -0.8332546949386597, 0.045432452112436295, 0.29655173420906067, -0.36802875995635986, -0.16711322963237762, -0.5910966992378235, -1.1630890369415283, 0.24467317759990692, 0.5007264614105225, 0.24671392142772675, -0.1383722871541977, 0.8365239500999451, -0.6480007767677307, 0.6902529001235962, 0.3884563148021698, -0.5754647254943848, 0.15220868587493896, 0.08212977647781372, -0.10337843000888824, 0.16271832585334778, -0.04519335925579071, 0.07319221645593643, -0.6495184898376465, 0.9296737313270569, -0.6676077842712402, 0.04112757369875908, 0.30770230293273926, -0.5989698767662048, -0.43302038311958313, 0.4606143832206726, 0.11287758499383926, 1.0697109699249268, -0.2278931736946106, -0.19413061439990997, 0.75652015209198, -0.8193495273590088, 0.19665385782718658, 1.1458841562271118, 0.910497784614563, 0.33549195528030396, -0.34531211853027344, -0.5679641366004944, 0.44981083273887634, -0.47041985392570496, -0.7439084053039551, 0.020808955654501915, -0.3655124306678772, -1.0134854316711426, 2.014972686767578, -0.8813610076904297, -0.1666368544101715, 0.4918936491012573, 0.43832865357398987, 1.1378121376037598, -0.6491722464561462, -0.582082986831665, -0.15244249999523163, 0.17022804915905, 0.3591259717941284, 0.8075852394104004, -0.29197999835014343, 0.16051587462425232, -0.5061620473861694, -0.47387436032295227, 0.2513348460197449, 0.26847442984580994, 0.06116127967834473, -0.46212509274482727, 0.1584758758544922, 0.39384138584136963, 0.16196325421333313, 0.8659663200378418, -0.1623695343732834, 0.9487091302871704, 0.7912465333938599, 0.4847544729709625, 0.22803080081939697, 0.3469003438949585, -0.3617178797721863, -0.2569104731082916, -0.7350550889968872, -0.29011788964271545, 0.01787407696247101, 1.1728073358535767, 0.18265943229198456, 0.4810156524181366, -0.5471886992454529, 0.40568193793296814, 1.034604787826538, -0.3317870497703552, 1.0409516096115112, -0.21720290184020996, 0.5241779685020447, -0.2316979169845581, 0.10536585748195648, -0.013032279908657074, -0.022927625104784966, -0.43086209893226624, -0.533404529094696, -1.0136866569519043, -0.5783901214599609, -0.09016495198011398, -0.7366669774055481, 0.6047231554985046, -0.4754743278026581, -0.12408256530761719, -0.0688498392701149, -0.01828872226178646, -0.09073588252067566, 0.41012436151504517, 0.3383745849132538, 0.2231593281030655, 0.025031721219420433, -0.664065420627594, -0.163213849067688, -1.1912034749984741, 0.525233805179596, -0.3202241063117981, 0.7011530995368958, -0.4198768734931946, -1.152925729751587, -0.9167179465293884, -0.9361584186553955, 0.5209237933158875, -0.2963586151599884, 0.21514518558979034, 1.4309282302856445, 0.5490672588348389, -0.7481181025505066, 1.107605218887329, -0.8215140700340271, -0.26144328713417053, 0.7497863173484802, 0.061245065182447433, 0.5277075171470642, -0.39728617668151855, -0.541236937046051, 0.04134195297956467, 0.20139500498771667, -1.103850245475769, -0.6112331748008728, -0.6901158690452576, -0.9913108348846436, -0.07110130041837692, 0.10519911348819733, -0.3772687315940857, 1.5885040760040283, -0.44234833121299744, -0.7936885952949524, 0.4394471049308777, 0.01923988200724125, 0.013241533190011978, 0.28977733850479126, 0.08772413432598114, -0.5644660592079163, -1.1013184785842896, -0.11632398515939713, -0.3046386241912842, 0.4957634210586548, -0.16709502041339874, -0.2117329239845276, 0.2879541218280792, 0.145943745970726, -0.6152162551879883, -0.4406513273715973, 0.2570590376853943, -0.530599057674408, -0.0377173125743866, -0.015305952169001102, 0.7452421188354492, 0.35016098618507385, -0.41075843572616577, 0.3004322350025177, -1.0773392915725708, 0.4558490812778473, -0.05718864127993584, 1.204774260520935, -1.3082140684127808, -1.0005708932876587, -0.3029725253582001, -0.39143791794776917, -0.2221345752477646, -0.7545530200004578, 0.5990590453147888, -0.6150573492050171, 0.4261590242385864, -0.2810719311237335, -1.058821201324463, -0.33155691623687744, -0.1970592588186264, -0.6682068109512329, 0.19061218202114105, 0.19855356216430664, 0.9535844326019287, -1.2016706466674805, 0.5751572251319885, -0.5634033679962158, 0.22388142347335815, -1.1945571899414062, 1.2604055404663086, -0.6525014042854309, -0.06390538811683655, -0.07855131477117538, 0.10895512998104095, 0.12652404606342316, -0.5396884679794312, 0.08129796385765076, -0.10270213335752487, -0.17831113934516907, 0.6376345753669739, 0.49127113819122314, 1.4600094556808472, -1.0322331190109253, 0.36336758732795715, -0.33856892585754395, -0.5667597651481628, 0.2968295216560364, 0.44057369232177734, -0.010717114433646202, -0.2589832544326782, 0.20525521039962769, 0.5116546750068665, -0.6148871183395386, -0.12339967489242554, 0.4885108172893524, 0.3167603313922882, -0.23667709529399872, 0.4484218657016754, 0.8073573708534241, -0.5463231801986694, 0.699367880821228, 0.13522113859653473, 1.169308066368103, 0.5478885173797607, 0.2649112641811371, 0.0657898485660553, 0.35934263467788696, -0.6364670395851135, -0.007120313122868538, 0.26504382491111755, 0.8575693964958191, 0.9010642170906067, 0.5784839391708374, -0.6723101735115051, -0.4756229817867279, -0.0722227692604065, 0.7923784852027893, 1.5944139957427979, -0.5597845315933228, 0.13136988878250122, -0.5519579648971558, -0.6889449954032898, -0.007432075683027506, 0.6760379672050476, -0.5667285323143005, -0.682104229927063, -0.4142109155654907, -1.5663609504699707, 0.5760700106620789, 0.005405204836279154, 0.8340649008750916, -0.22992472350597382, -0.06554978340864182, -0.10639426112174988, 0.7425017356872559, -0.12414901703596115, -1.0725853443145752, 0.45571866631507874, -0.6300647854804993, -0.2554926574230194, 0.24261276423931122, 0.009592178277671337, 0.3303912281990051, -0.37726345658302307, 1.1462737321853638, 0.05959105119109154, -0.4346136152744293, 0.8938806056976318, 0.5491203665733337, -0.6289620399475098, -1.2782831192016602, 0.6197156310081482, -0.11030380427837372, -0.14865350723266602, 0.6014539003372192, 0.062037065625190735, 0.07191956788301468, 0.011361815966665745, -0.7120627760887146, -0.46777334809303284, 0.19685891270637512, 0.188680961728096, 0.2212432473897934, 0.06474804133176804, 0.15179264545440674, -1.3584575653076172, 1.1447222232818604, -0.5281305313110352, -0.8899058103561401, 0.2184937447309494, -0.8541821241378784, -0.3194308280944824, 0.8358621001243591, -1.0845015048980713, -0.6296142935752869, -1.0611902475357056, 0.27010926604270935, -0.1662878841161728, 0.21366465091705322, 0.17976480722427368, 0.8014250993728638, -0.01745295524597168, 0.7515889406204224, 0.4253886640071869, 0.5338550209999084, -0.18280665576457977, 0.04576175659894943, -0.823666512966156, 0.566709041595459, -0.004428468178957701, 0.540157675743103, -0.018825393170118332, -0.652009904384613, -0.2739484906196594, -0.5319252014160156, -0.4407777786254883, -0.006322080735117197, -0.2515423893928528, -0.3822512626647949, -0.7331173419952393, -0.5455392003059387, -0.3989291787147522, -0.921504557132721, -0.6016974449157715, 0.0770430862903595, -0.2682002782821655, -0.2999536097049713, -1.0531209707260132, -0.9117689728736877, -0.48474377393722534, -0.5037693977355957, -0.930374026298523, 0.023288149386644363, -0.026665344834327698, -0.19949358701705933, -0.7522154450416565, 0.13243454694747925, -0.6294593811035156, 1.2164173126220703, -0.7553386688232422, 1.0348880290985107, 0.43888288736343384, -0.19866524636745453, -0.30970245599746704, 0.3204025626182556, 0.7777580618858337, -0.048960790038108826, 0.5430837273597717, -0.6336286067962646, -0.39034155011177063, 0.2069888412952423, -0.2372487336397171, -0.16766482591629028, -0.1837809979915619, 1.0003421306610107, 0.24528267979621887, -0.3473135828971863, 0.4710034430027008, 1.5763366222381592, -0.5217982530593872, -0.10393251478672028, 0.29063329100608826, 0.7688998579978943, -0.046560101211071014, -0.08360423892736435, 0.7029916048049927, 0.20085030794143677, -0.2410832941532135, 0.29569774866104126, 0.2189689427614212, -0.33204665780067444, -0.6043838858604431, 0.5412354469299316, 1.6778267621994019, 0.1682523638010025, 0.16880176961421967, -1.3357484340667725, 0.7756139039993286, -1.2294765710830688, -0.38782617449760437, 0.5953838229179382, 1.015113353729248, 0.714391827583313, -0.3161485493183136, -0.6484079957008362, -0.6637385487556458, 0.3659612238407135, -0.10033193975687027, -0.6576407551765442, -0.3287111818790436, 0.3098148703575134, -0.012139874510467052, 0.5109097361564636, 0.2082827091217041, -0.2551090121269226, 0.14261439442634583, 14.744939804077148, 0.7173805832862854, 0.02747769095003605, 1.092752456665039, 0.34573256969451904, 0.35217544436454773, -0.49312451481819153, -0.25352993607521057, -0.8862349987030029, -0.1917518973350525, 1.2125264406204224, -0.3831499516963959, 1.0941598415374756, 0.6266921758651733, -0.46919938921928406, -0.028540167957544327, -0.5622082352638245, 0.6029361486434937, 0.5917495489120483, -1.3299769163131714, 0.2778365910053253, -0.22131992876529694, 0.9810947179794312, 1.039473533630371, 1.0102113485336304, 1.2562613487243652, 0.5815399885177612, -0.02752910554409027, 0.64065021276474, 0.20670592784881592, 0.9574978351593018, 0.16949725151062012, 0.5485408902168274, 0.7157824039459229, -0.5979726314544678, -0.43538257479667664, -0.8707914352416992, -0.9016371965408325, -0.17653638124465942, -0.01121344044804573, -0.9153808355331421, -0.6958001255989075, -0.1249489039182663, 0.8307895660400391, -0.38405391573905945, 0.44026193022727966, -0.3230859637260437, 0.557555079460144, 0.3130451738834381, 0.27960288524627686, -0.1854514628648758, 0.7528586387634277, -0.19561824202537537, -0.06838958710432053, 0.197244331240654, -0.7113456726074219, -0.218931645154953, 0.6614621877670288, -0.8413493633270264, -0.18916857242584229, -0.2640621066093445, -0.3689638376235962, -0.40957802534103394, 0.22794422507286072, 0.6832809448242188, 0.19604043662548065, -1.144630789756775, 0.1392907202243805, 1.0381479263305664, 0.5550499558448792, -0.3278740644454956, 0.19353742897510529, 0.3739996552467346, 0.0807710736989975, 0.06502612680196762, 0.3680277168750763, -0.2613748013973236, -0.7369084358215332, -0.5436831116676331, -0.10808480530977249, 0.2636326849460602, -0.6563429832458496, -1.0958595275878906, 0.8599348068237305, -0.37749889492988586, -0.6004769802093506, 0.27853402495384216, -0.5400437712669373, -0.23300442099571228, 0.28266751766204834, -1.4380691051483154, -0.06886891275644302, 0.02982822246849537, -0.2598833441734314, -0.2595278322696686, -0.7075983881950378, 0.9779477715492249, 0.4645979404449463, -0.3420964479446411, 0.028756823390722275, 0.41692158579826355, -0.3312278687953949, 0.2434360831975937, -0.877185046672821, 1.1311500072479248, 0.09003603458404541, -0.684047520160675, 0.6735097169876099, -0.009135598316788673, -0.18315336108207703, -0.8508554100990295, -0.7048337459564209, 0.18710365891456604, -1.1481096744537354, 0.3731134235858917, -0.5272412896156311, -1.0619648694992065, 0.6824550032615662, 0.443335622549057, -0.3397710621356964, 0.25757846236228943, 0.20083223283290863, -0.8662384152412415, -0.15254423022270203, -0.8589102029800415, -0.22108948230743408, 0.4853256642818451, -0.9050089716911316, -0.5095050930976868, 0.27500665187835693, 0.2965298593044281, -0.8836734294891357, -0.7734092473983765, -0.27425453066825867, -0.2664479613304138, -0.31623515486717224, 0.48269355297088623, -0.19634060561656952, 0.6962045431137085, 0.6748349666595459, 0.2240580916404724, -0.7006235718727112, 0.004370240028947592, -0.8432655930519104, 0.356837660074234, 0.10466944426298141, 0.956445038318634, -0.6689696311950684, 0.17987877130508423, 1.0226078033447266, -0.011649545282125473, -0.05774705484509468, -0.634068489074707, -0.44111016392707825, 0.4370257258415222, -0.6795855164527893, 0.5661622881889343, -0.06344325095415115, -0.06368212401866913, -0.12621665000915527, 0.7871297597885132, 0.633285403251648, -0.573318362236023, -0.6189828515052795, 0.6633456945419312, 0.1159292683005333, -0.4301002621650696, -0.3824101984500885, -0.26131948828697205, -1.1454668045043945, 0.08505438268184662, -1.2599986791610718, 0.054782934486866, -0.32971081137657166, -0.19110475480556488, 0.19827142357826233, -0.16913370788097382, 0.002068835310637951, 0.4697360098361969, -0.6437968611717224, -0.9228869080543518, -0.23922723531723022, -0.5372577905654907, 1.0504404306411743, 0.6067557334899902, -0.9852768778800964, -0.06292880326509476, 0.44310125708580017, 0.40908387303352356, 0.41755029559135437, 0.56573086977005, -0.8374923467636108, -0.8944591283798218, -1.4755568504333496, 0.5552928447723389, -0.47653764486312866, -0.32156941294670105, -0.7381617426872253, 0.5497176051139832, 0.3318231701850891, -0.439337819814682, 0.38361024856567383, -0.1837979108095169, -0.9680101871490479, -0.14851249754428864, 0.24303975701332092, -0.37142056226730347, 0.38409188389778137, 0.6164335012435913, -0.15820680558681488, -0.4078071117401123, 0.29501965641975403, -0.032325275242328644, -1.1224159002304077, -0.23763105273246765, 0.612650454044342, -0.5780547857284546, 0.26154643297195435, -0.2548424303531647, 0.45423462986946106, -1.1130995750427246, -0.23527851700782776, 0.16005878150463104, 0.39901307225227356, 0.2963878810405731, 1.2386451959609985, -0.016853976994752884, -1.0734405517578125, 0.4321047365665436, 0.34921202063560486, 0.22365209460258484, 0.021055489778518677, 0.5117122530937195, 0.37775641679763794, -0.5455628037452698, 0.13722386956214905, 0.5212335586547852, 0.1819130778312683, -0.4229111671447754, 0.3872314989566803, 0.7454293370246887, -0.1266918033361435, 0.052885379642248154, 1.023732304573059, 0.08894605934619904, -1.2069978713989258, 0.10098680108785629, -1.0752555131912231, -0.1579311490058899, -0.8103779554367065, 0.49441275000572205, -0.09092279523611069, 0.20571839809417725, 0.24070727825164795, -0.18235425651073456, -0.00202431483194232, -0.14586324989795685, 0.02967573143541813, 0.976920485496521, -0.08360707759857178, -0.5418930649757385, 1.2298046350479126, 0.8978433012962341, -0.5763207674026489, -0.9766737222671509, -0.6181752681732178, -0.25731489062309265, -0.4831411838531494, 0.14734110236167908, -0.44121596217155457, -0.8264932036399841, 0.7195068001747131, 0.33360716700553894, 0.12432385981082916, -0.08939717710018158, -0.05482260510325432, -0.4014100432395935, 0.38426271080970764, 0.38555169105529785, -0.5391873121261597, -0.3478320240974426, 1.1370948553085327, 1.3822752237319946, -0.9484566450119019, 0.4344042241573334, -0.05944910645484924, -0.39899301528930664, 1.3360075950622559, 0.6657540798187256, -0.013317219913005829, 1.0823991298675537, -0.3248926103115082, -0.36920371651649475, -0.022446628659963608, -1.1299761533737183, 0.12629921734333038, 0.46646419167518616, 0.7961999177932739, 1.1426962614059448, 0.2986765503883362, 0.20982800424098969, 0.7927274703979492, 0.1815882921218872, 0.36548373103141785, 1.1159695386886597, 0.47989529371261597, 0.245749369263649, -0.09825152903795242, 0.11322680860757828, 0.9174830317497253, -0.6936746835708618, -0.29937347769737244, 0.8619529604911804, 0.5574114918708801, 0.33034858107566833, 0.3194652497768402, 0.6698973178863525, 0.0899127870798111, 0.6681091785430908, 0.3437621295452118, 0.42984408140182495, -1.0615290403366089, -0.42061516642570496, -0.43285343050956726, -0.6439808011054993, 0.15748009085655212, -0.06942443549633026, -0.7967254519462585, -0.26672831177711487, -0.1214112937450409, 0.34152552485466003, -0.01205508690327406, 0.41739290952682495, 0.6923316121101379, 0.4914450943470001, 0.7954099774360657, -0.16980719566345215, -0.1876152604818344, -0.5712472200393677, -1.085848331451416, 0.19855108857154846, -0.25531721115112305, 0.016265593469142914, -0.40289032459259033, -0.5196612477302551, -0.034512732177972794]}, "authors": [{"authorId": "1820831988", "name": "M. Weyssow"}, {"authorId": "2148928671", "name": "Xin Zhou"}, {"authorId": "35276441", "name": "Kisub Kim"}, {"authorId": "2150912791", "name": "David Lo"}, {"authorId": "9460712", "name": "H. Sahraoui"}], "references": [{"paperId": "9a7b9515b66bf83c9c808626206eabe9a8837c22", "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models"}, {"paperId": "1d74875aa4f415cb2c60b17fd1eb3e4ae543bfe1", "title": "Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models"}, {"paperId": "c0309a9906d11e7c28d2af2f4b45c8c298613109", "title": "Continual Few-Shot Learning Using HyperTransformers"}, {"paperId": "d15091e73f7295ba8c0bdbabe0b7188307c96039", "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective"}, {"paperId": "df1cc92fba512ce7d28d1d608ea19f18cda185ca", "title": "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence"}, {"paperId": "e9fc39f56abbc6b8aed1e05496d985e70345a95a", "title": "An extensive study on pre-trained models for program understanding and generation"}, {"paperId": "c5f208832fa22c50061d8480b92556750d785fd3", "title": "Alleviating Representational Shift for Continual Fine-tuning"}, {"paperId": "393fd928d39d067f865b6ebe2a97b33604ca02cf", "title": "Memory Efficient Continual Learning with Transformers"}, {"paperId": "4b27f18bff43d605805c92696a979714ced0b805", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation"}, {"paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "title": "A systematic evaluation of large language models of code"}, {"paperId": "b10c6201fec56772fa97bbcaf37b4ead61b6270a", "title": "DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "13783f9d98b47a5b525bc3bec01d02088d5b1ef7", "title": "Assessing Generalizability of CodeBERT"}, {"paperId": "e5b2e2a284db5ba7c2c011daba9769d2c56b6586", "title": "Towards Out-Of-Distribution Generalization: A Survey"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "77a096d80eb4dd4ccd103d1660c5a5498f7d026b", "title": "Dynabench: Rethinking Benchmarking in NLP"}, {"paperId": "097fcd76adb0447ec3525af154e37739d809f04f", "title": "Avalanche: an End-to-End Library for Continual Learning"}, {"paperId": "bec60a7f7d6e72a23a6d3d3c90a6936103c7873c", "title": "An Empirical Study on the Usage of BERT Models for Code Completion"}, {"paperId": "0646bb09db4d1ba24150e69b71edcd4aff691b3c", "title": "Unified Pre-training for Program Understanding and Generation"}, {"paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"}, {"paperId": "373bc164d7b552f8782988e7da6b0d00092a20b0", "title": "Continual Lifelong Learning in Natural Language Processing: A Survey"}, {"paperId": "40848b41ed8c9c255ecd8a920006877691b52d03", "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts"}, {"paperId": "5ac03eba3acf4b385c3cbe09064c9ef8d0de70f9", "title": "A Software-Repair Robot Based on Continual Learning"}, {"paperId": "87fe4a67e4cb5b8cdbc5da4d98888c037fb7d043", "title": "Learning Autocompletion from Real-World Datasets"}, {"paperId": "34b6871b40d3389f1d5c2a89fc75664d8619490c", "title": "Embracing Change: Continual Learning in Deep Neural Networks"}, {"paperId": "d8a842014e2679569b69db07bc0bfd0d3022aa0e", "title": "Incremental Event Detection via Knowledge Consolidation Networks"}, {"paperId": "f23a0e443fe931aa2fed932421bf47c1a4fcf619", "title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"}, {"paperId": "f16d68f8a949d8995322abb229436fa0bd45fb62", "title": "A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "807a348e8ed491c20df52f766293d9504c595e2e", "title": "Few-Shot Class-Incremental Learning"}, {"paperId": "97f08c1ae8ca5ddf5948c66bfbbc0546ac154807", "title": "Pretrained Transformers Improve Out-of-Distribution Robustness"}, {"paperId": "0dfe706526e5234338411489e1826b4060acc4e8", "title": "CC2Vec: Distributed Representations of Code Changes"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "1904d633fca15140e35d893637232803b6dde6d9", "title": "Learning under Concept Drift: A Review"}, {"paperId": "1a195105143918fdb4f64201d6a8e927a2a19aca", "title": "CLCDSA: Cross Language Code Clone Detection using Syntactical Features and API Documentation"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "7dcb67b19d0397821b84dd7e97bf6b2c4fe563e9", "title": "Continual learning: A comparative study on how to defy forgetting in classification tasks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "338c5f71f98b3a5ab5e1ce1b21ae232b9bfb0648", "title": "Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation"}, {"paperId": "157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3", "title": "Transfer Learning in Natural Language Processing"}, {"paperId": "d02285385fd2b53a6100b8d62421bba6693b788c", "title": "When Code Completion Fails: A Case Study on Real-World Completions"}, {"paperId": "8ace1951c95f9bbbcf83571ff6c0579521507e2e", "title": "The adverse effects of code duplication in machine learning models of code"}, {"paperId": "de016c4e618dd15fd94d76bd16a7c7812ea71902", "title": "Towards continual learning in medical imaging"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "49164d216b7e7968ded2d9863af161191f2c32e5", "title": "Summarizing Source Code with Transferred API Knowledge"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "9ea50b3408f993853f1c5e374690e5fbe73c2a3c", "title": "Continual Lifelong Learning with Neural Networks: A Review"}, {"paperId": "9958590c281e7a7b524dd594635037143c632c21", "title": "Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "713b0d9005944f80af00addc81b162ca74ea4b14", "title": "Memory Aware Synapses: Learning what (not) to forget"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "0eddc1600c51f524887b3c5fe3d972e3c4af4dcc", "title": "CORe50: a New Dataset and Benchmark for Continuous Object Recognition"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "a99d857ecc78316a0d9a774972b775058d5644ca", "title": "Continual Learning Through Synaptic Intelligence"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "ab2010c53580921fd32b45a2efddfad123d20b0c", "title": "Evaluating the evaluations of code recommender systems: A reality check"}, {"paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a", "title": "Learning without Forgetting"}, {"paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753", "title": "Progressive Neural Networks"}, {"paperId": "37f5f64c648d547ddd57b89f6bdcd9cecd05f205", "title": "CodeHow: Effective Code Search Based on API Understanding and Extended Boolean Model (E)"}, {"paperId": "38f35dd624cd1cf827416e31ac5e0e0454028eca", "title": "Regularization of Neural Networks using DropConnect"}, {"paperId": "0ec1af9988fca45cd28908594b12df0f638fc302", "title": "Using twinning to adapt programs to alternative APIs"}, {"paperId": "e81d908f98795af66f7246498f29c0bb4b0e992f", "title": "Graph-based mining of multiple object usage patterns"}, {"paperId": "2722b9e5ab8da95f03e578bb65879c452c105385", "title": "Catastrophic forgetting in connectionist networks"}, {"paperId": "83cdbbfad2d4bb4c348e09cf3db779f020809b05", "title": "Learning in the Presence of Concept Drift and Hidden Contexts"}, {"paperId": "81a7c1b850c4161da71998042d0f2de967515e79", "title": "CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation"}, {"paperId": "c6e6cb19e7055f3d0616c3314a85b0914132ae40", "title": "CodeS: A Distribution Shift Benchmark Dataset for Source Code Learning"}, {"paperId": "cb123f1afd67fb8bae15dc876709c842b626c49c", "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models"}, {"paperId": "ff62f1782e73c4c0766c24bcaf557fe497f6469e", "title": "How robust are pre-trained models to distribution shift?"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1", "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}, {"paperId": null, "title": "2023. On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code: Replication Package"}, {"paperId": null, "title": "Continual ne-tuning"}, {"paperId": null, "title": "Received 2023-02-02; accepted 2023-07-27"}, {"paperId": null, "title": "ESEC/FSE \u201923, December 3\u20139, 2023, San"}, {"paperId": null, "title": "ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA"}, {"paperId": null, "title": "We comparefivecontinuallearning methods,includingreplay-based and regularization-based approaches"}]}