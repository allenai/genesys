{"paperId": "0ce85c95bd0563c50f36784d87153f37a48dc1a1", "title": "CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking", "abstract": "Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others. To realize the idea, we enhance the vanilla tokenization and model architecture of a BERT model, \n \nconstruct and utilize attention masks, and introduce a new pre-training algorithm. We pre-train this BERT-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. It also substantially outperforms other general pre-training techniques of code understanding models.", "venue": "Proc. ACM Softw. Eng.", "year": 2024, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work enhances the vanilla tokenization and model architecture of a BERT model, construct and utilize attention masks, and introduces a new pre-training algorithm that substantially outperforms other general pre-training techniques of code understanding models."}, "embedding": {"model": "specter_v2", "vector": [-0.0633104145526886, 0.5218942761421204, -0.9655844569206238, 0.2110055387020111, -0.2077941745519638, -0.556559681892395, 0.4086770713329315, 0.09908352792263031, 0.37548428773880005, -0.3378346860408783, 0.22159983217716217, -0.7171210646629333, 0.8581663966178894, 0.0314752459526062, -0.48481425642967224, 0.419996052980423, -0.5806723833084106, -0.11211993545293808, 0.11820146441459656, -0.17246760427951813, 0.23477692902088165, -0.7905639410018921, -1.0374574661254883, 0.2180907428264618, 0.9860525131225586, 0.1295771598815918, -0.1443452090024948, 0.7347267866134644, -0.45358824729919434, 1.0398842096328735, 0.5396785140037537, -0.43999505043029785, -0.005872869398444891, 0.1561906486749649, -0.3029358386993408, -0.15782414376735687, -0.19665957987308502, 0.1277601420879364, -0.33240455389022827, 0.735180139541626, -0.5809699296951294, -0.09906370937824249, 0.2643798887729645, -0.9325488209724426, -0.29291480779647827, 0.803992748260498, 0.11697223037481308, 0.7619882822036743, -0.5058813691139221, -0.027944136410951614, 1.3756160736083984, -0.9903262853622437, -0.00568836135789752, 0.9154601693153381, 0.4747573435306549, 0.37779155373573303, -0.30058395862579346, -0.3305968940258026, 0.09975168108940125, -0.017636770382523537, -0.7639086246490479, -0.34292763471603394, -0.09009726345539093, -0.7353055477142334, 2.6030168533325195, -0.3651784062385559, -0.3442780077457428, 0.03562457859516144, 0.07290583103895187, 0.931270182132721, -0.2099919319152832, -0.7379173040390015, -0.18008489906787872, 0.06100109964609146, 0.3747096657752991, 1.2026976346969604, 0.10902111232280731, 0.02554571069777012, -0.3839515745639801, -0.6558524370193481, 0.26769545674324036, 0.3576093316078186, 0.31555497646331787, -0.5109084844589233, 0.18428806960582733, 0.594825267791748, -0.038429584354162216, 1.1001648902893066, 0.11341691762208939, 1.0250658988952637, 0.8092275857925415, 0.07299067080020905, -0.30809274315834045, 0.6941842436790466, -0.17772331833839417, -0.24818895757198334, -0.7940746545791626, -0.2533493638038635, 0.12159007042646408, 0.9382269978523254, 0.328876793384552, 0.7718865871429443, -0.6563131213188171, -0.06030341982841492, 1.1458206176757812, -0.05411230027675629, 0.20847263932228088, -0.2728366553783417, 0.28041672706604004, 0.032037507742643356, -0.056159552186727524, -0.25985074043273926, 0.47396814823150635, -0.3432534337043762, -0.31255432963371277, -0.9714199304580688, -0.7285658121109009, 0.2571258842945099, -0.9588108062744141, 0.31780678033828735, -0.7208713293075562, 0.27666381001472473, -0.21808119118213654, -0.22979258000850677, 0.4731692671775818, 0.20087403059005737, 0.15192633867263794, 0.2935214340686798, 0.5745223164558411, -1.026728630065918, -0.3662586212158203, -1.5357308387756348, 1.0594353675842285, -0.29708752036094666, 0.018968382850289345, -0.4573415517807007, -1.0611225366592407, -0.8529880046844482, -1.2116609811782837, 0.4190381169319153, -0.6660436391830444, 0.20850548148155212, 1.1213862895965576, 0.939338207244873, -0.9317573308944702, 0.8927689790725708, -0.767946720123291, 0.18879884481430054, 0.5357546210289001, 0.09216630458831787, 0.4531256854534149, -0.26026299595832825, -0.7975380420684814, 0.44540396332740784, 0.1761675328016281, -1.299643874168396, -0.5713112950325012, -0.5252945423126221, -1.6587791442871094, 0.10145152360200882, 0.5815720558166504, 0.05495837703347206, 1.085425615310669, 0.06117884814739227, -0.3880607485771179, 0.8002365827560425, -0.5550276041030884, 0.2077874094247818, -0.25884512066841125, -0.013140460476279259, -0.491120845079422, -0.8484532833099365, 0.25311294198036194, -0.731255054473877, 0.6180136203765869, -0.835664689540863, 0.23169174790382385, 0.3817749619483948, 0.1705653965473175, -0.9504464268684387, -0.01609950140118599, 1.1207643747329712, -0.5148677229881287, 0.2143385112285614, -0.46839889883995056, 0.8297297954559326, 0.1779550015926361, -0.3533255457878113, -0.4810560941696167, -0.9075369238853455, 0.856697678565979, -0.2589132487773895, 1.7143938541412354, -0.9659766554832458, -1.011358618736267, -0.0729416087269783, 0.05222488194704056, 0.42474156618118286, -0.17331983149051666, 0.5720657706260681, -1.0496025085449219, 0.9191240072250366, -0.7645770311355591, -0.7919831275939941, 0.10313519835472107, -0.00395004078745842, -0.5211421251296997, 0.009377620182931423, 0.375660240650177, 1.1903101205825806, -0.9029761552810669, -0.03329813852906227, 0.10658452659845352, 0.050918012857437134, -1.1450127363204956, 1.275405764579773, -0.46160784363746643, -0.29234349727630615, -0.051320772618055344, 0.24940061569213867, 0.3592699468135834, -0.5155154466629028, 0.3218301832675934, -0.04151739180088043, -0.3207029104232788, 0.4817308187484741, 0.024123670533299446, 0.8599668741226196, -0.8364814519882202, 0.568803071975708, 0.025542404502630234, -0.8643979430198669, 0.289534330368042, 0.42913514375686646, 0.06863092631101608, 0.02804550528526306, 0.2255224585533142, 0.23720863461494446, -0.4353134036064148, -0.36167559027671814, 0.349836140871048, 0.6869927048683167, -0.20896199345588684, 0.49977439641952515, 0.43578341603279114, -0.6271066069602966, 0.5488980412483215, 0.3281382918357849, 1.2711235284805298, 0.4788927733898163, 0.8804243803024292, 0.1446438580751419, 0.5141851305961609, -0.34043964743614197, 0.0302219670265913, 0.20078858733177185, 0.6702352166175842, 0.6925841569900513, 0.9916540384292603, -0.4718835949897766, -0.5088790655136108, 0.3493098020553589, 0.8060435652732849, 1.4422829151153564, -0.37888115644454956, -0.856249213218689, -0.6798571944236755, -0.8384954929351807, -0.12458744645118713, 0.52439284324646, -0.5426467657089233, -0.7245744466781616, -0.6052744388580322, -1.0655854940414429, 1.0226067304611206, 0.6487326622009277, 1.0460697412490845, -0.12540720403194427, -0.16130486130714417, -0.39067280292510986, 0.35754677653312683, -0.8341363072395325, -0.6156936287879944, 0.6324692964553833, 0.049576278775930405, -0.6929665803909302, 0.7967469096183777, -0.25308355689048767, 0.31325024366378784, -0.44992366433143616, 1.2448158264160156, 0.6390654444694519, -0.7343952059745789, 0.35271400213241577, 0.29544782638549805, -0.5305559635162354, -1.0050759315490723, -0.022002093493938446, -0.5076756477355957, -0.6116521954536438, 0.953356146812439, 0.3796975910663605, 0.502605140209198, 0.35527515411376953, -0.7533535957336426, -0.363693505525589, 0.053415704518556595, 0.33317825198173523, -0.05813141167163849, 0.10451880842447281, -0.7071786522865295, -1.1925727128982544, 0.40613439679145813, -0.25137859582901, -0.33402085304260254, 0.38327381014823914, -0.5751221776008606, -0.20518775284290314, 0.8224647045135498, -0.4385126531124115, -0.17774003744125366, -1.1206250190734863, 0.4972463846206665, -0.11673594266176224, 0.08033407479524612, 0.12674154341220856, 0.06792321801185608, -0.4127460718154907, 0.5506672859191895, 0.6631537079811096, 0.3119540512561798, 0.10858852416276932, 0.08114515244960785, -0.9073345065116882, 0.1491076797246933, -0.271710067987442, 0.84245365858078, -0.17507240176200867, -0.438825398683548, -0.3674553334712982, -0.1516346037387848, -0.3552922010421753, 0.0746246874332428, 0.16709992289543152, 0.21375210583209991, -0.482198566198349, -0.930088460445404, -0.47222402691841125, -1.196283221244812, -0.27671268582344055, 0.1498521864414215, -0.6902207732200623, -0.007365872152149677, -0.7081615328788757, -0.6889597773551941, -0.651107132434845, 0.019676195457577705, -1.3915339708328247, 0.24426306784152985, -0.15630680322647095, -0.19069233536720276, -0.6533152461051941, 0.26883235573768616, -0.24542571604251862, 1.1742424964904785, -0.2663581967353821, 1.2927210330963135, -0.03143811970949173, -0.43872132897377014, -0.05684453248977661, -0.003333117812871933, 0.7051228284835815, 0.14388497173786163, 0.7125129699707031, -0.6261282563209534, 0.3385144770145416, -0.047991152852773666, -0.2201324850320816, 0.16277571022510529, -0.22114154696464539, 1.185248613357544, 0.09613781422376633, -0.8096774220466614, 0.34662291407585144, 1.7315587997436523, -0.5570811033248901, 0.46369630098342896, 0.27030307054519653, 1.2508128881454468, 0.17469558119773865, -0.187768816947937, 0.8597143292427063, 0.12327193468809128, 0.08316894620656967, 0.7677561640739441, -0.020928217098116875, 0.04898754879832268, -0.2076617032289505, 0.4467332065105438, 1.140877604484558, -0.33309540152549744, 0.5316081643104553, -1.6648575067520142, 1.2101860046386719, -1.1510649919509888, -0.6487852334976196, 0.2749924659729004, 0.6327544450759888, 0.6049222946166992, -0.4610177278518677, -0.5922866463661194, 0.06910243630409241, 0.7975788116455078, 0.34359246492385864, -0.4376750588417053, -0.8410664796829224, 0.49804821610450745, 0.6646581888198853, 0.5122421979904175, 0.5438088178634644, -0.4317414164543152, 0.31369277834892273, 14.541213035583496, 0.8261435031890869, 0.1564265936613083, 0.7815716862678528, 0.3158513009548187, 0.7982355356216431, -0.7130221128463745, 0.4244902431964874, -1.0462710857391357, -0.056899234652519226, 0.906728982925415, -0.17886120080947876, 0.5111159682273865, 0.7295787334442139, -0.38246721029281616, 0.09365780651569366, -0.3938505947589874, 0.5364550352096558, 0.6561655402183533, -1.2282180786132812, 0.05782891437411308, 0.27812692523002625, 0.6060338020324707, 0.11264031380414963, 0.7618264555931091, 0.9276849627494812, 0.78885418176651, -0.922107458114624, 0.05236192047595978, -0.3725605607032776, 1.1511059999465942, -0.06611698120832443, 0.4730013310909271, 0.3706056773662567, -1.1462721824645996, -0.3551744818687439, -0.7116993069648743, -1.1482319831848145, -0.2799709439277649, 0.2756823003292084, -0.9748960733413696, -0.30300405621528625, -0.4480038285255432, 0.8162263035774231, -0.07588428258895874, 0.30766934156417847, -0.5482763051986694, 0.532870888710022, 0.576600193977356, 0.08282451331615448, -0.13472333550453186, 0.8966404795646667, -0.1805533915758133, 0.24257631599903107, -0.3508162498474121, -0.3014145791530609, 0.12177706509828568, 0.7045928835868835, -0.3112119138240814, -0.36564651131629944, -0.5831128358840942, -0.6193935871124268, -0.262626051902771, 0.7756385207176208, 0.4377037286758423, -0.013019019737839699, -1.1472676992416382, -0.11742866784334183, 0.4670777916908264, 0.04663078859448433, -0.8021272420883179, -0.33528298139572144, 0.5155826210975647, 0.11027276515960693, 0.13316605985164642, 0.30104339122772217, -0.7231858372688293, -0.6381367444992065, -0.6594005823135376, -0.43393266201019287, 0.14492245018482208, -0.4952128529548645, -0.37831568717956543, 0.7642489671707153, -0.35813024640083313, -0.8700885772705078, 0.0926293358206749, -1.0603591203689575, -0.7680975794792175, 0.2150193452835083, -1.3071706295013428, -0.45320868492126465, 0.023773502558469772, -0.11258888244628906, -0.6202828884124756, -0.07155182212591171, 1.2435773611068726, -0.06849468499422073, -0.05740204080939293, -0.06686949729919434, -0.07943017035722733, 0.488604873418808, 0.050592612475156784, -1.1731514930725098, 1.2761876583099365, 0.5965533256530762, -0.49777406454086304, 0.7368274927139282, 0.32602235674858093, -0.3140038251876831, -0.6736727952957153, -0.21942120790481567, 0.42911091446876526, -1.3452963829040527, 0.056604791432619095, -0.7906714677810669, -0.9669482111930847, 0.49697408080101013, 0.7218708395957947, -0.014411636628210545, 0.5114935040473938, -0.12186677753925323, -1.0981385707855225, 0.10399746149778366, -1.1541507244110107, -0.15340417623519897, 0.6863550543785095, -1.159603476524353, -0.5818215608596802, 0.12987251579761505, 0.11503534018993378, -0.7050740122795105, -0.5792315006256104, -0.2128433734178543, -0.21626530587673187, -0.5152578353881836, 0.7530474662780762, -0.09715278446674347, 1.378523349761963, 0.6947859525680542, 0.011081736534833908, -0.8259668946266174, -0.11678863316774368, -1.0467995405197144, -0.11350469291210175, 0.2700209319591522, 0.8364075422286987, -0.6410729289054871, 0.39232587814331055, 1.3964790105819702, 0.06446047127246857, -0.12699124217033386, -0.34697675704956055, -0.3577269911766052, 0.2144305408000946, -0.5208411812782288, 0.4499225914478302, 0.6853337287902832, 0.4334459602832794, -0.6515914797782898, 0.4844210743904114, 0.4596753418445587, -0.20052480697631836, -0.6956977844238281, 0.579719066619873, 0.07749831676483154, -0.5211431384086609, -0.17930296063423157, -0.4692729115486145, -0.8020881414413452, 0.42053383588790894, -0.97900390625, 0.24561792612075806, -0.5291803479194641, -0.18332624435424805, 0.5540372729301453, -0.1292821615934372, 0.5102977752685547, 0.01862839050590992, -0.5878076553344727, -0.853429913520813, -0.7579052448272705, -0.35842952132225037, 0.6142446398735046, 0.497222363948822, -0.9585602283477783, 0.4899598956108093, -0.10069263726472855, -0.385407418012619, 0.2595081925392151, 0.5731934905052185, -0.7624697089195251, -0.3371042311191559, -1.0821536779403687, 0.28077661991119385, -0.3502545654773712, -0.13038095831871033, -0.5120672583580017, 0.5403881072998047, 0.1360408365726471, -0.47476089000701904, 0.5356388092041016, 0.016327902674674988, -0.8535534143447876, -0.6635246872901917, 0.6240463852882385, -0.1866479367017746, 0.22818651795387268, 0.26431941986083984, -0.8716089129447937, -0.27178090810775757, 0.14793427288532257, 0.0979711264371872, -1.4731786251068115, -0.5611274242401123, 0.22074133157730103, -0.6867197155952454, 0.31889835000038147, 0.08436435461044312, -0.0438263937830925, -1.4960858821868896, -0.07913406193256378, 0.27525174617767334, 0.21742671728134155, 0.3651672601699829, 0.968487560749054, 0.053098153322935104, -0.6745766401290894, 0.2947763502597809, 0.37627771496772766, 0.14346806704998016, -0.2960794270038605, 0.14204949140548706, 0.14357483386993408, -0.5692777037620544, 0.43735790252685547, 0.2618318200111389, 0.22064021229743958, -0.9274200201034546, 0.41819214820861816, 0.4258829951286316, -0.5888707637786865, 0.30499371886253357, 0.8662075400352478, -0.08580402284860611, -0.7845694422721863, -0.5190602540969849, -1.099016547203064, -0.3511349558830261, -1.0123646259307861, 0.5906047821044922, 0.005921679083257914, 0.006993466056883335, -0.2175348699092865, -0.9409964680671692, 0.570171058177948, -0.17237795889377594, 0.09118440002202988, 0.7661009430885315, 0.3573254644870758, -0.881626546382904, 0.3551410436630249, 0.709306001663208, -0.08788205683231354, -0.4906512200832367, -0.448896586894989, -0.35922184586524963, -0.6845142245292664, -0.005049585364758968, -0.1753081977367401, -0.5161149501800537, 0.8121329545974731, -0.21669410169124603, 0.6666355729103088, -0.27566492557525635, -0.12673279643058777, -0.3094749450683594, 0.25301772356033325, 0.2558724284172058, -0.5464282631874084, -0.4404235780239105, 1.2870012521743774, 0.7731723785400391, -0.8999531865119934, 0.41365358233451843, -0.6411536931991577, -0.7802282571792603, 0.860460102558136, 0.8696773648262024, -0.29860177636146545, 0.7827979326248169, 0.2221502810716629, -0.2186291217803955, 0.18523667752742767, -0.9660527110099792, -0.43823564052581787, 0.22901441156864166, 1.376919150352478, 0.9084128141403198, -0.11928141862154007, 0.32904019951820374, 0.9727399945259094, 0.5013843178749084, 0.012414934113621712, 1.1191409826278687, 0.9717149138450623, -0.12610946595668793, -0.4174540042877197, -0.27734529972076416, 0.7608519792556763, -1.0291738510131836, -0.7677788734436035, 0.1030731350183487, 0.7265211939811707, 0.4789234399795532, 0.7186964154243469, 0.25110217928886414, -0.08162356913089752, 0.318395733833313, 0.603672444820404, 0.27080830931663513, -1.3124338388442993, -0.5710557103157043, -0.7106197476387024, -0.3776455223560333, -0.0692104697227478, -0.030954886227846146, -0.20761528611183167, -0.6162097454071045, -0.1967209428548813, 0.1216859370470047, 0.23779121041297913, 0.47519442439079285, 1.076689600944519, 0.5824616551399231, 0.8627666234970093, 0.06009381636977196, -0.28636378049850464, -0.015308275818824768, -0.6379038095474243, 0.07196348160505295, -0.4372022747993469, -0.03462330996990204, -0.507449746131897, -0.5023332834243774, -0.06765590608119965]}, "authors": [{"authorId": "66513455", "name": "Zian Su"}, {"authorId": "2028614325", "name": "Xiangzhe Xu"}, {"authorId": "2284728912", "name": "Ziyang Huang"}, {"authorId": "2220690905", "name": "Zhuo Zhang"}, {"authorId": "2107568083", "name": "Yapeng Ye"}, {"authorId": "2135634441", "name": "Jianjun Huang"}, {"authorId": "2268478340", "name": "Xiangyu Zhang"}], "references": [{"paperId": "daa7b3f8d5610cd2dd1badde8c3a3e6f58726aa3", "title": "A study on the impact of pre-trained model on Just-In-Time defect prediction"}, {"paperId": "60e7ce45b2b3a2a92430afa94595f19fca015994", "title": "PEM: Representing Binary Program Semantics for Similarity Analysis via a Probabilistic Execution Model"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "39f2636fee2545ac59d356e52c643fabdc65e4f2", "title": "kTrans: Knowledge-Aware Transformer for Binary Code Embedding"}, {"paperId": "2fd631d788cdca483e162878a87104dacac05975", "title": "Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis"}, {"paperId": "2601701c0015e47ea8a78a0665bf5080f268334c", "title": "iSyn: Semi-automated Smart Contract Synthesis from Legal Financial Agreements"}, {"paperId": "ea168dda6c8f7450b3d0f7feeed5c0619ce8813e", "title": "TRACED: Execution-Aware Pre-Training for Source Code"}, {"paperId": "f777a82c576e52928044225ce6a2733ccd2b8123", "title": "Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors"}, {"paperId": "08f53d20666077f3398cf23c598e5e42fb01aa94", "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities"}, {"paperId": "0b2d093ed117cacce230bf0f617ff4bb0fe7575b", "title": "CCT5: A Code-Change-Oriented Pre-trained Model"}, {"paperId": "ae495ee34fa97614c79949d528de9dec182f5365", "title": "RepresentThemAll: A Universal Learning Representation of Bug Reports"}, {"paperId": "c6808575096a6e4f3cbdc5f893384bc5a01cc6f8", "title": "A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair"}, {"paperId": "4c4d97cd03267c77235e6a1487339ec26a6382f9", "title": "Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "aae61ba5b629eba965b7f49a685b3d9f1bfb358c", "title": "Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models"}, {"paperId": "481671264a5a633426dc5dc679aac2687ffcdb59", "title": "SEAL: Integrating Program Analysis and Repository Mining"}, {"paperId": "faba3c4d4c2ba49817349804b16de46aa1394982", "title": "An empirical study of text-based machine learning models for vulnerability detection"}, {"paperId": "beb637ae4caf3d5168ddc9ebbd35d5fbc1d829fd", "title": "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection"}, {"paperId": "86cb833916760dc7800aec72e306864e6c6f5ea3", "title": "SymLM: Predicting Function Names in Stripped Binaries via Context-Sensitive Execution-Aware Code Embeddings"}, {"paperId": "b29d5d04e96b8bda78666ca6b31df1398788b69a", "title": "How to better utilize code graphs in semantic code search?"}, {"paperId": "8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab", "title": "Soft-Labeled Contrastive Pre-training for Function-level Code Representation"}, {"paperId": "f78b0a2351160e191ff3f0ab0dd96b8cb96f15d3", "title": "Two Sides of the Same Coin: Exploiting the Impact of Identifiers in Neural Code Comprehension"}, {"paperId": "8f094ae23ae2b35c72a67c6627cd624c3baec4bb", "title": "Improving cross-platform binary analysis using representation learning via graph alignment"}, {"paperId": "45263786d07f5751f7494fdeee3c8764836d02c4", "title": "NatGen: generative pre-training by \u201cnaturalizing\u201d source code"}, {"paperId": "79c3300cc33ab7a48a72e478fdf8af7c5f937006", "title": "Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph Convolutional Networks"}, {"paperId": "221d4a9395b626c9cf1fc19261663c18e1d5c63a", "title": "GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis"}, {"paperId": "372f60a267239c285f5f0bdf3d6605843787108c", "title": "jTrans: jump-aware transformer for binary code similarity detection"}, {"paperId": "b37cf366f8cdac6eb2d05a59e5963dab2e059f5b", "title": "FIRA: Fine-Grained Graph-Based Code Change Representation for Automated Commit Message Generation"}, {"paperId": "9a6730534295335247eebdec59b7decdeb83d59a", "title": "On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages"}, {"paperId": "a9e50a97cc7fa77ffd69a74c38cc23663506b1f6", "title": "Can pre-trained code embeddings improve model performance? Revisiting the use of code embeddings in software engineering tasks"}, {"paperId": "4b27f18bff43d605805c92696a979714ced0b805", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation"}, {"paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "title": "A systematic evaluation of large language models of code"}, {"paperId": "2b8a207189bc02d73d1dce850bcde24dbd984483", "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention"}, {"paperId": "73569460b023f9ac1fe5a1876c3401460d2fc15d", "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding"}, {"paperId": "9f0852ce9338c00135fe39426d893a36a289e5d5", "title": "GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "5c8d0c68e498ce35c04ed17acca5b734c8c26546", "title": "A comprehensive study on learning-based PE malware family classification methods"}, {"paperId": "5ef96284d4c41fccf0ad176accbf4b3d507f2bc2", "title": "StateFormer: fine-grained type recovery from binaries using generative state modeling"}, {"paperId": "06e36261b21af2943e464a562c92c09dac292a82", "title": "Augmenting Decompiler Output with Learned Variable Names and Types"}, {"paperId": "1c0752fb3e9ab5c9392f196225075422f26b5110", "title": "How could Neural Networks understand Programs?"}, {"paperId": "0646bb09db4d1ba24150e69b71edcd4aff691b3c", "title": "Unified Pre-training for Program Understanding and Generation"}, {"paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"}, {"paperId": "b478f8c17ca392270e78d9766d1505b646aed25d", "title": "Identifying Authorship Style in Malicious Binaries: Techniques, Challenges & Datasets"}, {"paperId": "0bc94523f44146e158f7fc5e5f1a2575b3265e0b", "title": "Automated Patch Transplantation"}, {"paperId": "41fb78963f03d7c73d34297950e7ad4e43d858f8", "title": "Trex: Learning Execution Semantics from Micro-Traces for Binary Similarity"}, {"paperId": "acd8e24eda17808e3528d2dfd52de1619d13241a", "title": "InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees"}, {"paperId": "b0a477ebd523633b9b018c44e34b5d1de1324630", "title": "GRAPHSPY: Fused Program Semantic-Level Embedding via Graph Neural Networks for Dead Store Detection"}, {"paperId": "5f818ecbfce3bc44325a4f8ef2d744bc94006d6c", "title": "Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks"}, {"paperId": "11a0bdb1a049eaeae72c38278936c1d599728f87", "title": "TreeCaps: Tree-Based Capsule Networks for Source Code Processing"}, {"paperId": "21e33bd0ad95ee1f79d8b778e693fd316cbb72d4", "title": "Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs"}, {"paperId": "3bfa808ce20b2736708c3fc0b9443635e3f133a7", "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "e38a118f18f1b6e1a4e3d3f6fe4838fcdc0af022", "title": "Learning and Evaluating Contextual Embedding of Source Code"}, {"paperId": "51a920c3d201eec57bcc2e97e0268304f53b5161", "title": "TreeGen: A Tree-Based Transformer Architecture for Code Generation"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ac5d1f25679d38ab29036c8d157b192650e2e0b6", "title": "BDA: practical dependence analysis for binary executables by unbiased whole-program path sampling and per-path abstract interpretation"}, {"paperId": "94194703e83b5447f519fd8bcbb903916e05aaf9", "title": "Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "b85c74900ff716119f51852146de7a3d7d43230e", "title": "Graph Matching Networks for Learning the Similarity of Graph Structured Objects"}, {"paperId": "6398cb8f2af1c988a097ed1e1cefb380195edfb8", "title": "(Preprint)"}, {"paperId": "36652428740cd30d245d55889f01a7fb04a91c93", "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning"}, {"paperId": "d86129473ebc932af4cdcf143a196e60e24fad9b", "title": "Semantics-Based Obfuscation-Resilient Binary Code Similarity Comparison with Applications to Software and Algorithm Plagiarism Detection"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "5f1d429ba574581ac14effe3ebab654a57dc0e39", "title": "Learning to Represent Programs with Graphs"}, {"paperId": "af9faf172ae7327b86ce7d49ce9e3b5e5cc51ce3", "title": "In-memory fuzzing for binary code similarity analysis"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "db0cabda6a78c993b40be74e11a3ac890e406ac8", "title": "Anatomization and Protection of Mobile Apps' Location Privacy Threats"}, {"paperId": "87ff7ef37bd35d97ea0316398e762cd642df64a6", "title": "Malware images: visualization and automatic classification"}, {"paperId": "a21921eb0c5600562c8dad8e2bc40fff1ec8906b", "title": "Automatic Reverse Engineering of Data Structures from Binary Execution"}, {"paperId": "38641821f40fe883c5ce51a5133d538099093f9a", "title": "The program dependence graph and its use in optimization"}, {"paperId": "81b949a01506a09fcd7ec4faf28e2fa0ec63f1e0", "title": "A program data flow analysis procedure"}, {"paperId": "f8bc101842cdd16e687beb69f5799da9a99c409b", "title": "SigmaDiff: Semantics-Aware Deep Graph Matching for Pseudocode Diffing"}, {"paperId": "c58c20e1f2de8ad32c4f2ef7816f24a69a1c1d7e", "title": "How Machine Learning Is Solving the Binary Function Similarity Problem"}, {"paperId": "2ee09367fb6d2a81f69eab968d12e66df23319f3", "title": "DeepDi: Learning a Relational Graph Convolutional Network Model on Instructions for Fast and Accurate Disassembly"}, {"paperId": "9592686682420e9ccc8a16f53c3b4ef5a23c54ed", "title": "MFF-AMD: Multivariate Feature Fusion for Android Malware Detection"}, {"paperId": "1cedb352a10828b3ec263a4da907794bea052282", "title": "PLUR: A Unifying, Graph-Based View of Program Learning, Understanding, and Repair"}, {"paperId": "20c59907cf81469b0ab9cc12cc33a4ba0917324e", "title": "DeepBinDiff: Learning Program-Wide Code Representations for Binary Diffing"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "42afb665ccceb331c817c85ab6f5ea4ac71a8057", "title": "Mining Multi-label Data"}, {"paperId": "aa567981b204bbef0209f337c8394590e2fa441e", "title": "ON TYPES AND PROGRAMMING LANGUAGES"}, {"paperId": null, "title": "2023. Automated repair of programs from large language models"}, {"paperId": null, "title": "IDA Pro 2023. A powerful disassembler and a versatile debugger"}, {"paperId": null, "title": "Better Code Models by Attention Regularization When Symbols Are Lacking"}, {"paperId": null, "title": "2022. Introduction to algorithms"}]}