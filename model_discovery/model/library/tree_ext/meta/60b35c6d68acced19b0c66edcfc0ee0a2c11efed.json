{"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers", "abstract": "While Transformers have shown remarkable success in natural language processing, their attention mechanism's large memory requirements have limited their ability to handle longer contexts. Prior approaches, such as recurrent memory or retrieval-based augmentation, have either compromised the random-access flexibility of attention (i.e., the capability to select any token in the entire context) or relied on separate mechanisms for relevant context retrieval, which may not be compatible with the model's attention. In this paper, we present a novel approach that allows access to the complete context while retaining random-access flexibility, closely resembling running attention on the entire context. Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism. Our approach seamlessly integrates with specialized data structures and the system's memory hierarchy, enabling processing of arbitrarily long context lengths. We demonstrate that our method can obtain comparable performance with Transformer-XL while significantly reducing the number of retrieved tokens in each step. Finally, we show that fine-tuning LLaMA 7B with our method successfully extends its context length capacity to over 32k tokens, allowing for inference at the context lengths of GPT-4. We release the implementation of landmark attention and the code to reproduce our experiments at https://github.com/epfml/landmark-attention/.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 88, "influentialCitationCount": 13, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.16300", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism."}, "embedding": {"model": "specter_v2", "vector": [0.3197685182094574, 0.645387589931488, -0.35879847407341003, 0.1441558301448822, -0.7441444396972656, -0.3492758870124817, 0.8238323330879211, 0.09513358771800995, -0.36842238903045654, -0.2582840621471405, 0.6407097578048706, -0.2944527864456177, 0.7325186729431152, -0.0018506716005504131, -0.11914293467998505, -0.16305699944496155, -1.056950330734253, 0.04878039285540581, -0.022562121972441673, -0.05312233045697212, 0.22655439376831055, -0.5176876187324524, -1.1435673236846924, 0.2608828544616699, 0.08529194444417953, 0.42534616589546204, 0.20809227228164673, 0.7991420030593872, -0.6650173664093018, 0.6503061056137085, 0.4133033752441406, -0.10682808607816696, 0.09792143106460571, 0.05627666413784027, -0.6064170598983765, -0.5495124459266663, 0.41661179065704346, -0.4149622917175293, -0.4742739200592041, 0.6099242568016052, -0.1654096245765686, 0.4091128706932068, 0.0722983330488205, -0.39294978976249695, -0.4210723638534546, 1.4328254461288452, 0.5026434659957886, 0.9261019229888916, -0.3672165274620056, -0.7057445049285889, 1.678092360496521, -1.4231908321380615, 0.27266108989715576, 1.3902586698532104, 0.21296854317188263, 0.3758001923561096, -0.41494131088256836, -0.5613988041877747, 1.1403682231903076, 0.5520541667938232, -0.9321542978286743, -0.5274180173873901, 0.04882531613111496, 0.2875271439552307, 2.124922275543213, -0.32526862621307373, 0.40656036138534546, 0.4251439571380615, -0.1023765429854393, 1.6418039798736572, -0.15048742294311523, -1.2771483659744263, -0.3674526512622833, -0.30862703919410706, 0.3376575708389282, 0.603864312171936, -0.47361838817596436, 0.2169286161661148, -0.6819764971733093, -0.10862553119659424, 0.525055468082428, -0.22389855980873108, 0.2878724932670593, -0.027992652729153633, -0.397746741771698, 0.438387393951416, 0.2229645550251007, 1.1059694290161133, -0.2315814346075058, 0.8411128520965576, 0.5654573440551758, 0.06963004916906357, -0.16265864670276642, 0.3903654217720032, -0.14569103717803955, 0.12431595474481583, -0.9339358806610107, 0.1686292141675949, -0.48991459608078003, 0.7588883638381958, -0.014589126221835613, 0.5109124779701233, -1.1215088367462158, 0.1413353830575943, 1.1722228527069092, 0.30222949385643005, 0.4952394366264343, -0.5798306465148926, 0.21640156209468842, -0.6992768049240112, 0.007858606986701488, -0.4084058701992035, 0.1035289391875267, -0.20165757834911346, -0.4991530179977417, -1.4570143222808838, -0.3043286204338074, 0.34953051805496216, -0.802665650844574, 1.0004736185073853, -0.10304739326238632, 0.3238319456577301, -0.0961347222328186, 0.06380502879619598, 0.5241148471832275, 0.646849513053894, 0.36301684379577637, 0.25593793392181396, 1.0901556015014648, -0.6120120286941528, -0.5403356552124023, -1.2176973819732666, 0.7267066836357117, -0.1979815512895584, 0.5090520977973938, -0.24204608798027039, -1.0409413576126099, -0.726335883140564, -0.9290775656700134, -0.12887366116046906, -0.7930138111114502, 0.11121120303869247, 0.7869066596031189, 0.39862650632858276, -0.933020293712616, 0.5062575340270996, -0.28503718972206116, 0.05524627864360809, 0.3085499405860901, -0.019742999225854874, 0.3072095513343811, -0.4547278583049774, -1.5391730070114136, 0.311123788356781, 0.21265782415866852, -0.1299200803041458, -0.3676358759403229, -0.5264959931373596, -1.3318874835968018, 0.2431865632534027, 0.48089876770973206, -0.09932564198970795, 1.3914618492126465, 0.23367808759212494, -1.0576578378677368, 0.7404245138168335, -0.7597865462303162, 0.10017543286085129, -0.2095600813627243, -0.35220441222190857, -0.22691938281059265, -0.48812440037727356, 0.03161075711250305, 0.3425011932849884, 0.5363070964813232, -0.15413016080856323, -0.3225603997707367, 0.07653197646141052, -0.33153021335601807, -0.17388980090618134, -0.025211205706000328, 0.8203537464141846, -0.26669612526893616, -0.24829111993312836, 0.5099770426750183, 0.7582885026931763, -0.023357180878520012, -0.8440546989440918, -0.5163238048553467, -1.1020915508270264, 0.5663765072822571, 0.16533780097961426, 1.3916329145431519, -0.8333603143692017, -1.032143473625183, -0.09555196017026901, -0.23357143998146057, -0.08607633411884308, -0.8502042889595032, 0.47557327151298523, -0.6464405655860901, 0.7302083373069763, -0.015767879784107208, -0.9857837557792664, 0.24178244173526764, -0.25843554735183716, -0.9453474879264832, -0.24950440227985382, 0.16376599669456482, 1.1888121366500854, -1.1031023263931274, -0.21070827543735504, 0.24561862647533417, 0.4094773232936859, -0.821344256401062, 1.0233240127563477, -0.33423924446105957, 0.23036165535449982, -0.199304461479187, -0.029154319316148758, -0.13469737768173218, -0.18529227375984192, 0.4566130042076111, -0.05796355381608009, -0.20917144417762756, 0.9808340668678284, -0.44761577248573303, 1.129843831062317, -0.6648479104042053, 0.6133277416229248, -0.18267332017421722, -0.629988968372345, 0.0775265023112297, 0.3385903835296631, -0.22680628299713135, -0.3477802574634552, 0.16101877391338348, 0.09126107394695282, -0.7442682981491089, 0.37121954560279846, 1.1786731481552124, 0.9467102289199829, -0.48179909586906433, 0.34154993295669556, 0.25445348024368286, -0.25594186782836914, 0.19873201847076416, 0.3835606276988983, 0.8093744516372681, 0.5977192521095276, 0.5338806509971619, 0.0767349824309349, 0.21056050062179565, -1.0153695344924927, -0.021626761183142662, 0.7611311674118042, 0.8140407204627991, 0.711348831653595, 0.6554229259490967, -0.6952996253967285, -0.3973327577114105, 0.23820839822292328, 0.3944585621356964, 1.749285101890564, -0.43674436211586, -0.34485524892807007, -0.5130108594894409, -0.2021075040102005, -0.6430013179779053, 0.25135287642478943, -0.6080365777015686, -0.0013705580495297909, -0.854743242263794, -0.7119714021682739, 0.871343731880188, 0.23431840538978577, 0.8027393221855164, -1.1393780708312988, -0.29775288701057434, -0.04761488363146782, 0.11044374108314514, -0.7427936792373657, -0.5445685386657715, 0.6534782648086548, -0.6286428570747375, -0.06756884604692459, 0.46456605195999146, -0.20641249418258667, 0.21271993219852448, -0.880410373210907, 1.1834133863449097, -0.282279372215271, -0.2331816554069519, 0.21445511281490326, 0.47201064229011536, -0.39365077018737793, -0.37275540828704834, 0.4880903959274292, -0.09352461993694305, -0.29101982712745667, 0.5364682078361511, 0.5187548995018005, -0.0028150712605565786, -0.07831580191850662, -0.4096476137638092, -0.23816540837287903, 0.004636652301996946, 0.529481828212738, 1.0159707069396973, -0.3416890501976013, -0.0076264552772045135, -1.3563423156738281, 0.3607898950576782, 0.20258468389511108, -0.5802350640296936, 0.2979542315006256, -0.7299336791038513, -0.17253977060317993, 0.5931396484375, -0.5070318579673767, -0.18373550474643707, -0.7489140033721924, 0.2532317638397217, -0.34905001521110535, -0.16411496698856354, 0.20115281641483307, -0.04373694956302643, 0.6523561477661133, 0.010372152552008629, 0.4930879473686218, -0.19372707605361938, 0.010689686983823776, 1.0101484060287476, -0.8722093105316162, 0.5686466693878174, -0.08863455802202225, -0.03489254042506218, -0.3172093629837036, -0.1342409998178482, -0.7867656946182251, -0.349162220954895, -0.3391686677932739, -0.22617845237255096, -0.05237526074051857, -0.1754993498325348, -0.4253304600715637, -0.9299154281616211, 0.043846338987350464, -0.9196702837944031, -0.6002322435379028, -0.18926575779914856, -0.4621221423149109, -0.025630513206124306, -1.0181604623794556, -1.0766030550003052, -0.7235904932022095, -0.7462104558944702, -0.9009709358215332, 0.38165268301963806, -0.12752708792686462, -0.31889668107032776, -0.6016741991043091, -0.00974198430776596, -0.6548346877098083, 0.9426273107528687, -0.6655080318450928, 0.8167462348937988, -0.07280129194259644, -0.6976737976074219, -0.2944706082344055, 0.38800275325775146, 0.21803703904151917, -0.1828467696905136, 0.2248205542564392, -1.182120442390442, 0.2768644690513611, -0.2942134141921997, -0.16774749755859375, 0.21778644621372223, 0.5362284183502197, 0.873081624507904, -0.31201863288879395, -0.5711711049079895, 0.13414905965328217, 1.2540483474731445, -0.4311858117580414, 0.47447526454925537, 0.3528246283531189, 0.7885796427726746, 0.0032900704536587, -0.06935644149780273, 0.621208906173706, 0.22650621831417084, 0.08821217715740204, 0.40899115800857544, 0.25179988145828247, 0.14974133670330048, -0.5986947417259216, 0.39985665678977966, 1.0469708442687988, 0.5377888083457947, 0.11223079264163971, -1.0755678415298462, 1.1025201082229614, -0.9623101353645325, -1.042737603187561, 0.9510102272033691, 0.9828664064407349, 0.4538475573062897, -0.33105528354644775, -0.6279101967811584, -0.07819020748138428, 0.5084980726242065, 0.4914252460002899, -0.28130415081977844, -0.7677993774414062, 0.22075971961021423, 0.6293066143989563, 0.10225475579500198, 0.8084830045700073, -0.46088743209838867, 0.8553690314292908, 14.795332908630371, 0.7102892398834229, 0.08110031485557556, 0.2538573741912842, 0.7177459001541138, 0.1637623906135559, -0.5826199054718018, 0.13290603458881378, -1.4981961250305176, -0.3174026608467102, 1.039842963218689, 0.21155528724193573, 0.6052790880203247, 0.22621314227581024, -0.15080219507217407, 0.07196210324764252, -0.704230010509491, 0.41595539450645447, 0.46009400486946106, -1.0211044549942017, 0.2743203043937683, 0.05571300536394119, 0.05958526208996773, 0.5439526438713074, 0.9335410594940186, 0.8699230551719666, 0.7083859443664551, -0.5045576691627502, 0.516821026802063, 0.3443339765071869, 1.063523530960083, -0.2261134684085846, 0.2938288450241089, 0.03396303206682205, -1.2554161548614502, -0.3976750373840332, -0.505139946937561, -0.9037812948226929, 0.009927934966981411, -0.032411448657512665, -0.7703421115875244, -0.8226703405380249, -0.12615737318992615, 0.6243197321891785, -0.15628202259540558, 0.2229630947113037, -0.461637407541275, 0.6690701246261597, -0.17106856405735016, -0.029325637966394424, 0.3727889358997345, 0.6149287819862366, 0.2732656002044678, 0.05467330291867256, 0.030710743740200996, 0.008244304917752743, -0.029093092307448387, 0.4237205982208252, -0.30530020594596863, -0.042416512966156006, -0.47485387325286865, -0.11450700461864471, 0.47136011719703674, 0.8533040881156921, 0.24120420217514038, 0.09766765683889389, -0.1912451684474945, -0.12126145511865616, 0.5742209553718567, 0.07415777444839478, -0.1370536834001541, -0.44081857800483704, 0.211777925491333, -0.5468178987503052, 0.3581268787384033, 0.7465582489967346, -0.11947529762983322, -0.4084857106208801, -0.8225058317184448, -0.20267552137374878, 0.34804990887641907, -0.62568598985672, -0.4104372262954712, 0.980685830116272, -0.11945194751024246, -0.4403872787952423, -0.10363727807998657, -0.6227529644966125, -0.24033614993095398, 0.5778130888938904, -1.3133654594421387, -0.8763840794563293, 0.5797972679138184, -0.3233426809310913, -0.5245756506919861, 0.1633438766002655, 1.4971461296081543, -0.18016581237316132, -0.33286017179489136, 0.11494193971157074, 0.09846701472997665, 0.3628157079219818, -0.04854869097471237, -1.1159378290176392, 1.0648683309555054, 0.5168525576591492, 0.09295962005853653, 0.7520203590393066, -0.01262629870325327, 0.24630065262317657, -0.41693970561027527, -0.07523568719625473, 0.8935955762863159, -1.3407492637634277, -0.29056477546691895, -0.8780702352523804, -0.8175562024116516, 0.7536975145339966, 0.6243639588356018, -0.020970549434423447, 0.3773363530635834, 0.5346208810806274, -0.6607496738433838, -0.4388355016708374, -0.5773377418518066, 0.2617742121219635, 0.39681899547576904, -0.9745872020721436, -0.5499534606933594, -0.6022433042526245, 0.2496178299188614, -1.123659372329712, -0.4404124617576599, -0.5107232332229614, 0.3929443061351776, 0.13839298486709595, 1.1499483585357666, -0.24988743662834167, 0.5693802833557129, 0.8303696513175964, -0.2121911197900772, -0.7388457655906677, -0.1132064238190651, -0.7988094687461853, -0.16520565748214722, 0.5245105028152466, 0.7796305418014526, -0.5145694613456726, 0.0893269032239914, 0.8954037427902222, 0.3686828315258026, -0.507429838180542, -0.9786580204963684, -0.01341262087225914, 0.42478424310684204, -0.8418267965316772, 0.9687640070915222, 0.01656016893684864, 0.13944081962108612, 0.1898009330034256, 0.6145714521408081, 0.5496275424957275, -0.3826926350593567, -0.44478052854537964, 0.3188621401786804, 0.16688543558120728, 0.22150331735610962, -0.564357340335846, -0.48456883430480957, -1.215625524520874, -0.020851127803325653, -1.1027454137802124, 0.32842621207237244, -0.86207115650177, -0.6787794232368469, -0.3075540065765381, -0.5498493313789368, 0.6972724199295044, 0.12956294417381287, -0.4548780620098114, -0.6394878625869751, -0.9260889887809753, -0.49577006697654724, 0.651930570602417, 0.6637366414070129, -0.7401580810546875, -0.09310085326433182, 0.032855477184057236, -0.04889063164591789, -0.090506911277771, 0.3617347180843353, -0.49193114042282104, -0.6153520941734314, -1.3402981758117676, 0.38475489616394043, -0.2721449136734009, -0.2718530595302582, -0.40761256217956543, 0.5216788649559021, 0.5179001092910767, 0.0246038269251585, -0.26212364435195923, 0.17890065908432007, -0.8169506788253784, -0.7644718885421753, -0.049161724746227264, -0.8790231347084045, 0.20904263854026794, 0.5435143113136292, -0.6729157567024231, -0.0682346448302269, 0.5438582301139832, -0.25164899230003357, -1.3540844917297363, -0.6489828824996948, 0.3285732865333557, -1.0155571699142456, 0.25536689162254333, -0.652773380279541, -0.005767594091594219, -1.2158061265945435, -0.16866226494312286, -0.0739029198884964, 0.4378964602947235, -0.32202669978141785, 0.9630163311958313, 0.4293639659881592, -0.9906637072563171, 0.28187668323516846, 0.4737054407596588, 0.013164173811674118, 0.39068979024887085, 0.46914854645729065, 0.18190017342567444, 0.01350833848118782, 0.6143624186515808, 0.5052327513694763, 0.024381564930081367, -0.9859775900840759, 0.20313304662704468, 0.5718388557434082, -0.9209561347961426, -0.2680927515029907, 0.9814774394035339, -0.3956860303878784, -1.0591462850570679, 0.3239274322986603, -1.7214782238006592, -0.7298271656036377, 0.007363556884229183, 0.8951467275619507, -0.09689020365476608, -0.0647423043847084, -0.10201549530029297, -0.5204764604568481, 0.18953648209571838, -0.2464262694120407, -0.5459484457969666, 0.5195299386978149, -0.32712361216545105, -0.5376126766204834, 0.756817102432251, 0.4127838909626007, -0.3903820812702179, -0.33810362219810486, -0.853201150894165, -0.2932277023792267, -0.13527782261371613, 0.5349436402320862, -0.36914145946502686, -0.3828761577606201, 0.8008407354354858, 0.39204126596450806, 0.5911674499511719, 0.08159139752388, -0.1785086989402771, -0.0494268424808979, 0.9619487524032593, 0.11756563186645508, -0.40268826484680176, -0.7179011702537537, 1.4912922382354736, 1.1240772008895874, -0.6270308494567871, 0.3161991536617279, 0.37084224820137024, -0.36744895577430725, 0.6663558483123779, 0.4347834289073944, 0.1771579086780548, 0.918424665927887, -0.31129416823387146, 0.2591450810432434, 0.1957370638847351, -1.1141937971115112, -0.33033058047294617, 0.39543217420578003, 1.0197306871414185, 0.7356945276260376, 0.09232272952795029, 0.6895114183425903, 0.7760171294212341, 0.09164971113204956, -0.1136149987578392, 0.5197672247886658, 0.8177631497383118, -0.17045320570468903, -0.21894176304340363, -0.20462855696678162, 0.686983048915863, -1.0636004209518433, -1.0948785543441772, 0.38337334990501404, 0.5315383076667786, 0.06539034098386765, 0.513620913028717, 0.8794993758201599, 0.3446277678012848, 0.41335391998291016, 0.5637289881706238, 0.5949522256851196, -0.6894196271896362, -0.3197688162326813, -0.22617541253566742, -0.8259032964706421, -0.2745840549468994, -0.1112065464258194, -0.5929097533226013, -0.30992743372917175, -0.06766294687986374, 0.08342187851667404, 0.20437392592430115, 0.09131056070327759, 0.994771420955658, 0.30808141827583313, 0.37820494174957275, -0.09910363703966141, -0.5483473539352417, -0.13599197566509247, -1.1589231491088867, -0.030962709337472916, -0.9233664274215698, 0.20076803863048553, -0.06808298826217651, -0.09537217766046524, -0.39237430691719055]}, "authors": [{"authorId": "1580494295", "name": "Amirkeivan Mohtashami"}, {"authorId": "2456863", "name": "Martin Jaggi"}], "references": [{"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "87126a964ed14d0d2207747fc732b197e2fc9493", "title": "Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "398e4061dde8f5c80606869cebfa2031de7b5b74", "title": "Few-shot Learning with Retrieval Augmented Language Models"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "590432f953b6ce1b4b36bf66a2ac65eeee567515", "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction"}, {"paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "title": "Do Long-Range Language Models Actually Use Long-Range Context?"}, {"paperId": "83238f3402fc4a7c4efa48bae180d81cabed23ee", "title": "\\infty-former: Infinite Memory Transformer"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "67ee20536c30a225b86902af2f091e28e5e19b40", "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "030d7d7ae48a9f81700b2c1f7cf835235777b8e7", "title": "Relevance-guided Supervision for OpenQA with ColBERT"}, {"paperId": "168fc3525f7b97695a97b04e257ee9bd1e832acb", "title": "Memory Transformer"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "2cbb8de53759e75411bc528518947a3094fbce3a", "title": "Billion-Scale Similarity Search with GPUs"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Blog post 'Introducing 100K Context Windows"}, {"paperId": null, "title": "claude-v1.3-100k"}]}