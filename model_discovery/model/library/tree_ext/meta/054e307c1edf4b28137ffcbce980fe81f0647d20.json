{"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs", "abstract": "Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism\u2019s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "citationCount": 47, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://aclanthology.org/2021.emnlp-main.830.pdf", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a swap-then-finetune procedure, which in an off-the-shelf pretrained transformer, replaces the softmax attention with its linear-complexity recurrent alternative and then finetune, and provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants."}, "embedding": {"model": "specter_v2", "vector": [0.39235720038414, 1.0662376880645752, 0.15560764074325562, -2.975033930852078e-05, -0.1318657398223877, -0.35983943939208984, 0.8093588352203369, -0.34940463304519653, -0.054605260491371155, -0.32483425736427307, 0.550324559211731, -0.26661524176597595, 0.4350210130214691, -0.28303539752960205, -0.23599255084991455, 0.22872066497802734, -0.4622817933559418, 0.22397278249263763, -0.016332514584064484, -0.44891026616096497, -0.2624265253543854, -0.6666898131370544, -1.3449801206588745, -0.1645141988992691, 0.2856854498386383, 0.8441855907440186, 0.29434892535209656, 0.8205418586730957, -0.3326326906681061, 0.5594587922096252, 0.46881407499313354, -0.5566992163658142, 0.20871689915657043, -0.3731662333011627, -0.4120747447013855, -0.16916942596435547, 0.2787460386753082, -0.10700184106826782, -0.24695183336734772, 0.7431889176368713, -0.34368401765823364, 0.29824069142341614, 0.17201866209506989, -0.5084593296051025, -0.49356377124786377, 1.4848848581314087, 0.9995625615119934, 0.7787032723426819, -0.3990478515625, -0.40235650539398193, 1.3060877323150635, -1.1688824892044067, 0.04253220930695534, 1.4807881116867065, 0.5070734620094299, 0.4037485718727112, -0.42794767022132874, -0.8032405376434326, 0.7922818064689636, -0.052828699350357056, -0.7619994282722473, -0.5086223483085632, 0.18144918978214264, 0.03403700888156891, 1.9750254154205322, -0.3085618019104004, 0.23870927095413208, 0.5739220380783081, -0.03876166790723801, 1.1150152683258057, 0.03801748529076576, -0.4906748831272125, -0.35999539494514465, 0.21255247294902802, 0.006097861565649509, 1.0699243545532227, -0.5780074596405029, 0.48483577370643616, -0.9115510582923889, 0.001277406234294176, 0.7226133942604065, -0.36160653829574585, 0.3014351725578308, 0.007414236199110746, 0.045387446880340576, 0.8507711887359619, 0.38074609637260437, 0.7607927918434143, -0.566373348236084, 0.6333022117614746, 0.5689745545387268, 0.30925148725509644, -0.13868626952171326, 0.7022002935409546, -0.20492422580718994, 0.3121805489063263, -0.7040278911590576, 0.07735970616340637, -0.31456416845321655, 0.6310490369796753, 0.08595964312553406, 0.9414398074150085, -0.7960429787635803, 0.3816400468349457, 1.2916920185089111, 0.05657622590661049, 0.8167868852615356, -0.8761736750602722, 0.24257270991802216, -0.6990571022033691, -0.4250403046607971, -0.446139395236969, 0.14507918059825897, -0.19281655550003052, -0.948502242565155, -1.3337125778198242, -0.5852930545806885, -0.0663333386182785, -1.0343844890594482, 0.9996073842048645, -0.5367174744606018, 0.2391348034143448, -0.036934442818164825, 0.19675317406654358, 0.3232884109020233, 0.7681925892829895, 0.2112814038991928, -0.028263693675398827, 0.8206648826599121, -0.703586995601654, -0.6998705863952637, -0.9697664976119995, 0.2829428017139435, 0.017766403034329414, 0.22500599920749664, -0.1541532725095749, -1.5167262554168701, -1.004590392112732, -0.6311735510826111, 0.15448956191539764, -0.5941696763038635, 0.1525365710258484, 1.3355295658111572, 0.6321277022361755, -1.2203058004379272, 0.7295123934745789, -0.1706904023885727, 0.030477842316031456, 0.3817838430404663, 0.26092779636383057, 0.24631920456886292, 0.21368151903152466, -1.425418734550476, 0.18711434304714203, 0.20372478663921356, -0.1626540869474411, -0.24666540324687958, -0.7732030749320984, -1.2357382774353027, 0.11022626608610153, 0.10441520810127258, -0.7845177054405212, 1.5100188255310059, -0.027960339561104774, -1.9455894231796265, 0.4143260717391968, -0.35020262002944946, -0.19305574893951416, -0.09129580855369568, 0.0008098799735307693, -0.20643891394138336, -0.7112258672714233, -0.17266766726970673, 0.3623029887676239, 0.5007004141807556, 0.2708950936794281, -0.18853916227817535, 0.46966153383255005, -0.2827298045158386, -0.3529002070426941, -0.09807144850492477, 0.855996310710907, 0.019233422353863716, -0.3428388833999634, 0.19970105588436127, 0.695327639579773, -0.12772150337696075, -0.9321449995040894, -0.6829025149345398, -1.3132797479629517, 0.26704850792884827, 0.045289624482393265, 0.9710227847099304, -0.4773610532283783, -0.6876888275146484, 0.12000774592161179, -0.24252861738204956, -0.03506974130868912, -0.8592360615730286, 0.33566200733184814, -0.7083298563957214, 0.5110136866569519, 0.11733746528625488, -0.9638948440551758, 0.21801148355007172, -0.4478769898414612, -0.8865419626235962, -0.524815022945404, 0.4522017240524292, 1.1628130674362183, -0.7555853128433228, 0.13510861992835999, 0.16203390061855316, 0.4515707790851593, -0.9788060188293457, 1.551052451133728, -0.11517695337533951, 0.009699034504592419, 0.021732917055487633, -0.3566475808620453, -0.12702029943466187, -0.35705214738845825, 0.2710108757019043, -0.07259421050548553, -0.08760079741477966, 1.2366830110549927, -0.2754795253276825, 0.9927185773849487, -0.684881329536438, 0.23250634968280792, -0.37649664282798767, -0.7468869090080261, 0.570042073726654, 0.29690971970558167, -0.4121125340461731, -0.53773432970047, 0.13210830092430115, 0.4708888828754425, -0.6185718178749084, 0.7964217662811279, 0.8856358528137207, 0.5601485967636108, -0.35977643728256226, 0.12924742698669434, 0.8656517267227173, -0.4253106713294983, 0.4810206890106201, 0.17113672196865082, 0.9880648255348206, 0.34460923075675964, 0.345893919467926, 0.32027667760849, 0.34357792139053345, -1.1322993040084839, 0.20794649422168732, 0.6646642684936523, 1.1855450868606567, 1.0742714405059814, 0.33950403332710266, -0.5644627213478088, -0.04839281365275383, -0.3084884583950043, 0.7790339589118958, 1.5400097370147705, -0.8823709487915039, -0.08481966704130173, -0.7073704600334167, -0.25160661339759827, -0.6839827299118042, 0.16807399690151215, -0.3893742561340332, -0.3110334873199463, -0.6691036224365234, -1.0165681838989258, 0.9381087422370911, 0.22273775935173035, 0.8662964701652527, -0.9776209592819214, -0.2596251666545868, -0.052372295409440994, 0.2546460032463074, -0.7207462191581726, -0.444116473197937, 0.4655701220035553, -0.5991225838661194, -0.0465170182287693, 0.20529237389564514, 0.009480101987719536, -0.09259787201881409, -1.1029547452926636, 0.9795121550559998, -0.5984721779823303, -0.16840048134326935, -0.11712122708559036, 0.7336204648017883, -0.6266982555389404, -0.8098264932632446, 0.5653358101844788, 0.023318655788898468, -0.07480372488498688, -0.05338442698121071, 0.2349640280008316, 0.3390640318393707, -0.15366151928901672, -0.1978166401386261, -0.15047883987426758, -0.008570540696382523, 0.15637722611427307, 0.8109312653541565, -0.2253323644399643, 0.10619135946035385, -1.456233024597168, 0.8992406725883484, 0.34338468313217163, -0.8512767553329468, 0.3576084077358246, -0.7681557536125183, 0.10184194892644882, 0.7493288516998291, -0.25848522782325745, -0.5133330225944519, -0.6882113814353943, 0.17202994227409363, -0.3587053716182709, -0.06518078595399857, 0.2431940734386444, 0.21949942409992218, 0.5356901288032532, -0.09089631587266922, 0.4973617494106293, 0.15012557804584503, -0.11412525177001953, 0.47489672899246216, -1.1372236013412476, 0.5839948654174805, 0.5577125549316406, 0.5364184975624084, -0.7483586072921753, -0.24099573493003845, -0.37469327449798584, -0.366707444190979, -0.003799988189712167, 0.21081802248954773, -0.0814269557595253, 0.07228723913431168, -0.43597349524497986, -1.1426522731781006, 0.06327279657125473, -1.1007100343704224, -0.35241374373435974, -0.24422317743301392, -0.4349668622016907, -0.19925236701965332, -1.3713792562484741, -1.323301911354065, -0.7946200966835022, -0.8003384470939636, -0.862469494342804, 0.27140048146247864, 0.2993438243865967, -0.2793852686882019, -0.6064490079879761, 0.24091148376464844, -0.4427092373371124, 1.2650156021118164, -0.9183557629585266, 0.9178138375282288, -0.039135802537202835, -0.47598928213119507, -0.004616154357790947, 0.37844395637512207, 0.6819717288017273, -0.2456531673669815, 0.232272669672966, -1.1074613332748413, 0.5355269908905029, -0.47104278206825256, -0.3429398834705353, 0.17250396311283112, 0.3420228362083435, 0.7972115874290466, -0.3245283365249634, -0.14466556906700134, 0.5569373965263367, 1.092474102973938, -0.6198990345001221, 0.3793942630290985, 0.29905083775520325, 0.9171689748764038, 0.0765061303973198, -0.3139750361442566, 0.3676055669784546, 0.5333839654922485, 0.449884831905365, 0.0903627872467041, 0.025462372228503227, -0.05556992441415787, -0.9688633680343628, 0.6188227534294128, 1.7750065326690674, -0.02048974297940731, -0.10587980598211288, -0.7599493861198425, 0.5490309000015259, -1.211127758026123, -0.8575514554977417, 0.9115206003189087, 0.7571422457695007, 0.5575978755950928, -0.48608019948005676, -0.4703054130077362, 0.05351078137755394, 0.4529368579387665, 0.3632332980632782, -0.19600899517536163, -1.1813818216323853, 0.0511523075401783, 0.8136388659477234, 0.034937530755996704, 0.6575441360473633, -0.14528055489063263, 0.6613336801528931, 14.58279037475586, 0.5992573499679565, -0.03178925812244415, 0.41403672099113464, 0.6186310052871704, 0.2963361144065857, -0.5327504277229309, 0.0013782953610643744, -1.5363569259643555, -0.2175300270318985, 1.2325912714004517, 0.07328810542821884, 0.9493797421455383, 0.14847774803638458, -0.07877132296562195, 0.47260966897010803, -0.4939275085926056, 0.5847273468971252, 0.38995006680488586, -1.4535356760025024, 0.6099480986595154, 0.2447369247674942, 0.22761856019496918, 0.35441774129867554, 0.7166272401809692, 1.111354947090149, 0.7998653054237366, -0.5014606714248657, 0.48929107189178467, 0.5241239070892334, 0.5461199283599854, -0.12782135605812073, 0.4039503335952759, 0.08429645001888275, -1.0068950653076172, -0.2092808037996292, -0.3475354313850403, -1.062971591949463, 0.2960459887981415, 0.19213984906673431, -0.6299143433570862, -0.6824825406074524, 0.030237680301070213, 0.7689743041992188, -0.11756858974695206, 0.14433600008487701, -0.6671090126037598, 0.7379701137542725, -0.1947748064994812, 0.09804528206586838, 0.04680194333195686, 0.25230148434638977, 0.03513805568218231, -0.061811335384845734, 0.35479870438575745, -0.09390226006507874, -0.24282081425189972, 0.7300089001655579, -0.7197239398956299, -0.4286080598831177, -0.23843322694301605, -0.373879998922348, 0.17236681282520294, 0.8540509939193726, 0.6492873430252075, 0.3915577828884125, 0.058586571365594864, 0.12250587344169617, 0.8228476643562317, 0.14177216589450836, -0.1457289606332779, -0.31151217222213745, 0.27950844168663025, -0.20655055344104767, 0.4107772409915924, 0.9120891690254211, -0.144119530916214, -0.24405694007873535, -0.6638646721839905, -0.5130292773246765, 0.3796532154083252, -0.9825201034545898, -0.6016688942909241, 1.0194356441497803, -0.4706200659275055, 0.08251401782035828, -0.4802418649196625, -0.5300466418266296, -0.39763563871383667, 0.5743570923805237, -1.4195250272750854, -0.6223889589309692, 0.5126075148582458, -0.4604831337928772, -0.3895149230957031, -0.4996919631958008, 1.241196870803833, -0.21226564049720764, -0.6139585375785828, 0.22455361485481262, -0.5140915513038635, 0.1562594622373581, -0.7202482223510742, -0.815275251865387, 1.075055718421936, 0.5684037804603577, 0.00295048370026052, 0.3649056553840637, -0.07269065827131271, 0.35087576508522034, -0.49579715728759766, -0.09538313746452332, 1.0952935218811035, -0.8108984231948853, -0.1248747929930687, -0.7826408743858337, -0.6059041619300842, 0.6025422215461731, 0.7178921699523926, -0.3131290674209595, 0.14820457994937897, 0.43394067883491516, -0.632104218006134, -0.3630201816558838, -0.44358304142951965, -0.12544165551662445, 0.6038981676101685, -0.9772416353225708, -0.38080841302871704, -0.5669764876365662, 0.39543506503105164, -0.9139737486839294, -0.5456439256668091, -0.15736013650894165, 0.06430433690547943, -0.30601489543914795, 0.9560167789459229, -0.34664392471313477, 0.6569048762321472, 0.9148131012916565, 0.3569609820842743, -1.0746761560440063, -0.3425903022289276, -1.1238385438919067, 0.18486124277114868, 0.6361323595046997, 0.8614271283149719, -0.6952564716339111, 0.18953770399093628, 0.7645313143730164, 0.054436735808849335, -0.15069858729839325, -0.6499318480491638, -0.1581621766090393, -0.31354907155036926, -0.5727475881576538, 0.6816128492355347, -0.21872033178806305, -0.21794010698795319, 0.4655776619911194, 0.5951198935508728, 0.5158194303512573, -0.4929582476615906, -0.7935036420822144, 0.04349343106150627, -0.0861348882317543, -0.13979116082191467, -0.6336624026298523, -0.5155075788497925, -1.1094636917114258, 0.30259013175964355, -1.217548131942749, 0.1579841524362564, -1.1076266765594482, -0.5804417729377747, -0.04171419516205788, -0.41113314032554626, 0.4363081157207489, 0.29780927300453186, -0.41625940799713135, -0.27962541580200195, -0.8745569586753845, -0.3927958905696869, 0.810006320476532, 0.32461005449295044, -0.7031901478767395, -0.08585865050554276, 0.2089552879333496, -0.21796758472919464, 0.17387233674526215, 0.5916098952293396, -0.6001483201980591, -0.8344054222106934, -1.212602972984314, 0.5395646691322327, 0.040944602340459824, -0.06989177316427231, -0.5312379598617554, 0.6836949586868286, 0.43951061367988586, -0.22334374487400055, 0.11722256988286972, 0.30178821086883545, -0.5707520246505737, -0.23109155893325806, 0.03338787332177162, -0.6642507910728455, 0.6038726568222046, 0.15823140740394592, -0.7427684664726257, -0.40191203355789185, 0.6558538675308228, -0.1190003976225853, -1.0923466682434082, -0.3985954523086548, 0.36863991618156433, -0.9489316344261169, 0.16675281524658203, -0.8483230471611023, -0.28287044167518616, -0.9498287439346313, -0.29255831241607666, 0.24697637557983398, 0.10230075567960739, -0.6698182225227356, 1.0575474500656128, 0.5833960771560669, -0.9591147899627686, 0.1785784810781479, 0.6522949934005737, -0.3307751715183258, -0.0870736762881279, 0.35733675956726074, 0.21265965700149536, 0.05899270996451378, 0.508976936340332, 0.29692092537879944, 0.5086027979850769, -0.7372548580169678, -0.47336992621421814, 1.1209807395935059, -0.8853522539138794, 0.05787920951843262, 1.0216034650802612, -0.37332114577293396, -1.432481288909912, 0.025779753923416138, -1.4011921882629395, -0.6692519187927246, -0.2157377302646637, 0.4144340753555298, 0.1298544704914093, 0.02113564871251583, 0.08739585429430008, -0.29610031843185425, 0.1640673577785492, 0.019067881628870964, -0.5629197955131531, 0.7893239855766296, -0.1338702142238617, -0.434416800737381, 0.7000147104263306, 0.44148820638656616, -0.7529400587081909, -0.5903008580207825, -0.5684424042701721, -0.3797784149646759, 0.20000070333480835, 0.4933007061481476, -0.3134000897407532, -1.1345531940460205, 0.7934314608573914, 0.3802567422389984, 0.317861407995224, 0.17929843068122864, -0.15774652361869812, -0.1302328109741211, 0.7320108413696289, 0.05917887017130852, -0.7362616062164307, -0.5498427748680115, 1.6592475175857544, 1.0847665071487427, -0.6099212169647217, -0.10419896245002747, -0.391961932182312, -0.6917662024497986, 0.764444887638092, 0.18614144623279572, 0.028831124305725098, 0.9492478966712952, -0.1799463927745819, 0.010468076914548874, 0.14168760180473328, -1.345394492149353, -0.5196669101715088, 0.5842563509941101, 1.2365832328796387, 0.8281595706939697, 0.14048278331756592, 0.7133932709693909, 1.016745686531067, -0.5080378651618958, 0.16192777454853058, 0.6878199577331543, 0.14934304356575012, 0.02975250594317913, -0.11192386597394943, 0.2280530482530594, 0.7918917536735535, -0.8081080317497253, -0.8116243481636047, 0.07419006526470184, 0.35667338967323303, 0.16660195589065552, 0.6237940788269043, 0.9121605753898621, 0.05225524306297302, 0.896569013595581, 0.24098488688468933, 0.494871586561203, -0.5340304970741272, -0.6586028337478638, -0.294924259185791, -0.39366671442985535, -0.33632785081863403, -0.12043170630931854, -0.6006510853767395, -0.2820335626602173, -0.012685813009738922, 0.06660615652799606, 0.19072818756103516, 0.45444464683532715, 1.0574889183044434, 0.39518237113952637, 0.685794472694397, 0.1482621133327484, -0.4616124629974365, -0.3897109031677246, -1.0017181634902954, 0.06714864820241928, -0.7539694309234619, -0.15310966968536377, 0.004495815373957157, -0.17662884294986725, -0.1346803605556488]}, "authors": [{"authorId": "11348687", "name": "Jungo Kasai"}, {"authorId": "1818378366", "name": "Hao Peng"}, {"authorId": "48378494", "name": "Yizhe Zhang"}, {"authorId": "1755465", "name": "Dani Yogatama"}, {"authorId": "2123694087", "name": "Gabriel Ilharco"}, {"authorId": "143958923", "name": "Nikolaos Pappas"}, {"authorId": "145469202", "name": "Yi Mao"}, {"authorId": "2109136147", "name": "Weizhu Chen"}, {"authorId": "144365875", "name": "Noah A. Smith"}], "references": [{"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d", "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "b5b006dc558cb7fbd532d67e989173b536e8ac80", "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers"}, {"paperId": "32999b20f890bcc5effe80197045d6c147226fe4", "title": "Findings of the Association for Computational Linguistics: EMNLP 2020"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "81a1037757bc4b794c4bc483939f76526ca8d813", "title": "Pay Attention when Required"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "ac6535d096fc79dde2d9ce0329e0626b79ede7f0", "title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "07a9f47885cae97efb7b4aa109392128532433da", "title": "Hard-Coded Gaussian Attention for Neural Machine Translation"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "3ff8d265f4351e4b1fdac5b586466bee0b5d6fff", "title": "Improving Transformer Models by Reordering their Sublayers"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "ba08784bb30de51f72f88d5d64a64310d030db10", "title": "From Research to Production and Back: Ludicrously Fast Neural Machine Translation"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "2efbff15a0332f65ddbe00efadf25ceaf6de3567", "title": "OpenNMT System Description for WNMT 2018: 800 words/sec on a single-core CPU"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "fc1d981dd051063ae586a56b05390fe3ea82f040", "title": "Achieving Human Parity on Automatic Chinese to English News Translation"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454", "title": "Non-Autoregressive Neural Machine Translation"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "704aa23d0be8817dd0aa2d4794068fc167243b85", "title": "Findings of the 2017 Conference on Machine Translation (WMT17)"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "424aef7340ee618132cc3314669400e23ad910ba", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"}, {"paperId": "32e934094c4d17fe4d734b2e169ba5e3cd0ee05e", "title": "Orthogonal Random Features"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "1a327709cc53ff9e52454e50a643abf4a0ac92af", "title": "Findings of the 2016 Conference on Machine Translation"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"}, {"paperId": "69e68bfaadf2dccff800158749f5a50fe82d173b", "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"}, {"paperId": null, "title": "Rethinking attention with"}, {"paperId": "2ff74d426e712522030057624510c03713fa77ba", "title": "Linear Transformers Are Secretly Fast Weight Memory Systems"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "All training is implemented in fairseq"}, {"paperId": null, "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": null, "title": "Randomly Initialized Training We generally follow the hyperparameters chosen in Vaswani et al"}, {"paperId": null, "title": "These datasets are all encoded into subwords by BPE (Sennrich et al., 2016). We run joint BPE on all language pairs except EN-ZH"}, {"paperId": null, "title": "Apart from EN\u2192ZH where we used separate BPE operations and only tied decoder input and output embeddings, we tie all embeddings"}, {"paperId": null, "title": "Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri"}, {"paperId": "1b7db8ad49f94da9b90db89bede5f27644bb9911", "title": "SGDR: Stochastic Gradient Descent with Restarts"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "The NVIDIA CUDA basic linear algebra subroutines (CUBLAS)"}, {"paperId": null, "title": "We used mixed precision and distributed training over"}]}