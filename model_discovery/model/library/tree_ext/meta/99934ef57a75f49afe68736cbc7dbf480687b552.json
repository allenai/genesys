{"paperId": "99934ef57a75f49afe68736cbc7dbf480687b552", "title": "Efficient Quantized Sparse Matrix Operations on Tensor Cores", "abstract": "The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-the-art with a comparable accuracy for end-to-end sparse Transformer inference.", "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis", "year": 2022, "citationCount": 15, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2209.06979", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores, which supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision."}, "embedding": {"model": "specter_v2", "vector": [0.15407082438468933, 0.4356344938278198, -0.3578857481479645, 0.4558291733264923, -0.1817481517791748, 0.15919993817806244, 0.5955368876457214, -0.26097041368484497, -0.26156118512153625, -0.5307507514953613, 0.4226154685020447, -0.11324188858270645, 0.670924723148346, -0.14454220235347748, -0.11973058432340622, -0.357831209897995, -0.8029391765594482, 0.03532073274254799, 0.18797509372234344, -0.21982425451278687, 0.21281640231609344, -0.17686350643634796, -1.2863965034484863, 0.30761203169822693, 0.05560891702771187, 1.5389821529388428, 0.038639117032289505, 0.4039151966571808, -0.8585823178291321, 0.8106803297996521, 0.8505399823188782, -0.5566431283950806, 0.8419417142868042, 0.29429227113723755, -0.217556431889534, -0.14692462980747223, 0.5883508324623108, -0.7848537564277649, -0.5622146725654602, 1.3956445455551147, -0.4217362403869629, -0.0876413956284523, 0.15211831033229828, -1.1798923015594482, 0.09095843881368637, 0.2120915800333023, 0.38485971093177795, 0.6687099933624268, -0.923028290271759, -0.25097230076789856, 1.100602388381958, -1.51286780834198, 0.0196691881865263, 1.1123411655426025, 0.8847835659980774, -0.14835460484027863, -0.37033671140670776, -0.8310896754264832, 0.09884223341941833, 0.228567436337471, -0.8584352135658264, -0.4091145694255829, 0.10425227880477905, -0.16581495106220245, 1.868971824645996, -0.391947478055954, 0.2256883829832077, 0.3032153248786926, 0.20595677196979523, 0.9092274904251099, 0.1701578050851822, -0.5400401949882507, -0.006569964811205864, -0.3103945553302765, 0.11297298222780228, 0.9735034704208374, 0.0422247014939785, 0.3550347089767456, -1.3155241012573242, -0.16896075010299683, 0.7435239553451538, 0.30643242597579956, 0.6398959159851074, -0.5287946462631226, -0.5048331618309021, 0.8254075646400452, 0.473206102848053, 0.21524369716644287, -0.4654386341571808, 1.0649811029434204, 1.0752649307250977, 0.07572613656520844, -0.3082210123538971, 0.2546033263206482, 0.09403473883867264, 0.18600666522979736, -0.9566717743873596, -0.021002791821956635, 0.0706060454249382, 0.5979796648025513, -0.18892823159694672, 0.6352336406707764, -0.2509995996952057, -0.0492231510579586, 0.880742609500885, 0.3040193021297455, 0.4018607437610626, -0.7381001114845276, 0.13068874180316925, -0.7634878158569336, -0.23652254045009613, -1.1450077295303345, 0.1807611882686615, -0.6066603064537048, -1.4947456121444702, -0.8524707555770874, -0.8603503704071045, 0.3434094786643982, -0.645427942276001, 0.27233684062957764, -0.26793280243873596, 0.9355958700180054, -0.10350500792264938, 0.46189597249031067, 0.7080566883087158, 0.6448546648025513, -0.0998292863368988, 0.4569135010242462, 1.0814106464385986, -1.1642450094223022, -0.40819212794303894, -0.7247412204742432, 0.481123685836792, -0.36484256386756897, -0.06085410341620445, 0.2903711795806885, -1.2574135065078735, -1.1047370433807373, -0.6066142916679382, -0.017716065049171448, -0.3648075759410858, -0.046994324773550034, 1.4344068765640259, 0.13320255279541016, -0.954948365688324, 0.637438178062439, -0.7412552833557129, 0.2529051899909973, 0.5038086175918579, 0.43055400252342224, 0.507327139377594, -0.19571392238140106, -1.0134625434875488, 0.3468446135520935, 0.033948998898267746, -0.49378424882888794, -0.5134283900260925, -0.8900730013847351, -0.9006132483482361, 0.29993361234664917, -0.052330102771520615, -0.5413364171981812, 0.9724220633506775, 0.07226812094449997, -0.9229512214660645, 0.6647423505783081, -0.10508637130260468, -0.2934175431728363, -0.06938862055540085, 0.30384451150894165, -0.3591238856315613, 0.022248562425374985, 0.0402955487370491, 0.5580140352249146, 0.8712021112442017, 0.22095121443271637, -0.04597899317741394, 0.11585823446512222, -0.425114244222641, -0.2498125284910202, -0.5632424354553223, 0.8820320963859558, -0.27192777395248413, -0.4377356469631195, 0.5934455394744873, 0.6076158881187439, -0.450557142496109, 0.13034003973007202, -0.39960333704948425, -0.43856704235076904, 0.7467715740203857, 0.2537894546985626, 0.7009595036506653, -1.188664197921753, -0.8354361653327942, 0.17178906500339508, 0.3271714746952057, 0.034448228776454926, -0.5738791227340698, 0.4496287703514099, -0.6300575733184814, -0.12541431188583374, 0.4458007216453552, -0.6482932567596436, 0.08566675335168839, -0.22198672592639923, -1.1930781602859497, -0.12233090400695801, 0.18653297424316406, 0.8066458702087402, -0.6697513461112976, 0.07379066199064255, 0.11697313189506531, 0.6032718420028687, -1.2353121042251587, 0.8678876161575317, -0.06311284005641937, -0.32140052318573, 0.2161499708890915, -0.09269783645868301, 0.28706124424934387, -0.8434629440307617, 0.6665883660316467, -1.2147114276885986, -0.011606711894273758, 0.786960780620575, -1.073764443397522, 1.2352972030639648, -0.5024186372756958, 0.3467620313167572, 0.2779456377029419, -0.8977001905441284, 0.4664098024368286, 0.05360317975282669, 0.03367546200752258, -0.4983517527580261, 0.24194957315921783, 0.5899354815483093, -0.5014664530754089, 0.6082831621170044, 0.7839564681053162, 0.6613157391548157, 0.07639414817094803, 0.1605248898267746, 0.5189339518547058, -0.2982155978679657, 0.4381215572357178, 0.6086022853851318, 0.6429987549781799, -0.05941425636410713, 0.25613096356391907, -0.15079741179943085, 0.10252334922552109, -0.7804545164108276, -0.2125571221113205, 0.5184282660484314, 0.7108994126319885, 0.6231932640075684, 0.674991250038147, -0.7374321222305298, -0.6946058869361877, -0.13394254446029663, 0.4578417241573334, 1.2401494979858398, -0.09724415838718414, -0.3496863543987274, -0.2406972348690033, -0.05465077981352806, -0.04375847429037094, -0.5668965578079224, 0.25500378012657166, -0.1715051829814911, -0.07076302170753479, -1.177913784980774, 0.7000508904457092, 0.4638221859931946, 1.2607070207595825, -0.02255195379257202, -0.34740662574768066, -0.52756267786026, 0.62666916847229, -1.0542042255401611, -0.4162886440753937, 0.73105788230896, -0.9547969698905945, -0.07434982061386108, 0.35145995020866394, -0.1284792721271515, 0.24243773519992828, -0.33944767713546753, 1.0254244804382324, -0.4097835421562195, -0.3671351969242096, -0.1409125030040741, 0.912534236907959, -0.21857589483261108, -0.2281559854745865, -0.0724029690027237, 0.27617207169532776, -0.025887247174978256, 0.3128732144832611, -0.23492832481861115, -0.011386987753212452, -0.3770826458930969, -0.2922566533088684, 0.24578779935836792, 0.2526923716068268, 0.37343981862068176, 0.8601208329200745, -0.1720411628484726, -0.22333011031150818, -0.6937622427940369, 0.5372397303581238, 0.04914126172661781, -0.5211502909660339, -0.4289058744907379, -0.665492832660675, 0.05121893435716629, 0.7185637950897217, -0.583699643611908, 0.17741143703460693, -0.7868276238441467, -0.015208779834210873, -1.044887900352478, -0.05223628133535385, 0.0018421380082145333, 0.5386573672294617, -0.3976941406726837, 0.3145029544830322, 0.6203000545501709, 0.732978105545044, 0.16877639293670654, 0.41771334409713745, -0.8941656351089478, 0.5794139504432678, -0.04180768132209778, 0.28242045640945435, 0.13014894723892212, 0.4842056930065155, -0.5560883283615112, -0.6397333145141602, -0.40857619047164917, -0.17300280928611755, -0.0437651164829731, -0.15250548720359802, -0.861527681350708, -0.6549246311187744, 0.010925771668553352, -0.907863438129425, 0.0941169336438179, -0.05660244822502136, -0.19715413451194763, -0.08739595860242844, -0.8448923826217651, -1.6434893608093262, -0.26402127742767334, -1.1721384525299072, -1.4890475273132324, 0.6425454020500183, -0.18201293051242828, -0.06924964487552643, -0.5694915652275085, -0.5706068873405457, -0.5329647660255432, 1.0899806022644043, -0.662032425403595, 0.6658031940460205, -0.20493224263191223, -0.5439280271530151, 0.11071472615003586, -0.5474057793617249, 0.7902942895889282, -0.5864869952201843, 0.2599962055683136, -0.6213536858558655, 0.31984463334083557, -0.4302613139152527, -0.494347482919693, 0.11487763375043869, 0.08585301786661148, 1.0242946147918701, 0.10063429921865463, -0.31075319647789, 0.984039843082428, 1.5105016231536865, -0.9906067848205566, 0.2261650711297989, 0.02205536887049675, 0.9712140560150146, -0.643360435962677, -0.39588025212287903, 0.9542469382286072, -0.2648407518863678, 0.37007656693458557, 0.6085138916969299, -0.22406361997127533, -0.4087958335876465, -0.37208256125450134, 0.2998107969760895, 1.4164586067199707, 0.6984140276908875, 0.9410505294799805, -0.8400663137435913, 0.48347774147987366, -0.6258875727653503, -0.8732921481132507, 0.46380460262298584, 0.5200081467628479, 0.42366597056388855, 0.008634084835648537, -0.3995732367038727, 0.16121503710746765, 0.38317567110061646, 0.7248682975769043, -0.444838285446167, -0.7333728671073914, 0.17258964478969574, 1.080675482749939, 0.6684929132461548, 0.23097267746925354, -0.1908852756023407, 0.17241336405277252, 14.705916404724121, 1.0619025230407715, -0.6391357779502869, 0.6650919318199158, 0.5205055475234985, 0.2035457342863083, -0.06163450703024864, -0.07411576807498932, -1.465914249420166, 0.13601288199424744, 1.085472822189331, 0.325354665517807, 0.321228951215744, 0.6496196985244751, -0.23984265327453613, 0.45012208819389343, -0.4187430739402771, 1.2540730237960815, 0.438730388879776, -2.013688564300537, 0.23635880649089813, 0.12547166645526886, 0.5708374977111816, 0.7251003980636597, 0.9973211884498596, 0.6157853603363037, 0.17469792068004608, -0.4303605258464813, 0.2760869860649109, 0.058041173964738846, 1.2912665605545044, -0.09445350617170334, 0.4599851965904236, 0.37514784932136536, -0.9413032531738281, 0.22688092291355133, -0.5247100591659546, -1.4909050464630127, -0.3506690263748169, 0.6917533874511719, -0.9497887492179871, -0.6032087206840515, -0.0809880718588829, 1.190303921699524, 0.2720426321029663, 0.35782769322395325, 0.0768427923321724, 0.43834754824638367, -0.4590509831905365, 0.12320464104413986, 0.3402380049228668, 0.050214558839797974, -0.25134745240211487, -0.042767055332660675, 0.1157144159078598, -0.43136608600616455, 0.4185303747653961, 0.6221343874931335, -0.37977516651153564, -0.5157134532928467, -0.12160591036081314, -0.20725443959236145, -0.056845348328351974, 1.249081015586853, 0.3716672360897064, 0.10561179369688034, -0.6634345650672913, 0.41046449542045593, 0.3213809132575989, -0.16190655529499054, -0.46878859400749207, -0.16191881895065308, 0.6285977363586426, -0.2805124521255493, 0.2333122044801712, 0.4379320442676544, -0.7891989946365356, -0.7637439966201782, -0.8973866105079651, -0.7405005693435669, 0.1995980441570282, -0.7806589007377625, -0.9276244044303894, 0.46408456563949585, -0.5677632689476013, -0.3636273443698883, 0.4540506601333618, -1.2544922828674316, -0.12134604901075363, 0.46751174330711365, -1.2243304252624512, -0.28365275263786316, 0.22175361216068268, -0.40226656198501587, -0.6772710680961609, -0.11453235894441605, 1.1728168725967407, 0.29606321454048157, -0.2807566821575165, -0.22446243464946747, 0.030244886875152588, 0.21850937604904175, -0.6138644814491272, -0.5114343762397766, 0.8093832731246948, 0.40501201152801514, -0.02538113482296467, 0.4171351194381714, 0.029842399060726166, 0.42317667603492737, -0.8824290037155151, -0.3259308636188507, 0.2277047485113144, -0.22338230907917023, 0.6042180061340332, -0.5696935653686523, -0.5040761828422546, 0.4684585630893707, 0.14942429959774017, 0.36111417412757874, 0.3925836980342865, 0.03872760757803917, -0.9004833698272705, -0.5130100250244141, -0.4944438934326172, 0.20851384103298187, 0.2400510162115097, -0.8643009662628174, -0.07344891130924225, 0.1788213551044464, -0.11853958666324615, -1.04257333278656, -0.9377521276473999, 0.17464889585971832, -0.05396607518196106, -0.5061690211296082, 1.479014277458191, -0.36360323429107666, 1.1022506952285767, 0.833213746547699, -0.37681081891059875, -0.3088766634464264, 0.42615145444869995, -0.7201542854309082, -0.35327231884002686, -0.4480587840080261, 0.09949461370706558, -0.3548274636268616, 0.7192111015319824, 0.4786554276943207, 0.15817268192768097, -0.6296797394752502, -0.6110429167747498, -0.2540946304798126, -0.4609207510948181, -0.4790453314781189, 0.04704131558537483, 0.05762270838022232, -0.2138885259628296, 0.1430530697107315, 0.5301113128662109, 0.4025460481643677, 0.20485955476760864, -0.16195958852767944, 0.35080090165138245, -0.004291234537959099, -0.36382749676704407, -0.5820542573928833, -0.9618627429008484, -1.6649056673049927, 0.008702633902430534, -1.554818034172058, -0.6050182580947876, -0.5623284578323364, -0.22804082930088043, 0.11062365025281906, -0.3036079406738281, 0.16048087179660797, 0.5207460522651672, 0.3775201439857483, -0.48946505784988403, -0.47670719027519226, -0.5872109532356262, 0.7708015441894531, 0.9177550077438354, -0.4737149775028229, 0.586685299873352, -0.4421505331993103, 0.2490719109773636, 0.47176414728164673, 0.306640625, -0.15705101191997528, -0.7466920018196106, -0.8258384466171265, 0.3248516619205475, -0.054179999977350235, 0.3844338059425354, -1.141600489616394, 0.6953967809677124, 0.33780789375305176, -0.17327898740768433, -0.018900224938988686, 0.48840638995170593, -0.9500165581703186, -0.26760169863700867, 0.863153874874115, -0.5199458003044128, 0.2571386396884918, 0.25517112016677856, -0.7782595157623291, -0.3581220507621765, 0.6252945065498352, 0.1710919886827469, -0.8323004245758057, -0.9723657965660095, 0.5264330506324768, -0.2443893849849701, 0.2035519927740097, -0.341999888420105, -0.08112950623035431, -1.192663311958313, -0.20171920955181122, -0.07237765938043594, -0.0744716003537178, -0.2991150915622711, 0.2937262952327728, 0.3928733468055725, -1.3650556802749634, 0.6993495225906372, 0.7013447880744934, -0.4775224030017853, -0.2103193998336792, 0.2570737898349762, 0.9243718981742859, -0.7522232532501221, 0.32126984000205994, -0.11704806238412857, 0.10900232940912247, -0.7090687155723572, 0.12122627347707748, 0.8728004097938538, -0.6547666192054749, -0.3048207759857178, 1.0340291261672974, -0.372726708650589, -0.6897433400154114, 0.2002004235982895, -1.246811866760254, 0.0261691901832819, -0.4440001845359802, 0.5821385383605957, 0.2432733029127121, 0.5119608640670776, 0.25759586691856384, -0.5049264430999756, -0.11859571933746338, 0.1493644416332245, -0.06865018606185913, 0.3965398967266083, 0.44116222858428955, -0.8740960359573364, 0.31041303277015686, 0.9955556392669678, -0.665604829788208, -0.5580814480781555, -0.8446356654167175, -0.6843163371086121, -0.45058679580688477, 0.5124090313911438, 0.17287012934684753, -1.138750672340393, 0.9486605525016785, 0.482471227645874, 0.3694550096988678, 0.36770692467689514, -0.5388986468315125, 0.5531214475631714, 0.7649951577186584, 0.2970142364501953, -0.4231075048446655, -0.10303226858377457, 1.3192046880722046, 0.8621407151222229, -0.7109273672103882, 0.3768989145755768, -0.6328588724136353, -0.5923090577125549, 0.7545080780982971, 0.2010100781917572, -0.2985354959964752, 1.1602332592010498, 0.5283951163291931, -0.45741137862205505, 0.17634066939353943, -0.9368784427642822, -0.25740644335746765, 0.6693597435951233, 0.8846070170402527, 0.7195425033569336, -0.18973197042942047, 0.3788592517375946, 0.8955966830253601, -0.11753285676240921, 0.03489190340042114, 0.23245927691459656, 0.3271059989929199, -0.22796575725078583, 0.35796600580215454, -0.4314245879650116, 1.0955594778060913, -0.7747440338134766, -1.166670799255371, 0.7251632213592529, 0.5541067719459534, 0.36782920360565186, 0.19329382479190826, 1.029670000076294, -0.23739847540855408, 0.4521639943122864, -0.07522208243608475, 0.431172251701355, -0.6435527801513672, -0.0944090411067009, -0.13683979213237762, -0.782737135887146, -0.5185405611991882, -0.052844755351543427, -0.311638742685318, -0.6047831773757935, -0.5596966743469238, 0.6682934165000916, -0.06797504425048828, 0.3686656057834625, 0.5112195014953613, 0.6370562314987183, 0.6892975568771362, -0.29561275243759155, -0.5814325213432312, -0.9639633297920227, -0.5845233798027039, -0.27291497588157654, -0.2615237236022949, -0.3323470950126648, -0.3064082860946655, -0.23246464133262634, -0.3311556875705719]}, "authors": [{"authorId": "2298884755", "name": "Shigang Li"}, {"authorId": "25003800", "name": "Kazuki Osawa"}, {"authorId": "1713648", "name": "T. Hoefler"}], "references": [{"paperId": "dabb102ae61a06bfb45f174081dd57f1768f6e6a", "title": "A Pattern-Based SpGEMM Library for Multi-Core and Many-Core Architectures"}, {"paperId": "5679ff44de6c462c6320ab497f80860d2faa14e8", "title": "Efficient Tensor Core-Based GPU Kernels for Structured Sparsity under Reduced Precision"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "10f3ca78e194552427ebe9173b19d1b910469e27", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"}, {"paperId": "71ca4c16fe6ba98bfa6a6a2b0b94151f35b809b4", "title": "KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks"}, {"paperId": "1243f1b6d3df83961fde788c9c2da90ac646ed68", "title": "APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores"}, {"paperId": "8a0a7170977cf5c94d9079b351562077b78df87a", "title": "A White Paper on Neural Network Quantization"}, {"paperId": "1d0b2f974d22cc82da6d9d1b4c741d4b065e004c", "title": "TileSpMV: A Tiled Algorithm for Sparse Matrix-Vector Multiplication on GPUs"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "e0076ac1166617d3d4dbb36c7bb6de85ee78a1e7", "title": "TPrune"}, {"paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "title": "ViViT: A Video Vision Transformer"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "d1d1f5aac45ccd1e51ea1bac872091ac80dc45d5", "title": "Sparta: high-performance, element-wise sparse tensor contraction on heterogeneous memory"}, {"paperId": "9d6acac70b2d1fdb861a08b00766ef263109cd7f", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "905e2fdee5e30a8aee32f2008854bf543f7293a7", "title": "Fast and Scalable Sparse Triangular Solver for Multi-GPU Based HPC Architectures"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "76e486e18c1af63dca5b72276fa55bd5031c0bc0", "title": "Joint Pruning & Quantization for Extremely Sparse Neural Networks"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "2b6a58d265e33705fd28c42b1941c5e4e5ac2621", "title": "Accelerating Binarized Neural Networks via Bit-Tensor-Cores in Turing GPUs"}, {"paperId": "70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328", "title": "Sparse GPU Kernels for Deep Learning"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "9bb5665fe48e7122beda73a53316de9f7f243b19", "title": "APQ: Joint Search for Network Architecture, Pruning and Quantization Policy"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "946a43eb327c5ee2087187720f15c46b2985cb1c", "title": "Bayesian Bits: Unifying Quantization and Pruning"}, {"paperId": "c9d3c181d999b0e11c6e4c51b3f9aefd01489e0f", "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "159dc82a5ee901716b0154051988b5408acfc861", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression"}, {"paperId": "610d0b290f0f1c22b03f220d6ba332627a5f6f3e", "title": "Deep Neural Network Compression by In-Parallel Pruning-Quantization"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "02db9a52d50e346243a8ce4a51f06d87f445677f", "title": "BSTC: a novel binarized-soft-tensor-core design for accelerating bit-based approximated neural nets"}, {"paperId": "3f8df14de4320cca46c547a538b615b88fc6a083", "title": "Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "d28c18a3c2a0afdc0a8634d18345af8d36e1f948", "title": "A Constructive Prediction of the Generalization Error Across Scales"}, {"paperId": "336868be817536e7c7fc88c391a2860cd869ea2b", "title": "Drawing early-bird tickets: Towards more efficient training of deep networks"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "3dce66616c3e74706ca71064a7fbf1e246dc65f8", "title": "IA-SpGEMM: an input-aware auto-tuning framework for parallel sparse matrix-matrix multiplication"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "3366e9eb81880d172752d4397cb8e9e6de02b935", "title": "Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model"}, {"paperId": "d0c85c0204aeb13c57adea30555c3fa37acf9ebb", "title": "Joint Optimization of Quantization and Structured Sparsity for Compressed Deep Neural Networks"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "9351db066908667129533962702b8b8582cee443", "title": "Performance-Aware Model for Sparse Matrix-Matrix Multiplication on the Sunway TaihuLight Supercomputer"}, {"paperId": "d9e97a293b98292713f4ab28b915b093fb923fb3", "title": "Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks"}, {"paperId": "54c4642d017830e1faddbb49f0377228d2b01493", "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision"}, {"paperId": "a8e1b91b0940a539aca302fb4e5c1f098e4e3860", "title": "LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks"}, {"paperId": "0e5d529befc3ca2e3e3371a0c39dc05731c1d5b7", "title": "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking"}, {"paperId": "beaa8f6cefa222e013e4e03f8da69a091c5ec875", "title": "swSpTRSV: a fast sparse triangular solve with sparse level tile layout on sunway architectures"}, {"paperId": "a1c922be467d1c0c64b963e65dae41778b81b2a0", "title": "Deep Learning Scaling is Predictable, Empirically"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6a821cb17b30c26218e3eb5c20d609dc04a47bcb", "title": "Exploring the Regularity of Sparse Structure in Convolutional Neural Networks"}, {"paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"paperId": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "5672ce28f2927b81b01303e4926643c55a4c8133", "title": "OSKI: A library of automatically tuned sparse matrix kernels"}, {"paperId": "bbb001ccf4ae68c312ac03a2e2a8affbc571fcda", "title": "On improving the performance of sparse matrix-vector multiplication"}, {"paperId": null, "title": "NVIDIA Hopper Architecture In-Depth"}, {"paperId": "851c2e1942642537499c743c324f10624b7b77ac", "title": "Accurate Post Training Quantization With Small Calibration Sets"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Accelerating inference with sparsity using the nvidia ampere architecture and nvidia tensorrt"}, {"paperId": "77e73174e606c0820a52a940088832b32d9a033e", "title": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"paperId": null, "title": "\u201cTuning CUDA Applications for Ampere,\u201d"}, {"paperId": null, "title": "\u201cDeep learning matrix collection,\u201d"}, {"paperId": null, "title": "\u201cParallel thread execution isa application guide,\u201d"}, {"paperId": null, "title": "\u201cCUDA C++ Programming Guide,\u201d"}, {"paperId": null, "title": "Exploiting NVIDIA Ampere Structured Sparsity with cuSPARSELt"}, {"paperId": null, "title": "\u201ccuSPARSE,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "\u201cAI and compute,\u201d"}, {"paperId": null, "title": "\u201cNvidia tesla v100 gpu architecture,\u201d"}, {"paperId": "4a61a6368aa881cccf115a12280b72e1c39be8c5", "title": "Improving Performance of Sparse Matrix-Vector Multiplication"}, {"paperId": null, "title": "\u201cAMD Instinct MI200 Instruction Set Architecture Reference Guide,\u201d"}, {"paperId": "1d27a56a8133f947a5a0217b00241d26f585f834", "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}, {"paperId": null, "title": "Google Research . [ n . d ] , \u201c Deep learning matrix collection , \u201d 2021 . [ Online ]"}]}