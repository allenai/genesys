{"paperId": "1607ee857e7fd5bcd16581859f4049cd61fc909a", "title": "Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning", "abstract": "Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and data distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed strategies for identifying informative training examples out of large datasets. However, these strategies come with additional computational costs associated with subset selection or data distillation before training begins, and furthermore, many are shown to even under-perform random sampling in high data compression regimes. As such, many data pruning, coreset selection, or distillation methods may not reduce 'time-to-accuracy', which has become a critical efficiency measure of training deep neural networks over large datasets. In this work, we revisit a powerful yet overlooked random sampling strategy to address these challenges and introduce an approach called Repeated Sampling of Random Subsets (RSRS or RS2), where we randomly sample the subset of training data for each epoch of model training. We test RS2 against thirty state-of-the-art data pruning and data distillation methods across four datasets including ImageNet. Our results demonstrate that RS2 significantly reduces time-to-accuracy compared to existing techniques. For example, when training on ImageNet in the high-compression regime (using less than 10% of the dataset each epoch), RS2 yields accuracy improvements up to 29% compared to competing pruning methods while offering a runtime reduction of 7x. Beyond the above meta-study, we provide a convergence analysis for RS2 and discuss its generalization capability. The primary goal of our work is to establish RS2 as a competitive baseline for future data selection or distillation techniques aimed at efficient training.", "venue": "arXiv.org", "year": 2023, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.18424", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "Repeated Sampling of Random Subsets (RSRS or RS2) is introduced as a competitive baseline for future data selection or distillation techniques aimed at efficient training and demonstrates that RS2 significantly reduces time-to-accuracy compared to existing techniques."}, "embedding": {"model": "specter_v2", "vector": [0.2713771462440491, 0.5682132244110107, -0.7677591443061829, 0.15644140541553497, -0.579253077507019, 0.3150423765182495, 0.3222869038581848, -0.48354390263557434, -0.883363664150238, 0.0372958742082119, 0.07320769131183624, -0.0941324383020401, 0.251433789730072, 0.05652419477701187, -0.15934032201766968, 0.023556681349873543, -0.6818428039550781, 0.05182678624987602, 0.29634055495262146, -0.04208742082118988, -0.008561278693377972, -0.47949501872062683, -1.3780122995376587, -0.1633886694908142, 0.7026317715644836, 1.1538134813308716, 0.025637056678533554, 0.6249229311943054, -0.26785916090011597, 0.469023734331131, 0.3640313744544983, -0.24549108743667603, 0.7522369623184204, -0.20419375598430634, -0.5421627163887024, 0.05498446524143219, 0.5153214931488037, -0.7181011438369751, -0.6562893986701965, 1.0102713108062744, -0.008817845955491066, 0.5135934352874756, -0.13947021961212158, -0.33448076248168945, 0.17298975586891174, 0.4148661196231842, 0.2723211944103241, 0.7589658498764038, -0.6060121655464172, -0.38734182715415955, 1.0816553831100464, -1.3403034210205078, 0.3474498987197876, 1.1627042293548584, 0.9840285778045654, 0.5883018970489502, -0.46709391474723816, -0.840307891368866, 0.5170936584472656, 0.18921108543872833, -0.7335545420646667, -0.09639047086238861, 0.38159313797950745, -0.03765923157334328, 1.328092336654663, -0.03710980713367462, 0.1498439908027649, 0.4170161187648773, -0.25977739691734314, 1.6086385250091553, -0.023759042844176292, -0.48876768350601196, 0.21818028390407562, 0.12677688896656036, 0.6617661118507385, 0.5230830907821655, 0.10505034029483795, 0.4465622305870056, -0.7723444104194641, -0.1504272222518921, -0.04721081256866455, 0.3624850809574127, 0.07665397226810455, -0.14961466193199158, 0.9079366326332092, 0.8192898631095886, 0.4535007178783417, 0.3395986258983612, -0.3657376766204834, 1.1377718448638916, 0.5082526803016663, 0.3274597227573395, 0.46172550320625305, 0.5979703664779663, -0.08526083827018738, 0.2924969792366028, -1.0740535259246826, 0.059054650366306305, -0.23877812922000885, 0.8884860277175903, -0.01821655035018921, 0.3503682017326355, -0.39908334612846375, 0.7162246704101562, 0.703281819820404, -0.6262339353561401, 0.8286680579185486, -0.9012441039085388, 0.4045082628726959, -0.3131325840950012, -0.32630282640457153, -0.3201482892036438, 0.0025737020187079906, -0.5834816098213196, -1.0557653903961182, -0.9497972130775452, -0.8588383197784424, -0.17917075753211975, -0.7337445616722107, 0.7168770432472229, -0.3152620196342468, 0.2787195146083832, -0.2770780026912689, 0.5702569484710693, 0.4964554011821747, 0.2502305507659912, 0.155930757522583, -0.017509236931800842, 0.6476041078567505, -0.6823375821113586, 0.11497629433870316, -0.8575165867805481, 0.381508469581604, 0.10702750086784363, 0.12869353592395782, 0.08813925832509995, -1.5283616781234741, -1.0671589374542236, -0.9546790719032288, 0.06358321011066437, -0.39015793800354004, 0.25959014892578125, 0.8469394445419312, 0.20839978754520416, -0.4728035032749176, 1.4525694847106934, -0.3874693512916565, 0.12191888689994812, 1.3669044971466064, 0.2462940663099289, 0.4731246531009674, -0.4472863972187042, -0.6064612865447998, 0.17973248660564423, 0.3540123403072357, -0.7695247530937195, -0.3667531907558441, -0.8392805457115173, -0.4333224594593048, -0.09285245835781097, 0.6354290246963501, -0.5042724609375, 1.1612262725830078, -0.27473175525665283, -0.708829939365387, 0.5821994543075562, 0.09555912017822266, -0.3988381326198578, 1.042880892753601, -0.6052119731903076, -0.007357209920883179, 0.11010508239269257, -0.3395734131336212, 0.636063277721405, 0.48130476474761963, -0.41252684593200684, 0.04217390716075897, 0.38120904564857483, -0.45571768283843994, -0.4120088219642639, -0.1897502839565277, 0.3017284870147705, -0.5423647165298462, -0.6368693709373474, 0.5999800562858582, 0.4765419363975525, -0.416821151971817, -0.03613525629043579, -0.7445054650306702, -0.9459884166717529, 1.0107289552688599, -0.12332464009523392, 0.779086709022522, -0.7897253632545471, -1.06902277469635, -0.06294912099838257, 0.21292272210121155, 0.16052421927452087, -0.619036853313446, -0.10809657722711563, -0.3705807328224182, 0.9568116068840027, -0.22384731471538544, -1.072623372077942, -0.17674776911735535, -0.22394058108329773, -0.6624112129211426, -0.07205066084861755, 0.41706526279449463, 0.8415321111679077, -0.9234991669654846, 0.2728777825832367, 0.03709913790225983, 0.1539127081632614, -1.4655178785324097, 0.6327229738235474, -0.5566917657852173, -0.05646127462387085, -0.2649098038673401, -0.01778586208820343, 0.32206305861473083, -0.1766190230846405, 0.2399957925081253, -0.30831971764564514, 0.23569029569625854, 1.0849120616912842, -0.17287856340408325, 0.911262035369873, -0.4863542914390564, 0.6231647729873657, -0.26202014088630676, -0.6604677438735962, 0.1861874759197235, 0.014106783084571362, 0.26711606979370117, -0.34841474890708923, 0.37096577882766724, 0.49950549006462097, -0.7170659303665161, 0.3445236384868622, 0.7114269733428955, 0.7603263258934021, -0.11863880604505539, 0.37738895416259766, 0.7666764855384827, -0.2748849093914032, 0.4660326838493347, 0.40492334961891174, 0.9610129594802856, 0.25899019837379456, 0.30518078804016113, 0.09614989161491394, -0.05998104065656662, -0.7523898482322693, 0.22880791127681732, 0.9062857031822205, 0.7053916454315186, 1.1868226528167725, 0.2865055799484253, -1.0664986371994019, -0.7947291135787964, 0.25249311327934265, 0.5318734645843506, 1.1802949905395508, -0.22323264181613922, -0.11928503215312958, -0.6403790712356567, -0.8635178804397583, -0.2679428160190582, 0.3074401617050171, -0.3986613154411316, -0.38440150022506714, -0.4642527997493744, -1.4399924278259277, 0.6366745233535767, -0.12776201963424683, 1.7371470928192139, -0.49628832936286926, -0.473313570022583, -0.5000240206718445, 0.2845773994922638, -0.6830174922943115, -0.12043013423681259, 0.7453014850616455, -0.9690781831741333, -0.6463455557823181, 0.21445101499557495, 0.1705424189567566, 0.31936314702033997, -0.19860030710697174, 1.2007511854171753, 0.12087208777666092, -0.6818434596061707, -0.011129084974527359, 0.47666293382644653, -0.6593413949012756, -0.3726422190666199, 0.42959317564964294, -0.24622267484664917, -0.5937247276306152, 0.3834913372993469, 0.10269130766391754, -0.3079618215560913, 0.172494575381279, -0.29584816098213196, -0.19760923087596893, 0.14389348030090332, 0.2888439893722534, 1.1398199796676636, 0.012775787152349949, 0.4680461883544922, -1.4797558784484863, 0.7965112328529358, -0.28761839866638184, -0.19685648381710052, -0.04421830549836159, -0.617789626121521, -0.2963293194770813, 0.760539174079895, -0.7277814149856567, -0.012617292813956738, -1.2000508308410645, 0.10412145406007767, -0.8794808387756348, -0.13177672028541565, -0.1875927746295929, 0.9392273426055908, -0.1891242265701294, 0.5113608837127686, 0.20245766639709473, 0.5282385349273682, -0.43951916694641113, 0.22629886865615845, -0.9552915096282959, 0.45785000920295715, 0.18601104617118835, -0.11101367324590683, -0.17921453714370728, -0.025753051042556763, -0.5765966176986694, -0.6689231395721436, -0.050832752138376236, 0.12199165672063828, -0.16000282764434814, -0.3031221032142639, -0.34978705644607544, -0.17420350015163422, -0.3290456533432007, -0.5234332084655762, -0.2164025753736496, -0.7125589847564697, -0.12474168092012405, -0.44705134630203247, -1.1744449138641357, -1.042581558227539, -0.5121996998786926, -0.9321945905685425, -1.2633867263793945, 0.22214093804359436, 0.004110707901418209, -0.5261821150779724, 0.025224482640624046, -0.4786761701107025, -0.44819962978363037, 0.6939890384674072, -0.47802039980888367, 0.5217084884643555, -0.454066663980484, -0.5399371981620789, -0.3885461688041687, -0.04273104667663574, 0.8191592693328857, -0.610615074634552, 0.051604777574539185, -1.1226959228515625, -0.011377532035112381, -0.02484208531677723, -0.7881878018379211, 0.738268256187439, 0.35189786553382874, 1.40109384059906, -0.431430846452713, -0.21403352916240692, 0.8468164205551147, 1.6653183698654175, -0.6874194741249084, 0.2534281611442566, -0.07659409940242767, 0.564524233341217, -0.3316328227519989, -0.1529492288827896, 0.7322458624839783, -0.5761058330535889, -0.012633217498660088, 0.28210321068763733, -0.10696548968553543, -0.5774756669998169, -0.6382102966308594, -0.19262449443340302, 1.0200961828231812, 0.6614184379577637, 0.4691723883152008, -0.606160581111908, 0.5266247391700745, -0.8658777475357056, -0.7190864086151123, 1.0977375507354736, 0.9289780259132385, 0.4668032228946686, -0.2601252794265747, -0.21783088147640228, 0.10620741546154022, 0.42192116379737854, 0.1685224175453186, -0.8002820014953613, -0.19857485592365265, 0.13277548551559448, 0.7920371890068054, 0.767352819442749, 0.25805240869522095, -0.35122403502464294, 0.24679990112781525, 14.868703842163086, 1.058849811553955, 0.01822241209447384, 0.6962323188781738, 1.2702840566635132, -0.44585636258125305, -0.2597462832927704, -0.3278355002403259, -1.2339260578155518, 0.24917609989643097, 0.6665210723876953, 0.5948281288146973, 0.8096688389778137, 0.7345151305198669, -0.056183379143476486, -0.05721336603164673, -0.19285638630390167, 0.8863525390625, 0.18460895121097565, -1.5598386526107788, 0.22263944149017334, -0.026423972100019455, 0.7983609437942505, 1.0997915267944336, 0.7445946335792542, 0.8601075410842896, 0.6171676516532898, -0.32874491810798645, 0.36402684450149536, 0.10455858707427979, 1.2018027305603027, -0.16339907050132751, 0.417793333530426, 0.3966725766658783, -0.47522372007369995, -0.3515157103538513, -0.7649452090263367, -0.706770658493042, -0.08017981052398682, 0.4704739451408386, -0.39059513807296753, -0.4915224611759186, 0.10832884907722473, 0.6538988351821899, -0.5839340090751648, 0.5822955369949341, 0.015228238888084888, 0.5524648427963257, -0.5725331902503967, -0.00961040984839201, 0.2570754289627075, 0.2422875463962555, 0.2826802432537079, -0.22258798778057098, 0.028641700744628906, 0.03346298262476921, 0.077764593064785, 0.5973237156867981, -0.9312568306922913, -0.33996453881263733, -0.5593298673629761, 0.1845231056213379, 0.3463500142097473, 0.45586100220680237, 0.5607247352600098, 0.02258138917386532, -0.12209194153547287, -0.15915662050247192, 0.7027487754821777, 0.14219555258750916, -0.3877458870410919, -0.2728089392185211, 0.2667427659034729, -0.6353710889816284, -0.03499054163694382, 0.7424476146697998, -1.0161447525024414, -0.39410221576690674, -0.5693226456642151, -0.49282729625701904, 0.13358575105667114, -0.6371970176696777, -0.6968255639076233, 0.7664027214050293, -0.10847928375005722, -0.552478015422821, 0.4441995620727539, -0.3427608013153076, -0.11820108443498611, 0.34824976325035095, -1.1382793188095093, 0.17413419485092163, -0.02334827557206154, -0.5907883048057556, 0.008904263377189636, -0.4223756194114685, 0.6255585551261902, 0.2581459879875183, -0.5053118467330933, 0.22870387136936188, -0.028818314895033836, -0.19365620613098145, -0.7211889624595642, -0.49268820881843567, 0.8901206254959106, 0.25843438506126404, -0.3444453179836273, 0.2631164491176605, -0.3155946731567383, 0.3101136386394501, -0.34449025988578796, -0.6149940490722656, 0.06619811058044434, -0.10551421344280243, -0.594450056552887, -0.318730890750885, -0.8118632435798645, 0.33427712321281433, 0.08958414196968079, -0.03436104208230972, 0.42562592029571533, 0.01060378085821867, -0.35064876079559326, -0.2172839343547821, -1.3047887086868286, 0.26271435618400574, 0.8532685041427612, -0.7018134593963623, -0.11032198369503021, 0.010274172760546207, 0.4271680414676666, -0.5867255926132202, -0.49861255288124084, 0.2638978362083435, 0.021102769300341606, -0.6007741093635559, 1.1951876878738403, -0.4415894150733948, 0.9344884157180786, 1.1630595922470093, -0.180257648229599, -0.6495983600616455, 0.2607190012931824, -1.0091890096664429, 0.0528809055685997, -0.005716101732105017, 0.3193061649799347, -0.4601776897907257, 1.0504692792892456, 0.7800089120864868, 0.20407293736934662, -0.6463439464569092, -0.8561952710151672, -0.35003918409347534, -0.006939242128282785, -0.7020532488822937, 0.30220654606819153, -0.19071029126644135, -0.46039098501205444, -0.015329965390264988, 0.5390101075172424, -0.1476716846227646, -0.06599168479442596, -0.85312420129776, 0.3772628605365753, 0.012128897942602634, -0.14302925765514374, -0.8764538764953613, -0.5359746813774109, -1.7159491777420044, -0.20286986231803894, -1.112667441368103, -0.2231481820344925, -0.22518758475780487, -0.6148582100868225, -0.34506723284721375, 0.08360709249973297, -0.23065543174743652, 0.5382348895072937, -0.08784519135951996, -0.33138135075569153, -0.2852998971939087, -0.5325770974159241, 0.9759150743484497, 0.7161535620689392, -0.589969277381897, -0.09014805406332016, 0.22395069897174835, -0.050991930067539215, 0.6681066155433655, 0.7936733365058899, -0.4549177289009094, -0.8934357762336731, -0.9993031620979309, 0.0977361649274826, -0.3707471489906311, 0.1324859857559204, -1.0146450996398926, 0.5640847682952881, 0.4106302857398987, -0.04279354587197304, -0.3623442053794861, 0.3026733994483948, -1.3214049339294434, -0.33679986000061035, 0.5226754546165466, -0.7473005652427673, -0.04050689935684204, 0.08904808014631271, -0.39425748586654663, -0.15019266307353973, -0.08443324267864227, 0.5046474933624268, -1.1510928869247437, -0.45584988594055176, 0.3322848975658417, -0.32023102045059204, 0.43248313665390015, -0.2705373764038086, -0.23078612983226776, -1.4696792364120483, -0.02662086859345436, -0.16478410363197327, 0.35232675075531006, -0.41639062762260437, 0.3269728124141693, 0.5971039533615112, -1.3731956481933594, 0.6197994351387024, 0.3679500222206116, -0.510145366191864, 0.6510348916053772, 0.4259833097457886, 0.48629045486450195, -0.7419193387031555, -0.15374386310577393, 0.5544344782829285, 0.3857709467411041, -0.5398350954055786, -0.07908924669027328, 1.1057727336883545, -0.47461163997650146, -0.3168477714061737, 1.1085647344589233, -0.7341100573539734, -1.0284686088562012, 0.546352207660675, -0.9605263471603394, 0.04384300857782364, -0.2123560905456543, 0.45313891768455505, 0.8199346661567688, 0.18847529590129852, 0.6409544944763184, -0.28052905201911926, -0.10985837876796722, -0.23291052877902985, -0.35838258266448975, 0.6774832606315613, 0.06227242574095726, 0.11783585697412491, 0.4500841796398163, 1.3376104831695557, -0.8890132308006287, -1.1774715185165405, -0.871613621711731, -0.3234870433807373, -0.665851354598999, 0.49066558480262756, -0.27251875400543213, -1.181525468826294, 0.4672800302505493, 0.3620205223560333, 0.35963356494903564, 0.5047996044158936, -0.4498041272163391, 0.17338107526302338, 0.6930815577507019, 0.10233050584793091, -0.8469692468643188, -0.18506430089473724, 0.9593493342399597, 1.2780983448028564, -0.9635434746742249, 1.1494905948638916, -0.0925101637840271, -0.5606535077095032, 0.6184126734733582, 0.43817514181137085, -0.9259801506996155, 1.121159553527832, -0.5953370928764343, -0.694614827632904, -0.06879326701164246, -1.0793631076812744, -0.1739014834165573, 0.5950495600700378, 0.8310356736183167, 0.32763198018074036, -0.09648521989583969, 0.019038204103708267, 0.844892144203186, -0.10595356673002243, 0.2211514413356781, 0.3643946051597595, 0.5469527840614319, -0.41088950634002686, 0.3346492350101471, 0.28963011503219604, 1.4012101888656616, -1.0616517066955566, -0.43195056915283203, 0.07534672319889069, 0.8402179479598999, 0.430982768535614, 0.28572243452072144, 0.9627863764762878, -0.5576469898223877, 0.3311874568462372, 0.06615401059389114, 0.3214225471019745, -0.3665239214897156, -0.43558526039123535, -0.054749488830566406, -0.4506976902484894, -0.20211762189865112, -0.025533460080623627, -0.6585093140602112, 0.38540181517601013, -0.573943555355072, -0.0226543378084898, -0.13637182116508484, 0.05813949182629585, 0.7690326571464539, 0.5669944286346436, 0.669583261013031, 0.11193084716796875, -0.762115478515625, -0.6943777799606323, -0.8062615394592285, 0.16780926287174225, -0.3965616524219513, -0.29417791962623596, -0.1157313659787178, -0.3604671359062195, -0.6927491426467896]}, "authors": [{"authorId": "2173711120", "name": "Patrik Okanovic"}, {"authorId": "103967638", "name": "R. Waleffe"}, {"authorId": "2148975767", "name": "Vasilis Mageirakos"}, {"authorId": "52210145", "name": "Konstantinos E. Nikolakakis"}, {"authorId": "1697131", "name": "Amin Karbasi"}, {"authorId": "2215926645", "name": "Dionysis Kalogerias"}, {"authorId": "51274710", "name": "Nezihe Merve Gurel"}, {"authorId": "145071799", "name": "Theodoros Rekatsinas"}], "references": [{"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "eb0410bd2de945d69d95e3eef04f40c5de2915c6", "title": "Towards Sustainable Learning: Coresets for Data-efficient Deep Learning"}, {"paperId": "f6615f84dded34f1d7f748866ee9ce5023460b3a", "title": "Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "bb4721b1a806ac00308bfb174edf3c36b6f0b620", "title": "Data pruning and neural scaling laws: fundamental limitations of score-based algorithms"}, {"paperId": "eee717fe442232623ea4abdb3f1e6cf7987577e1", "title": "Dataset Distillation: A Comprehensive Review"}, {"paperId": "7619ed35ac30712fefd8f3e7f5d921f209f1268c", "title": "Coverage-centric Coreset Selection for High Pruning Rates"}, {"paperId": "59fed7ca092c7e83583906456756abba8ce9295a", "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities"}, {"paperId": "ef881d8000acbbcda3660eec1a592e76aa6e75ee", "title": "Efficient Adversarial Training With Data Pruning"}, {"paperId": "45122c8f76a4e2fd0163d1f0522db37e97ea4721", "title": "Beyond neural scaling laws: beating power law scaling via data pruning"}, {"paperId": "6a8db14262ca2017cb253e12b8daeb57989a38df", "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"}, {"paperId": "b18e54ed671373a080998a8ce903c1c127a128d4", "title": "Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks"}, {"paperId": "2d77266137673c6c5eacbb37db386da8810d4479", "title": "Dataset Condensation via Efficient Synthetic-Data Parameterization"}, {"paperId": "97efe211f9aa74afccd9ea498b32dcaa3cfe51b5", "title": "GraB: Finding Provably Better Data Permutations than Random Reshuffling"}, {"paperId": "4a664c728b8a84d30e0526dc575eb356efbbd56f", "title": "DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning"}, {"paperId": "11154b89486fd7b41bfab5f8b0e19756c488523e", "title": "Dataset Distillation by Matching Training Trajectories"}, {"paperId": "c92f1bc0b93404482054e290d526f66d70f9ac62", "title": "Dataset Condensation with Contrastive Signals"}, {"paperId": "a66606a0bb11a1d5a7a14ec09df8a3481121ad6c", "title": "MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification"}, {"paperId": "c03f42a6686eb5fa8b62088c4c2b5545bc60ebdc", "title": "Dataset Condensation with Distribution Matching"}, {"paperId": "216d093cb2ad81bf55c21dbce2217f2b9032e67b", "title": "Just Train Twice: Improving Group Robustness without Training Group Information"}, {"paperId": "a6e25ca9ee9d3e45c6d1957c0dc3324a9816c34e", "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training"}, {"paperId": "bdb0c3228deb252e7d20bea2768bb1d11d405a27", "title": "SVP-CF: Selection via Proxy for Collaborative Filtering Data"}, {"paperId": "9257963d2b442c72eb8d17f7951da582b22d9990", "title": "RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning"}, {"paperId": "22bc75c1b758c83a6e29384453a4574c8b78f197", "title": "GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "1f180118903044c4d1b39be35710be7221daf4ca", "title": "Dataset Condensation with Differentiable Siamese Augmentation"}, {"paperId": "199ac333b1ff37917d1ab4a2e2002d9605b3db1e", "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b", "title": "A Survey of Deep Active Learning"}, {"paperId": "690fdc0595ff72a95ac4ce0b7ea1cd19ab6b60e4", "title": "Contextual Diversity for Active Learning"}, {"paperId": "27088479c86dc8912eff9dacdda23a9f2494d41b", "title": "Enabling On-Device CNN Training by Self-Supervised Instance Filtering and Error Map Pruning"}, {"paperId": "2119bc054f3369e19933b8fa2318b4ac9fa23da3", "title": "Submodular Combinatorial Information Measures with Applications in Machine Learning"}, {"paperId": "6b67b1f55ebad02eaa73328c3989d64c1dc23dff", "title": "Flexible Dataset Distillation: Learn Labels Instead of Images"}, {"paperId": "5a94bcc168330318d3020aa4d41bd73cf68ab285", "title": "Dataset Condensation with Gradient Matching"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0dd4784c31758f40b73c04ef67e700d84bdaac4a", "title": "E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings"}, {"paperId": "3ce32038cb0360181e4109211d505790e3f80d3e", "title": "How Good is SGD with Random Shuffling?"}, {"paperId": "db787640c9b42416ff8d7015546e667e58267177", "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty"}, {"paperId": "790985a4bee821046992ff3d5322ff11dd1b4262", "title": "Coresets for Data-efficient Training of Machine Learning Models"}, {"paperId": "25d2938d4a024c140d07c344f2e2a3cc1ffc0497", "title": "Teaching a black-box learner"}, {"paperId": "a2b5d224895d96bfe2e384e2dcf1ebd136ac3782", "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning"}, {"paperId": "baafa551c19d5684082a266c04dace955a9d048b", "title": "Random Shuffling Beats SGD after Finite Epochs"}, {"paperId": "6748549de116d89307cd07694fc3b65c82e925f4", "title": "On Coresets for Logistic Regression"}, {"paperId": "114c1ed1ae2e2a4b3d775c5a55d2c916f9d85972", "title": "Adversarial Active Learning for Deep Networks: a Margin Based Approach"}, {"paperId": "1269e191091eadeed6f246cf5a6692b178bb4d94", "title": "Super-convergence: very fast training of neural networks using large learning rates"}, {"paperId": "c6b61535f1544835cca3851ceb34222ebc5b4377", "title": "State-of-the-Art Speech Recognition with Sequence-to-Sequence Models"}, {"paperId": "e19f1a1cebac63621e35296170155ce1851d88a0", "title": "Stochastic Nonconvex Optimization with Large Minibatches"}, {"paperId": "c342c71cb23199f112d0bc644fcce56a7306bf94", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "0f7c85357c366b314b5b55c400869a62fd23372c", "title": "Train faster, generalize better: Stability of stochastic gradient descent"}, {"paperId": "95d327660aaee3f1fd8b98517a7ad355149663e3", "title": "Coresets for Nonparametric Estimation - the Case of DP-Means"}, {"paperId": "ddab254c1071dc8036336a7e192ab3f719391747", "title": "Distributed Balanced Clustering via Mapping Coresets"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "d0b0c3e5a1e768490bc9b759685930541957508b", "title": "Introductory Lectures on Convex Optimization - A Basic Course"}, {"paperId": "0dadb9383f3d8b2b1d45f09c6b41d88fd9b7d430", "title": "Accelerated gradient methods for nonconvex nonlinear and stochastic programming"}, {"paperId": "e7590aa6a2e74642e82586b5713661b18cd84e20", "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods"}, {"paperId": "1621f05894ad5fd6a8fcb8827a8c7aca36c81775", "title": "An optimal method for stochastic composite optimization"}, {"paperId": "4b197d60de05e14781d67a318b29a4d4600a7460", "title": "Optimal Distributed Online Prediction Using Mini-Batches"}, {"paperId": "1ad0ffeb6e69a5bc09ffa53712888b84a3b9df95", "title": "Super-Samples from Kernel Herding"}, {"paperId": "cc58edaacecbcc20b8a54ad4f68648415a7b974a", "title": "Herding dynamical weights to learn"}, {"paperId": "8f683dbe9ac52a4faef2464b99eabbbba1ab211d", "title": "Moderate Coreset: A Universal Method of Data Selection for Real-world Data-efficient Deep Learning"}, {"paperId": "be8933165282991265bb7d05a7e0a4701422a54a", "title": "Active Learning is a Strong Baseline for Data Subset Selection"}, {"paperId": "5e8c8dd23f69354c56724b636a19445085ac51ac", "title": "CAFE Learning to Condense Dataset by Aligning Features"}, {"paperId": "2f41f29dc3a2cb50b87010e8ad2bc9f962868f57", "title": "Random Reshuffling is Not Always Better"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "A Efros. Dataset distillation"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "8d3a318b62d2e970122da35b2a2e70a5d12cc16f", "title": "A method for solving the convex programming problem with convergence rate O(1/k^2)"}, {"paperId": null, "title": "Aligning Features (CAFE)"}, {"paperId": null, "title": "When less is more: Investigating data pruning for pretraining llms at scale"}, {"paperId": null, "title": "Self-supervised prototypes with easy examples (SSP-Easy"}, {"paperId": null, "title": "Random: standard baseline; sample a static random subset of the dataset once before training 2. Contextual Diversity (CD) [1] 3. Herding [8,58] 4. K-Center Greedy [52] 5."}, {"paperId": null, "title": "Less is more for alignment"}, {"paperId": null, "title": "Distribution Matching (DM)"}, {"paperId": null, "title": "Active Learning with loss-based example informativeness (AL (LL)"}]}