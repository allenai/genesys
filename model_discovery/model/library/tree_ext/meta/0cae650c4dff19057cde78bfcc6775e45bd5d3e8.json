{"paperId": "0cae650c4dff19057cde78bfcc6775e45bd5d3e8", "title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese", "abstract": null, "venue": "Machine Learning with Applications", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development."}, "embedding": {"model": "specter_v2", "vector": [0.2260526418685913, 0.8655480742454529, -0.5138701796531677, 0.32400962710380554, -0.2927784323692322, -0.09428413957357407, 0.6526557803153992, 0.009660485200583935, -0.4000170826911926, -0.11233372241258621, 0.4445590376853943, 0.008777456358075142, 0.470329612493515, -0.0899239256978035, -0.16634973883628845, 0.2838476300239563, -0.6549891233444214, 0.5171579122543335, -0.07770224660634995, -0.04858488216996193, -0.5258575081825256, -0.47123321890830994, -1.0109494924545288, -0.13565993309020996, 0.8214049935340881, 0.054829180240631104, 0.39579179883003235, 1.0516135692596436, -0.45547446608543396, 0.22880315780639648, 0.4049144685268402, -0.31775596737861633, -0.006608503870666027, 0.1469888538122177, -0.21929891407489777, 0.07618387788534164, 0.13632425665855408, -0.2574041187763214, -0.31935766339302063, 0.5959052443504333, -0.15414473414421082, 0.10499192774295807, 0.0803527683019638, -0.880366861820221, -0.37232547998428345, 1.5343141555786133, 0.804623007774353, 0.7992669939994812, -0.043753355741500854, -0.1635909080505371, 1.0230242013931274, -1.3610113859176636, 0.20587372779846191, 1.666012167930603, 0.8418334126472473, 0.4579887390136719, -0.534265398979187, -0.629165768623352, 0.22491870820522308, -0.3531528413295746, -0.7475080490112305, -0.6611969470977783, -0.1681123971939087, 0.10156259685754776, 2.0500998497009277, -0.3261950612068176, 0.19969208538532257, 0.7046566605567932, 0.02055249735713005, 1.2754791975021362, 0.15771746635437012, -0.8896616101264954, -0.39474719762802124, 0.16471725702285767, -0.13383236527442932, 0.7906756401062012, -0.19380950927734375, 0.05050593987107277, -0.899852991104126, -0.2324216663837433, 0.24555277824401855, -0.19221042096614838, 0.03147874027490616, 0.02006475254893303, -0.47882312536239624, 1.0885604619979858, 0.2761417031288147, 1.1049988269805908, -0.06493993103504181, 0.4001745581626892, 0.21626970171928406, 0.08538070321083069, 0.16244971752166748, 0.4329080283641815, -0.4421254098415375, 0.5700146555900574, -1.289695382118225, 0.17137038707733154, 0.21575693786144257, 0.6029408574104309, -0.1723611205816269, 0.48151588439941406, -0.9327215552330017, 0.1552901715040207, 1.453597068786621, 0.4895421862602234, 1.1669740676879883, -0.5928990840911865, 0.34702450037002563, -0.8064002990722656, 0.039967603981494904, -0.2766524851322174, -0.30406928062438965, 0.022562870755791664, -0.14469224214553833, -1.5481758117675781, -0.40762215852737427, -0.048730719834566116, -1.0915266275405884, 0.6965463757514954, -0.0705631822347641, -0.12489213794469833, 0.4496062695980072, 0.12190945446491241, 0.8368414044380188, 0.648811936378479, 0.22180801630020142, -0.018724575638771057, 1.0232378244400024, -1.1303223371505737, -0.6171210408210754, -1.4076567888259888, 0.4419727623462677, -0.557539165019989, 0.27228298783302307, -0.43835464119911194, -1.3472076654434204, -0.9284246563911438, -0.5976375341415405, -0.08700220286846161, -0.7431644797325134, 0.5181372165679932, 1.2434567213058472, 0.8818972110748291, -0.8869386315345764, 0.6876094341278076, -0.40688371658325195, -0.6622288823127747, 0.48132622241973877, 0.18628691136837006, 0.3948265016078949, -0.1783515065908432, -1.357933759689331, 0.1954062283039093, 0.363810271024704, -0.5987438559532166, -0.42558708786964417, -0.1801753044128418, -1.0912870168685913, -0.2700158357620239, 0.2651746869087219, -0.24114172160625458, 1.4041374921798706, 0.009868886321783066, -1.2126044034957886, 0.6856708526611328, -0.6022648811340332, -0.4218839704990387, 0.21912118792533875, 0.03633129596710205, -0.7252135276794434, -0.2075665444135666, 0.17974624037742615, 0.9075663685798645, 0.6870512962341309, 0.19226905703544617, 0.34302011132240295, 0.36771321296691895, -0.15206941962242126, -0.3105931878089905, -0.5936151742935181, 1.2110772132873535, -0.2808622717857361, -0.5453040599822998, 0.21454818546772003, 0.41491445899009705, -0.33215445280075073, -0.1331590712070465, -0.7513018846511841, -1.042962908744812, 0.5843925476074219, -0.25337469577789307, 0.974429190158844, -0.6956984400749207, -0.8192653059959412, -0.17106181383132935, -0.2532334327697754, 0.013568036258220673, -0.7951157093048096, 0.6093452572822571, -0.34668412804603577, 0.7443405389785767, -0.30353930592536926, -1.1701570749282837, 0.00413197698071599, 0.10729965567588806, -0.4818103611469269, -0.3977556824684143, 0.40498414635658264, 1.2934592962265015, -0.5861251950263977, -0.1906709372997284, -0.39450061321258545, 0.372659295797348, -0.8773950338363647, 0.9102950096130371, -0.41131356358528137, 0.03790675103664398, -0.22954963147640228, -0.0996372401714325, 0.028226299211382866, -0.23902155458927155, 0.455861896276474, -0.06839852035045624, -0.3457646369934082, 0.4633442759513855, -0.48017778992652893, 1.551786184310913, -0.17869912087917328, 0.13056571781635284, -0.20395591855049133, -0.11556430906057358, 0.10315204411745071, 0.6296220421791077, -0.2738594114780426, -0.17611326277256012, 0.3496491312980652, 0.9612628221511841, -0.4076288044452667, 0.3828708231449127, 0.7052779793739319, 0.5688655376434326, -0.2394668459892273, 0.5331279039382935, 0.5700353980064392, -0.5785415172576904, 1.1974233388900757, 0.6293158531188965, 0.30211687088012695, 0.4540228843688965, 0.28774717450141907, -0.013738087378442287, 0.42480793595314026, -0.7888541221618652, 0.02349986881017685, 0.37874194979667664, 0.5112529397010803, 0.8679885864257812, 0.34881284832954407, -0.6809346675872803, -0.033979516476392746, -0.09668347984552383, 0.5452775955200195, 1.1150479316711426, -0.6536785364151001, -0.12376590818166733, -0.9627864360809326, -0.36205583810806274, -0.4271516501903534, -0.07214502990245819, -0.33574143052101135, 0.18411652743816376, -0.41742703318595886, -1.1176577806472778, 0.9428344964981079, -0.0687856376171112, 0.38451606035232544, -1.1509513854980469, -0.446245402097702, -0.23177237808704376, 0.03674831613898277, -0.9118824601173401, -0.7604690790176392, 0.25364309549331665, -0.7560856342315674, -0.1455887109041214, -0.15974745154380798, -0.011837155558168888, 0.17908264696598053, -0.7289888858795166, 0.7261561155319214, -0.5111296772956848, -0.2884438931941986, -0.06711237132549286, 0.6180307865142822, -0.589726984500885, -0.9122322797775269, 0.32892942428588867, 0.3503667712211609, -0.15828564763069153, 0.12724292278289795, 0.4063122272491455, 0.512736976146698, -0.20036500692367554, -0.403894305229187, 0.11309519410133362, -0.06299451738595963, -0.3438490629196167, 0.6497836709022522, -0.36136218905448914, -0.07106652110815048, -1.050031304359436, 1.065155267715454, 0.1576012820005417, -0.2575209438800812, 0.2242903858423233, -0.574306070804596, -0.21083572506904602, 0.5095822811126709, -0.7726373076438904, -0.19929315149784088, -0.8361715078353882, 0.5185210704803467, -0.0626431554555893, -0.22689107060432434, 0.23025229573249817, 0.3520106077194214, 0.44127902388572693, 0.16783548891544342, 0.3139767348766327, -0.05275117605924606, -0.4433479607105255, 0.748965322971344, -0.7734047770500183, 0.2049436718225479, 0.2671215534210205, 0.319886714220047, -0.11440131068229675, -0.7515621185302734, -0.47514745593070984, -0.49603405594825745, 0.17905721068382263, -0.2566693425178528, -0.14771662652492523, 0.284595251083374, -0.8393183350563049, -0.48556244373321533, -0.04220720753073692, -0.9456164240837097, -0.04518681392073631, -0.23900477588176727, -0.34355005621910095, 0.013117962516844273, -0.8142012357711792, -1.2451329231262207, -0.46858182549476624, -0.9684304594993591, -1.1259115934371948, 0.5299980044364929, 0.5273382663726807, -0.4515654444694519, -0.5882113575935364, 0.3387017548084259, -0.1678662747144699, 0.8689069747924805, -0.5263246297836304, 0.6357247829437256, -0.19270390272140503, 0.2042299062013626, -0.35733890533447266, 0.25557008385658264, 0.1628296673297882, -0.4867858290672302, 0.6396947503089905, -0.8895334005355835, -0.06901907175779343, -0.5717779994010925, -0.6817408800125122, -0.029044359922409058, 0.39191967248916626, 0.2504715919494629, 0.08577640354633331, -0.5265798568725586, 0.2248588651418686, 1.1990509033203125, -1.1434022188186646, -0.3868217468261719, -0.05982279032468796, 0.950775146484375, 0.3122001588344574, -0.7478234767913818, 0.4726192057132721, 0.5430482625961304, 0.3164105713367462, -0.04384594410657883, -0.05222523584961891, -0.3332079350948334, -0.7189549803733826, 0.720427393913269, 1.797290563583374, 0.508922815322876, -0.18355825543403625, -1.2163753509521484, 0.7027378082275391, -0.8224746584892273, -0.3677249252796173, 0.34373870491981506, 0.5591722130775452, 0.5477572083473206, -0.29590582847595215, -0.33602139353752136, 0.24150727689266205, 0.47963228821754456, 0.7307556867599487, -0.16715288162231445, -1.1721217632293701, -0.49809980392456055, 0.6474869251251221, -0.08133100718259811, 0.5438200235366821, -0.35184746980667114, 0.7158768177032471, 14.822124481201172, 0.7804850935935974, -0.06816409528255463, 0.8549205660820007, 0.41749706864356995, -0.08625692874193192, -0.7691240906715393, 0.15763522684574127, -1.0728626251220703, -0.20260323584079742, 1.5225814580917358, 0.10119085013866425, 0.973671555519104, 0.07599765807390213, 0.3202584981918335, 0.25146669149398804, 0.005630942527204752, 0.08230943977832794, 0.6140502691268921, -1.0661377906799316, 0.3980541527271271, 0.20955370366573334, 0.8983006477355957, 1.0764566659927368, 0.5103383660316467, 0.7421712875366211, 0.4888218343257904, -0.5573781728744507, 0.7277696132659912, 0.21848593652248383, 0.7165577411651611, -0.08731616288423538, 0.16933271288871765, 0.32211053371429443, -1.1779608726501465, -0.5988996028900146, -0.6671070456504822, -1.1814419031143188, 0.3334987163543701, 0.4399958550930023, -0.45105305314064026, -0.5978842377662659, -0.07307322323322296, 0.9663358330726624, -0.33570432662963867, 0.12310577929019928, -0.404418408870697, 0.5124615430831909, -0.2570471167564392, 0.1033773422241211, 0.2562927007675171, 0.6069886088371277, 0.31676435470581055, -0.3490750193595886, 0.40764060616493225, -0.7257594466209412, 0.4858228862285614, 0.539244532585144, -0.8232693076133728, 0.25811663269996643, -0.2571018934249878, -0.6474414467811584, -0.14881718158721924, 0.959472119808197, -0.028312968090176582, 0.37264761328697205, -0.3994693160057068, 0.2368115335702896, 0.6407129764556885, -0.07899238914251328, -0.210253044962883, -0.08725464344024658, -0.13036681711673737, -0.318157821893692, 0.15477429330348969, 0.6551157236099243, 0.12668266892433167, -0.6894556879997253, -0.6612341403961182, -0.42884477972984314, 0.24945566058158875, -0.9330939054489136, -0.4744143784046173, 1.0450814962387085, -0.5464337468147278, -0.1233842521905899, 0.26482143998146057, -0.5673232078552246, -0.47619080543518066, 0.5880314707756042, -1.623166561126709, -1.195538878440857, 0.6641294956207275, -0.2548956871032715, -0.31369748711586, -0.020721029490232468, 1.4855059385299683, -0.23418691754341125, -0.6506380438804626, -0.15575085580348969, 0.04905518516898155, 0.016696404665708542, -0.39980223774909973, -0.41179126501083374, 1.1011581420898438, 0.6807400584220886, 0.465247243642807, 0.13497591018676758, -0.0017822717782109976, 0.010251891799271107, -0.5181110501289368, 0.2108449637889862, 1.5283845663070679, -0.8655523061752319, 0.01996663585305214, -0.7961857318878174, -0.45700275897979736, 0.12663209438323975, 0.7831996083259583, -0.06366842985153198, 0.2928851544857025, -0.21456781029701233, -0.37367966771125793, 0.026315107941627502, -0.8477399349212646, 0.05320146307349205, 0.2541619837284088, -0.9506396055221558, -0.34184515476226807, 0.3046080768108368, 0.4229649603366852, -0.5338312387466431, -0.666698157787323, -0.3322433531284332, 0.23237314820289612, -0.13763916492462158, 0.6437272429466248, -0.43421700596809387, 0.4354754388332367, 0.9545707106590271, 0.1266510933637619, -0.7115090489387512, 0.44775623083114624, -1.4119701385498047, -0.09515548497438431, 0.1406383365392685, 1.049617052078247, -0.53945392370224, 0.06091767176985741, 0.3379194736480713, 0.701435923576355, -0.35947102308273315, -0.8622444868087769, -0.11330591887235641, 0.12583515048027039, -0.7591274976730347, 0.6164048910140991, -0.1809975504875183, -0.25649169087409973, -0.1816878318786621, 0.4021044969558716, 0.5445942282676697, -0.24914245307445526, -0.5366613864898682, 0.5304964184761047, -0.13251985609531403, -0.32233816385269165, -0.41485831141471863, -0.03535394370555878, -0.7255603075027466, -0.007332453038543463, -1.1570144891738892, -0.09401977062225342, -0.9124513864517212, -0.3075423836708069, 0.06684231758117676, -0.21071653068065643, 0.35272857546806335, 0.3961978256702423, 0.15714481472969055, -0.19376197457313538, -0.6569973230361938, -0.2130981981754303, 0.8490368723869324, 0.9590787887573242, -0.8022543787956238, -0.04549708217382431, -0.052448902279138565, 0.297758013010025, 0.520102322101593, 0.4746944010257721, -0.8313400149345398, -0.7769498229026794, -1.572933554649353, 0.42829927802085876, -0.3471328318119049, -0.37602099776268005, -0.9180023074150085, 0.3354896605014801, 0.6394661664962769, -0.45906704664230347, 0.3608171045780182, 0.2075328677892685, -0.33387669920921326, -0.4162709414958954, 0.7745509743690491, -0.42637699842453003, 0.5001437067985535, 0.1863774210214615, -0.9251961708068848, -0.14954392611980438, 0.42901507019996643, -0.271415650844574, -0.9929765462875366, -0.2292335480451584, 0.6245102286338806, -0.9795311689376831, 0.369436651468277, -0.39514583349227905, -0.29409950971603394, -0.80263352394104, -0.10306835919618607, -0.2455139309167862, 0.056345030665397644, -0.40884459018707275, 1.3792083263397217, 0.28756025433540344, -0.7705913186073303, -0.035665977746248245, 0.601874053478241, -0.09073668718338013, -0.5835884809494019, 0.22492371499538422, 0.5608183741569519, -0.408416748046875, 0.6153445243835449, 0.4176251292228699, 0.38120412826538086, -0.657735288143158, -0.38937392830848694, 0.5341293811798096, -0.8650849461555481, 0.003637416986748576, 1.105712890625, -0.9189855456352234, -1.6711070537567139, -0.1001162901520729, -1.3448840379714966, -0.9252121448516846, -0.3862622380256653, 0.580115556716919, 0.056693412363529205, -0.3769565522670746, -0.515293300151825, -0.4911060035228729, 0.25795337557792664, 0.35085436701774597, -0.45304954051971436, 0.6095201969146729, -0.05410715937614441, -0.6127398610115051, 0.583836555480957, 0.10095534473657608, -0.07419296354055405, -0.5555577874183655, -0.4036969542503357, -0.5065664649009705, 0.08392713218927383, 0.20680086314678192, -0.31868603825569153, -0.7061115503311157, 0.9330931305885315, 0.46362531185150146, 0.6090316772460938, -0.11871595680713654, -0.4187440574169159, 0.6092054843902588, 0.36433470249176025, 0.16999273002147675, -0.7650436758995056, -0.6066437363624573, 1.3631175756454468, 0.7870355248451233, -0.8704896569252014, 0.41946232318878174, 0.032843217253685, -0.6202751994132996, 0.7479757070541382, -0.010716009885072708, 0.3372015058994293, 0.9166902303695679, -0.3190457224845886, 0.2176685929298401, 0.4230394959449768, -0.8936124444007874, -0.2579456865787506, 0.7367234826087952, 0.8757399320602417, 0.9298927187919617, 0.5695816278457642, -0.15616042912006378, 1.0024464130401611, -0.098789282143116, 0.07554443180561066, 0.609665036201477, 0.6166433691978455, -0.1613650768995285, -0.24697503447532654, -0.07450161874294281, 0.6146823763847351, -0.6986995339393616, -0.7975658774375916, 0.06129291281104088, 0.40082085132598877, 0.7939392328262329, 0.9372255206108093, 0.43013063073158264, 0.5786253809928894, 0.268170028924942, 0.11948459595441818, 0.5430726408958435, -0.7138254642486572, -0.46541258692741394, -0.12919756770133972, -0.5380055904388428, 0.0013328298227861524, -0.24138076603412628, -0.47345641255378723, -0.5304667353630066, -0.18228985369205475, 0.26126232743263245, 0.09335201233625412, 0.57421875, 1.3080567121505737, 0.4649890661239624, 0.37322673201560974, -0.19694499671459198, -0.7922189831733704, -0.04811735451221466, -1.1722766160964966, 0.13871589303016663, -0.9573966264724731, -0.5852007269859314, 0.17366555333137512, -0.04301752150058746, -0.47779014706611633]}, "authors": [{"authorId": "144191865", "name": "N. Corr\u00eaa"}, {"authorId": "2281831530", "name": "Sophia Falk"}, {"authorId": "2281826857", "name": "Shiza Fatimah"}, {"authorId": "2281827893", "name": "Aniket Sen"}, {"authorId": "51486703", "name": "N. D. Oliveira"}], "references": [{"paperId": "c6cb23bb92a9b6a36614a153efb24decaa146e83", "title": "Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task"}, {"paperId": "7260442ef9c0448f07ce3803efd49cebaffcebe9", "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"}, {"paperId": "560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd", "title": "TinyLlama: An Open-Source Small Language Model"}, {"paperId": "1cff5549753a518ed9d6a3517b5050968d710b27", "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer"}, {"paperId": "3783c7c074513fcee62c851b2abb2e264c2cbde3", "title": "YAYI 2: Multilingual Open-Source Large Language Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "7673114da5d82381cf8e75408089c98f73fad06d", "title": "adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds"}, {"paperId": "2c0312c604f9f7638bb4533b39e0ae81e7f6ab12", "title": "The Falcon Series of Open Language Models"}, {"paperId": "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b", "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "bde98ab6970961b6a1bde2ab4c5ce83f7060b3ef", "title": "Cabrita: closing the gap for foreign languages"}, {"paperId": "afbc46dcbc954bce5b0a2cde5bfae561ee340767", "title": "Challenging AI for Sustainability: what ought it mean?"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "e534e65562e945cc67f4075ac2757051fc188ea8", "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "3fae5fb60ba9eeead5f1b2681ac6b09fe9cc4926", "title": "A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "title": "Scaling Data-Constrained Language Models"}, {"paperId": "647f520eea08cf8e98b99319dcb7e32cc3a78c72", "title": "Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation"}, {"paperId": "dbfd154190667087ed1cd6c7f75a81858c2f397e", "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "7c217cc7524251f42887438834912e06129c3299", "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "ff56d14db96239d7da3789f276a6e04a83179a83", "title": "Advancing Neural Encoding of Portuguese with Transformer Albertina PT-"}, {"paperId": "e5adc219685c9941b9a3d029480af4a51c0ea05a", "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"}, {"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "ece77610adfb0fb162dd22ef694f2777393c319a", "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "bd8412c233bf3815d8e905911b58e12bd6f279da", "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "a2a278c7b1ea7d851b49a310cccad45df14082f3", "title": "ClueWeb22: 10 Billion Web Documents with Rich Information"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "7a25155364476839b6d1fc0653cd8611327ab9ba", "title": "mGPT: Few-Shot Learners Go Multilingual"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "2ac19d63e1adba20473a6d1122c598f81efc3c58", "title": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "645a317c9305207e95d03b5756a65e7e850f32d5", "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "ff69d758764157e612f92f97a987838312c568a9", "title": "Compute and Energy Consumption Trends in Deep Learning Inference"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "cddf40e579a596d0110b260313adf43470617c4c", "title": "Datasets: A Community Library for Natural Language Processing"}, {"paperId": "2132eac5628bc200de226b51f1dfb82423ff1d24", "title": "MarIA: Spanish Language Models"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "6fd41ee12ae828bb7de965bf7e06bb1bd5259fe7", "title": "IndT5: A Text-to-Text Transformer for 10 Indigenous Languages"}, {"paperId": "c58e10adfc33a7660f8ef88b30ceb066224dfbef", "title": "HateBR: A Large Expert Annotated Corpus of Brazilian Instagram Comments for Offensive Language and Hate Speech Detection"}, {"paperId": "c342798bafc1eaaa60c652fc90fd738941542133", "title": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "f0520b991c1a16449a95ff98771d3cf86ed71428", "title": "GottBERT: a pure German Language Model"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "1d95011355628f7aef068ab1914198e43258c530", "title": "BERTimbau: Pretrained BERT Models for Brazilian Portuguese"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "528dd0da358b4939d99eeb92548deccfeac48bd6", "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages"}, {"paperId": "6a93dee02af9548a3255a396b19c479c8183a55c", "title": "The ASSIN 2 Shared Task: A Quick Overview"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "d1faad340b63ac3c507701cb72ce5fef6c911c77", "title": "Estimation of energy consumption in machine learning"}, {"paperId": "1d2d3ba8511767d0311413d9081284f6d550b559", "title": "Energy Usage Reports: Environmental awareness as part of algorithmic accountability"}, {"paperId": "b61c6405f4de381758e8b52a20313554d68a9d85", "title": "CamemBERT: a Tasty French Language Model"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187", "title": "Quantifying the Carbon Emissions of Machine Learning"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "3f9df96b26c42dea6dd6cad64557a3b7d698ea90", "title": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning"}, {"paperId": "66117f82def0c69a3b9cc77eb3e2694b0245ca86", "title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "92343cecdc990380de362b969eec60081959f507", "title": "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "8c207ece66e0a63627869c49fb37c6811072539b", "title": "The brWaC Corpus: A New Open Resource for Brazilian Portuguese"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "2c9a868423a9552bf746810c240399607f2ddd5a", "title": "University Entrance Exam as a Guiding Test for Artificial Intelligence"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": null, "title": "Nicolas de Camaret"}, {"paperId": null, "title": "NousResearch"}, {"paperId": null, "title": "Openllama: An open reproduction of llama"}, {"paperId": null, "title": "Faquad-nli: a benchmark for textual entailment"}, {"paperId": null, "title": "Koala: A dialogue model for academic research"}, {"paperId": null, "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1", "title": "Few-shot Learning with Multilingual Language Models"}, {"paperId": null, "title": "Fully sharded data parallel: faster ai training with fewer gpus"}, {"paperId": null, "title": "Gportuguese-2 (portuguese gpt-2 small): a language model for por-tuguese text generation (and more nlp tasks...)"}, {"paperId": null, "title": "Codecarbon: Track emissions from compute and recommend ways to reduce their impact on the environment"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Tokenizers: Fast state-of-the-art tokenizers optimized for research and production"}, {"paperId": null, "title": "Weights & biases: A tool for visualizing and tracking your machine learning experiments"}, {"paperId": null, "title": "Accelerate: Training and inference at scale made simple, efficient and adaptable"}, {"paperId": null, "title": "Maicon Domingues (2023b)"}, {"paperId": null, "title": "Bruno Henrique (2024b)"}]}