{"paperId": "7e2cf6f2f2b0df1c4e4bc8c9dd5d1dfd2e38caae", "title": "ULSeq-TA: Ultra-Long Sequence Attention Fusion Transformer Accelerator Supporting Grouped Sparse Softmax and Dual-Path Sparse LayerNorm", "abstract": "Transformer networks have been increasingly successful in various fields. The input sequence lengths have become much larger as the algorithm and task complexity develops, which is challenging due to high computational and storage cost. Softmax and LayerNorm are bottleneck nonlinear operators in ultra-long sequence Transformer networks. To improve the efficiency of Softmax, assumption-based and quantization-based Softmax approaches are introduced. However, the sparsity potential to accelerate Softmax itself is not fully discovered. To improve the efficiency of LayerNorm, some works reduce the input size, and some works explore the pipeline. However, the sparsity potential is also not yet explored. To address these challenges, this article presents the ULSeq-TA software\u2013hardware co-design framework. The software includes 1) the grouped sparse Softmax method to leverage the data magnifying characteristic to explore the middle and post-Softmax sparse processing and 2) the dual-path sparse LayerNorm method which explores the dimensional significance for sparse calculation. The hardware includes 1) an attention fusion architecture which reduces the on-chip memory with fused operators; 2) the grouped sparse Softmax core; and 3) the dual-path sparse LayerNorm core. Experiments show that the software achieves <inline-formula> <tex-math notation=\"LaTeX\">$4.45\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$7.59\\times $ </tex-math></inline-formula> computation reduction with little output difference for Softmax and LayerNorm, respectively. The hardware architecture supports at most 32768 sequence length with only 186-kB on-chip memory and achieves <inline-formula> <tex-math notation=\"LaTeX\">$1.75\\times -1.98\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$3.22\\times -4.32\\times $ </tex-math></inline-formula> speedups for sparse Softmax core and sparse LayerNorm core with little accuracy loss, respectively.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The ULSeq-TA software\u2013hardware co-design framework is presented, which includes the grouped sparse Softmax method to leverage the data magnifying characteristic to explore the middle and post-Softmax sparse processing and the dual-path sparse LayerNorm method which explores the dimensional significance for sparse calculation."}, "embedding": {"model": "specter_v2", "vector": [0.7940036058425903, 0.8661000728607178, -0.46332526206970215, 0.034079086035490036, -0.2694670557975769, 0.22698350250720978, 0.20118926465511322, -0.2438865453004837, -0.13249613344669342, -0.18223987519741058, 0.7959907650947571, 0.10995840281248093, 0.6372300386428833, 0.26216021180152893, 0.002343361033126712, -0.28590884804725647, -1.0652695894241333, -0.37892428040504456, -0.19790178537368774, -0.3094342052936554, 0.2017560452222824, -0.9518892765045166, -1.0037258863449097, 0.3243938982486725, 0.2817378640174866, 1.2464301586151123, 0.4189296364784241, 0.9061408042907715, -0.5465506911277771, 0.6341352462768555, 0.380123496055603, -0.4137847125530243, 0.7293190360069275, -0.1358335018157959, -0.08505815267562866, 0.0029834620654582977, 0.5637998580932617, -0.28289324045181274, -0.5592074990272522, 1.4598346948623657, 0.00804007425904274, 0.23576638102531433, 0.08014487475156784, -0.778099775314331, 0.01582350581884384, 1.1224116086959839, 0.3332463800907135, 0.5612146258354187, -0.5756458640098572, -0.4828455150127411, 1.032654881477356, -1.2577018737792969, -0.37461942434310913, 1.1442865133285522, 0.5790620446205139, 0.11527351289987564, 0.14844369888305664, -0.7234646677970886, 0.5211185216903687, 0.3153042197227478, -0.47658711671829224, -0.7234792709350586, 0.2631414830684662, -0.12200288474559784, 1.589808702468872, -0.4819157123565674, 0.07399274408817291, 0.593504786491394, 0.16983014345169067, 0.9354967474937439, -0.18989406526088715, -0.6384992003440857, 0.1995120644569397, -0.18053743243217468, 0.41925927996635437, 0.7429362535476685, -0.3567039668560028, 0.27632907032966614, -1.3620104789733887, -0.03240125998854637, 0.20187832415103912, 0.24440568685531616, 0.4144338369369507, 0.026087410748004913, -0.09930869191884995, 0.5629968643188477, 0.3043077886104584, 0.48096203804016113, -0.3435281813144684, 0.6555103659629822, 0.9358715415000916, 0.25360411405563354, -0.021148860454559326, 0.061154864728450775, -0.006625449284911156, 0.17541056871414185, -1.2054675817489624, -0.11474084854125977, -0.2924783527851105, 1.0714848041534424, -0.22008615732192993, 0.9016179442405701, -0.6262680292129517, 0.12547022104263306, 1.2210956811904907, 0.24954763054847717, 0.42321252822875977, -0.5345136523246765, 0.22218385338783264, -0.8377227187156677, -0.1970459520816803, -0.7277510166168213, -0.19879312813282013, -0.5256397128105164, -1.178792119026184, -0.575823187828064, -0.5814400911331177, 0.14081180095672607, -0.9895741939544678, 0.5399230718612671, -0.9001948237419128, 0.454479455947876, 0.44544824957847595, 0.47456976771354675, 0.5266326665878296, 0.6705719232559204, 0.14843156933784485, -0.12353909015655518, 1.2901462316513062, -1.1286593675613403, -0.863396942615509, -1.0469659566879272, 0.2757834196090698, -0.3464996814727783, 0.040592294186353683, 0.01704469881951809, -1.6127382516860962, -1.1066486835479736, -1.0404572486877441, 0.1037319153547287, -0.3948168158531189, -0.09440013766288757, 0.963407039642334, 0.3900620937347412, -0.9558703899383545, 0.5705944895744324, -0.3311571180820465, -0.13424870371818542, 0.6233808994293213, 0.1604251116514206, 0.6352124214172363, -0.09500931948423386, -1.352245807647705, 0.17989331483840942, 0.40890470147132874, -0.3934956192970276, 0.003625499550253153, -0.8034030199050903, -1.2441198825836182, 0.5801039338111877, 0.3440416753292084, -0.3477766811847687, 1.1573598384857178, -0.4660036265850067, -1.360711932182312, 0.46715179085731506, -0.1211501806974411, -0.26405179500579834, -0.2474355399608612, 0.18466776609420776, -0.6276515126228333, -0.13717541098594666, -0.5266021490097046, 0.2981242835521698, 0.5676650404930115, 0.12288206070661545, -0.29801592230796814, 0.14823418855667114, -0.6390256881713867, 0.04967755451798439, -0.5477190613746643, 0.9865517020225525, -0.32088565826416016, -0.5584084391593933, 0.3074672818183899, 0.6422935128211975, -0.45833462476730347, -0.040794048458337784, -0.24168720841407776, -0.4609152674674988, 0.6938374042510986, 0.20792801678180695, 0.7969993948936462, -1.1379694938659668, -1.0008182525634766, 0.05452711135149002, -0.17133964598178864, 0.3201731741428375, -0.5683684945106506, 0.5347850918769836, -0.5513352751731873, -0.2326231598854065, 0.06146750599145889, -0.8727520704269409, 0.03877054899930954, -0.24590249359607697, -1.0115385055541992, -0.22107021510601044, -0.11716033518314362, 1.0941545963287354, -0.8677983283996582, -0.07835858315229416, -0.09685962647199631, -0.0813152864575386, -0.987409234046936, 1.053218126296997, -0.23769636452198029, -0.35057002305984497, 0.28218311071395874, 0.06576990336179733, 0.2769612967967987, -0.33206015825271606, 0.502712070941925, -0.9257755279541016, 0.06571634113788605, 0.3175797164440155, -0.527787983417511, 1.9640130996704102, 0.0110661955550313, 0.7095531821250916, 0.15905117988586426, -1.0323582887649536, 0.6645737290382385, 0.5049101114273071, 0.21995532512664795, -0.7086076140403748, 0.614492654800415, 0.14247144758701324, -0.4825018644332886, -0.06753181666135788, 1.0328972339630127, 1.0175552368164062, -0.5398182272911072, -0.2339063584804535, 0.9191606640815735, -0.004418336786329746, 0.332275927066803, 0.4236495792865753, 0.15973536670207977, 0.03922363743185997, 0.564643919467926, -0.38123610615730286, 0.2356174886226654, -1.1615822315216064, 0.34070447087287903, 0.6474007964134216, 0.20975345373153687, 0.7676339149475098, 0.37574148178100586, -0.5724124312400818, -0.521355152130127, 0.05618085712194443, 0.628061830997467, 1.3040615320205688, 0.1170089840888977, -0.062134869396686554, -0.29089826345443726, -0.018540922552347183, -0.3720529079437256, -0.36870235204696655, 0.09963996708393097, -0.41258764266967773, -0.2347705364227295, -0.8463308215141296, 0.8183842897415161, 0.34497079253196716, 0.7690834403038025, -0.7951554656028748, -0.08311967551708221, -0.2996411621570587, 0.5425163507461548, -1.0198652744293213, -0.7823064923286438, 0.602349579334259, -0.38537272810935974, 0.13587987422943115, -0.15322698652744293, -0.09851466119289398, 0.23789098858833313, -0.6160323023796082, 0.6717286705970764, -1.079600214958191, -0.23045839369297028, -0.08956602215766907, 0.259940505027771, -0.6395865678787231, -0.4816124737262726, 0.11790065467357635, 0.1254674792289734, -0.029616789892315865, 0.48067009449005127, -0.14408710598945618, 0.1689518541097641, -0.6273021101951599, -0.14209075272083282, 0.1300412118434906, 0.3326123058795929, -0.18218252062797546, 0.48696911334991455, -0.6061685085296631, 0.2635761797428131, -0.8582172989845276, 0.9702849388122559, -0.16819661855697632, -0.510726273059845, -0.13750694692134857, -0.6769648194313049, -0.13714465498924255, 0.5420582294464111, -0.360564649105072, -0.2363993227481842, -0.6432627439498901, 0.035922497510910034, -0.920380175113678, -0.1865214854478836, 0.16575907170772552, 0.5416654944419861, 0.02638380043208599, 0.30574026703834534, 0.8418363332748413, 0.47451460361480713, 0.31036028265953064, 0.5554956197738647, -0.41542333364486694, 0.8863242864608765, 0.40250855684280396, -0.07305469363927841, -0.11944461613893509, -0.006290116813033819, -0.6953672766685486, -0.4903038442134857, -0.3027809262275696, -0.009389751590788364, -0.22902920842170715, 0.1969551146030426, -0.6268090009689331, -1.2808018922805786, 0.02976708672940731, -0.674234926700592, -0.05294187739491463, -0.23397386074066162, 0.045985132455825806, -0.483054518699646, -0.7675338387489319, -0.9504547715187073, -0.7165951728820801, -1.2128889560699463, -1.0202678442001343, 0.14976687729358673, 0.4370768070220947, -0.22150152921676636, -0.4283157289028168, -0.2929929494857788, -0.7388551831245422, 1.1848876476287842, -0.7355462312698364, 0.1779869794845581, -0.43039876222610474, -0.01417158916592598, 0.008055515587329865, 0.052229851484298706, 0.5117686986923218, -0.12382103502750397, 0.023634932935237885, -0.8653795719146729, 0.34876635670661926, -0.31407028436660767, -0.5308014154434204, 0.32168614864349365, 0.5249698758125305, 0.8463335037231445, 0.06188013777136803, -0.40664294362068176, 0.7182468771934509, 1.1949094533920288, -0.23184148967266083, 0.2550332546234131, -0.4860917031764984, 0.610550045967102, -0.2678641974925995, -0.23087722063064575, 0.7219058275222778, -0.13810545206069946, 0.5641542673110962, 0.3182249367237091, -0.44680941104888916, -0.18128769099712372, -0.07720444351434708, 0.5629599690437317, 1.7223801612854004, 0.7654576301574707, 0.07161343097686768, -0.5554450154304504, 0.5944985151290894, -1.1872501373291016, -1.1202188730239868, 0.5302040576934814, 0.6121587157249451, 0.42223408818244934, 0.11666849255561829, -0.3387645184993744, 0.14596202969551086, 0.19490671157836914, 0.6373871564865112, -0.24460653960704803, -1.1242234706878662, -0.041495431214571, 0.5596985220909119, 0.4789169728755951, 0.8533635139465332, -0.16134947538375854, 0.36274829506874084, 15.039142608642578, 0.82827228307724, -0.45789316296577454, 0.2133471667766571, 0.5436404943466187, 0.09885294735431671, 0.17071211338043213, -0.2586331367492676, -1.1691147089004517, 0.39424628019332886, 1.2463215589523315, 0.08702889829874039, 0.4077078700065613, 0.41079872846603394, -0.07216282933950424, 0.29239577054977417, -0.24175536632537842, 0.8457774519920349, 0.5269535779953003, -1.5363404750823975, -0.24435031414031982, 0.03796834871172905, 0.4541495740413666, 0.6378735303878784, 0.7528514266014099, 0.6366087198257446, 0.41035816073417664, -0.2642271816730499, 0.32584047317504883, 0.4540403187274933, 0.991047203540802, 0.12213955074548721, 0.4758715033531189, 0.23322083055973053, -1.0802963972091675, -0.11239873617887497, -0.5304540395736694, -1.3533917665481567, 0.3736672103404999, 0.5167626738548279, -0.44988998770713806, -0.20367497205734253, -0.4663696587085724, 0.7738380432128906, 0.7632175087928772, 0.35263776779174805, -0.03801164776086807, 0.6947593688964844, -0.07218658924102783, 0.02120976895093918, 0.1288042962551117, 0.44692766666412354, 0.14516642689704895, 0.09754154831171036, 0.13948853313922882, -0.1582493782043457, 0.4687173366546631, 0.42744144797325134, -0.2981419265270233, -0.5772074460983276, 0.016465868800878525, -0.244375541806221, -0.06846289336681366, 0.9608911871910095, 0.7209827899932861, -0.08830617368221283, -0.639441967010498, 0.33543673157691956, 0.23286780714988708, 0.1911965012550354, -0.33027487993240356, -0.2070007175207138, 0.7678473591804504, -1.0697458982467651, 0.0943727120757103, 0.45882996916770935, -0.7066934704780579, -0.5890268087387085, -0.844822883605957, -0.36757439374923706, 0.48917141556739807, -0.9451162219047546, -0.7185770869255066, 0.8405053615570068, -0.506909966468811, -0.5350531339645386, 0.2691037356853485, -0.6093127131462097, -0.41340553760528564, 0.6678717136383057, -1.453558087348938, -0.20115117728710175, 0.04364309832453728, -0.22173984348773956, -0.13319119811058044, -0.29947835206985474, 1.0291831493377686, 0.3177584409713745, -0.2626168131828308, 0.34640657901763916, -0.17371034622192383, -0.1647631824016571, -0.34336352348327637, -0.30428534746170044, 0.7369065880775452, 0.6402416229248047, -0.28585749864578247, -0.05251777544617653, 0.035583000630140305, 0.2872655987739563, -0.6004418730735779, -0.13208645582199097, 0.30983075499534607, 0.20025360584259033, -0.25059062242507935, -1.0201623439788818, -0.8479452729225159, 0.2862915098667145, 0.7511211037635803, 0.288471519947052, 0.35826554894447327, 0.06794638931751251, -0.10697045922279358, -0.32302793860435486, -0.32024672627449036, 0.12228041142225266, 0.2845901846885681, -0.5536201596260071, -0.13952048122882843, -0.30896541476249695, 0.4540541470050812, -1.211458683013916, -0.7020274996757507, -0.029412057250738144, 0.05432618781924248, -0.12228850275278091, 1.1112807989120483, -0.1784343272447586, 0.7358718514442444, 0.88841313123703, -0.1473620980978012, -0.5303308963775635, -0.12531307339668274, -0.9639183282852173, -0.33276909589767456, 0.09247661381959915, 0.232621967792511, -0.38493531942367554, 0.8046774864196777, 0.2638060748577118, -0.37535804510116577, -0.6484994888305664, -0.5715203881263733, -0.06717264652252197, -0.5588077306747437, -0.4930224120616913, 0.00900756474584341, -0.12270399928092957, 0.6614181399345398, -0.22525428235530853, -0.0009272279567085207, 0.4438639283180237, -0.11759490519762039, -0.47107845544815063, -0.02312971092760563, 0.3381969928741455, -0.23859235644340515, -0.7515506148338318, -0.8826048374176025, -1.6459074020385742, -0.310107946395874, -1.3670620918273926, 0.12482189387083054, -0.3519188463687897, -0.5540356636047363, 0.048805128782987595, -0.577723503112793, 0.22418129444122314, 0.08567482978105545, -0.25700029730796814, -0.05641673877835274, -0.6455849409103394, -0.3779177963733673, 1.0428073406219482, 0.7276510000228882, -0.43977272510528564, 0.07886382192373276, -0.26795312762260437, -0.03611738607287407, 0.22281457483768463, 0.4878353774547577, -0.4462233781814575, -0.8785526156425476, -0.8089086413383484, 0.2632046341896057, 0.0961114764213562, -0.4509584903717041, -1.2940741777420044, 1.0089555978775024, 0.2528175413608551, -0.4172682464122772, 0.001827435800805688, 0.6487683057785034, -0.995870053768158, -0.737591564655304, 0.539413332939148, -0.9949207901954651, -0.35722264647483826, 0.32259005308151245, -0.8190106749534607, -0.40874573588371277, 0.6142878532409668, -0.007480539381504059, -0.5716206431388855, -0.9939327836036682, 0.5981700420379639, -0.523919403553009, 0.08099629729986191, -0.012975400313735008, -0.11335179954767227, -1.1440130472183228, -0.199151873588562, -0.07422672212123871, 0.2660176753997803, -0.7201250791549683, 0.876893162727356, 0.3716287314891815, -1.2349263429641724, 0.10026170313358307, 0.7165535092353821, -0.20711222290992737, -0.2610795199871063, 0.39998483657836914, 0.6493615508079529, -0.0846594050526619, 0.38406887650489807, -0.13879817724227905, 0.07281285524368286, -0.83404541015625, 0.32173091173171997, 0.7069116830825806, -0.30800071358680725, 0.02112440951168537, 1.0458922386169434, -0.5821958184242249, -0.4597274661064148, 0.3955537676811218, -1.280420184135437, -0.24078035354614258, -0.2704266905784607, 0.7093576192855835, 0.03078114427626133, -0.04408818855881691, 0.12610027194023132, -0.526052713394165, -0.12705475091934204, 0.006852482911199331, -0.18419304490089417, 0.10063185542821884, 0.08765602856874466, 0.0947561264038086, 0.3363526165485382, 0.9212285876274109, -0.6897162795066833, -0.8920782208442688, -0.9546946287155151, -0.714006245136261, 0.0153970280662179, 0.3014043867588043, 0.011213730089366436, -1.0665897130966187, 0.7242494225502014, 0.8316580653190613, 0.46240973472595215, 0.486562043428421, -0.30429956316947937, 0.24281036853790283, 0.32479262351989746, -0.26571381092071533, -0.4277471601963043, -0.42949748039245605, 1.3491415977478027, 0.9620023369789124, -0.8431703448295593, 0.3021566867828369, -0.5098652243614197, -0.4662824273109436, 0.9139254689216614, 0.1967664211988449, -0.18805354833602905, 0.9817360043525696, 0.3938209116458893, 0.10621939599514008, 0.2514060437679291, -0.9348126649856567, -0.518821120262146, 0.6298954486846924, 0.8035550117492676, 0.5151374340057373, -0.08712181448936462, 0.21593552827835083, 0.9449285864830017, 0.22652526199817657, 0.2495722770690918, 0.3325367569923401, 0.4636538326740265, -0.4325256645679474, 0.2856505811214447, -0.2286587804555893, 0.664338231086731, -0.6028044819831848, -0.8862048983573914, 0.8974736928939819, 0.10055536031723022, 0.2719537317752838, 0.5757427215576172, 1.162097454071045, -0.2531299293041229, 0.28199419379234314, -0.0545702800154686, 0.38253021240234375, -0.2640243172645569, -0.14617343246936798, -0.16054552793502808, -0.8181679844856262, -0.3144688010215759, -0.1020825207233429, -0.18232446908950806, -0.4943067729473114, -0.13549746572971344, 0.5056589245796204, -0.2314349114894867, 0.37604549527168274, 1.130977749824524, 0.8836590647697449, 1.0092676877975464, -0.14099672436714172, -0.8837986588478088, -0.34624356031417847, -0.9177617430686951, -0.1232372298836708, -0.23446209728717804, -0.03941059485077858, 0.3203918933868408, 0.21321450173854828, 0.2540242075920105]}, "authors": [{"authorId": "2115891016", "name": "Jingyu Wang"}, {"authorId": "2156145685", "name": "Lu Zhang"}, {"authorId": "2116253566", "name": "Xueqing Li"}, {"authorId": "2193535807", "name": "Huazhong Yang"}, {"authorId": "2442306", "name": "Yongpan Liu"}], "references": [{"paperId": "ecd0b23e4828fca585a05eff56563852d35858d9", "title": "ChatGPT"}, {"paperId": "303f66b6c0d6d917e35c34f8123cee55f9f7bec7", "title": "Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning"}, {"paperId": "6824ae8511c3b4b47b85bed93e121743906445e9", "title": "Enabling Energy-Efficient Inference for Self-Attention Mechanisms in Neural Networks"}, {"paperId": "2a682d3ecda1a3003d5b249867eae53c35c35e68", "title": "A 17\u201395.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "1c1c186fa5bd75953b2874af4f74750c57c8d4a7", "title": "A 28nm 27.5TOPS/W Approximate-Computing-Based Transformer Processor with Asymptotic Sparsity Speculating and Out-of-Order Computing"}, {"paperId": "1ee05cd919590eaba129caa0fda5e850c87b75a5", "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer"}, {"paperId": "c67b1a62b868a758791c88d5465c7b6d53510fc3", "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "b080ba53a471348e7e76234decdf14e730fea7db", "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "b6451cfb71be72f8f9e0f5d2f529fea231adb382", "title": "Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "a581806d34b3bd6a9fe390da2393c1ad15456a4e", "title": "SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a", "title": "Automatic differentiation in PyTorch"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Xi\u2019an, China"}, {"paperId": "830ccb44084d9d6cdcb70d623df5012ae4835142", "title": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters"}, {"paperId": null, "title": "An attention fusion sparse nonlinear architecture"}, {"paperId": null, "title": "respectively"}, {"paperId": null, "title": "include data converters, emerging memory-oriented computing and architecture"}, {"paperId": null, "title": "National Science and Technology Major Project, 863 Program, and Ninth Five-Year National Program"}]}