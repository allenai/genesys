{"paperId": "e2f8864c3e40298513ca320de0012818ce092bea", "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "abstract": "Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": null}, "embedding": {"model": "specter_v2", "vector": [-0.307828813791275, 0.8691349625587463, -0.40233737230300903, -0.21866685152053833, -0.4271206259727478, -0.8868123292922974, 1.0119975805282593, 0.0017104040598496795, -0.09847606718540192, -0.1914445161819458, 0.5389235019683838, -0.6215198040008545, 0.23645566403865814, -0.03808888792991638, -0.25752052664756775, -0.020593835040926933, -0.2796727120876312, 0.4573492407798767, -0.1237824559211731, -0.21610401570796967, -0.40362805128097534, -0.6081896424293518, -1.122574806213379, -0.3005584180355072, 0.6751506924629211, 0.14245782792568207, 0.2618338167667389, 1.018681287765503, -0.6051297187805176, 0.0284640584141016, 0.6254257559776306, -0.5100786089897156, 0.25840216875076294, -0.2839840054512024, -0.3852427005767822, -0.17107412219047546, 0.19595938920974731, -0.6787595152854919, -0.6498274803161621, 0.24292102456092834, 0.06030077114701271, 0.20411621034145355, 0.8813408613204956, -0.49001550674438477, -0.5741715431213379, 0.7152019143104553, 0.8903850317001343, 0.9635468125343323, -0.1381751149892807, -0.33496302366256714, 0.9853572845458984, -1.2924786806106567, 0.0718880370259285, 1.3416897058486938, 0.10533604770898819, 0.41904783248901367, -0.15688665211200714, -0.39645594358444214, 0.7010262608528137, -0.009936507791280746, -0.5513462424278259, 0.06387007236480713, -0.11627961695194244, -0.34763357043266296, 1.3063418865203857, -0.34642934799194336, 0.23029400408267975, 0.7135159969329834, -0.6147043704986572, 1.836616039276123, 0.2778145968914032, -0.9043397307395935, -0.3651210069656372, 0.48803675174713135, 0.7023804783821106, 1.1846880912780762, -0.2573181986808777, 0.8375552296638489, -0.7273703217506409, -0.17740844190120697, 0.8830907940864563, -0.3176894187927246, -0.03008144162595272, -0.06773233413696289, 0.11496174335479736, 0.9803622961044312, 0.012260016985237598, 0.5844619870185852, -0.021273557096719742, 0.34343671798706055, 0.25045454502105713, 0.17141224443912506, -0.40339601039886475, 0.2840958535671234, -0.32671502232551575, -0.13517062366008759, -0.6422098278999329, 0.5212380290031433, 0.28203168511390686, 0.9263496994972229, -0.11847212165594101, 0.3340045213699341, -1.1966294050216675, -0.03121679648756981, 1.3691946268081665, 0.09659454971551895, 0.5485608577728271, -0.8620789051055908, 0.1504029631614685, -0.6148207783699036, -0.050679244101047516, -0.5863601565361023, 0.09826836735010147, -0.5023779273033142, -0.5485891699790955, -1.6356000900268555, -0.34774768352508545, -0.13371048867702484, -0.9372187852859497, 0.7580008506774902, 0.04402017965912819, 0.5554197430610657, 0.025179360061883926, 0.5629749894142151, 0.7478042840957642, 0.7186568975448608, 0.45747536420822144, 0.04536972567439079, 0.7493346333503723, -0.5168618559837341, -0.5193886160850525, -0.9378503561019897, 0.7496374249458313, 0.016374658793210983, 0.32189875841140747, -0.19706912338733673, -0.9780797958374023, -1.1192315816879272, -0.6357459425926208, -0.17443200945854187, -0.46026328206062317, 0.4371545910835266, 1.3112350702285767, 0.8710311651229858, -0.7372975945472717, 0.3856976628303528, -0.24879488348960876, 0.1465192437171936, 0.22106710076332092, -0.19036422669887543, 0.6603560447692871, -0.6552537679672241, -1.2823495864868164, -0.17948243021965027, 0.38361501693725586, -1.0744911432266235, -0.6170322895050049, -0.9562941789627075, -0.8761370182037354, 0.228311687707901, 0.47757017612457275, -0.7501497864723206, 1.7366098165512085, 0.3159593641757965, -1.2613911628723145, 0.6423337459564209, -0.23082274198532104, 0.34953418374061584, 0.4019416570663452, -0.31842294335365295, -0.12293510884046555, -0.5507477521896362, -0.01672426238656044, 0.5434873700141907, 0.3132750391960144, -0.4184080958366394, -0.06597522646188736, 0.39517614245414734, -0.21933583915233612, -0.0316011905670166, -0.2200453281402588, 0.41291821002960205, -0.4496821463108063, -0.4068256616592407, 0.21775151789188385, 0.705797553062439, 0.13751336932182312, -0.27747493982315063, -0.47802093625068665, -1.3729172945022583, 0.8203989863395691, -0.08067013323307037, 1.121457815170288, -0.8364744186401367, -1.0190975666046143, 0.040268223732709885, 0.060311347246170044, 0.47374260425567627, -0.672001302242279, 0.4209898114204407, -0.30085495114326477, 0.3142521381378174, 0.005232914350926876, -0.8788173198699951, 0.4578079879283905, 0.23521673679351807, -0.7570172548294067, -0.8266876935958862, 0.2160765528678894, 1.1224815845489502, -0.8718535304069519, 0.04254743456840515, 0.06998994201421738, 0.392055481672287, -1.244812250137329, 1.7125098705291748, -0.5008846521377563, 0.5148765444755554, -0.4874016046524048, -0.36790212988853455, -0.03173433244228363, -0.31520378589630127, 0.1884089857339859, 0.17181424796581268, 0.2723265290260315, 0.7652546763420105, -0.8917378783226013, 1.2860450744628906, -0.15500806272029877, 0.10544697195291519, -0.27511048316955566, -0.9462877511978149, 0.6954249739646912, 0.4175785779953003, -0.5814127326011658, -0.32655665278434753, -0.03403255343437195, 0.672933042049408, -0.6606283783912659, -0.03193004056811333, 0.551651656627655, 0.5024521946907043, -0.20641568303108215, 0.530473530292511, 0.6983402967453003, -0.5215615034103394, 0.35391005873680115, 0.7692333459854126, 0.5502055287361145, 0.3648824393749237, 0.2847402095794678, 0.03546098619699478, -0.0022109767887741327, -0.9678838849067688, -0.18669889867305756, 0.6465035676956177, 0.9128316044807434, 0.6148934364318848, 0.2691080868244171, -0.6662556529045105, 0.03780403360724449, -0.18837690353393555, 0.6250760555267334, 1.596332311630249, -0.3672167658805847, -0.2203899621963501, -0.7909726500511169, -0.5850763916969299, -0.10133498907089233, 0.6671636700630188, -0.36077919602394104, -0.6256689429283142, -0.6310060024261475, -0.9337260723114014, 0.6438600420951843, 0.7001798748970032, 1.083969235420227, 0.06415475904941559, -0.30437082052230835, -0.17025965452194214, -0.006272102706134319, -0.3488911986351013, -0.5226545333862305, 0.7177630066871643, -0.44533771276474, -0.11112619191408157, -0.014732211828231812, -0.09623949974775314, -0.011813716031610966, -0.6167935729026794, 1.45648193359375, -0.13938605785369873, -0.47838929295539856, 0.4402109980583191, 0.5913658738136292, -0.6508134603500366, -0.7138505578041077, 0.29970216751098633, 0.24211463332176208, -0.6364233493804932, 0.5263972878456116, 0.5661165714263916, 0.32944607734680176, -0.12310583144426346, -0.364125519990921, 0.22854679822921753, -0.2369977980852127, 0.13970088958740234, 0.706868588924408, -0.35997167229652405, 0.12357338517904282, -1.1313552856445312, 0.812791645526886, -0.0736369863152504, -0.4163336157798767, 0.1424332857131958, -0.8765467405319214, -0.18790721893310547, 0.37828654050827026, -0.398404985666275, -0.44724956154823303, -1.038726806640625, 0.07414465397596359, -0.029813449829816818, -0.6325206756591797, 0.41662147641181946, 0.31560665369033813, 0.31525564193725586, 0.3165851831436157, 0.47386306524276733, 0.10504183918237686, -0.33282381296157837, 0.8697478771209717, -0.932054340839386, 0.6559883952140808, -0.447365939617157, 0.3242565989494324, -0.9025743007659912, -0.290759801864624, -0.37865081429481506, -0.8541673421859741, -0.1964290738105774, -0.39656925201416016, -0.11600926518440247, 0.0073499660938978195, -0.6616948843002319, -0.7993324398994446, -0.13683167099952698, -0.8717892169952393, -0.4033147394657135, -0.284117192029953, -0.3461960256099701, -0.2202456146478653, -1.2684855461120605, -1.0576906204223633, -0.5420827865600586, 0.23167400062084198, -0.9190107583999634, 0.3774057626724243, 0.4429168701171875, -0.25194427371025085, -1.10909104347229, 0.29408368468284607, -0.21361549198627472, 0.9040729999542236, -0.8217189908027649, 1.2559690475463867, -0.25996604561805725, -0.6743200421333313, -0.031384147703647614, 0.4141521751880646, 0.30524763464927673, -0.20353904366493225, 0.16732892394065857, -0.9271947741508484, 0.3084698021411896, -0.5906636118888855, -0.7935853600502014, 0.06656675785779953, 0.18710064888000488, 0.6933443546295166, -0.25267499685287476, -0.6184989809989929, 0.4840013384819031, 1.224410057067871, -0.6283156871795654, -0.12049129605293274, 0.07433439046144485, 1.204466700553894, 0.1256066858768463, -0.19681039452552795, 0.41357046365737915, 0.7905176877975464, 0.46845105290412903, -0.09247265011072159, 0.03412647917866707, -0.07016617059707642, -0.9363846182823181, 0.3143386244773865, 1.4092774391174316, 0.2954193949699402, 0.08086509257555008, -1.2591108083724976, 0.5280303359031677, -0.9390445351600647, -0.16573882102966309, 0.33980122208595276, 0.5983436703681946, 0.8098130226135254, -0.8326600193977356, -0.3109802007675171, -0.1078278049826622, -0.009830387309193611, 0.016702832654118538, -0.20683784782886505, -0.5433469414710999, 0.3826476037502289, 0.5627519488334656, -0.3656998872756958, 0.8316099047660828, -0.15061049163341522, 0.4028257727622986, 14.629385948181152, 1.0140711069107056, 0.11605403572320938, 0.6215693354606628, 0.9033250212669373, 0.6247808933258057, -0.7882235050201416, -0.3707677125930786, -1.6738306283950806, -0.3922123312950134, 1.165892243385315, 0.21640107035636902, 0.27613717317581177, 0.1838335543870926, -0.11616174876689911, 0.01994534209370613, -0.33113497495651245, 0.6080904006958008, 0.3406696319580078, -1.2087080478668213, 0.5498762726783752, 0.16025996208190918, 0.582607090473175, -0.29434967041015625, 1.051706314086914, 1.201454997062683, 0.7058167457580566, -0.448496013879776, 0.5134194493293762, 0.40847742557525635, 0.44946780800819397, -0.3267940878868103, 0.09225157648324966, 0.9031020402908325, -0.4968240261077881, -0.02577005699276924, -0.31980639696121216, -0.9866064190864563, 0.134266197681427, 0.20029273629188538, -0.7828620672225952, -0.47350555658340454, -0.49023646116256714, 0.5150434374809265, -0.18457838892936707, 0.14142440259456635, -0.4265996813774109, 1.3295375108718872, 0.02504701353609562, 0.21071143448352814, 0.39424315094947815, 0.41888633370399475, 0.03996139392256737, 0.05409504100680351, 0.3815954327583313, 0.6037684082984924, 0.5398650169372559, 0.7679396867752075, -0.21589834988117218, -0.29513606429100037, -0.45478251576423645, -0.2907354533672333, 0.0519743412733078, 0.6325781941413879, 0.5468346476554871, 0.09504946321249008, -0.20366451144218445, 0.12475727498531342, 0.6239928007125854, 0.17025218904018402, 0.04645722359418869, 0.40338242053985596, 0.20349353551864624, -0.24047036468982697, -0.13051806390285492, 0.2959817349910736, -0.5319021940231323, -0.22765929996967316, -1.098309874534607, -0.46307387948036194, 0.42063695192337036, -0.8469586372375488, -0.7428351640701294, 0.3505447804927826, -0.020730482414364815, -0.22985343635082245, -0.5045471787452698, -0.6242371201515198, -0.2868441343307495, 0.7902555465698242, -1.253252387046814, -0.5275415182113647, 0.24281202256679535, -0.31724175810813904, -0.5222875475883484, -0.17814527451992035, 1.5902494192123413, -0.06888933479785919, -0.34023043513298035, 0.17296122014522552, 0.10291227698326111, -0.22001242637634277, -0.2590424716472626, -1.243766188621521, 0.9078638553619385, 0.19472487270832062, 0.2901974022388458, 0.4531850814819336, 0.09841913729906082, 0.003142673522233963, -0.7761895656585693, -0.2122703492641449, 0.8168997764587402, -1.2748185396194458, -0.9228721857070923, -0.6564660668373108, -1.104911208152771, 0.4458741247653961, 0.5642269253730774, -0.3614341914653778, 0.8538669943809509, 0.21073292195796967, -0.736051082611084, 0.3466838300228119, -0.599739670753479, -0.20659346878528595, 0.18751151859760284, -0.5276685357093811, -0.5275676250457764, -0.0822485014796257, 0.3788823187351227, -1.3571709394454956, -0.3576733469963074, -0.6609355807304382, -0.3841695785522461, -0.11790381371974945, 0.9990586042404175, -0.3628413677215576, 0.5661230087280273, 1.1393488645553589, 0.15135450661182404, -1.251685380935669, -0.40627264976501465, -0.9163755178451538, -0.1575499325990677, 0.6048024892807007, 0.6118213534355164, -0.15522010624408722, 0.10503848642110825, 1.2637172937393188, 0.009813038632273674, -0.4524616003036499, -0.10313931852579117, -0.2936027944087982, 0.48507973551750183, -0.4685879647731781, 0.48603522777557373, 0.4774651527404785, -0.47314244508743286, 0.31638410687446594, 0.7701871991157532, 0.4673381447792053, -0.40598756074905396, -0.6828685998916626, 0.5088139772415161, -0.4141053557395935, -0.3785870373249054, -0.2808120846748352, 0.007334824185818434, -1.71721613407135, 0.012833029963076115, -1.3610903024673462, -0.03224385157227516, -0.9502129554748535, -0.14882512390613556, -0.4053530693054199, 0.13236293196678162, 0.21765954792499542, -0.28874480724334717, -0.7271589040756226, -0.7170158624649048, -0.8942714929580688, -0.11338598281145096, 0.8525708913803101, 1.0638195276260376, -0.05263402312994003, 0.18273282051086426, -0.14326365292072296, -0.5835038423538208, 0.011236477643251419, 0.2064741551876068, -0.38432392477989197, -0.9899587035179138, -1.2898752689361572, 0.012188355438411236, -0.006213989108800888, 0.10991892963647842, -0.19928520917892456, 0.5173279047012329, 0.25015753507614136, -0.08573480695486069, 0.10542482882738113, 0.11947350949048996, -0.7441795468330383, -0.6349579095840454, 0.14953118562698364, -0.7520241737365723, -0.14378343522548676, 0.319541335105896, -0.4476146996021271, -0.0941835418343544, 0.32948893308639526, -0.13041798770427704, -1.1222331523895264, -0.6017657518386841, 0.5020756125450134, -0.6276928782463074, 0.46715959906578064, -0.4141392409801483, 0.10049759596586227, -1.1645796298980713, -0.637601375579834, 0.15180176496505737, 0.2214483618736267, -0.5929931402206421, 0.8629616498947144, 0.16115957498550415, -1.467319369316101, 0.12805381417274475, 0.63385009765625, -0.21173705160617828, 0.3262713849544525, 0.4488735496997833, 0.1263519525527954, -0.021882222965359688, 0.8805394768714905, 0.3455366790294647, 0.23120996356010437, -0.55140221118927, -0.16168130934238434, 0.5850439071655273, -0.7010771036148071, -0.16646794974803925, 1.4210286140441895, -0.373412162065506, -1.4420433044433594, 0.21012309193611145, -1.1662436723709106, -0.51765376329422, -0.3571546673774719, 0.6137282252311707, 0.05351830646395683, -0.13863466680049896, 0.18640221655368805, -0.01068062987178564, 0.1489836573600769, -0.08824267238378525, -0.5089683532714844, 0.4320349097251892, -0.2047426700592041, 0.1516171097755432, 0.7200148701667786, 0.7582469582557678, -0.8960566520690918, -0.8932490348815918, -0.46687185764312744, -0.25325262546539307, -0.3829244375228882, 0.3925202786922455, -0.8981043696403503, -0.4573982357978821, 0.9267504811286926, 0.47910311818122864, 0.25861653685569763, 0.08790040016174316, -0.11137174814939499, -0.012535279616713524, 0.7530908584594727, 0.2205774039030075, -0.3801838755607605, -0.2967176139354706, 1.5340346097946167, 0.875837504863739, -0.9780278205871582, -0.2698153555393219, -0.42905136942863464, -0.8348589539527893, 0.7182220220565796, 0.6320444941520691, 0.33312273025512695, 0.8010355830192566, -0.3462061882019043, 0.3096616864204407, -0.09348541498184204, -0.9925346374511719, -0.1855124533176422, 0.8942040205001831, 1.2999777793884277, 1.1425204277038574, 0.5939213037490845, 0.34173110127449036, 1.3012375831604004, -0.2369956523180008, 0.22927965223789215, 0.5358039736747742, 0.5357201099395752, -0.47013309597969055, -0.3842739164829254, -0.013072526082396507, 0.8522332906723022, -0.2681208848953247, -0.5995123982429504, 0.03581712022423744, 0.5249375104904175, 0.2555445432662964, 0.43700554966926575, 0.39189884066581726, 0.3709580898284912, 0.6259115934371948, 0.8146942257881165, 0.6810271143913269, -0.761447548866272, -0.261454313993454, -0.2689988613128662, -0.7406477928161621, -0.10831032693386078, -0.10602978616952896, -0.5245645046234131, -0.4591616094112396, -0.015185411088168621, 0.6104736328125, 0.1928257942199707, 0.0674915760755539, 1.2394766807556152, 0.5458417534828186, 0.5843164324760437, -0.32218489050865173, 0.08956733345985413, -0.5226834416389465, -0.7116096615791321, -0.20375624299049377, -0.8864209651947021, -0.24235393106937408, -0.5521780848503113, -0.07755910605192184, 0.052574895322322845]}, "authors": [{"authorId": "2135792196", "name": "Gyeongman Kim"}, {"authorId": "2284765585", "name": "Doohyuk Jang"}, {"authorId": "2284765685", "name": "Eunho Yang"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "0b641cc51d8e25c6c0b2362317fec7e0bf26fbf1", "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications"}, {"paperId": "ad0ab3406b8fd1cf85c09aca35468d999bb0cf36", "title": "Tailoring Instructions to Student\u2019s Learning Levels Boosts Knowledge Distillation"}, {"paperId": "aad167be3c902388ea625da4117fcae4325b8b7d", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"paperId": "91b95b98cc1a7e974e62d0b8295d3b974b94aa0e", "title": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44", "title": "Large Language Models Are Reasoning Teachers"}, {"paperId": "6f4cc536f9ed83d0dbf7e919dc609be12aa0848a", "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"}, {"paperId": "2bf7978afacd83e5fc813741c11a0c0421cb2dfd", "title": "Sparse Teachers Can Be Dense with Knowledge"}, {"paperId": "33be243ac9dd8723e6267dea45fd6a6172d4f6a5", "title": "Less is More: Task-aware Layer-wise Distillation for Language Model Compression"}, {"paperId": "0ade637337402b3a2270e75fd3869392937212dd", "title": "MetaPrompting: Learning to Learn Better Prompts"}, {"paperId": "fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a", "title": "Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "0e3d6a7c9c04cf3ba9c902724548846a5ade04b4", "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "e79957010ec7c5c221888e368624289cf301146e", "title": "Distilling Linguistic Context for Language Model Compression"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "a5266f10b50d7bce78af2f49320bdd7aae158ef6", "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"}, {"paperId": "af9d0a18123c3bd8d8d2b1c44b03fdea5e9c3ae3", "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e", "title": "WARP: Word-level Adversarial ReProgramming"}, {"paperId": "ab31e33b3620988368008ea1cf20dadc0a614983", "title": "Characterising Bias in Compressed Models"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "20db5ac6e88e2457c82856354a2e5d521482f360", "title": "LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "a75649771901a4881b44c0ceafa469fcc6e6f968", "title": "How Can We Know What Language Models Know?"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "8de7f044a673d1f5e3b454d0663811f91aa9811a", "title": "On the Efficacy of Knowledge Distillation"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "31f531484e963da69c21f0d72ae0d59142d9e7db", "title": "Bridging the Gap between Training and Inference for Neural Machine Translation"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "ffdcad14d2f6a12f607b59f88da4a939f4821691", "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": null, "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "AutoPrompt: Elic-iting Knowledge from Language Models with Auto-matically Generated Prompts"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2024. MiniLLM: Knowledge distillation of large language models"}, {"paperId": null, "title": "and OPT model families"}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Below is an instruction that describes a task. Write a response that appropriately completes the request"}, {"paperId": null, "title": "phases both have a maximum sequence length of 512 and a maximum prompt length of 256"}, {"paperId": null, "title": "B Qualitative Results For the qualitative results, we present samples generated by student models trained using various methods. The samples are drawn from the S-NI"}, {"paperId": null, "title": "with GPT-2 Large employed as the student model Results are shown in"}]}