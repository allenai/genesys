{"paperId": "9d63a12148ab4f61be0a7218902238f00c1009b7", "title": "Automating Continual Learning", "abstract": "General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF) -- previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives. Our experiments demonstrate that ACL effectively solves\"in-context catastrophic forgetting\"; our ACL-learned algorithms outperform hand-crafted ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification datasets.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms, and encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives."}, "embedding": {"model": "specter_v2", "vector": [0.15024161338806152, 0.4607716500759125, 0.04564487934112549, 0.14954166114330292, 0.17757058143615723, 0.38494718074798584, 0.5889568328857422, -0.10225024074316025, -0.7117965221405029, 0.3339892327785492, -0.05652136728167534, -0.06719127297401428, 0.21718169748783112, -0.02841942571103573, -0.5922529697418213, 0.13331855833530426, -1.198917269706726, -0.32659050822257996, 0.6130258440971375, -0.5952276587486267, 0.0027787683065980673, -0.6120668649673462, -1.2632962465286255, 0.14437603950500488, 0.0883624330163002, 0.5100740194320679, 0.20038500428199768, 0.9581025242805481, -0.226203054189682, 0.8786100149154663, 0.5465502142906189, -0.4295118749141693, 0.3490787446498871, 0.024673637002706528, -0.6265249848365784, 0.11133301258087158, 0.32176557183265686, -0.268085241317749, -0.805354118347168, 0.4124881625175476, -0.5060266852378845, 0.47112926840782166, 0.17217972874641418, -0.6449397206306458, -0.36031660437583923, 0.4603438079357147, 0.5437576174736023, 0.9593862295150757, -0.39409151673316956, -0.12337064743041992, 0.9247902631759644, -1.3224656581878662, 0.2593236565589905, 1.2787266969680786, 1.0493708848953247, 0.854019045829773, -0.4369010031223297, -0.8326341509819031, 1.0789265632629395, 0.19021417200565338, -0.7128477096557617, -0.24280546605587006, -0.07853349298238754, -0.13854071497917175, 1.5476356744766235, -1.038039207458496, 0.2232770472764969, 0.7608823776245117, 0.6427104473114014, 1.5488923788070679, -0.340394526720047, -0.5598264336585999, -0.15537819266319275, 0.227207213640213, 0.4184342622756958, 0.5647430419921875, -0.2345440536737442, 0.4307848811149597, -1.0524400472640991, 0.2966555058956146, 0.6807874441146851, 0.46088242530822754, -0.012398477643728256, -0.3576321303844452, -0.24702318012714386, 0.38092300295829773, 0.46052828431129456, 0.8709148168563843, -0.671891450881958, 0.9246608018875122, 0.5904858112335205, 1.0036176443099976, -0.287063866853714, 0.5240129232406616, 0.2293483316898346, 0.1835867464542389, -0.5007340312004089, 0.020548071712255478, -0.26622411608695984, 0.9950417876243591, 0.2988586723804474, 0.308689683675766, -0.04123454913496971, 0.4099275767803192, 0.6892812252044678, -0.7504431009292603, 1.0138105154037476, -0.6875925064086914, -0.0036692507565021515, -0.3946157991886139, 0.22870470583438873, -0.567280650138855, -0.3774549961090088, -0.6479200124740601, -0.750562310218811, -0.5929551720619202, -0.3872409462928772, 0.3761619031429291, -0.3375028073787689, 0.795188844203949, -0.32994344830513, 0.004601054359227419, 0.15559987723827362, 0.22408431768417358, -0.022721407935023308, 0.2506861686706543, 0.6168142557144165, -0.21193549036979675, 0.2006070613861084, -0.8897235989570618, -0.6544117331504822, -0.990585446357727, 0.35985836386680603, 0.31483107805252075, 0.469328373670578, 0.056450702250003815, -1.17606520652771, -1.3555634021759033, -1.3494200706481934, 0.2000160664319992, -0.46188294887542725, -0.3131679892539978, 1.4012202024459839, 0.30327412486076355, -0.6239962577819824, 1.2738542556762695, -0.11149293184280396, -0.20419719815254211, 0.3492352068424225, 0.10418010503053665, 0.3892451524734497, -0.3802531361579895, -0.9477089643478394, 0.18268245458602905, 0.9131289720535278, -0.48823410272598267, -0.9755200743675232, -0.6162100434303284, -0.5854038000106812, -0.4838551878929138, 0.6292242407798767, -0.6471335291862488, 1.5750608444213867, -0.7836835384368896, -0.559585452079773, 0.3584720194339752, 0.1471949815750122, 0.009366719052195549, 0.6523433923721313, -0.29331377148628235, -0.9528160095214844, -0.32886090874671936, -0.49194687604904175, 0.7570282220840454, 0.5096920728683472, -0.6206797957420349, -0.6169215440750122, 0.13348373770713806, -0.009254910983145237, -0.27029168605804443, -0.5728621482849121, -0.28550928831100464, -0.2124411165714264, -0.34142571687698364, 0.43436408042907715, 0.9521449208259583, 0.23903411626815796, -0.21313367784023285, 0.24468359351158142, -1.3173511028289795, 0.9154311418533325, 0.30936408042907715, 0.8746315836906433, -1.279032588005066, -1.1808222532272339, -0.3470373749732971, 0.04337932541966438, -0.394042432308197, -1.253412127494812, 0.6642940044403076, -0.3530227839946747, 0.1766311079263687, 0.015063721686601639, -0.9139217734336853, -0.3480837941169739, -0.07357177138328552, -0.5156776309013367, -0.27671000361442566, 0.15813013911247253, 1.0194495916366577, -1.4798316955566406, 0.35968196392059326, -0.15386030077934265, -0.02068876102566719, -0.6282976865768433, 1.020269513130188, -0.6161010265350342, -0.22270600497722626, 0.2445148080587387, -0.18277789652347565, -0.06207937002182007, -0.1586548238992691, 0.34987229108810425, -0.47046056389808655, 0.08527936786413193, 0.2901369035243988, -0.475895494222641, 1.6017192602157593, -0.694820761680603, 0.7210057973861694, -0.31256404519081116, -0.35745546221733093, -0.038084812462329865, 0.5863826870918274, -0.089472196996212, -0.5626466274261475, 0.3558308780193329, 0.003959322813898325, -0.59089195728302, 0.27817514538764954, 1.0040277242660522, 0.41494107246398926, -0.07543658465147018, 0.3952842056751251, 1.105810523033142, -0.06191616132855415, 0.3756963014602661, 0.5635218620300293, 0.6521002054214478, 0.42382922768592834, 0.3896791636943817, -0.07138889282941818, 0.23363669216632843, -0.9045236706733704, 0.0640716403722763, 1.0260149240493774, 0.39631810784339905, 0.709125816822052, 0.19217471778392792, -1.0217396020889282, -0.5804898142814636, -0.2198324352502823, 0.6139687299728394, 1.7534376382827759, -0.012938289903104305, 0.30546998977661133, -0.4063108265399933, -0.5589441657066345, -0.49413636326789856, 0.8471965789794922, -0.9022634029388428, -0.7818838357925415, -0.20785017311573029, -0.8014655113220215, 0.60813969373703, 0.42757847905158997, 1.4254390001296997, -0.9911250472068787, -0.30262491106987, 0.32230865955352783, 0.667336642742157, -0.2517761290073395, -0.7024524807929993, 0.3216947913169861, -0.8605300784111023, -0.47365203499794006, 0.19345015287399292, -0.24229182302951813, -0.11628862470388412, 0.044039011001586914, 0.9701597094535828, -0.12099422514438629, -0.22240929305553436, 0.8281990885734558, 0.941457986831665, -0.8068884611129761, -0.7730291485786438, 0.2745210826396942, 0.41190916299819946, 0.1810840666294098, 0.09239087998867035, 0.14407770335674286, -0.27231565117836, 0.33456382155418396, -0.7766735553741455, 0.03368714079260826, 0.29288193583488464, 0.32583698630332947, 0.6683551669120789, 0.00584971671923995, 1.2606109380722046, -1.5102201700210571, 1.3201782703399658, -0.015478084795176983, -0.03542749211192131, 0.5153699517250061, -1.0263538360595703, -0.5399025678634644, 0.34578701853752136, -1.3314499855041504, -0.6614413857460022, -1.1732691526412964, 0.4576888084411621, -0.46884825825691223, -0.26074522733688354, 0.12914684414863586, 0.5429844856262207, -0.10608812421560287, 0.7264519929885864, 0.7508240938186646, 0.3708985149860382, 0.3614289164543152, 0.26719221472740173, -0.9967687726020813, 0.8780283331871033, -0.02674570120871067, -0.035752732306718826, -0.22016404569149017, -0.2523168921470642, -0.5393164753913879, -0.961841344833374, -0.4395633935928345, -0.23652218282222748, -0.9292460680007935, -0.5540711283683777, -0.6166539788246155, -1.1338403224945068, 0.11663630604743958, -0.6946038603782654, -0.677510142326355, -0.4335889518260956, -0.2825828492641449, -0.5503792762756348, -1.0568046569824219, -0.9014837145805359, -0.8716403245925903, -0.47415870428085327, -0.3345571458339691, -0.5626182556152344, 0.31620660424232483, -0.25958818197250366, -0.8957000970840454, -0.2605700194835663, -0.5888420343399048, 1.1532096862792969, -0.8681777119636536, 0.35531479120254517, 0.40107738971710205, -0.09740541130304337, -0.30351975560188293, 0.151115283370018, 0.8211933374404907, -0.051614921540021896, -0.5565834045410156, -1.1430237293243408, -0.16857294738292694, 0.13875606656074524, -0.8894373178482056, 0.31002798676490784, -0.39385074377059937, 0.8830226063728333, -0.030276108533143997, -0.13228745758533478, -0.19313909113407135, 1.487835168838501, -0.30901771783828735, 0.200252503156662, 0.47728264331817627, 0.4755333662033081, -0.05911313742399216, -0.07389385998249054, 0.37887176871299744, 0.06408016383647919, -0.3368378281593323, 0.39077872037887573, 0.43581289052963257, -0.41076141595840454, -0.9670429825782776, 0.12666207551956177, 0.8832308650016785, 0.1391143649816513, 0.5294704437255859, -0.8228216767311096, 0.7740685939788818, -1.4572713375091553, -0.6745107769966125, 1.0860753059387207, 1.3013945817947388, 0.5922834873199463, -0.12654082477092743, -0.195854052901268, -0.2617309093475342, 0.31560054421424866, -0.17675265669822693, -0.8884497284889221, 0.0570870041847229, -0.03999662771821022, -0.20165365934371948, 0.015522465109825134, 0.3339160978794098, -0.2845280170440674, 0.5453202128410339, 14.406413078308105, 0.7459341287612915, 0.18198926746845245, 1.053123116493225, 0.47739651799201965, 0.19435107707977295, -0.3239024579524994, -0.2951947748661041, -0.9819965958595276, -0.732450008392334, 1.0589280128479004, 0.296108603477478, 0.896245002746582, 0.23688766360282898, -0.5609292984008789, -0.21168623864650726, -0.47185418009757996, 0.6004489064216614, 0.3960932195186615, -1.3505949974060059, 0.5932429432868958, -0.25413617491722107, 0.49312418699264526, 0.45087188482284546, 0.8652083277702332, 1.4421786069869995, 0.17684213817119598, 0.27905142307281494, 0.8130979537963867, 0.6384926438331604, 0.8493884801864624, -0.5071114301681519, 0.27693819999694824, 0.7160863280296326, -0.48086440563201904, -0.5735644102096558, -0.879389226436615, -0.5545234680175781, -0.02102218195796013, -0.1840493083000183, -0.5383937954902649, -0.6503814458847046, -0.24164585769176483, 0.5297825932502747, -0.2785831391811371, 0.3537793457508087, -0.04487833008170128, 0.36597707867622375, 0.25343745946884155, 0.21262818574905396, 0.11446942389011383, 0.782201886177063, 0.21592086553573608, -0.24899409711360931, -0.3358190953731537, 0.09875863790512085, -0.3608188331127167, 0.48716023564338684, -0.6554016470909119, -0.10605471581220627, -0.32770562171936035, 0.0730598121881485, -0.18831951916217804, 0.5688971281051636, 0.7170636653900146, 0.32233288884162903, -0.2618197798728943, 0.16011162102222443, 0.6475932598114014, 0.29904747009277344, 0.05028178170323372, -0.1441146582365036, -0.029700525104999542, -0.203894704580307, -0.47202223539352417, 0.23555545508861542, -0.13609619438648224, -0.6479570865631104, -0.8800809979438782, -0.13478025794029236, 0.2853924036026001, -1.107783317565918, -1.093213677406311, 0.9765112996101379, -0.5835562944412231, -0.3566916584968567, 0.4688807725906372, -0.4791935682296753, -0.44256365299224854, -0.21352429687976837, -1.1231701374053955, -0.47552743554115295, -0.1694193184375763, 0.2512464225292206, -0.07217507809400558, -0.6014485955238342, 0.7802088856697083, -0.009914056397974491, -0.3903144598007202, 0.28221577405929565, -0.1207612082362175, -0.5019910931587219, -0.010702879168093204, -0.5145779252052307, 0.27817025780677795, -0.15381625294685364, -0.3734856843948364, 0.4370943009853363, 0.011861280538141727, 0.13116581737995148, -0.7813504934310913, -0.4032498002052307, 0.24303825199604034, -0.5977491140365601, -0.2773691415786743, -0.4260369837284088, -1.4603183269500732, 0.6227308511734009, 0.535825252532959, -0.2431264966726303, 0.32020601630210876, 0.5944482684135437, -0.6765434741973877, -0.5068166255950928, -0.8623965382575989, 0.4180477559566498, 0.5736675262451172, -0.6068997383117676, -0.8912563323974609, -0.1744316667318344, 0.42197492718696594, -0.541779637336731, -0.43292903900146484, -0.5798752903938293, 0.1102631539106369, -0.4856877326965332, 0.9795771241188049, -0.5221630930900574, 0.23077091574668884, 0.7893437743186951, 0.11037483811378479, -0.6043900847434998, -0.19365474581718445, -0.6929355263710022, 0.19438007473945618, 0.33192941546440125, 0.30528920888900757, -0.8696960806846619, 0.00864716898649931, 0.6272345185279846, 0.16404153406620026, -0.1893959790468216, -0.7589254975318909, -0.4009914994239807, 0.026789920404553413, -1.0331692695617676, 0.2646147906780243, -0.012738680467009544, -0.23423685133457184, 0.3553661108016968, 0.5098928213119507, 0.3071998655796051, -0.01604960858821869, -0.8679805994033813, 0.25974470376968384, -0.0016967039555311203, 0.11767132580280304, -0.5457253456115723, -0.34908533096313477, -1.2399181127548218, -0.3894713819026947, -1.0502123832702637, -0.2156868726015091, -0.34047406911849976, -0.5168737173080444, 0.028952764347195625, -0.70469731092453, 0.10599066317081451, 0.4920275807380676, -0.4991259276866913, -0.25923746824264526, -0.2600747346878052, -1.146725058555603, 1.1202343702316284, 1.0040860176086426, -0.78507000207901, -0.3392837345600128, 0.184686079621315, -0.05883725732564926, 0.216732457280159, 0.7030088901519775, -0.472231388092041, -1.0388245582580566, -0.752053439617157, 0.5207154154777527, -0.06948202103376389, 0.1280941516160965, -1.700336217880249, 1.154527187347412, 0.22487935423851013, 0.22654873132705688, -0.22453811764717102, 0.26013922691345215, -1.2234811782836914, -0.40201500058174133, 0.23176397383213043, -0.7814487218856812, 0.2694106101989746, 0.7554788589477539, 0.09215971827507019, -0.30472037196159363, 0.3532080054283142, 0.20386455953121185, -1.2117793560028076, -1.0274579524993896, 0.43850088119506836, -0.7486718893051147, -0.08307251334190369, 0.00721780676394701, -0.20409846305847168, -0.9245020747184753, -0.30238717794418335, -0.45114174485206604, 0.9013084173202515, -0.40585771203041077, 1.0355427265167236, 0.9348315596580505, -1.1146842241287231, 0.33826881647109985, 0.47370171546936035, 0.13916803896427155, 0.04841233044862747, 1.3680826425552368, 0.3785495460033417, 0.06556360423564911, 0.06067176163196564, -0.048083748668432236, 0.5343546867370605, -0.0807633325457573, 0.5329082012176514, 1.4657360315322876, 0.09469085186719894, -0.14091584086418152, 0.6827611923217773, -0.11599545180797577, -1.0787763595581055, 0.3614296317100525, -0.2932813763618469, -0.5091066360473633, -0.1476241499185562, 0.8757365345954895, 0.3114181160926819, 0.011783181689679623, 0.6358206272125244, 0.2527497410774231, -0.18381687998771667, -0.5619516372680664, -0.3022695779800415, 0.7234480977058411, -0.5801453590393066, -0.06329435855150223, 1.4683431386947632, 0.3858770728111267, -1.1135237216949463, -1.4311943054199219, -0.9279419183731079, -0.37921983003616333, 0.04981817305088043, 0.22206655144691467, -0.6687202453613281, -0.5446210503578186, 0.5753706693649292, 0.806941568851471, 0.28466707468032837, 0.28280046582221985, -0.3590555489063263, -0.2256847620010376, 1.0397459268569946, 0.27314257621765137, -0.9467809796333313, 0.1857762187719345, 1.141268253326416, 1.8203827142715454, -0.9593568444252014, 0.3972969353199005, 0.24498561024665833, -0.5010201930999756, 1.3929953575134277, 0.9354321360588074, -0.247926726937294, 0.8802695870399475, -0.9287621378898621, -0.245049387216568, 0.27143585681915283, -1.6156526803970337, 0.14363864064216614, 0.48528626561164856, 0.8184453845024109, 0.41435906291007996, 0.014696880243718624, 0.2262117713689804, 1.0499593019485474, 0.4712754189968109, 0.4860987365245819, 1.1432383060455322, 0.5830636620521545, -0.3762322664260864, 0.31454455852508545, 0.5582354068756104, 0.6120520234107971, -0.35376495122909546, -0.11264663934707642, 0.7611875534057617, 0.9330189824104309, 0.3656884729862213, 0.5136469006538391, 0.6458189487457275, -0.11471273005008698, 0.7263050079345703, 0.47385528683662415, 0.6290931701660156, -0.3906572461128235, -0.4850935637950897, -0.2257566750049591, -0.9395677447319031, 0.2279025763273239, -0.11107929795980453, -0.8316707015037537, -0.22512595355510712, -0.1722257435321808, 0.2394683063030243, -0.18849794566631317, 0.07260119169950485, 0.7954450845718384, 0.3211197555065155, 0.5897548198699951, -0.23849283158779144, -0.36239105463027954, -0.5976011157035828, -1.01277494430542, 0.025961507111787796, -0.45606765151023865, -0.027576979249715805, -0.361970454454422, -0.2134373039007187, -0.33747074007987976]}, "authors": [{"authorId": "2350348", "name": "Kazuki Irie"}, {"authorId": "2258963332", "name": "R'obert Csord'as"}, {"authorId": "2260653300", "name": "J\u00fcrgen Schmidhuber"}], "references": [{"paperId": "9c71d178705989cd4371f8e760508f11b18a4bb4", "title": "Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions"}, {"paperId": "90f17a693e82e5046979b7b2300a4876f93cde64", "title": "Recasting Continual Learning as Sequence Modeling"}, {"paperId": "9bb3deca32af8d632e0d916c587cca6c185a6576", "title": "Uncovering mesa-optimization algorithms in Transformers"}, {"paperId": "832a122c1008a10949406c30dce1830d763d9e4e", "title": "Are LSTMs good few-shot learners?"}, {"paperId": "286c3587f2616839286748461cbc90261ea49caf", "title": "Meta-in-context learning in large language models"}, {"paperId": "847e5a8df8c08cc26165b57966a7cf59ee032a4e", "title": "Accelerating Neural Self-Improvement via Bootstrapping"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "93fdf5cf598aefb0335f001039e83494dc721c3a", "title": "General-Purpose In-Context Learning by Meta-Learning Transformers"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "42e1790c7979796634d15920b4a08990e847243e", "title": "Transformers generalize differently from information stored in context vs in weights"}, {"paperId": "8f30c30f92fbe1c307ee3c7a68c80a2c0dc8d619", "title": "Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules"}, {"paperId": "a501f42f10f6857715f8325f261788eb07a62949", "title": "A simple but strong baseline for online continual learning: Repeated Augmented Rehearsal"}, {"paperId": "ac3c0bfa0e38cbd26aca06cf0fcf7ad6d7deaa4d", "title": "Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules"}, {"paperId": "316206a2f89eb94ce02a81fba1dc304586f21b39", "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations"}, {"paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "663662fbd9be73f99c764a7b85982bf825acfe8a", "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention"}, {"paperId": "4fd61f6b860acc9c5da8766b7c9064f0ec896301", "title": "A Modern Self-Referential Weight Matrix That Learns to Modify Itself"}, {"paperId": "42ab2e42221a7fbd66ba368cf90b5e63b5270010", "title": "Improving Baselines in the Wild"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "ed535e93d5b5a8b689e861e9c6083a806d1535c2", "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"}, {"paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61", "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "2bb6dd038ee17a111323431f5900d32ae0aa6379", "title": "Meta-Learning Bidirectional Update Rules"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "76e3ad12881e7ab1c36318d8f8818eca3f828349", "title": "Meta Learning Backpropagation And Improving It"}, {"paperId": "56912f12c35af9579999b45fe6ab7d5b9f090df6", "title": "Efficient Continual Learning with Modular Networks and Task-Driven Priors"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "361850e0ec285528990c61235fc6c1e15cfd5ab6", "title": "Addressing Catastrophic Forgetting in Few-Shot Problems"}, {"paperId": "001a9f506055901129c3792982469f778be5974c", "title": "TaskNorm: Rethinking Batch Normalization for Meta-Learning"}, {"paperId": "1e889d45eaaa12d44b2a4c30b826934b749f9de8", "title": "Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning"}, {"paperId": "4bd7cc7d1fd2454956bafae1e00d2507bcbf5702", "title": "Learning to Continually Learn"}, {"paperId": "6c72c4ef7c76b4d64fa4c244ec38d70de053d97e", "title": "Torchmeta: A Meta-Learning library for PyTorch"}, {"paperId": "a513bb6e1967f5a31ad4f38954e66d4169b613e5", "title": "Metalearned Neural Memory"}, {"paperId": "d8bafd3a23c5ce9a7ebef036d5f2c67e1386ff11", "title": "Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes"}, {"paperId": "faf08cc7a7897f7f183673306abe6fd5b92091f4", "title": "Task Agnostic Continual Learning via Meta Learning"}, {"paperId": "475d35a691580d0082ecae212a81d9a4b6be787d", "title": "Meta-Learning Representations for Continual Learning"}, {"paperId": "23f425d6cb57938ceaa98fce8133a6924c2f953b", "title": "Three scenarios for continual learning"}, {"paperId": "c5875654fecca84e61a7287b8fbf30d4caccada6", "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples"}, {"paperId": "d9ff7a9344dd5d6653bd7a02bfd704422bb29951", "title": "Experience Replay for Continual Learning"}, {"paperId": "fe111c477ea0ce9186eb58ff847052976d9a2dd9", "title": "Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines"}, {"paperId": "2b877889ac31b73d1ede70b00eb4c7118ef8eca2", "title": "Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference"}, {"paperId": "146c231532d4e38de95e63368dcd09d0f8cea291", "title": "Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity"}, {"paperId": "29af8829e70aec87731dd5022a71cd77589e9b87", "title": "Generative replay with feedback connections as a general strategy for continual learning"}, {"paperId": "b48f327d8f2515216ac19314a58292c43bb53422", "title": "Metalearning with Hebbian Fast Weights"}, {"paperId": "e6a83abec5cffb0bf1669f2f2c1efdf2b15cb171", "title": "TADAM: Task dependent adaptive metric for improved few-shot learning"}, {"paperId": "ae7619604821adce52c28daa2aed14f5a191d975", "title": "Progress & Compress: A scalable framework for continual learning"}, {"paperId": "249ac07c5b87f44b85500e2d26b68a7edb93e83d", "title": "Differentiable plasticity: training plastic neural networks with backpropagation"}, {"paperId": "8bffce7de83c4a9bb48317c0dd3f38dac053a2f6", "title": "One Big Net For Everything"}, {"paperId": "713b0d9005944f80af00addc81b162ca74ea4b14", "title": "Memory Aware Synapses: Learning what (not) to forget"}, {"paperId": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a", "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm"}, {"paperId": "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32", "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms"}, {"paperId": "7e9c1e0d247b20a0683f4797d9ea248c3b53d424", "title": "A Simple Neural Attentive Meta-Learner"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "118fae4b4d07453561f1eded88654f812c7c61ec", "title": "Gradient Episodic Memory for Continual Learning"}, {"paperId": "59a922212153d3407e658109f36c11a34ee7d283", "title": "Continual Learning with Deep Generative Replay"}, {"paperId": "30ec43601d48236f8983d7cc55bed9cc68262b57", "title": "Automated Curriculum Learning for Neural Networks"}, {"paperId": "a99d857ecc78316a0d9a774972b775058d5644ca", "title": "Continual Learning Through Synaptic Intelligence"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "470d11b8ca4586c930adbbfc3f60bff08f2a0161", "title": "Meta Networks"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "282a380fb5ac26d99667224cef8c630f6882704f", "title": "Learning to reinforcement learn"}, {"paperId": "29c887794eed2ca9462638ff853e6fe1ab91d5d8", "title": "Optimization as a Model for Few-Shot Learning"}, {"paperId": "954b01151ff13aef416d27adc60cd9a076753b1a", "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"}, {"paperId": "6a5dd5f40fa0865c4aa56118cae329b763766467", "title": "Learning to Learn Neural Networks"}, {"paperId": "63de0ad39d807f0c256f851428f211e8d5fcd3bb", "title": "Instance Normalization: The Missing Ingredient for Fast Stylization"}, {"paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a", "title": "Learning without Forgetting"}, {"paperId": "3904315e2eca50d0086e4b7273f7fd707c652230", "title": "Meta-Learning with Memory-Augmented Neural Networks"}, {"paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753", "title": "Progressive Neural Networks"}, {"paperId": "be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6", "title": "Matching Networks for One Shot Learning"}, {"paperId": "815c84ab906e43f3e6322f2ca3fd5e1360c64285", "title": "Human-level concept learning through probabilistic program induction"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "e9f60363fd3d448492d6d670e5b333b4b26a5064", "title": "Compete to Compute"}, {"paperId": "044b2c29e0a54dc689786bd4d029b9ba6e355d58", "title": "Learning to Learn Using Gradient Descent"}, {"paperId": "2722b9e5ab8da95f03e578bb65879c452c105385", "title": "Catastrophic forgetting in connectionist networks"}, {"paperId": "f67e7dd2495500f3975f39e541fa38073d49a2ee", "title": "Fixed-weight on-line learning"}, {"paperId": "dde691805cfa7d6f1bb88c7411c1c3377b6cdc67", "title": "Lifelong Learning Algorithms"}, {"paperId": "6e7241121c688abbd9329bdcebce4b6320fc619d", "title": "Shifting Inductive Bias with Success-Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement"}, {"paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49", "title": "Multitask Learning"}, {"paperId": "2ebf18e7892e660a833152ddc6cf8f1d21a7b881", "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory."}, {"paperId": "5ac423a83b4321b43249224fcc528bb70e086826", "title": "Catastrophic Forgetting, Rehearsal and Pseudorehearsal"}, {"paperId": "7ee98330fb5969839d88bcabdb44d03848dc9d35", "title": "A \u2018Self-Referential\u2019 Weight Matrix"}, {"paperId": "b64e846fe88acaf302248249696c3b7badde41b5", "title": "Meta-neural networks that learn by learning"}, {"paperId": "bc22e87a26d020215afe91c751e5bdaddd8e4922", "title": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"}, {"paperId": "add21b42c325bdbf145b536ee4bb66cb8cf1a59a", "title": "Learning algorithms and fixed dynamics"}, {"paperId": "8001b80755bb8b189f3e1a51db8d108303b9fe7b", "title": "Fixed-weight networks can learn"}, {"paperId": "591b52d24eb95f5ec3622b814bc91ac872acda9e", "title": "Connectionist models of recognition memory: constraints imposed by learning and forgetting functions."}, {"paperId": "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7", "title": "Connectionism and cognitive architecture: A critical analysis"}, {"paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725", "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "dfd4135cc81c6dfe5ccc4d6d54b4652fa3dd831f", "title": "Generative vs. Discriminative: Rethinking The Meta-Continual Learning"}, {"paperId": null, "title": "Livewired: The inside story of the ever-changing brain"}, {"paperId": "d94b70f294d2aa58d1d95d0b9e2a8a614ce6e92b", "title": "Theoretical foundation of potential functions method in pattern recognition"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "85d346297ec37a451d4500afad53fed8f281af92", "title": "Adaptive Switching Circuits"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2", "title": "The mnist database of handwritten digits"}, {"paperId": "81dcd9f8b2e553dfe6754ddca09a2f09af26fb27", "title": "On Learning How to Learn Learning Strategies Technical Report Fki-198-94 (revised)"}, {"paperId": "643c6c542fc31110952cf5b8c36f03076b14ef97", "title": "Beyond \\genetic Programming\": Incremental Self-improvement"}, {"paperId": "082b1f5c791cadef18c4920ecc1396615a3fe7cb", "title": "Continual learning in reinforcement environments"}, {"paperId": null, "title": "Steps towards \u201cself-referential\u201d learning. Technical Report CU-CS-627-92"}, {"paperId": "19c942248849ef95e991dc5b7ebc6180a000d1f0", "title": "Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks"}, {"paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1", "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}, {"paperId": "bdaec1b3eb9a8f7a2b296be009a148c35236f3ce", "title": "Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook"}, {"paperId": "99d2a4bcd77d0851cdb26e8e2fe63aa80b113fe9", "title": "Studies of mind and brain : neural principles of learning, perception, development, cognition, and motor control"}, {"paperId": null, "title": "Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments"}]}