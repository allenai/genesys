{"paperId": "cd3359ed7119210bfa0b34ba5796d1314a05e212", "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data", "abstract": "Learning from preference labels plays a crucial role in fine-tuning large language models. There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning. Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient. This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why? In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a\"negative gradient\") outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.", "venue": "arXiv.org", "year": 2024, "citationCount": 36, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems finds that approaches that use on-policy sampling or attempt to push down the likelihood on certain responses outperform offline and maximum likelihood objectives."}, "embedding": {"model": "specter_v2", "vector": [-0.19100236892700195, 0.5404890775680542, -1.0017611980438232, -0.15139372646808624, -0.8239962458610535, -0.5348958373069763, 0.6801562905311584, 0.15266256034374237, -0.6837859153747559, 0.10318280756473541, 0.4110892713069916, -0.58986496925354, -0.09061110019683838, 0.42451396584510803, -0.09399383515119553, -0.09808389097452164, -0.8766666054725647, 0.6026831865310669, -0.42007574439048767, -0.17665699124336243, -0.35497990250587463, -0.5782288312911987, -0.5593109130859375, 0.029305946081876755, 0.2168259620666504, 0.6301819682121277, 0.29454100131988525, 0.9253174662590027, -0.3155893385410309, -0.11975876241922379, 0.7714768648147583, -0.057608943432569504, 0.3018069267272949, 0.08066282421350479, -0.33818551898002625, 0.22562958300113678, 0.03460974618792534, -0.30729201436042786, -0.3625292181968689, 0.7870057821273804, 0.14907296001911163, 0.6512289643287659, 0.5979503393173218, -0.65234375, -0.4534752666950226, 0.7419412136077881, 0.30706164240837097, 0.600620687007904, -0.22835563123226166, -0.29622572660446167, 1.4150474071502686, -0.7478654980659485, -0.025582322850823402, 1.9047185182571411, 0.2225736528635025, 0.5347461700439453, -0.6352733969688416, -0.3190160393714905, 1.11052405834198, 0.021717477589845657, -0.704763650894165, 0.16021552681922913, -0.18373428285121918, -0.16225458681583405, 1.1809842586517334, -0.32555800676345825, -0.18663877248764038, 0.7865248322486877, -0.37325340509414673, 1.449106216430664, 0.19894270598888397, -0.8957826495170593, 0.21242910623550415, 0.6610497832298279, -0.2347666174173355, 0.6926701664924622, -0.8280186653137207, 0.748723566532135, -0.7174892425537109, -0.6665265560150146, 0.09013651311397552, -0.48513516783714294, 0.1579068899154663, -0.600086510181427, 0.4733562767505646, 1.1162549257278442, 0.2521454989910126, 0.38783708214759827, -0.06642389297485352, 0.37375929951667786, 0.48971909284591675, 0.1977338343858719, 0.24620522558689117, 0.5501284003257751, -0.6585501432418823, -0.1015143096446991, -0.4877546727657318, 0.12498406320810318, 0.009455118328332901, 0.6120887994766235, -0.33807799220085144, -0.1500258445739746, -1.2560625076293945, 0.5787004828453064, 1.6925033330917358, 0.1312321573495865, 0.533437192440033, -0.8122960329055786, 0.26047050952911377, -0.9645773768424988, 0.6222130656242371, -0.24861621856689453, -0.28582659363746643, -0.1752844601869583, -0.5854704976081848, -0.8338534235954285, -0.22322295606136322, 0.18012388050556183, -0.7413758039474487, 0.9207016229629517, -0.18308545649051666, -0.748542308807373, -0.31346967816352844, 0.8237383365631104, -0.07759922742843628, 0.6219446659088135, 0.6478043794631958, -0.16620907187461853, 0.6578400731086731, -0.4463074207305908, -0.2034599334001541, -0.845426082611084, 0.7292311787605286, 0.027315011247992516, 0.9756705164909363, -0.07466166466474533, -1.2178385257720947, -0.8778859376907349, -0.40143564343452454, 0.33894914388656616, -0.17223097383975983, 0.5553964972496033, 1.0357228517532349, 0.6352851986885071, -0.7375853657722473, 0.9111242294311523, -0.16697511076927185, -0.2955852150917053, 0.2148212194442749, 0.21055832505226135, 0.5345732569694519, -0.15693019330501556, -1.3253045082092285, 0.2921716570854187, 0.3502010703086853, -0.8280687928199768, -0.41042348742485046, -0.47254541516304016, -0.7315146327018738, -0.16361646354198456, 0.3958888053894043, -0.4333232045173645, 1.6894365549087524, -0.6120996475219727, -1.6259034872055054, 0.7043114304542542, 0.17218917608261108, 0.25239917635917664, 1.0081493854522705, 0.1702718436717987, -0.5320383906364441, -0.7854574918746948, -0.4329720735549927, 0.8591315746307373, 0.29918813705444336, -0.3813353180885315, -0.6484059691429138, 0.4290884733200073, 0.021862072870135307, 0.007660457398742437, -0.5033514499664307, 0.528692364692688, -0.2364814728498459, -0.15415093302726746, -0.1735745072364807, 0.6906797885894775, -0.38038626313209534, -0.05763167142868042, 0.08868526667356491, -1.4484319686889648, 0.3900591731071472, 0.14003446698188782, 0.9715464115142822, -0.6610300540924072, -0.7485333681106567, 0.1165483295917511, -0.46649932861328125, 0.2277088165283203, -1.079687237739563, 0.7767942547798157, 0.03927532583475113, 0.3471260070800781, -0.15597309172153473, -0.9521573781967163, 0.45889681577682495, -0.12403929233551025, -0.5652221441268921, -0.07413819432258606, -0.19728289544582367, 0.6543299555778503, -1.2439521551132202, 0.16281041502952576, 0.053308699280023575, 0.1502663493156433, -1.313933253288269, 1.485541820526123, -0.8565319776535034, 0.6003984212875366, -0.072896808385849, -0.5788037180900574, 0.2856825292110443, -0.6209873557090759, 0.8896708488464355, -0.12026511132717133, 0.0057356758043169975, 0.6564286351203918, -0.5929943919181824, 1.171568512916565, -0.581712543964386, 0.14812491834163666, 0.15844383835792542, -0.4698205292224884, -0.07076980918645859, 0.09572896361351013, 0.030867932364344597, -0.48916077613830566, 0.07709038257598877, 0.3785403072834015, -0.8412697911262512, -0.04521807283163071, 0.5893828868865967, 0.7252981066703796, -0.06278009712696075, 0.3802450895309448, 0.12183776497840881, -0.10555271059274673, 0.7048518061637878, 0.059719279408454895, 0.03803437948226929, 0.6241706609725952, 0.722420871257782, -0.2989298701286316, 0.3964381515979767, -0.9700808525085449, -0.12967953085899353, 0.7995191812515259, 0.6165530681610107, 0.4329068064689636, 0.365959495306015, -0.6706557273864746, -0.43335384130477905, -0.2661784887313843, 0.3545893430709839, 1.8800528049468994, -0.34825611114501953, 0.006753345485776663, -0.621957004070282, -0.7525432705879211, -0.11713524162769318, 0.10800611227750778, -0.26161059737205505, 0.08368370682001114, -0.5293439030647278, -1.3984439373016357, 0.2898232638835907, 0.5453112125396729, 0.33270084857940674, -0.5015608668327332, -0.04785839840769768, 0.008181489072740078, 0.20353162288665771, -0.30140581727027893, -0.8272648453712463, 0.45916083455085754, -0.3174227774143219, 0.249198317527771, -0.018569065257906914, -0.06311751157045364, 0.12433479726314545, -0.9520047903060913, 1.046117901802063, -0.5038537979125977, 0.20757988095283508, 0.09565266966819763, 0.2626914083957672, -0.5099155902862549, -0.8720571994781494, 0.526679277420044, 0.3893197476863861, 0.053750231862068176, 0.12860135734081268, 0.4322565793991089, 0.2936651408672333, -0.053913962095975876, -0.43360796570777893, -0.17674896121025085, 0.18875271081924438, 0.350175678730011, 0.4137703478336334, -0.3468343913555145, -0.13233473896980286, -1.1465604305267334, 1.2091975212097168, 0.01930440217256546, -1.0008277893066406, 0.5027968883514404, -0.9014512896537781, 0.15156009793281555, 0.6451206207275391, -0.9997938871383667, -0.21615484356880188, -0.6915367841720581, -0.19965386390686035, -0.15361909568309784, -0.156374990940094, 0.05046553537249565, 0.11316782236099243, 0.3807254731655121, 0.43319159746170044, 0.14966994524002075, 0.096502386033535, -0.4553265869617462, 0.563437283039093, -0.5696543455123901, 0.037434451282024384, -0.34378036856651306, 0.010015112347900867, -0.23647575080394745, -0.5969178676605225, -0.29355740547180176, -0.6443650722503662, -0.21025680005550385, -0.028500596061348915, 0.24780264496803284, -0.23046787083148956, -0.39988547563552856, -1.4434362649917603, -0.373835951089859, -0.6454283595085144, -0.7882857322692871, 0.09609778970479965, -0.02894165739417076, -0.3671223521232605, -1.069710373878479, -1.0595347881317139, -0.9858642220497131, -0.07652954757213593, -0.9640594720840454, 0.06609999388456345, 0.15214255452156067, -0.7098154425621033, -0.6820544600486755, 0.26016488671302795, -0.8179548382759094, 0.7840886116027832, -1.4157785177230835, 1.3248059749603271, 0.23898804187774658, 6.726480205543339e-05, 0.0338173508644104, 0.18136104941368103, 0.07719503343105316, -0.1148383840918541, 0.28689950704574585, -1.125779390335083, -0.37687548995018005, -0.7295801043510437, -0.773659884929657, 0.04419989138841629, 0.5219524502754211, 1.3767825365066528, -0.4257749617099762, -0.4187397360801697, 0.4577121436595917, 1.1561247110366821, -0.5378157496452332, -0.3709348142147064, 0.34879082441329956, 0.795055091381073, 0.49943965673446655, 0.3517332971096039, 1.0940001010894775, 0.26695212721824646, 0.676033079624176, -0.15425720810890198, -0.02532377652823925, 0.19432732462882996, -1.0717551708221436, 0.4262554347515106, 1.0583853721618652, 0.5481783151626587, -0.2646760940551758, -0.5976873636245728, 0.535309374332428, -1.625483512878418, -0.4247395694255829, 0.983191967010498, 0.8754647374153137, 0.6827069520950317, -0.41645684838294983, 0.32014381885528564, -0.2372390329837799, 0.11742642521858215, 0.5075764060020447, -0.2923314869403839, -0.8053243160247803, 0.3960200548171997, -0.1907273828983307, 0.6066651344299316, 1.123929738998413, -0.6178176999092102, 0.3842063844203949, 14.6429443359375, 1.3640987873077393, -0.09226551651954651, 0.7037070393562317, 0.8545984625816345, 0.21201381087303162, -0.9599347114562988, -0.20205353200435638, -1.4614155292510986, 0.21905075013637543, 1.074474573135376, 0.43370521068573, 1.0515801906585693, 0.014381020329892635, 0.39813241362571716, 0.03729031980037689, -0.5317320227622986, 0.689630925655365, 0.04743465781211853, -1.286036491394043, 0.3207240104675293, 0.2650272846221924, 0.688645601272583, 0.08984344452619553, 0.98700350522995, 0.5997413992881775, 1.1974471807479858, -0.37210729718208313, 1.0325475931167603, 0.028915872797369957, 1.011093020439148, -0.14231379330158234, 0.07212062925100327, 0.7978115081787109, -0.5073261857032776, -0.5600758790969849, -0.625464141368866, -1.0487509965896606, 0.20747186243534088, 0.18025758862495422, -0.6533862948417664, -0.6206456422805786, 0.05283577740192413, 0.40714746713638306, 0.4706195592880249, 0.20446352660655975, -0.32825687527656555, 0.6238752007484436, -0.08445204049348831, -0.09226234257221222, 0.5363963842391968, 0.544789731502533, 0.04041793942451477, 0.027299465611577034, -0.03162189945578575, 0.07999270409345627, 0.6362783312797546, 0.3176495432853699, -0.4960821568965912, -0.18127422034740448, -0.017580347135663033, -0.14789517223834991, 0.4541623294353485, 0.7347508072853088, 0.9671862721443176, 0.4625248610973358, 0.07889065891504288, -0.0814422219991684, 0.7998389601707458, 0.2123747169971466, 0.007725562900304794, 0.6592645049095154, 0.5598328113555908, -0.5271051526069641, -0.09924761950969696, 0.7823807597160339, -0.2071380317211151, -0.11638441681861877, -0.9379380941390991, -0.8656432032585144, 0.6311802268028259, -0.4681011438369751, -1.217514157295227, 0.549757182598114, -0.014342734590172768, -0.613834023475647, 0.12745977938175201, -0.7132138013839722, 0.20532338321208954, 0.5330445170402527, -0.9630903601646423, -0.37970343232154846, 1.0302278995513916, -0.9269222021102905, -0.44544321298599243, 0.12312845140695572, 1.2053471803665161, 0.021778441965579987, -0.4012560546398163, 0.568348228931427, 0.6578122973442078, -0.43632057309150696, 0.16070666909217834, -0.7601293921470642, 0.7624137997627258, -0.11664566397666931, -0.17434488236904144, 0.11304166167974472, 0.1323794275522232, 0.3604143261909485, -0.2341042459011078, 0.031153298914432526, 0.7248576283454895, -0.8823400139808655, -0.386412113904953, -0.5010103583335876, -0.4990152418613434, 0.10287345945835114, 0.28334006667137146, -0.14594601094722748, 0.1706584393978119, 0.19373083114624023, -0.5689100623130798, -0.1628032773733139, -0.8953076601028442, -0.01804918237030506, 0.47943344712257385, -0.6299590468406677, 0.15975631773471832, -0.1986827552318573, -0.16748596727848053, -1.1664018630981445, -0.4417969584465027, -0.13891403377056122, 0.25106319785118103, 0.28884223103523254, 1.1564518213272095, -1.0087616443634033, 0.482138991355896, 0.6714453101158142, -0.20896901190280914, -1.2300310134887695, -0.502085268497467, -0.88302081823349, 0.1796656996011734, 0.32206445932388306, 0.6490356922149658, -0.36710482835769653, 0.24855995178222656, 0.8343672156333923, 0.8066993355751038, -0.46819955110549927, -0.16005662083625793, -0.20389726758003235, 0.21002934873104095, -0.3841142952442169, 0.5428032875061035, -0.06364914029836655, -0.3899700343608856, -0.07321182638406754, -0.009815031662583351, 0.9052912592887878, -0.05244974046945572, -0.4510171413421631, 0.07494284957647324, 0.08711270242929459, -0.5447024703025818, -0.8500856161117554, 0.3944096863269806, -1.6127222776412964, 0.017391059547662735, -1.107004165649414, 0.16119249165058136, -0.7724809646606445, -0.43446528911590576, -0.40257367491722107, -0.21308380365371704, -0.1738794595003128, 0.17352621257305145, -0.6549772024154663, -0.29634013772010803, -0.6320478916168213, -0.5526484847068787, 0.871680736541748, 0.7390304803848267, -0.9774797558784485, -0.014885107055306435, 0.24736753106117249, -0.1939699500799179, 0.4771420955657959, 0.6147338151931763, -0.6154149770736694, -0.7614454627037048, -1.084533929824829, 0.32449886202812195, -0.0355888195335865, -0.1814199686050415, -0.09697435051202774, 0.279231458902359, -0.5366212725639343, 0.12943290174007416, 0.11646262556314468, 0.23157618939876556, -0.9610976576805115, -0.6707691550254822, 0.2831787168979645, -1.3626080751419067, 0.04516514018177986, 0.051830753684043884, -0.4254140555858612, -0.04167657718062401, 0.4797230362892151, 0.2717701494693756, -1.1399046182632446, -0.8103620409965515, 0.6571903228759766, -0.6759459972381592, 0.394913911819458, -0.6732423901557922, 0.42582783102989197, -0.5856132507324219, -0.5158526301383972, 0.16152255237102509, 0.7094191908836365, -0.2720559537410736, 0.7474703192710876, -0.11199099570512772, -1.3783435821533203, 0.1335754096508026, 0.48571041226387024, -0.0016819695010781288, -0.23004549741744995, 0.33653420209884644, 0.3641756474971771, -0.09786826372146606, 0.7580916881561279, 0.046646807342767715, 0.4032417833805084, -0.9379440546035767, -0.1975967437028885, 0.8863664865493774, -1.0786023139953613, 0.21397051215171814, 1.497219443321228, -0.07866690307855606, -1.7308660745620728, 0.8311651349067688, -0.5799798965454102, -0.5361987948417664, -0.33650147914886475, 0.4918181598186493, 0.40470677614212036, -0.2674635946750641, 0.44982630014419556, -0.682987630367279, 0.10106991976499557, 0.06381142884492874, -0.7728309035301208, 0.3725224435329437, -0.7427085041999817, -0.1395728886127472, 0.6258075833320618, 0.8525983095169067, -0.4194665551185608, -1.5919119119644165, -0.49596789479255676, -0.10208196938037872, -0.45961976051330566, 0.22999748587608337, -0.6512953042984009, -0.5573198199272156, 0.6992883682250977, 0.8030857443809509, 0.03769126906991005, -0.14815755188465118, -0.09429647028446198, -0.2016012966632843, 0.557766854763031, 0.3320138156414032, -0.8817402720451355, -0.6620200872421265, 0.8273895382881165, 1.1124142408370972, -1.019906997680664, 0.19933800399303436, 0.2682841718196869, -0.9337525963783264, 0.7162741422653198, 0.5314117670059204, 0.23939178884029388, 0.8990368247032166, -0.5334251523017883, 0.5444962978363037, 0.5143204927444458, -1.0712897777557373, -0.2408890724182129, 1.1956977844238281, 1.0356826782226562, 0.6913259625434875, 0.8625677824020386, 0.0231743473559618, 0.6824215650558472, 0.034470364451408386, 0.06848885864019394, 0.2891705334186554, 0.23070579767227173, -0.08010902255773544, -0.8458109498023987, -0.11349920183420181, 0.6521264314651489, -0.6085954308509827, -0.18773597478866577, 0.0814238116145134, 0.09391912817955017, 0.07801874727010727, 0.5679800510406494, 0.5796307921409607, -0.03762874752283096, 0.20322705805301666, -0.12604814767837524, 0.5746248960494995, -0.4965983033180237, -0.2592509686946869, -0.13137362897396088, -0.4543900787830353, -0.22595806419849396, -0.07826648652553558, -0.5701180696487427, -0.17284950613975525, -0.41285425424575806, 0.7818775177001953, 0.1893417090177536, -0.0079199755564332, 0.9670233726501465, 0.3589964807033539, 0.18636639416217804, -0.07776948064565659, -0.6032364368438721, -1.4002101421356201, -0.8965120911598206, -0.0863533765077591, -0.6416459679603577, -0.11714839190244675, -0.08732884377241135, -0.09054414182901382, -0.6099481582641602]}, "authors": [{"authorId": "83137609", "name": "Fahim Tajwar"}, {"authorId": "2111007256", "name": "Anika Singh"}, {"authorId": "50465276", "name": "Archit Sharma"}, {"authorId": "102801230", "name": "Rafael Rafailov"}, {"authorId": "2298568770", "name": "Jeff Schneider"}, {"authorId": "2297771684", "name": "Tengyang Xie"}, {"authorId": "2282987268", "name": "Stefano Ermon"}, {"authorId": "2284774407", "name": "Chelsea Finn"}, {"authorId": "1488785534", "name": "Aviral Kumar"}], "references": [{"paperId": "77dbbafed1edc6f159845f4a9b58886d4ab6ab62", "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"}, {"paperId": "b16cbdacf53ab4870ce7645d899c7e9e6f41c51e", "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study"}, {"paperId": "f036c481cb86c96228c66e2e3e9778ba4730435c", "title": "Dataset Reset Policy Optimization for RLHF"}, {"paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641", "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"}, {"paperId": "087699924e3dc468a486e0763f1cc097824a60d2", "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models"}, {"paperId": "4d4432514695e0f36720c73c23d15d8e21abe2fe", "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models"}, {"paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4", "title": "Direct Language Model Alignment from Online AI Feedback"}, {"paperId": "04d64be16fb402f28348faffef484bd419c8bd8f", "title": "Self-Rewarding Language Models"}, {"paperId": "324786abbbc22ca1fba487709536ee682fe0af60", "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"}, {"paperId": "26b2adbe089ea36617c3ec0aa009319929da0550", "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity"}, {"paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5", "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"}, {"paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed", "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint"}, {"paperId": "b4afc7b4a6836054c1b4568ce9d49993afbb0461", "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"}, {"paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa", "title": "Nash Learning from Human Feedback"}, {"paperId": "a8f860fe5950445c94d49b47184225474f140bc6", "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks"}, {"paperId": "8dc334f0dd716585eee05d2daaff330de4916a9e", "title": "SuperHF: Supervised Iterative Learning from Human Feedback"}, {"paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f", "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"}, {"paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a", "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"}, {"paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579", "title": "A Long Way to Go: Investigating Length Correlations in RLHF"}, {"paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534", "title": "Reward Model Ensembles Help Mitigate Overoptimization"}, {"paperId": "22ab4219371366a4e890382bc0ca606130840ca7", "title": "Statistical Rejection Sampling Improves Preference Optimization"}, {"paperId": "78b0c5d96a05b4be6a00702fba24c9174e8173af", "title": "Aligning Language Models with Offline Learning from Human Feedback"}, {"paperId": "182c7b40ff7560a5545764814338f55a2098e441", "title": "Reinforced Self-Training (ReST) for Language Modeling"}, {"paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623", "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d", "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"}, {"paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a", "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"}, {"paperId": "db6f58b6aca4931afbd7d9760fea13cbc2c1953c", "title": "The Role of Baselines in Policy Gradient Optimization"}, {"paperId": "795736777f08e92a80c95dab7f205d1d7c28a10b", "title": "The CRINGE Loss: Learning what language not to model"}, {"paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b", "title": "Scaling Laws for Reward Model Overoptimization"}, {"paperId": "16bc62ca0672c7cdd0ef693b5ae0a73731af41a8", "title": "When to Ask for Help: Proactive Interventions in Autonomous Reinforcement Learning"}, {"paperId": "1e34c51b52002796fea6f523b9f794f1d75d9ba8", "title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting"}, {"paperId": "023edab4738690444e3924e224c2641017a0d794", "title": "Quark: Controllable Text Generation with Reinforced Unlearning"}, {"paperId": "e06c005e98281af455c454ce2478285f6f3afeca", "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning"}, {"paperId": "245682e8b3fa76f4a3e2991b5497577af95cbb3f", "title": "COMBO: Conservative Offline Model-Based Policy Optimization"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c", "title": "MOPO: Model-based Offline Policy Optimization"}, {"paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9", "title": "MOReL : Model-Based Offline Reinforcement Learning"}, {"paperId": "6568423cfaca7e24c88ea208cb0e67129e43aa9b", "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c", "title": "Neural Text Generation with Unlikelihood Training"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "42ba8ecee544a7ed87b201bca3f88d742ecfe6a1", "title": "The Self-Normalized Estimator for Counterfactual Learning"}, {"paperId": "6b69c970d01e93bbeb73b4ed360a759fbfb4befc", "title": "Finite-Time Bounds for Fitted Value Iteration"}, {"paperId": "1f869232f148ec52066fab06a49855937f84098b", "title": "Reinforcement learning by reward-weighted regression for operational space control"}, {"paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b", "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"}, {"paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff", "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"}, {"paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b", "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"}, {"paperId": "51cda783aa6a97e0b3b5915a2bb5a35f31f3c083", "title": "GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": "70d6dfdc40c4681ba5d51d60116db0311b5126ce", "title": "Language Models"}, {"paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614", "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"}, {"paperId": null, "title": "A simulation framework for methods"}, {"paperId": null, "title": "finetuning"}, {"paperId": null, "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"}, {"paperId": null, "title": "Human-aware loss functions (halos)"}, {"paperId": null, "title": "increases probability mass only on a subset of categories that equal target likelihoods"}, {"paperId": null, "title": "Andrej Karpathy"}, {"paperId": null, "title": "Monolithic preference optimization without reference"}]}