{"paperId": "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8", "title": "Blockwise Parallel Transformers for Large Context Models", "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel transformers (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 8, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Blockwise Parallel transformers is presented, that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods."}, "embedding": {"model": "specter_v2", "vector": [0.4468527138233185, 0.42838412523269653, 0.10063020139932632, -0.1559661626815796, -0.13369132578372955, -0.31167399883270264, 0.9856273531913757, -0.03977534547448158, -0.624968945980072, 0.016994623467326164, 0.638039767742157, -0.021860318258404732, 0.5325872302055359, 0.19272154569625854, -0.28809040784835815, 0.04457778111100197, -0.8092203736305237, 0.2723436951637268, 0.19036521017551422, -0.6198903322219849, 0.04358384758234024, -0.7324178218841553, -0.8748835921287537, -0.02428855560719967, 0.1558002382516861, 0.40890809893608093, 0.35773301124572754, 0.6596499085426331, -0.5231936573982239, 0.6661563515663147, 0.6568493247032166, -0.2963607907295227, 0.4945822060108185, 0.06681783497333527, -0.4221937656402588, -0.4930112361907959, 0.6748235821723938, -0.3309473991394043, -0.6093255281448364, 0.6787524819374084, -0.31676149368286133, 0.10317675769329071, 0.19808708131313324, -0.5296272039413452, -0.03820284083485603, 1.3664023876190186, 0.7409786581993103, 0.8908324837684631, -0.30553138256073, -0.506328821182251, 1.7602108716964722, -1.1278265714645386, 0.046469807624816895, 1.4229156970977783, 0.5536083579063416, 0.5639081001281738, -0.22109410166740417, -1.1381042003631592, 1.142030954360962, 0.24105089902877808, -0.4211134612560272, -0.4625549018383026, 0.4029143452644348, -0.003946533892303705, 2.1027708053588867, -0.3443397879600525, 0.4409041106700897, 0.63362056016922, 0.306868314743042, 1.3323959112167358, -0.0047675915993750095, -0.8705759048461914, -0.36622944474220276, -0.4127907454967499, 0.445797324180603, 0.8559100031852722, -0.6747068166732788, 0.772741436958313, -1.0240964889526367, -0.11950016766786575, 0.6502083539962769, 0.14490731060504913, 0.12907825410366058, -0.126622274518013, -0.44710811972618103, 0.5962077975273132, 0.46856072545051575, 1.2005242109298706, -0.2964093089103699, 0.9704955220222473, 0.5015487670898438, 0.2562771141529083, -0.2962057292461395, 0.5013406276702881, 0.046969495713710785, 0.4814372658729553, -0.6302350759506226, 0.5392928719520569, -0.2593259811401367, 0.8844085931777954, -0.09169366955757141, 0.42298224568367004, -0.7706181406974792, 0.34813040494918823, 1.4659456014633179, 0.11710476875305176, 0.7418249845504761, -0.8252301812171936, 0.34948739409446716, -0.4843560755252838, 0.028222423046827316, -0.6785226464271545, -0.25265222787857056, -0.3321969211101532, -0.5777981281280518, -1.266615629196167, -0.32256028056144714, 0.2793636620044708, -0.5928465127944946, 1.1592274904251099, -0.2534084618091583, -0.10749076306819916, -0.15590928494930267, 0.1459093987941742, 0.15065830945968628, 0.44019320607185364, 0.7109970450401306, -0.038209620863199234, 0.865978479385376, -0.8187943696975708, -0.7736488580703735, -1.507569670677185, 0.5941883325576782, 0.2874562442302704, 0.060528721660375595, -0.11898858845233917, -1.36285400390625, -1.0028529167175293, -0.9488515257835388, -0.14033718407154083, -0.36062753200531006, -0.2098625898361206, 0.9005443453788757, 0.06580697000026703, -0.9346780776977539, 0.946845531463623, -0.17069901525974274, -0.016551712527871132, 0.3897416889667511, 0.18820220232009888, 0.3025735318660736, -0.2932985723018646, -1.4382262229919434, 0.4288579523563385, 0.49690330028533936, -0.17071935534477234, -0.49949735403060913, -0.5630475878715515, -1.3183672428131104, 0.07037466019392014, 0.5311734080314636, -0.4425986707210541, 1.4540951251983643, -0.28640544414520264, -1.156655192375183, 0.5555691123008728, -0.48342710733413696, -0.2792017459869385, 0.17899280786514282, -0.41363224387168884, -0.4646228849887848, -0.6064072251319885, -0.23546023666858673, 0.6997525095939636, 0.41561952233314514, -0.25456756353378296, -0.49749353528022766, -0.2854786813259125, -0.20071518421173096, -0.15243369340896606, -0.35406482219696045, 0.7782350182533264, -0.04690880328416824, -0.2559722065925598, 0.22946792840957642, 0.6571712493896484, -0.07288283109664917, -0.5575017333030701, -0.28696155548095703, -1.3798621892929077, 0.5145849585533142, 0.10690146684646606, 0.9527934193611145, -0.878673255443573, -0.8336423635482788, -0.2701799273490906, 0.07300648093223572, -0.10285501927137375, -0.9956046342849731, 0.8432640433311462, -0.8267220854759216, 0.3005695343017578, -0.4479221999645233, -0.7838451862335205, -0.060047801584005356, 0.2005281299352646, -0.8110844492912292, -0.3488059639930725, 0.0217890627682209, 1.3163633346557617, -1.4031985998153687, -0.33257079124450684, 0.21890141069889069, -0.09328745305538177, -0.8241187334060669, 1.2670338153839111, -0.38627251982688904, 0.07595013827085495, 0.2837716042995453, -0.5847326517105103, -0.14933371543884277, -0.01943359151482582, 0.7744284272193909, -0.5524265766143799, -0.23634643852710724, 0.8066231608390808, -0.09419529139995575, 1.395801305770874, -0.29828712344169617, 0.5665120482444763, -0.1629004031419754, -0.744161069393158, 0.047049980610609055, 0.43178442120552063, -0.03512529656291008, -0.7599270939826965, 0.35802093148231506, 0.03145717829465866, -0.44002848863601685, 0.2536018490791321, 0.5542888641357422, 0.36326050758361816, -0.17888616025447845, 0.02465234138071537, 0.7473961710929871, -0.5164185166358948, 0.7334325909614563, 0.5913803577423096, 0.6727073788642883, 0.5646358728408813, 0.6924859285354614, -0.1833072006702423, 0.4756825268268585, -0.7830259203910828, 0.23297247290611267, 0.3251977860927582, 0.48373034596443176, 0.5021808743476868, 0.2996637523174286, -0.7329626083374023, -0.5051276683807373, 0.4001942276954651, 0.8694210648536682, 1.6129297018051147, -0.4569275677204132, -0.33940213918685913, -0.6311421990394592, -0.429941326379776, -0.44798725843429565, 0.4323031008243561, -0.48657435178756714, -0.08628743141889572, -0.8733267188072205, -0.3606419563293457, 0.706003725528717, 0.7785062789916992, 0.9597216248512268, -0.7190892100334167, -0.3640443682670593, 0.10172849148511887, 0.13444235920906067, -0.7526882886886597, -0.6769805550575256, 0.5014679431915283, -0.7731501460075378, -0.36476823687553406, 0.2883964478969574, -0.19141888618469238, -0.0705401748418808, -0.47398650646209717, 1.1828725337982178, -0.670285165309906, 0.06594045460224152, 0.3373264670372009, 0.8687284588813782, -0.6923861503601074, -0.6813787817955017, -0.22153082489967346, 0.23742757737636566, -0.3234361708164215, 0.3914579451084137, 0.2884310185909271, 0.11779892444610596, -0.008877407759428024, -0.2811278998851776, 0.18502752482891083, 0.1215236708521843, 0.35734134912490845, 0.6323751211166382, -0.39147424697875977, 0.09782096743583679, -1.539509892463684, 0.7785824537277222, 0.31035906076431274, -0.6500877737998962, 0.43029606342315674, -0.6850911974906921, -0.31137776374816895, 0.4387527108192444, -0.5246374011039734, -0.5331814289093018, -0.5261165499687195, 0.3547652065753937, -0.49065545201301575, -0.4194490611553192, 0.19529075920581818, 0.30956193804740906, 0.4536226689815521, 0.3469884991645813, 0.6022019386291504, 0.15906758606433868, 0.3104393780231476, 0.6803807616233826, -0.983927845954895, 0.7241038680076599, 0.2984931468963623, 0.024415140971541405, -0.32759228348731995, -0.15597355365753174, -0.8190491199493408, -0.37936288118362427, -0.5352531671524048, -0.3755020797252655, -0.22446638345718384, 0.18963782489299774, -0.5243579149246216, -1.1198265552520752, -0.04608486220240593, -1.0735905170440674, -0.3584432005882263, -0.0241552647203207, -0.4291037917137146, -0.11237295717000961, -0.7302160263061523, -1.0797576904296875, -0.8391788601875305, -0.6609021425247192, -0.414478063583374, 0.0071439738385379314, 0.07282335311174393, -0.3789370059967041, -0.5466731190681458, 0.2878537178039551, -0.2863861322402954, 0.8376718759536743, -0.8786160349845886, 0.6427159905433655, -0.05696437135338783, -0.045806679874658585, -0.11145829409360886, 0.38952067494392395, 0.5618342757225037, -0.07867859303951263, 0.11726944148540497, -1.05124032497406, 0.23590001463890076, -0.15870065987110138, -0.2647155225276947, 0.17493797838687897, 0.16877025365829468, 0.6069102883338928, -0.07765188068151474, -0.6262606382369995, -0.15758945047855377, 1.4155175685882568, -0.16807955503463745, -0.0790693461894989, -0.184160515666008, 1.1394907236099243, 0.3294692039489746, -0.28036338090896606, 0.22602571547031403, 0.4709787964820862, 0.13463570177555084, 0.4332652986049652, 0.16055136919021606, 0.1964273452758789, -0.7463673949241638, 0.360491544008255, 1.2966201305389404, 0.21338003873825073, 0.2701445519924164, -1.2256250381469727, 1.0163283348083496, -1.1918716430664062, -0.8887976408004761, 0.6145671010017395, 0.6237419843673706, 0.49160441756248474, -0.45374414324760437, -0.3044613003730774, -0.02533748932182789, 0.26750698685646057, 0.28652405738830566, -0.536650538444519, -0.33992937207221985, -0.04894048348069191, 0.38721680641174316, 0.1983068585395813, 0.6054410338401794, -0.33701997995376587, 0.7035298943519592, 14.79197883605957, 0.43842434883117676, -0.12053908407688141, 0.5874770283699036, 0.49996769428253174, 0.2924829423427582, -0.5539700388908386, 0.08108033239841461, -0.9992655515670776, -0.4526250958442688, 1.0045114755630493, 0.3678446412086487, 0.7584651112556458, -0.010257519781589508, -0.17369897663593292, 0.21369175612926483, -0.9266477227210999, 0.7983549237251282, 0.4247072637081146, -1.0871515274047852, 0.23103828728199005, -0.10811641067266464, 0.16889213025569916, 0.707811713218689, 0.4275209307670593, 0.924004852771759, 0.9807085990905762, -0.06715939939022064, 0.3953028619289398, 0.201114684343338, 0.6504014134407043, 0.3326260447502136, -0.04803197830915451, 0.6742814183235168, -0.7995865941047668, -0.6731895208358765, -0.43261849880218506, -1.24323308467865, 0.22253760695457458, -0.1554344743490219, -0.4428700804710388, -0.5109297037124634, -0.08312247693538666, 0.9079614281654358, 0.23593486845493317, -0.1250944286584854, -0.45278629660606384, 0.7178673148155212, -0.1010323017835617, 0.050599437206983566, 0.34943681955337524, 0.4140142798423767, 0.030046185478568077, -0.011394263245165348, -0.16497163474559784, 0.44576284289360046, -0.15389110147953033, 0.5072164535522461, -0.3092096745967865, -0.1599007248878479, -0.46928367018699646, -0.4619520604610443, -0.04930383339524269, 0.907537043094635, 0.8645870685577393, -0.006011865567415953, -0.2823777496814728, -0.008180646225810051, 0.814145028591156, 0.10709863156080246, -0.19650691747665405, 0.22181588411331177, -0.006344085559248924, -0.733875572681427, -0.07237066328525543, 0.3605608642101288, -0.29008471965789795, -0.48582327365875244, -1.0163544416427612, -0.31207937002182007, 0.5485146045684814, -0.8845833539962769, -0.8817105889320374, 0.938636302947998, -0.1610574573278427, -0.23203244805335999, -0.05740680918097496, -0.8255228400230408, -0.3912348449230194, 0.32633936405181885, -1.2131761312484741, -0.6801266074180603, 0.06466805189847946, -0.03984949365258217, -0.22370362281799316, 0.042642418295145035, 1.1529000997543335, -0.11630453169345856, -0.3825843334197998, -0.1595989316701889, -0.2788105309009552, -0.02174512855708599, -0.28584471344947815, -0.9249520897865295, 0.6248102784156799, 0.2519668936729431, 0.3022327423095703, 0.39229616522789, 0.12273875623941422, 0.05997747927904129, -0.5216690897941589, 0.29376545548439026, 0.9607698917388916, -0.7769559621810913, -0.5444506406784058, -0.8722402453422546, -0.9401267766952515, 0.6346578001976013, 0.9205417037010193, -0.27271488308906555, 0.35790398716926575, 0.10846156626939774, -0.6992615461349487, -0.2659718990325928, -0.5821157693862915, 0.41407036781311035, 0.7269242405891418, -0.8231037259101868, -0.7869175672531128, -0.22072111070156097, 0.23488150537014008, -0.7267706394195557, -0.39189866185188293, -0.3798481523990631, 0.2587069272994995, -0.22973713278770447, 0.9103867411613464, -0.5829777717590332, 0.7566248178482056, 0.8699547648429871, 0.016072776168584824, -0.7467436194419861, -0.43818333745002747, -0.7051869630813599, 0.17475968599319458, 0.3407042622566223, 0.4222092926502228, -0.5020111799240112, -0.00030707038240507245, 1.1102036237716675, 0.0758165568113327, -0.19883336126804352, -0.819229245185852, -0.1508893370628357, 0.16556556522846222, -0.594243049621582, 0.33633044362068176, 0.18138720095157623, 0.23740975558757782, 0.6683855652809143, 0.3782336413860321, 0.21839408576488495, -0.18912571668624878, -0.5879707336425781, 0.12219155579805374, -0.057229336351156235, 0.1995696872472763, -0.7141944766044617, -0.0013044968945905566, -1.3817002773284912, -0.015853367745876312, -1.0949039459228516, 0.06506498157978058, -0.9821716547012329, -0.6616153717041016, 0.011699710041284561, -0.43135252594947815, 0.4799109101295471, 0.16560031473636627, -0.750686526298523, -0.20718643069267273, -0.44954484701156616, -0.31869640946388245, 0.8653824925422668, 0.624098002910614, -1.0270016193389893, 0.1367887407541275, -0.3592337369918823, 0.07185838371515274, 0.006197701208293438, 0.45091792941093445, -0.29940348863601685, -0.7656176686286926, -1.2022649049758911, 0.47488510608673096, 0.07944342494010925, -0.19958119094371796, -0.9861271977424622, 0.7971146702766418, 0.15166686475276947, -0.18495988845825195, -0.018141360953450203, 0.6066583395004272, -1.2704038619995117, -0.6529261469841003, 0.4909549653530121, -1.0754095315933228, 0.2765495777130127, 0.3657435178756714, -0.3128490149974823, -0.40877506136894226, 0.8638032078742981, -0.2854050099849701, -1.4154624938964844, -0.6254026293754578, 0.32217302918434143, -0.7192043662071228, 0.005908753722906113, -0.1690051406621933, -0.048787157982587814, -0.9373142123222351, -0.22307522594928741, 0.24157652258872986, 0.8862598538398743, -0.847503125667572, 0.8627402782440186, 0.6109566688537598, -0.9902194142341614, 0.11750708520412445, 0.4967831075191498, 0.08785504847764969, 0.2358826994895935, 0.7699494361877441, 0.27490586042404175, 0.06432013213634491, 0.8117616176605225, -0.06546838581562042, 0.34463173151016235, -1.0211560726165771, 0.4209005534648895, 1.01323664188385, -0.725860059261322, 0.020341960713267326, 1.0686038732528687, 0.016897983849048615, -1.241300106048584, 0.007308862172067165, -1.2694257497787476, -1.090787410736084, -0.34007224440574646, 0.5228500366210938, 0.08957961946725845, -0.40481138229370117, 0.012891787104308605, -0.40590348839759827, 0.1415412575006485, -0.20248761773109436, -0.6371005177497864, 0.6656467914581299, 0.050766799598932266, -0.3178855776786804, 0.8937943577766418, 0.2959147095680237, -0.7198605537414551, -0.9098617434501648, -1.0511873960494995, -0.48819980025291443, -0.0625089704990387, 0.3073948621749878, -0.5795088410377502, -0.6277022957801819, 1.0674655437469482, 0.5038178563117981, 0.2764946222305298, -0.07690746337175369, -0.3990532457828522, 0.050988808274269104, 0.6644585728645325, -0.00345026608556509, -0.5555166006088257, -0.6446651816368103, 1.5655109882354736, 1.728999376296997, -0.7055035829544067, -0.1688951998949051, -0.06912049651145935, -0.7222583889961243, 0.7856271266937256, 0.6205613613128662, -0.2826542854309082, 0.8637232184410095, -0.42543134093284607, 0.24586467444896698, 0.0483589842915535, -1.2357230186462402, -0.11335422843694687, 0.5916467308998108, 1.0161426067352295, 0.8484262824058533, 0.15713152289390564, 0.11081262677907944, 0.7906161546707153, 0.13425788283348083, 0.26251673698425293, 0.09154410660266876, 0.5251365900039673, 0.11384483426809311, -0.06932124495506287, 0.3293251693248749, 0.5380791425704956, -0.5204848051071167, -0.7603116631507874, 0.127495676279068, 0.32659608125686646, 0.08721714466810226, 0.6285713911056519, 0.7882551550865173, 0.029712757095694542, 0.6089336276054382, 0.40470874309539795, 0.5910153388977051, -0.24384282529354095, -0.5799483060836792, -0.1537008285522461, -0.6232895851135254, -0.20337921380996704, -0.3857341706752777, -0.5665122270584106, -0.37620413303375244, 0.22840018570423126, 0.030120130628347397, -0.16989707946777344, 0.21023039519786835, 1.282834768295288, 0.3203374743461609, 0.4893198311328888, -0.4459497332572937, -0.23702774941921234, -0.5044655799865723, -1.3382370471954346, 0.2593681216239929, -0.4302728474140167, -0.20543532073497772, -0.3354877829551697, 0.21465270221233368, -0.3602254390716553]}, "authors": [{"authorId": "2256317240", "name": "Hao Liu"}, {"authorId": "2253464956", "name": "Pieter Abbeel"}], "references": [{"paperId": "f11044596cf2eaf59f83d82b8167b16ba6a08617", "title": "Emergent Agentic Transformer from Chain of Hindsight Experience"}, {"paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "title": "Teaching Large Language Models to Self-Debug"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "cb3125e4f63f3d058a2a39270ecb585e86c3d1ff", "title": "Chain of Hindsight Aligns Language Models with Feedback"}, {"paperId": "a128b1c47e6842605fb95bceae930d2135fc38fc", "title": "Pretraining Without Attention"}, {"paperId": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e", "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"}, {"paperId": "860bc4f071f35d6d8529a52c2c1858d030779a6a", "title": "In-context Reinforcement Learning with Algorithm Distillation"}, {"paperId": "7bb956fe972902884c48607b88c8599bd165a0c7", "title": "Palm up: Playing in the Latent Manifold for Unsupervised Pretraining"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "df434c1289f3c7243b585cb9982afac3c5bf0439", "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "56a35ffb3ca0d820155e5655b527a74bf8e7b13a", "title": "Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "9317736e9bb1b25c9d8e7325b7b364b7fbae0f3f", "title": "URLB: Unsupervised Reinforcement Learning Benchmark"}, {"paperId": "561f9f5abb2c0960a886ab6221c821295f0461a1", "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"}, {"paperId": "960aaaa85d1de9f8e2490d872165b720e3d4b565", "title": "AlphaFold and implications for intrinsically disordered proteins."}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "509b16378deec0fb6bbec1d7aeb32a4bdeedddb1", "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"}, {"paperId": "deee48c5e0ac0407a1e002905caaf2b174bdb0e6", "title": "MSA Transformer"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": null, "title": "Aps: Active pre-training with successor features"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "GitHub - karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs"}, {"paperId": null, "title": "Automating inter-and { Intra-Operator } parallelism for distributed deep learning"}, {"paperId": null, "title": "Fully Sharded Data Parallel: faster AI training with fewer GPUs \u2014 engineering"}]}