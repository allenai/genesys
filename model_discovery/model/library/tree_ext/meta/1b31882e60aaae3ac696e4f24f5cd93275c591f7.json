{"paperId": "1b31882e60aaae3ac696e4f24f5cd93275c591f7", "title": "FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization", "abstract": "Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined \\emph{FlexRound}, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corresponding scales, and thus, flexibly quantize pre-trained weights depending on their magnitudes. We empirically validate the efficacy of FlexRound on a wide range of models and tasks. To the best of our knowledge, our work is the first to carry out comprehensive experiments on not only image classification and natural language understanding but also natural language generation. Moreover, we demonstrate, for the first time, that large language models can be efficiently quantized, with only a negligible impact on performance compared to half-precision baselines, achieved by reconstructing the output in a block-by-block manner. Our code is available at \\url{https://github.com/onliwad101/FlexRound_LRQ}.", "venue": "International Conference on Machine Learning", "year": 2023, "citationCount": 17, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.00317", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight."}, "embedding": {"model": "specter_v2", "vector": [0.4332368075847626, 0.7095938920974731, -0.4565277099609375, -0.34991782903671265, -0.5286771059036255, 0.030426474288105965, 0.3752754032611847, -0.029013870283961296, -0.7001247406005859, -0.46372267603874207, 0.5863352417945862, 0.1316094547510147, 0.4348316490650177, -0.2083427906036377, -0.24352766573429108, 0.1914009153842926, -0.7696504592895508, -0.10028732568025589, -0.2005714774131775, -0.4543447196483612, 0.18113955855369568, -0.6434189081192017, -0.7960818409919739, 0.401248961687088, 0.3092769980430603, 1.1262781620025635, -0.10097910463809967, 0.9534725546836853, -0.3665996193885803, 0.37526291608810425, 0.2520006000995636, -0.7321625351905823, 0.6171862483024597, 0.10077595710754395, -0.31858891248703003, 0.18723921477794647, 0.4485304057598114, -0.775557816028595, -0.35482701659202576, 1.1413768529891968, -0.08713547140359879, 0.2251744568347931, 0.4479142725467682, -0.5324715375900269, -0.8455274701118469, 0.694800853729248, 0.5549393892288208, 0.36616405844688416, -0.49057015776634216, -0.5432610511779785, 1.367006778717041, -1.631689190864563, 0.06378063559532166, 1.375795841217041, 0.7231550812721252, 0.556934654712677, -0.3116326332092285, -0.7522475123405457, 0.5508927702903748, 0.20702360570430756, -0.9132364988327026, -0.4285450279712677, -0.3160407245159149, 0.07688555866479874, 1.5086389780044556, -0.2661736309528351, 0.0486341193318367, 0.27726948261260986, 0.21859775483608246, 0.8114076852798462, 0.15828393399715424, -0.6422862410545349, -0.07783595472574234, -0.20071794092655182, -0.0974772498011589, 0.9920903444290161, -0.42164167761802673, 0.24335992336273193, -0.9720732569694519, 0.20421797037124634, 0.3613359332084656, -0.03823838010430336, 0.08708381652832031, -0.07937950640916824, -0.03872351348400116, 0.6126161813735962, 0.9230469465255737, 0.1859370321035385, -0.25889214873313904, 1.2430225610733032, 0.6101917028427124, 0.25900352001190186, 0.35782021284103394, 0.015036414377391338, -0.08770371973514557, 0.581036388874054, -0.9228386282920837, 0.20091989636421204, -0.4886466860771179, 0.3561779856681824, 0.011349368840456009, 0.15694962441921234, -0.8878329992294312, 0.13965445756912231, 1.323534607887268, -0.10797613859176636, 0.49729377031326294, -0.850764811038971, 0.3390989303588867, -0.8518201112747192, 0.057498808950185776, -0.4250270426273346, 0.011078755371272564, -0.26762086153030396, -1.0995972156524658, -0.6931567788124084, -0.8194198608398438, 0.33641353249549866, -1.2417746782302856, 1.0330793857574463, -0.5361089706420898, 0.2696113586425781, 0.2694063186645508, 0.5073285698890686, 0.4074012041091919, 0.9522517919540405, 0.3456171751022339, 0.19810420274734497, 1.1596461534500122, -0.8098151683807373, -0.5979211926460266, -0.6211662888526917, 0.6707156300544739, -0.29137060046195984, 0.5072804689407349, 0.14428013563156128, -1.2187750339508057, -1.2106523513793945, -1.0743070840835571, -0.3971535563468933, -0.658077597618103, 0.5343953371047974, 0.5605953335762024, 0.5703136920928955, -1.1991639137268066, 1.0443360805511475, -0.15251140296459198, 0.12901341915130615, 0.7241588830947876, 0.29110902547836304, 0.2592319846153259, -0.41986724734306335, -1.0646218061447144, 0.16523315012454987, 0.2895503640174866, -0.2252187877893448, 0.0138673922047019, -0.7961898446083069, -1.0291101932525635, 0.03180212154984474, 0.03920696675777435, -0.459123432636261, 1.2476987838745117, -0.1018858402967453, -1.3281508684158325, 0.7804839015007019, -0.19501861929893494, -0.16671016812324524, 0.3862258195877075, -0.2627164125442505, -0.38463518023490906, -0.4189883768558502, -0.43470603227615356, 1.1134215593338013, 0.5937206149101257, -0.10676786303520203, -0.056518539786338806, 0.24117836356163025, -0.3575873374938965, 0.2471550554037094, -0.7040327191352844, 0.5802174806594849, -0.3559952676296234, -0.8245076537132263, 0.48951396346092224, 0.9224111437797546, 0.07599849998950958, 0.1899873912334442, -0.40779149532318115, -0.9990525245666504, 0.6418549418449402, 0.05817651376128197, 0.6936997771263123, -1.1032850742340088, -0.8951363563537598, -0.04502246156334877, 0.09695646166801453, -0.15036703646183014, -0.8968062400817871, 0.27383893728256226, -0.5304639339447021, 0.4995369613170624, 0.0007605302962474525, -1.4841786623001099, 0.5639609694480896, -0.2572702169418335, -0.9698201417922974, -0.15928764641284943, 0.30907562375068665, 1.3490139245986938, -0.40985897183418274, 0.3208789825439453, -0.26546862721443176, 0.44384369254112244, -1.158056616783142, 0.7965094447135925, -0.16896817088127136, -0.26607099175453186, 0.4361547827720642, -0.2437201887369156, 0.2178105264902115, -0.32048502564430237, 0.1303955465555191, -0.8558812141418457, 0.02435573749244213, 0.7807490825653076, -0.598956286907196, 1.3101327419281006, -0.31719261407852173, 0.37350282073020935, -0.3587048351764679, -0.650686502456665, 0.39550265669822693, 0.2090095579624176, -0.48273587226867676, -0.3677746057510376, 0.21908608078956604, 0.38650238513946533, -0.8034991025924683, 0.7255595922470093, 1.0477595329284668, 0.3665054142475128, -0.3581900894641876, 0.03028203174471855, 0.7337702512741089, -0.5160700678825378, 0.42952436208724976, 0.15799860656261444, 0.389210045337677, 0.3011157512664795, 0.5883480906486511, 0.06050218269228935, 0.19660541415214539, -1.0670766830444336, 0.035435374826192856, 0.3160738945007324, 0.42706015706062317, 1.3344799280166626, 0.49105045199394226, -0.821617841720581, -0.5851712822914124, -0.32167482376098633, 0.6140345931053162, 1.6179702281951904, -0.06844814121723175, -0.20372925698757172, -0.7922166585922241, -0.04321591183543205, -0.5114139914512634, -0.29387450218200684, -0.44668272137641907, -0.49829599261283875, -0.27049726247787476, -1.2146620750427246, 0.7720186114311218, 0.21979528665542603, 1.185589075088501, -0.40841391682624817, -0.05313827469944954, -0.43838968873023987, 0.28181275725364685, -0.7004632949829102, -0.91716468334198, 0.6004976034164429, -0.36687228083610535, 0.34174346923828125, -0.15791606903076172, -0.27590179443359375, 0.3446735739707947, -0.9248579144477844, 0.862153947353363, -0.44827428460121155, -0.4713897705078125, -0.36022210121154785, 0.5397293567657471, -0.4970352053642273, -0.7039406299591064, 0.11620613932609558, 0.07333139330148697, 0.11556041240692139, 0.17939169704914093, 0.48437899351119995, -0.1867850124835968, -0.24831612408161163, -0.6018627285957336, 0.08784129470586777, 0.04488808661699295, -0.2217881977558136, 1.1324657201766968, -0.25647398829460144, 0.06093847006559372, -1.0255169868469238, 1.0670280456542969, 0.6183326244354248, -0.38710319995880127, 0.36819544434547424, -0.5629512071609497, -0.2958361804485321, 0.6186686754226685, -0.41600728034973145, -0.031265754252672195, -0.9681534171104431, 0.09027664363384247, -0.4193073511123657, -0.10830070823431015, -0.08233575522899628, 0.4363901913166046, 0.18754331767559052, 0.5086153149604797, 0.19507968425750732, -0.03634282574057579, 0.14480046927928925, 0.8452979922294617, -0.8461445569992065, 0.9240789413452148, 0.2301121950149536, 0.2644028663635254, -0.05563436821103096, -0.2544984817504883, -0.2323305606842041, -0.6538956761360168, -0.19545991718769073, -0.10814427584409714, -0.051325421780347824, 0.6785905957221985, -0.8822867274284363, -0.7160743474960327, -0.11117131263017654, -0.8775036334991455, -0.1779400110244751, -0.4722261428833008, -0.2880229949951172, -0.32182446122169495, -1.1370586156845093, -1.3576140403747559, -0.14367765188217163, -0.9153919219970703, -1.1402424573898315, 0.28685715794563293, 0.1503850668668747, -0.29595518112182617, -0.45860105752944946, -0.37678709626197815, -0.47504132986068726, 0.8667125701904297, -0.7203455567359924, 0.8580662608146667, 0.2292342483997345, -0.040278110653162, -0.01181580126285553, -0.13436712324619293, 1.1098779439926147, -0.2638300955295563, 0.23343788087368011, -0.9564012289047241, 0.25394338369369507, -0.4115184545516968, -0.7495046257972717, 0.41134825348854065, 0.1301882266998291, 1.0551824569702148, -0.0698317289352417, 0.34990882873535156, 0.9339587092399597, 1.5743637084960938, -1.0500690937042236, 0.34668219089508057, -0.1347949057817459, 0.94146728515625, -0.1491352617740631, -0.6517283320426941, 0.6673975586891174, 0.280755877494812, 0.12410371005535126, 0.26798081398010254, -0.3578260540962219, -0.6218575835227966, -0.6122351884841919, 0.4991030991077423, 1.6204367876052856, 0.41333186626434326, -0.15366947650909424, -0.8297567963600159, 0.48009660840034485, -0.5980362296104431, -0.6632187962532043, 0.933660089969635, 0.5433982610702515, 0.3592630624771118, -0.14386716485023499, -0.6325194239616394, 0.09358260780572891, 0.2928687632083893, 0.4763636589050293, -0.35616883635520935, -1.219277262687683, 0.06730492413043976, 0.5208444595336914, 0.3856257498264313, 0.5229817628860474, -0.4047122001647949, 0.38847291469573975, 14.565390586853027, 0.9558075070381165, -0.4516834020614624, 0.4520944356918335, 1.4148575067520142, 0.3195282816886902, -0.5326330661773682, -0.430702805519104, -1.2136563062667847, -0.33115220069885254, 0.698837161064148, 0.4373457431793213, 0.6964404582977295, -0.10227759182453156, -0.08721676468849182, 0.5956605672836304, -0.5040838122367859, 0.9394093751907349, 0.49385663866996765, -1.547723650932312, 0.5379627346992493, 0.2931359112262726, 0.911781907081604, 0.6097177267074585, 1.261749029159546, 0.7036433219909668, 0.1105777770280838, -0.5403480529785156, 0.8712695837020874, 0.27014032006263733, 1.2588452100753784, 0.008392670191824436, 0.22533433139324188, 0.34720680117607117, -0.8661509156227112, -0.2520042359828949, -0.8631001114845276, -1.2399020195007324, 0.030006971210241318, 0.2587805688381195, -0.3710855543613434, -0.4214029908180237, 0.021612782031297684, 0.819930911064148, 0.10507407039403915, 0.19562433660030365, -0.2670285701751709, 0.6904354691505432, -0.26731377840042114, 0.3653630316257477, 0.41076067090034485, 0.22830352187156677, -0.21406471729278564, -0.05805153399705887, 0.5010980367660522, 0.07165585458278656, 0.2950788736343384, 0.5650200843811035, -1.2355996370315552, -0.14410355687141418, 0.29454103112220764, 0.12494082003831863, -0.1503841131925583, 0.8029890656471252, 0.0945051833987236, 0.0592716708779335, -0.11105185747146606, 0.6586437821388245, 0.6140103340148926, 0.18567633628845215, -0.33705002069473267, -0.19915422797203064, 0.30747470259666443, -0.5822358131408691, 0.33008110523223877, 0.6763631701469421, -0.5051090717315674, -0.4747392237186432, -0.4444112479686737, -0.14473223686218262, 0.043123360723257065, -0.9260745048522949, -0.6154777407646179, 0.7201433777809143, -0.4122855067253113, -0.47784069180488586, 0.47047504782676697, -0.8859989047050476, -0.23586277663707733, 0.5460866093635559, -1.5088298320770264, -0.3654744625091553, 0.33502617478370667, -0.5593177080154419, -0.3728072941303253, -0.09827280044555664, 1.225701093673706, 0.28158149123191833, -0.1572030931711197, 0.3311077058315277, 0.03268739581108093, 0.030902819707989693, -0.14589594304561615, -0.9220678806304932, 0.9991598725318909, 0.19946877658367157, 0.08647383749485016, 0.04586465284228325, -0.37556496262550354, 0.626691997051239, -0.25817760825157166, -0.29242372512817383, 0.5503002405166626, -0.1834987848997116, -0.5342650413513184, -0.9895213842391968, -0.6291224956512451, 0.04320693761110306, 0.47505515813827515, 0.5033187866210938, 0.41023799777030945, -0.1638081669807434, -0.7879011631011963, -0.3324989676475525, -0.7342900633811951, 0.13733038306236267, 0.2768704891204834, -0.8555947542190552, -0.09617864340543747, -0.14081589877605438, 0.28617510199546814, -0.97711181640625, -0.5795994997024536, -0.14630953967571259, -0.06133904308080673, -0.3839785158634186, 1.2725225687026978, -0.19036255776882172, 1.0753381252288818, 0.8163436055183411, -0.41525793075561523, -0.6882476210594177, -0.17649726569652557, -0.8032125234603882, 0.41642773151397705, 0.2333572655916214, 0.5326990485191345, -0.2795722186565399, 0.5959446430206299, 0.15230078995227814, 0.16127178072929382, -0.8910975456237793, -0.757047176361084, 0.053635407239198685, 0.17493127286434174, -0.684509813785553, 0.20421412587165833, -0.37431421875953674, -0.5760810375213623, 0.01530469674617052, 0.41041189432144165, 0.378775417804718, -0.24700890481472015, -0.7246728539466858, -0.07092517614364624, 0.2950573265552521, -0.15842176973819733, -0.4934629499912262, -0.8203418254852295, -1.7366193532943726, -0.16896511614322662, -1.4073143005371094, -0.013715670444071293, -1.0625653266906738, -0.5827353596687317, 0.06303474307060242, -0.47849974036216736, 0.11982674151659012, 0.2804330289363861, 0.7590734958648682, 0.05833757296204567, -0.4969181716442108, -0.5414146780967712, 1.043021559715271, 0.9673427939414978, -0.8385933041572571, 0.3021676540374756, -0.29099252820014954, 0.05192676559090614, 0.5660268664360046, 0.4287148714065552, -0.3454275131225586, -1.164736270904541, -1.2385884523391724, 0.7229176163673401, -0.4845390319824219, 0.07426931709051132, -1.0767128467559814, 0.3746493458747864, 0.7214334607124329, 0.11841916292905807, -0.009789939969778061, 0.45595675706863403, -0.5648717880249023, -0.8044949769973755, 0.5439176559448242, -1.074885606765747, 0.15510281920433044, 0.07698097825050354, -0.48841407895088196, -0.43903109431266785, 0.4872158169746399, 0.014030267484486103, -0.7613027095794678, -0.8889837861061096, 0.4845007658004761, -0.2768365442752838, 0.1454453021287918, -0.4658880829811096, -0.28243982791900635, -0.9605914354324341, -0.2627747654914856, -0.23886655271053314, 0.09179097414016724, -0.5576529502868652, 0.6225129961967468, 0.6542956829071045, -1.386934757232666, 0.010965688154101372, 0.4461475610733032, -0.12601438164710999, 0.00510381767526269, 0.4458974301815033, 0.16263696551322937, -0.6611177921295166, 0.13934174180030823, 0.2866767942905426, 0.42693519592285156, -0.6491736769676208, -0.2562299370765686, 0.9168699383735657, -0.7200223803520203, -0.20394793152809143, 1.4607309103012085, -0.10249658674001694, -1.4335486888885498, 0.15014369785785675, -1.5807428359985352, -0.2867134213447571, -0.23142656683921814, 0.6800584197044373, -0.020037848502397537, 0.4924357831478119, 0.15660156309604645, -0.07767535001039505, 0.37259605526924133, -0.16346976161003113, -0.4670039713382721, 0.4309541881084442, -0.04290568828582764, -0.561222493648529, 0.5290070176124573, 1.1781201362609863, -0.8326676487922668, -0.9529821872711182, -0.746510922908783, -0.6995100975036621, 0.09424550086259842, 0.3959781527519226, -0.30968818068504333, -0.6924853324890137, 0.9603614211082458, 0.79778653383255, 0.17032630741596222, 0.650806188583374, -0.49401068687438965, 0.2821574807167053, 0.8821136951446533, -0.22855126857757568, -0.8730215430259705, -0.19797475636005402, 1.1676357984542847, 1.2789027690887451, -0.7493112683296204, 0.42366844415664673, -0.5806536078453064, -0.3527645468711853, 0.9673647880554199, -0.015763765200972557, -0.30647414922714233, 1.0905917882919312, -0.16132807731628418, 0.2153158038854599, 0.4737032949924469, -1.0433294773101807, 0.033854901790618896, 1.006737232208252, 0.9912459850311279, 0.7827692627906799, 0.2102816104888916, 0.30736759305000305, 0.9414383172988892, -0.26171594858169556, 0.04501637443900108, 0.42971938848495483, 0.09623900800943375, -0.24874648451805115, 0.06253165006637573, -0.2978905141353607, 0.7042221426963806, -0.7685607671737671, -0.6531983017921448, 0.5701402425765991, 0.22120404243469238, 0.7628448605537415, 0.2825697660446167, 1.0140013694763184, -0.0140022998675704, 0.5457783341407776, 0.07141212373971939, 0.4984422028064728, -0.5328242182731628, -0.36654093861579895, -0.2882745563983917, -1.12018620967865, -0.06457643210887909, 0.04142327234148979, -0.03704429790377617, -0.6740102171897888, -0.20462273061275482, 0.48312464356422424, -0.36415326595306396, 0.3564980626106262, 0.9557691812515259, 0.297124981880188, 0.7873796820640564, -0.17629186809062958, -0.6096073985099792, -0.8509382605552673, -0.6816098093986511, -0.07705631852149963, -0.12916885316371918, -0.17834888398647308, 0.10703543573617935, 0.1475115865468979, 0.14703477919101715]}, "authors": [{"authorId": "2119171752", "name": "J. H. Lee"}, {"authorId": "2144193082", "name": "Jeonghoon Kim"}, {"authorId": "12693169", "name": "S. Kwon"}, {"authorId": "122808525", "name": "Dongsoo Lee"}], "references": [{"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "edfea69d9cd1a4d40f4d879aa36f93ad7d26a659", "title": "QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization"}, {"paperId": "e9a09f8e474b4c74c700ebbe84d5b0696395a521", "title": "Towards Efficient Post-training Quantization of Pre-trained Language Models"}, {"paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0", "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"}, {"paperId": "d9d6db76dc44f8d6917196389de0ffc060261491", "title": "Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "8a0a7170977cf5c94d9079b351562077b78df87a", "title": "A White Paper on Neural Network Quantization"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3", "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "c5757decb2d7fd2f785b1353542b8b290642e285", "title": "Towards Accurate Post-training Network Quantization via Bit-Split and Stitching"}, {"paperId": "db1eb78f0f8a138d17f5768720d1b8c5b1980318", "title": "Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware"}, {"paperId": "0c0dfe47afcec2e229015f3c8f213d4c88e86b28", "title": "Up or Down? Adaptive Rounding for Post-Training Quantization"}, {"paperId": "c9d3c181d999b0e11c6e4c51b3f9aefd01489e0f", "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "42a7b7031ffa6a5d2e40ba4009b27862188e54f0", "title": "Loss aware post-training quantization"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "d77123b54dcc8014949584ab624e97298617bcad", "title": "Data-Free Quantization Through Weight Equalization and Bias Correction"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "7738c18b70c54a4aa1730f30d026392901e8e6c9", "title": "Trained Quantization Thresholds for Accurate and Efficient Fixed-Point Inference of Deep Neural Networks"}, {"paperId": "dc160709bbe528b506a37ead334f60d258413357", "title": "Learned Step Size Quantization"}, {"paperId": "74559f96e4a21e0402d1fbae6f6bf853eee85a0f", "title": "AutoQ: Automated Kernel-Wise Neural Network Quantization"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235", "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"}, {"paperId": "3cd3f1585ced02cbb56a9e1428176a6c2b211da2", "title": "Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "a4c40532e68728fbeab5d9415f6ad8e9530db360", "title": "The WebNLG Challenge: Generating Text from RDF Data"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": null, "title": "AdaRound\u2019 and \u2018Q + AdaRound"}, {"paperId": "851c2e1942642537499c743c324f10624b7b77ac", "title": "Accurate Post Training Quantization With Small Calibration Sets"}, {"paperId": "4897a03aaa2a0e6948c38347eab2a92dd0c35ae2", "title": "Performance Evaluation of INT8 Quantized Inference on Mobile GPUs"}, {"paperId": null, "title": "Learnable Rounding based on Element-wise"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "per-tensor quantization format"}, {"paperId": null, "title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or-1"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}, {"paperId": null, "title": "much data is used"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}]}