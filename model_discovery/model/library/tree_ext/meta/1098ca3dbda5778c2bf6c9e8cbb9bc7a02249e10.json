{"paperId": "1098ca3dbda5778c2bf6c9e8cbb9bc7a02249e10", "title": "Staged Training for Transformer Language Models", "abstract": "The current standard approach to scaling transformer language models trains each model size from a different random initialization. As an alternative, we consider a staged training setup that begins with a small model and incrementally increases the amount of compute used for training by applying a\"growth operator\"to increase the model depth and width. By initializing each stage with the output of the previous one, the training process effectively re-uses the compute from prior stages and becomes more efficient. Our growth operators each take as input the entire training state (including model parameters, optimizer state, learning rate schedule, etc.) and output a new training state from which training continues. We identify two important properties of these growth operators, namely that they preserve both the loss and the\"training dynamics\"after applying the operator. While the loss-preserving property has been discussed previously, to the best of our knowledge this work is the first to identify the importance of preserving the training dynamics (the rate of decrease of the loss during training). To find the optimal schedule for stages, we use the scaling laws from (Kaplan et al., 2020) to find a precise schedule that gives the most compute saving by starting a new stage when training efficiency starts decreasing. We empirically validate our growth operators and staged training for autoregressive language models, showing up to 22% compute savings compared to a strong baseline trained from scratch. Our code is available at https://github.com/allenai/staged-training.", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 26, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2203.06211", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "To find the optimal schedule for stages, this work uses the scaling laws from (Kaplan et al., 2020) to find a precise schedule that gives the most compute saving by starting a new stage when training efficiency starts decreasing."}, "embedding": {"model": "specter_v2", "vector": [0.12808680534362793, 0.42841973900794983, -0.2798349857330322, -0.019038110971450806, -0.3391270041465759, -0.24045729637145996, 0.8394806981086731, -0.2291373759508133, -0.47226986289024353, -0.5507705807685852, 0.32452014088630676, -0.5289852023124695, 0.2708718478679657, 0.007377143483608961, -0.03571353107690811, 0.24528095126152039, -0.9261820316314697, 0.9448062181472778, -0.17410056293010712, -0.37315699458122253, -0.6509570479393005, -0.0877230316400528, -0.9102678298950195, -0.2197461724281311, 0.5301272869110107, 0.6247797608375549, -0.04853469133377075, 0.8410620093345642, -0.5724736452102661, 0.2743844985961914, 0.6114246845245361, -0.23799175024032593, 0.5177407264709473, -0.05900606885552406, -0.2231656014919281, 0.1599360853433609, 0.18959976732730865, -0.3590540587902069, -0.29753562808036804, 0.5162564516067505, -0.22653907537460327, 0.41644319891929626, -0.18768996000289917, -1.0445609092712402, -0.028256844729185104, 0.73885178565979, 0.35155215859413147, 1.0797553062438965, -0.4149548411369324, -0.8469808101654053, 0.7815264463424683, -1.4939353466033936, -0.2873072922229767, 1.4779489040374756, 0.7753314971923828, 0.464746356010437, -0.5094441771507263, -0.46886757016181946, 0.5822558999061584, -0.6014471650123596, -0.933742344379425, -0.23242393136024475, -0.2907574772834778, -0.23233480751514435, 1.8328752517700195, -0.4229857623577118, 0.3705797493457794, 0.04082932323217392, -0.11086875945329666, 1.0533678531646729, 0.1389303356409073, -0.683769941329956, -0.3089832663536072, 0.247173011302948, 0.22433575987815857, 0.8835885524749756, -0.29064133763313293, 0.37881365418434143, -0.9632546901702881, -0.19778810441493988, 0.36529839038848877, 0.05536792054772377, 0.45306089520454407, -0.6070605516433716, 0.2765040397644043, 0.7413768768310547, 0.03618476167321205, 0.8991428017616272, -0.20926257967948914, 0.7493094205856323, 0.624951183795929, 0.5676042437553406, 0.7470800876617432, 0.16863568127155304, -0.3979625403881073, 0.5455960035324097, -1.0374757051467896, -0.12000682204961777, -0.2168251872062683, 0.6392776966094971, -0.09666197001934052, 0.3408854901790619, -0.5846129655838013, 0.2263026237487793, 1.224057674407959, 0.35173705220222473, 0.40922224521636963, -0.36034512519836426, 0.13727262616157532, -0.43034979701042175, -0.2770877182483673, -0.11011198908090591, -0.2825111448764801, -0.5669924020767212, -0.7316753268241882, -1.2780449390411377, -0.7400462627410889, 0.12791217863559723, -1.0432407855987549, 0.6924533843994141, -0.23636168241500854, 0.44248074293136597, 0.1203056275844574, 0.2378217875957489, 0.10259184241294861, 0.5221196413040161, 0.38336804509162903, 0.005889646243304014, 0.8690459132194519, -0.8466807007789612, -0.42711523175239563, -0.9171542525291443, 0.8961493372917175, -0.15804466605186462, 0.6073057651519775, -0.3200286328792572, -1.5301096439361572, -0.9765838980674744, -0.4522044062614441, 0.07828713953495026, -0.47652360796928406, -0.022462982684373856, 1.250787615776062, 0.787557065486908, -1.3625701665878296, 0.4740629494190216, -0.6287322640419006, -0.15911716222763062, 0.49464431405067444, 0.3777461051940918, 0.2773517370223999, -0.27766966819763184, -1.2336784601211548, 0.4806080162525177, -0.08868678659200668, -0.7036021947860718, -0.3068642318248749, -1.030279278755188, -0.8014198541641235, -0.09487074613571167, -0.007349993567913771, -0.4855828881263733, 1.5172196626663208, -0.022524401545524597, -1.2577999830245972, 0.6989660859107971, -0.2622433304786682, -0.30688080191612244, 0.40325307846069336, -0.2030630111694336, -0.21289163827896118, -0.6109328269958496, -0.0847388282418251, 0.14217080175876617, 0.3081664741039276, 0.12357130646705627, -0.28569093346595764, 0.30051523447036743, -0.17523223161697388, -0.0969843938946724, -0.46896877884864807, 0.9423636198043823, -0.4915367662906647, -0.11903101950883865, 0.34473568201065063, 0.46911096572875977, -0.5364881753921509, -0.2651147246360779, -0.0071867117658257484, -0.9910414218902588, 0.45260488986968994, -0.38035985827445984, 1.0947129726409912, -0.980234682559967, -0.41156765818595886, 6.25600223429501e-05, -0.14027981460094452, -0.22821667790412903, -0.5956230759620667, 0.44692614674568176, -0.5344524383544922, 0.624427080154419, -0.01608726754784584, -1.4975531101226807, 0.032569367438554764, -0.11588477343320847, -0.5835549235343933, -0.45604267716407776, 0.02537183277308941, 0.906891405582428, -0.6238114237785339, 0.1212703287601471, 0.030618816614151, 0.5438898205757141, -1.1285861730575562, 1.4192516803741455, -0.15148845314979553, 0.16856618225574493, 0.008834464475512505, -0.22020307183265686, 0.37189042568206787, -0.11275868862867355, 0.26514768600463867, -0.13641121983528137, -0.18385395407676697, 0.504263162612915, -0.4332849681377411, 1.3881837129592896, -0.8888595700263977, 0.041704434901475906, 0.1926352083683014, -0.8325522541999817, 0.03414306044578552, 0.20462045073509216, 0.014069405384361744, -0.41180840134620667, 0.23668354749679565, 0.7978841662406921, -0.7167665362358093, 0.7951194643974304, 0.6848780512809753, 0.16709481179714203, -0.55372554063797, 0.3038652241230011, 0.8495720624923706, -0.5279160141944885, 0.5180646777153015, 0.5264860391616821, 0.532899796962738, 0.28761419653892517, 0.09256524592638016, -0.09962845593690872, 0.24150066077709198, -1.0158404111862183, -0.2197779417037964, 0.27968132495880127, 0.6217212677001953, 0.4856526255607605, 0.43426164984703064, -0.7658683657646179, -0.47050294280052185, -0.19769684970378876, 0.7347826957702637, 1.6339964866638184, -0.5212987065315247, -0.0062581440433859825, -0.503493070602417, -0.22281129658222198, -0.4317086338996887, 0.1470891684293747, -0.32537826895713806, -0.18990273773670197, -0.8629302382469177, -0.9269337058067322, 0.8582351803779602, 0.2401324361562729, 0.9420992732048035, 0.0832507312297821, -0.17914505302906036, -0.4333343207836151, 0.5467501282691956, -0.9714912176132202, -0.6883379220962524, 0.6804373860359192, -0.8791847825050354, 0.2590521574020386, -0.012488298118114471, -0.290912389755249, 0.16828866302967072, -0.29244235157966614, 0.9918232560157776, -0.3290463089942932, -0.06470797210931778, 0.2928145229816437, 0.5294331908226013, -0.7978888154029846, -1.0513086318969727, 0.3473641872406006, 0.5507795214653015, -0.21116717159748077, -0.162210613489151, 0.416230708360672, 0.4776107966899872, -0.11631555110216141, -0.439564973115921, 0.4213770925998688, -0.20897185802459717, -0.2413589507341385, 0.6705811619758606, -0.19627578556537628, -0.43651923537254333, -1.2566832304000854, 1.1304682493209839, 0.30332377552986145, -0.9547824859619141, 0.5861364006996155, -0.7357777953147888, -0.13930563628673553, 0.4383002519607544, -0.4497011601924896, -0.19923697412014008, -0.9652124047279358, 0.1609017252922058, -0.7001832127571106, 0.2893995940685272, 0.036201246082782745, 0.6555042862892151, 0.11670888960361481, -0.12017721682786942, 0.39009907841682434, 0.35717329382896423, -0.4153071939945221, 0.5365294814109802, -1.0622533559799194, -0.12337061762809753, 0.18349114060401917, 0.746355414390564, -0.3163491487503052, 0.02087932452559471, -0.5060349702835083, -0.7606441974639893, -0.08830903470516205, -0.25985872745513916, -0.08787395805120468, 0.1262141913175583, -0.9831687808036804, -0.44855809211730957, 0.23892027139663696, -0.693135678768158, -0.6541712284088135, -0.2636299729347229, -0.39482682943344116, 0.03184475377202034, -1.2824703454971313, -1.4056060314178467, -0.6584309935569763, -0.8306533098220825, -0.8553395867347717, 0.4861600697040558, 0.1683603674173355, 0.08896549046039581, -0.9270222187042236, 0.033209607005119324, -0.4147026836872101, 1.1275118589401245, -0.8220390677452087, 1.0676167011260986, -0.3056262135505676, -0.0668032318353653, -0.22746780514717102, 0.49424728751182556, 0.6574658751487732, -0.3891701400279999, 0.2725668251514435, -0.5937457084655762, -0.10289808362722397, -0.28595295548439026, -0.32874733209609985, -0.08177700638771057, 0.45526760816574097, 0.4927639365196228, 0.2034987509250641, -0.3662807047367096, 0.47010537981987, 1.2932548522949219, -0.8451372385025024, -0.1943967491388321, 0.0493512786924839, 0.8381962776184082, 0.010602453723549843, -0.3205595910549164, 0.3710882365703583, 0.14618584513664246, 0.49911636114120483, -0.11258965730667114, -0.5611165165901184, -0.08215229213237762, -0.7267825603485107, 0.445630818605423, 1.8263046741485596, 0.34015798568725586, -0.011670056730508804, -1.1532071828842163, 0.34017980098724365, -0.5599964261054993, -0.36230701208114624, 1.0189414024353027, 0.9490526914596558, 0.14663831889629364, -0.15036322176456451, -0.11918855458498001, 0.08850681036710739, 0.30720454454421997, 0.2411559671163559, -0.1656043529510498, -1.069265604019165, 0.1634218692779541, 0.9980714321136475, 0.30510613322257996, 0.8633413314819336, 0.026796922087669373, 0.6940866708755493, 15.031311988830566, 0.5507448315620422, -0.05109187215566635, 0.9746541976928711, 0.8425479531288147, 0.19710125029087067, -0.6053184270858765, 0.041017815470695496, -0.9329741597175598, -0.2457904815673828, 1.478062629699707, -0.11236518621444702, 0.9991766810417175, 0.1425812840461731, 0.19236238300800323, 0.32006028294563293, -0.2778127193450928, 0.35966917872428894, 0.32359033823013306, -1.2164286375045776, 0.46799689531326294, 0.1989302635192871, 0.5360299944877625, 0.7094245553016663, 0.6021349430084229, 0.9475933313369751, 0.6106897592544556, -0.3766116797924042, 0.607839047908783, 0.09866484254598618, 1.0185444355010986, -0.050551727414131165, 0.3374400734901428, 0.5347577333450317, -1.243286371231079, -0.21598763763904572, -0.10196739435195923, -1.2088391780853271, 0.37386661767959595, 0.23690424859523773, -0.7956287860870361, -0.6242770552635193, -0.12536939978599548, 0.45791295170783997, 0.04324304312467575, 0.0392453595995903, -0.019851256161928177, 0.935420572757721, -0.6024017930030823, 0.3460327088832855, 0.10231219232082367, 0.24640388786792755, -0.08531513810157776, -0.3060632646083832, 0.29480406641960144, -0.15040025115013123, 0.22375577688217163, 0.2509678602218628, -0.6727640628814697, -0.25775447487831116, -0.22732578217983246, -0.40863320231437683, 0.12377065420150757, 0.6942335367202759, 0.23800936341285706, 0.2079598605632782, -0.4117804765701294, 0.311990886926651, 0.5304956436157227, -0.15406394004821777, -0.3809623718261719, 0.4407900273799896, 0.4590405225753784, -0.11570002138614655, -0.28728461265563965, 0.15519915521144867, -0.2367888242006302, -0.44128772616386414, -0.948314905166626, -0.5540386438369751, 0.4826411306858063, -0.8477882146835327, -0.4272560477256775, 0.815260648727417, -0.026156950742006302, -0.4029349982738495, 0.39036640524864197, -0.4155619740486145, -0.009445864707231522, 0.6137576699256897, -1.3812413215637207, -0.8364907503128052, 0.355116605758667, -0.10138789564371109, -0.2315586805343628, -0.12710417807102203, 1.2919936180114746, 0.4328887462615967, -0.6168729066848755, 0.11873719841241837, -0.16386745870113373, 0.06798423081636429, -0.718468427658081, -0.6064432263374329, 1.3486729860305786, 0.5056115388870239, 0.08273905515670776, 0.31480297446250916, 0.08978447318077087, 0.5275869965553284, -0.8561829328536987, -0.023785846307873726, 0.871986985206604, -0.7341891527175903, -0.07000488042831421, -0.9282060265541077, -0.4318423867225647, 0.47558578848838806, 0.19088630378246307, -0.14398805797100067, 0.4106956720352173, 0.1967419981956482, -0.4902270436286926, -0.2200598418712616, -0.7872406840324402, -0.3530467450618744, 0.7445545196533203, -0.9874497056007385, -0.09094735234975815, 0.11195801198482513, 0.9932849407196045, -1.1691538095474243, -0.6160988211631775, -0.04525637626647949, -0.2212597131729126, 0.014558757655322552, 0.8807763457298279, -0.5393434166908264, 0.09068954735994339, 1.0664300918579102, -0.061365608125925064, -0.7509741187095642, 0.026086721569299698, -0.812665581703186, 0.03272789716720581, 0.23490355908870697, 0.6271317005157471, -0.5590358972549438, 0.15476597845554352, 0.7413148880004883, 0.0474180243909359, -0.5052180290222168, -0.6895413398742676, -0.17455673217773438, 0.17761856317520142, -0.5285894274711609, 0.4245375990867615, -0.2627108097076416, -0.6209657192230225, 0.2803577482700348, 0.3589218556880951, 0.6949401497840881, -0.35785090923309326, -0.7304129600524902, 0.2554960250854492, -0.22228999435901642, 0.01546492986381054, -0.3800029754638672, -0.4454944133758545, -1.2501485347747803, -0.11112990230321884, -1.1463850736618042, -0.25229620933532715, -0.9738397002220154, -0.36258774995803833, -0.41110140085220337, -0.27996155619621277, 0.10085761547088623, 0.7506240010261536, -0.2873166799545288, -0.42914366722106934, -0.4994431436061859, -0.19243952631950378, 0.6203328967094421, 0.6110725998878479, -0.25295501947402954, -0.07241301983594894, -0.003119439585134387, 0.27593186497688293, 0.39956486225128174, 0.44177356362342834, -0.5912002921104431, -1.1479945182800293, -1.4152348041534424, 0.5330812335014343, -0.1366254985332489, -0.28930601477622986, -0.6789938807487488, 0.34320950508117676, 0.34889185428619385, -0.5335057973861694, 0.21587812900543213, 0.26823610067367554, -0.6007717847824097, -0.3862150013446808, 0.5278924703598022, -0.7976402640342712, 0.40168389678001404, 0.4875093996524811, -0.8558663129806519, -0.011514966376125813, 0.7913803458213806, 0.1155882254242897, -0.7534184455871582, -0.444613516330719, 0.7877296209335327, -0.7372997403144836, 0.33642005920410156, -0.35360443592071533, -0.20155069231987, -0.49141114950180054, -0.2975446879863739, -0.020839108154177666, 0.16692869365215302, -0.3315143585205078, 1.1029367446899414, -0.11181221902370453, -1.0206127166748047, 0.10252115875482559, 0.44998595118522644, -0.513867974281311, 0.048293933272361755, 0.3371362090110779, 0.4793335795402527, -0.4341588318347931, 0.47860345244407654, 0.40934136509895325, 0.38694053888320923, -0.9564628601074219, 0.05676999315619469, 1.0040067434310913, -0.8868953585624695, 0.04030494764447212, 1.5816469192504883, -0.6644750237464905, -1.2547483444213867, -0.013506726361811161, -1.439224362373352, -0.30484795570373535, -0.29206007719039917, 0.265363484621048, -0.13173559308052063, -0.06709111481904984, 0.09498019516468048, -0.4674207270145416, 0.35001882910728455, 0.3410521149635315, -0.5599862337112427, 0.7334941029548645, -0.1750655174255371, -0.47836676239967346, 0.935837984085083, 0.6224536895751953, -0.3267466127872467, -0.5345828533172607, -0.8344852328300476, -0.5622283220291138, 0.023344412446022034, 0.4399043619632721, -0.0093959616497159, -0.8121410608291626, 0.8496569991111755, 1.0107276439666748, 0.518168568611145, 0.5229066014289856, -0.14257152378559113, 0.13804787397384644, 0.2539295554161072, 0.8774447441101074, -0.5354072451591492, -0.5440611839294434, 1.3425239324569702, 0.909823477268219, -0.6821883320808411, 0.3281063139438629, -0.11200112849473953, -0.6827022433280945, 0.45610013604164124, 0.07721517980098724, -0.09513889253139496, 1.257309079170227, 0.15186652541160583, -0.03246743604540825, 0.24447958171367645, -0.9155372977256775, -0.3920334279537201, 0.5686312317848206, 0.7238786816596985, 0.8323567509651184, 0.3391743004322052, 0.12404728680849075, 0.8358156085014343, -0.057260990142822266, 0.08024705201387405, 0.02249065414071083, 0.5766379833221436, -0.2824711501598358, -0.2796461284160614, 0.08380814641714096, 0.5304495692253113, -0.36141419410705566, -0.7943226099014282, 0.11917708814144135, 0.563765823841095, 0.28075385093688965, 0.5804031491279602, 1.0276594161987305, -0.14543631672859192, 0.7998037934303284, 0.20127931237220764, 0.5721574425697327, -0.642836332321167, -0.45987433195114136, -0.0810171589255333, -0.4736725091934204, 0.061900246888399124, -0.23491019010543823, -0.4386511743068695, -0.5537591576576233, -0.46864625811576843, 0.21463103592395782, 0.2784983515739441, 0.41928407549858093, 0.9657555222511292, 0.6615498661994934, 0.42868414521217346, 0.20568160712718964, -0.44902345538139343, -0.35663217306137085, -0.7017369866371155, 0.1755986213684082, -0.5880795121192932, -0.33746570348739624, -0.0862630158662796, -0.20815469324588776, -0.5851554274559021]}, "authors": [{"authorId": "2191455", "name": "Sheng Shen"}, {"authorId": "2158819969", "name": "Pete Walsh"}, {"authorId": "1732330", "name": "K. Keutzer"}, {"authorId": "34176020", "name": "Jesse Dodge"}, {"authorId": "39139825", "name": "Matthew E. Peters"}, {"authorId": "46181066", "name": "Iz Beltagy"}], "references": [{"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "eeb29f35b1a4e2cbe2c76e6c63df858194ed5d5d", "title": "GradMax: Growing Neural Networks using Gradient Information"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39", "title": "On the Transformer Growth for Progressive BERT Training"}, {"paperId": "53af14897ccca4f8d6c465ee133e859f99ddd7e7", "title": "Shallow-to-Deep Training for Neural Machine Translation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8771679aac0e90371340bd8c657317f5be113e81", "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3ab182265575c82f7d90b1e45a5c0731c6ed4443", "title": "Schedule"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a29c3bb07d478a354fd5bc5635f98560ede8f8bb", "title": "Splitting Steepest Descent for Growing Neural Architectures"}, {"paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b", "title": "Green AI"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "53c9443e4e667170acc60ca1b31a0ec7151fe753", "title": "Progressive Neural Networks"}, {"paperId": "02f1c79f1a09d9501cd39fc7b527cd9e13dff7ad", "title": "Network Morphism"}, {"paperId": "16cb6876666f3a7b56a636c1d85ad00bd0d98bf3", "title": "Net2Net: Accelerating Learning via Knowledge Transfer"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "bc51b02996e1ecbcbf1a0b3a0d28b628f133d433", "title": "Parameters"}, {"paperId": "d931f84abfc4550c10ceb113b142c8eb3e07571e", "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training"}, {"paperId": "2388d8c242c2def0a41d778046a0084baccea387", "title": "Scaling Laws"}, {"paperId": null, "title": "2020) derived empirical fits for the language model loss L as it relates to the total amount of compute C, number of non-embedding parameters N , number of gradient update steps S, and batch size B"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "34ae417121b4aeadc7c266efbd52b00b379e20c0", "title": "Knowledge Transfer"}, {"paperId": null, "title": "explains the potential negative compute we have"}, {"paperId": null, "title": "threshold gives us a good estimate of the OPTIMAL - ITY of the loss curve"}, {"paperId": null, "title": "total compute is given by C \u2248 6 NBS"}]}