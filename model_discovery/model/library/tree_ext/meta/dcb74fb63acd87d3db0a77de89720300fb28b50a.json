{"paperId": "dcb74fb63acd87d3db0a77de89720300fb28b50a", "title": "A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking", "abstract": "Vision Transformer (ViT) architectures are becoming increasingly popular and widely employed to tackle computer vision applications. Their main feature is the capacity to extract global information through the self-attention mechanism, outperforming earlier convolutional neural networks. However, ViT deployment and performance have grown steadily with their size, number of trainable parameters, and operations. Furthermore, self-attention's computational and memory cost quadratically increases with the image resolution. Generally speaking, it is challenging to employ these architectures in real-world applications due to many hardware and environmental restrictions, such as processing and computational capabilities. Therefore, this survey investigates the most efficient methodologies to ensure sub-optimal estimation performances. More in detail, four efficient categories will be analyzed: compact architecture, pruning, knowledge distillation, and quantization strategies. Moreover, a new metric called Efficient Error Rate has been introduced in order to normalize and compare models' features that affect hardware devices at inference time, such as the number of parameters, bits, FLOPs, and model size. Summarizing, this paper firstly mathematically defines the strategies used to make Vision Transformer efficient, describes and discusses state-of-the-art methodologies, and analyzes their performances over different application scenarios. Toward the end of this paper, we also discuss open challenges and promising research directions.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2023, "citationCount": 8, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The strategies used to make Vision Transformer efficient are mathematically defined, state-of-the-art methodologies are described, and their performances over different application scenarios are analyzed."}, "embedding": {"model": "specter_v2", "vector": [0.5149829387664795, 0.6926109194755554, -0.25600409507751465, 0.5915516018867493, -0.09365258365869522, 0.25506141781806946, 0.3467618227005005, -0.38296908140182495, -0.2311301827430725, -0.7583902478218079, 0.09552589803934097, 0.5322502255439758, 0.30339479446411133, -0.3418716490268707, -0.0786939412355423, -0.18132074177265167, -0.5387728214263916, -0.1470317840576172, 0.6537954807281494, -0.046810463070869446, 0.22017374634742737, -0.4778778553009033, -1.4239975214004517, -0.05764563009142876, 0.38388627767562866, 1.6339051723480225, 0.1889379769563675, 0.9992513656616211, -0.3228025436401367, 0.8314602375030518, 0.4932630658149719, -0.6081096529960632, 0.6244953274726868, -0.029398130252957344, 0.005854502785950899, 0.09194125980138779, 1.0888346433639526, -0.4714542329311371, -0.7711175680160522, 1.159144639968872, 0.19380010664463043, 0.0032189078629016876, 0.3123182952404022, -0.7833017110824585, -0.24771375954151154, -0.14460556209087372, 0.4982946813106537, 0.9250085353851318, -0.8554448485374451, -0.06449848413467407, 0.9414528608322144, -1.0544159412384033, -0.24343988299369812, 1.3087291717529297, 0.7039092779159546, 0.18145549297332764, -0.04933919757604599, -0.7193414568901062, 0.1370324343442917, 0.3951528072357178, -0.5535720586776733, -0.6913005709648132, 0.379246324300766, -0.25510022044181824, 1.6942859888076782, -0.7416435480117798, -0.026556605473160744, 0.3994249701499939, 0.4029683470726013, 1.3124642372131348, 0.4340059757232666, -0.773576021194458, 0.19676367938518524, -0.1827208399772644, 0.2561320960521698, 1.2417489290237427, -0.030126968398690224, 0.2872813642024994, -1.1623092889785767, 0.2701466381549835, 0.6888024806976318, -0.4787379205226898, 0.2543416917324066, -0.5668240189552307, 0.22046591341495514, 0.8720256090164185, 0.7278190851211548, 0.32911908626556396, -0.23609435558319092, 0.972233772277832, 0.49251919984817505, 0.11294245719909668, -0.22840286791324615, 0.3598138093948364, 0.29542478919029236, 0.8195701837539673, -0.8881954550743103, -0.2617122232913971, -0.5035538673400879, 0.9724845886230469, -0.009837293066084385, 0.9579354524612427, -0.44948241114616394, 0.38435328006744385, 1.1251198053359985, 0.35508307814598083, 0.5009936690330505, -0.7349607348442078, -0.06631916761398315, -0.5067048668861389, -0.15353737771511078, -0.6188247203826904, 0.19987176358699799, -0.5586375594139099, -1.1245269775390625, -0.5636674761772156, -0.6012856364250183, 0.5713789463043213, -1.371238112449646, 0.09546332061290741, -0.6773950457572937, 0.2628418803215027, -0.013731078244745731, 0.45649200677871704, 0.4113337993621826, 0.08621510118246078, 0.0942360982298851, 0.10616914927959442, 1.4230586290359497, -0.9740385413169861, -0.47984546422958374, -1.0488524436950684, -0.6439611911773682, -0.3532472252845764, -0.06289646774530411, 0.12893898785114288, -1.284462571144104, -1.5039259195327759, -0.9224650859832764, -0.23694352805614471, -0.2724490165710449, 0.3770166039466858, 1.0975929498672485, 0.4697229266166687, -1.3241468667984009, 0.500033974647522, -0.20572227239608765, -0.22721020877361298, 0.7099428772926331, 0.05410751327872276, 0.7023652195930481, -0.04578455165028572, -0.5341981649398804, 0.38542231917381287, 0.027096766978502274, -0.3703575134277344, -0.4750717282295227, -0.017548728734254837, -1.0842584371566772, 0.05148664489388466, 0.24457737803459167, -0.9883997440338135, 1.0440229177474976, -0.5501779913902283, -1.083880066871643, 0.6038162708282471, -0.38275620341300964, -0.4648401737213135, -0.005095384549349546, -0.024425353854894638, -0.17796091735363007, 0.14755038917064667, -0.15525874495506287, 0.5738589763641357, 1.524247646331787, 0.11006394028663635, -0.5977339148521423, 0.19024813175201416, -0.35448598861694336, -0.2881275415420532, -0.19752459228038788, 0.8087653517723083, -1.1988458633422852, -0.397818386554718, 0.9277635216712952, 0.6781920194625854, -0.11136971414089203, -0.1287006437778473, -0.04860202223062515, -0.7658030986785889, 0.5828030109405518, 0.17473842203617096, 0.670250654220581, -0.7434535026550293, -1.1730743646621704, -0.1628275364637375, 0.03305763006210327, 0.03821039944887161, -0.7593353986740112, -0.0048273405991494656, -0.3290638327598572, -0.2809382379055023, 0.18791119754314423, -0.8618927597999573, -0.06304439157247543, -0.4344545602798462, -0.93849116563797, 0.11739402264356613, 0.3747263550758362, 1.1557648181915283, -0.5144239068031311, -0.2502976953983307, 0.372489869594574, 0.20343253016471863, -0.7749283909797668, 0.9298052191734314, -0.11932140588760376, -0.29704609513282776, -0.12368369102478027, 0.41416165232658386, 0.32046595215797424, -0.4575282037258148, 0.45306286215782166, -0.8908624649047852, -0.007518102880567312, 0.3332638442516327, -0.20525924861431122, 1.1446534395217896, -0.13423387706279755, 1.036673665046692, -0.2502872049808502, -0.6521108150482178, 0.5486522912979126, 0.18306639790534973, -0.19012567400932312, -1.1622835397720337, 0.5872701406478882, -0.02801772952079773, -0.7522693276405334, 0.2506188154220581, 0.5371941924095154, 1.3854999542236328, 0.0956210345029831, -0.28260380029678345, 1.0781972408294678, -0.5693243145942688, 0.13958166539669037, -0.0383400022983551, 0.6155180335044861, 0.22795945405960083, 0.2973594069480896, -0.1458396464586258, 0.2131313979625702, -0.7309512495994568, 0.1674349308013916, 0.8473016023635864, 0.2727114260196686, 1.0386875867843628, 0.22174446284770966, -0.7765073776245117, -0.6025828123092651, -0.2851869761943817, 0.5371848940849304, 0.9930988550186157, 0.06655394285917282, 0.08175478130578995, -0.8952560424804688, -0.07453423738479614, -0.8607057332992554, -0.5021429061889648, -0.08806117624044418, -0.22864972054958344, -0.1926518678665161, -0.9344009160995483, 0.9280256032943726, 0.1679530143737793, 1.8267840147018433, -0.4256129860877991, -0.8459310531616211, -0.32610824704170227, 0.20868967473506927, -0.8973747491836548, -0.3354330360889435, 0.49307531118392944, -0.6661706566810608, -0.7883583903312683, -0.13829119503498077, -0.27750086784362793, 0.12725608050823212, -0.26127898693084717, 0.9274814128875732, -0.8190514445304871, -0.7376614212989807, 0.21471284329891205, 0.7823341488838196, -0.8202766180038452, 0.021441997960209846, -0.279668390750885, -0.13445132970809937, 0.02821819670498371, 0.49982473254203796, 0.2012929469347, -0.19508489966392517, -0.10777533054351807, -0.11061536520719528, -0.3683594763278961, 0.4198214113712311, 0.23024843633174896, 1.0479077100753784, -0.006786175072193146, -0.09797085076570511, -0.5650264620780945, 0.7647286653518677, 0.6372078061103821, -0.819261908531189, -0.07061345875263214, -0.9806593060493469, -0.007961364462971687, 0.20895665884017944, -0.3551362156867981, -0.07325246930122375, -0.4579489231109619, 0.49145859479904175, -1.2339799404144287, -0.14238320291042328, -0.9284918308258057, 0.5299463272094727, -0.662843644618988, 0.43896472454071045, 0.5006014108657837, 0.22570228576660156, 0.29022806882858276, 0.32275354862213135, -0.7236276865005493, 1.045100450515747, 0.4470982253551483, 0.1693785935640335, 0.05822011083364487, 0.028390346094965935, -0.6768096685409546, -0.5553343892097473, -0.4338751435279846, 0.03611557558178902, -0.45727571845054626, 0.5657346844673157, -0.8245435357093811, -0.840591311454773, 0.24149776995182037, -0.5578033328056335, 0.017499301582574844, -0.11297392100095749, -0.13856558501720428, -0.6248897314071655, -0.8953313231468201, -0.7082864046096802, -0.695846676826477, -1.2040326595306396, -1.324029564857483, 0.31208565831184387, 0.5812928676605225, 0.3491247594356537, -0.07535470277070999, -0.21213741600513458, -0.4884208142757416, 1.167604684829712, -0.5493919253349304, 0.3290865421295166, 0.11088064312934875, -0.7448500990867615, 0.3005470335483551, -0.3240950107574463, 0.5130198001861572, -0.4108869135379791, 0.01040730532258749, -1.5373531579971313, 0.33712005615234375, -0.05933087691664696, -0.3180806040763855, 0.8104034066200256, 0.6014124155044556, 0.5379500389099121, 0.07518389075994492, -0.08186186105012894, 0.8754607439041138, 1.6084833145141602, -0.566358208656311, 0.5403092503547668, 0.2619137465953827, 0.5712753534317017, -0.09619288891553879, -0.141391783952713, 0.20217131078243256, 0.14849688112735748, 0.3142348527908325, 0.5981038212776184, -0.3185131251811981, -0.712947428226471, -0.39535683393478394, 0.20731870830059052, 0.7495145797729492, 0.3493194878101349, 0.2173483669757843, -0.4498797655105591, 0.8100135326385498, -0.7092427015304565, -0.6747968792915344, 0.4787750542163849, 0.5920671820640564, -0.020103955641388893, 0.10619653016328812, -0.533919095993042, -0.07070057839155197, 0.5644068121910095, 0.588416337966919, -0.5482202768325806, -0.6417283415794373, -0.28336361050605774, 0.9801585078239441, 0.9875633716583252, 0.16276895999908447, -0.6991403102874756, 0.5518391132354736, 14.809441566467285, 0.8404634594917297, -0.38299447298049927, 0.3275894224643707, 0.5040191411972046, 0.5837869644165039, 0.3120071291923523, 0.21234631538391113, -0.8741809129714966, -0.2451753467321396, 0.6086455583572388, 0.49265289306640625, 0.41777220368385315, 0.6766618490219116, -0.4861617982387543, -0.18188799917697906, -0.2111886739730835, 1.302333116531372, 0.9438346028327942, -1.3536789417266846, 0.11538174003362656, 0.10273203253746033, 0.22354476153850555, 0.6253361701965332, 0.8095754981040955, 0.48685070872306824, 0.3948802649974823, -0.13440415263175964, 0.45252957940101624, 0.4240567982196808, 0.976453423500061, 0.04660644382238388, 0.1532205194234848, -0.01706637442111969, -1.1647696495056152, -0.18304286897182465, -0.940368115901947, -0.7004573345184326, -0.4156665802001953, 0.09704390913248062, -0.4198424220085144, -0.32517024874687195, 0.23215903341770172, 0.32410451769828796, -0.4941418170928955, 0.8100780248641968, -0.3860725462436676, 0.6114475131034851, -0.24505040049552917, 0.2712026536464691, 0.198895663022995, 0.27679160237312317, 0.054239291697740555, -0.28926509618759155, -0.06955576688051224, -0.027766456827521324, -0.03899591043591499, 0.35841670632362366, -0.1931922882795334, -0.7094656229019165, -0.3817480504512787, -0.10738303512334824, -0.2693772315979004, 0.9399080276489258, 0.22281286120414734, 0.4261571764945984, 0.14517241716384888, 0.24615779519081116, 0.27630615234375, 0.11181456595659256, -0.43613967299461365, -0.2679777443408966, 0.3821101486682892, -0.6405244469642639, 0.5882863402366638, 0.6336070895195007, -0.27781859040260315, -0.4538096785545349, -0.7240170836448669, -0.4981934130191803, 0.49009969830513, -1.317272424697876, -0.3265707492828369, 0.9906306266784668, -0.47913509607315063, -0.32365068793296814, 0.3634118437767029, -1.1389154195785522, -0.4116021990776062, 0.22062020003795624, -1.6881754398345947, -0.8187751770019531, -0.34596139192581177, -0.26729580760002136, -0.018193988129496574, -0.6383904814720154, 0.7983792424201965, -0.29764044284820557, -0.12690117955207825, 0.265448659658432, -0.35434526205062866, 0.005058566108345985, -0.11723631620407104, -0.4338288903236389, 0.6934388279914856, 0.8357958197593689, 0.15250267088413239, -0.08649730682373047, -0.013837919570505619, 0.43415766954421997, -0.9567435383796692, -0.15474475920200348, 0.047274891287088394, -0.11423305422067642, -0.4350475072860718, -0.37533578276634216, -0.5386611223220825, 0.04211178421974182, 0.6369315981864929, 0.04193701967597008, -0.4784187376499176, -0.31199535727500916, -0.758601725101471, -0.37538760900497437, -0.7416701912879944, -0.09623853117227554, 0.22366207838058472, -0.918001651763916, -0.3292859196662903, -0.4560486376285553, -0.08523932099342346, -0.8929935097694397, -0.15476509928703308, 0.06768578290939331, 0.018413355574011803, -0.7529272437095642, 1.3227126598358154, 0.3271932899951935, 0.2317962497472763, 0.5700269341468811, -0.1086636483669281, -0.30758243799209595, -0.1195106953382492, -0.8267682790756226, 0.0085770757868886, 0.1408049315214157, -0.006050090305507183, -0.8029136061668396, 0.5405581593513489, 0.20823964476585388, 0.12121708691120148, -0.15809454023838043, -0.8413302302360535, -0.02328462339937687, -0.6005774140357971, -0.4646472632884979, 0.04078806936740875, -0.5513850450515747, -0.5372235774993896, 0.1502562165260315, 0.46910154819488525, 0.5799194574356079, 0.5097813010215759, -0.1403149515390396, 0.4751996397972107, -0.08009718358516693, -0.15566033124923706, -0.4418710470199585, -0.9341131448745728, -1.5031448602676392, -0.2750277519226074, -0.7335134148597717, -0.16333046555519104, -1.0401426553726196, -0.7610715627670288, 0.11518421769142151, -0.191299706697464, -0.08990269154310226, 0.6389762163162231, 0.21252849698066711, -0.298102468252182, -0.3032425343990326, -0.5778519511222839, 0.5750231742858887, 0.7219696640968323, -0.9553546905517578, 0.22078688442707062, -0.34096527099609375, 0.01787293143570423, 0.8447055220603943, 0.4557107090950012, -0.23735938966274261, -0.8027474284172058, -1.0964932441711426, 0.13640347123146057, -0.2565171718597412, -0.019687343388795853, -1.3649299144744873, 1.3021115064620972, 0.5766351222991943, 0.25202035903930664, -0.268675833940506, 0.5363988280296326, -0.8920620679855347, -0.5101750493049622, 0.29183143377304077, -0.2639043629169464, -0.3208995461463928, 0.3978688418865204, -0.4199466109275818, -0.3881142735481262, 0.497028648853302, 0.5520783066749573, -1.0203572511672974, -1.352964162826538, 0.35366684198379517, -0.23550376296043396, 0.31191781163215637, 0.031463075429201126, -0.6591992974281311, -1.3875291347503662, -0.18046078085899353, 0.04260033369064331, 0.21726076304912567, -0.5115931034088135, 0.7702579498291016, 0.9111661314964294, -0.9079049229621887, 0.35908564925193787, 0.8524369597434998, -0.11149945855140686, 0.010487822815775871, 0.3384326100349426, 0.4834480583667755, -0.4413546919822693, 0.5794938206672668, -0.2184346318244934, -0.013207823969423771, -0.37428244948387146, 0.16880099475383759, 1.328128695487976, -0.2819928526878357, -0.08746524155139923, 0.9279913902282715, -0.0396755114197731, -0.25814568996429443, 0.32728469371795654, -1.1079261302947998, -0.33258819580078125, -0.11807309091091156, 0.22016341984272003, 0.1910441666841507, 0.12561000883579254, 0.23336900770664215, -0.25415462255477905, 0.5822575688362122, 0.1194460391998291, -0.41633155941963196, 0.07558652013540268, 0.0051193032413721085, -0.062031738460063934, 0.07990774512290955, 0.3666716516017914, -0.8184122443199158, -1.2737908363342285, -0.8360534310340881, -0.528319776058197, -0.3372557759284973, 0.6849185228347778, -0.08642303943634033, -1.106550931930542, 0.879762589931488, 1.1301453113555908, 0.17913277447223663, 0.9585605263710022, -0.3245190978050232, -0.10559500008821487, 0.7108389735221863, 0.23683013021945953, -0.8201006650924683, -0.23809179663658142, 1.063071608543396, 0.9528717398643494, -0.7378908395767212, 0.33307304978370667, -0.2691757380962372, -0.41416460275650024, 0.8558444380760193, 0.3845135271549225, -0.5347849130630493, 0.690912663936615, -0.2272527664899826, -0.046062394976615906, 0.1464301496744156, -0.7064350843429565, -0.9558531641960144, 0.9508172869682312, 1.4041657447814941, 0.198044091463089, -0.40390658378601074, 0.8117313981056213, 0.5147938132286072, 0.01828623004257679, 0.1429377943277359, 0.3849720060825348, 0.10916458815336227, -0.3570203185081482, 0.4910973608493805, -0.4037426710128784, 0.6316511631011963, -0.6109939813613892, -0.7179141044616699, 0.21543432772159576, 0.8170448541641235, 0.37908849120140076, 0.7829288244247437, 1.254289984703064, -0.034172557294368744, 0.7658523321151733, -0.10067960619926453, 0.8382875919342041, -0.14199846982955933, -0.06295856833457947, 0.39282289147377014, -0.6747856140136719, -0.4194897413253784, -0.33681583404541016, -0.3211831748485565, 0.1714024394750595, 0.1019705981016159, -0.04007304832339287, -0.3753754794597626, 0.5710339546203613, 0.43057769536972046, 0.5732294321060181, 0.7426909804344177, -0.32076185941696167, -0.45895761251449585, -0.3270517587661743, -0.9392737746238708, 0.33551955223083496, -0.9633502960205078, -0.13157989084720612, -0.3772212564945221, -0.09952063858509064, -0.009024705737829208]}, "authors": [{"authorId": "2164141997", "name": "Lorenzo Papa"}, {"authorId": "2047330818", "name": "Paolo Russo"}, {"authorId": "1702698", "name": "Irene Amerini"}, {"authorId": "2237944863", "name": "Luping Zhou"}], "references": [{"paperId": "c85268696fe1435605ae66a18653cfdcf8153753", "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"}, {"paperId": "9c53679e7c2107e36e6c60c19464c607bf3459cd", "title": "EfficientViT: Lightweight Multi-Scale Attention for High-Resolution Dense Prediction"}, {"paperId": "ed8ac4ff13d32a291bbe74f3e5a138800bba47fd", "title": "Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks"}, {"paperId": "07be590365e7fb76680be4ed67a5505763ec2d96", "title": "Boost Vision Transformer with GPU-Friendly Sparsity and Quantization"}, {"paperId": "362e7184fcec5e3daf52cf5fb993799e457ba1c5", "title": "Supervised Masked Knowledge Distillation for Few-Shot Transformers"}, {"paperId": "e60b6836b45ad0ae02a5fa663c8c31119f0c0a94", "title": "X-Pruner: eXplainable Pruning for Vision Transformers"}, {"paperId": "f35016b3180808fa97d59acbdecf47d6e2ed2819", "title": "Rethinking Vision Transformers for MobileNet Size and Speed"}, {"paperId": "8b87d39baf53d982bad7df8ab6c5c8e67c124c67", "title": "NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers"}, {"paperId": "977351c92f156db27592e88b14dee2c22d4b312a", "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference"}, {"paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f", "title": "Token Merging: Your ViT But Faster"}, {"paperId": "b5e71069f091d52f474a2928ed07b6546157af82", "title": "Towards Accurate Post-Training Quantization for Vision Transformer"}, {"paperId": "4a78e0616011314cb89e2e8d2cb1cbc996b604b1", "title": "Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration"}, {"paperId": "2475b38a76a9c2dc67f74446e2e686815764b0f2", "title": "EcoFormer: Energy-Saving Attention with Linear Complexity"}, {"paperId": "ec139916edd6feb9b3cb3a0325ca96e21dbb0147", "title": "Hydra Attention: Efficient Attention with Many Heads"}, {"paperId": "f27c847e2909f30745f4a3528b574f5acfd76ea7", "title": "Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization"}, {"paperId": "2fe71acc2c3f1e75b6149dea72838f0b594ad013", "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"}, {"paperId": "968f628c3d42dbfd16fd4516e61cfedc16612310", "title": "Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks"}, {"paperId": "d451901a6a12c61179289cac7a4588a86c234112", "title": "Width & Depth Pruning for Vision Transformers"}, {"paperId": "1966c4df2cda0fb8daf7f36366d909a021b6d5c1", "title": "SimA: Simple Softmax-free Attention for Vision Transformers"}, {"paperId": "c431408780586268e8bcf2483b01a80728d10960", "title": "Vision Transformer Adapter for Dense Predictions"}, {"paperId": "3a1dbfb6875bfac8251627d60db313623fbb8b04", "title": "DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers"}, {"paperId": "58c486ad4020177f5ed3d9f2883f3fc327b55770", "title": "MiniViT: Compressing Vision Transformers with Weight Multiplexing"}, {"paperId": "259c681c76335540e13081efad584efdf9101868", "title": "DaViT: Dual Attention Vision Transformers"}, {"paperId": "7bffc157b3b3626a3912a3b0ef74ce5904630fce", "title": "FlowFormer: A Transformer Architecture for Optical Flow"}, {"paperId": "75c642ebdcfbd4c16d6c3161130b72ff9af5c311", "title": "Three things everyone should know about Vision Transformers"}, {"paperId": "daee1a8ee8fc6b73812ae854716a87d5db962716", "title": "Neural Window Fully-connected CRFs for Monocular Depth Estimation"}, {"paperId": "c4d65ecf0f5dd8d84f1a2c395527164068aa19e4", "title": "DropNAS: Grouped Operation Dropout for Differentiable Architecture Search"}, {"paperId": "722d71a19e4049b30a03d1028158881560432135", "title": "SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning"}, {"paperId": "c2a0c18e810535db52e5ebaf180c64ce70356748", "title": "A-ViT: Adaptive Tokens for Efficient Vision Transformer"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "39a620939887c9fc1f9bdd7ecfabde985a4aad3a", "title": "PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "c051ee2ad7ac203a26fa8f50eb6312424c729b27", "title": "Global Vision Transformer Pruning with Hessian-Aware Saliency"}, {"paperId": "c723187a2230749b1e706df2217e928c8271a660", "title": "Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "e43eaeca5077d01061a38aebd24f8e3fa5948ad9", "title": "Co-advise: Cross Inductive Bias Distillation"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "33fd56e5067a1e8a9713378af3e1c1c08d5ce93b", "title": "Patch Slimming for Efficient Vision Transformers"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "03db529f0bfae6d0b64b0feef565196327fe8d50", "title": "Intriguing Properties of Vision Transformers"}, {"paperId": "5e4f03f68c6867d850f457dc5cc36738e5dff6c1", "title": "Vision Transformers are Robust Learners"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "cc9f3a61ea4eaabf43cbb30cd1dd718074932679", "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "93efaf8c27940aaef145d8bcbca957be634d26e5", "title": "Vision Transformer Pruning"}, {"paperId": "43e51c1bfd69df518e2907f7a955e485985ba423", "title": "On the Robustness of Vision Transformers to Adversarial Examples"}, {"paperId": "fbd730a948a06cd4918c1d632ffdb4572b52d99b", "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "d40c77c010c8dbef6142903a02f2a73a85012d5d", "title": "A Survey on Vision Transformer"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "0c7701fc6129ddc86b2dcce65d5188332eb84d1f", "title": "Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization Framework"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "0dd89a3b906e9a90a47834dcc766a0b9967d10d1", "title": "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "3827ecf84bc0429b73d9c57a6b2b55e723d4cfba", "title": "Dynamic Graph Message Passing Networks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "a84906dbd4d6640f918d0b6ed2a7313dda0d55f1", "title": "Panoptic Feature Pyramid Networks"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "04957e40d47ca89d38653e97f728883c0ad26e5d", "title": "Cascade R-CNN: Delving Into High Quality Object Detection"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "90a16f34d109b63d95ab4da2d491cbe3a1c8b656", "title": "Learning Efficient Convolutional Networks through Network Slimming"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "4a73a1840945e87583d89ca0216a2c449d50a4a3", "title": "Deformable Convolutional Networks"}, {"paperId": "0479e17998429a368eaac38940314f1d5be01f20", "title": "Construction of Information Ecosystem on Enterprise Information Portal (EIP)"}, {"paperId": "1a327709cc53ff9e52454e50a643abf4a0ac92af", "title": "Findings of the 2016 Conference on Machine Translation"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "7b58ba0c8571f36226060577e4640d05b5675e62", "title": "Stochastic Local Search: Foundations & Applications"}, {"paperId": "c8e8b54f87f43c4a6f85695712dff55e0edec760", "title": "A simple procedure for pruning back-propagation trained neural networks"}, {"paperId": "a116191ec44687336518f37407329ef3b886dc93", "title": "Pruning versus clipping in neural networks."}, {"paperId": "7925339228d5abad97d3a644ed19054d7095a63e", "title": "Orthogonal Transformer: An Efficient Vision Transformer Backbone with Token Orthogonalization"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "and the M.S. degree in Artificial Intelligence and Robotics from University of Rome La Sapienza, Italy"}, {"paperId": "2826f9dccdcceb113b33ccf2841d488f1419bb30", "title": "Stanford Neural Machine Translation Systems for Spoken Language Domains"}, {"paperId": "68d54f9dacbb5416c1aafb3399c072497c320021", "title": "Network Flows: Theory, Algorithms, and Applications"}, {"paperId": null, "title": "Her main research activities include digital image processing, computer vision multimedia forensics"}, {"paperId": null, "title": "been a researcher at Italian Institute of Technology (IIT) in Tourin, Italy"}, {"paperId": null, "title": "main research interests are"}, {"paperId": null, "title": "and the IAPR TC6 - Computational Forensics Committee"}, {"paperId": null, "title": "EER Computation: Subsequently, once each reference value for each parameter has been identified, instead of considering each of the previous metrics independently"}]}