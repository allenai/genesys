{"paperId": "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d", "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory", "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies. Our code can be found in \\url{https://github.com/thunlp/InfLLM}.", "venue": "", "year": 2024, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences."}, "embedding": {"model": "specter_v2", "vector": [0.24379701912403107, 0.34994202852249146, -0.30177077651023865, -0.249653160572052, -0.22346065938472748, -0.3478674292564392, 0.7744438052177429, -0.041589997708797455, -0.9244884848594666, -0.14659926295280457, 0.31599006056785583, 0.023031633347272873, 0.6977241039276123, 0.4165770411491394, -0.4302312731742859, 0.07240080833435059, -1.4181907176971436, 0.1305796355009079, 0.05961316078901291, -0.19237421452999115, -0.029805462807416916, -0.5558125376701355, -0.7774882912635803, 0.20874707400798798, 0.3180720806121826, 0.10610326379537582, 0.6012617349624634, 1.214453101158142, -0.2824154794216156, 0.6551576256752014, 0.15951891243457794, -0.07287203520536423, 0.14353960752487183, 0.006746574304997921, -0.4887126386165619, -0.23700669407844543, 0.33918845653533936, -0.5863447785377502, -0.5121387243270874, 0.48109087347984314, -0.16805113852024078, 0.5178618431091309, 0.08960243314504623, -0.4040226936340332, -0.30039864778518677, 0.998092532157898, 0.623909056186676, 0.6545804738998413, -0.15457433462142944, -0.2786746025085449, 1.057449460029602, -1.1753698587417603, 0.2486177384853363, 1.5637036561965942, 0.47974053025245667, 0.6341167092323303, 0.06531134247779846, -0.46464043855667114, 1.4606499671936035, 0.30544179677963257, -0.4659960865974426, -0.3969266414642334, -0.04275309294462204, 0.07941246777772903, 2.044776678085327, -0.5171697735786438, 0.34016120433807373, 1.0556408166885376, 0.13351890444755554, 1.7236130237579346, -0.4457146227359772, -1.1478673219680786, -0.13134410977363586, -0.1260615587234497, 0.27623826265335083, 0.6017752885818481, -0.6650491952896118, 0.36009877920150757, -0.8145900964736938, 0.012314734980463982, 0.5534188747406006, 0.037833936512470245, 0.2806564271450043, 0.20661528408527374, -0.3028563857078552, 0.8104560971260071, 0.34907424449920654, 0.7331027984619141, -0.03104572370648384, 0.6570642590522766, 0.6558407545089722, 0.3055570423603058, 0.0941033810377121, 0.1367357224225998, -0.07219286262989044, 0.18597806990146637, -0.8268095254898071, 0.24611517786979675, -0.20858044922351837, 0.9316310286521912, -0.3496297299861908, 0.2977910041809082, -0.4563787281513214, 0.3331888020038605, 1.2622421979904175, 0.05170890688896179, 0.7976604700088501, -0.7719608545303345, 0.18061017990112305, -0.7399482131004333, 0.2805691957473755, -0.32866784930229187, -0.3242439329624176, -0.3687575161457062, -0.2069966346025467, -1.0839829444885254, -0.292649507522583, -0.0564238578081131, -0.4518027901649475, 1.1765353679656982, -0.035041093826293945, 0.498881071805954, 0.2866609990596771, 0.48770400881767273, 0.28168147802352905, 0.655905544757843, 0.4964103400707245, -0.10462940484285355, 0.5502786636352539, -1.3171544075012207, -0.5164310932159424, -1.6749982833862305, 0.6825247406959534, 0.014500194229185581, 0.6133981943130493, -0.32973238825798035, -1.2425025701522827, -0.9690649509429932, -1.057405948638916, 0.11341673880815506, -0.5865881443023682, 0.11924710124731064, 1.0845773220062256, -0.12552201747894287, -0.6291157007217407, 0.9793297648429871, -0.4325503706932068, -0.00955906230956316, -0.01712310127913952, 0.0917004868388176, 0.3060322701931, -0.3893537223339081, -1.5110526084899902, 0.45890071988105774, 0.40339016914367676, -0.22879479825496674, -0.3206803798675537, -0.28291112184524536, -1.3596488237380981, -0.38114383816719055, 0.4030337333679199, -0.18481718003749847, 1.60894775390625, -0.5886765718460083, -1.3022539615631104, 0.3946084678173065, -0.3340442180633545, -0.17734524607658386, 0.3009975254535675, -0.4480851888656616, -0.6011261343955994, -0.23039352893829346, -0.3355848789215088, 0.8808031678199768, 0.4614587128162384, -0.013890172354876995, -0.4133688807487488, -0.02158154547214508, -0.1798335611820221, 0.08844029158353806, -0.15869148075580597, 0.6489378809928894, -0.22115272283554077, -0.3666912615299225, -0.18576037883758545, 0.41286423802375793, -0.048203617334365845, -0.8195629715919495, -0.032265253365039825, -1.2447947263717651, 0.5643086433410645, 0.12319289147853851, 0.9962744116783142, -0.7238242626190186, -0.8869156837463379, -0.6853877305984497, -0.47679033875465393, 0.13529330492019653, -1.1342648267745972, 1.0213583707809448, -0.0888616293668747, 0.06218070909380913, -0.017639854922890663, -1.3717989921569824, 0.11978290230035782, -0.06314250826835632, -0.5909785032272339, -0.18870776891708374, 0.16317707300186157, 1.1086459159851074, -1.270409345626831, 0.01346647460013628, 0.058144111186265945, 0.22220174968242645, -0.9417871236801147, 1.2393672466278076, -0.8151548504829407, 0.3452762961387634, 0.1596265584230423, -0.5349191427230835, -0.18499894440174103, -0.3289385437965393, 0.6471550464630127, 0.15623348951339722, -0.2672708034515381, 0.41462478041648865, -0.2495158165693283, 1.7674567699432373, -0.5916566252708435, 0.7910642027854919, -0.4045480787754059, -0.23200105130672455, -0.13148212432861328, 0.2100854068994522, -0.44560205936431885, -0.3067818582057953, 0.174683079123497, 0.2998032867908478, -0.6494658589363098, -0.20042647421360016, 0.7382975220680237, 0.8345865607261658, -0.339961975812912, 0.04083780571818352, 0.41062265634536743, 0.3415001630783081, 0.5785917639732361, 0.5648961663246155, 0.34024131298065186, 0.7212533354759216, 0.3382061719894409, 0.17305533587932587, 0.34334465861320496, -0.9866640567779541, -0.15252815186977386, 0.45806998014450073, 0.6622856855392456, 0.43321746587753296, -0.05435524135828018, -1.0506929159164429, -0.32568031549453735, 0.39015355706214905, 0.6174558997154236, 1.766182780265808, -0.02581152692437172, -0.04615766555070877, -1.0505609512329102, -0.19596454501152039, -0.4265346825122833, 0.28346407413482666, -0.38490092754364014, 0.04911574348807335, -0.7734349966049194, -0.6467422246932983, 0.7910378575325012, 0.3588728606700897, 0.8345465660095215, -1.1238007545471191, -0.8010105490684509, 0.2663252651691437, 0.08629225939512253, -0.9363648295402527, -1.052762508392334, 0.01499225851148367, -0.4444054067134857, -0.13246826827526093, 0.16086776554584503, -0.4083889424800873, -0.13093355298042297, -0.7874287366867065, 0.9510245323181152, -0.4382179081439972, -0.19764670729637146, -0.00496393209323287, 0.47428077459335327, -0.7375244498252869, -0.6439109444618225, 0.3302229642868042, 0.37224280834198, -0.20862458646297455, 0.48266083002090454, 0.461650013923645, -0.22153910994529724, -0.34620749950408936, -0.32079577445983887, 0.20044995844364166, 0.23924100399017334, -0.3603796660900116, 0.5800143480300903, -0.7532345652580261, 0.867327094078064, -0.9927351474761963, 0.7137301564216614, 0.24782374501228333, -0.4762501120567322, 0.23270344734191895, -0.5689066648483276, -0.39171379804611206, 0.5667361617088318, -1.2293334007263184, -0.23506221175193787, -0.7429563999176025, 0.0019941115751862526, -0.3796103894710541, -0.4872264862060547, 0.10730844736099243, 0.36831262707710266, 0.3830339312553406, 0.1715417504310608, 0.5160811543464661, 0.09652858972549438, -0.2599153518676758, 0.4509066641330719, -0.6869503259658813, 0.7984672784805298, 0.47997531294822693, -0.5944132208824158, -0.28956174850463867, -0.30162331461906433, -0.9422937631607056, -0.5341589450836182, -0.3385089039802551, -0.45056796073913574, -0.5260313749313354, -0.11077846586704254, -0.5080522894859314, -1.2581686973571777, 0.03704221546649933, -1.0671191215515137, -0.8272191286087036, 0.29480552673339844, 0.04715878143906593, -0.4614495038986206, -0.7225602269172668, -1.3718571662902832, -0.8618707060813904, -0.4800782799720764, -0.7208850383758545, 0.045712247490882874, 0.15616874396800995, -0.5719456672668457, -0.9639138579368591, 0.3489830195903778, -0.7074947953224182, 0.7285981774330139, -0.451992392539978, 0.6184852719306946, -0.022587928920984268, -0.1485060453414917, -0.3579949140548706, 0.735805332660675, 0.17865800857543945, -0.27245232462882996, 0.06208677589893341, -1.1271631717681885, -0.22424723207950592, -0.46914398670196533, -0.39183303713798523, 0.09027332812547684, 0.5704652070999146, 0.4323577582836151, -0.2759506106376648, -0.44578900933265686, 0.26606953144073486, 1.3513970375061035, -0.35992708802223206, -0.22519822418689728, 0.2119462490081787, 0.8249685168266296, 0.29697415232658386, -0.020039603114128113, 0.7397149205207825, 0.5111867189407349, 0.16086219251155853, 0.33533793687820435, 0.10639810562133789, -0.07745034247636795, -0.9451836943626404, 0.8023483753204346, 1.6800401210784912, 0.35588619112968445, -0.08214025944471359, -0.733576238155365, 0.7815905213356018, -1.5215409994125366, -0.8604428172111511, 0.9488602876663208, 0.8478981256484985, 0.5442681908607483, -0.3906612694263458, 0.04336106404662132, -0.5451497435569763, 0.40568435192108154, 0.395321249961853, -0.9607208967208862, -0.8931419253349304, -0.051477398723363876, -0.28088808059692383, -0.3391450345516205, 1.0695164203643799, -0.45057299733161926, 0.7048191428184509, 14.659305572509766, 0.7361044883728027, 0.0211392380297184, 0.44601738452911377, 0.5429474115371704, -0.13588428497314453, -0.03249228745698929, -0.3792315125465393, -1.4730042219161987, 0.010960845276713371, 1.533367395401001, 0.40501776337623596, 0.9050911664962769, -0.020430676639080048, 0.2723070979118347, 0.11749545484781265, -1.0840822458267212, 0.7634336948394775, 0.7059290409088135, -1.1422569751739502, 0.2929951548576355, -0.07052978873252869, 0.34956446290016174, 0.6544113159179688, 0.7702190279960632, 0.9261848330497742, 0.48310521245002747, -0.2931577265262604, 0.7709643244743347, 0.348888635635376, 0.9466911554336548, -0.3253660500049591, 0.2441837638616562, 1.063637614250183, -0.7597660422325134, -0.3277057409286499, -0.720930814743042, -1.2869397401809692, 0.22565162181854248, -0.2944749891757965, -0.37614595890045166, -0.5986436605453491, -0.29084622859954834, 0.762976884841919, 0.041364412754774094, 0.23123939335346222, -0.10088873654603958, 0.5837259888648987, 0.12195762991905212, 0.03695887327194214, 0.8190709352493286, 0.3853593170642853, 0.27764785289764404, 0.4016095697879791, -0.16168458759784698, 0.042284414172172546, 0.2763688564300537, 0.20175771415233612, -0.38731318712234497, -0.11771833151578903, -0.2883143723011017, -0.09055361896753311, 0.33804798126220703, 0.42453476786613464, 0.7345314621925354, 0.24504344165325165, -0.3419072926044464, 0.24939824640750885, 0.7819881439208984, 0.35157403349876404, -0.22397643327713013, 0.009857296012341976, 0.19664093852043152, -0.9142982363700867, -0.10378198325634003, 0.3271143436431885, 0.2460096776485443, -0.5983900427818298, -0.6596220135688782, -0.42029061913490295, 0.31366950273513794, -0.5122414827346802, -0.610651969909668, 1.0245023965835571, -0.13125358521938324, -0.2960814833641052, -0.316866010427475, -0.5000227689743042, -0.29016759991645813, 0.34455713629722595, -1.5035687685012817, -0.8605346083641052, 0.6167235970497131, -0.1970060020685196, 0.3477373719215393, 0.16867922246456146, 1.174672245979309, 0.10206839442253113, -0.6459891200065613, 0.21417903900146484, 0.20748110115528107, -0.21806980669498444, -0.12002190947532654, -0.6392399072647095, 0.9265846610069275, 0.353488951921463, -0.17420046031475067, 0.29517102241516113, -0.08506393432617188, 0.14994995296001434, -0.6757180690765381, -0.2529192566871643, 0.7063334584236145, -1.0505650043487549, -0.6344274282455444, -0.8079856038093567, -0.8447211384773254, 0.8626012802124023, 0.8593251705169678, -0.304656982421875, 0.24917493760585785, 0.061098694801330566, -0.2772907614707947, 0.0763082355260849, -0.5653572678565979, 0.26222220063209534, 0.4054907560348511, -0.5109035968780518, -0.4601823389530182, -0.2038482278585434, 0.6259965300559998, -0.8762613534927368, -0.33312171697616577, -0.4396125376224518, 0.32387450337409973, -0.20516911149024963, 0.928226113319397, -0.4980667531490326, 0.3582491874694824, 1.1937685012817383, 0.040138501673936844, -0.9798680543899536, 0.12325696647167206, -0.8727763295173645, -0.12497405707836151, 0.16133292019367218, 0.45863470435142517, -0.3621121644973755, -0.19394251704216003, 0.7034314870834351, 0.3071332275867462, -0.5241140127182007, -0.4850580096244812, -0.5947133302688599, 0.4466124176979065, -0.5622308254241943, 0.4830901026725769, -0.015037093311548233, 0.3304319381713867, 0.27518853545188904, 0.3879391551017761, 0.7607097029685974, -0.10637066513299942, -0.37483909726142883, 0.11555097252130508, 0.3021203577518463, -0.19084958732128143, -0.48419034481048584, -0.15028929710388184, -1.6211353540420532, 0.15495619177818298, -1.0912646055221558, -0.13553474843502045, -0.6765154600143433, -0.5163393616676331, -0.12049736827611923, -0.11772042512893677, -0.12186998128890991, 0.16301049292087555, -0.630312442779541, 0.08671178668737411, -0.4577753245830536, -0.8299479484558105, 0.6855262517929077, 0.7852402329444885, -0.5775573253631592, -0.018191048875451088, 0.030309006571769714, 0.6598578095436096, 0.5742048025131226, 0.48805826902389526, -0.17497067153453827, -0.875412106513977, -1.5284861326217651, 0.5894806981086731, -0.048641420900821686, -0.171800896525383, -0.6995668411254883, 0.45318496227264404, 0.10458607971668243, 0.004320745822042227, -0.15395277738571167, 0.6192877292633057, -0.6562165021896362, -0.6476890444755554, 0.3110761344432831, -1.0894227027893066, 0.30882081389427185, 0.38651415705680847, -0.5122220516204834, -0.23008090257644653, 0.39058220386505127, -0.08381052315235138, -1.1922962665557861, -1.0094091892242432, 0.5535321831703186, -0.6689336895942688, 0.26893141865730286, -0.2865930199623108, 0.1409047245979309, -1.0985134840011597, -0.42735356092453003, -0.2383447140455246, 0.5522180795669556, -0.36343690752983093, 1.0465525388717651, 0.6540386080741882, -1.0883204936981201, -0.130616694688797, 0.4452967643737793, -0.1936938762664795, 0.41680851578712463, 0.7002840042114258, 0.13044990599155426, -0.0029620465356856585, 0.6832637786865234, 0.34603893756866455, 0.2666415572166443, -0.8957170844078064, 0.4178698658943176, 0.7940664291381836, -0.4894431233406067, -0.13620001077651978, 0.9100755453109741, -0.09283746033906937, -1.1653101444244385, 0.4095219373703003, -1.1892541646957397, -0.7091270685195923, -0.3463067412376404, 1.0105117559432983, 0.25968024134635925, -0.30308642983436584, -0.16686274111270905, -0.6452111601829529, 0.3156692385673523, -0.11574816703796387, -0.4837345480918884, 0.5879600644111633, -0.6086263656616211, 0.1450800746679306, 1.061851143836975, 0.8186138272285461, -0.4992561638355255, -1.2656519412994385, -0.687909722328186, -0.4361308217048645, -0.03289838880300522, 0.0014406050322577357, -0.48829522728919983, -0.19658882915973663, 0.8649157881736755, 0.39513206481933594, 0.37380096316337585, -0.28510674834251404, -0.028715770691633224, 0.15927597880363464, 0.7343849539756775, 0.10296095907688141, -0.7383617162704468, -0.3687167465686798, 1.4002606868743896, 1.387510895729065, -0.9467836022377014, -0.0878177285194397, 0.2289670705795288, -0.7885855436325073, 0.8072925806045532, 0.5418672561645508, -0.2630932331085205, 0.3170013427734375, -0.35853028297424316, 0.25136151909828186, 0.4874929189682007, -1.4016735553741455, 0.05009845271706581, 0.42227470874786377, 0.9808614253997803, 0.539322555065155, 0.42320340871810913, 0.2602745294570923, 0.6707740426063538, 0.09195327013731003, 0.06725763529539108, 0.4366573095321655, 0.454228013753891, -0.3109051585197449, 0.02410508506000042, 0.1413000375032425, 0.18577773869037628, -0.7886924743652344, -0.5628516674041748, 0.40217912197113037, 0.42741113901138306, 0.16616642475128174, 0.851040244102478, 0.8930534720420837, 0.5339739322662354, 0.40021950006484985, 0.30950281023979187, 0.7051836848258972, -0.28974026441574097, -0.29199376702308655, -0.10501600056886673, -0.8371276259422302, -0.16665992140769958, -0.18288995325565338, -0.6780730485916138, -0.51692134141922, -0.12368399649858475, 0.3294706344604492, -0.11560511589050293, 0.21458172798156738, 1.160603642463684, 0.4592128098011017, 0.47521406412124634, -0.25358787178993225, -0.9411041736602783, -0.5496928691864014, -1.0964391231536865, 0.2336886078119278, -0.32424813508987427, 0.2618708610534668, 0.09245942533016205, 0.04413183405995369, -0.09510291367769241]}, "authors": [{"authorId": "51131083", "name": "Chaojun Xiao"}, {"authorId": "2261405658", "name": "Pengle Zhang"}, {"authorId": "48506411", "name": "Xu Han"}, {"authorId": "2046958974", "name": "Guangxuan Xiao"}, {"authorId": "2427350", "name": "Yankai Lin"}, {"authorId": "2621696", "name": "Zhengyan Zhang"}, {"authorId": "2141313179", "name": "Zhiyuan Liu"}, {"authorId": "2249530374", "name": "Song Han"}, {"authorId": "2273551430", "name": "Maosong Sun"}], "references": [{"paperId": "1784c987e681d60c634765fe64c8d9c26f73d5ff", "title": "SnapKV: LLM Knows What You are Looking for Before Generation"}, {"paperId": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"paperId": "cf7ab5df804575bad88a9fcf0fbf7707bf500944", "title": "Training-Free Long-Context Scaling of Large Language Models"}, {"paperId": "c9603ec967879c24973b5bd48861df2e5555932e", "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"}, {"paperId": "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b", "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"}, {"paperId": "88ca4ee548d07263386ca8e4effc4a001bb2716f", "title": "Improving Text Embeddings with Large Language Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "4ea5ca620122e6a9a2b000444d36491cebf49c7c", "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"}, {"paperId": "a54761081c2b001c057fb6e1ea9a48058d5aa5e0", "title": "CLEX: Continuous Length Extrapolation for Large Language Models"}, {"paperId": "2b35b946a8ad64e018c24b283bc1c6c65d36fb67", "title": "Retrieval meets Long Context Large Language Models"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "28c6ac721f54544162865f41c5692e70d61bccab", "title": "A Survey on Large Language Model based Autonomous Agents"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "dbc368bc8b49347dd27679894524fa62f88492c9", "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"}, {"paperId": "5278a8eb2ba2429d4029745caf4e661080073c81", "title": "Generative Agents: Interactive Simulacra of Human Behavior"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "68adb03744692247fb834406798894db9fe77010", "title": "A Survey on Long Text Modeling with Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "a4867148c2f692efc6c22c3935a59be2d04ea3e9", "title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "88a74e972898de887ad9587d4c87c3a9f03f1dc5", "title": "MTEB: Massive Text Embedding Benchmark"}, {"paperId": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c", "title": "Code as Policies: Language Model Programs for Embodied Control"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "94f94e8892261d0377159379ca5a166ceae19a14", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "b03cf6324ecf7a295a4aeae5970c88d1a1c3f336", "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "bba5f2852b1db8a18004eb7328efa5e1d57cc62a", "title": "Key-Value Memory Networks for Directly Reading Documents"}, {"paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": "3689b7ca7b07924b6135b8a71b9f1b7937b0a3d5", "title": "Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding"}, {"paperId": "4747e72c5bc706c50e76953188f0144df18992d0", "title": "Communicative Agents for Software Development"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality"}, {"paperId": null, "title": "How long can open-source llms truly promise on context length?"}, {"paperId": null, "title": "LocalLLaMA. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "Memorizing trans-formers"}]}