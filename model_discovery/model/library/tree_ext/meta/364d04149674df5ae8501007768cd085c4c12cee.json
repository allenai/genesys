{"paperId": "364d04149674df5ae8501007768cd085c4c12cee", "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching", "abstract": "The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching. On the algorithm level, ALISA prioritizes tokens that are most important in generating a new token via a Sparse Window Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and reduces the memory footprint of KV caching at negligible accuracy loss. On the system level, ALISA employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems. In a single GPU-CPU system, we demonstrate that under varying workloads, ALISA improves the throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X, respectively.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching, employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems."}, "embedding": {"model": "specter_v2", "vector": [0.4028656780719757, 0.2591415047645569, -0.7201092839241028, 0.24399343132972717, -0.65608811378479, -0.2893070876598358, 0.606998860836029, 0.23105943202972412, -0.5841797590255737, -0.4646960198879242, 0.35698577761650085, -0.1741807907819748, 0.46535831689834595, -0.1251620203256607, -0.4408271312713623, 0.13116034865379333, -0.8434106707572937, 0.4148130714893341, -0.14368706941604614, 0.18578536808490753, -0.25248828530311584, -0.3692435324192047, -1.2002531290054321, 0.2225896418094635, 0.39759865403175354, 0.7687968015670776, 0.17969126999378204, 0.9342213273048401, -0.5366607904434204, 0.3195057511329651, 0.7004503607749939, 0.00984276831150055, 0.26146307587623596, 0.023496685549616814, -0.30353766679763794, -0.34291163086891174, 0.19558696448802948, -0.19527684152126312, -0.5465250611305237, 0.8072689771652222, -0.3359852433204651, 0.43769964575767517, 0.17578072845935822, -0.9395666718482971, -0.21859273314476013, 1.0098562240600586, 0.5200783610343933, 0.7372934222221375, -0.5552094578742981, -0.2262878715991974, 1.5921549797058105, -1.565976858139038, 0.3352031409740448, 1.1660352945327759, 0.4319193363189697, 0.01687656156718731, 0.008507699705660343, -0.5246168971061707, 0.8218324780464172, 0.40892294049263, -0.9912732243537903, -0.7674317359924316, -0.2989520728588104, 0.16229093074798584, 2.3262627124786377, -0.09411051124334335, 0.39977550506591797, 0.09354090690612793, -0.3422435522079468, 1.7729228734970093, -0.5273694396018982, -0.9988693594932556, -0.12225475907325745, -0.08861945569515228, 0.22003485262393951, 0.8564441204071045, -0.300099641084671, -0.05851403623819351, -0.967374324798584, -0.6612628698348999, 0.196663498878479, -0.2128351330757141, 0.31995177268981934, -0.13205447793006897, -0.17928509414196014, 0.9149633049964905, 0.08196805417537689, 0.5920350551605225, -0.2449701875448227, 0.5880056023597717, 0.7280110120773315, 0.16252563893795013, 0.37219947576522827, 0.496366411447525, -0.11294836550951004, -0.016417350620031357, -1.2167952060699463, 0.45356109738349915, -0.12740440666675568, 0.7980718612670898, -0.5414577722549438, 0.5758359432220459, -0.997946560382843, 0.18453514575958252, 1.3875741958618164, 0.4075618088245392, 0.28880876302719116, -0.48919034004211426, 0.06197463721036911, -0.6211887001991272, -0.06760028004646301, -0.23121057450771332, 0.006610036361962557, -0.05134229734539986, -0.8136354684829712, -1.172136664390564, -0.3252575099468231, 0.4535854160785675, -0.709185779094696, 0.5309687256813049, -0.1601371467113495, 0.629310131072998, 0.21496860682964325, 0.08252499997615814, 0.6360737085342407, 0.808550238609314, 0.30402055382728577, -0.3586680293083191, 1.3114560842514038, -0.9830897450447083, -0.4694114327430725, -1.101525068283081, 0.7998685240745544, -0.21009080111980438, 0.16723290085792542, -0.028012799099087715, -1.672602653503418, -0.6048553586006165, -0.4863692820072174, -0.10350798815488815, -0.6578823924064636, 0.2852446734905243, 0.9005336761474609, 0.13644848763942719, -0.8474358320236206, 0.34180694818496704, -0.5157017111778259, 0.004302314016968012, 0.5255416035652161, 0.12912912666797638, 0.4311165511608124, -0.2773030698299408, -1.4055540561676025, 0.24812236428260803, 0.08443119376897812, -0.3342334032058716, -0.06317900121212006, -0.9533108472824097, -1.2624261379241943, 0.3242044150829315, 0.3998885750770569, -0.3199087679386139, 1.4510682821273804, 0.3723572790622711, -1.4089210033416748, 0.5279498100280762, -0.750490665435791, -0.3111060857772827, 0.09247216582298279, -0.32864171266555786, -0.3941512703895569, -0.49098891019821167, -0.3310365676879883, 0.21092069149017334, 0.6402474641799927, 0.24137434363365173, -0.48447564244270325, 0.17768052220344543, -0.3379549980163574, -0.06678730249404907, -0.21232539415359497, 0.7554763555526733, -0.6239637732505798, -0.09140622615814209, 0.12952108681201935, 0.5772551894187927, -0.5958632826805115, -0.30786141753196716, -0.12906865775585175, -1.2302922010421753, 0.6182154417037964, 0.191628098487854, 1.282222867012024, -0.6729708313941956, -0.7034682035446167, 0.0963125005364418, 0.13748833537101746, 0.3722727298736572, -0.5777038335800171, 0.6221035122871399, -0.16245615482330322, 0.07470627129077911, 0.12158006429672241, -0.7868837714195251, 0.03753814473748207, -0.3196524381637573, -0.8434738516807556, -0.05738912895321846, 0.07987631857395172, 1.2962760925292969, -1.184890627861023, 0.029038388282060623, 0.14072969555854797, 0.14010556042194366, -1.2904599905014038, 1.322323203086853, -0.6849427223205566, -0.2558538615703583, -0.19226643443107605, -0.08630026876926422, 0.18591532111167908, -0.5946502089500427, 0.7012873888015747, -0.5312780737876892, -0.5418694615364075, 0.49528729915618896, -0.3636457622051239, 1.1863985061645508, -0.5826999545097351, 0.4451017379760742, -0.21093204617500305, -0.4527987837791443, -0.12040098756551743, 0.4663431942462921, -0.36677196621894836, -0.6546015739440918, 0.2383372038602829, 0.522372305393219, -0.5116254687309265, 0.15287072956562042, 0.9918204545974731, 1.2129155397415161, -0.45760393142700195, 0.15775810182094574, 0.4492439925670624, -0.08788221329450607, 0.6745831370353699, 0.5885868668556213, 0.6153934001922607, 0.06149754673242569, 0.7763971090316772, 0.027605026960372925, 0.5132800340652466, -0.5719543695449829, -0.19432762265205383, 0.9093371033668518, 0.9289057850837708, 0.41639208793640137, 0.4309088885784149, -0.7525425553321838, -0.5943344235420227, 0.3346948027610779, 0.4539145529270172, 1.487070083618164, -0.31494829058647156, -0.3241673707962036, -0.7204369306564331, -0.12600548565387726, -0.29307985305786133, 0.06145123764872551, -0.0316237211227417, 0.022530945017933846, -0.5596514940261841, -1.033734917640686, 1.2008601427078247, 0.2841602861881256, 0.5962204337120056, -0.9181959629058838, -0.3169225752353668, -0.27173227071762085, 0.15508529543876648, -1.0602208375930786, -0.5187550187110901, 0.2449520230293274, -0.3071236312389374, 0.08102349191904068, 0.2933797240257263, 0.10540205240249634, 0.16959074139595032, -0.6300073266029358, 0.8133370280265808, -0.6020966172218323, -0.3975825607776642, 0.0029613282531499863, 0.5448494553565979, -0.6124858856201172, -0.9404605031013489, 0.3504924476146698, 0.0979117900133133, -0.2715309262275696, 0.34428513050079346, 0.5694876909255981, 0.19071659445762634, -0.6421735882759094, -0.2902187705039978, 0.056314948946237564, 0.17301812767982483, 0.21203915774822235, 0.757490873336792, -0.7221333384513855, -0.29791948199272156, -1.3584283590316772, 0.9499306678771973, 0.10215053707361221, -0.9081758856773376, 0.37523719668388367, -0.755372166633606, -0.31438761949539185, 0.5792727470397949, -0.7572312355041504, -0.12216144800186157, -0.6414522528648376, -0.0788888931274414, -0.47172051668167114, -0.13821040093898773, 0.29888013005256653, 0.27025842666625977, 0.24865682423114777, 0.01635712757706642, 1.0305570363998413, 0.3734063506126404, -0.1875811070203781, 0.868753969669342, -0.5208296775817871, 0.5415701866149902, 0.12343621999025345, 0.1749570518732071, -0.08026526123285294, -0.18663473427295685, -1.0418668985366821, -0.32514268159866333, -0.6922606825828552, -0.0014340791385620832, -0.08978971838951111, -0.14975230395793915, -0.9034651517868042, -1.1149060726165771, 0.162028506398201, -1.4336655139923096, -0.28475603461265564, 0.11015815287828445, -0.12189193069934845, -0.2217976301908493, -1.1508378982543945, -1.302706003189087, -0.6848801374435425, -1.147490382194519, -1.2312390804290771, 0.5025160908699036, -0.09069106727838516, -0.5491192936897278, -0.45649850368499756, 0.2507992684841156, -0.6401336789131165, 0.9252361059188843, -0.8632721900939941, 0.918261706829071, -0.1592419296503067, -0.22236531972885132, -0.18091081082820892, 0.09947552531957626, 0.012148027308285236, -0.6408357620239258, 0.16577410697937012, -0.8958976864814758, 0.3320225179195404, -0.7689765691757202, -0.24358157813549042, 0.10677029192447662, 0.6003456115722656, 0.856002688407898, -0.20611247420310974, -0.8564606308937073, 0.4388388693332672, 1.4549673795700073, -0.6276934146881104, 0.10495979338884354, 0.19012729823589325, 1.1244243383407593, -0.6569836139678955, 0.1204301118850708, 0.8258494138717651, 0.14768239855766296, 0.5164086222648621, 0.19969554245471954, -0.34058675169944763, 0.23328588902950287, -0.32887357473373413, 0.48137128353118896, 1.7103818655014038, 0.3573704957962036, -0.4109189808368683, -0.8101757764816284, 0.7377961277961731, -1.5213613510131836, -0.822881817817688, 0.3788210451602936, 0.6140896677970886, 0.3646557629108429, -0.5481497049331665, -0.10593274980783463, -0.29283571243286133, 0.10775338858366013, 0.4331931173801422, -0.46977460384368896, -1.1141242980957031, 0.2885824143886566, 0.5776223540306091, 0.3126245141029358, 0.6320326924324036, -0.21904143691062927, 1.0401078462600708, 14.548126220703125, 1.4909579753875732, 0.10152484476566315, 0.594201385974884, 0.6070652604103088, 0.24560725688934326, -0.29102617502212524, -0.27680885791778564, -1.6569429636001587, -0.3303971290588379, 1.3513859510421753, -0.26878154277801514, 0.44199874997138977, 0.46028727293014526, 0.3731662333011627, 0.35467609763145447, -0.2303788810968399, 0.49637266993522644, 0.30114755034446716, -1.8100132942199707, 0.7250903844833374, 0.144380122423172, 0.09374148398637772, 0.8064441680908203, 0.5878576636314392, 0.6561646461486816, 0.7007112503051758, -0.6462613940238953, 0.6301578879356384, 0.07951273769140244, 0.9676764011383057, -0.26170435547828674, 0.2322702258825302, 0.5663067102432251, -1.3728783130645752, 0.09953892976045609, -0.6639368534088135, -1.1703715324401855, 0.13668391108512878, 0.33108797669410706, -0.8299828171730042, -0.5322939157485962, -0.3281210660934448, 0.8740793466567993, 0.6696742177009583, 0.4643881320953369, 0.0013549347640946507, 0.5747659206390381, -0.14174336194992065, 0.054542217403650284, 0.5416688323020935, 0.46856409311294556, 0.19260647892951965, 0.3197999894618988, 0.1743561029434204, 0.10728021711111069, 0.2492070347070694, 0.5088247060775757, -0.5527083277702332, -0.08959842473268509, -0.5452911853790283, -0.4022197425365448, 0.4153904914855957, 0.9845317602157593, 0.45300015807151794, 0.05407276377081871, -0.5642305612564087, 0.16708841919898987, 0.8093096613883972, -0.29414451122283936, -0.2621273100376129, 0.11541122943162918, 0.7502492070198059, -0.697062075138092, 0.342549592256546, 0.701449453830719, 0.01839594542980194, -0.5063737630844116, -0.8799983859062195, -0.8131502270698547, 0.37769508361816406, -0.29859375953674316, -0.5531675219535828, 0.9935023784637451, -0.19269822537899017, -0.43699315190315247, 0.04210474342107773, -0.5149461627006531, -0.24369892477989197, 0.292184442281723, -1.2764406204223633, -0.8841857314109802, 0.8501355051994324, -0.30123570561408997, -0.07236624509096146, 0.21976031363010406, 1.5242630243301392, 0.06581256538629532, -0.24470946192741394, 0.32211753726005554, 0.37995076179504395, -0.06382831186056137, -0.6766823530197144, -0.46578025817871094, 1.3964896202087402, 0.46322348713874817, 0.08795186877250671, 0.33469200134277344, -0.015115919522941113, 0.300048291683197, -0.9473994374275208, -0.26095446944236755, 0.9001022577285767, -0.7470394968986511, -0.11973809450864792, -1.1140305995941162, -0.7930006384849548, 0.346955269575119, 0.35004788637161255, 0.015439122915267944, 0.010835056193172932, 0.1289689838886261, -0.46122053265571594, 0.1639806628227234, -0.5019213557243347, 0.08668551594018936, 0.5215299725532532, -0.6236953735351562, 0.293933242559433, -0.07358071953058243, 0.5976681113243103, -1.3724029064178467, -0.8177544474601746, -0.2891007363796234, 0.43174663186073303, 0.4464833736419678, 1.2484289407730103, -0.4039919376373291, 0.4420582950115204, 0.9323336482048035, 0.3284517228603363, -0.726628839969635, 0.08300003409385681, -0.9522308111190796, -0.5021124482154846, -0.014106766320765018, 0.816222608089447, -0.29562485218048096, 0.3451978266239166, 0.6222599148750305, 0.48190441727638245, -0.5798805952072144, -0.6117077469825745, -0.3322136402130127, -0.0036933801602572203, -0.6785874962806702, 0.4671132564544678, 0.012988514266908169, 0.20658957958221436, -0.08950253576040268, 0.05822458118200302, 0.9666700959205627, -0.2257343828678131, -0.39202451705932617, 0.30466219782829285, 0.12236882746219635, 0.09865639358758926, -0.49434417486190796, -0.30055129528045654, -1.1117719411849976, 0.03550740331411362, -1.1632895469665527, 0.12833011150360107, -0.5901727080345154, -0.2209641933441162, -0.03549722582101822, 0.00697158370167017, 0.37712910771369934, 0.25816670060157776, -0.31183433532714844, -0.5271552801132202, -0.7329619526863098, -0.6839064955711365, 0.6234773397445679, 0.596876859664917, -0.620147168636322, 0.09854798018932343, 0.1095358356833458, 0.44812455773353577, 0.35586604475975037, 0.24200282990932465, -0.5493592619895935, -0.5242886543273926, -1.01896333694458, 0.44716936349868774, 0.08290785551071167, -0.1941041797399521, -0.3841748833656311, 0.907899796962738, 0.28262361884117126, -0.39740827679634094, 0.02023239992558956, 0.17694737017154694, -0.7065768837928772, -0.6115012764930725, 0.5180898904800415, -0.8427050113677979, 0.35858362913131714, 0.5865383744239807, -0.6804006099700928, -0.28609007596969604, 0.6799411177635193, -0.20537225902080536, -1.1912317276000977, -0.8585044145584106, 0.5732079148292542, -0.94098299741745, 0.49905985593795776, -0.6422680616378784, 0.16554036736488342, -0.8395463228225708, -0.12189926952123642, 0.18471474945545197, 0.3865870237350464, -0.5473387837409973, 0.9166568517684937, 0.5174111127853394, -0.9263739585876465, 0.04048692807555199, 0.4407866895198822, -0.0944511666893959, 0.11131548881530762, 0.4318448007106781, 0.4786612391471863, -0.13773402571678162, 0.7161404490470886, 0.4833921194076538, 0.13813011348247528, -1.3659898042678833, -0.041808851063251495, 0.3174454867839813, -1.0656373500823975, -0.26435521245002747, 1.084944248199463, -0.5500536561012268, -0.9049029350280762, -0.15053187310695648, -1.419446349143982, -0.46122270822525024, -0.42130136489868164, 0.672655463218689, 0.2036675661802292, 0.16823254525661469, 0.09146443754434586, -0.7005715370178223, -0.27305999398231506, -0.4808715879917145, -0.2639217972755432, 0.6229957342147827, 0.10539010167121887, -0.560439944267273, 0.3334019184112549, 0.5257096886634827, -0.44540539383888245, -0.2097230851650238, -0.9470660090446472, -0.46602803468704224, -0.05210757628083229, 0.6097787022590637, 0.11018344759941101, -0.40391674637794495, 0.5425149202346802, 0.2114853858947754, 0.37082362174987793, -0.19145311415195465, -0.16243234276771545, 0.5067722797393799, 0.6087633967399597, -0.09181546419858932, -0.42516908049583435, -1.0193626880645752, 1.34911048412323, 0.7124332785606384, -0.7201806902885437, 0.36454635858535767, -0.15136800706386566, -0.6709258556365967, 0.7398476004600525, 0.19159169495105743, 0.008702808059751987, 0.7333042025566101, 0.20351631939411163, 0.05160468444228172, 0.09669254720211029, -1.2827786207199097, -0.5685808658599854, 0.8621244430541992, 0.6615602970123291, 0.6794593334197998, 0.2583814859390259, 0.13612224161624908, 0.6763082146644592, 0.5248673558235168, 0.3320801854133606, 0.3091149926185608, 0.4562300145626068, -0.1362183541059494, -0.26464295387268066, -0.02181336283683777, 0.8959399461746216, -0.652015209197998, -1.3238264322280884, 0.4037976562976837, 0.6449934244155884, -0.09099029004573822, 0.22999368607997894, 1.0743563175201416, 0.5016955733299255, 0.30003291368484497, 0.0635455921292305, 0.5469313263893127, -0.5129414796829224, -0.23371116816997528, -0.21590359508991241, -0.58795565366745, -0.20227313041687012, 0.0068941363133490086, -0.5496012568473816, -0.536014974117279, -0.19271516799926758, 0.1387549638748169, 0.33322450518608093, -0.0029129167087376118, 1.2637566328048706, 0.750423789024353, 0.3946045935153961, -0.6070148944854736, -0.5088151693344116, -0.27849361300468445, -0.6697458624839783, 0.03265192359685898, -0.6724834442138672, -0.28948092460632324, 0.20941394567489624, 0.10069626569747925, -0.41310808062553406]}, "authors": [{"authorId": "2240287994", "name": "Youpeng Zhao"}, {"authorId": "2293656271", "name": "Di Wu"}, {"authorId": "2239968290", "name": "Jun Wang"}], "references": [{"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "e28f4687b9ddf562807d12d9799add07aa191d51", "title": "Efficient Transformer Inference with Statically Structured Sparse Attention"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "9b069ba5259d229bfd4fe3ac3768148e2d1092f8", "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "9807032064b278b72611ddbe504a3ce396d33517", "title": "Robust Quantization: One Model to Rule Them All"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "eb2eb53f46071170fdc9aa70d703b6aea52c932c", "title": "Visual Choice of Plausible Alternatives: An Evaluation of Image-based Commonsense Causal Reasoning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9", "title": "Dynamic Network Surgery for Efficient DNNs"}, {"paperId": "7601b995303f953955004db7b9b8b206c0e02ff8", "title": "Learning Structured Sparsity in Deep Neural Networks"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "d7e89fc53355892081fa03f17e80c58bb4dae36d", "title": "An anomaly in space-time characteristics of certain programs running in a paging machine"}, {"paperId": "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d", "title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training"}, {"paperId": "97d825693c8fce2211700e6075195abade77ec32", "title": "Llama"}, {"paperId": null, "title": "\u201cA framework for few-shot language model evaluation,\u201d"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "\u201cStanford alpaca: An instruction-following llama model,\u201d"}, {"paperId": null, "title": "\u201cIntroducting chatgpt,\u201d"}]}