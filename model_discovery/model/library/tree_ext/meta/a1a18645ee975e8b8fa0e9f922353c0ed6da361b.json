{"paperId": "a1a18645ee975e8b8fa0e9f922353c0ed6da361b", "title": "Does compressing activations help model parallel training?", "abstract": "Large-scale Transformer models are known for their exceptional performance in a range of tasks, but training them can be difficult due to the requirement for communication-intensive model parallelism. One way to improve training speed is to compress the message size in communication. Previous approaches have primarily focused on compressing gradients in a data parallelism setting, but compression in a model-parallel setting is an understudied area. We have discovered that model parallelism has fundamentally different characteristics than data parallelism. In this work, we present the first empirical study on the effectiveness of compression methods for model parallelism. We implement and evaluate three common classes of compression algorithms - pruning-based, learning-based, and quantization-based - using a popular Transformer training framework. We evaluate these methods across more than 160 settings and 8 popular datasets, taking into account different hyperparameters, hardware, and both fine-tuning and pre-training stages. We also provide analysis when the model is scaled up. Finally, we provide insights for future development of model parallelism compression algorithms.", "venue": "Conference on Machine Learning and Systems", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2301.02654", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work presents the first empirical study on the effectiveness of compression methods for model parallelism, and implements and evaluates three common classes of compression algorithms - pruning-based, learning- based, and quantization-based - using a popular Transformer training framework."}, "embedding": {"model": "specter_v2", "vector": [0.39882326126098633, 0.2295760214328766, -0.5855574607849121, -0.02358674630522728, -0.33865097165107727, 0.6584284901618958, 0.5279671549797058, 0.07624949514865875, -0.6418097615242004, -0.5438292026519775, 0.5635734796524048, -0.10907013714313507, 0.289863258600235, -0.014859343878924847, -0.273800790309906, -0.13866934180259705, -1.2739390134811401, 0.1079074963927269, 0.16328653693199158, -0.23356680572032928, -0.0016283428994938731, -0.10920003056526184, -1.1629818677902222, 0.5521597266197205, 0.19899165630340576, 1.168235421180725, -0.19038650393486023, 0.7345166802406311, -0.4977022707462311, 0.35703328251838684, 0.4503653049468994, 0.027377862483263016, 0.6098082661628723, 0.13445624709129333, -0.18689768016338348, -0.19852860271930695, 0.6105451583862305, -0.2056899219751358, -0.7448675632476807, 0.6586734652519226, -0.24367578327655792, 0.11272180080413818, -0.06426765769720078, -0.8410413265228271, 0.36817044019699097, 0.5437325835227966, 0.03188462555408478, 1.192993402481079, -0.7820949554443359, -0.8308305740356445, 0.7357134819030762, -1.181908130645752, -0.255887508392334, 1.3762645721435547, 0.855438768863678, 0.14181482791900635, -0.26228848099708557, -0.6893121004104614, 0.5654007196426392, 0.1666254848241806, -0.7146349549293518, -0.32790958881378174, -0.06354735791683197, 0.06567749381065369, 1.811690092086792, -0.019154895097017288, 0.3866001069545746, 0.5526102781295776, 0.28229135274887085, 1.276625633239746, 0.2804887592792511, -0.674357533454895, -0.04367799684405327, -0.05518459901213646, 0.1997109353542328, 0.8516746759414673, -0.32241156697273254, 0.8564896583557129, -1.2399497032165527, -0.17493517696857452, 0.5187415480613708, 0.11486257612705231, 0.19005727767944336, -0.11655566096305847, 0.23546215891838074, 0.6770751476287842, 0.6222888231277466, 0.4868520200252533, -0.35866454243659973, 1.1414542198181152, 0.7063008546829224, 0.4023342430591583, 0.18317028880119324, 0.04189953953027725, 0.1343047171831131, 0.16719330847263336, -1.1985121965408325, 0.01584979146718979, -0.20567767322063446, 0.8299331665039062, -0.2093542218208313, 0.47163352370262146, -0.5899092555046082, 0.34906280040740967, 1.2009143829345703, -0.3592734634876251, 0.8756917715072632, -0.6241986155509949, 0.5638442039489746, -0.9110692739486694, -0.3718798756599426, -0.4515862762928009, -0.1414852887392044, -0.6845429539680481, -1.003859043121338, -0.9948256611824036, -0.8612945675849915, 0.07882523536682129, -0.9184941053390503, 0.42399677634239197, -0.47512707114219666, 0.647085428237915, 0.03193620592355728, 0.47408294677734375, -0.012408715672791004, 0.48933157324790955, 0.5026232004165649, 0.04303734004497528, 0.8205637335777283, -1.0867478847503662, -0.18230865895748138, -1.2104661464691162, 0.37357887625694275, -0.28550809621810913, -0.029454320669174194, -0.08160494267940521, -1.5319561958312988, -1.2328613996505737, -0.8986262679100037, 0.28187698125839233, -0.25408655405044556, -0.3777828514575958, 1.438361406326294, 0.2648319602012634, -1.0983637571334839, 1.3775721788406372, -0.5371703505516052, 0.04113433510065079, 0.8590518832206726, 0.38850146532058716, 0.40506696701049805, -0.10798536241054535, -1.057151436805725, -0.07184604555368423, 0.09464242309331894, -0.7325630784034729, -0.22044450044631958, -0.9058414697647095, -0.8068245649337769, 0.40624991059303284, 0.0072546168230473995, -0.5138397216796875, 1.134702205657959, -0.0006314089987426996, -0.5971543788909912, 0.33131369948387146, -0.09347996860742569, -0.43305349349975586, 0.3229769766330719, -0.06993187963962555, -0.5204825401306152, -0.04692872241139412, -0.3456086218357086, 1.0281035900115967, 0.7502579092979431, -0.11786025762557983, -0.39383620023727417, -0.09473126381635666, -0.5350629091262817, 0.3029590845108032, -0.6146085262298584, 0.8076366186141968, -0.558781623840332, -0.2813464403152466, 0.7179177403450012, 0.6940528750419617, -0.4567054808139801, -0.11066058278083801, -0.017268477007746696, -0.43953970074653625, 0.778128445148468, -0.0008369555580429733, 0.8051155805587769, -0.9775159358978271, -1.1674262285232544, 0.24277116358280182, 0.013412073254585266, -0.16294682025909424, -0.8250145316123962, 0.6102495193481445, -0.5574483275413513, 0.48966163396835327, 0.12746533751487732, -1.0174529552459717, -0.30477163195610046, 0.08232203125953674, -0.8997558355331421, -0.477984756231308, 0.008548719808459282, 0.8866925835609436, -0.673391580581665, -0.05138279125094414, -0.22484195232391357, 0.6236814260482788, -0.9243542551994324, 0.8320212364196777, -0.04353862255811691, -0.0636686161160469, 0.14196236431598663, -0.1638840138912201, 0.23946058750152588, -0.373890221118927, 0.5931732654571533, -0.4390641450881958, 0.0036977725103497505, 0.7143610715866089, -0.2574740946292877, 1.0572221279144287, -0.23645718395709991, 0.2878604829311371, -0.12271097302436829, -0.9639027118682861, 0.303699791431427, 0.4174313545227051, 0.361002653837204, -0.8239587545394897, 0.2993186116218567, 0.35444533824920654, -0.4322115480899811, 0.6583187580108643, 1.0591152906417847, 0.6520545482635498, -0.1372240036725998, -0.09592306613922119, 0.5174983143806458, -0.45939046144485474, 0.7898309826850891, 0.2991222143173218, 0.5768623948097229, 0.15563271939754486, -0.1197516992688179, 0.06312774121761322, -0.17950105667114258, -0.9888728857040405, -0.046556055545806885, 0.7100958228111267, 0.6741995215415955, 0.639580488204956, 0.47576913237571716, -0.7187303900718689, -0.6941249966621399, 0.1665862500667572, 0.48450592160224915, 1.381278395652771, -0.37230217456817627, -0.3539215326309204, -0.5588744282722473, -0.30309152603149414, -0.09599324315786362, -0.37076741456985474, -0.22105689346790314, -0.5646772980690002, -0.4711161255836487, -1.2543458938598633, 0.793127715587616, 0.7597415447235107, 1.2498880624771118, -0.13076251745224, -0.42772242426872253, -0.48091450333595276, 0.49016639590263367, -0.544783890247345, -0.5552452206611633, 1.0432872772216797, -1.3227440118789673, -0.19046896696090698, 0.22315233945846558, -0.04012349247932434, 0.15419551730155945, -0.24772818386554718, 0.9491152763366699, 0.004660334438085556, -0.42214709520339966, -0.06723440438508987, 0.7741745114326477, -0.765049934387207, -0.5770682096481323, 0.027918273583054543, -0.031909141689538956, -0.5815984010696411, 0.4085323214530945, 0.06715646386146545, -0.003465226385742426, -0.1313953548669815, -0.448356568813324, 0.32214027643203735, 0.2436159998178482, -0.017018062993884087, 1.0944180488586426, -0.15860126912593842, -0.2565070688724518, -0.8664461374282837, 1.2967612743377686, 0.11572366952896118, -0.3538767397403717, -0.07786090672016144, -1.0495244264602661, -0.11082766205072403, 0.593382716178894, -0.5494224429130554, 0.1497981995344162, -1.1099183559417725, 0.030884230509400368, -0.9850771427154541, -0.21579746901988983, 0.11202478408813477, 1.1007441282272339, -0.32163000106811523, 0.407271146774292, 0.3788895010948181, 0.3358306586742401, -0.07416222989559174, 0.27991175651550293, -0.9946664571762085, 0.6447432041168213, 0.15027190744876862, -0.1753111481666565, -0.05884614586830139, 0.32304802536964417, -0.6942986249923706, -0.16803361475467682, -0.37425777316093445, -0.0817023366689682, -0.1140187457203865, -0.15747743844985962, -0.4254474639892578, -0.7079010605812073, -0.6342661380767822, -0.6188857555389404, -0.2949755787849426, -0.4151380658149719, -0.016229767352342606, -0.2395373284816742, -0.8862091898918152, -1.728401780128479, -0.3232784867286682, -0.8874215483665466, -1.1305482387542725, 0.23723934590816498, 0.20640264451503754, -0.08165301382541656, -0.34338051080703735, -0.4779432415962219, -0.681864857673645, 1.3635191917419434, -0.7652938961982727, 0.5275169610977173, -0.08189421147108078, 0.1082150787115097, -0.06173507124185562, -0.08512694388628006, 0.6698926687240601, -0.598385214805603, 0.04525337740778923, -1.0881935358047485, 0.2327301949262619, -0.146174818277359, -0.4858599007129669, 0.6461402177810669, 0.389503538608551, 1.2355802059173584, 0.05561544746160507, -0.5938745737075806, 0.5829421281814575, 1.123722791671753, -0.4924672544002533, 0.13608375191688538, -0.0640663355588913, 0.5486422181129456, -0.05540277063846588, -0.374739408493042, 0.5433170199394226, -0.42844149470329285, 0.3071150779724121, 0.11921785026788712, -0.3518514931201935, -0.3590947687625885, -0.6651676297187805, 0.10917577892541885, 1.6437649726867676, 0.3949296772480011, 0.3561311364173889, -0.7621221542358398, 0.05672147125005722, -1.011447787284851, -0.7546685338020325, 0.7706632018089294, 0.8771436214447021, 0.3706379532814026, -0.14223894476890564, -0.274597704410553, 0.30080556869506836, 0.1698182374238968, 0.5228750109672546, -0.10536627471446991, -0.8721143007278442, 0.4286211431026459, 0.9297801852226257, 0.7783068418502808, 0.5003811120986938, -0.20369668304920197, 0.2816922664642334, 14.920485496520996, 0.7815504670143127, -0.5173072218894958, 0.4854535162448883, 0.8752634525299072, 0.3268983066082001, -0.10213753581047058, -0.18849970400333405, -1.130393147468567, 0.1524672508239746, 1.5189599990844727, 0.382863312959671, 0.6768764853477478, 0.2853001356124878, -0.032311610877513885, 0.09446559101343155, -0.21086083352565765, 0.7043421268463135, 0.40446528792381287, -1.4170427322387695, 0.17044131457805634, 0.15057630836963654, 0.22050097584724426, 0.5472912788391113, 0.7626965641975403, 0.8784748911857605, 0.13599525392055511, -0.46818971633911133, 0.32049092650413513, 0.18556855618953705, 0.7951862215995789, -0.10090324282646179, 0.358856737613678, 0.26262444257736206, -1.0061168670654297, -0.3759593665599823, -0.8914427757263184, -1.0714796781539917, 0.5043473839759827, 0.24369366466999054, -0.7338382601737976, -0.3057040870189667, -0.10901404917240143, 0.5316150188446045, -0.1697012335062027, 0.5335038304328918, -0.04049110412597656, 0.8537312150001526, -0.547299325466156, 0.48346805572509766, 0.43067964911460876, 0.2099626362323761, -0.3214789628982544, 0.11414012312889099, -0.031762368977069855, -0.4259357154369354, 0.42858168482780457, 0.11920152604579926, -0.49846237897872925, -0.2858889698982239, -0.14016593992710114, -0.42034149169921875, -0.12242167443037033, 0.7181771993637085, 0.2950476109981537, -0.003678832668811083, -0.3799336552619934, 0.2596146762371063, 0.4597766399383545, 0.077403724193573, -0.5272157788276672, 0.22658158838748932, 0.24283051490783691, -0.4402475655078888, -0.06460005789995193, 0.6798046231269836, -0.39119377732276917, -0.24281133711338043, -0.8091784119606018, -0.3966107964515686, 0.47800514101982117, -0.7087574601173401, -0.5008271932601929, 0.8238775134086609, 0.22413058578968048, -0.3660418689250946, 0.3445514142513275, -0.6843031048774719, -0.21251565217971802, 0.3501066565513611, -1.5125222206115723, -0.1460069715976715, -0.22122454643249512, -0.4254629611968994, -0.399885356426239, -0.024731211364269257, 1.2290765047073364, 0.6767091751098633, -0.23935002088546753, 0.007115879561752081, -0.4936443269252777, -0.4699084162712097, -0.3416612446308136, -0.5835517048835754, 0.9786991477012634, 0.2745223939418793, 0.4626425504684448, 0.15664644539356232, -0.3197832405567169, 0.2928942143917084, -0.500005304813385, -0.1342010796070099, 0.5321227312088013, 0.23313385248184204, -0.42628008127212524, -0.4666668474674225, -1.0193742513656616, 0.3545655310153961, 0.28353971242904663, -0.01821422390639782, 0.5485306978225708, 0.1461743414402008, -0.5505635738372803, -0.15528559684753418, -0.6144192218780518, -0.11497873812913895, 0.6119521260261536, -0.5781496167182922, 0.04876229166984558, 0.06702164560556412, 0.1865045577287674, -1.3047772645950317, -0.6528858542442322, -0.07758096605539322, -0.18457429111003876, -0.2814166247844696, 1.1514711380004883, -0.5818649530410767, 0.864514946937561, 1.1340593099594116, -0.17029798030853271, -0.6852784156799316, -0.0016913923900574446, -0.4426313042640686, -0.00569115299731493, -0.37592095136642456, 0.2749021053314209, -0.13210709393024445, 0.7673203945159912, 0.5278098583221436, -0.2417021244764328, -0.615679919719696, -0.42863136529922485, -0.11558908224105835, -0.30195704102516174, -0.6685399413108826, 0.30739349126815796, -0.31279560923576355, 0.011439060792326927, 0.0893675684928894, 0.5812969207763672, 0.09273979812860489, -0.5180732011795044, -0.8164059519767761, -0.01694699563086033, 0.1360025554895401, -0.23298504948616028, -0.46685564517974854, -0.5901038646697998, -1.3880047798156738, -0.28938329219818115, -1.3299105167388916, -0.12163587659597397, -0.7638975381851196, -0.5430110096931458, -0.34807756543159485, -0.3790501058101654, 0.22636224329471588, 0.5246153473854065, -0.2419234961271286, -0.23482944071292877, -0.46530482172966003, -0.3631304204463959, 0.6897498965263367, 0.581150233745575, -0.39477914571762085, 0.1558687686920166, -0.19365893304347992, 0.12378255277872086, 0.26037609577178955, 0.7256473898887634, -0.575497031211853, -1.0237431526184082, -1.1548898220062256, -0.07879996299743652, -0.013351362198591232, -0.17629437148571014, -1.322260856628418, 0.653247594833374, 0.2609442472457886, 0.2430199831724167, 0.17186781764030457, 0.5677977800369263, -0.9983715415000916, -0.4855068624019623, 0.4908536970615387, -0.8658373355865479, 0.4484938681125641, 0.7181193828582764, -0.4739251732826233, -0.4687972366809845, 0.6774622201919556, 0.24931778013706207, -0.7033556699752808, -0.8253718018531799, 0.40403661131858826, -0.32829421758651733, 0.044935230165719986, -0.39004960656166077, 0.043454550206661224, -1.0989990234375, -0.03481665998697281, 0.08140150457620621, 0.11168263107538223, -0.48214784264564514, 0.831684947013855, 0.5500982403755188, -1.2332936525344849, 0.5876023173332214, 0.9278445839881897, -0.4093778133392334, -0.2965649664402008, 0.44458144903182983, 0.8760010004043579, -0.3805904686450958, 0.37615957856178284, -0.18627884984016418, 0.3051447868347168, -0.8851834535598755, -0.013742197304964066, 0.9110946655273438, -0.6637348532676697, -0.12876258790493011, 1.0474785566329956, -0.8325068950653076, -0.7128159403800964, 0.3398827016353607, -1.4841883182525635, -0.41822555661201477, -0.5300095081329346, 0.6248361468315125, 0.4072163999080658, 0.35277634859085083, 0.4133561849594116, -0.2235729843378067, -0.16848881542682648, 0.07517825812101364, -0.4966927170753479, 0.21755191683769226, 0.29654091596603394, 0.06280597299337387, 0.24571682512760162, 0.9189167618751526, -0.8071728348731995, -0.8287668824195862, -0.4743025600910187, -0.23387311398983002, -0.6476766467094421, 0.5268939137458801, -0.09173774719238281, -1.1298521757125854, 1.036530613899231, 0.9414657354354858, 0.17330628633499146, 0.3117847442626953, -0.37696272134780884, 0.17330844700336456, 0.3448682129383087, 0.06529482454061508, -0.9001139402389526, -0.2904314398765564, 0.8729561567306519, 1.071025013923645, -0.6149411201477051, 0.6292715668678284, -0.3850173056125641, -0.7179426550865173, 0.9627389311790466, 0.37468254566192627, -0.06358931958675385, 0.7569452524185181, 0.38450688123703003, -0.15392711758613586, 0.027505090460181236, -1.0559583902359009, -0.17010672390460968, 0.4042094051837921, 1.0969442129135132, 0.9102910757064819, 0.05620811507105827, -0.03276732936501503, 0.5493439435958862, 0.05085030943155289, 0.30806785821914673, 0.16100341081619263, 0.6734534502029419, -0.3172224760055542, -0.29298320412635803, -0.00977426115423441, 1.1447948217391968, -0.7969661951065063, -0.6971670389175415, 0.6405072808265686, 0.817884624004364, 0.2453535795211792, 0.5670378804206848, 1.2928831577301025, -0.3520929515361786, 0.7134789824485779, 0.12293785065412521, 0.46118441224098206, -0.306656152009964, -0.6351621747016907, 0.11749640852212906, -0.6322639584541321, -0.433869332075119, -0.09457384794950485, -0.20225217938423157, -0.566459596157074, -0.268317848443985, 0.857641875743866, 0.3285025954246521, 0.5530195832252502, 0.47301483154296875, 0.7230176329612732, 0.8721176981925964, -0.40194404125213623, -0.6699352264404297, -0.8855673670768738, -0.8550987243652344, -0.07367101311683655, -0.21798129379749298, -0.6807482242584229, 0.005515099037438631, -0.045814234763383865, -0.33283379673957825]}, "authors": [{"authorId": "3399186", "name": "S. Bian"}, {"authorId": "2117961435", "name": "Dacheng Li"}, {"authorId": "2109798334", "name": "Hongyi Wang"}, {"authorId": "2064963077", "name": "Eric P. Xing"}, {"authorId": "2697906", "name": "S. Venkataraman"}], "references": [{"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "ff5eb1bd55d61ae919865f6b4dab84e6ae1974f3", "title": "Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression"}, {"paperId": "2701d5b55ddacbdc82bc33901451f593a92127f1", "title": "MPCFormer: fast, performant and private Transformer inference with MPC"}, {"paperId": "1e2fd6d64e7eea23d713c98bcc8664c1cf052d35", "title": "AMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness"}, {"paperId": "b7a4c84f699d85716b7b26de29e832ec5f928ded", "title": "Fine-tuning Language Models over Slow Networks using Activation Compression with Guarantees"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9", "title": "How to Train BERT with an Academic Budget"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "0e2d8b8d81092037f9866c1ceddcebb87318e38b", "title": "AST: Audio Spectrogram Transformer"}, {"paperId": "63e838bb935f5ebe3498107e753f07f08a8b5689", "title": "An Image is Worth 16x16 Words, What is a Video Worth?"}, {"paperId": "f1ceb2df3ab1ace364edf30398a770ba9260b17a", "title": "Adaptive Gradient Communication via Critical Learning Regime Identification"}, {"paperId": "8c518d1859d892640247fc6b4beb75545864ebc9", "title": "Pufferfish: Communication-efficient Models At No Extra Cost"}, {"paperId": "d99ae9e262dbba2d0232cac12bb1a3bfadeabb53", "title": "On the Utility of Gradient Compression in Distributed Training Systems"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "040ad14a2c97e51510889ae6a0c3c23b29da801d", "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "05e41e129cb49cafc36810c7b062f707ada13fce", "title": "The Design and Operation of CloudLab"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "b4a632a7097e7d0631250884dfc6e1f76b376996", "title": "PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "38f1ef8ab96e5e0195abcd197bf6df47eb308e8a", "title": "Sparsified SGD with Memory"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "b611636f3cfe7b9aa41a606bec1d9fa72e1359ae", "title": "ATOMO: Communication-efficient Learning via Atomic Sparsification"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "e2c8726d092aea573e69f5b0a2654225883cfacf", "title": "Horovod: fast and easy distributed deep learning in TensorFlow"}, {"paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54", "title": "signSGD: compressed optimisation for non-convex problems"}, {"paperId": "92495abbac86394cb759bec15a763dbf49a8e590", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "079932bf6ff8b99c899172ba60071818f6b5dfcb", "title": "Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters"}, {"paperId": "e8437e3b32f091f62f3796556435e139db130f90", "title": "Sparse Communication for Distributed Gradient Descent"}, {"paperId": "ce6403e99465e5e8a48d5c2017fc23976e29fe59", "title": "FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks"}, {"paperId": "c9d64aaa2007b60ef7814acc895dd90f15578a20", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2a894be44d07a963c28893cc6f45d29fbfa872f7", "title": "STRADS: a distributed framework for scheduled model parallel machine learning"}, {"paperId": "510a6ec82a2af8cdee185418c3296cb8e2b9716c", "title": "8-Bit Approximations for Parallelism in Deep Learning"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "50684b147b752a07c313cb73d864f7b21bd8b703", "title": "Scaling Distributed Machine Learning with the Parameter Server"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "3439a127e45fb763881f03ef3ec735a1db0e0ccc", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"paperId": "acbd13c7be621a7284da4ab9d8caa40f1a558ce2", "title": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server"}, {"paperId": "3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f", "title": "Autoencoders, Minimum Description Length and Helmholtz Free Energy"}, {"paperId": null, "title": "Alpa: Automating inter-and intra-operator parallelism for distributed deep learning"}, {"paperId": null, "title": "Pytorch distributed: Experiences on accelerating data parallel training"}, {"paperId": "a9278b95d8bd1ef3833424611a85856a3ff62bc7", "title": "Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "The convergence of sparsi-fied gradient methods"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "f8b1b43f284f1246ca015cc002ac949bb67c5645", "title": "Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures"}, {"paperId": null, "title": "Does compressing activations help model parallel training?"}]}